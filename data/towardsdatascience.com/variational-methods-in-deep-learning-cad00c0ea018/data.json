{"url": "https://towardsdatascience.com/variational-methods-in-deep-learning-cad00c0ea018", "time": 1683017519.923979, "path": "towardsdatascience.com/variational-methods-in-deep-learning-cad00c0ea018/", "webpage": {"metadata": {"title": "Variational Methods in Deep Learning | by Branislav Holl\u00e4nder | Towards Data Science", "h1": "Variational Methods in Deep Learning", "description": "Deep neural networks are a flexible family of models wide applications in AI and other fields. Even though these networks often encompass millions or even billions of parameters, it is still possible\u2026"}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/Variational_Bayesian_methods", "anchor_text": "Wikipedia", "paragraph_index": 17}, {"url": "https://www.jeremyjordan.me/variational-autoencoders/", "anchor_text": "this great tutorial", "paragraph_index": 30}], "all_paragraphs": ["Deep neural networks are a flexible family of models wide applications in AI and other fields. Even though these networks often encompass millions or even billions of parameters, it is still possible to train them effectively using the maximum likelihood principle as well as stochastic gradient descent techniques. Unfortunately, this learning procedure only gives us a point estimate of the parameters and it is hard to endow the model with any sort of prior knowledge about the parameters. Additionally, it is not easily possible to incorporate any stochastic elements into the models, such as samples from a predefined or learned distribution.", "Bayesian variational inference provides a natural framework for these issues, since the very idea of Bayesian learning is to infer the shapes of distributions instead of point estimates of parameters. Unfortunately, the added complexity of this approach makes it hard to use in deep neural networks.", "In this post, I will try to show how we can overcome these difficulties through an approach known as probabilistic programming. This method allows us to largely automatize the process of statistical inference in the models, making it easy to use without having to know all the tricks and intricacies of Bayesian inference in large models.", "Variational inference is an essential technique in Bayesian statistics and statistical learning. It was originally developed as an alternative to Monte-Carlo techniques. Like Monte-Carlo, variational inference allows us to sample from and analyze distributions that are too complex to calculate analytically.", "In variational inference, we use a distribution which is easy to sample from and adjust its parameters to resemble a target posterior distribution as closely as possible. Surprisingly, we may perform this approximation even though we do not know the target distribution exactly.", "More concretely, let us assume that we are given a set of latent (i.e. unobserved) variables Z_1, \u2026, Z_n and some data X. As an example, the data could be images, while the latent variables could represent latent factors in the images, such as if the image is a portrait, if it depicts a cat or a dog or if it is a photograph or a painting. Since the latent variables are hidden, we don\u2019t know their values in general.", "Let us now define a parametrized statistical model including both Z and X as:", "In this context, P(Z) is known as the prior, while P(X|Z) is the likelihood of the data given the latents. The learning criterion is usually assumed as maximizing the log-evidence:", "Hereby, P(X) is called the evidence, since it describes the probability of the data (evidence) with parameters \u03b8. P(X) is defined as:", "Unfortunately, this integral is usually intractable even for known values of \u03b8. If we tried to maximize the log-evidence directly, we would have to calculate the integral anew for every value of \u03b8 during training.", "During training, we are also interested to calculate the probability of the latent variables given the data, which is given by the Bayes theorem as:", "This probability is called the posterior in Bayesian literature and the procedure of calculating it is often referred to as inference. Note that this quantity is intractable due to the evidence term in the denominator.", "In variational inference, we now define a variational distribution Q(Z) to approximate the posterior for the given data:", "We define Q such that we can easily sample from it. In principle, we are free to take any distribution we like, however if Q resembles P more closely, the approximation will be tighter. Notice that Q comes with its own set of parameters \u03c6. During the training, we will try to optimize \u03c6 and \u03b8 to achieve two goals:", "Although this task now seems even more difficult than the original one, I will show you how to efficiently solve it using gradient descent by defining an appropriate loss function.", "As mentioned above, our goal is to make Q as close to P as possible while maximizing the log-evidence. We want to perform this optimization iteratively using gradient descent. However, we have yet to find a suitable loss function for our needs. Intuitively, we would like the loss function to include two terms:", "It turns out that the Kullback-Leibler divergence (KL-divergence) is a good measure for the distance between distributions. It is defined as", "By shuffling the terms of this equation around (you can find the precise derivation on Wikipedia) we arrive at the equation", "This equation is significant since it tells us that we can write the intractable log evidence as the KL-divergence between Q and P minus a term we will call Evidence Lower BOund (ELBO). Since the KL-divergence is non-negative, it follows that maximizing the ELBO will also maximize the evidence. Our loss function is thus:", "The terms inside the expectation are easy to calculate since we have everything we need: the log-joint is simply the sum of the log-prior and the log-likelihood, and log-Q is tractable by definition. We will look at how to optimize the ELBO in the next section.", "Recently, a new class of log-evidence bounds emerged called Importance-Weighted Lower Bound (IWLB) defined as", "This bound is equal to ELBO for K=1 and otherwise is tigher than the ELBO. For more information please refer to [1].", "Let us now figure out how to estimate the gradient of ELBO using arbitrary stochastic functions P and Q. In particular, we want to obtain unbiased Monte-Carlo estimates of", "In other words, we have to get the gradient computation inside the expectation. How do we do this in general, if the expectation depends on the gradient parameters?", "If Q has a particular form, it turns out that we may circumvent the problem by reparametrizing the distribution:", "As you can see, the second expectation does not depend on \u03c6 anymore. Therefore, we may pull the gradient computation inside the expectation. As an example, consider the reparameterization of the normal distribution:", "Since N(0,1) does not depend on any parameters, we may freely differentiate w.r.t. the operation.", "Unfortunately, this trick does not work for all distributions. In particular, it fails with discrete distributions. In this case, our only hope is a so-called REINFORCE estimator. This estimator uses the following equation:", "Therefore, we may rewrite the expectation gradient as", "This solves our issue of differentiability and provides us with a Monte-Carlo estimator. Unfortunately, this estimator tends to have a large variance. In some cases, it is not even possible to efficiently calculate the gradient at all. Fortunately, it is often possible to reduce the variance of the estimator, using e.g. the structure of the model or a baseline reduction similar to the one used in policy gradient methods in reinforcement learning. For more details, please refer to [2].", "I will now present an application of the above variational framework: the variational autoencoder [3]. The variational autoencoder is a directed probabilistic generative model. If you are unfamiliar with the basics of variational autoencoders, have a look at this great tutorial.", "At runtime, the variational autoencoder takes a random value sampled from a prior P(Z) and passes it through a neural network called the decoder P(X|Z) with parameters \u03b8 to obtain the result, e.g. an image. The number of dimensions in the latent (prior) space as well as the underlying distribution can be varied to suit our dataset.", "We may write the joint probability density of the variational autoencoder as", "This means that for every data point Xi we have a point in latent space Zi. In the case of variational inference, we would also require the variational parameters to be different for every data point. For instance, if we used a normal distribution as the prior (as we often do), the mean and variance would be the variational parameters and they would have a different value for every input data point.", "In order to avoid this and learn a single set of parameters, we introduce another neural network, the encoder Q(Z|X), to represent a variational estimate of the posterior P(Z|X). We use the encoder during the training phase to learn how to map the input X to the variational parameters. Thus, we will use a single set of neural network parameters \u03c6 to parametrize Q. This is usually referred to as amortized inference, since we amortize the inference cost across the entire dataset.", "In order to learn both the optimal latent space as well as minimize the reconstruction error in the autoencoder by minimizing the ELBO in this model. Since we are usually working with a continuous distribution, we can use the reparametrization trick in order to compute the derivatives. The ELBO can be written as", "The first term actually corresponds to the reconstruction error, so it could be the mean squared error. The second term minimizes the distance between the encoder and the prior given the data point.", "Furthermore, since the joint distribution P(X,Z) factors over all data points, we may freely use mini-batch sampling as we usually do for feed-forward neural networks. You may also use your favorite optimizer (SGD, Adam etc.) for the optimization procedure. One implementation detail: please be aware that most implementations of optimizers only allow you to minimize a value instead of maximizing as we are doing here. In this case, you can just minimize the negative of the ELBO and you\u2019re good to go.", "This blog post focused on the applications of variational inference in deep learning. As you hopefully saw, variational inference can be automatized to a large degree by solving gradient estimation for a wide variety of distributions. However, one always has to be mindful of the estimator variance, especially when using the REINFORCE estimator. Fortunately, frameworks such as Pyro make variational inference and probabilistic reasoning simple to use and often also take care of variance reduction and other tricks.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "AI | Software Development | Other Crazy Interests"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fcad00c0ea018&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvariational-methods-in-deep-learning-cad00c0ea018&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvariational-methods-in-deep-learning-cad00c0ea018&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvariational-methods-in-deep-learning-cad00c0ea018&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvariational-methods-in-deep-learning-cad00c0ea018&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----cad00c0ea018--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----cad00c0ea018--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@branislav.hollander?source=post_page-----cad00c0ea018--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@branislav.hollander?source=post_page-----cad00c0ea018--------------------------------", "anchor_text": "Branislav Holl\u00e4nder"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcb9a2fa1a025&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvariational-methods-in-deep-learning-cad00c0ea018&user=Branislav+Holl%C3%A4nder&userId=cb9a2fa1a025&source=post_page-cb9a2fa1a025----cad00c0ea018---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcad00c0ea018&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvariational-methods-in-deep-learning-cad00c0ea018&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcad00c0ea018&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvariational-methods-in-deep-learning-cad00c0ea018&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.pexels.com/@skitterphoto?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels", "anchor_text": "Skitterphoto"}, {"url": "https://www.pexels.com/photo/red-cylindrical-metal-9333/?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels", "anchor_text": "Pexels"}, {"url": "https://en.wikipedia.org/wiki/Variational_Bayesian_methods", "anchor_text": "Wikipedia"}, {"url": "https://www.jeremyjordan.me/variational-autoencoders/", "anchor_text": "this great tutorial"}, {"url": "http://pyro.ai/examples/svi_part_iii.html", "anchor_text": "http://pyro.ai/examples/svi_part_iii.html"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----cad00c0ea018---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----cad00c0ea018---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/bayesian-statistics?source=post_page-----cad00c0ea018---------------bayesian_statistics-----------------", "anchor_text": "Bayesian Statistics"}, {"url": "https://medium.com/tag/variational-inference?source=post_page-----cad00c0ea018---------------variational_inference-----------------", "anchor_text": "Variational Inference"}, {"url": "https://medium.com/tag/variational-autoencoder?source=post_page-----cad00c0ea018---------------variational_autoencoder-----------------", "anchor_text": "Variational Autoencoder"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcad00c0ea018&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvariational-methods-in-deep-learning-cad00c0ea018&user=Branislav+Holl%C3%A4nder&userId=cb9a2fa1a025&source=-----cad00c0ea018---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcad00c0ea018&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvariational-methods-in-deep-learning-cad00c0ea018&user=Branislav+Holl%C3%A4nder&userId=cb9a2fa1a025&source=-----cad00c0ea018---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcad00c0ea018&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvariational-methods-in-deep-learning-cad00c0ea018&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----cad00c0ea018--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fcad00c0ea018&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvariational-methods-in-deep-learning-cad00c0ea018&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----cad00c0ea018---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----cad00c0ea018--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----cad00c0ea018--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----cad00c0ea018--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----cad00c0ea018--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----cad00c0ea018--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----cad00c0ea018--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----cad00c0ea018--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----cad00c0ea018--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@branislav.hollander?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@branislav.hollander?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Branislav Holl\u00e4nder"}, {"url": "https://medium.com/@branislav.hollander/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcb9a2fa1a025&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvariational-methods-in-deep-learning-cad00c0ea018&user=Branislav+Holl%C3%A4nder&userId=cb9a2fa1a025&source=post_page-cb9a2fa1a025--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F61e7fe52ba73&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvariational-methods-in-deep-learning-cad00c0ea018&newsletterV3=cb9a2fa1a025&newsletterV3Id=61e7fe52ba73&user=Branislav+Holl%C3%A4nder&userId=cb9a2fa1a025&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}