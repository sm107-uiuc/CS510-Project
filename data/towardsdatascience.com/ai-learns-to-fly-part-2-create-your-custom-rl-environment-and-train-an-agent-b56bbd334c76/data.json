{"url": "https://towardsdatascience.com/ai-learns-to-fly-part-2-create-your-custom-rl-environment-and-train-an-agent-b56bbd334c76", "time": 1683013018.8308558, "path": "towardsdatascience.com/ai-learns-to-fly-part-2-create-your-custom-rl-environment-and-train-an-agent-b56bbd334c76/", "webpage": {"metadata": {"title": "AI learns to fly (Part 2) | Create your custom RL environment and train an agent | Towards Data Science", "h1": "AI learns to fly (Part 2) | Create your custom Reinforcement Learning environment and train your agent", "description": "In this article we will be presenting a complete example on how to simply create your custom RL environment using Python/Tensorforce and how to customize and train/test state of the art agents in your environment."}, "outgoing_paragraph_urls": [{"url": "https://medium.com/@yannberthelot1/how-i-taught-a-plane-to-fly-using-rl-c170a152b771", "anchor_text": "here", "paragraph_index": 1}, {"url": "https://tensorforce.readthedocs.io/en/latest/", "anchor_text": "Tensorforce", "paragraph_index": 8}, {"url": "https://tensorforce.readthedocs.io/en/latest/basics/getting-started.html?highlight=environment#initializing-an-environment", "anchor_text": "Tensorforce\u2019s doc", "paragraph_index": 8}, {"url": "https://tensorforce.readthedocs.io/en/latest/basics/getting-started.html?highlight=environment#initializing-an-environment", "anchor_text": "doc", "paragraph_index": 9}, {"url": "https://medium.com/@jonathan_hui/rl-proximal-policy-optimization-ppo-explained-77f014ec3f12#:~:text=A%20quote%20from%20OpenAI%20on,simpler%20to%20implement%20and%20tune.&text=Instead%20of%20imposing%20a%20hard,penalty%20in%20the%20objective%20function.", "anchor_text": "this", "paragraph_index": 28}, {"url": "https://medium.com/aureliantactics/ppo-hyperparameters-and-ranges-6fc2d29bccbe", "anchor_text": "this article", "paragraph_index": 38}, {"url": "https://arxiv.org/pdf/1707.06347.pdf", "anchor_text": "original paper", "paragraph_index": 38}, {"url": "https://www.coursera.org/learn/complete-reinforcement-learning-system/home/welcome", "anchor_text": "https://www.coursera.org/learn/complete-reinforcement-learning-system/home/welcome", "paragraph_index": 61}], "all_paragraphs": ["From Icarus burning his wings to the Wright brothers soaring through the sky, it took mankind thousands of years to learn how to fly, but how long will it take an AI to do the same?", "Welcome back to this series of articles where I am going to write about my journey into using Artificial Intelligence to make a plane fly (Part 1 is here).", "In the previous article, we defined the transition function from a state to another based on the agent\u2019s action.", "In this article, we will be presenting a complete example of:", "We will begin with a brief recap of what reinforcement learning is. Then we are going to define our environment (actions, states, rewards) for today\u2019s exercise (the plane will learn to take-off as efficiently as possible). Finally, we will define our agent (The type of agent we will use, hyperparameters optimization, training, and testing).", "Reinforcement Learning is the branch of Machine Learning that allows training an agent on how to select the best sequence of actions depending on the current state to maximize long term returns. This sequence of actions is called a policy. Reinforcement Learning mimics the real-life process of learning through trial and error.", "The agent starts in a given state, it takes an action and then observes the state of its environment and gets a reward (or punishment) for reaching this state. The goal of Reinforcement Learning is to understand the relation between the actions, states, and rewards in order to find the best sequence of actions to take in any given situation. This fits perfectly our problem as controlling the plane can easily be described as the sequence of actions to take on thrust and pitch allowing for the best trajectories.", "In our example of a virtual plane, the relationship between actions (thrust and pitch) and outcomes (reward and new state) is known since we have designed the physical model to compute them in the previous article. However, it is not known to the agent (otherwise it would become planification and not Reinforcement Learning). Therefore the agent will have to understand how its environment works to select the best actions.", "As mentioned earlier we will be using Tensorforce to implement our work. This library allows us to create our custom environments fairly easily with a nice amount of customization. It provides an implementation of state of the art agents which is easily customizable. Tensorforce\u2019s custom environment definition requires the following information (from Tensorforce\u2019s doc):", "-Init is self-explanatory and initializes the values needed to create the environment (see the doc for more details)", "-States defines the shape and type of the state representation. In the example, it is represented by 8 floats (hence it is of 8 continuous dimensions).", "-Actions defines the shape and type of the actions made available to the agent. In the example, the agent has the possibility to pick an action between 1,2,3 and 4 (where each action would then be processed by the environment to create the next step, for example, we could imagine the following relations in a grid world game, 1: Go up, 2: Go down, 3: Go left, 4: Go right).", "-Reset puts the environment back into its starting state in order for a new episode to start. It is thus identical to States.", "-Execute processes the action chosen by the agent and collects the new state, the reward, and whether or not the agent reached a terminal state.", "Let\u2019s now customize our environment! For our first case, we will train CaptAIn (our AI pilot) to learn how to take-off. We will use it as an example to go through all the steps in detail.", "Let\u2019s begin by defining our states and actions.", "To be able to learn how to fly, we want CaptAIn to observe the position and velocity (both horizontal and vertical) of its aircraft. We, therefore, want our states spaces to be of 4 dimensions (2 dimensions for the horizontal and vertical position and 2 dimensions for horizontal en vertical velocity).", "To fly the aircraft, we want our CaptAIn to be able to control the Thrust (or power) and the Pitch (or direction) of the plane. The thrust will be limited from 50% to 100% of the engine\u2019s thrust by 10% increments (50, 60, 70, 80, 90, 100% of maximum power). This will help the training by limiting the power to only values realistically used during flight. The pitch will be limited to realistic positive values (from 0 to 15\u00b0).", "This leads to the following implementation :", "Let\u2019s now investigate the terminal state. We will define 3 ways for an episode to end:", "self.FlightModel.Pos[1] is the vertical position of the plane above the runway and self.FlightModel.Pos[0] its horizontal position from the start of the runway.", "The next step is the rewards definition. They need to be designed to encourage the agent to perform our task, it needs to be encountered sufficiently frequently for the agent to learn quickly while being sufficiently precise for the agent to learn only the behavior we seek. We want our CaptAIn to take-off as quickly as possible, therefore we want to encourage the shortest take-off distance or the shortest take-off time (the two are directly related).", "To achieve this we will be penalizing the agent for every time step it takes to reach the objective. We can think of it as withdrawing 1 dollar from the CaptAIn\u2019s bank account for every second it takes to take-off. We will also reward him if it reaches its goal (reaching an altitude of 25m), the amount will be based on how much runway there was left when it took-off. We can think of it as roughly giving him 1 dollar per meter of runway left in order to motivate him to use the least runway possible while also rewarding him for successfully taking-off.", "Last but not least we will use the transition function defined in the previous article to compute the next state and reward based on the current state and action (execute the agent\u2019s actions).", "The transition function (compute_timestep) is defined in our airplane environment and goes as follows:", "For your custom environment, you would have to provide your own transition function. Now that the environment is defined, let\u2019s move on to the Agent definition!", "Our CaptAIn needs to be able to understand a complex relation (our Transition function, which is unknown to him) between continuous states (position and velocity) and discrete actions (thrust and pitch) in order to maximize its long-term rewards.", "Tensorforce has multiple agents (Deep Q-network, Dueling QDN, Actor-Critic, PPO \u2026) already implemented so we don\u2019t have to do it ourselves.", "In order not to overload this article we won\u2019t go in detail into the agent selection nor how they work. After trying out the different agents it was clear that the most fitted for our task was the PPO (Proximal Policy Optimization) algorithm. For more details on how it works please refer to this article. The specificities and advantages between each of them could be the subject of an entire article but we will stay focused on implementation today.", "Here is a basic implementation of the PPO agent taken from the Tensorforce documentation. We will go over the hyperparameters in a following part and stick with the default values for the moment.", "With the agent operational, we have completed the whole framework.", "Now that the environment and the agent are good to go, fasten your seatbelt and get ready for take-off!", "Let\u2019s take a look at the basic code to initialize our environment and train and test the agent.", "This function takes the environment we defined earlier as well as our agent and runs it for the given number of episodes. There are two modes, training mode (where exploration is allowed), used for training the agent over a large number of episodes; and test mode (where exploration is not allowed) used to evaluate the agent\u2019s performance.", "The run function will be called by the runner function in order to execute episodes into batches to monitor the results during training by performing a test run after each batch and gather the results.", "And here is the main script which controls those functions.", "Let\u2019s take a look at the results of this basic script:", "The results are not very impressive. Not only does the agent gets worse over time by needing more runway to take-off, but the best take-off distance is also far from the real-life results of about 1.7km.", "In order to improve our results, let\u2019s take a look at our agent\u2019s main hyperparameters (for a deeper understanding of those hyperparameters, please read this article and the original paper on PPO):", "In order to find the best values for these parameters we will be running a series of episodes for different values of them and observe the best combination by using the take-off distance as a score. The script we are going to use for this is basically a grid-search. We will directly freeze the other hyperparameters to their default values or to arbitrary values which we won\u2019t try to optimize in this example.", "We then use this function to iterate over a range of hyperparameters and compute the best combination. Each combination will be run over the span of 100 batches of 100 episodes.", "Let\u2019s begin by examining the non-network parameters results (subsampling fraction, likelihood ratio clipping, l2 regularization, entropy regularization).", "The following graphs represent the mean of the take-off distances amongst all configurations with the same value of a given parameter (e.g. in the first graph the blue curve represents the mean take-off distance for all episodes where the subsampling fraction was of 0.2). We can see the evolution of this distance as the agent learns over episodes with the same parameters. The colored area around the curves represents standard deviation.", "From these graphs we can get the best values for the studied parameters while also observing that they don\u2019t have a very important impact on the results:", "We then do the same for the network parameters (size of each layer, and depth of the neural network)", "The best network parameters are :", "Let\u2019s now take a look at the last two parameters, discount, and exploration :", "For the discount factor, we can see that for the value of 0.5, it does not really improve over the span of the study, for the 0.9 discount it does reach a plateau of around 2000m take-off distance; and finally, the 1.0 discount reaches a plateau of 1700m while also taking less time to converge than the 0.9 discount factor (around 50 episodes rather than around 80). We will thus select the 1.0 discount factor.", "For the exploration factor, the results are more tied and all 3 factors (1%, 0.1%, and 0.01%) all seem to do quite well in the end (even though 0.1% seems to converge more slowly). However the 1% exploration factor seems to be the best at the end of the study, we will thus select this value.", "Using the fixed hyperparameters, the environment we defined, and the agent we trained we can now observe how CaptAIn is doing.", "On this graph, we can see the take-off distance achieved after each batch for a hundred batch of a hundred episodes each. We can see the take-off distance is decreasing overall with some periodic degradation due to exploration. The final take-off distance achieved is of 1725 meters.", "This value is comparable to the ones achieved by actual A320 (the plane we are modeling) therefore the results are encouraging! Let\u2019s now take a look at how the agent does it, a.k.a its policy. We start by taking a look at the training phase :", "This is the evolution of the policy every thousand episodes. Each graph represents what the agent did in terms of Thrust and Pitch at each timestep of the episode. The blue curve (with values on the left) is the Thrust and the red curve (with values on the right) is the Pitch angle. At first, the agent performs seemingly random actions and does not manage to take-off.", "We can then see how the agent progressively converges towards applying full thrust for the whole episode and only rotating once the plane has reached enough speed. The high variability in the actions is due to exploration and stochasticity inherent to training the agent. We will now switch to test mode to observe the actual policy (without exploration).", "In this animation, we can see the evolution of the policy for every batch of 100 episodes over 100 batches. Similarly to the training phase, we observe the agent converge towards its final policy: applying full power for the whole episode and only rotating after gaining sufficient speed (at around 30s). We can also observe the take-off time reducing from the max allowed time for an episode of 100s to 33s.", "Here is how it goes :", "To conclude CaptAIn has managed to take-off, and even better, as good as the actual pilots! Even though this does not seem very impressive as a human would have quite easily figured out that applying full power and rotating the plane after some speed would be the way, we have to remember that CaptAIn has never seen a plane taking-off and has discovered it entirely on its own!", "Now that we have seen how to create a custom environment in Tensorforce and use it to train and optimize an agent, you can now use the framework we have defined to apply Reinforcement Learning to your own environments and problems!", "CaptAIn has just learned how to take-off, it remains to be seen how to make him achieve level flight and landing. However, those tasks were not as easy as taking-off (like in reality) and require some more complex approaches than just using pre-defined agents and \u201csimple\u201d rewards. In order to keep the present article on the subject of how to do a simple RL problem from scratch, we will go over reward engineering on a following article in order for captAIn to land!", "Thanks for reading, I truly enjoyed sharing my journey into Reinforcement Learning and I would greatly appreciate your returns. Stay tuned for next part!", "The full code (Python) I used can be found on my Github :", "https://www.coursera.org/learn/complete-reinforcement-learning-system/home/welcome : Great MOOC on RL theory (but lacks a little bit or real implementation, it is mostly maths, henceforth the importance of trying your own projects !)", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Scientist | Fond of Reinforcement Learning and Research"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fb56bbd334c76&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fai-learns-to-fly-part-2-create-your-custom-rl-environment-and-train-an-agent-b56bbd334c76&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fai-learns-to-fly-part-2-create-your-custom-rl-environment-and-train-an-agent-b56bbd334c76&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fai-learns-to-fly-part-2-create-your-custom-rl-environment-and-train-an-agent-b56bbd334c76&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fai-learns-to-fly-part-2-create-your-custom-rl-environment-and-train-an-agent-b56bbd334c76&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----b56bbd334c76--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b56bbd334c76--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@yannberthelot1?source=post_page-----b56bbd334c76--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@yannberthelot1?source=post_page-----b56bbd334c76--------------------------------", "anchor_text": "Yann Berthelot"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fbe4a07da40d5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fai-learns-to-fly-part-2-create-your-custom-rl-environment-and-train-an-agent-b56bbd334c76&user=Yann+Berthelot&userId=be4a07da40d5&source=post_page-be4a07da40d5----b56bbd334c76---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb56bbd334c76&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fai-learns-to-fly-part-2-create-your-custom-rl-environment-and-train-an-agent-b56bbd334c76&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb56bbd334c76&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fai-learns-to-fly-part-2-create-your-custom-rl-environment-and-train-an-agent-b56bbd334c76&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/@yannberthelot1/how-i-taught-a-plane-to-fly-using-rl-c170a152b771", "anchor_text": "here"}, {"url": "https://tensorforce.readthedocs.io/en/latest/", "anchor_text": "Tensorforce"}, {"url": "https://tensorforce.readthedocs.io/en/latest/basics/getting-started.html?highlight=environment#initializing-an-environment", "anchor_text": "Tensorforce\u2019s doc"}, {"url": "https://tensorforce.readthedocs.io/en/latest/basics/getting-started.html?highlight=environment#initializing-an-environment", "anchor_text": "doc"}, {"url": "https://gfycat.com/fr/sleepyjampackedauklet", "anchor_text": "gfycat"}, {"url": "https://medium.com/@jonathan_hui/rl-proximal-policy-optimization-ppo-explained-77f014ec3f12#:~:text=A%20quote%20from%20OpenAI%20on,simpler%20to%20implement%20and%20tune.&text=Instead%20of%20imposing%20a%20hard,penalty%20in%20the%20objective%20function.", "anchor_text": "this"}, {"url": "https://giphy.com/gifs/season-15-the-simpsons-15x18-3orieOcI2dFlcEqH9S", "anchor_text": "Giphy"}, {"url": "https://medium.com/aureliantactics/ppo-hyperparameters-and-ranges-6fc2d29bccbe", "anchor_text": "this article"}, {"url": "https://arxiv.org/pdf/1707.06347.pdf", "anchor_text": "original paper"}, {"url": "https://giphy.com/gifs/foofighters-foo-fighters-learn-to-fly-3ohs7MNCq8mhWzWdyM/links", "anchor_text": "Giphy"}, {"url": "https://giphy.com/gifs/season-14-the-simpsons-14x9-d2VNG3GkpLdlyio8", "anchor_text": "Giphy"}, {"url": "https://github.com/YannBerthelot/PlaneModel", "anchor_text": "YannBerthelot/PlaneModelPlaneModel environment for RL. Contribute to YannBerthelot/PlaneModel development by creating an account on GitHub.github.com"}, {"url": "https://www.coursera.org/learn/complete-reinforcement-learning-system/home/welcome", "anchor_text": "https://www.coursera.org/learn/complete-reinforcement-learning-system/home/welcome"}, {"url": "https://tensorforce.readthedocs.io/en/latest/", "anchor_text": "https://tensorforce.readthedocs.io/en/latest/"}, {"url": "https://medium.com/aureliantactics/ppo-hyperparameters-and-ranges-6fc2d29bccbe", "anchor_text": "https://medium.com/aureliantactics/ppo-hyperparameters-and-ranges-6fc2d29bccbe"}, {"url": "https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e", "anchor_text": "https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e"}, {"url": "https://arxiv.org/pdf/1707.06347.pdf", "anchor_text": "https://arxiv.org/pdf/1707.06347.pdf"}, {"url": "https://toppng.com/black-and-white-airplane-PNG-free-PNG-Images_71843", "anchor_text": "https://toppng.com/black-and-white-airplane-PNG-free-PNG-Images_71843"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----b56bbd334c76---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----b56bbd334c76---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----b56bbd334c76---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/python?source=post_page-----b56bbd334c76---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/tutorial?source=post_page-----b56bbd334c76---------------tutorial-----------------", "anchor_text": "Tutorial"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb56bbd334c76&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fai-learns-to-fly-part-2-create-your-custom-rl-environment-and-train-an-agent-b56bbd334c76&user=Yann+Berthelot&userId=be4a07da40d5&source=-----b56bbd334c76---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb56bbd334c76&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fai-learns-to-fly-part-2-create-your-custom-rl-environment-and-train-an-agent-b56bbd334c76&user=Yann+Berthelot&userId=be4a07da40d5&source=-----b56bbd334c76---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb56bbd334c76&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fai-learns-to-fly-part-2-create-your-custom-rl-environment-and-train-an-agent-b56bbd334c76&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b56bbd334c76--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fb56bbd334c76&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fai-learns-to-fly-part-2-create-your-custom-rl-environment-and-train-an-agent-b56bbd334c76&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----b56bbd334c76---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----b56bbd334c76--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----b56bbd334c76--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----b56bbd334c76--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----b56bbd334c76--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----b56bbd334c76--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----b56bbd334c76--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----b56bbd334c76--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----b56bbd334c76--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@yannberthelot1?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@yannberthelot1?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Yann Berthelot"}, {"url": "https://medium.com/@yannberthelot1/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "94 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fbe4a07da40d5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fai-learns-to-fly-part-2-create-your-custom-rl-environment-and-train-an-agent-b56bbd334c76&user=Yann+Berthelot&userId=be4a07da40d5&source=post_page-be4a07da40d5--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F61a4bf566099&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fai-learns-to-fly-part-2-create-your-custom-rl-environment-and-train-an-agent-b56bbd334c76&newsletterV3=be4a07da40d5&newsletterV3Id=61a4bf566099&user=Yann+Berthelot&userId=be4a07da40d5&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}