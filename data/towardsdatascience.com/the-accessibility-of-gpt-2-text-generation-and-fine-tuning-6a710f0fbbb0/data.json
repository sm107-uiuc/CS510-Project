{"url": "https://towardsdatascience.com/the-accessibility-of-gpt-2-text-generation-and-fine-tuning-6a710f0fbbb0", "time": 1683001662.2183008, "path": "towardsdatascience.com/the-accessibility-of-gpt-2-text-generation-and-fine-tuning-6a710f0fbbb0/", "webpage": {"metadata": {"title": "The Accessibility of GPT-2 \u2014 Text Generation and Fine-tuning | by Yoel Zeldes | Towards Data Science", "h1": "The Accessibility of GPT-2 \u2014 Text Generation and Fine-tuning", "description": "Natural Language Generation (NLG) is a well studied subject among the NLP community. With the rise of deep learning methods, NLG has become better and better. Recently, OpenAI has pushed the limits\u2026"}, "outgoing_paragraph_urls": [{"url": "https://openai.com/blog/better-language-models", "anchor_text": "GPT-2", "paragraph_index": 0}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "Transformers", "paragraph_index": 0}, {"url": "https://arxiv.org/abs/1508.07909", "anchor_text": "token", "paragraph_index": 0}, {"url": "https://huggingface.co/transformers", "anchor_text": "released an API", "paragraph_index": 1}, {"url": "https://spell.run", "anchor_text": "Spell", "paragraph_index": 2}, {"url": "https://community.spell.run/hc/en-us/articles/360038909713-GPT-2-text-generation-is-not-something-to-joke-about", "anchor_text": "here", "paragraph_index": 3}, {"url": "https://arxiv.org/abs/1508.07909", "anchor_text": "Byte Pair Encoding", "paragraph_index": 6}, {"url": "https://openai.com/blog/better-language-models", "anchor_text": "GPT-2", "paragraph_index": 8}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "Transformers architecture", "paragraph_index": 8}, {"url": "https://huggingface.co/transformers", "anchor_text": "released an API", "paragraph_index": 9}, {"url": "https://raw.githubusercontent.com/amoudgl/short-jokes-dataset/master/data/onelinefun.csv", "anchor_text": "short-jokes-dataset", "paragraph_index": 13}, {"url": "http://anotherdatum.com/", "anchor_text": "www.anotherdatum.com", "paragraph_index": 19}], "all_paragraphs": ["Natural Language Generation (NLG) is a well studied subject among the NLP community. With the rise of deep learning methods, NLG has become better and better. Recently, OpenAI has pushed the limits, with the release of GPT-2 \u2014 a Transformers based model that predicts the next token at each time space.", "Nowadays it\u2019s quite easy to use these models \u2014 you don\u2019t need to implement the code yourself, or train the models using expensive resources. HuggingFace, for instance, has released an API that eases the access to the pre-trained GPT-2 OpenAI has published. Some of its features include generating text, as well as fine-tuning the model on your own dataset \u2014 shifting the learned distribution so that the model will generate text from a new domain.", "Doing all of these is easy \u2014 it\u2019s only a matter of pip installing the relevant packages and launching a python script. However, to save you the trouble, you could use one of the available platforms such as Spell \u2014 you just specify what you want to run, and Spell will take care of the rest (download the code, install the packages, allocate compute resources, manage results).", "While not being a Spell advocate (I haven\u2019t even tried other features of the platform, or tried other platforms at all), I decided to write a tutorial that details the process I\u2019ve just described. To find out more, you can find the tutorial here.", "If you also like to play around with machine generated text, feel free to leave a comment with interesting texts you got. :)", "UPDATE: it seems the tutorial is no longer available in the aforementioned link. Although it\u2019s a bit outdated (the HuggingFace API has changed a lot since then), here is the full text:", "Natural Language Generation (NLG) is a well studied subject among the NLP community. One approach to tackle the challenge of text generation is to factorize the probability of a sequence of tokens (e.g. words or Byte Pair Encoding) P(x_1, \u2026, x_n) into the multiplication of the probabilities of getting each of the tokens x_1, \u2026, x_n conditioned on the tokens preceding it:", "Given a training dataset, one could train such a model to maximize the probability of the next token at each time step. Once the model has been trained, you could generate text by sampling from the distribution one token at a time. Easy as a breeze.", "With the rise of deep learning methods, NLG has become better and better. Recently, OpenAI have pushed the limits, with the release of GPT-2. This model uses the well known Transformers architecture: in order to calculate the distribution over the next token, the model simultaneously uses the previous tokens using a self-attention mechanism.", "Recently, HuggingFace have released an API easing the access to GPT-2. One of its features is generating text using the pre-trained model:", "That was easy! OpenAI have used diverse data found on the web for training the model, so the generated text can be pretty much any natural looking text. But what if instead of diversity, we\u2019d like to generate a specific kind of text? Let\u2019s try to generate jokes! To do so, we\u2019ll have to train the model using a dataset of jokes. Unfortunately, getting such a dataset would be ridiculously hard! To train GPT-2, which has 124M weights to be learned (and this is merely the smaller version of the architecture), we need a huge amount of data! But how are we going to get that many jokes? The short answer is: we won\u2019t.", "Learning to generate jokes involves learning how to generate natural-looking text, as well as making sure this text is funny. The first part is where most of the learning happens. Using the pre-trained version of GPT-2 as a starting point, the model won\u2019t have to learn how to generate natural-looking text from scratch. All it\u2019ll have to learn is to concentrate the distribution over text that is funny. A relatively small dataset will do for the task.", "Don\u2019t get me wrong, the dataset we\u2019ll be using isn\u2019t big enough to meaningfully learn anything useful. Moreover, training a model to generalize the concept of humor is a hard problem. However, for the purpose of this post \u2014 learning how to use and fine-tune a model such as GPT-2 \u2014 this will do: we\u2019ll witness how the dataset shifts the model\u2019s distribution towards text that looks, to some extent, like jokes.", "We\u2019ll use one-liner jokes from short-jokes-dataset to fine-tune GPT-2. Being shorter than the average joke, it\u2019ll be easier for the model to learn their distribution. So first thing\u2019s first, let\u2019s get the data:", "HuggingFace have already provided us with a script to fine-tune GPT-2:", "Note that the downloaded data from the previous run is mounted using the -m flag.", "Even though we\u2019ve used a small dataset (3K examples), running 10 epochs on a CPU took about 44 hours. It only shows how big the model is. This is why you should use a GPU if you want to use a bigger dataset or run many experiments (e.g. tune hyper parameters).", "Let\u2019s try to generate a joke, after mounting the result of the previous run:", "The model has learned to generate short sentences, which is typical for our dataset. This relatively easy to grasp data statistic was well learned! Regarding how funny the model is \u2014 well\u2026 I\u2019ll leave you to judge!", "This post was originally posted by me at www.anotherdatum.com.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F6a710f0fbbb0&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-accessibility-of-gpt-2-text-generation-and-fine-tuning-6a710f0fbbb0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-accessibility-of-gpt-2-text-generation-and-fine-tuning-6a710f0fbbb0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-accessibility-of-gpt-2-text-generation-and-fine-tuning-6a710f0fbbb0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-accessibility-of-gpt-2-text-generation-and-fine-tuning-6a710f0fbbb0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----6a710f0fbbb0--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----6a710f0fbbb0--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://yoelzeldes.medium.com/?source=post_page-----6a710f0fbbb0--------------------------------", "anchor_text": ""}, {"url": "https://yoelzeldes.medium.com/?source=post_page-----6a710f0fbbb0--------------------------------", "anchor_text": "Yoel Zeldes"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa609ddb637b2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-accessibility-of-gpt-2-text-generation-and-fine-tuning-6a710f0fbbb0&user=Yoel+Zeldes&userId=a609ddb637b2&source=post_page-a609ddb637b2----6a710f0fbbb0---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6a710f0fbbb0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-accessibility-of-gpt-2-text-generation-and-fine-tuning-6a710f0fbbb0&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6a710f0fbbb0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-accessibility-of-gpt-2-text-generation-and-fine-tuning-6a710f0fbbb0&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://openai.com/blog/better-language-models", "anchor_text": "GPT-2"}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "Transformers"}, {"url": "https://arxiv.org/abs/1508.07909", "anchor_text": "token"}, {"url": "https://huggingface.co/transformers", "anchor_text": "released an API"}, {"url": "https://spell.run", "anchor_text": "Spell"}, {"url": "https://community.spell.run/hc/en-us/articles/360038909713-GPT-2-text-generation-is-not-something-to-joke-about", "anchor_text": "here"}, {"url": "https://arxiv.org/abs/1508.07909", "anchor_text": "Byte Pair Encoding"}, {"url": "https://openai.com/blog/better-language-models", "anchor_text": "GPT-2"}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "Transformers architecture"}, {"url": "https://huggingface.co/transformers", "anchor_text": "released an API"}, {"url": "https://github.com/huggingface/transformers.git", "anchor_text": "https://github.com/huggingface/transformers.git"}, {"url": "https://raw.githubusercontent.com/amoudgl/short-jokes-dataset/master/data/onelinefun.csv", "anchor_text": "short-jokes-dataset"}, {"url": "https://raw.githubusercontent.com/amoudgl/short-jokes-dataset/master/data/onelinefun.csv", "anchor_text": "https://raw.githubusercontent.com/amoudgl/short-jokes-dataset/master/data/onelinefun.csv"}, {"url": "https://raw.githubusercontent.com/amoudgl/short-jokes-dataset/master/data/onelinefun.csv", "anchor_text": "https://raw.githubusercontent.com/amoudgl/short-jokes-dataset/master/data/onelinefun.csv"}, {"url": "https://github.com/huggingface/transformers.git", "anchor_text": "https://github.com/huggingface/transformers.git"}, {"url": "https://github.com/huggingface/transformers.git", "anchor_text": "https://github.com/huggingface/transformers.git"}, {"url": "https://www.pexels.com/@gratisography?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels", "anchor_text": "Gratisography"}, {"url": "https://www.pexels.com/photo/man-person-people-emotions-1990/?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels", "anchor_text": "Pexels"}, {"url": "http://anotherdatum.com/", "anchor_text": "www.anotherdatum.com"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----6a710f0fbbb0---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----6a710f0fbbb0---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----6a710f0fbbb0---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/nlp?source=post_page-----6a710f0fbbb0---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----6a710f0fbbb0---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6a710f0fbbb0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-accessibility-of-gpt-2-text-generation-and-fine-tuning-6a710f0fbbb0&user=Yoel+Zeldes&userId=a609ddb637b2&source=-----6a710f0fbbb0---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6a710f0fbbb0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-accessibility-of-gpt-2-text-generation-and-fine-tuning-6a710f0fbbb0&user=Yoel+Zeldes&userId=a609ddb637b2&source=-----6a710f0fbbb0---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6a710f0fbbb0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-accessibility-of-gpt-2-text-generation-and-fine-tuning-6a710f0fbbb0&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----6a710f0fbbb0--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F6a710f0fbbb0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-accessibility-of-gpt-2-text-generation-and-fine-tuning-6a710f0fbbb0&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----6a710f0fbbb0---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----6a710f0fbbb0--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----6a710f0fbbb0--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----6a710f0fbbb0--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----6a710f0fbbb0--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----6a710f0fbbb0--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----6a710f0fbbb0--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----6a710f0fbbb0--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----6a710f0fbbb0--------------------------------", "anchor_text": ""}, {"url": "https://yoelzeldes.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://yoelzeldes.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Yoel Zeldes"}, {"url": "https://yoelzeldes.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "816 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa609ddb637b2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-accessibility-of-gpt-2-text-generation-and-fine-tuning-6a710f0fbbb0&user=Yoel+Zeldes&userId=a609ddb637b2&source=post_page-a609ddb637b2--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Ffd4674272ff5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-accessibility-of-gpt-2-text-generation-and-fine-tuning-6a710f0fbbb0&newsletterV3=a609ddb637b2&newsletterV3Id=fd4674272ff5&user=Yoel+Zeldes&userId=a609ddb637b2&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}