{"url": "https://towardsdatascience.com/deep-variational-inference-2a7e43244723", "time": 1683006425.6018538, "path": "towardsdatascience.com/deep-variational-inference-2a7e43244723/", "webpage": {"metadata": {"title": "Deep Variational Inference. Studying Variational Inference using DL\u2026 | by Natan Katz | Towards Data Science", "h1": "Deep Variational Inference", "description": "In the world of Machine Learning (ML), Bayesian inference is often treated as the peculiar enigmatic uncle that no one wants to adopt. On one hand, Bayesian inference offers massive exposure to\u2026"}, "outgoing_paragraph_urls": [{"url": "https://projecteuclid.org/download/pdf_1/euclid.aoms/1177729694", "anchor_text": "https://projecteuclid.org/download/pdf_1/euclid.aoms/1177729694", "paragraph_index": 15}, {"url": "http://publications.aston.ac.uk/id/eprint/373/", "anchor_text": "http://publications.aston.ac.uk/id/eprint/373/", "paragraph_index": 32}, {"url": "http://www0.cs.ucl.ac.uk/staff/d.silver/web/Publications_files/viral.pdf", "anchor_text": "http://www0.cs.ucl.ac.uk/staff/d.silver/web/Publications_files/viral.pdf", "paragraph_index": 37}, {"url": "http://proceedings.mlr.press/v70/chou17a/chou17a.pdf", "anchor_text": "http://proceedings.mlr.press/v70/chou17a/chou17a.pdf", "paragraph_index": 42}, {"url": "https://towardsdatascience.com/understanding-actor-critic-methods-931b97b6df3f", "anchor_text": "https://towardsdatascience.com/understanding-actor-critic-methods-931b97b6df3f", "paragraph_index": 43}, {"url": "https://arxiv.org/pdf/1401.0118.pdf", "anchor_text": "https://arxiv.org/pdf/1401.0118.pdf", "paragraph_index": 52}], "all_paragraphs": ["In the world of Machine Learning (ML), Bayesian inference is often treated as the peculiar enigmatic uncle that no one wants to adopt. On one hand, Bayesian inference offers massive exposure to theoretical scientific tools from mathematics, statistics and physics. Furthermore, it carries an impressive historical legacy of scientific breakthroughs. On the other hand, as nearly every data scientist will state: it hardly ever works in practice.", "This post consists of two parts:", "1. A brief survey on the emergence of variational inference (VI)", "2. A description of my attempts to solve a VI problem using DL techniques for data with a Beta likelihood function.", "The Bayesian problem can be described as follow: We are provided with observed data X", "where the data can represent numbers, categories or any other data-type of that one can imagine. We assume that this data was generated using a latent variable Z. We have four distributions at our hands:", "1. P(Z) The prior distribution of the latent variable", "2. P( X| Z) The likelihood \u2014 The distribution type of this function is determined by the data, (for example: if we have integers we may think of Poisson, if we have positive numbers we may use the Gamma distribution).", "3. P(X) \u2014 The distribution of the observed data.", "4. P(Z|X) The posterior distribution \u2014 The probability to have a value of Z given a value of X", "These distributions are bound together by Bayes formula :", "The Bayesian problem is therefore about finding the posterior distribution and the values of Z. The obstacle is that in real-life models, P(X) is seldom tractable.", "Up until 1999, the modus operandi to solve Bayesian problems was the utilization of sampling. Algorithms such as the Metropolis Hastings or Gibbs were used successfully for resolving posterior functions in a wide array of scientific domains. Although they worked well they suffered from two significant problems:", "In 1999 Michael Jordan published a paper \u201cAn introduction to variational graphic models\u201d (This is the year that the more famous MJ quitted, which raises the question whether the universe is governed by \u201cMJ preservation law\u201d)", "In this paper, he described an analytical solution to the Bayesian problem. In the core of this solution he suggested that rather than chasing the posterior function with sampling, we can introduce a distribution function q of the latent variable Z. By setting q\u2019s family functions and a metric we can approximate the posterior function using this q. For this solution, he used Euler-Lagrange equation which is a fundamental equation in Calculus of Variations, that brought the notion: Variational Inference (VI).", "In this section, we describe VI in detail. Recall that we aim to approximate a posterior function P(Z|X) using a function q which is the distribution of Z. The metric that Jordan suggested was KL divergence (https://projecteuclid.org/download/pdf_1/euclid.aoms/1177729694)", "This idea of analytical solution reduces the obstacles of high variance and the slow convergence. On the other hand, since we set a family of functions we introduce some bias. The following formula illustrates the offered solution:", "The LHS does not depend on Z and therefore it can be considered as a constant. Minimizing KL divergence is equivalent to maximizing the second term: \u201cEvidence Lower Bound\u201d (ELBO)", "As a function with a predefined shape, q has constants that are denoted by \u03bb.", "We can consider VI as an ELBO maximization problem:", "The motivation of Jordan to use this solution came from works of Helmholtz and Boltzmann in thermodynamics. The ELBO function is extremely similar to Helmholtz free energy where the first term is the energy and second is the entropy of q. Maximizing the ELBO is about increasing the energy and reducing the entropy. Jordan used mean field theorem (MFT) from the Ising model, there magnetic spins are assumed to be uncorrelated which simplified the way to handle q. Ising problem has an exponential solution (Boltzmann distribution), which became the common choice for the shape of q", "I will now provide an example of how one formulate a VI problem. We will assume that we have a data of real numbers with a Gaussian likelihood. The latent variable Z , is therefore the pair", "Using mean field theorem, q can be written as a product", "A detailed formulation can be found here", "One can see that such formulation requires massive analytical calculations for every model. We will discuss this in one of the sections below when we will talk about BBVI.", "VI has been strongly boosted in 2002 when Blei published his \u201cLatent Dirichlet allocation\u201d, where he used VI for topics extraction", "There are several available tools to be used for VI such as Edward, Stan, PyMC3 and Pyro", "Using VI for distribution tail\u2019s inference", "The problem I had to handle was identifying a tail of an observed data. The data itself is a set of probability values thus we assume that the likelihood has a Beta distribution. Before I discuss the modeling steps, I will give an example for the tail\u2019s typical problem.", "Let\u2019s assume that we have a model that predicts a disease with accuracy of 99%. We can assume that 0.1% of the population is sick. Each day 100,000 people are tested. 990 healthy people will hear that they are sick. This mistake is costly, since they are going to do medical procedures. Such problems occur mainly because models are often trained to optimize accuracy under symmetric assumptions. If we rely on our models, tiny changes in the threshold may cause huge damages. We need therefore to understand the 1% tail as accurately as possible.", "VI as an analytical solution seems to be an interesting tool for achieving this purpose. Surprisingly I found out that the world of DL didn\u2019t massively embrace VI, which indeed encouraged me to walk on this way.", "In this part, I will describe my trials to use DL for solving VI problems. Recall that our data\u2019s likelihood has a Beta distribution. This distribution has an extremely difficult prior. We, therefore, take Gamma as a prior since it has support on the positive real line. In VI terminology Gamma is the prior and q distribution. Hence we train Gamma to sample Z which itself is the pair {\u03b1, \u03b2} of Beta.", "The first framework to be considered was MDN (Mixture Density Networks). This class of networks that proposed by Bishop (http://publications.aston.ac.uk/id/eprint/373/) aims to learn parameters of a given distribution. We modify the loss: rather using likelihood we use ELBO.", "In all the plots, red represents our VI based sample and blue is a synthetic sample that has been created by a random engine.", "Here is a typical outcome of our MDN trials:", "The VI based sample is relatively close to the synthetic sample. However, it is definitely improvable. Moreover while in the inner part and the left side (which is the interested tail from my perspective) are fairly similar, in the upper tail we suffer from huge differences.", "We wish to try additional tools.", "Variational inference is often interpreted as a reinforcement learning problem (http://www0.cs.ucl.ac.uk/staff/d.silver/web/Publications_files/viral.pdf) . We consider the q function as the policy function, Z is the option and the reward is the ELBO. When we use RL tools for such problem, we must notice that two modifications are needed:", "\u00b7 We don\u2019t have an episode -We have IID samples", "\u00b7 The options are not discrete but Gamma distributed", "In order to resolve the first clause we have to modify the Q learning update formula:", "As there is no real episode one has to remove the second term, since it indicates the future expected rewards.", "Regarding the second clause, we have to train a Gamma form policy function (a nice motivation exists here http://proceedings.mlr.press/v70/chou17a/chou17a.pdf)", "In order to train a policy function, we will use actor-critic with experience replay https://towardsdatascience.com/understanding-actor-critic-methods-931b97b6df3f and add our modifications.", "These are the plots that we got:", "The outcomes are fairly good in an interval[0,1-a) for a small positive. We totally fail to model the upper tail.", "The appearance of VI provided new directions for studying posterior distributions. Nevertheless, it imposed a massive analytical computation of the expectations (see the examples of Gaussian dist. Above). There was a clear objective to find a more generic scheme that reduces the amount of analysis and better handles a huge amount of data. The slide below is Blei\u2019s description for this need:", "The offered solution used stochastic optimization. A class of algorithms that has been presented by Robbins and Monro (A stochastic Approximation method 1951)", "The Robbins Monro algorithm aims to solve a root problem: Let F a function and \u03b1 a constant. We assume that there exists a unique solution for the eq:", "Assume that F is not observable but there exists a random variable T such that", "Robbins and Monro have shown that for certain regulations the following algorithm converges with L\u00b2:", "This sequence is called Robbins Monro sequence.", "In a paper from 2013 https://arxiv.org/pdf/1401.0118.pdf ,Blei suggested two steps improvement for VI by using Robbins-Monro sequence. This algorithm has been denoted \u201cBlack Box Variational Inference\u201d (BBVI). The first step is presented below:", "The main idea is to construct a Monte Carlo estimator to the ELBO gradient and use Robbins Monro sequence as described in the section beyond. This algorithm (as many other Monte Carlo algorithms) suffers from high variance. Blei suggested two methods to reduce this variance:", "In the paper, Blei uses Rao Blackwell to create an estimator for each of the constants by using the Rao Blackwell property for conditional distribution.", "I used BBVI without the variance reduction techniques", "We can see that we reduced the tail issues in comparison to AC but still suffer from this problem. In the last plot, we did not have tail issues but the approximation became weaker", "We saw that using Deep learning tools for VI works with partial success. We performed fairly good approximation within the interval but fail to approximate the upper tail. Moreover in all the frameworks, I suffered a certain level of instability.", "\u00b7 Architecture is not sophisticated enough", "\u00b7 Traditionally, DL problems are studied on common distributions such as Gaussian or Uniform. These distributions don\u2019t have parameters dependent Skewness and Kurtosis. This is not the case in Gamma, where the parameters that are trained by the engine fully determine the Skewness and Kurtosis. It may require different techniques", "\u00b7 We use ELBO as the loss function. This function relies strongly on the prior shapes. Gamma is not the real prior of Beta. Which may suggest that loss modifications are required", "\u00b7 Beta has compact support. The fact that we have issues only near the tail (sort of a \u201csingular point\u201d of the distribution) may raise obstacles. This \u201csingularity\u201d issues are to be studied", "We can summarize that these experiments have shown that VI can be resolved using DL mechanism. However, it requires further work. Nevertheless, we got some clarifications for the enigmatic uncle.", "A torch code that mimics my trials (the real code has been used for some commercial work I did, sorry ) exists here", "I wish to acknowledge Uri Itai for fruitful discussions before and during the work and for his comprehensive reviewing, to Shlomo Kashani and Yuval Shachaf for their ideas and comments and to Leonard Newnham, for clarifying issues in the reinforcement learning world", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Interested in theory behind the ML non-linearity stochasticity sampling Bayesian inference & generative models \u201cTiefe Gedanken sind ewig, daher der gr\u00f6\u00dfte Spa\u00df\u201d"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F2a7e43244723&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-variational-inference-2a7e43244723&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-variational-inference-2a7e43244723&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-variational-inference-2a7e43244723&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-variational-inference-2a7e43244723&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----2a7e43244723--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2a7e43244723--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://natan-katz.medium.com/?source=post_page-----2a7e43244723--------------------------------", "anchor_text": ""}, {"url": "https://natan-katz.medium.com/?source=post_page-----2a7e43244723--------------------------------", "anchor_text": "Natan Katz"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6d8b879ecc6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-variational-inference-2a7e43244723&user=Natan+Katz&userId=6d8b879ecc6&source=post_page-6d8b879ecc6----2a7e43244723---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2a7e43244723&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-variational-inference-2a7e43244723&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2a7e43244723&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-variational-inference-2a7e43244723&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://projecteuclid.org/download/pdf_1/euclid.aoms/1177729694", "anchor_text": "https://projecteuclid.org/download/pdf_1/euclid.aoms/1177729694"}, {"url": "https://towardsdatascience.com/variational-inference-in-bayesian-multivariate-gaussian-mixture-model-41c8cc4d82d7", "anchor_text": "https://towardsdatascience.com/variational-inference-in-bayesian-multivariate-gaussian-mixture-model-41c8cc4d82d7"}, {"url": "http://publications.aston.ac.uk/id/eprint/373/", "anchor_text": "http://publications.aston.ac.uk/id/eprint/373/"}, {"url": "http://www0.cs.ucl.ac.uk/staff/d.silver/web/Publications_files/viral.pdf", "anchor_text": "http://www0.cs.ucl.ac.uk/staff/d.silver/web/Publications_files/viral.pdf"}, {"url": "http://proceedings.mlr.press/v70/chou17a/chou17a.pdf", "anchor_text": "http://proceedings.mlr.press/v70/chou17a/chou17a.pdf"}, {"url": "https://towardsdatascience.com/understanding-actor-critic-methods-931b97b6df3f", "anchor_text": "https://towardsdatascience.com/understanding-actor-critic-methods-931b97b6df3f"}, {"url": "https://arxiv.org/pdf/1401.0118.pdf", "anchor_text": "https://arxiv.org/pdf/1401.0118.pdf"}, {"url": "https://github.com/natank1/DL-_VI", "anchor_text": "https://github.com/natank1/DL-_VI"}, {"url": "http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf", "anchor_text": "http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf"}, {"url": "https://people.eecs.berkeley.edu/~jordan/papers/variational-intro.pdf", "anchor_text": "https://people.eecs.berkeley.edu/~jordan/papers/variational-intro.pdf"}, {"url": "https://towardsdatascience.com/a-hitchhikers-guide-to-mixture-density-networks-76b435826cca", "anchor_text": "https://towardsdatascience.com/a-hitchhikers-guide-to-mixture-density-networks-76b435826cca"}, {"url": "https://publications.aston.ac.uk/id/eprint/373/1/NCRG_94_004.pdf", "anchor_text": "https://publications.aston.ac.uk/id/eprint/373/1/NCRG_94_004.pdf"}, {"url": "https://www.ri.cmu.edu/wp-content/uploads/2017/06/thesis-Chou.pdf", "anchor_text": "https://www.ri.cmu.edu/wp-content/uploads/2017/06/thesis-Chou.pdf"}, {"url": "http://www.michaeltsmith.org.uk/", "anchor_text": "http://www.michaeltsmith.org.uk/#"}, {"url": "https://github.com/trevorcampbell/ubvi/tree/master/examples", "anchor_text": "https://github.com/trevorcampbell/ubvi/tree/master/examples"}, {"url": "https://arxiv.org/pdf/1811.01132.pdf", "anchor_text": "https://arxiv.org/pdf/1811.01132.pdf"}, {"url": "http://proceedings.mlr.press/v70/chou17a/chou17a.pdf", "anchor_text": "http://proceedings.mlr.press/v70/chou17a/chou17a.pdf"}, {"url": "https://towardsdatascience.com/understanding-actor-critic-methods-931b97b6df3f", "anchor_text": "https://towardsdatascience.com/understanding-actor-critic-methods-931b97b6df3f"}, {"url": "https://www.freecodecamp.org/news/an-intro-to-advantage-actor-critic-methods-lets-play-sonic-the-hedgehog-86d6240171d/", "anchor_text": "https://www.freecodecamp.org/news/an-intro-to-advantage-actor-critic-methods-lets-play-sonic-the-hedgehog-86d6240171d/"}, {"url": "http://incompleteideas.net/book/the-book-2nd.html", "anchor_text": "http://incompleteideas.net/book/the-book-2nd.html"}, {"url": "https://arxiv.org/pdf/1401.0118.pdf", "anchor_text": "https://arxiv.org/pdf/1401.0118.pdf"}, {"url": "https://towardsdatascience.com/variational-inference-in-bayesian-multivariate-gaussian-mixture-model-41c8cc4d82d7", "anchor_text": "https://towardsdatascience.com/variational-inference-in-bayesian-multivariate-gaussian-mixture-model-41c8cc4d82d7"}, {"url": "https://projecteuclid.org/download/pdf_1/euclid.aoms/1177729586", "anchor_text": "https://projecteuclid.org/download/pdf_1/euclid.aoms/1177729586"}, {"url": "http://edwardlib.org/tutorials/klqp", "anchor_text": "http://edwardlib.org/tutorials/klqp"}, {"url": "https://mc-stan.org/users/interfaces/", "anchor_text": "https://mc-stan.org/users/interfaces/"}, {"url": "https://github.com/stan-dev/pystan/tree/develop/pystan", "anchor_text": "https://github.com/stan-dev/pystan/tree/develop/pystan"}, {"url": "https://docs.pymc.io/notebooks/getting_started.html", "anchor_text": "https://docs.pymc.io/notebooks/getting_started.html"}, {"url": "https://projecteuclid.org/download/pdf_1/euclid.aoms/1177729694", "anchor_text": "https://projecteuclid.org/download/pdf_1/euclid.aoms/1177729694"}, {"url": "https://www.cs.princeton.edu/courses/archive/fall11/cos597C/lectures/variational-inference-i.pdf", "anchor_text": "ttps://www.cs.princeton.edu/courses/archive/fall11/cos597C/lectures/variational-inference-i.pdf"}, {"url": "http://publications.aston.ac.uk/id/eprint/373/", "anchor_text": "http://publications.aston.ac.uk/id/eprint/373/"}, {"url": "https://www.researchgate.net/profile/Thomas_Leitner/publication/14066429/figure/fig1/AS:349553518759937@1460351461646/The-gamma-distribution-described-by-different-shape-parameters-01-to-13-15-17.png", "anchor_text": "https://www.researchgate.net/profile/Thomas_Leitner/publication/14066429/figure/fig1/AS:349553518759937@1460351461646/The-gamma-distribution-described-by-different-shape-parameters-01-to-13-15-17.png"}, {"url": "https://medium.com/tag/sotchastic-optimization?source=post_page-----2a7e43244723---------------sotchastic_optimization-----------------", "anchor_text": "Sotchastic Optimization"}, {"url": "https://medium.com/tag/variational-inference?source=post_page-----2a7e43244723---------------variational_inference-----------------", "anchor_text": "Variational Inference"}, {"url": "https://medium.com/tag/actor-critic?source=post_page-----2a7e43244723---------------actor_critic-----------------", "anchor_text": "Actor Critic"}, {"url": "https://medium.com/tag/bbvi?source=post_page-----2a7e43244723---------------bbvi-----------------", "anchor_text": "Bbvi"}, {"url": "https://medium.com/tag/mixture-density-networks?source=post_page-----2a7e43244723---------------mixture_density_networks-----------------", "anchor_text": "Mixture Density Networks"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2a7e43244723&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-variational-inference-2a7e43244723&user=Natan+Katz&userId=6d8b879ecc6&source=-----2a7e43244723---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2a7e43244723&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-variational-inference-2a7e43244723&user=Natan+Katz&userId=6d8b879ecc6&source=-----2a7e43244723---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2a7e43244723&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-variational-inference-2a7e43244723&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2a7e43244723--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F2a7e43244723&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-variational-inference-2a7e43244723&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----2a7e43244723---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----2a7e43244723--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----2a7e43244723--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----2a7e43244723--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----2a7e43244723--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----2a7e43244723--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----2a7e43244723--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----2a7e43244723--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----2a7e43244723--------------------------------", "anchor_text": ""}, {"url": "https://natan-katz.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://natan-katz.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Natan Katz"}, {"url": "https://natan-katz.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "103 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6d8b879ecc6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-variational-inference-2a7e43244723&user=Natan+Katz&userId=6d8b879ecc6&source=post_page-6d8b879ecc6--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F5785648073aa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-variational-inference-2a7e43244723&newsletterV3=6d8b879ecc6&newsletterV3Id=5785648073aa&user=Natan+Katz&userId=6d8b879ecc6&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}