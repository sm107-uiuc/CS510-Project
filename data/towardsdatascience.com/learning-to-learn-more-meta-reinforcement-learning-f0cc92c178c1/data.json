{"url": "https://towardsdatascience.com/learning-to-learn-more-meta-reinforcement-learning-f0cc92c178c1", "time": 1683014902.056532, "path": "towardsdatascience.com/learning-to-learn-more-meta-reinforcement-learning-f0cc92c178c1/", "webpage": {"metadata": {"title": "Learning to Learn More: Meta Reinforcement Learning | by Rajat Sahay | Towards Data Science", "h1": "Learning to Learn More: Meta Reinforcement Learning", "description": "The ELI5 definition for Reinforcement Learning would be training a model to perform better by iteratively learning from its previous mistakes. Reinforcement learning provides a framework for agents\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.dictionary.com/e/slang/eli5/", "anchor_text": "ELI5", "paragraph_index": 0}, {"url": "https://arxiv.org/abs/1611.05763", "anchor_text": "Wang et al.", "paragraph_index": 3}, {"url": "https://lilianweng.github.io/lil-log/2018/01/23/the-multi-armed-bandit-problem-and-its-solutions.html", "anchor_text": "multi-armed bandit problem", "paragraph_index": 6}, {"url": "https://hackernoon.com/learning-policies-for-learning-policies-meta-reinforcement-learning-rl%C2%B2-in-tensorflow-b15b592a2ddf", "anchor_text": "Practically", "paragraph_index": 8}, {"url": "https://hackernoon.com/learning-policies-for-learning-policies-meta-reinforcement-learning-rl%C2%B2-in-tensorflow-b15b592a2ddf", "anchor_text": "MAML", "paragraph_index": 14}, {"url": "https://arxiv.org/abs/1803.02999", "anchor_text": "Reptile", "paragraph_index": 14}, {"url": "https://en.wikipedia.org/wiki/Evolutionary_algorithm", "anchor_text": "Evolutionary algorithms", "paragraph_index": 17}, {"url": "https://eng.uber.com/poet-open-ended-deep-learning/", "anchor_text": "POET", "paragraph_index": 17}, {"url": "https://arxiv.org/abs/1806.04640", "anchor_text": "Gupta et al.", "paragraph_index": 20}, {"url": "https://lilianweng.github.io/lil-log/2019/06/23/meta-reinforcement-learning.html", "anchor_text": "Source", "paragraph_index": 20}, {"url": "https://arxiv.org/abs/1806.04640", "anchor_text": "this", "paragraph_index": 22}, {"url": "https://arxiv.org/abs/1905.10985", "anchor_text": "AI-GAs", "paragraph_index": 27}, {"url": "https://medium.com/@awjuliani", "anchor_text": "Arthur Juliani", "paragraph_index": 28}, {"url": "https://github.com/awjuliani/Meta-RL", "anchor_text": "here", "paragraph_index": 28}, {"url": "https://arxiv.org/abs/1611.05763", "anchor_text": "research paper", "paragraph_index": 28}, {"url": "https://eng.uber.com/poet-open-ended-deep-learning/", "anchor_text": "post", "paragraph_index": 28}, {"url": "https://lilianweng.github.io/lil-log/2019/06/23/meta-reinforcement-learning.html", "anchor_text": "blog", "paragraph_index": 28}], "all_paragraphs": ["The ELI5 definition for Reinforcement Learning would be training a model to perform better by iteratively learning from its previous mistakes. Reinforcement learning provides a framework for agents to solve problems in case of real-world scenarios. They are able to learn rules (or policies) to solve specific problems, but one of the major limitations of these agents are that they are unable to generalize the learned policy to newer problems. A previously learned rule would cater to a specific problem only, and would often be useless for other (even similar) cases.", "A good meta-learning model on the other hand, is expected to generalize to new tasks or environments that have not been encountered by the model in training. The process of adaption to this new environment can be termed a mini learning session and happens with testing with limited exposure to newer configuration. In the absence of explicitly fine-tuning models, it is observed that meta-learning is able to autonomously adjust internal states to generalize to newer environments.", "Meta-Reinforcement Learning is just Meta-Learning applied to Reinforcement Learning", "Furthermore, Wang et al. described meta-RL as \u201cthe special category of meta-learning that use recurrent models, applied to RL\u201d, which seems like a much more comprehensive definition than the one above.", "In reinforcement learning, an agent receives observations at each step (such as the position of a character in a video game) and based on those observations, it outputs actions like \u2018move forward\u2019 or \u2018turn right\u2019. Based on the results of these actions, the agent receives rewards or penalties which guide it further in training, helping it make more meaningful observations for later steps. The aim of the model is to maximize rewards and minimize penalties.", "In meta-reinforcement Learning, the training and testing tasks are different, but are drawn from the same family of problems. A good example would be mazes with different layouts, or different probabilities of a multi-armed bandit problem (explained below).", "The multi-armed bandit problem is a classic problem which demonstrates the exploration vs exploitation dilemma in an excellent way. You can put yourself in the situation by imagining yourself in a room with two levers and nothing else.", "To answer this question logically, you would have to know the value of the probability p. A higher value of p would guarantee a higher chance of reward from the lever A1, while a lower value of p would guarantee a higher chance of reward from the lever A2. And this is exactly why meta-RL is so interesting. If you throw enough values at your model, and it will get increasingly better at choosing the correct lever after interacting with the environment and learning from it. A traditional RL-based approach would not be able to handle the changing probabilities and would usually fail with different values of p.", "Practically, the meta-RL agent trained on the two-lever problem with different probabilities is able to choose the correct lever out of the two which leads to the highest reward using a very small number of data points. It uses the couplet (action, reward) for computing the \u2018risk vs reward\u2019 factor for each of the levers.", "Here is an example of an untrained agent on the left (with p=0.92) and a trained meta-RL agent on the right (with p=0.76)", "There are three key components involved in meta-RL. They are described in detail below.", "A model with memory: Without memory, a meta-RL model would be useless. It needs memory to acquire and store knowledge about the current task from the immediate environment, which would help it to update its hidden state. A recurrent neural network maintains the hidden state of the meta-RL model.", "The dynamics to creating a good RNN would be too broad for the scope of this article. However, meta-RL and meta-RL\u00b2, both used an LSTM to manage their hidden states.", "The meta-learning algorithm: A meta-learning algorithm would define how we update the weights of the model based on what it learnt. The main objective of the algorithm is to help optimize the model to solve an unseen task in the minimum amount of time, applying what it learnt from previous tasks. Previous research usually used ordinary gradient descent update of the LSTM cell.", "MAML and Reptile, are proven methods that are able to update model parameters in order to achieve a good generalization performance on new and unseen tasks.", "A proper distribution of MDPs: A Markov Decision Process (MDP) refers to the entire process of the agent observing the environment output, consisting of a reward and a next state, and then making further decisions based on that. Since the agent is exposed to a number of different types of environments and tasks during its training, it needs to be able to quickly adapt to changing conditions and different MDPs.", "Among the three components, this is the least studied and probably the most specific component in regards to meta-RL. As each task is a MDP, we can build a distribution of MDPs by either modifying the reward configuration or the environment.", "Evolutionary algorithms are a great way to ensure the generation of a good environment. They are usually heuristic-based and inspired by the process of natural selection. A population of randomly generated solutions go through a loop of evaluation, selection, mutation (if we throw genetic algorithms in the mix) and reproduction; out of which the good solutions make it till the end. POET, by Wang et al. is a good example of a framework based on an evolutionary algorithm.", "The Pair Open-Ended Trailblazer (POET), demonstrated below, initially begins with a trivial environment and a randomly initialized agent. It then grows and maintains a population of one-to-one paired environments and agents. According to the authors, POET aims to achieve two goals, to evolve the population of environments with regards to diversity and complexity; and optimizing the agents to solve their parallel environments.", "An MDP without a reward function is known as a Controlled Markov Process (CMP) Given a predefined CMP, we can learn knowledge about a variety of tasks by generating a collection of reward functions R, that encourage the training of an effective meta-learning policy.", "Gupta et al. proposed two unsupervised approaches for growing the task distribution in the context of CMP. Assuming that there is an underlying latent variable associated with every task, it parameterizes the reward function as a function of the latent variable along with a discriminator function (which is used to extract the latent variable from the state). \u2014 Source", "The research paper described two main ways for constructing the discriminator function:", "The intricacies of growing the task distribution in the context of CMP is beyond the scope of the article, and I would highly recommend anyone interested to delve into this paper for a more in-depth view.", "A meta-RL system is pretty much similar to that of an ordinary RL algorithm, except for the fact that the last reward as well as the last action are also included into the policy observation, along with the current state. The purpose of this change is to feed and keep track of the history of all tasks and observations, so that the model can internally update the dynamics between the states, actions and rewards based on the current MDP, and adjust its strategy for other MDPs accordingly.", "Both meta-RL and meta-RL\u00b2 implemented an LSTM policy, where the LSTM\u2019s hidden state served as the memory to track the change in characteristics. The policy being recurrent in nature, the need to explicitly input the final values is not seen.", "The training procedure works as follows:", "Training RL algorithms can be difficult sometimes. If a meta-learning agent could become so smart that the distribution of tasks that it could solve from the knowledge it inferred while being trained on a particular task, becomes extremely broad, we would be well on our way towards generalized intelligence (or as the new buzzword stands \u2014 Artificial General Intelligence {AGI}) \u2014 essentially building a brain which would be able to solve all types of RL problems.", "As a side-note, I would also like to point out the unsurprising resemblance between meta-RL and AI-GAs (by J Clune), which proposed that the efficient way towards AGI is by making learning autonomous. It stood on three pillars: meta-learning architectures, meta-learning algorithms and automatically generated algorithms for effective learning.", "This work would not have been possible without the efforts of Arthur Juliani. You can check out his excellent implementation of the meta-RL algorithms here. This research paper by Wang et al. also gave me a great insight into some of the core concepts of meta-RL, and I would highly recommend reading it if you want a more comprehensive view. Lastly, this post on the Uber Engineering blog by Clune, Stanley, Lehman and Wang also helped me understand the open-endedness of fields like these, and a way to overcome prohibitively difficult challenges. This blog by Lilian Weng is also a great resource for further reading.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Just another kid fascinated by AI"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Ff0cc92c178c1&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-to-learn-more-meta-reinforcement-learning-f0cc92c178c1&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-to-learn-more-meta-reinforcement-learning-f0cc92c178c1&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-to-learn-more-meta-reinforcement-learning-f0cc92c178c1&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-to-learn-more-meta-reinforcement-learning-f0cc92c178c1&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----f0cc92c178c1--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f0cc92c178c1--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@rajatsahay719?source=post_page-----f0cc92c178c1--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@rajatsahay719?source=post_page-----f0cc92c178c1--------------------------------", "anchor_text": "Rajat Sahay"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fed96c192a8e2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-to-learn-more-meta-reinforcement-learning-f0cc92c178c1&user=Rajat+Sahay&userId=ed96c192a8e2&source=post_page-ed96c192a8e2----f0cc92c178c1---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff0cc92c178c1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-to-learn-more-meta-reinforcement-learning-f0cc92c178c1&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff0cc92c178c1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-to-learn-more-meta-reinforcement-learning-f0cc92c178c1&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@photoshobby?utm_source=medium&utm_medium=referral", "anchor_text": "Photos Hobby"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://www.dictionary.com/e/slang/eli5/", "anchor_text": "ELI5"}, {"url": "https://arxiv.org/abs/1611.05763", "anchor_text": "Wang et al."}, {"url": "https://lilianweng.github.io/lil-log/2018/01/23/the-multi-armed-bandit-problem-and-its-solutions.html", "anchor_text": "multi-armed bandit problem"}, {"url": "https://hackernoon.com/learning-policies-for-learning-policies-meta-reinforcement-learning-rl%C2%B2-in-tensorflow-b15b592a2ddf", "anchor_text": "Practically"}, {"url": "https://hackernoon.com/learning-policies-for-learning-policies-meta-reinforcement-learning-rl%C2%B2-in-tensorflow-b15b592a2ddf", "anchor_text": "source"}, {"url": "https://hackernoon.com/learning-policies-for-learning-policies-meta-reinforcement-learning-rl%C2%B2-in-tensorflow-b15b592a2ddf", "anchor_text": "MAML"}, {"url": "https://arxiv.org/abs/1803.02999", "anchor_text": "Reptile"}, {"url": "https://en.wikipedia.org/wiki/Evolutionary_algorithm", "anchor_text": "Evolutionary algorithms"}, {"url": "https://eng.uber.com/poet-open-ended-deep-learning/", "anchor_text": "POET"}, {"url": "https://eng.uber.com/poet-open-ended-deep-learning/", "anchor_text": "source"}, {"url": "https://arxiv.org/abs/1806.04640", "anchor_text": "Gupta et al."}, {"url": "https://lilianweng.github.io/lil-log/2019/06/23/meta-reinforcement-learning.html", "anchor_text": "Source"}, {"url": "https://arxiv.org/abs/1802.06070", "anchor_text": "DIAYN"}, {"url": "https://arxiv.org/abs/1806.04640", "anchor_text": "this"}, {"url": "https://arxiv.org/abs/1611.05763", "anchor_text": "source"}, {"url": "https://arxiv.org/abs/1905.10985", "anchor_text": "AI-GAs"}, {"url": "https://medium.com/@awjuliani", "anchor_text": "Arthur Juliani"}, {"url": "https://github.com/awjuliani/Meta-RL", "anchor_text": "here"}, {"url": "https://arxiv.org/abs/1611.05763", "anchor_text": "research paper"}, {"url": "https://eng.uber.com/poet-open-ended-deep-learning/", "anchor_text": "post"}, {"url": "https://lilianweng.github.io/lil-log/2019/06/23/meta-reinforcement-learning.html", "anchor_text": "blog"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----f0cc92c178c1---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----f0cc92c178c1---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----f0cc92c178c1---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/ai?source=post_page-----f0cc92c178c1---------------ai-----------------", "anchor_text": "AI"}, {"url": "https://medium.com/tag/research?source=post_page-----f0cc92c178c1---------------research-----------------", "anchor_text": "Research"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff0cc92c178c1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-to-learn-more-meta-reinforcement-learning-f0cc92c178c1&user=Rajat+Sahay&userId=ed96c192a8e2&source=-----f0cc92c178c1---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff0cc92c178c1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-to-learn-more-meta-reinforcement-learning-f0cc92c178c1&user=Rajat+Sahay&userId=ed96c192a8e2&source=-----f0cc92c178c1---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff0cc92c178c1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-to-learn-more-meta-reinforcement-learning-f0cc92c178c1&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f0cc92c178c1--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Ff0cc92c178c1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-to-learn-more-meta-reinforcement-learning-f0cc92c178c1&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----f0cc92c178c1---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----f0cc92c178c1--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----f0cc92c178c1--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----f0cc92c178c1--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----f0cc92c178c1--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----f0cc92c178c1--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----f0cc92c178c1--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----f0cc92c178c1--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----f0cc92c178c1--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@rajatsahay719?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@rajatsahay719?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Rajat Sahay"}, {"url": "https://medium.com/@rajatsahay719/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "60 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fed96c192a8e2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-to-learn-more-meta-reinforcement-learning-f0cc92c178c1&user=Rajat+Sahay&userId=ed96c192a8e2&source=post_page-ed96c192a8e2--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F5524b26b2808&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-to-learn-more-meta-reinforcement-learning-f0cc92c178c1&newsletterV3=ed96c192a8e2&newsletterV3Id=5524b26b2808&user=Rajat+Sahay&userId=ed96c192a8e2&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}