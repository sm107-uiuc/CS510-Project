{"url": "https://towardsdatascience.com/deep-q-network-dqn-i-bce08bdf2af", "time": 1683012477.711763, "path": "towardsdatascience.com/deep-q-network-dqn-i-bce08bdf2af/", "webpage": {"metadata": {"title": "Deep Q-Network (DQN)-I. OpenAI Gym Pong and Wrappers | by Jordi TORRES.AI | Towards Data Science", "h1": "Deep Q-Network (DQN)-I", "description": "In this post we will introduce how to code a Deep Q-Network for OpenAI Gym Pong Environment"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/mc-control-methods-50c018271553", "anchor_text": "previous post", "paragraph_index": 0}, {"url": "https://torres.ai/deep-reinforcement-learning-explained-series/", "anchor_text": "Deep Reinforcement Learning Explained", "paragraph_index": 0}, {"url": "https://gym.openai.com/envs/Pong-v0/", "anchor_text": "OpenAI Gym framework of Pong", "paragraph_index": 0}, {"url": "https://hallab.cs.dal.ca/images/0/00/Minh2015.pdf", "anchor_text": "can be found here", "paragraph_index": 3}, {"url": "https://arxiv.org/abs/1207.4708", "anchor_text": "Arcade Learning Environment", "paragraph_index": 4}, {"url": "https://gym.openai.com/envs/Pong-v0/", "anchor_text": "Pong-v0", "paragraph_index": 4}, {"url": "https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Second-Edition/tree/master/Chapter04", "anchor_text": "the code of Maxim Lapan who has written an excellent practical book on the subject", "paragraph_index": 6}, {"url": "https://github.com/jorditorresBCN/Deep-Reinforcement-Learning-Explained/blob/master/DRL_15_16_17_DQN_Pong.ipynb", "anchor_text": "can be found on GitHub", "paragraph_index": 7}, {"url": "https://colab.research.google.com/github/jorditorresBCN/Deep-Reinforcement-Learning-Explained/blob/master/DRL_15_16_17_DQN_Pong.ipynb", "anchor_text": "can be run as a Colab google notebook using this link", "paragraph_index": 7}, {"url": "https://en.wikipedia.org/wiki/Arcade_game#Arcade_video_games", "anchor_text": "arcade video game", "paragraph_index": 9}, {"url": "https://en.wikipedia.org/wiki/Atari,_Inc", "anchor_text": "Atari", "paragraph_index": 9}, {"url": "https://gym.openai.com/envs/Pong-v0/", "anchor_text": "OpenAI Gym framework version of Pong", "paragraph_index": 9}, {"url": "https://towardsdatascience.com/drl-02-formalization-of-a-reinforcement-learning-problem-108b52ebfd9a", "anchor_text": "Markov property", "paragraph_index": 16}, {"url": "https://hallab.cs.dal.ca/images/0/00/Minh2015.pdf", "anchor_text": "DeepMind\u2019s paper", "paragraph_index": 30}, {"url": "https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Second-Edition/tree/master/Chapter04", "anchor_text": "Lapan\u2019s Book", "paragraph_index": 31}, {"url": "https://github.com/openai/baselines/blob/master/baselines/common/atari_wrappers.py", "anchor_text": "OpenAI Baselines repository", "paragraph_index": 31}, {"url": "https://gym.openai.com/envs/Pong-v0/", "anchor_text": "OpenAI Gym framework of Pong", "paragraph_index": 42}, {"url": "https://towardsdatascience.com/deep-q-network-dqn-ii-b6bf911b6b2c", "anchor_text": "Post 16", "paragraph_index": 42}, {"url": "https://towardsdatascience.com/deep-q-network-dqn-iii-c5a83b0338d2", "anchor_text": "Post 17", "paragraph_index": 42}, {"url": "https://www.upc.edu/en", "anchor_text": "UPC Barcelona Tech", "paragraph_index": 43}, {"url": "https://www.bsc.es/", "anchor_text": "Barcelona Supercomputing Center", "paragraph_index": 43}, {"url": "https://torres.ai/deep-reinforcement-learning-explained-series/", "anchor_text": "series", "paragraph_index": 44}, {"url": "https://twitter.com/hashtag/StayAtHome?src=hashtag_click", "anchor_text": "#StayAtHome", "paragraph_index": 45}, {"url": "https://torres.ai", "anchor_text": "https://torres.ai", "paragraph_index": 48}], "all_paragraphs": ["In the previous post, we have presented solution methods that represent the action-values in a small table. We referred to this table as a Q-table. In the next three posts of the \u201cDeep Reinforcement Learning Explained\u201d series, we will introduce the reader to the idea of using neural networks to expand the size of the problems that we can solve with reinforcement learning presenting the Deep Q-Network (DQN), that represents the optimal action-value function as a neural network, instead of a table. In this post, we will do an overview of DQN as well as introduce the OpenAI Gym framework of Pong. In the next two posts, we will present the algorithm and its implementation.", "The Q-learning method that we have just covered in previous posts solves the issue by iterating over the full set of states. However often we realize that we have too many states to track. An example is Atari games, that can have a large variety of different screens, and in this case, the problem cannot be solved with a Q-table.", "The Atari 2600 game console was very popular in the 1980s, and many arcade-style games were available for it. The Atari console is archaic by today\u2019s gaming standards, but its games still are challenging for computers and is a very popular benchmark within RL research (using an emulator)", "In 2015 DeepMind leveraged the so-called Deep Q-Network (DQN) or Deep Q-Learning algorithm that learned to play many Atari video games better than humans. The research paper that introduces it, applied to 49 different games, was published in Nature (Human-Level Control Through Deep Reinforcement Learning, doi:10.1038/nature14236, Mnih, and others) and can be found here.", "The Atari 2600 game environment can be reproduced through the Arcade Learning Environment in the OpenAI Gym framework. The framework has multiple versions of each game but for the purpose of this post, the Pong-v0 Environment will be used.", "We will study this algorithm because it really allows us to learn tips and tricks that will be very useful in future posts in this series. DeepMind\u2019s Nature paper contained a table with all the details about hyperparameters used to train its model on all 49 Atari games used for evaluation. However, our goal here is much more modest: we want to solve just the Pong game.", "As we have done in some previous posts, the code presented in this post has been inspired by the code of Maxim Lapan who has written an excellent practical book on the subject.", "The entire code of this post can be found on GitHub and can be run as a Colab google notebook using this link.", "Our previous examples for FrozenLake, or CartPole, were not demanding from a computation requirements perspective, as observations were small. However, from now on, that\u2019s not the case. The version of code shared in this post converges to a mean score of 19.0 in 2 hours (using a NVIDIA K80). So don\u2019t get nervous during the execution of the training loop. ;-)", "Pong is a table tennis-themed arcade video game featuring simple two-dimensional graphics, manufactured by Atari and originally released in 1972. In Pong, one player scores if the ball passes by the other player. An episode is over when one of the players reaches 21 points. In the OpenAI Gym framework version of Pong, the Agent is displayed on the right and the enemy on the left:", "There are three actions an Agent (player) can take within the Pong Environment: remaining stationary, vertical translation up, and vertical translation down. However, if we use the method action_space.n we can realize that the Environment has 6 actions:", "Even though OpenAI Gym Pong Environment has six actions:", "three of the six being redundant (FIRE is equal to NOOP, LEFT is equal to LEFTFIRE and RIGHT is equal to RIGHTFIRE).", "At the heart of the Agent of this new approach, we found a deep neural network instead of a Q-table as we saw in the previous post. It should be noted that the Agent was only given raw pixel data, what a human player would see on screen, without access to the underlying game state, position of the ball, paddles, etc.", "As a reinforcement signal, it is fed back the change in game score at each time step. At the beginning, when the neural network is initialized with random values, it\u2019s really bad, but overtime it begins to associate situations and sequences in the game with appropriate actions and learns to actually play the game well (that, without a doubt, the reader will be able to verify for himself with the code that will be presented in this series).", "Atari games are displayed at a resolution of 210 by 60 pixels, with 128 possible colors for each pixel:", "This is still technically a discrete state space but very large to process as it is and we can optimize it. To reduce this complexity, it is performed some minimal processing: convert the frames to grayscale, and scale them down to a square 84 by 84 pixel block. Now let\u2019s think carefully if with this fixed image we can determine the dynamics of the game. There is certainly ambiguity in the observation, right? For example, we cannot know in which direction the ball is going). This obviously violates the Markov property.", "The solution is maintaining several observations from the past and using them as a state. In the case of Atari games, the authors of the paper suggested to stack 4 subsequent frames together and use them as the observation at every state. For this reason, the preprocessing stacks four frames together resulting in a final state space size of 84 by 84 by 4:", "Unlike until now we presented a traditional reinforcement learning setup where only one Q-value is produced at a time, the Deep Q-network is designed to produce in a single forward pass a Q-value for every possible action available in the Environment:", "This approach of having all Q-values calculated with one pass through the network avoids having to run the network individually for every action and helps to increase speed significantly. Now, we can simply use this vector to take an action by choosing the one with the maximum value.", "The original DQN Agent used the same neural network architecture, for the all 49 games, that takes as an input an 84x84x4 image.", "The screen images are first processed by three convolutional layers. This allows the system to exploit spatial relationships, and can sploit spatial rule space. Also, since four frames are stacked and provided as input, these convolutional layers also extract some temporal properties across those frames. Using PyTorch, we can code the convolutional part of the model as:", "where input_shape is the observation_space.shape of the Environment.", "The convolutional layers are followed by one fully-connected hidden layer with ReLU activation and one fully-connected linear output layer that produced the vector of action values:", "where conv_out_size is the number of values in the output from the convolution layer produced with the input of the given shape. This value is needed to pass to the first fully connected layer constructor and can be hard-coded due it is a function of the input shape (for 84x84 input, the output from the convolution layer will have 3136). However, in order to code a generic model (for all the games) that can accept different input shape, we will use a simple function, _get_conv_out that accepts the input shape and applies the convolution layer to a fake tensor of such a shape:", "Another issue to solve is the requirement of feeding convolution output to the fully connected layer. But PyTorch doesn\u2019t have a \u201cflatter\u201d layer and we need to reshape the batch of 3D tensors into a batch of 1D vectors. In our code, we suggest solving this problem in the forward() function, where we can reshape our batch of 3D tensors into a batch of 1D vectors using the view() function of the tensors.", "The view() function \u201creshape\u201d a tensor with the same data and number of elements as input, but with the specified shape. The interesting thing of this function is that lets one single dimension be a -1 in which case it\u2019s inferred from the remaining dimensions and the number of elements in the input (the method will do the math in order to fill that dimension). For example, if we have a tensor of shape (2, 3, 4, 6), which is a 4D tensor of 144 elements, we can reshape it into a 2D tensor with 2 rows and 72 columns using view(2,72). The same result could be obtained by view(2,-1), due [144/ (3*4*6) = 2].", "In our code, actually, the tensor has a batch size in the first dimension and we flatten a 4D tensor (the first dimension is batch size and the second is the color channel, which is our stack of subsequent frames; the third and fourth are image dimensions.)from the convolutional part to 2D tensor as an input to our fully connected layers to obtain Q-values for every batch input.", "The complete code for class DQN that we just described is written below:", "We can use the print function to see a summary of the network architecture:", "In DeepMind\u2019s paper, several transformations (as the already introduced the conversion of the frames to grayscale, and scale them down to a square 84 by 84 pixel block) is applied to the Atari platform interaction in order to improve the speed and convergence of the method. In our example, that uses OpenAI Gym simulator, transformations are implemented as OpenAI Gym wrappers.", "The full list is quite lengthy and there are several implementations of the same wrappers in various sources. I used the version of Lapan\u2019s Book that is based in the OpenAI Baselines repository. Let\u2019s introduce the code for each one of them.", "For instance, some games as Pong require a user to press the FIRE button to start the game. The following code corresponds to the wrapper FireResetEnvthat presses the FIRE button in environments that require that for the game to start:", "In addition to pressing FIRE, this wrapper checks for several corner cases that are present in some games.", "The next wrapper that we will require is MaxAndSkipEnv that codes a couple of important transformations for Pong:", "On one hand, it allows us to speed up significantly the training by applying max to N observations (four by default) and returns this as an observation for the step. This is because on intermediate frames, the chosen action is simply repeated and we can make an action decision every N steps as processing every frame with a Neural Network is quite a demanding operation, but the difference between consequent frames is usually minor.", "On the other hand, it takes the maximum of every pixel in the last two frames and using it as an observation. Some Atari games have a flickering effect (when the game draws different portions of the screen on even and odd frames, a normal practice among Atari 2600 developers to increase the complexity of the game\u2019s sprites), which is due to the platform\u2019s limitation. For the human eye, such quick changes are not visible, but they can confuse a Neural Network.", "Remember that we already mentioned that before feeding the frames to the neural network every frame is scaled down from 210x160, with three color frames (RGB color channels), to a single-color 84 x84 image using a colorimetric grayscale conversion. Different approaches are possible. One of them is cropping non-relevant parts of the image and then scaling down as is done in the following code:", "As we already discussed as a quick solution to the lack of game dynamics in a single game frame, the class BufferWrapper stacks several (usually four) subsequent frames together:", "The input shape of the tensor has a color channel as the last dimension, but PyTorch\u2019s convolution layers assume the color channel to be the first dimension. This simple wrapper changes the shape of the observation from HWC (height, width, channel) to the CHW (channel, height, width) format required by PyTorch:", "The screen obtained from the emulator is encoded as a tensor of bytes with values from 0 to 255, which is not the best representation for an NN. So, we need to convert the image into floats and rescale the values to the range [0.0\u20261.0]. This is done by the ScaledFloatFrame wrapper:", "Finally, it will be helpful for the following simple function make_env that creates an environment by its name and applies all the required wrappers to it:", "This is the first of three posts devoted to Deep Q-Network (DQN), in which we provide an overview of DQN as well as an introduction of the OpenAI Gym framework of Pong. In the next two posts (Post 16, Post 17), we will present the algorithm and its implementation, where we will cover several tricks for DQNs to improve their training stability and convergence.", "by UPC Barcelona Tech and Barcelona Supercomputing Center", "A relaxed introductory series that gradually and with a practical approach introduces the reader to this exciting technology that is the real enabler of the latest disruptive advances in the field of Artificial Intelligence.", "I started to write this series in May, during the period of lockdown in Barcelona. Honestly, writing these posts in my spare time helped me to #StayAtHome because of the lockdown. Thank you for reading this publication in those days; it justifies the effort I made.", "Disclaimers \u2014 These posts were written during this period of lockdown in Barcelona as a personal distraction and dissemination of scientific knowledge, in case it could be of help to someone, but without the purpose of being an academic reference document in the DRL area. If the reader needs a more rigorous document, the last post in the series offers an extensive list of academic resources and books that the reader can consult. The author is aware that this series of posts may contain some errors and suffers from a revision of the English text to improve it if the purpose were an academic document. But although the author would like to improve the content in quantity and quality, his professional commitments do not leave him free time to do so. However, the author agrees to refine all those errors that readers can report as soon as he can.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Professor at UPC Barcelona Tech & Barcelona Supercomputing Center. Research focuses on Supercomputing & Artificial Intelligence https://torres.ai @JordiTorresAI"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fbce08bdf2af&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-q-network-dqn-i-bce08bdf2af&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-q-network-dqn-i-bce08bdf2af&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-q-network-dqn-i-bce08bdf2af&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-q-network-dqn-i-bce08bdf2af&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----bce08bdf2af--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----bce08bdf2af--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://torres-ai.medium.com/?source=post_page-----bce08bdf2af--------------------------------", "anchor_text": ""}, {"url": "https://torres-ai.medium.com/?source=post_page-----bce08bdf2af--------------------------------", "anchor_text": "Jordi TORRES.AI"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F497013a3c715&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-q-network-dqn-i-bce08bdf2af&user=Jordi+TORRES.AI&userId=497013a3c715&source=post_page-497013a3c715----bce08bdf2af---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbce08bdf2af&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-q-network-dqn-i-bce08bdf2af&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbce08bdf2af&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-q-network-dqn-i-bce08bdf2af&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/tagged/deep-r-l-explained", "anchor_text": "DEEP REINFORCEMENT LEARNING EXPLAINED \u2014 15"}, {"url": "https://towardsdatascience.com/mc-control-methods-50c018271553", "anchor_text": "previous post"}, {"url": "https://torres.ai/deep-reinforcement-learning-explained-series/", "anchor_text": "Deep Reinforcement Learning Explained"}, {"url": "https://gym.openai.com/envs/Pong-v0/", "anchor_text": "OpenAI Gym framework of Pong"}, {"url": "https://medium.com/aprendizaje-por-refuerzo/9-m%C3%A9todos-value-based-deep-q-network-b52b5c3da0ba", "anchor_text": "Spanish version of this publication"}, {"url": "https://medium.com/aprendizaje-por-refuerzo/9-m%C3%A9todos-value-based-deep-q-network-b52b5c3da0ba", "anchor_text": "9. M\u00e9todos value-based: Deep Q-NetworkAcceso abierto al cap\u00edtulo 9 del libro Introducci\u00f3n al aprendizaje por refuerzo profundomedium.com"}, {"url": "https://en.wikipedia.org/wiki/Atari_2600", "anchor_text": "Wikipedia"}, {"url": "https://hallab.cs.dal.ca/images/0/00/Minh2015.pdf", "anchor_text": "can be found here"}, {"url": "https://arxiv.org/abs/1207.4708", "anchor_text": "Arcade Learning Environment"}, {"url": "https://gym.openai.com/envs/Pong-v0/", "anchor_text": "Pong-v0"}, {"url": "https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Second-Edition/tree/master/Chapter04", "anchor_text": "the code of Maxim Lapan who has written an excellent practical book on the subject"}, {"url": "https://github.com/jorditorresBCN/Deep-Reinforcement-Learning-Explained/blob/master/DRL_15_16_17_DQN_Pong.ipynb", "anchor_text": "can be found on GitHub"}, {"url": "https://colab.research.google.com/github/jorditorresBCN/Deep-Reinforcement-Learning-Explained/blob/master/DRL_15_16_17_DQN_Pong.ipynb", "anchor_text": "can be run as a Colab google notebook using this link"}, {"url": "https://en.wikipedia.org/wiki/Arcade_game#Arcade_video_games", "anchor_text": "arcade video game"}, {"url": "https://en.wikipedia.org/wiki/Atari,_Inc", "anchor_text": "Atari"}, {"url": "https://gym.openai.com/envs/Pong-v0/", "anchor_text": "OpenAI Gym framework version of Pong"}, {"url": "https://torrs.ai", "anchor_text": "torres.ai"}, {"url": "https://towardsdatascience.com/drl-02-formalization-of-a-reinforcement-learning-problem-108b52ebfd9a", "anchor_text": "Markov property"}, {"url": "https://torrs.ai", "anchor_text": "torres.ai"}, {"url": "https://torrs.ai", "anchor_text": "torres.ai"}, {"url": "https://hallab.cs.dal.ca/images/0/00/Minh2015.pdf", "anchor_text": "DeepMind\u2019s paper"}, {"url": "https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Second-Edition/tree/master/Chapter04", "anchor_text": "Lapan\u2019s Book"}, {"url": "https://github.com/openai/baselines/blob/master/baselines/common/atari_wrappers.py", "anchor_text": "OpenAI Baselines repository"}, {"url": "https://gym.openai.com/envs/Pong-v0/", "anchor_text": "OpenAI Gym framework of Pong"}, {"url": "https://towardsdatascience.com/deep-q-network-dqn-ii-b6bf911b6b2c", "anchor_text": "Post 16"}, {"url": "https://towardsdatascience.com/deep-q-network-dqn-iii-c5a83b0338d2", "anchor_text": "Post 17"}, {"url": "https://www.upc.edu/en", "anchor_text": "UPC Barcelona Tech"}, {"url": "https://www.bsc.es/", "anchor_text": "Barcelona Supercomputing Center"}, {"url": "https://torres.ai/deep-reinforcement-learning-explained-series/", "anchor_text": "series"}, {"url": "https://torres.ai/deep-reinforcement-learning-explained-series/", "anchor_text": "Deep Reinforcement Learning Explained \u2014 Jordi TORRES.AIContent of this series"}, {"url": "https://twitter.com/hashtag/StayAtHome?src=hashtag_click", "anchor_text": "#StayAtHome"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----bce08bdf2af---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----bce08bdf2af---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----bce08bdf2af---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/deep-r-l-explained?source=post_page-----bce08bdf2af---------------deep_r_l_explained-----------------", "anchor_text": "Deep R L Explained"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----bce08bdf2af---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbce08bdf2af&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-q-network-dqn-i-bce08bdf2af&user=Jordi+TORRES.AI&userId=497013a3c715&source=-----bce08bdf2af---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbce08bdf2af&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-q-network-dqn-i-bce08bdf2af&user=Jordi+TORRES.AI&userId=497013a3c715&source=-----bce08bdf2af---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbce08bdf2af&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-q-network-dqn-i-bce08bdf2af&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----bce08bdf2af--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fbce08bdf2af&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-q-network-dqn-i-bce08bdf2af&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----bce08bdf2af---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----bce08bdf2af--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----bce08bdf2af--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----bce08bdf2af--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----bce08bdf2af--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----bce08bdf2af--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----bce08bdf2af--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----bce08bdf2af--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----bce08bdf2af--------------------------------", "anchor_text": ""}, {"url": "https://torres-ai.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://torres-ai.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Jordi TORRES.AI"}, {"url": "https://torres-ai.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "2.1K Followers"}, {"url": "https://torres.ai", "anchor_text": "https://torres.ai"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F497013a3c715&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-q-network-dqn-i-bce08bdf2af&user=Jordi+TORRES.AI&userId=497013a3c715&source=post_page-497013a3c715--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F9fb911e344f9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-q-network-dqn-i-bce08bdf2af&newsletterV3=497013a3c715&newsletterV3Id=9fb911e344f9&user=Jordi+TORRES.AI&userId=497013a3c715&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}