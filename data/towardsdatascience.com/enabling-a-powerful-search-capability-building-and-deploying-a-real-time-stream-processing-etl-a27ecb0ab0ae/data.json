{"url": "https://towardsdatascience.com/enabling-a-powerful-search-capability-building-and-deploying-a-real-time-stream-processing-etl-a27ecb0ab0ae", "time": 1683017596.648203, "path": "towardsdatascience.com/enabling-a-powerful-search-capability-building-and-deploying-a-real-time-stream-processing-etl-a27ecb0ab0ae/", "webpage": {"metadata": {"title": "Building and Deploying a Real-Time Stream Processing ETL Engine with Kafka and ksqlDB | by Sahil Malhotra | Towards Data Science", "h1": "Building and Deploying a Real-Time Stream Processing ETL Engine with Kafka and ksqlDB", "description": "At Koverhoop, we are building a couple of large-scale projects in the Insurance domain. For one of our projects, which is a Multi-tenant Group Insurance Brokerage Platform, klient.ca, we were to\u2026"}, "outgoing_paragraph_urls": [{"url": "https://klient.ca", "anchor_text": "klient.ca", "paragraph_index": 0}, {"url": "https://debezium.io/documentation/reference/connectors/postgresql.html", "anchor_text": "Debezium\u2019s", "paragraph_index": 12}, {"url": "https://www.postgresql.org/docs/9.0/wal-intro.html", "anchor_text": "WAL", "paragraph_index": 12}, {"url": "https://github.com/behindthescenes-group/oesophagus/blob/master/services/postgres/Dockerfile", "anchor_text": "Dockerfile", "paragraph_index": 23}, {"url": "https://cwiki.apache.org/confluence/display/KAFKA/KIP-500%3A+Replace+ZooKeeper+with+a+Self-Managed+Metadata+Quorum", "anchor_text": "Kafka without a zookeeper", "paragraph_index": 26}, {"url": "https://cwiki.apache.org/confluence/display/KAFKA/KIP-695%3A+Further+Improve+Kafka+Streams+Timestamp+Synchronization", "anchor_text": "here", "paragraph_index": 41}, {"url": "https://www.confluent.io/resources/kafka-summit-2020/the-flux-capacitor-of-kafka-streams-and-ksqldb/", "anchor_text": "watch this video", "paragraph_index": 42}, {"url": "https://forms.gle/GGg2hvnEpG6r4bgg7", "anchor_text": "here", "paragraph_index": 55}, {"url": "http://linkedin.com/in/sahil-malhotra", "anchor_text": "linkedin.com/in/sahil-malhotra", "paragraph_index": 59}, {"url": "http://sahilmalhotra.com", "anchor_text": "sahilmalhotra.com", "paragraph_index": 59}], "all_paragraphs": ["At Koverhoop, we are building a couple of large-scale projects in the Insurance domain. For one of our projects, which is a Multi-tenant Group Insurance Brokerage Platform, klient.ca, we were to build a Powerful Search Capability, by being able to denormalize incoming events in real-time. Also, we wanted our search results to appear as we type. Below is what we were able to achieve, and in this post, I\u2019ll be talking about the core infrastructure, how we fully automate its deployment, and how you too can set it up really quick.", "Also, consider this as a two-part series, where I\u2019ll be discussing the following:", "Part 1: Understanding the stack used to power this Search Capability and deploying it using Docker and docker-compose. (this post)", "Part 2: Making a Scalable Production deployment of these services using Kubernetes. (yet to be published)", "To build a search engine that was quick, and real-time we had to make certain design decisions. We use Postgres as our primary database. So we had the following options we could use:", "Considering, we were already a multi-tenant application, also the entities to be searched for could require processing a lot of joins (if we used Postgres), and the scale we were projecting was quite high, we decided to not use the former option of querying the database directly, for performance implications and latency-related concerns.", "So we had to decide on a reliable, and efficient way to move data from Postgres to Elasticsearch in real-time. Again the following decisions were to be made:", "Option 1 was struck out pretty quick as it was not real-time, and even if we query at shorter intervals it would put a significant load on the Postgres server, and also could incur additional query latency and server instability as our data grows. Choosing between the other two options could be a different decision at different companies. We could foresee some issues for our use case if we chose option 2:", "Hence, we decided to go with building an event-based queue infrastructure. Also because we have planned some future use cases and services that were appropriate to be based on events, like notification services, data warehousing, microservice architecture, etc. Without further ado, let\u2019s jump directly into the general overview of the solution and services used.", "For the implementation of an event-based streaming infrastructure, we decided to go with the Confluent Kafka Stack.", "Following are the services we incorporated:", "Apache Kafka: Kafka is an open-source-based distributed event streaming platform. It would be the main storage area of our database events (inserts, updates, and deletes).", "Kafka Connect: We use Kafka-connect to ingest data into Kafka from the Debezium\u2019s Postgres connector, which fetches the events from Postgres WAL files.At the sink side, we use ElasticSearch Connector to process and load data into Elasticsearch. Connect can run either as a standalone application, or as a fault-tolerant, and scalable service for a production environment.", "ksqlDB: ksqlDB allows to build a stream processing application over data in Kafka. It uses Kafka-streams internally, to transform events as they come. We used it for enriching events of a particular stream with pre-existing events of other tables already persisted in Kafka that might be relevant for search abilities, for example, the tenant_id from the root table.", "With ksqlDB, it is as easy as writing SQL queries to Filter, Aggregate, Join, and Enrich data. For example, let's assume we are receiving a stream of events on two topics with information related to brands and brand_products. Considering this as a multi-tenant data source, we need to enrich brand_products with tenant_id which is currently only associated with brands. Then we can use these enriched records and store them in Elasticsearch in a denormalized form (to make the search work).", "We can set up a KStream using a topic:", "To use only a few columns and partition the stream by id, we can create a new stream called enriched_brands:", "The set of events can then be materialized by the latest offset in a KTable. We use this so that we can join the current state of brand events with some other stream.", "Now we add a new stream called brand_products that has a field brand_id with it, but not tenant_id.", "We can enrich the brand_products with tenant_id using the following join query:", "Schema Registry: It\u2019s a layer over Kafka for storing metadata of the events you ingest in Kafka. It is based on AVRO schemas and provides a REST interface for storing and retrieving them. It helps in ensuring some schema compatibility checks and their evolution over time.", "We use Docker and docker-compose to configure and deploy our services. Below are ready-to-build services written in a docker-compose file that will run Postgres, Elasticsearch, and Kafka-related services. I\u2019ll also explain each of the services mentioned below.", "For streaming out the events from the source database, we need to enable logical decoding to allow replication from its logs. As in the case of Postgres, these logs are called Write-Ahead Logs (WAL), and they get written to a file. We need a logical decoding plugin, in our case wal2json to extract easy-to-read information about persistent database changes so that it can be emitted out as events to Kafka.", "For setting up the required extensions you may refer to this Postgres Dockerfile.", "For both, Elasticsearch and Postgres, we specify some necessary variables in the environment file, to set them up with a username, password, etc.", "Zookeeper, in general, acts as a centralized service to distributed platforms like Kafka, which stores all the metadata like the status of Kafka nodes and keeps track of topics or partitions.", "There are plans to run Kafka without a zookeeper, but for now, it is a necessary requirement for managing the cluster.", "For the sake of simplicity, we\u2019d be setting up a single-node Kafka cluster. I would be discussing more on multiple brokers cluster in Part 2 of this series.", "It is particularly important to understand some configurations we made here for the Kafka broker.", "ListenersAs Kafka is designed to be a distributed platform, we need to provide certain ways of allowing Kafka brokers to communicate with each other internally, and with other clients externally based on your network structure. So we do that using listeners, and a listener is a combination of Host, Port, and Protocol.", "\u2192 KAFKA_LISTENERSThis is a list of host, port, and protocol combination interfaces, the Kafka binds to. By default, it is set to 0.0.0.0; that is listening on all interfaces.", "\u2192 KAFKA_ADVERTISED_LISTENERSThe values for this is again a combination of host and port which the client will use to connect to a Kafka broker. So if the client is within docker, it can connect to the broker using broker:9092 or if there\u2019s a client external to docker, it is returned localhost:9092 to make the connection. We also need to mention the listener name which is then mapped to an appropriate protocol, to make the connection.", "\u2192 KAFKA_LISTENER_SECURITY_PROTOCOL_MAPHere we map a user-defined listener name to the protocol we want to use for communication; It can be either PLAINTEXT (unencrypted) or SSL (encrypted). These names are further used in KAFKA_LISTENERS and KAFKA_ADVERTISED_LISTENERS to use appropriate protocols with host/IP.", "As we are configuring only a single-node Kafka cluster, the returned, or let\u2019s say the advertised address to any client is of the same broker itself.", "For a single-node schema-registry, we specify the connection string for the zookeeper, used by the Kafka cluster; to store schema-related data.", "We see some new parameters like:", "\u2192 CONNECT_BOOTSTRAP_SERVERS: A set of host and port combinations that are used for establishing the initial connection to the Kafka cluster.", "\u2192 CONNECT_KEY_CONVERTER: Used for serializing the key from connect format to a format that is compatible with Kafka. Similarly, for CONNECT_VALUE_CONVERTER we use the AvroConverter for serialization.", "It\u2019s pretty important to map volumes for our source and sink connector plugins and specify them in the CONNECT_PLUGIN_PATH", "If you don\u2019t intend to use Kafka-Connect and when there is no need to scale Kafka-Connect independent of ksql, you can set the embedded-connect configurations for ksql; which also exposes connect endpoints from the ksqldb-server", "Other than that, there\u2019s an environment variable that needs some consideration:", "\u2192 KSQL_KSQL_STREAMS_MAX_TASK_IDLE_MS: For current versions of ksqlDB, for stream-table joins, the joining of results can become non-deterministic i.e you may not get a successful join if in real-time the event in the table to be joined isn\u2019t created/updated before the stream event. Configuring this environment variable helps in somewhat waiting for the event to load in the table when an event in the stream arrives for that particular timestamp. This improves join predictability but may cause some performance degradation. Efforts are being put to improve this here.", "Actually, if you don\u2019t understand the above clearly, I would suggest you use this config for now, as it just works; it actually needs another post to discuss time synchronization in detail or if you\u2019re still curious, you can watch this video by Matthias J. Sax, from Confluent.", "It is pretty handy to use a ksqldb-cli service to try and test streams when you\u2019re in testing or development environments. Even in production environments if you want to explore event-streams or Ktables; or manually create or filter streams. Though it is suggested that you automate stream, table, or topic creation either using ksql or kafka clients or their REST endpoints, which we will discuss below.", "The purpose of this service is to initialize the streams and configure things inside Kafka and other services we are using. At the time of deployment, we would not want to manually create topics, streams, connections, etc. on the server. So we utilize the REST services offered for each of the services and write a shell script to automate this process.", "Our setup script looks something like the below:", "This is what currently works for us:", "\u2192 We make sure all the services are ready before we run any jobs over them;\u2192 We need to be sure the topics exist on Kafka, or we create new ones; \u2192 Our streams should be functional even if there\u2019s any schema update;\u2192 Connections are made again, to account for the password or version changes of the underlying data sources or sinks.", "The purpose of sharing this setup script is only to demonstrate a way to automate these pipelines. The exact same setup might not work for you, but the idea remains the same for automating your workflows and avoiding any manual work on every deployment across any environment.", "To have this Data Infrastructure up and running really quick for you, refer to this Github Repository:", "So, clone the repository and do a:", "Create brands and brand_products tables inside the store Postgres database:", "Insert some records in the brands table:", "And some records in brand_products table:", "See the brand_products getting enriched with tenant_id in Elasticsearch:", "I\u2019ll be continuously contributing to the above repository; adding deployment configurations for a multi-node Kafka infrastructure with Kubernetes; writing more connectors; implementing a framework for a plug-and-play architecture, using only the desired services. Feel free to contribute to it or let me know of any Data Engineering problems that you face in your current setup, here.", "I hope this post gives you a fair idea on deploying and running a complete Kafka stack, for a basic yet effective use case of building Real-time Stream Processing Applications.", "Based on the nature of your product or company, the deployment process may vary to suit your requirements. I do have plans to address the scalability aspect of such a system in the next part of this series which would be about deploying such an infrastructure on Kubernetes, for exactly the same use case.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Building ML & Data Platforms at MongoDB \ud83c\udf43 LinkedIn: linkedin.com/in/sahil-malhotra \u2022 Website: sahilmalhotra.com"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fa27ecb0ab0ae&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fenabling-a-powerful-search-capability-building-and-deploying-a-real-time-stream-processing-etl-a27ecb0ab0ae&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fenabling-a-powerful-search-capability-building-and-deploying-a-real-time-stream-processing-etl-a27ecb0ab0ae&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fenabling-a-powerful-search-capability-building-and-deploying-a-real-time-stream-processing-etl-a27ecb0ab0ae&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fenabling-a-powerful-search-capability-building-and-deploying-a-real-time-stream-processing-etl-a27ecb0ab0ae&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----a27ecb0ab0ae--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a27ecb0ab0ae--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@sahilmalhotra_?source=post_page-----a27ecb0ab0ae--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@sahilmalhotra_?source=post_page-----a27ecb0ab0ae--------------------------------", "anchor_text": "Sahil Malhotra"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff9edbf23da83&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fenabling-a-powerful-search-capability-building-and-deploying-a-real-time-stream-processing-etl-a27ecb0ab0ae&user=Sahil+Malhotra&userId=f9edbf23da83&source=post_page-f9edbf23da83----a27ecb0ab0ae---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa27ecb0ab0ae&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fenabling-a-powerful-search-capability-building-and-deploying-a-real-time-stream-processing-etl-a27ecb0ab0ae&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa27ecb0ab0ae&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fenabling-a-powerful-search-capability-building-and-deploying-a-real-time-stream-processing-etl-a27ecb0ab0ae&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://klient.ca", "anchor_text": "klient.ca"}, {"url": "https://www.elastic.co/logstash", "anchor_text": "Logstash"}, {"url": "https://confluent.io", "anchor_text": "Confluent"}, {"url": "https://debezium.io/documentation/reference/connectors/postgresql.html", "anchor_text": "Debezium\u2019s"}, {"url": "https://www.postgresql.org/docs/9.0/wal-intro.html", "anchor_text": "WAL"}, {"url": "https://github.com/behindthescenes-group/oesophagus/blob/master/services/postgres/Dockerfile", "anchor_text": "Dockerfile"}, {"url": "https://cwiki.apache.org/confluence/display/KAFKA/KIP-500%3A+Replace+ZooKeeper+with+a+Self-Managed+Metadata+Quorum", "anchor_text": "Kafka without a zookeeper"}, {"url": "https://cwiki.apache.org/confluence/display/KAFKA/KIP-695%3A+Further+Improve+Kafka+Streams+Timestamp+Synchronization", "anchor_text": "here"}, {"url": "https://www.confluent.io/resources/kafka-summit-2020/the-flux-capacitor-of-kafka-streams-and-ksqldb/", "anchor_text": "watch this video"}, {"url": "https://github.com/behindthescenes-group/oesophagus", "anchor_text": "behindthescenes-group/oesophagusWill help in shaping initial features. Oesophagus enables you to deploy an entirely plug-n-play Data Infrastructure to\u2026github.com"}, {"url": "https://forms.gle/GGg2hvnEpG6r4bgg7", "anchor_text": "here"}, {"url": "https://medium.com/tag/kafka?source=post_page-----a27ecb0ab0ae---------------kafka-----------------", "anchor_text": "Kafka"}, {"url": "https://medium.com/tag/elasticsearch?source=post_page-----a27ecb0ab0ae---------------elasticsearch-----------------", "anchor_text": "Elasticsearch"}, {"url": "https://medium.com/tag/stream-processing?source=post_page-----a27ecb0ab0ae---------------stream_processing-----------------", "anchor_text": "Stream Processing"}, {"url": "https://medium.com/tag/data-science?source=post_page-----a27ecb0ab0ae---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/data-engineering?source=post_page-----a27ecb0ab0ae---------------data_engineering-----------------", "anchor_text": "Data Engineering"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa27ecb0ab0ae&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fenabling-a-powerful-search-capability-building-and-deploying-a-real-time-stream-processing-etl-a27ecb0ab0ae&user=Sahil+Malhotra&userId=f9edbf23da83&source=-----a27ecb0ab0ae---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa27ecb0ab0ae&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fenabling-a-powerful-search-capability-building-and-deploying-a-real-time-stream-processing-etl-a27ecb0ab0ae&user=Sahil+Malhotra&userId=f9edbf23da83&source=-----a27ecb0ab0ae---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa27ecb0ab0ae&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fenabling-a-powerful-search-capability-building-and-deploying-a-real-time-stream-processing-etl-a27ecb0ab0ae&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a27ecb0ab0ae--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fa27ecb0ab0ae&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fenabling-a-powerful-search-capability-building-and-deploying-a-real-time-stream-processing-etl-a27ecb0ab0ae&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----a27ecb0ab0ae---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----a27ecb0ab0ae--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----a27ecb0ab0ae--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----a27ecb0ab0ae--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----a27ecb0ab0ae--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----a27ecb0ab0ae--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----a27ecb0ab0ae--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----a27ecb0ab0ae--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----a27ecb0ab0ae--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@sahilmalhotra_?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@sahilmalhotra_?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Sahil Malhotra"}, {"url": "https://medium.com/@sahilmalhotra_/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "56 Followers"}, {"url": "http://linkedin.com/in/sahil-malhotra", "anchor_text": "linkedin.com/in/sahil-malhotra"}, {"url": "http://sahilmalhotra.com", "anchor_text": "sahilmalhotra.com"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff9edbf23da83&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fenabling-a-powerful-search-capability-building-and-deploying-a-real-time-stream-processing-etl-a27ecb0ab0ae&user=Sahil+Malhotra&userId=f9edbf23da83&source=post_page-f9edbf23da83--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F462321f7df98&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fenabling-a-powerful-search-capability-building-and-deploying-a-real-time-stream-processing-etl-a27ecb0ab0ae&newsletterV3=f9edbf23da83&newsletterV3Id=462321f7df98&user=Sahil+Malhotra&userId=f9edbf23da83&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}