{"url": "https://towardsdatascience.com/twitter-topic-modeling-e0e3315b12e2", "time": 1683010822.6363468, "path": "towardsdatascience.com/twitter-topic-modeling-e0e3315b12e2/", "webpage": {"metadata": {"title": "Twitter Topic Modeling. Using Machine Learning (Gensim Linear\u2026 | by Amin Azad | Towards Data Science", "h1": "Twitter Topic Modeling", "description": "I\u2019m a machine learning geek and I\u2019d like to apply machine learning on everything that I can just to see the results. On the other hand, I started a new role at SoMe (Social Media Management Platform)\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.so-me.net/", "anchor_text": "SoMe (Social Media Management Platform)", "paragraph_index": 0}, {"url": "https://www.so-me.net/", "anchor_text": "SoMe", "paragraph_index": 1}, {"url": "https://www.tweepy.org/", "anchor_text": "Tweepy", "paragraph_index": 2}, {"url": "https://colab.research.google.com/notebooks/intro.ipynb", "anchor_text": "Google Colab", "paragraph_index": 3}, {"url": "https://towardsdatascience.com/so-whats-spacy-ad65aa1949e0", "anchor_text": "here", "paragraph_index": 7}, {"url": "https://rare-technologies.com/what-is-topic-coherence/", "anchor_text": "topic coherence", "paragraph_index": 12}, {"url": "https://scikit-learn.org/stable/index.html", "anchor_text": "Scikit-learn\u2019s", "paragraph_index": 17}, {"url": "https://groups.google.com/forum/#!topic/gensim/z0wG3cojywM", "anchor_text": "google forum post", "paragraph_index": 23}, {"url": "https://www.google.com/url?q=https%3A%2F%2Fwww.cs.princeton.edu%2F~blei%2Fpapers%2FHoffmanBleiBach2010b.pdf&sa=D&sntz=1&usg=AFQjCNEqb96J5aHxeTva_s_ieCKGzOBbDA", "anchor_text": "here", "paragraph_index": 23}, {"url": "https://www.so-me.net/", "anchor_text": "SoMe", "paragraph_index": 34}, {"url": "https://radimrehurek.com/gensim/models/wrappers/ldamallet.html", "anchor_text": "gensim LDA Mallet", "paragraph_index": 37}, {"url": "https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/", "anchor_text": "gensim tutorial on LDA", "paragraph_index": 37}, {"url": "https://www.linkedin.com/in/jacobpadgett/", "anchor_text": "Jacob Padgett", "paragraph_index": 38}, {"url": "https://www.linkedin.com/in/lawrence-kimsey/", "anchor_text": "Lawrence Kimsey", "paragraph_index": 38}, {"url": "https://github.com/some-labs-24/data-science", "anchor_text": "here", "paragraph_index": 38}], "all_paragraphs": ["I\u2019m a machine learning geek and I\u2019d like to apply machine learning on everything that I can just to see the results. On the other hand, I started a new role at SoMe (Social Media Management Platform) and within the data science team, we continuously brainstorm around using machine learning to create more value for our users and help them grow their follower base. We thought long and hard to come up with the best ideas, and in the process, we realized the first step is to provide the user with an overall understanding of their follower base.", "There is a wide range of data science and machine learning techniques that could be applied for providing a better understanding of your followers, from applying CNN (Convolutional Neural Networks) on Instagram to applying natural language processing techniques on Twitter, Linkedin or any other text-based data. Majority of our users use SoMe to sort and schedule their future posts on Twitter so as a first step we decided to provide our users with insights about the topics their Twitter followers engage with the most. To do that we first had to define and score follower engagement. After a long discussion, we defined and scored follower engagement as \u201cWhen a follower interacts with the content that user posts by 1. Retweeting (5/5) \u2014 2. Retweeting with a comment (4/5) \u2014 3. Liking (3/5) \u20144. Comment(2/5) \u2014 5. Mentioning (1/5)\u201d.", "Next step was to actually get the data from a Twitter api, I will write about applying for a Twitter api and using Tweepy and other tools to pull data and refine data to get to the data you need from the api in a future post. You can use the following techniques on any kind of text data and figure out what are the most important topics discussed within your dataset.", "Depending on your choice of python notebook, you are going to need to install and load the following packages to perform topic modeling. Among the SoMe, data science team we use a wide range of notebook options, from Azure to Jupyter labs and notebook. My personal favorite however is Google Colab. I recommend running all the following snippets on either Google Colab or Jupyter notebook. I\u2019m also going to link the Github repository for this project and a link to the final notebook we used for your reference.", "After pulling and refining the data from the Twitter api and importing the required packages, we had to clean the data from emojis and urls so we can tokenize it for the next steps. You can find the cleaning syntaxes we used in the below snippet:", "As you can see in the table below, there are a lot of emojis and urls you want to remove before tokenizing. Data cleaning is essential for ultimately delivering accurate results, we do not want words from websites or emojis end up in our topic modeling results since they deliver almost no value in figuring out what is the general topic of a bag of words.", "Our goal in the data pre-processing stage is to convert sentences into words, convert words to their root and removing words that are too common or too irrelevant to the purpose of our topic modeling project. We used the following techniques to reach our goal, I\u2019m going to share the code and walk you through each stage:", "Tokenization is always the first step before we can do any text data processing. What this means is that spaCy will segment sentences into words, punctuations, symbols and others by applying specific rules to each language. Spacy is a pre-trained natural language processing model capable of figuring out the relationship between words. You can learn more about Spacy here.", "Lemmatization is a process where we convert words to its root word. For example: \u2018Studying\u2019 becomes \u2018Study\u2019, \u2018Meeting becomes \u2018Meet\u2019, \u2018Better\u2019 and \u2018Best\u2019 become \u2018Good\u2019.The advantage of this is, we get to reduce the total number of unique words in the dictionary. As a result, the number of columns in the document-word matrix will be denser with lesser columns. The ultimate goal of lemmatization is to help the LDA model to produce better topics in the end.", "We produce a base model first to be used to track our progress as we go through the hyper-parameter tuning stage. The LDA topic model algorithm requires a document word matrix and a dictionary as the main inputs. For the first few steps to be taken before running the LDA model, we created a dictionary, filtered the extremes and, create a corpus object which is the document matrix LDA model needs as the main input.", "We prepared everything required to train the LDA model. In addition to the corpus and dictionary, we needed to provide the number of topics as well. We chose 5 for the base model. In the hyperparameter tuning stage we will reach to an optimum number of topics to be used.", "By printing out the topics produced from the LDA model, we can have a rough guess about the topic related to each bag of words. It\u2019s important to mention the bag of words have been sorted from the most relevant to the least relevant for each topic.", "Model perplexity and topic coherence provide a convenient measure to judge how good a given topic model is. In my experience, topic coherence score, in particular, has been more helpful.", "We got 0.17 for coherence score, which is a very low score for almost any LDA model but remember this is only the base model, we are going to substantially improve it.", "Now that the LDA model is built, the next step is to examine the produced topics and the associated keywords. There is no better tool than pyLDAvis package\u2019s interactive chart and is designed to work well with Google Colab and Jupyter notebooks.", "So how to infer pyLDAvis\u2019s output?", "Each bubble on the left-hand side plot represents a topic. The larger the bubble, the more prevalent is that topic. A good topic model will have fairly big, non-overlapping bubbles scattered throughout the chart instead of being clustered in one quadrant. A model with too many topics, will typically have many overlaps, small sized bubbles clustered in one region of the chart.", "Alright, if we move the cursor over one of the bubbles, the words and bars on the right-hand side will update. These words are the salient keywords that form the selected topic. We have successfully built a good looking topic model. Up next, we will improve upon this model (Hyperparameter Tuning) by using Scikit-learn\u2019s Grid Search and then we will focus on how to arrive at the optimal number of topics and other variables in within the LDA model.", "The most important tuning parameter for LDA models is n_components (number of topics). In addition, we are going to search learning_decay (which controls the learning rate) as well. Besides these, other possible search params could be learning_offset (down weight early iterations. Should be > 1) and max_iter. These could be worth experimenting if you have enough time and computing resources.", "Be warned, the grid search constructs multiple LDA models for all possible combinations of param values in the param_grid dict. So, this process can consume a lot of time and resources.", "We know there is a high chance number of topics is much more than 10 but through the grid search we found out 10 topics performs better than other number of topics. This makes us think deeply about the results, and we figured Scikit-learn\u2019s grid search tracks perplexity not coherence value and for our use case, coherence value delivers the best result. In the next step we\u2019re going to get to an optimal number of topics.", "Our approach to finding the optimal number of topics was to build many LDA models with different values of number of topics and pick the one that gives the highest coherence value.", "Choosing a number of topic that marks the end of a rapid growth of topic coherence usually offers meaningful and interpretable topics. Picking an even higher value can sometimes provide more granular sub-topics.If you see the same keywords being repeated in multiple topics, it\u2019s probably a sign that the \u2018number of topics\u2019 is too large. By following these principles we chose 68 topics as an optimal number of topics for our use case.", "Passes, chunksize and update_every are parameters that have a EM/Variational relationship together. We are not going to go into the details of EM/Variational Bayes here, but if you are curious, check out this google forum post and the paper it references here. For our use case, hyperparameter tuning number of passes would suffice considering chuncksize is not substantial and update_every would not change much in the final results.", "We tried many numbers of passes and 20 seems to produce the best result. Not substantially though, only a few decimal points.", "Alpha is the hyperparameter for the Dirichlet prior. The Dirichlet prior is the distribution from which we draw theta. And theta becomes the parameter that decides what shape the topic distribution is. So essentially, alpha influences how we draw topic distributions. That is why we\u2019re gonna try to hyperparameter tune to choose the best alpha type that gives you a better topic distribution.", "In our use case, symmetric alpha delivers a better result, not substantially but we are going to use symmetric alpha for our LDA model.", "Learning decay controls the learning rate of the model. Since you can only choose 0.5, 0.7 and 0.9, we\u2019re going to try all three and see which option delivers the best coherence value.", "In our use case, 0.5 decay delivers the best coherence value. In fact, decay and coherence value in our use case seems to be negatively correlated.", "iterations is somewhat technical, but essentially it controls how often we repeat a particular loop over each document. It is important to set the number of \u201citerations\u201d high enough.", "In our use case, we tried 50 to 100 iterations but since we want to prevent the model from overfitting we chose 70 which showed a decent amount of coherence score improvement.", "This hyperparameter disregards topics with probability assigned lower than what is assigned. It can not be lower than 0.01 and higher than 0.1.", "We tried the full range of 0.01 to 0.1 and coherence value remained constant across the full range so we left it to the default value of 0.01 to get the most amount of topics possible.", "It is always a good practice to track your progress from the beginning. We have been tracking our progress going through 24 iterations of model hyperparameter tuning and our best model delivers a coherence value of 0.47 which is a pretty good number considering the wide range of topics that could be discussed among users follower base.", "We finally got to our final model. We\u2019re currently using the model in the snippet below at SoMe to provide our users with the topics their followers interact with the most.", "If we take a closer look at the topics produced by the final model, we can guess with a high accuracy what each topic is about. Now we have to remember they are sorted from the highest importance to the least. For example, if we look at Topic one we figure it's about the COVID-19 crisis as the first three words are \u201cdie-virus-lockdown\u201d and not so much about \u201cgold\u201d since \u201cgold\u201d is the fourth word which does not reflect a high probability in the bag of words.", "As we covered before, a good topic model will have non-overlapping, fairly big sized blobs for each topic and if we increase number of topics based on the use case we\u2019re going to have smaller blobs and some overlap in some cases but overall we can see in the final model, the distribution and overlap have significantly improved. This seems to be the case here. So, we are good.", "We\u2019ve covered some cutting-edge topic modeling approaches in this post. We continuously work to build new features for our users and harness machine learning\u2019s power to provide a better experience for our users. To improve this model you can explore modifying it by using gensim LDA Mallet which in some cases provides more accurate results. For those concerned about the time, memory consumption and variety of topics when building topic models check out the gensim tutorial on LDA.", "At the end, I have to thank the rest of SoMe data science team especially Jacob Padgett and Lawrence Kimsey. At SoMe, we strongly believe in open source software and you can find all our analytics models notebooks here.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "AI & Machine Learning \ud83e\udd16 Skydive \ud83e\ude82 Tennis \ud83c\udfbe"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fe0e3315b12e2&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftwitter-topic-modeling-e0e3315b12e2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftwitter-topic-modeling-e0e3315b12e2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftwitter-topic-modeling-e0e3315b12e2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftwitter-topic-modeling-e0e3315b12e2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----e0e3315b12e2--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e0e3315b12e2--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@amindazad?source=post_page-----e0e3315b12e2--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@amindazad?source=post_page-----e0e3315b12e2--------------------------------", "anchor_text": "Amin Azad"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcf1382641795&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftwitter-topic-modeling-e0e3315b12e2&user=Amin+Azad&userId=cf1382641795&source=post_page-cf1382641795----e0e3315b12e2---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe0e3315b12e2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftwitter-topic-modeling-e0e3315b12e2&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe0e3315b12e2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftwitter-topic-modeling-e0e3315b12e2&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/photos/V6CdmV277nY", "anchor_text": "Morning Brew Unsplash Channel"}, {"url": "https://radimrehurek.com/gensim/models/ldamodel.html", "anchor_text": "Gensim Linear Discriminant Analysis"}, {"url": "https://www.so-me.net/", "anchor_text": "SoMe (Social Media Management Platform)"}, {"url": "https://www.so-me.net/", "anchor_text": "SoMe"}, {"url": "https://www.tweepy.org/", "anchor_text": "Tweepy"}, {"url": "https://colab.research.google.com/notebooks/intro.ipynb", "anchor_text": "Google Colab"}, {"url": "https://towardsdatascience.com/so-whats-spacy-ad65aa1949e0", "anchor_text": "here"}, {"url": "https://rare-technologies.com/what-is-topic-coherence/", "anchor_text": "topic coherence"}, {"url": "https://scikit-learn.org/stable/index.html", "anchor_text": "Scikit-learn\u2019s"}, {"url": "https://groups.google.com/forum/#!topic/gensim/z0wG3cojywM", "anchor_text": "google forum post"}, {"url": "https://www.google.com/url?q=https%3A%2F%2Fwww.cs.princeton.edu%2F~blei%2Fpapers%2FHoffmanBleiBach2010b.pdf&sa=D&sntz=1&usg=AFQjCNEqb96J5aHxeTva_s_ieCKGzOBbDA", "anchor_text": "here"}, {"url": "https://www.so-me.net/", "anchor_text": "SoMe"}, {"url": "https://radimrehurek.com/gensim/models/wrappers/ldamallet.html", "anchor_text": "gensim LDA Mallet"}, {"url": "https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/", "anchor_text": "gensim tutorial on LDA"}, {"url": "https://www.linkedin.com/in/jacobpadgett/", "anchor_text": "Jacob Padgett"}, {"url": "https://www.linkedin.com/in/lawrence-kimsey/", "anchor_text": "Lawrence Kimsey"}, {"url": "https://github.com/some-labs-24/data-science", "anchor_text": "here"}, {"url": "https://medium.com/tag/data-science?source=post_page-----e0e3315b12e2---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/naturallanguageprocessing?source=post_page-----e0e3315b12e2---------------naturallanguageprocessing-----------------", "anchor_text": "Naturallanguageprocessing"}, {"url": "https://medium.com/tag/lda?source=post_page-----e0e3315b12e2---------------lda-----------------", "anchor_text": "Lda"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----e0e3315b12e2---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----e0e3315b12e2---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe0e3315b12e2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftwitter-topic-modeling-e0e3315b12e2&user=Amin+Azad&userId=cf1382641795&source=-----e0e3315b12e2---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe0e3315b12e2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftwitter-topic-modeling-e0e3315b12e2&user=Amin+Azad&userId=cf1382641795&source=-----e0e3315b12e2---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe0e3315b12e2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftwitter-topic-modeling-e0e3315b12e2&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e0e3315b12e2--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fe0e3315b12e2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftwitter-topic-modeling-e0e3315b12e2&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----e0e3315b12e2---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----e0e3315b12e2--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----e0e3315b12e2--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----e0e3315b12e2--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----e0e3315b12e2--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----e0e3315b12e2--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----e0e3315b12e2--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----e0e3315b12e2--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----e0e3315b12e2--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@amindazad?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@amindazad?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Amin Azad"}, {"url": "https://medium.com/@amindazad/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "61 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcf1382641795&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftwitter-topic-modeling-e0e3315b12e2&user=Amin+Azad&userId=cf1382641795&source=post_page-cf1382641795--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fbf82d5c6e3e4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftwitter-topic-modeling-e0e3315b12e2&newsletterV3=cf1382641795&newsletterV3Id=bf82d5c6e3e4&user=Amin+Azad&userId=cf1382641795&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}