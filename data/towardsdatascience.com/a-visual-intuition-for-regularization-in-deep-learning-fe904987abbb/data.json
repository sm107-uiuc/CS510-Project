{"url": "https://towardsdatascience.com/a-visual-intuition-for-regularization-in-deep-learning-fe904987abbb", "time": 1683005134.979993, "path": "towardsdatascience.com/a-visual-intuition-for-regularization-in-deep-learning-fe904987abbb/", "webpage": {"metadata": {"title": "A Visual Intuition For Regularization in Deep Learning | by Kelvin Lee | Towards Data Science", "h1": "A Visual Intuition For Regularization in Deep Learning", "description": "In machine learning, regularization is a approach used to combat high variance \u2014 in other words, the issue of your model learning to reproduce the data, rather than the underlying semantics about\u2026"}, "outgoing_paragraph_urls": [{"url": "https://mybinder.org/v2/gh/laserkelvin/understanding-ml/master", "anchor_text": "you can run some of the code on Binder.", "paragraph_index": 40}, {"url": "https://github.com/laserkelvin/understanding-ml", "anchor_text": "My github repo with the experiments", "paragraph_index": 41}, {"url": "https://twitter.com/cmmmsubmm", "anchor_text": "Twitter", "paragraph_index": 42}, {"url": "https://www.linkedin.com/in/kelvin-lee-378b2a134/?lipi=urn%3Ali%3Apage%3Ad_flagship3_feed%3BLXdZXMdLQp6UIHsHCXK9tA%3D%3D", "anchor_text": "LinkedIn", "paragraph_index": 42}], "all_paragraphs": ["In machine learning, regularization is a approach used to combat high variance \u2014 in other words, the issue of your model learning to reproduce the data, rather than the underlying semantics about your problem. In an analogous way to humans learning, the idea is to construct your homework problems to test and build for knowledge, rather than simply rote learning: for example, learning multiplication tables as opposed to learning how to multiply.", "This kind of phenomenon is especially prevalent in learning by neural networks \u2014 with great learning capacity comes a large likelihood for memorization, and it is up to us practitioners to guide deep learning models into soaking up our problem, not our data. Many of you will have come across these methods in the past, and may have developed your own intuition for how different regularization methods affect the outcome. For those of you who don\u2019t (and even for those who do!) this article provides a visual guide for how neural network parameters are shaped by regularization. It\u2019s important to visualize these aspects, as it\u2019s extremely easy to take many concepts for granted; the figures and their explanations in this article will hopefully help you create an intuition for what really happens to your model parameters as you increase regularization.", "In this article, I\u2019ll cover L2 and dropouts as standard forms of regularization. I won\u2019t discuss how other methods (such as collecting more data) might change the way your model works; that\u2019s probably for another time.", "All of the figures and models were made with the standard scientific Python stack: numpy, matplotlib, scipy, sklearn, and the neural network models were built with PyTorch.", "One of the core tenets for deep learning is the ability for deep neural networks to act as universal function approximations. The idea that whatever you may be interested in\u2014spread of disease, self-driving cars, astronomy\u2014can be compressed and expressed by a self-learning model is absolutely amazing! This is despite whether or not the problem you are interested in can actually be expressed as some analytic function f. As you condition a machine learning model through training, the model takes on parameters \u03b8 that allows the model to approximately learn f*.", "For illustration purposes, we\u2019re going to be looking at some relatively simple data: ideally, something in one-dimension that is complex enough for old-school curve fitting to be a pain, but not hard enough to make abstraction and understanding for us to be difficult. So, I\u2019m going to create some arbitrarily complex function that mimics periodic signals, but with some funkiness added to it. The function below implements the following equation:", "where A, B, C are random numbers sampled from different Gaussian distributions. The effect of each of these values is to add lag between very similar functions, such that they add together randomly to generate very different values of f. We will also add white (Gaussian) noise to the data, to simulate the effect of the data being collected.", "Let\u2019s visualize a randomly generated sample of this data: for the rest of this article, we\u2019ll be looking to reproduce this curve with a small neural network.", "To do our model training, we\u2019re going to have split up this into training/validation sets. For this, I\u2019ll be using the extremely convenient train_test_split function in sklearn.model_selection. Let\u2019s plot up the training and validation sets:", "As we see in the plot, both sets do a pretty good job in representing the overall curve: if we were to remove one or the other, we could glean more or less the same picture of what the data represents. This is a pretty vital aspect to cross-validation!", "Now that we have a data set, we\u2019re going to need a relatively simple model to try and reproduce it. For this purpose, we\u2019re just going to be dealing with a four-layer neural network, comprising single input and output values with three hidden layers, each 64 neurons wide.", "For convenience, each hidden layer has a LeakyReLU activation, with ReLU activation on the output. In principle these shouldn\u2019t matter so much, but during testing sometimes the model had trouble learning some of the \u201ccomplex\u201d functions, particularly when activations like tanh and sigmoid were used, which saturated easily. For the purposes of this article, the specifics of this model doesn\u2019t matter much: all that matters is that it\u2019s a fully-connected neural network that has some capacity to learn to approximate some function.", "Just to demonstrate that the model works, I performed the usual training/validation cycles using the mean-squared-error (MSE) loss and the ADAM optimizer, without any form of regularization and ended up with these results:", "When we use this model to predict the function:", "Apart from regions of rapidly changing curvature (near x=11) the model reproduces our \u201ccomplex\u201d function pretty well!", "Now, I can hear you asking: why am I doing any regularization if the model works well? For the purposes of this demonstration, it doesn\u2019t matter so much whether or not our model is overfitting: what I want to get across is how regularization affects a model; in our case, how it can even detrimentally affect a perfectly working model. In some sense, you can interpret this as a word of warning: deal with overfitting as you encounter it, but not before. In the wise words of Donald Knuth, \u201cPremature optimization is the root of all evil\u201d.", "Now that we\u2019ve got all the boilerplate stuff out of the way, we can get to the meat of the article! Our focus is to try and develop an intuition for how different ways of regularization can affect our simple model from three perspectives:", "While the first two points are somewhat straightforward, many of you may not be familiar on how the third point can be quantified. In this demonstration, I\u2019ll be using kernel density estimation to measure the spread/variation in parameter values: for those who are familiar with Tensorboard, you\u2019ll have seen these plots; for those who aren\u2019t, think of these plots as sophisticated histograms. The goal is to visualize how our model parameters change with regularization, and the plot below shows the difference in the distributions of \u03b8 before and after training:", "The blue curve is labelled \u201cuniform\u201d, because it represents our model parameters initialized with a uniform distribution: you can see how this manifests as basically a top-hat function, with equal probability across the center. This contrasts with the trained model parameters: after training, the model requires values of \u03b8 that are non-uniform in order to actually express our function.", "One of the most straightforward approaches to regularization is the so called L2 regularization: the L2 refers to the fact that the L2 norm of our parameter matrices are used. From linear algebra, the norm of a matrix is given by:", "In pre-neural network machine learning, where parameters are more often expressed as vectors rather than matrices/tensors, this is simply the Euclidean norm. In deep learning, we\u2019re more commonly dealing with matrices/high dimensional tensors, and the Euclidean norm doesn\u2019t extend very well (beyond Euclidean geometry). The L2 norm is actually a special case of the equation above, where p=q=2 and is referred to as the Frobenius or Hilbert-Schmidt norm, which generalizes to infinite dimensionality (i.e. Hilbert space).", "In deep learning applications, the general form of applying this L2 regularization is to append a \u201cpenalty\u201d factor at the end of your cost function J:", "Very simply, this equation defines the cost function J as the MSE loss, as well as the L2 norm. The effect of the L2 norm has on the cost is multiplied by this prefactor \u03bb; this is referred to in many implementations as a \u201cweight decay\u201d hyperparameter, usually between 0 and 1. Since it controls the amount of regularization, we need to understand what this does to our model!", "In a series of experiments, we\u2019re going to repeat the same training/validation/visualization cycle as we did before, however on a range of values of \u03bb. First, how does it affect our training?", "Let\u2019s break down the plot above. The deeper shades of red correspond to larger values of \u03bb (although it\u2019s not a linear map!), showing traces of the training loss as the log of the MSE loss. Remember in our un-regularized model, these curves decreased monotonically. Here, as we increase the value of \u03bb the ultimate training error is substantially increased, and the decrease in loss at early epochs are also not as dramatic. What happens when we try to use these models to predict our function?", "We can see that, with small values of \u03bb the function can still be expressed reasonably well. The turning point appears to be around \u03bb=0.01, where the qualitative shape of the curve is reproduced but not the actual data points. From \u03bb>0.01, the model just predicts what appears to be the mean of the whole data set: as if we simply tried to do linear regression. If we interpret these with respect to our training loss, it\u2019s not wonder that the loss stops decreasing\u2014there\u2019s only so much you can do with a straight line!", "What about the distribution of parameters?", "We see that the spread of parameter values is substantially hampered, as we go from low to high \u03bb. Compared to the uniform distribution, the spread of parameter values shrinks closer and closer to zero, and with \u03bb=1.0 the distribution of \u03b8 just looks like a Dirac delta function at zero. From this, we can take away that L2 regularization acts to constrain parameter space\u2014forcing \u03b8 to be very sparse and close to zero.", "Another popular and cost-efficient way of regularization is to incorporate dropouts in your model. The idea is that with each model pass, a number of neurons are deactivated by setting their weights to zero according to some probability p. In other words, we apply a boolean mask to our parameters, and each time data passes through different units are activated. The rationale behind this is to distribute the model learning throughout the network, as opposed to specifically one or two layers/neurons.", "For our experiments, we\u2019re going to include dropout layers between each hidden layer, and adjust the dropout probability p from zero to one. In the former limit, we should just have an un-regularized model while in the latter we should have severally decreased learning capacity, as every hidden layer is deactivated.", "We see a very similar effect to the L2 regularization: overall, the learning capacity of the model is decreased and with larger values of dropout probability the ultimate loss is proportionally larger.", "When we try to use these models to predict our function:", "Going down the page, we increase in the dropout probability. Starting from p=0.1, we can see that our model starts being quite shaky about its prediction: the most interesting thing is that it appears to approximately trace our data, including the noise!", "At p=0.2 and 0.3, this is even more obvious around x=11\u2014recall that our un-regularized model had difficulty getting this area of the function right. We see that the predictions with dropouts actually makes this region incredibly fuzzy, which is almost like the model telling us that it\u2019s uncertain! (More on this later).", "From p=0.4 onwards, it seems like the capacity of the model is sufficiently hampered that it fails to reproduce most of the curve with the exception of the first portion. At p=0.6, it looks like the predictions nearly approximate the data set mean, which was what seemed to also happen to large values of L2 regularization.", "Compare this with our the L2 norm results: with dropouts, our distribution of parameters is much wider, which increases the ability of our model to express. With the exception of p=1.0, it does not appear that the actual value of dropout probability affects the distribution of parameters much, if at all. At p=1.0, our model fails to learn anything, and just resembles the uniform distribution. At a reduced value of p, the model can still manage to learn albeit at a reduced rate.", "From our simple experiments, I hope you have developed some mental model of how these two regularization methods affect neural network models, from the three perspectives we explored.", "L2 regularization is as straightforward as it gets, with a single hyperparameter to tune. As we increase the weighting of the L2 penalty, the variation in parameter space\u2014and therefore model capacity\u2014decreases extremely quickly for large values (0.01\u20131). With smaller values, you may not even see a difference in your model predictions, although it becomes apparent when you plot out the distributions of \u03b8.", "Dropouts is a more sophisticated method for regularization, as you now have to deal with another layer of hyperparameter complexity (p can have different values for different layers). Despite this, depending on how you look at it, this can actually provide another dimension of model expression: in the form of model uncertainty. I plan to cover this more in another blog post (send me a message to encourage me!). The effect of including dropouts is that the variation in \u03b8 becomes significantly larger and spread out over different possible values of \u03b8.", "In both methods, we\u2019ve seen that regularization increases the ultimate training loss. The cost of these artificial forms of regularization (as opposed to getting more training data) is that they can decrease the capacity of your model: it\u2019s not something you want to include unless you are certain that your model needs regularization. With this guide, however, you should now know how either form affects your model!", "If you\u2019re interested, you can run some of the code on Binder. I wouldn\u2019t necessarily run the torch models (it\u2019ll exhaust their poor resources), but you can use it to explore the code in a notebook.", "My github repo with the experiments", "Please look out for more of this kind of article! You can reach out to me on Twitter, LinkedIn, and of course on the Medium network!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Astrochemistry researcher at the Center for Astrophysics | Harvard & Smithsonian. Obsessed with automated workflows, machine learning, and inference."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Ffe904987abbb&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-visual-intuition-for-regularization-in-deep-learning-fe904987abbb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-visual-intuition-for-regularization-in-deep-learning-fe904987abbb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-visual-intuition-for-regularization-in-deep-learning-fe904987abbb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-visual-intuition-for-regularization-in-deep-learning-fe904987abbb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----fe904987abbb--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----fe904987abbb--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@kin.long.kelvin.lee?source=post_page-----fe904987abbb--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@kin.long.kelvin.lee?source=post_page-----fe904987abbb--------------------------------", "anchor_text": "Kelvin Lee"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd04c34f2d3b8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-visual-intuition-for-regularization-in-deep-learning-fe904987abbb&user=Kelvin+Lee&userId=d04c34f2d3b8&source=post_page-d04c34f2d3b8----fe904987abbb---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffe904987abbb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-visual-intuition-for-regularization-in-deep-learning-fe904987abbb&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffe904987abbb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-visual-intuition-for-regularization-in-deep-learning-fe904987abbb&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://mybinder.org/v2/gh/laserkelvin/understanding-ml/master", "anchor_text": "you can run some of the code on Binder."}, {"url": "http://jmlr.org/papers/v15/srivastava14a.html", "anchor_text": "Dropout regularization"}, {"url": "https://github.com/laserkelvin/understanding-ml", "anchor_text": "My github repo with the experiments"}, {"url": "https://twitter.com/cmmmsubmm", "anchor_text": "Twitter"}, {"url": "https://www.linkedin.com/in/kelvin-lee-378b2a134/?lipi=urn%3Ali%3Apage%3Ad_flagship3_feed%3BLXdZXMdLQp6UIHsHCXK9tA%3D%3D", "anchor_text": "LinkedIn"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----fe904987abbb---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/regularization?source=post_page-----fe904987abbb---------------regularization-----------------", "anchor_text": "Regularization"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----fe904987abbb---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/visualization?source=post_page-----fe904987abbb---------------visualization-----------------", "anchor_text": "Visualization"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffe904987abbb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-visual-intuition-for-regularization-in-deep-learning-fe904987abbb&user=Kelvin+Lee&userId=d04c34f2d3b8&source=-----fe904987abbb---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffe904987abbb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-visual-intuition-for-regularization-in-deep-learning-fe904987abbb&user=Kelvin+Lee&userId=d04c34f2d3b8&source=-----fe904987abbb---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffe904987abbb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-visual-intuition-for-regularization-in-deep-learning-fe904987abbb&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----fe904987abbb--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Ffe904987abbb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-visual-intuition-for-regularization-in-deep-learning-fe904987abbb&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----fe904987abbb---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----fe904987abbb--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----fe904987abbb--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----fe904987abbb--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----fe904987abbb--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----fe904987abbb--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----fe904987abbb--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----fe904987abbb--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----fe904987abbb--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@kin.long.kelvin.lee?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@kin.long.kelvin.lee?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Kelvin Lee"}, {"url": "https://medium.com/@kin.long.kelvin.lee/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "28 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd04c34f2d3b8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-visual-intuition-for-regularization-in-deep-learning-fe904987abbb&user=Kelvin+Lee&userId=d04c34f2d3b8&source=post_page-d04c34f2d3b8--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Fd04c34f2d3b8%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-visual-intuition-for-regularization-in-deep-learning-fe904987abbb&user=Kelvin+Lee&userId=d04c34f2d3b8&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}