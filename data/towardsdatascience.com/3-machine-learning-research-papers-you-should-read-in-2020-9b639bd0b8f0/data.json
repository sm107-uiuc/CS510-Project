{"url": "https://towardsdatascience.com/3-machine-learning-research-papers-you-should-read-in-2020-9b639bd0b8f0", "time": 1683008295.350136, "path": "towardsdatascience.com/3-machine-learning-research-papers-you-should-read-in-2020-9b639bd0b8f0/", "webpage": {"metadata": {"title": "Top 4 Important Machine Learning Papers You Should Read in 2021 | Towards Data Science", "h1": "Top 4 Important Machine Learning and Deep Learning Papers You Should Read in 2021", "description": "Machine Learning suddenly became one of the most critical domains of Computer Science. In this article, we will discuss the Top 4 Recent Machine Learning.."}, "outgoing_paragraph_urls": [{"url": "https://www.iunera.com/kraken/fabric/machine-learning/", "anchor_text": "Machine Learning", "paragraph_index": 1}, {"url": "https://www.iunera.com/kraken/fabric/machine-learning/", "anchor_text": "Machine Learning", "paragraph_index": 2}, {"url": "https://www.iunera.com/kraken/fabric/machine-learning/", "anchor_text": "Machine Learning", "paragraph_index": 3}, {"url": "https://scholar.google.com/", "anchor_text": "scholar.google.com", "paragraph_index": 4}, {"url": "https://academic.microsoft.com/", "anchor_text": "academic.microsoft.com", "paragraph_index": 4}, {"url": "https://www.semanticscholar.org/", "anchor_text": "semanticscholar.org", "paragraph_index": 4}, {"url": "https://www.iunera.com/kraken/fabric/machine-learning/", "anchor_text": "natural language processing, conversational AI, computer vision, reinforcement learning", "paragraph_index": 5}, {"url": "https://www.iunera.com/kraken/fabric/machine-learning/", "anchor_text": "Machine Learning", "paragraph_index": 6}, {"url": "https://www.iunera.com/kraken/fabric/machine-learning/", "anchor_text": "Machine Learning", "paragraph_index": 6}, {"url": "https://www.iunera.com/kraken/fabric/machine-learning/", "anchor_text": "Neural Network", "paragraph_index": 6}, {"url": "https://www.kdnuggets.com/2017/04/top-20-papers-machine-learning.html", "anchor_text": "extremely difficult", "paragraph_index": 8}, {"url": "https://www.kdnuggets.com/2017/04/top-20-papers-machine-learning.html", "anchor_text": "identity function is minuscule", "paragraph_index": 8}, {"url": "https://www.kdnuggets.com/2017/04/top-20-papers-machine-learning.html", "anchor_text": "adding more layers", "paragraph_index": 8}, {"url": "https://www.kdnuggets.com/2017/04/top-20-papers-machine-learning.html", "anchor_text": "1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation", "paragraph_index": 10}, {"url": "https://www.kdnuggets.com/2017/04/top-20-papers-machine-learning.html", "anchor_text": "Machine Learning, NLP and Deep Learning", "paragraph_index": 11}, {"url": "https://www.kdnuggets.com/2017/04/top-20-papers-machine-learning.html", "anchor_text": "arge feed-forward layer found in Transformers", "paragraph_index": 15}, {"url": "https://www.kdnuggets.com/2017/04/top-20-papers-machine-learning.html", "anchor_text": "The Surprisal-Driven Zoneout", "paragraph_index": 16}, {"url": "https://www.kdnuggets.com/2017/04/top-20-papers-machine-learning.html", "anchor_text": "probability distribution of the next character", "paragraph_index": 17}, {"url": "https://www.iunera.com/kraken/fabric/machine-learning/", "anchor_text": "Machine Learning", "paragraph_index": 18}, {"url": "https://arxiv.org/abs/1704.04861", "anchor_text": "MobileNet", "paragraph_index": 22}, {"url": "https://arxiv.org/abs/1512.03385", "anchor_text": "ResNet", "paragraph_index": 22}, {"url": "https://www.cs.toronto.edu/~kriz/cifar.html", "anchor_text": "CIFAR-100", "paragraph_index": 24}, {"url": "http://www.robots.ox.ac.uk/~vgg/data/flowers/", "anchor_text": "Flowers", "paragraph_index": 24}, {"url": "https://openai.com", "anchor_text": "OpenAI", "paragraph_index": 25}, {"url": "https://www.iunera.com/kraken/fabric/machine-learning/", "anchor_text": "Neural Network", "paragraph_index": 25}, {"url": "https://www.iunera.com/kraken/fabric/machine-learning/", "anchor_text": "Machine Learning", "paragraph_index": 37}, {"url": "https://towardsdatascience.com/@premstroke95", "anchor_text": "Prem Kumar", "paragraph_index": 38}, {"url": "https://www.linkedin.com/in/premstrk/", "anchor_text": "LinkedIn", "paragraph_index": 38}, {"url": "https://www.premstroke.com/", "anchor_text": "https://www.premstroke.com/", "paragraph_index": 40}], "all_paragraphs": ["Important Machine Learning and Deep Learning Papers in 2021", "Machine Learning suddenly became one of the most critical domains of Computer Science and just about anything related to Artificial Intelligence.", "Every company is applying Machine Learning and developing products that take advantage of this domain to solve their problems more efficiently.", "Every year, 1000s of research papers related to Machine Learning are published in popular publications like NeurIPS, ICML, ICLR, ACL, and MLDS.", "The criteria are using citation counts from three academic sources: scholar.google.com; academic.microsoft.com; and semanticscholar.org.", "\u201cKey research papers in natural language processing, conversational AI, computer vision, reinforcement learning, and AI ethics are published yearly\u201d", "Almost all of the papers provide some level of findings in the Machine Learning field. However, three papers particularly stood, which provided some real breakthrough in the field of Machine Learning, particularly in the Neural Network domain.", "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions.", "Learning the identity function is extremely difficult as the scope of all possible combination of weights and biases is enormous, thus the chance of learning the identity function is minuscule. As seen above, adding more layers to a neural network can actually do the opposite: more layers = lower accuracy (diminishing returns).", "The paper identifies that there is one solution to this. That is by adding the inputs of the hidden layers to the output", "By implementing this idea on deeper networks, they are able to obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where they also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.", "In this paper, the Harvard grad Steven Merity introduces a state-of-the-art NLP model called as Single Headed Attention RNN or SHA-RNN. Stephen Merity, an independent researcher that is primarily focused on Machine Learning, NLP and Deep Learning. The author demonstrates by taking a simple LSTM model with SHA to achieve a state-of-the-art byte-level language model results on enwik8.", "The author\u2019s primary goal is to show that the entire field might have evolved in a different direction if we had instead been obsessed with a slightly different acronym and somewhat different results.", "The central concept of the model architecture proposed by Steven consists of a LSTM architecture with a SHA based network with three variables (Q, K and V).", "Each SHA-RNN layer contains only a single head of attention that helps with keeping the memory consumption of the model to the minimum by eliminating the need to update and maintain multiple matrices.", "The Boom layer is related strongly to the large feed-forward layer found in Transformers and other architectures. This block reduces and removes an entire matrix of parameters compared to traditional down-projection layers by using Gaussian Error Linear Unit (GeLu) multiplication to break down the input to minimize computations.", "Let\u2019s look at the actual comparison below. In 2016, The Surprisal-Driven Zoneout, a regularization method for RNN, achieved an outstanding compression score of 1.313bpc on the Hutter Prize dataset, enwiki8 which is a one-hundred-megabyte file of Wikipedia pages.", "The SHA-RNN managed to achieve even lower (bpc) compared to the model in 2016. That is impressive. Bits per character is a model proposed by Alex Graves to approximate the probability distribution of the next character given past characters.", "Further on, the Single Headed Attention RNN (SHA-RNN) managed to achieve strong state-of-the-art results with next to no hyper-parameter tuning and by using a single Titan V GPU workstation. And also, his work has undergone no intensive hyper-parameter tuning and lived entirely on a commodity desktop machine that made the author\u2019s small studio apartment a bit too warm to his liking. Now that\u2019s the passion for Machine Learning.", "In this paper, the authors systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. A new scaling method that uniformly scales all dimensions of depth, width and resolution using a simple yet highly effective compound coefficient is demonstrated in this paper.", "The papers propose a simple yet effective compound scaling method described below:", "A network that goes through dimensional scaling (width, depth or resolution) improves accuracy. But the caveat is that the model accuracy drops with larger models. Hence, it is critical to balance all three dimensions of a network (width, depth, and resolution) during CNN scaling for getting improved accuracy and efficiency.", "The compound scaling method as above consistently improves model accuracy and efficiency for scaling up existing models such as MobileNet (+1.4% Image Net accuracy), and ResNet (+0.7%), compared to conventional scaling methods", "Scaling doesn\u2019t change the layer operations; instead, they obtained their base network by doing a Neural Architecture Search (NAS) that optimizes for both accuracy and FLOPS. The scaled EfficientNet models consistently reduce parameters and FLOPS by an order of magnitude (up to 8.4x parameter reduction and up to 16x FLOPS reduction) than existing ConvNets such as ResNet-50 and DenseNet-169.", "EfficientNets also achieved state-of-the-art accuracy in 5 out of the eight datasets, such as CIFAR-100 (91.7%) and Flowers (98.8%), with an order of magnitude fewer parameters (up to 21x parameter reduction), suggesting that the EfficientNets also transfers well.", "In this paper, the authors at OpenAI defines the effective model complexity (EMC) of a training procedure of a Neural Network as the maximum number of samples on which it can achieve close to zero training error. The experiments that were conducted suggests that there is a critical interval around the interpolation threshold.", "Interpolation threshold means that the model is varied across the number of model parameters, the length of training, the amount of label noise in the distribution, and the number of train samples. The critical region is simply a small region between the under and over-parameterized risk domains.", "In most research, the bias-variance trade-off is a fundamental concept in classical statistical learning theory. The idea is that models of higher complexity have lower bias but higher variance.", "The paper defines where three scenarios where the performance of the model reduces as these regimes below becomes more significant.", "The papers demonstrate model-wise double descent occurrence across different architectures, datasets, optimizers, and training procedures.", "The paper concludes that with the usual modifications that are performed on the dataset before training (e.g., adding label noise, using data augmentation, and increasing the number of train samples), there is a shift in the peak in test error towards larger models.", "Also, in the chart above, the peak in test error occurs around the interpolation threshold, when the models are just barely large enough to fit the train set.", "In this section, the chart shows the effect of varying the number of training samples for a fixed model. Increasing the number of samples shifts the curve downwards towards lower test error but also shifts the peak error to the right.", "For a given number of optimization steps (fixed y-coordinate), test and train error exhibit model-size double descent. For a given model size as the training process proceeds, test and train error decreases, increases, and decreases again; we call this phenomenon epoch-wise double descent.", "Further on, larger models with more width parameters such as the ResNet architecture can undergo a significant double descent behavior where the test error first decreases (faster than other size models) then increases near the interpolation threshold, and then decreases again as seen below.", "For models at the interpolation threshold, there is effectively only one global model that fits the train data \u2014 and forcing it to fit even with small misspecified labels will destroy its global structure. The paper then concludes that there are no good models which both interpolate the train set and perform well on the test set.", "The characterization of these critical regimes, as stated above, provides a useful way of thinking for practitioners, hopefully, to give a breakthrough in Machine Learning soon.", "More and more papers will be published as the Machine Learning community grows every year. It is our part to read up on the new and reasonable articles to equip ourselves with the latest and state-of-the-art breakthrough in the community. Keep reading fellow enthusiast!", "If I have managed to retain your attention to this point, please leave a comment if you have any advice for this series as it would significantly increase my knowledge and improve my way of writing. Prem Kumar is a selfless learner that is passionate about the everyday data that revolves us. Please connect with me on LinkedIn mentioning this story if you would want to speak about this and the future developments that await.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Machine Learning Engineer at Wise AI (Face Recognition and eKYC). Check me out here https://www.premstroke.com/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F9b639bd0b8f0&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F3-machine-learning-research-papers-you-should-read-in-2020-9b639bd0b8f0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2F3-machine-learning-research-papers-you-should-read-in-2020-9b639bd0b8f0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F3-machine-learning-research-papers-you-should-read-in-2020-9b639bd0b8f0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2F3-machine-learning-research-papers-you-should-read-in-2020-9b639bd0b8f0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----9b639bd0b8f0--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----9b639bd0b8f0--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://premstroke95.medium.com/?source=post_page-----9b639bd0b8f0--------------------------------", "anchor_text": ""}, {"url": "https://premstroke95.medium.com/?source=post_page-----9b639bd0b8f0--------------------------------", "anchor_text": "Prem Kumar"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcb3ecdf8fc0d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F3-machine-learning-research-papers-you-should-read-in-2020-9b639bd0b8f0&user=Prem+Kumar&userId=cb3ecdf8fc0d&source=post_page-cb3ecdf8fc0d----9b639bd0b8f0---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9b639bd0b8f0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F3-machine-learning-research-papers-you-should-read-in-2020-9b639bd0b8f0&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9b639bd0b8f0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F3-machine-learning-research-papers-you-should-read-in-2020-9b639bd0b8f0&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@dandimmock?utm_source=medium&utm_medium=referral", "anchor_text": "Dan Dimmock"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://www.iunera.com/kraken/fabric/machine-learning/", "anchor_text": "Machine Learning"}, {"url": "https://www.iunera.com/kraken/fabric/machine-learning/", "anchor_text": "Machine Learning"}, {"url": "https://www.iunera.com/kraken/fabric/machine-learning/", "anchor_text": "Machine Learning"}, {"url": "https://scholar.google.com/", "anchor_text": "scholar.google.com"}, {"url": "https://academic.microsoft.com/", "anchor_text": "academic.microsoft.com"}, {"url": "https://www.semanticscholar.org/", "anchor_text": "semanticscholar.org"}, {"url": "https://www.iunera.com/kraken/fabric/machine-learning/", "anchor_text": "natural language processing, conversational AI, computer vision, reinforcement learning"}, {"url": "https://www.iunera.com/kraken/fabric/machine-learning/", "anchor_text": "Machine Learning"}, {"url": "https://www.iunera.com/kraken/fabric/machine-learning/", "anchor_text": "Machine Learning"}, {"url": "https://www.iunera.com/kraken/fabric/machine-learning/", "anchor_text": "Neural Network"}, {"url": "https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf", "anchor_text": "https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf"}, {"url": "https://www.kdnuggets.com/2017/04/top-20-papers-machine-learning.html", "anchor_text": "extremely difficult"}, {"url": "https://www.kdnuggets.com/2017/04/top-20-papers-machine-learning.html", "anchor_text": "identity function is minuscule"}, {"url": "https://www.kdnuggets.com/2017/04/top-20-papers-machine-learning.html", "anchor_text": "adding more layers"}, {"url": "https://www.kdnuggets.com/2017/04/top-20-papers-machine-learning.html", "anchor_text": "1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation"}, {"url": "https://arxiv.org/pdf/1911.11423.pdf", "anchor_text": "https://arxiv.org/pdf/1911.11423.pdf"}, {"url": "https://scholar.google.com/citations?hl=en&user=AolIi4QAAAAJ&view_op=list_works&sortby=pubdate", "anchor_text": "Steven Merity"}, {"url": "https://www.kdnuggets.com/2017/04/top-20-papers-machine-learning.html", "anchor_text": "Machine Learning, NLP and Deep Learning"}, {"url": "https://github.com/smerity/sha-rnn", "anchor_text": "Smerity/sha-rnnFor full details, see the paper Single Headed Attention RNN: Stop Thinking With Your Head. In summary, \u201cstop thinking\u2026github.com"}, {"url": "https://arxiv.org/pdf/1911.11423.pdf", "anchor_text": "https://arxiv.org/pdf/1911.11423.pdf"}, {"url": "https://www.kdnuggets.com/2017/04/top-20-papers-machine-learning.html", "anchor_text": "arge feed-forward layer found in Transformers"}, {"url": "https://www.kdnuggets.com/2017/04/top-20-papers-machine-learning.html", "anchor_text": "The Surprisal-Driven Zoneout"}, {"url": "https://www.kdnuggets.com/2017/04/top-20-papers-machine-learning.html", "anchor_text": "probability distribution of the next character"}, {"url": "https://arxiv.org/pdf/1911.11423.pdf", "anchor_text": "https://arxiv.org/pdf/1911.11423.pdf"}, {"url": "https://www.iunera.com/kraken/fabric/machine-learning/", "anchor_text": "Machine Learning"}, {"url": "https://arxiv.org/abs/1905.11946", "anchor_text": "https://arxiv.org/abs/1905.11946"}, {"url": "https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+M", "anchor_text": "Mingxing Tan"}, {"url": "https://arxiv.org/search/cs?searchtype=author&query=Le%2C+Q+V", "anchor_text": "Quoc V. Le"}, {"url": "https://github.com/narumiruna/efficientnet-pytorch", "anchor_text": "narumiruna/efficientnet-pytorchA PyTorch implementation of \u201cEfficientNet: Rethinking Model Scaling for Convolutional Neural Networks\u201d. \u2026github.com"}, {"url": "https://arxiv.org/abs/1905.11946", "anchor_text": "https://arxiv.org/abs/1905.11946"}, {"url": "https://arxiv.org/abs/1704.04861", "anchor_text": "MobileNet"}, {"url": "https://arxiv.org/abs/1512.03385", "anchor_text": "ResNet"}, {"url": "https://www.cs.toronto.edu/~kriz/cifar.html", "anchor_text": "CIFAR-100"}, {"url": "http://www.robots.ox.ac.uk/~vgg/data/flowers/", "anchor_text": "Flowers"}, {"url": "https://arxiv.org/abs/1912.02292", "anchor_text": "https://arxiv.org/abs/1912.02292"}, {"url": "https://arxiv.org/search/cs?searchtype=author&query=Nakkiran%2C+P", "anchor_text": "Preetum Nakkiran"}, {"url": "https://arxiv.org/search/cs?searchtype=author&query=Kaplun%2C+G", "anchor_text": "Gal Kaplun"}, {"url": "https://arxiv.org/search/cs?searchtype=author&query=Bansal%2C+Y", "anchor_text": "Yamini Bansal"}, {"url": "https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+T", "anchor_text": "Tristan Yang"}, {"url": "https://arxiv.org/search/cs?searchtype=author&query=Barak%2C+B", "anchor_text": "Boaz Barak"}, {"url": "https://arxiv.org/search/cs?searchtype=author&query=Sutskever%2C+I", "anchor_text": "Ilya Sutskever"}, {"url": "https://openai.com", "anchor_text": "OpenAI"}, {"url": "https://www.iunera.com/kraken/fabric/machine-learning/", "anchor_text": "Neural Network"}, {"url": "https://www.lesswrong.com/posts/FRv7ryoqtvSuqBxuT/understanding-deep-double-descent", "anchor_text": "https://www.lesswrong.com/posts/FRv7ryoqtvSuqBxuT/understanding-deep-double-descent"}, {"url": "https://openai.com/blog/deep-double-descent/", "anchor_text": "Deep Double DescentWe show that the double descent phenomenon occurs in CNNs, ResNets, and transformers: performance first improves, then\u2026openai.com"}, {"url": "https://arxiv.org/abs/1912.02292", "anchor_text": "https://arxiv.org/abs/1912.02292"}, {"url": "https://arxiv.org/abs/1912.02292", "anchor_text": "https://arxiv.org/abs/1912.02292"}, {"url": "https://arxiv.org/abs/1912.02292", "anchor_text": "https://arxiv.org/abs/1912.02292"}, {"url": "https://mltheory.org/deep.pdf", "anchor_text": "https://mltheory.org/deep.pdf"}, {"url": "https://www.lesswrong.com/posts/FRv7ryoqtvSuqBxuT/understanding-deep-double-descent", "anchor_text": "Understanding \u201cDeep Double Descent\u201d \u2014 LessWrong 2.0If you\u2019re not familiar with the double descent phenomenon, I think you should be. I consider double descent to be one\u2026www.lesswrong.com"}, {"url": "https://openai.com/blog/deep-double-descent/", "anchor_text": "Deep Double DescentWe show that the double descent phenomenon occurs in CNNs, ResNets, and transformers: performance first improves, then\u2026openai.com"}, {"url": "https://www.iunera.com/kraken/fabric/machine-learning/", "anchor_text": "Machine Learning"}, {"url": "https://towardsdatascience.com/@premstroke95", "anchor_text": "Prem Kumar"}, {"url": "https://www.linkedin.com/in/premstrk/", "anchor_text": "LinkedIn"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----9b639bd0b8f0---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----9b639bd0b8f0---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----9b639bd0b8f0---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----9b639bd0b8f0---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/programming?source=post_page-----9b639bd0b8f0---------------programming-----------------", "anchor_text": "Programming"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9b639bd0b8f0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F3-machine-learning-research-papers-you-should-read-in-2020-9b639bd0b8f0&user=Prem+Kumar&userId=cb3ecdf8fc0d&source=-----9b639bd0b8f0---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9b639bd0b8f0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F3-machine-learning-research-papers-you-should-read-in-2020-9b639bd0b8f0&user=Prem+Kumar&userId=cb3ecdf8fc0d&source=-----9b639bd0b8f0---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9b639bd0b8f0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F3-machine-learning-research-papers-you-should-read-in-2020-9b639bd0b8f0&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----9b639bd0b8f0--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F9b639bd0b8f0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F3-machine-learning-research-papers-you-should-read-in-2020-9b639bd0b8f0&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----9b639bd0b8f0---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----9b639bd0b8f0--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----9b639bd0b8f0--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----9b639bd0b8f0--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----9b639bd0b8f0--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----9b639bd0b8f0--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----9b639bd0b8f0--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----9b639bd0b8f0--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----9b639bd0b8f0--------------------------------", "anchor_text": ""}, {"url": "https://premstroke95.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://premstroke95.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Prem Kumar"}, {"url": "https://premstroke95.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "229 Followers"}, {"url": "https://www.premstroke.com/", "anchor_text": "https://www.premstroke.com/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcb3ecdf8fc0d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F3-machine-learning-research-papers-you-should-read-in-2020-9b639bd0b8f0&user=Prem+Kumar&userId=cb3ecdf8fc0d&source=post_page-cb3ecdf8fc0d--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F4c1324a026ec&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F3-machine-learning-research-papers-you-should-read-in-2020-9b639bd0b8f0&newsletterV3=cb3ecdf8fc0d&newsletterV3Id=4c1324a026ec&user=Prem+Kumar&userId=cb3ecdf8fc0d&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}