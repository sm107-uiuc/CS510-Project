{"url": "https://towardsdatascience.com/a-minimal-working-example-for-discrete-policy-gradients-in-tensorflow-2-0-d6a0d6b1a6d7", "time": 1683013408.220221, "path": "towardsdatascience.com/a-minimal-working-example-for-discrete-policy-gradients-in-tensorflow-2-0-d6a0d6b1a6d7/", "webpage": {"metadata": {"title": "A Minimal Working Example for Discrete Policy Gradients in TensorFlow 2.0 | by Wouter van Heeswijk, PhD | Towards Data Science", "h1": "A Minimal Working Example for Discrete Policy Gradients in TensorFlow 2.0", "description": "Training discrete actor networks with TensorFlow 2.0 is easy once you know how to do it, but also rather different from implementations in TensorFlow 1.0. As the 2.0 version was only released in\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/a-minimal-working-example-for-continuous-policy-gradients-in-tensorflow-2-0-d3413ec38c6b", "anchor_text": "related article", "paragraph_index": 0}, {"url": "https://www.researchgate.net/publication/344102641_Implementing_Actor_Networks_for_Discrete_Control_in_TensorFlow_20", "anchor_text": "\u2018Implementing Actor Networks for Discrete Control in TensorFlow 2.0", "paragraph_index": 10}, {"url": "http://www.github.com/woutervanheeswijk/example_discrete_control", "anchor_text": "GitHub repository", "paragraph_index": 11}, {"url": "https://www.pyimagesearch.com/2020/03/23/using-tensorflow-and-gradienttape-to-train-a-keras-model/", "anchor_text": "https://www.tensorflow.org/api_docs/python/tf/GradientTape", "paragraph_index": 13}], "all_paragraphs": ["Training discrete actor networks with TensorFlow 2.0 is easy once you know how to do it, but also rather different from implementations in TensorFlow 1.0. As the 2.0 version was only released in September 2019, most examples that circulate on the web are still designed for TensorFlow 1.0. In a related article \u2014 in which we also discuss the mathematics in more detail \u2014 we already treated the continuous case. Here, we use a simple multi-armed bandit problem to show how we can implement and update an actor network the discrete setting [1].", "We use the classical policy gradient algorithm REINFORCE in which the actor is represented by a neural network known as the actor network. In the discrete case, the network output is simply the probability of selecting each of the actions. So, if the set of actions is defined by A and the action by a \u2208 A, then the network output are the probabilities p(a), \u2200a \u2208 A. The input layer contains the state s or a feature array \u03d5(s), followed by one or more hidden layers that transform the input, with the output being the probabilities for each action that might be selected.", "The policy \u03c0 is parameterized by \u03b8, which in deep reinforcement learning represents the neural network weights. After each action we take, we observe a reward v. Computing the gradients for \u03b8 and using learning rate \u03b1, the update rule typically encountered in textbooks looks as follows [2,3]:", "When applying backpropagation updates to neural networks we must slightly modify this update rule, but the procedure follows the same lines. Although we might update the network weights manually, we typically prefer to let TensorFlow (or whatever library you use) handle the update. We only need to provide a loss function; the computer handles the calculation of gradients and other fancy tricks such as customized learning rates. In fact, the sole thing we have to do is add a minus sign, as we perform gradient descent rather than ascent. Thus, the loss function \u2014 which is known as the log loss function or cross-entropy loss function[4] \u2014 looks like this:", "Now let\u2019s move on to the actual implementation. If you have some experience with TensorFlow, you likely first compile your network withmodel.compileand then perform model.fitormodel.train_on_batchto fit the network to your data. As TensorFlow 2.0 requires a loss function to have exactly two arguments, (y_true and y_predicted) we cannot use these methods though, since we need the action, state and reward as input arguments. The GradientTapefunctionality \u2014 which did not exist in TensorFlow 1.0 [5] \u2014 conveniently solves this problem. After storing a forward pass through the actor network on a `tape' , it is able to perform automatic differentiation in a backward pass later on.", "We start by defining our cross entropy loss function:", "In the next step, we use the function .trainable_variables to retrieve the network weights. Subsequently, tape.gradient calculates all the gradients for you by simply plugging in the loss value and the trainable variables. With optimizer.apply_gradients we update the network weights using a selected optimizer. As mentioned earlier, it is crucial that the forward pass (in which we obtain the action probabilities from the network) is included in the GradientTape. The code to update the weights is as follows:", "In the multi-armed bandit problem, we are able to play several slot machines with unique pay-off properties [6]. Each machine i has a mean payoff \u03bc_i and a standard deviation \u03c3_i, which are unknown to the player. At every decision moment you play one of the machines and observe the reward. After sufficient iterations and exploration, you should be able to fairly accurately estimate the mean reward of each machine. Naturally, the optimal policy is to always play the slot machine with the highest expected payoff.", "Using Keras, we define a dense actor network. It takes a fixed state (a tensor with value 1) as input. We have two hidden layers that use five ReLUs per layer as activation functions. The network outputs the probabilities of playing each slot machine. The bias weights are initialized in such a way that each machine has equal probability at the beginning. Finally, the chosen optimizer is Adam with its default learning rate of 0.001.", "We test four settings with differing mean payoffs. For simplicity we set all standard deviations equal. The figures below show the learned probabilities for each slot machine, testing with four machines. As expected, the policy learns to play the machine(s) with the highest expected payoff. Some exploration naturally persists, especially when payoffs are close together. A bit of fine-tuning and you surely will do a lot better during your next Vegas trip.", "This article is partially based on my method paper: \u2018Implementing Actor Networks for Discrete Control in TensorFlow 2.0\u2019 [1]", "The GitHub code (implemented using Python 3.8 and TensorFlow 2.3) can be found at my GitHub repository .", "Looking to implement the continuous variant or deep Q-learning? Check out:", "[5] Rosebrock, A. (2020) Using TensorFlow and GradientTape to train a Keras model. https://www.tensorflow.org/api_docs/python/tf/GradientTape", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Assistant professor in Financial Engineering and Operations Research. Writing about reinforcement learning, optimization problems, and data science."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fd6a0d6b1a6d7&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-minimal-working-example-for-discrete-policy-gradients-in-tensorflow-2-0-d6a0d6b1a6d7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-minimal-working-example-for-discrete-policy-gradients-in-tensorflow-2-0-d6a0d6b1a6d7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-minimal-working-example-for-discrete-policy-gradients-in-tensorflow-2-0-d6a0d6b1a6d7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-minimal-working-example-for-discrete-policy-gradients-in-tensorflow-2-0-d6a0d6b1a6d7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----d6a0d6b1a6d7--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d6a0d6b1a6d7--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://wvheeswijk.medium.com/?source=post_page-----d6a0d6b1a6d7--------------------------------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=post_page-----d6a0d6b1a6d7--------------------------------", "anchor_text": "Wouter van Heeswijk, PhD"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F33f45c9ab481&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-minimal-working-example-for-discrete-policy-gradients-in-tensorflow-2-0-d6a0d6b1a6d7&user=Wouter+van+Heeswijk%2C+PhD&userId=33f45c9ab481&source=post_page-33f45c9ab481----d6a0d6b1a6d7---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd6a0d6b1a6d7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-minimal-working-example-for-discrete-policy-gradients-in-tensorflow-2-0-d6a0d6b1a6d7&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd6a0d6b1a6d7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-minimal-working-example-for-discrete-policy-gradients-in-tensorflow-2-0-d6a0d6b1a6d7&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@helloimnik", "anchor_text": "Hello I\u2019m Nik"}, {"url": "https://unsplash.com/photos/NYpOl-PJDkM", "anchor_text": "Unsplash"}, {"url": "https://towardsdatascience.com/a-minimal-working-example-for-continuous-policy-gradients-in-tensorflow-2-0-d3413ec38c6b", "anchor_text": "related article"}, {"url": "https://www.researchgate.net/publication/344102641_Implementing_Actor_Networks_for_Discrete_Control_in_TensorFlow_20", "anchor_text": "\u2018Implementing Actor Networks for Discrete Control in TensorFlow 2.0"}, {"url": "http://www.github.com/woutervanheeswijk/example_discrete_control", "anchor_text": "GitHub repository"}, {"url": "https://towardsdatascience.com/a-minimal-working-example-for-continuous-policy-gradients-in-tensorflow-2-0-d3413ec38c6b", "anchor_text": "A Minimal Working Example for Continuous Policy Gradients in TensorFlow 2.0A simple example for training Gaussian actor networks. Defining a custom loss function and applying the GradientTape\u2026towardsdatascience.com"}, {"url": "https://towardsdatascience.com/a-minimal-working-example-for-deep-q-learning-in-tensorflow-2-0-e0ca8a944d5e", "anchor_text": "A Minimal Working Example for Deep Q-Learning in TensorFlow 2.0A multi-armed bandit example to train a Q-network. The update procedure takes just a few lines of code using TensorFlowtowardsdatascience.com"}, {"url": "https://www.researchgate.net/publication/344102641_Implementing_Actor_Networks_for_Discrete_Control_in_TensorFlow_20", "anchor_text": "https://www.researchgate.net/publication/344102641_Implementing_Actor_Networks_for_Discrete_Control_in_TensorFlow_20"}, {"url": "http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-5.pdf", "anchor_text": "http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-5.pdf"}, {"url": "https://jamesmccaffrey.wordpress.com/2016/09/25/log-loss-and-cross-entropy-are-almost-the-same/", "anchor_text": "https://jamesmccaffrey.wordpress.com/2016/09/25/log-loss-and-cross-entropy-are-almost-the-same/"}, {"url": "https://www.pyimagesearch.com/2020/03/23/using-tensorflow-and-gradienttape-to-train-a-keras-model/", "anchor_text": "https://www.tensorflow.org/api_docs/python/tf/GradientTape"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----d6a0d6b1a6d7---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----d6a0d6b1a6d7---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/multi-armed-bandit?source=post_page-----d6a0d6b1a6d7---------------multi_armed_bandit-----------------", "anchor_text": "Multi Armed Bandit"}, {"url": "https://medium.com/tag/tensorflow?source=post_page-----d6a0d6b1a6d7---------------tensorflow-----------------", "anchor_text": "TensorFlow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd6a0d6b1a6d7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-minimal-working-example-for-discrete-policy-gradients-in-tensorflow-2-0-d6a0d6b1a6d7&user=Wouter+van+Heeswijk%2C+PhD&userId=33f45c9ab481&source=-----d6a0d6b1a6d7---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd6a0d6b1a6d7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-minimal-working-example-for-discrete-policy-gradients-in-tensorflow-2-0-d6a0d6b1a6d7&user=Wouter+van+Heeswijk%2C+PhD&userId=33f45c9ab481&source=-----d6a0d6b1a6d7---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd6a0d6b1a6d7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-minimal-working-example-for-discrete-policy-gradients-in-tensorflow-2-0-d6a0d6b1a6d7&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d6a0d6b1a6d7--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fd6a0d6b1a6d7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-minimal-working-example-for-discrete-policy-gradients-in-tensorflow-2-0-d6a0d6b1a6d7&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----d6a0d6b1a6d7---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----d6a0d6b1a6d7--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----d6a0d6b1a6d7--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----d6a0d6b1a6d7--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----d6a0d6b1a6d7--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----d6a0d6b1a6d7--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----d6a0d6b1a6d7--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----d6a0d6b1a6d7--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----d6a0d6b1a6d7--------------------------------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Wouter van Heeswijk, PhD"}, {"url": "https://wvheeswijk.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F33f45c9ab481&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-minimal-working-example-for-discrete-policy-gradients-in-tensorflow-2-0-d6a0d6b1a6d7&user=Wouter+van+Heeswijk%2C+PhD&userId=33f45c9ab481&source=post_page-33f45c9ab481--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fd27145dcd242&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-minimal-working-example-for-discrete-policy-gradients-in-tensorflow-2-0-d6a0d6b1a6d7&newsletterV3=33f45c9ab481&newsletterV3Id=d27145dcd242&user=Wouter+van+Heeswijk%2C+PhD&userId=33f45c9ab481&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}