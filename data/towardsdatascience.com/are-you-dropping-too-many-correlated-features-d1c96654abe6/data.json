{"url": "https://towardsdatascience.com/are-you-dropping-too-many-correlated-features-d1c96654abe6", "time": 1683013851.9831731, "path": "towardsdatascience.com/are-you-dropping-too-many-correlated-features-d1c96654abe6/", "webpage": {"metadata": {"title": "Are you dropping too many correlated features? | by Brian Pietracatella | Towards Data Science", "h1": "Are you dropping too many correlated features?", "description": "Update: The updated Python correlation function described in this article can be found in the exploretransform package on PYPI. Some commonly used correlation filtering methods have a tendency to\u2026"}, "outgoing_paragraph_urls": [{"url": "https://pypi.org/project/exploretransform/", "anchor_text": "exploretransform", "paragraph_index": 0}, {"url": "https://www.linkedin.com/in/brianpietracatella/", "anchor_text": "LinkedIn", "paragraph_index": 39}], "all_paragraphs": ["Update: The updated Python correlation function described in this article can be found in the exploretransform package on PYPI.", "Some commonly used correlation filtering methods have a tendency to drop more features than required. This problem is amplified as datasets become larger and with more pairwise correlations above a specified threshold. If we drop more variables than necessary, less information will be available potentially leading to suboptimal model performance. In this article, I will be demonstrating the shortcomings of current methods and proposing a possible solution.", "Let\u2019s look at an example of how current methods drop features that should have remained in the dataset. We will use the Boston Housing revised dataset and show examples in both R and Python.", "R: The code below uses the findCorrelation() function from the caret package to determine which columns should be dropped.", "The function determined that [ \u2018indus\u2019, \u2018nox\u2019. \u2018lstat\u2019, \u2018age\u2019, \u2018dis\u2019 ] should be dropped based on the correlation cutoff of 0.6.", "Python: Python doesn\u2019t have a built in function like findCorrelation(), so I wrote a function called corrX_orig().", "How do these functions work?A correlation matrix is created first. These numbers represent the pairwise correlations for all combinations of numeric variables.", "Then, the mean correlation for each variable is calculated. This can be accomplished by taking the mean of every row or every column since they are equivalent.", "After, the lower triangle of the matrix and the diagonal is masked. We don\u2019t need the lower triangle because the same information exists on either side of the diagonal (see matrix above). We don\u2019t require the diagonal because that represents correlations between variables and themselves (it\u2019s always 1).", "Here is pseudocode to demonstrate how the rest of the function works. I hard coded 0.6 as the correlation cutoff for this example:", "Now to the part you\u2019ve been waiting for. Why should the functions not drop age?", "Below is a table that shows variables I captured from the original function. Remember the functions told us to drop [ \u2018indus\u2019, \u2018nox\u2019. \u2018lstat\u2019, \u2018age\u2019, \u2018dis\u2019 ]. So we manually eliminate [ \u2018indus\u2019, \u2018nox\u2019, \u2018lstat\u2019, \u2018dis\u2019 ] from the table. As you can see in the table, there are no other variables left to compare against age to make a drop decision. Therefore age should not be dropped.", "Because of the sequential nature of the R and python functions, they are unable to consider the state of all the variables holistically. The decision to drop variables happens in order and is final.", "How can we prove age belongs in the dataset?", "We can remove age from the drop list resulting in [ indus, nox, dis, lstat], and then remove those four columns from the original dataset. When we rerun this subset of variables, we would expect \u2018age\u2019 as the output if it should be dropped. If we get no output, that means \u2018age\u2019 should have stayed in the set.", "As you will see below both functions provided no output. Age should have stayed.", "In this example, we have demonstrated that the commonly used correlation filter functions overstate the number of drop columns. My assertion is that this is due to the sequential nature of how each cell in the correlation matrix is evaluated and dropped.", "2. Revised: Calculate which variables to drop using res", "Below is the output of res containing the variable states along with variable definitions", "v1, v2: The row and column being analyzedv1, v2 [.mean]: The average correlation for each v1 and v2corr: The pairwise correlation between v1 and v2drop: The initial drop decision to drop higher of (v1.mean, v2.mean)", "Revised (2) Steps in drop calculation", "I would encourage the reader to manually walk through the steps below using captured variable states (res) illustration above. I\u2019ve also embedded the code for each step from the calcDrop() function. The entire function is at the end of this section.", "Step 1: all_vars_corr = All variables that exceeded the correlation cutoff of 0.6. Since our logic will capture variables meeting this condition, this will be the set of unique variables in columns v1 + v2 from the res table above.", "Step 2: poss_drop = Unique variables from the drop column. These may or may not be dropped in the end.", "Step 3: keep = Variables from v1 and v2 not in poss_drop. Essentially, any variables that aren\u2019t possibly going to be dropped are going to be kept", "Step 4: drop = Variables from v1 and v2 appearing in the same row as keep. If we know which variables to keep, then any variable paired with those will be dropped.", "Step 5: poss_drop = Remove drop variables from poss_drop. We are removing variables we know we are dropping from the list of possibles.", "Result: [\u2018age\u2019] This is the last variable left out of the possibles.", "Step 6: Subset the dataframe to include only poss_drop variables in v1 and v2. We want to see if there is any reason to drop age.", "Step7: Remove rows where drop variables are in v1 or v2 and store unique variables from drop column. Store the result in more_drop. Here we are removing rows we know contain variables we are dropping. In this smaller example, we will get an empty set since all the rows contained variables we know we are dropping. This is correct result: age is not in this set.", "Step 8: Add more_drop variables to drop and return drop", "Result: [\u2018lstat\u2019, \u2018nox\u2019, \u2018dis\u2019, \u2018indus\u2019] : more_drop doesn\u2019t contain age after manually completing the steps on the res table which is exactly what we expect", "Here is the entire calcDrop() function:", "In this example, we have demonstrated a revised pair of functions for filtering variables based on correlation. The functions work in the following way:", "Let\u2019s use the (mdrr) dataset from R\u2019s caret package which contains many correlated features. We will use the old and new functions in this section, and it will be less verbose since we\u2019ve covered the general testing routine.", "There are 9 columns identified that shouldn\u2019t have been dropped from the dataset. Let\u2019s confirm in R and Python.", "When the columns identified by python are added back to the main set in R, no columns drops are identified.", "In this article, we have demonstrated how commonly used correlation filtering methods have a tendency to unnecessarily drop features. We\u2019ve shown how the problem can be exacerbated when the data becomes larger. Although we haven\u2019t shown evidence, it\u2019s a fair assumption that unnecessary feature removal can have a negative effect on model performance", "We have also provided an efficacious solution with code, explanations and examples. In a future article, we will extend this solution adding target correlation to the filtering decision.", "Feel free to reach out to me on LinkedIn.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fd1c96654abe6&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fare-you-dropping-too-many-correlated-features-d1c96654abe6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fare-you-dropping-too-many-correlated-features-d1c96654abe6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fare-you-dropping-too-many-correlated-features-d1c96654abe6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fare-you-dropping-too-many-correlated-features-d1c96654abe6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----d1c96654abe6--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d1c96654abe6--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@bxp151?source=post_page-----d1c96654abe6--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@bxp151?source=post_page-----d1c96654abe6--------------------------------", "anchor_text": "Brian Pietracatella"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F88adf0f37b3d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fare-you-dropping-too-many-correlated-features-d1c96654abe6&user=Brian+Pietracatella&userId=88adf0f37b3d&source=post_page-88adf0f37b3d----d1c96654abe6---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd1c96654abe6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fare-you-dropping-too-many-correlated-features-d1c96654abe6&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd1c96654abe6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fare-you-dropping-too-many-correlated-features-d1c96654abe6&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@karla_rivera?utm_source=medium&utm_medium=referral", "anchor_text": "Karla Rivera"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://pypi.org/project/exploretransform/", "anchor_text": "exploretransform"}, {"url": "https://www.linkedin.com/in/brianpietracatella/", "anchor_text": "LinkedIn"}, {"url": "https://medium.com/tag/data-science?source=post_page-----d1c96654abe6---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/correlation?source=post_page-----d1c96654abe6---------------correlation-----------------", "anchor_text": "Correlation"}, {"url": "https://medium.com/tag/feature-engineering?source=post_page-----d1c96654abe6---------------feature_engineering-----------------", "anchor_text": "Feature Engineering"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----d1c96654abe6---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/analytics?source=post_page-----d1c96654abe6---------------analytics-----------------", "anchor_text": "Analytics"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd1c96654abe6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fare-you-dropping-too-many-correlated-features-d1c96654abe6&user=Brian+Pietracatella&userId=88adf0f37b3d&source=-----d1c96654abe6---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd1c96654abe6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fare-you-dropping-too-many-correlated-features-d1c96654abe6&user=Brian+Pietracatella&userId=88adf0f37b3d&source=-----d1c96654abe6---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd1c96654abe6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fare-you-dropping-too-many-correlated-features-d1c96654abe6&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d1c96654abe6--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fd1c96654abe6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fare-you-dropping-too-many-correlated-features-d1c96654abe6&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----d1c96654abe6---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----d1c96654abe6--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----d1c96654abe6--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----d1c96654abe6--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----d1c96654abe6--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----d1c96654abe6--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----d1c96654abe6--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----d1c96654abe6--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----d1c96654abe6--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@bxp151?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@bxp151?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Brian Pietracatella"}, {"url": "https://medium.com/@bxp151/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "27 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F88adf0f37b3d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fare-you-dropping-too-many-correlated-features-d1c96654abe6&user=Brian+Pietracatella&userId=88adf0f37b3d&source=post_page-88adf0f37b3d--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F88adf0f37b3d%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fare-you-dropping-too-many-correlated-features-d1c96654abe6&user=Brian+Pietracatella&userId=88adf0f37b3d&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}