{"url": "https://towardsdatascience.com/xlm-enhancing-bert-for-cross-lingual-language-model-5aeed9e6f14b", "time": 1682994987.688011, "path": "towardsdatascience.com/xlm-enhancing-bert-for-cross-lingual-language-model-5aeed9e6f14b/", "webpage": {"metadata": {"title": "XLM \u2014 Enhancing BERT for Cross-lingual Language Model | by Rani Horev | Towards Data Science", "h1": "XLM \u2014 Enhancing BERT for Cross-lingual Language Model", "description": "Attention models, and BERT in particular, have achieved promising results in Natural Language Processing, in both classification and translation tasks. A new paper by Facebook AI, named XLM, presents\u2026"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/abs/1901.07291", "anchor_text": "paper", "paragraph_index": 0}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "Transformers", "paragraph_index": 3}, {"url": "https://jalammar.github.io/illustrated-transformer/", "anchor_text": "here", "paragraph_index": 3}, {"url": "https://www.lyrn.ai/2018/11/07/explained-bert-state-of-the-art-language-model-for-nlp/", "anchor_text": "here", "paragraph_index": 5}, {"url": "https://arxiv.org/abs/1804.07755", "anchor_text": "Lample et al.", "paragraph_index": 6}, {"url": "https://github.com/facebookresearch/fastText", "anchor_text": "FastText", "paragraph_index": 6}, {"url": "https://www.lyrn.ai/2019/01/06/massively-multilingual-sentence-embeddings-for-zero-shot-transfer/#appendix-a-bpe", "anchor_text": "here", "paragraph_index": 9}, {"url": "https://www.lyrn.ai/2019/01/06/massively-multilingual-sentence-embeddings-for-zero-shot-transfer/", "anchor_text": "Artetxe et al.", "paragraph_index": 13}, {"url": "https://arxiv.org/abs/1804.07755", "anchor_text": "Lample et al.", "paragraph_index": 14}, {"url": "https://arxiv.org/abs/1812.10464", "anchor_text": "Artetxe et al.", "paragraph_index": 16}, {"url": "https://github.com/facebookresearch/XLM", "anchor_text": "here", "paragraph_index": 17}, {"url": "https://www.lyrn.ai/2019/01/16/transformer-xl-sota-language-model/", "anchor_text": "Transformer-XL", "paragraph_index": 19}, {"url": "https://www.lyrn.ai", "anchor_text": "LyrnAI", "paragraph_index": 20}], "all_paragraphs": ["Attention models, and BERT in particular, have achieved promising results in Natural Language Processing, in both classification and translation tasks. A new paper by Facebook AI, named XLM, presents an improved version of BERT to achieve state-of-the-art results in both types of tasks.", "XLM uses a known pre-processing technique (BPE) and a dual-language training mechanism with BERT in order to learn relations between words in different languages. The model outperforms other models in a cross-lingual classification task (sentence entailment in 15 languages) and significantly improves machine translation when a pre-trained model is used for initialization of the translation model.", "XLM is based on several key concepts:", "Transformers, invented in 2017, introduced an attention mechanism that processes the entire text input simultaneously to learn contextual relations between words (or sub-words). A Transformer includes two parts \u2014 an encoder that reads the text input and generates a lateral representation of it (e.g. a vector for each word), and a decoder that produces the translated text from that representation. A great in-depth review of Transformers can be found here.", "While the vanilla Transformer has only limited context of each word, i.e. only the predecessors of each word, in 2018 the BERT model took it one step forward. It uses the Transformer\u2019s encoder to learn a language model by masking (dropping) some of the words and then trying to predict them, allowing it to uses the entire context, i.e. words to the left and right of a masked word.", "Due to the concurrent processing of all tokens in the attention module, the model needs more information about the position of each token. By adding a fixed value to each token based on its position (e.g. sinusoidal function) \u2014 a step named Positional Encoding \u2014 the network can successfully learn relations between tokens. Our summary of BERT can be found here.", "In 2018, Lample et al. presented a translation model that combines Transformers and statistical phrase-based model (PBSMT). The latter is a probabilities table for pairs of phrases in different languages. An important concept in the paper is Back-Translation, in which a sentence is translated to the target language and back to the source. This concept enables using monolingual datasets, which are bigger and more common than bilingual datasets, in a supervised manner. One of the conclusions of Lample et al. is that initialization of the token embeddings is of high importance for the success of the model, especially when using Back-Translation. While the authors used a \u201csimple\u201d word embeddings using FastText, they suggest that \u201cmore powerful language models may further improve our results\u201d.", "The paper presents two innovative ideas \u2014 a new training technique of BERT for multilingual classification tasks and the use of BERT as initialization of machine translation models.", "Tough BERT was trained on over 100 languages, it wasn\u2019t optimized for multi-lingual models \u2014 most of the vocabulary isn\u2019t shared between languages and therefore the shared knowledge is limited. To overcome that, XLM modifies BERT in the following way:", "First, instead of using word or characters as the input of the model, it uses Byte-Pair Encoding (BPE) that splits the input into the most common sub-words across all languages, thereby increasing the shared vocabulary between languages. This is a common pre-processing algorithm and a summary of it can be found here.", "Second, it upgrades the BERT architecture in two manners:", "The upgraded BERT is denoted as Translation Language Modeling (TLM) while the \u201cvanilla\u201d BERT with BPE inputs is denoted as Masked Language Modeling (MLM).", "The complete model was trained by training both MLM and TLM and alternating between them.", "To assess the contribution of the model, the paper presents its results on sentence entailment task (classify relationship between sentences) using XNLI dataset that includes sentences in 15 languages. The model significantly outperforms other prominent models, such as Artetxe et al. and BERT, in all configurations \u2014 train only on English and test on all (Zero-Shot), train on translated data to English (Translate-Train), train on English, and test on translated data (Translate-Test). These results are considered state-of-the-art.", "The paper presents another contribution of BERT, and more precisely of the MLM model \u2014 as a better initialization technique for Lample et al. translation model. Instead of using FastText embeddings, the initial embeddings of the tokens are taken from a pretrained MLM and fed into the translation model.", "By using these embeddings to initialize the tokens of both the encoder and the decoder of the translation model (which uses Transformer), the translation quality improves by up to 7 BLEU as shown in the table below.", "Note: The paper also shows that training a cross-lingual language-model can be very beneficial for low-resource languages, as they can leverage data from other languages, especially similar ones mainly due to the BPE pre-processing. This conclusion is similar to the one from Artetxe et al. (Our summary can be found here).", "The models are implemented in PyTorch and can be found here, including pretrained models. The training was done with 64 Volta GPUs for the language modeling tasks and 8 GPUs for the translation tasks, though the duration isn\u2019t specified. Exact implementation details can be found in section 5.1 and 5.2 of the paper.", "As in many recent studies, the paper shows the power of language models and transfer learning, and BERT in particular, to improve performance in many NLP tasks. By using simple but smart tweaks of BERT it can outperform other cross-lingual classification models and significantly improve translations models.", "Interestingly, the translation model used in the paper and the MLM model that was used for initialization are both based on Transformer. It\u2019d be safe to assume that we\u2019ll see more combinations of this kind, such as using the new Transformer-XL for initialization.", "To stay updated with the latest Deep Learning research, subscribe to my newsletter on LyrnAI", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Learn something new every day. Currently Deep Learning :)"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F5aeed9e6f14b&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxlm-enhancing-bert-for-cross-lingual-language-model-5aeed9e6f14b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxlm-enhancing-bert-for-cross-lingual-language-model-5aeed9e6f14b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxlm-enhancing-bert-for-cross-lingual-language-model-5aeed9e6f14b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxlm-enhancing-bert-for-cross-lingual-language-model-5aeed9e6f14b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----5aeed9e6f14b--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5aeed9e6f14b--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@ranihorev?source=post_page-----5aeed9e6f14b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ranihorev?source=post_page-----5aeed9e6f14b--------------------------------", "anchor_text": "Rani Horev"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F53f9e9fdd8d8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxlm-enhancing-bert-for-cross-lingual-language-model-5aeed9e6f14b&user=Rani+Horev&userId=53f9e9fdd8d8&source=post_page-53f9e9fdd8d8----5aeed9e6f14b---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5aeed9e6f14b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxlm-enhancing-bert-for-cross-lingual-language-model-5aeed9e6f14b&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5aeed9e6f14b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxlm-enhancing-bert-for-cross-lingual-language-model-5aeed9e6f14b&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://arxiv.org/abs/1901.07291", "anchor_text": "paper"}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "Transformers"}, {"url": "https://jalammar.github.io/illustrated-transformer/", "anchor_text": "here"}, {"url": "https://www.lyrn.ai/2018/11/07/explained-bert-state-of-the-art-language-model-for-nlp/", "anchor_text": "here"}, {"url": "https://arxiv.org/abs/1804.07755", "anchor_text": "Lample et al."}, {"url": "https://github.com/facebookresearch/fastText", "anchor_text": "FastText"}, {"url": "https://www.lyrn.ai/2019/01/06/massively-multilingual-sentence-embeddings-for-zero-shot-transfer/#appendix-a-bpe", "anchor_text": "here"}, {"url": "https://arxiv.org/abs/1901.07291", "anchor_text": "XLM"}, {"url": "https://www.lyrn.ai/2019/01/06/massively-multilingual-sentence-embeddings-for-zero-shot-transfer/", "anchor_text": "Artetxe et al."}, {"url": "https://arxiv.org/abs/1901.07291", "anchor_text": "XLM"}, {"url": "https://arxiv.org/abs/1804.07755", "anchor_text": "Lample et al."}, {"url": "https://arxiv.org/abs/1901.07291", "anchor_text": "XLM"}, {"url": "https://arxiv.org/abs/1812.10464", "anchor_text": "Artetxe et al."}, {"url": "https://github.com/facebookresearch/XLM", "anchor_text": "here"}, {"url": "https://www.lyrn.ai/2019/01/16/transformer-xl-sota-language-model/", "anchor_text": "Transformer-XL"}, {"url": "https://www.lyrn.ai", "anchor_text": "LyrnAI"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----5aeed9e6f14b---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/nlp?source=post_page-----5aeed9e6f14b---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----5aeed9e6f14b---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/lyrnai?source=post_page-----5aeed9e6f14b---------------lyrnai-----------------", "anchor_text": "Lyrnai"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5aeed9e6f14b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxlm-enhancing-bert-for-cross-lingual-language-model-5aeed9e6f14b&user=Rani+Horev&userId=53f9e9fdd8d8&source=-----5aeed9e6f14b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5aeed9e6f14b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxlm-enhancing-bert-for-cross-lingual-language-model-5aeed9e6f14b&user=Rani+Horev&userId=53f9e9fdd8d8&source=-----5aeed9e6f14b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5aeed9e6f14b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxlm-enhancing-bert-for-cross-lingual-language-model-5aeed9e6f14b&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5aeed9e6f14b--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F5aeed9e6f14b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxlm-enhancing-bert-for-cross-lingual-language-model-5aeed9e6f14b&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----5aeed9e6f14b---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----5aeed9e6f14b--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----5aeed9e6f14b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----5aeed9e6f14b--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----5aeed9e6f14b--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----5aeed9e6f14b--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----5aeed9e6f14b--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----5aeed9e6f14b--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----5aeed9e6f14b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ranihorev?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ranihorev?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Rani Horev"}, {"url": "https://medium.com/@ranihorev/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.7K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F53f9e9fdd8d8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxlm-enhancing-bert-for-cross-lingual-language-model-5aeed9e6f14b&user=Rani+Horev&userId=53f9e9fdd8d8&source=post_page-53f9e9fdd8d8--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F9bc3579798b7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxlm-enhancing-bert-for-cross-lingual-language-model-5aeed9e6f14b&newsletterV3=53f9e9fdd8d8&newsletterV3Id=9bc3579798b7&user=Rani+Horev&userId=53f9e9fdd8d8&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}