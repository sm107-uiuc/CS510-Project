{"url": "https://towardsdatascience.com/binary-classification-example-4190fcfe4a3c", "time": 1683007981.405896, "path": "towardsdatascience.com/binary-classification-example-4190fcfe4a3c/", "webpage": {"metadata": {"title": "Binary Classification Example. Predicting Opioid Use | by Kamil Mysiak | Towards Data Science", "h1": "Binary Classification Example", "description": "This global crisis has impacted all of our lives in one way or another but this is a perfect opportunity to hone your craft. I happened to renew my Coursera account specifically trying my hand at\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/@kamilmysiak/classification-metrics-thresholds-explained-caff18ad2747", "anchor_text": "LINK", "paragraph_index": 42}, {"url": "https://github.com/Kmysiak/Opioid_Classification", "anchor_text": "LINK", "paragraph_index": 60}, {"url": "https://www.linkedin.com/in/kamil-mysiak-b789a614/", "anchor_text": "https://www.linkedin.com/in/kamil-mysiak-b789a614/", "paragraph_index": 62}], "all_paragraphs": ["This global crisis has impacted all of our lives in one way or another but this is a perfect opportunity to hone your craft. I happened to renew my Coursera account specifically trying my hand at network analysis. Cryptocurrency/blockchain medium articles have become part of my daily routine. Finally, I wanted to post an ML classification example to help those looking for a detailed \u2018Beginning to End\u2019 use case and call for constructive feedback from the community at large. This journey will be long and detailed, I hope you are ready for the ride.", "We will utilize an insurance dataset which outlines a series of patient centered features with the ultimate goal to correctly predict whether or not opioid abuse has occurred.", "ClaimID Unique: Identifier for a claimAccident DateID: Number of days since the accident occurred from an arbitrary dateClaim Setup DateID: Number of days since the Resolution Manager sets up the claim from an arbitrary dateReport To DateID: Number of days since the employer notifies insurance of a claim from an arbitrary dateEmployer Notification DateID: Number of days since the claimant notifies employer of an injury from an arbitrary dateBenefits State: The jurisdiction whose benefits are applied to a claimAccident State: State in which the accident occurredIndustry ID: Broad industry classification categoriesClaimant Age: Age of the injured worker Claimant Sex: Sex of the injured worker Claimant State: State in which the claimant residesClaimant Marital Status: Marital status of the injured worker Number Dependents: Number of dependents the claimant hasWeekly Wage: An average of the claimant\u2019s weekly wages as of the injury date.Employment Status Flag: F \u2014 Regular full-time employee P \u2014 Part-time employee U \u2014 Unemployed S \u2014 On strike D \u2014 Disabled R \u2014 Retired O \u2014 Other L \u2014 Seasonal worker V \u2014 Volunteer worker A \u2014 Apprenticeship full-time B \u2014 Apprenticeship part-time C \u2014 Piece workerRTW Restriction Flag: A Y/N flag, used to indicate whether the employees responsibilities upon returning to work were limited as a result of his/her illness or injury.Max Medical Improvement DateID: DateID of Maximum Medical Improvement, after which further recovery from or lasting improvements to an injury or disease can no longer be anticipated based on reasonable medical probability.Percent Impairment: Indicates the percentage of anatomic or functional abnormality or loss, for the body as a whole, which resulted from the injury and exists after the date of maximum medical improvementPost Injury Weekly Wage: The weekly wage of the claimant after returning to work, post-injury, and/or the claim is closed.NCCI Job Code: A code that is established to identify and categorize jobs for workers\u2019 compensation.Surgery Flag: Indicates if the claimant\u2019s injury will require or did require surgeryDisability Status: \u2014 Temporary Total Disability (TTD) \u2014 Temporary Partial Disability (TPD) \u2014 Permanent Partial Disability (PPD) \u2014 Permanent Total Disability (PTD)SIC Group: Standard Industry Classification group for the clientNCCI BINatureOfLossDescription: Description of the end result of the bodily injury (BI) loss occurrenceAccident Source Code: A code identifying the object or source which inflicted the injury or damage.Accident Type Group: A code identifying the general action which occurred resulting in the lossNeurology Payment Flag: Indicates if there were any payments made for diagnosis and treatment of disorders of the nervous system without surgical interventionNeurosurgery Payment Flag: Indicates if there were any payments made for services by physicians specializing in the diagnosis and treatment of disorders of the nervous system, including surgical intervention if neededDentist Payment Flag: Indicates if there were any payments made for prevention, diagnosis, and treatment of diseases of the teeth and gumsOrthopedic Surgery Payment Flag: Indicates if there were any payments made for surgery dealing with the skeletal system and preservation and restoration of its articulations and structures.Psychiatry Payment Flag: Indicates if there were any payments made for treatment of mental, emotional, or behavioral disorders.Hand Surgery Payment Flag: Indicates if there were any payments made for surgery only addressing one or both hands.Optometrist Payment Flag: Indicates if there were any payments made to specialists who examine the eye for defects and faults of refraction and prescribe correctional lenses or exercises but not drugs or surgeryPodiatry Payment Flag: Indicates if there were any payments made for services from a specialist concerned with the care of the foot, including its anatomy, medical and surgical treatment, and its diseases.HCPCS A Codes \u2014 HCPCS Z Codes: Count of the number of HCPCS codes that appear on the claim within each respective code groupICD Group 1 \u2014 ICD Group 21: Count of the number of ICD codes that appear on the claim w/in each respective code groupCount of the number of codes on the claim \u2014 CPT Category \u2014 Anesthesia \u2014 CPT Category \u2014 Eval_Mgmt \u2014 CPT Category \u2014 Medicine \u2014 CPT Category \u2014 Path_Lab \u2014 CPT Category \u2014 Radiology \u2014 CPT Category \u2014 SurgeryCount of the number of NDC codes on the claim within each respective code class \u2014 NDC Class \u2014 Benzo \u2014 NDC Class \u2014 Misc (Zolpidem) \u2014 NDC Class \u2014 Muscle Relaxants \u2014 NDC Class \u2014 StimulantsOpioids Used: A True (1) or False (0) indicator for whether or not the claimant abused an opioid", "Let us begin with importing all the required libraries along with our dataset.", "Our dataset contains just over 16,000 observations along with 92 features including the target (ie. Opiods Used). We also have a variety of feature types including integers, floats, strings, booleans and mixed type.", "Before we tackle missing data, outliers or cardinality, let\u2019s see if we can quickly delete any features to simplify our further analysis.", "As we scroll through the output we can see the number of unique values for each feature along with the total length of the entire dataset. Features which have a similar number of unique values as the total length of the dataframe can be removed as they don\u2019t provide much predictive ability (ie. variance). \u2018ClaimID\u2019 is the only feature which meet this criteria and can be removed.", "Next, we can examine the correlations between our numerical features and delete features which are very highly correlated. It is up to you to determine what is considered \u201chighly correlated\u201d but in this case we will select a correlation of 90 and above. Notice in the code we have constructed a correlation matrix and converted the correlations to their absolute values in order to deal with negative correlations. \u2018Claim Setup DateID\u2019, \u2018Report to DateID\u2019, \u2018Employer Notification DateID\u2019 and \u2018Max Medical Improvement DateID\u2019 can be removed.", "We can examine the percentage of missing values for the remaining features and remove any features with an excessive missing data. One feature \u2018Accident Source Code\u2019 has over 50% of missing values but that\u2019s not enough to warrant deletion.", "Examining \u2018Claimant State\u2019, \u2018Accident State\u2019 and \u2018Benefits State\u2019 we find that the vast majority of the values are the same. We will keep \u2018Benefits State\u2019 as it contains the least amount of missing values.", "When we examine the unique values for each feature we can start to see some discrepancies which require our attention. First, we notice blank or null values which have not been converted to Np.nan. Next, we are seeing the value of \u2018X\u2019 for many features and this seems like a recording discrepancy where the individual recording the data recorded missing values with an \u2018X\u2019. Finally, the feature named \u2018Accident Type Group\u2019 is what we call a mixed-type which because it contains both string and numerical values. Let\u2019s separate the string and numerical values into their own features and delete the original \u2018Accident Type Group\u2019 feature.", "Let\u2019s now turn our attention to cardinality or the number of unique values/categories for each feature. Continuous feature such as \u2018Weekly Wage\u2019 will no doubt have hundreds or even thousands of unique categories. Nominal and discrete features (ie. gender and number of dependents) will have a much smaller number of categories. The goal of this exercise is to determine if any categories hold the majority (90%+) of the values. If a feature contains one or two categories which hold 90%+ of the values there simply isn\u2019t enough variability in the data to retain the feature. It is ultimately up to you to determine the cut off but we feel 90% or more is a safe assumption.", "Now that we have successfully eliminated many of the features due to high correlations, duplicate values and lack of variability we can focus on examining feature characteristics and deciding how to tackle each problem.", "You will notice that in this section we are simply identifying the issue and making a mental note. We wouldn\u2019t be actually applying the discussed changes until the end of the notebook into a feature engineering pipeline.", "From our previous look at missing values, we discovered that only one feature contained more than 50% of missing values and the vast majority did not contain any missing data. That said, we do have a number of features which do contain missing data and we need to determine how we will deal with this issue as many ML algorithms require full clean datasets. This is an extensive topic and we do not wish to cover the intricacies in this blog but the reader should get well acquainted with this topic. First, one must orient oneself with the various types of missing data such as \u2018Missing Completely at Random\u2019, \u2018Missing at Random\u2019 and \u2018Missing not at Random\u2019. These topics help to pinpoint how and when you should deal with your missing data. Additionally, a further reading into imputation techniques such as Mean/Median/Mode, Arbitrary Value Imputation, Adding Missing Data Indicator, Random Sample Imputation, ML imputation, etc. will provide a great overview.", "We have 9 features with missing data. For features with less than 5% of missing values (ie. Claimant Sex, Claimant Marital Status, Employment Status Flag, RTW Restriction Flag) we will replace the missing values with the mode of their distributions. Due to their low percentage of missing values, a mode imputation wouldn\u2019t change the distribution by very much. \u2018Accident DateID\u2019 is our only continuous feature with missing data and we\u2019ll impute missing values with an arbitrary number of -99999. All other features are categorical in nature and since they have more than 5% of missing values we\u2019ll impute the missing values with the string \u2018missing\u2019. This ultimately wouldn\u2019t change the distribution and only add a new category to their distributions.", "In the previous section, we looked at cardinality in order to remove features with low variability (ie. features with categories which contained the majority of the data). We need to examine cardinality a bit deeper and identify \u2018rare\u2019 categories. In other words, which categories contain only a very small percentage of the data (=<1%). We will aggregate all the categories into a \u2018rare\u2019 category thereby, reducing the cardinality of each feature and simplifying the model. This method will be very helpful when we discuss encoding categorical and discrete features.", "As an example, the feature \u2018Employment Status Flag\u2019 currently has 13 categories (including np.nan) but as you can see the \u201cF = Full-Time\u201d and \u201cP=Part-Time\u201d categories make up almost 96% of the data. All other categories barely show up 0.5% of the time and they will all be aggregated as \u2018rare\u2019 categories. Any categorical or discrete feature with categories occurring less than 1% of the time will have those categories encoded as \u2018rare\u2019.", "The dataset only contains two continuous features \u2018Accident DateID\u2019 and \u2018Weekly Wage\u2019. We need to determine whether nor not these features contain skewed distributions and if they contain any outliers.", "Both features certainly maintain skewed distributions but only \u2018Weekly Wage\u2019 contains any outliers. ML algorithms have certain assumptions about the data which we need to follow in other to increase their predictive ability. For example, linear regression assumes the relationship between your predictors and target is linear (linearity). It also assumes there are no outliers in the data. It has a particularly difficult time with highly correlated features (multicollinearity). Finally, it assumes your features are normally distributed.", "Normally distributed features follow a Gaussian distribution which you probably remember from your high school statistics course resembles a bell shape. As you can see neither \u2018Accident DateID\u2019 nor \u2018Weekly Wage\u2019 are normally distributed. There are several commonly used methods to fix skewed distributions such as log, reciprocal, square root, box-cox and yeo-johnson transformations.", "The corrected skews of \u2018Accident DateID\u2019 are as follows:", "We can see the initial skew of \u2018Accident DateID\u2019 was 0.137 which technically speaking isn\u2019t very skewed as a normal distribution has a skew of zero (0). That said, applying the reciprocal transformation adjusted our skew to zero (0).", "The corrected skews of \u2018Weekly Wage\u2019 are as follows:", "\u2018Weekly Wage\u2019 had a much larger initial skew at 2.56 but a square root transformation brought the skew down significantly (0.40).", "Now that we have fixed the skewness, let\u2019s address the outliers located inside \u2018Weekly Wage_sqrt\u2019 as we have dropped the original feature. Since \u2018Weekly Wage_sqrt\u2019 is normally distributed we can use the \u20183 Standard Deviations from the Mean\u2019 rule to identify the outliers. If your distribution was skewed you would be better off calculating the quantiles and then the IQR to identify your upper and lower boundaries.", "It is important to note continuous features such as \u2018Accident DateID\u2019 and \u2018Weekly Wage_sqrt\u2019 can often benefit from discretization or binning. Discretization entails cutting the feature values into groups or bins. This method is also a valid way to deal with outliers as they are typically brought closer to a mean of the distribution. This will ultimately change the feature from continuous to discrete as the end result will be the number of observations in each bin (ie. 0\u20131000, 1000\u20132000, 2000\u20135000, etc.).", "The upper boundary was 51.146 and the lower boundary was -0.763. In other words, any value which falls outside either of these boundaries will be considered as an outlier. Notice that we used the three (3) standard deviation rule to determine outliers. We could have changed this value to 2 and our boundaries would have shrunk resulting more outliers.", "Our target is \u2018Opiods Used\u2019 and as with most classification problems the false class tends to be the majority in terms of sheer numbers. As you can see above, almost 90% of all the cases are False or did not abuse opioids. Some ML algorithms such as decision trees tend to bias their predictions towards the majority class (ie. False in our case). There are a number of techniques we can use to solve this issue. Oversampling is a technique which attempts to add random copies of the minority class to the dataset until the imbalance is eliminated. Undersampling is the opposite of oversampling as it entails removing majority class observations. The major drawback is the idea of removing data which can lead to underfitting of your model. Last but not least, synthetic minority oversampling technique (SMOTE) uses the KNN algorithm to generate new observations to eliminate the imbalance. We\u2019ll use the SMOTE technique in this use case to generate new (synthetic) observations.", "It is important to note any technique used should only be used to generate or eliminate observations in the training set.", "We need to split our data into train and test datasets but first let\u2019s convert the our features to proper format in order to coincide with our pipeline\u2019s requirements.", "In order to simplify the task of processing the data for missing data, rare values, cardinality, and encoding we will utilize Scikit-Learn\u2019s make_pipeline library. A pipeline allows us to apply multiple processes into a single piece of code which will run each processes in series, one after another. Using a pipeline makes our code much easier to understand and much more reproducible.", "Our data processing pipeline makes extensive use of the \u201cFeature-Engine\u201d library. We could have used Scikit-Learn to accomplish these tasks but feature-engine has certain advantages which we would like to point out. First, being built on top of scikit-learn, pandas, Numpy and SciPy, feature-engine is able to return pandas dataframes instead of numpy arrays like scikit-learn. Secondly, feature-engine transformers are able to learn and store training parameters and transform your test data using the stored parameters.", "But enough about feature-engine, let\u2019s discuss the pipeline in more detail. It is important to understand the steps in a pipeline are run in series, starting with the top transformer. Often students make the mistake of applying the first step which ultimately changes the structure of the data or the name of the feature which is not recognized by the second stop.", "The first transformer \u201cArbitraryNumberImputer\u201d imputes continuous features with more than 5% of missing data with the value -99999. The second transformer, \u201cCategoricalVariableImputer\u201d, imputes categorical data with more than 5% of missing data with the string value of \u2018Missing\u201d. The third transformer, \u201cFrequentCategoryImputer\u201d, imputes categorical data with less than 5% of missing data with the mode of the feature. The fourth transformer, \u201cRareLabelCategoricalEncoder\u201d, encodes categorical and discrete feature observations which appear less than 1% of the time into a new category named \u201crare\u201d.", "The fifth transformer, \u201cOneHotCategoricalEncoder\u201d, transforms each unique value for each categorical feature into binary form stored in a new feature. For example, \u201cGender\u201d has the values of \u201cM\u201d, \u201cF\u201d and \u201cU\u201d. One hot encoding will produce three (or two \u201ck-1\u201d depending on your settings) new features (ie. Gender_M, Gender_F, Gender_U). If the observation has a value of \u201cM\u201d under the original \u201cGender\u201d feature then \u201cGender_M\u201d will have the value of 1 and \u201cGender_F\u201d and \u201cGender_U\u201d will have the values of 0. As this method greatly expands the feature space, now you understand why it was important to bin rare observations (<1%) as \u201crare\u201d.", "The final transformer, \u201cOridinalCategoricalEncoder\u201d, is specifically used to encode discrete features in order to maintain their ordered relationship with the target feature. Utilizing this encoder will generate missing values or throw an error for categories present in the test set which were not encoded in the training set. This is yet another reason to handle rare values before you encode ordinal/discrete features. To better understand this ordinal encoder let\u2019s examine the \u201cClaimant age\u201d feature. We have three potential values of \u201cF\u201d, \u201cM\u201d and \u201cU\u201d. Let\u2019s assume the average opioid abuse for \u201cF\u201d is 10%, \u201cM\u201d is 25% and \u201cU\u201d is 5%. The encoder will encode \u201cM\u201d as 1, \u201cF\u201d as 2, and \u201cU\u201d as 3 according to the magnitude of their average opioid abuse.", "Notice the expanded feature space to 155 features", "Next, fit the pipeline onto X_train and y_train and transform X_train and X_test. Notice that our feature space has increased greatly to 155 features, this is due to the one-hot encoder we used on categorical features.", "Finally, we have to scale the features in order to have all their values on the same range or magnitude. This step has to be done as some ML classifiers use Euclidean distance and features with higher magnitudes or range would have more influence on the prediction. For example temperature, 32 degree Fahrenheit is the same as 273.15 degrees Kelvin and if we were to use both features in a model Kelvin would have more weight or influence the prediction.", "We will compare four different classifiers on their relative recall score. We are using recall as we want to minimize the number of false negatives (ie. abused opioids but predicted not to abuse). First, we want to establish a baseline to which we can compare against additional iterations of the classifiers to determine relative improvement. The baseline model includes the entire feature space with an imbalanced target. To properly evaluate our classifiers we\u2019ll use 5-fold stratified cross-validation applied only to the training dataset thereby drastically reducing data leakage. In other words, each classifier will be trained and tested 5 times on five unique splits of training data. Five unique recall scores will be calculated for each classifier and averaged together to produce the final recall score. The classifiers will not \u201csee\u201d any of the test data during training. Finally, each classifier will be tested on the held-out test dataset to determine generalizability and overfitting.", "LoLogistic regression, random forest, and gradient boosting classifier have achieved great overall accuracy. However, naive bayes managed to achieve the highest recall as it only had 331 false-negative predictions.", "If you wish to read more about classification metrics (LINK)", "The imbalanced target is imposing bias onto our prediction. As expected logistic regression improved greatly as the algorithm performs significantly better with balanced targets.", "A few words on SMOTE and its methodology. If you recall from our initial examination of the dataset the target variable was imbalanced. We had significantly more observations which did not result in opioid abuse (89%) compared to those which resulted in opioid abuse (10%). We also decided to use the SMOTE method as it creates new synthetic observations of the minority class instead of copying existing observations. SMOTE uses KNN (typically k=5) where a random observation from the minority class is selected and k of the nearest neighbors are found. Then, one of the k neighbors is randomly selected and a synthetic sample is built from a randomly selected point between the original observation and the randomly selected neighbor.", "Modern datasets in areas such as natural language processing and IoT are typically highly dimensional. It is not uncommon to see thousands even millions of features. Knowing how to narrow down the features to a selected few not only improves our chances of finding a generalizable model but also decreases our reliance on expensive computational power. By accurately reducing the number of features/dimensions in our data we are ultimately removing unnecessary noise from our data.", "It is important to note feature selection consists of not only the reduction in the feature space but also feature creation. Understanding how to find trends in your dataset and relationships among features (ie. polynomial features) takes many years of practice but pays big dividends in predictive power. There are whole college courses dedicated to feature selection/engineering but those interested more in the topic please research Filter, Wrapper, and Embedded methods as an introduction.", "RandomForestClassifier from Scikit-Learn has a \u201cfeature_importances_\u201d attribute which is used to determine the relative importance of each feature in your dataset. As random forest tends to perform better under a balanced target we are going to use the SMOTE balanced X_train_std_sm and y_train_sm datasets.", "We were able to eliminate most of the original features down to just 30 which accounts for 91% of the performance variance. Any additional features would only add a very small additional predictive power.", "The reduction in features resulted in a slight decrease in recall performance. This is expected as using only 30 features accounted for 91% of the performance variance. Increasing the number of features would have certainly increased the recall. Only naive bayes was not affected by the reduction in features. Next, the imbalance in our dataset also affected the recall performance of the classifiers. In general, most of the classifiers improved their performance once the imbalance was correct. Logistic regression was affected by the imbalance the most.", "In terms of overall classifier performance, it must be said all classifiers performed best with a reduction in features along with a balanced target. Of course one could argue naive bayes had performed the best as it managed to achieve the best test recall (0.949) but I would argue it is logistic regression outperformed the field. It achieved very similar recall 0.945 compared to 0.949 of naive bayes which only accounted for an increase in the false negatives by 14. What is more impressive is the fact it had 2,276 additional correct true negative predictions compared to naive bayes. In the next section, we\u2019ll attempt hyperparameter tuning to see if we can increase the classification recall of the logistic regression model.", "Think of hyperparameters are \u2018tuning\u2019 knobs. Imagine you are editing a picture to achieve a certain effect. You can tune the picture using \u2018knobs\u2019 such as exposure, highlights, shadows, contrast, brightness, saturation, warmth, tint, etc. Well, hyperparameters are the same type of \u2018knobs\u2019 but for classifiers.", "Logistic regression does not have many hyperparameters to tune besides the solver, penalty and C.", "The solvers try to optimize the parameter weights or thetas which will elicit the least error or cost function. Sklearn comes with several solvers: newton-cg, lbfgs, liblinear.", "The penalty parameter is also called \u201cregularization\u201d and its purpose is to help a model avoid overfitting the training data thereby producing a more generalizable model. It does this by \u201cpenalizing\u201d features which are considered noise or contribute very little to the model.", "Finally, the \u201cC\u201d parameter determines the strength of the regularization penalty. The larger the C parameter the less regularization and the more complex the model becomes and over-fitting increases. The smaller the C parameter the more regularization is applied and under-fitting increases.", "To reduce training times and the number of warnings we\u2019ll focus on tuning the \u201cpenalty\u201d and \u201cC\u201d parameters. Understanding not only what each hyperparameter does but also how the parameters interact with each other is critical to their tuning. Furthermore, the tuning process is often iterative as we begin with wide ranges for each parameter and then begin to narrow our range until a specific parameter value is chosen.", "GridSearchCV applies an exhaustive approach as it considers all combinations of supplied parameters. As balancing the target produced the best recall during classifier evaluation, we opted to include it into our pipeline along with the log regression classifier which was passed into gridsearchcv. This way each cross-validation training/test split was balanced only with its data. In other words, neither SMOTE nor the classifier suffered from data leakage during cross-validation.", "Unfortunately, we did not increase our training recall but we were able to increase our test recall from 0.945 (log regression w/RF features and SMOTE) to 0.954. Furthermore, we decreased our false-negative count by 42. Our false positive count increased by 269 but once again it is better to been predicted to use opioids when in reality you\u2019re not.", "Understanding the full nature of a data science classification problem is key in your maturity as a data scientist. I hope you found this tutorial informative and easily understood. I welcome any feedback and suggestions as we are all just honing our craft.", "All the code can be found on my GitHub. LINK", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Scientist | I/O Psychologist | Motorcycle Enthusiast | On a Search for my Personal Legend/ https://www.linkedin.com/in/kamil-mysiak-b789a614/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F4190fcfe4a3c&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbinary-classification-example-4190fcfe4a3c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbinary-classification-example-4190fcfe4a3c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbinary-classification-example-4190fcfe4a3c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbinary-classification-example-4190fcfe4a3c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----4190fcfe4a3c--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4190fcfe4a3c--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://kamilmysiak.medium.com/?source=post_page-----4190fcfe4a3c--------------------------------", "anchor_text": ""}, {"url": "https://kamilmysiak.medium.com/?source=post_page-----4190fcfe4a3c--------------------------------", "anchor_text": "Kamil Mysiak"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F98fe4fecb558&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbinary-classification-example-4190fcfe4a3c&user=Kamil+Mysiak&userId=98fe4fecb558&source=post_page-98fe4fecb558----4190fcfe4a3c---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4190fcfe4a3c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbinary-classification-example-4190fcfe4a3c&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4190fcfe4a3c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbinary-classification-example-4190fcfe4a3c&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@rcrazy?utm_source=medium&utm_medium=referral", "anchor_text": "Ricardo Rocha"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://medium.com/@kamilmysiak/classification-metrics-thresholds-explained-caff18ad2747", "anchor_text": "LINK"}, {"url": "https://github.com/Kmysiak/Opioid_Classification", "anchor_text": "LINK"}, {"url": "https://medium.com/tag/classification?source=post_page-----4190fcfe4a3c---------------classification-----------------", "anchor_text": "Classification"}, {"url": "https://medium.com/tag/classification-algorithms?source=post_page-----4190fcfe4a3c---------------classification_algorithms-----------------", "anchor_text": "Classification Algorithms"}, {"url": "https://medium.com/tag/auc-roc-curve?source=post_page-----4190fcfe4a3c---------------auc_roc_curve-----------------", "anchor_text": "Auc Roc Curve"}, {"url": "https://medium.com/tag/hyperparameter-tuning?source=post_page-----4190fcfe4a3c---------------hyperparameter_tuning-----------------", "anchor_text": "Hyperparameter Tuning"}, {"url": "https://medium.com/tag/precision-and-recall?source=post_page-----4190fcfe4a3c---------------precision_and_recall-----------------", "anchor_text": "Precision And Recall"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4190fcfe4a3c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbinary-classification-example-4190fcfe4a3c&user=Kamil+Mysiak&userId=98fe4fecb558&source=-----4190fcfe4a3c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4190fcfe4a3c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbinary-classification-example-4190fcfe4a3c&user=Kamil+Mysiak&userId=98fe4fecb558&source=-----4190fcfe4a3c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4190fcfe4a3c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbinary-classification-example-4190fcfe4a3c&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4190fcfe4a3c--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F4190fcfe4a3c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbinary-classification-example-4190fcfe4a3c&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----4190fcfe4a3c---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----4190fcfe4a3c--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----4190fcfe4a3c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----4190fcfe4a3c--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----4190fcfe4a3c--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----4190fcfe4a3c--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----4190fcfe4a3c--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----4190fcfe4a3c--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----4190fcfe4a3c--------------------------------", "anchor_text": ""}, {"url": "https://kamilmysiak.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://kamilmysiak.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Kamil Mysiak"}, {"url": "https://kamilmysiak.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "348 Followers"}, {"url": "https://www.linkedin.com/in/kamil-mysiak-b789a614/", "anchor_text": "https://www.linkedin.com/in/kamil-mysiak-b789a614/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F98fe4fecb558&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbinary-classification-example-4190fcfe4a3c&user=Kamil+Mysiak&userId=98fe4fecb558&source=post_page-98fe4fecb558--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F63448b4832be&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbinary-classification-example-4190fcfe4a3c&newsletterV3=98fe4fecb558&newsletterV3Id=63448b4832be&user=Kamil+Mysiak&userId=98fe4fecb558&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}