{"url": "https://towardsdatascience.com/trends-in-model-pre-training-for-natural-language-understanding-d34424cf5715", "time": 1683012728.9702551, "path": "towardsdatascience.com/trends-in-model-pre-training-for-natural-language-understanding-d34424cf5715/", "webpage": {"metadata": {"title": "Trends in Model Pre-training for Natural Language Understanding | by Julia Turc | Towards Data Science", "h1": "Trends in Model Pre-training for Natural Language Understanding", "description": "Pre-training is now ubiquitous in natural language understanding (NLU). Regardless of the target application (e.g., sentiment analysis, question answering, or machine translation), models are first\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.theverge.com/2019/12/11/20993407/ai-language-models-muppets-sesame-street-muppetware-elmo-bert-ernie", "anchor_text": "Muppet series", "paragraph_index": 4}, {"url": "https://arxiv.org/abs/2002.08909", "anchor_text": "REALM", "paragraph_index": 8}, {"url": "https://arxiv.org/pdf/2006.15020.pdf", "anchor_text": "MARGE", "paragraph_index": 8}, {"url": "http://storia.ai", "anchor_text": "storia.ai", "paragraph_index": 16}, {"url": "http://twitter.com/juliarturc", "anchor_text": "twitter.com/juliarturc", "paragraph_index": 16}], "all_paragraphs": ["Pre-training is now ubiquitous in natural language understanding (NLU). Regardless of the target application (e.g., sentiment analysis, question answering, or machine translation), models are first pre-trained on vast amounts of free-form text, often hundreds of gigabytes. The intention is to initialize models with general linguistic knowledge that can be later leveraged in multiple contexts. A pre-trained model that is linguistically well-versed can then be fine-tuned on a much smaller dataset to perform the target application.", "While we\u2019ve emphatically determined the usefulness of exposing a model to endless Internet blabber, it\u2019s still not obvious how the model should interact with it. There are two requirements regarding this interaction. First, the data needs to be gamified into a task: during each training step, the model attempts to solve the task, receives feedback on its performance, then adjusts its parameters accordingly. Second, because of the data magnitude, the task needs to be unsupervised: the correct predictions should already be present in the raw data, without the need for human annotation.", "Traditionally, pre-training tasks revolved around predicting tokens that were artificially removed from a text document. Despite their simplicity (or maybe because of it), these techniques have been dominating the field since the inception of pre-training, with truly remarkable results. Yet we are probably only scratching the surface. There must be a lot of unleashed potential in datasets that exceed by several orders of magnitude the number of tokens we are exposed to in our childhood. Innovative ideas have sprouted in recent research proposing more elaborate pre-training tasks like document retrieval and paraphrasing.", "A simple yet effective technique is next-token prediction: given a text document, a model is trained to traverse it left-to-right and predict each token along the way, based on what it has read so far. This task is also known as language modeling (LM). The vanilla unidirectional formulation of language modeling was adopted by the now famous GPT models from OpenAI, whose massive computational scale compensate for the simplicity of the training objective. The GPT-3 model [1], with 175 billion parameters trained on 400 billion tokens, records unprecedented few-shot performance: in order to solve real-world tasks, it requires no or very little fine-tuning after pre-training.", "Before the pre-training + fine-tuning paradigm started dominating NLU, pseudo-bidirectional language models had their moment of glory; instead of a single pass, they would traverse the input text twice (left-to-right and right-to-left) to give the illusion of bidirectional processing. For instance, ELMo [2], which set the trend for the Muppet series, used this technique to produce continuous input representations that would be later fed into an end-task model (in other words, only the input embeddings were pre-trained rather the entire network stack). Despite their popularity at the time, pseudo-bidirectional LMs never resurged in the context of pre-training + fine-tuning.", "In the past two years, the de facto building block for NLU has been Google\u2019s BERT [3], pre-trained with two objectives: masked language modeling (MLM) and next-sentence prediction. During MLM training, the model is exposed to text documents in which 15% of the tokens were replaced with a special [MASK] token; its task is to recover these elisions. Access to context on both sides of the masked token encourages the model to process text in both directions. It is posited that MLM encourages models to emulate human reasoning more closely than unidirectional and pseudo-bidirectional LMs.", "The main disadvantage of MLM over its next-word prediction predecessor is reduced sample efficiency, since only 15% of the tokens are predicted. Additionally, the [MASK] tokens introduce a discrepancy between the inputs observed during the pre-training and fine-tuning stages, since downstream tasks do not mask their inputs. XLNet [4] proposed a variation of MLM that addressed these issues, but its adoption has remained relatively limited compared to BERT.", "Despite their success, token-prediction objectives are not flawless. Their major criticism is that they focus exclusively on linguistic form: models learn the characteristics of coherent language without necessarily associating meaning to it. Generative models like GPT-2 [5] are known to hallucinate \u2014 that is, to produce convincing factual-looking text that is not anchored in reality. This is perhaps the reason why OpenAI is reluctant to open-sourcing their models.", "Recent work has made progress towards grounding natural language into the reality of our world. Research projects such as REALM (Retrieval-Augmented Language Model Pre-training) [6] and MARGE (Multilingual Autoencoder that Retrieves and Generates) [7] introduce more elaborate pre-training techniques that go beyond simple token prediction.", "REALM focuses on the specific application of open-domain question answering (open-QA): given a question and a database of documents, the task is to extract the correct answer from one of the documents. Following standard practices, pre-training is performed on a large corpus of free-form text. The innovation is an adjustment to the classic MLM task: before predicting a masked token, the model is trained to first retrieve a document that helps filling in the gap.", "This technique has two major advantages. First, it encourages evidence-based predictions rather than well-sounding guesses (it also helps that masks are applied over salient spans like \u201cJuly 1696\u201d rather than arbitrarily). Second, it conveniently sets the stage for end-to-end open-QA fine-tuning, as shown in the figure below. Note that the training data does not explicitly link the question-answer pairs to relevant documents. But since the model acquires some notion of document relevance during pre-training, the lack of this explicit signal is less damaging. The main disadvantage is the engineering complexity behind retrieving a document on each training step, while ensuring that this operation (over a potentially large set) remains differentiable.", "You might be tempted to think that including a retrieval step during pre-training reduces the generality of the pre-trained model (after all, REALM was only applied to open-QA). But MARGE shows that is not the case.", "All the methods above propose some sort of reconstruction of the input after it was altered. Left-to-right LMs remove all the text to the right hand side of the token being predicted, and MLMs elide arbitrary tokens from the input text. MARGE pre-training takes this challenge to the next level and asks the model to do the seemingly impossible: reconstruct a \u201ctarget\u201d document that it has never seen, not even crippled with truncations or omissions. Instead, the model is shown other \u201cevidence\u201d documents related to the input (e.g. paraphrases or even translations of it to another language), and challenged to regenerate the original text. The figure below shows an example.", "One of the major differences between MARGE and REALM is that the former is a sequence-to-sequence model (consisting of an encoder and a decoder), while the latter is solely a decoder. This enables MARGE to be fine-tuned on a wide range of downstream tasks, including discriminative (e.g., classification or extractive question answering) and generative tasks (e.g. machine translation, summarization or paraphrasing). MARGE makes the interesting observation that the pre-training + fine-tuning paradigm holds even when retrieval is only performed during pre-training (remember that REALM used its retriever during both stages). A truly remarkable outcome is that MARGE can perform decent zero-shot machine translation \u2014 that is, without any fine-tuning on parallel data!", "Increasing the amount of training data remains a surefire way to boost model quality, and this trend doesn\u2019t seem to slow down even in the presence of hundreds of billions of tokens. But despite being exposed to more text than a human being will ever process in their lifetime, machines are still underperforming us, especially in tasks that are generative in nature or that require complex reasoning. Which is to say \u2014 the way models interact with the data is very inefficient. The research community has started moving away from pre-training tasks that solely rely on linguistic form and incorporate objectives that encourage anchoring language understanding in the real world.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Building storia.ai | Ex-Google Research | twitter.com/juliarturc"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fd34424cf5715&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrends-in-model-pre-training-for-natural-language-understanding-d34424cf5715&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrends-in-model-pre-training-for-natural-language-understanding-d34424cf5715&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrends-in-model-pre-training-for-natural-language-understanding-d34424cf5715&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrends-in-model-pre-training-for-natural-language-understanding-d34424cf5715&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----d34424cf5715--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d34424cf5715--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@turc.raluca?source=post_page-----d34424cf5715--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@turc.raluca?source=post_page-----d34424cf5715--------------------------------", "anchor_text": "Julia Turc"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff758859396fc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrends-in-model-pre-training-for-natural-language-understanding-d34424cf5715&user=Julia+Turc&userId=f758859396fc&source=post_page-f758859396fc----d34424cf5715---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd34424cf5715&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrends-in-model-pre-training-for-natural-language-understanding-d34424cf5715&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd34424cf5715&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrends-in-model-pre-training-for-natural-language-understanding-d34424cf5715&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@impatrickt?utm_source=medium&utm_medium=referral", "anchor_text": "Patrick Tomasso"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://www.theverge.com/2019/12/11/20993407/ai-language-models-muppets-sesame-street-muppetware-elmo-bert-ernie", "anchor_text": "Muppet series"}, {"url": "https://arxiv.org/abs/2002.08909", "anchor_text": "REALM"}, {"url": "https://arxiv.org/pdf/2006.15020.pdf", "anchor_text": "MARGE"}, {"url": "https://arxiv.org/abs/2005.14165", "anchor_text": "Language Models are Few-Shot Learners"}, {"url": "https://arxiv.org/abs/1802.05365", "anchor_text": "Deep contextualized word representations"}, {"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"url": "https://arxiv.org/abs/1906.08237", "anchor_text": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"}, {"url": "https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf", "anchor_text": "Language Models and Unsupervised Multitask Learners"}, {"url": "https://arxiv.org/abs/2002.08909", "anchor_text": "REALM: Retrieval-Augmented Language Model Pre-Training"}, {"url": "https://arxiv.org/abs/2006.15020", "anchor_text": "Pre-training via Paraphrasing"}, {"url": "https://medium.com/tag/nlp?source=post_page-----d34424cf5715---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/naturallanguageprocessing?source=post_page-----d34424cf5715---------------naturallanguageprocessing-----------------", "anchor_text": "Naturallanguageprocessing"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----d34424cf5715---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----d34424cf5715---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----d34424cf5715---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd34424cf5715&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrends-in-model-pre-training-for-natural-language-understanding-d34424cf5715&user=Julia+Turc&userId=f758859396fc&source=-----d34424cf5715---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd34424cf5715&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrends-in-model-pre-training-for-natural-language-understanding-d34424cf5715&user=Julia+Turc&userId=f758859396fc&source=-----d34424cf5715---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd34424cf5715&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrends-in-model-pre-training-for-natural-language-understanding-d34424cf5715&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d34424cf5715--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fd34424cf5715&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrends-in-model-pre-training-for-natural-language-understanding-d34424cf5715&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----d34424cf5715---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----d34424cf5715--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----d34424cf5715--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----d34424cf5715--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----d34424cf5715--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----d34424cf5715--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----d34424cf5715--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----d34424cf5715--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----d34424cf5715--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@turc.raluca?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@turc.raluca?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Julia Turc"}, {"url": "https://medium.com/@turc.raluca/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "392 Followers"}, {"url": "http://storia.ai", "anchor_text": "storia.ai"}, {"url": "http://twitter.com/juliarturc", "anchor_text": "twitter.com/juliarturc"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff758859396fc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrends-in-model-pre-training-for-natural-language-understanding-d34424cf5715&user=Julia+Turc&userId=f758859396fc&source=post_page-f758859396fc--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F779df2fab045&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrends-in-model-pre-training-for-natural-language-understanding-d34424cf5715&newsletterV3=f758859396fc&newsletterV3Id=779df2fab045&user=Julia+Turc&userId=f758859396fc&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}