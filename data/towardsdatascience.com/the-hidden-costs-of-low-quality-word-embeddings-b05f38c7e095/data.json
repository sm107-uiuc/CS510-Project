{"url": "https://towardsdatascience.com/the-hidden-costs-of-low-quality-word-embeddings-b05f38c7e095", "time": 1683009776.73798, "path": "towardsdatascience.com/the-hidden-costs-of-low-quality-word-embeddings-b05f38c7e095/", "webpage": {"metadata": {"title": "The Hidden Costs of Low Quality Word Embeddings | by Todd Cook | Towards Data Science", "h1": "The Hidden Costs of Low Quality Word Embeddings", "description": "Why are non-English language models particularly susceptible to being handicapped with poor quality word embeddings? Find out why and how to remedy."}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/Word_embedding", "anchor_text": "word embeddings", "paragraph_index": 0}, {"url": "https://spacy.io/", "anchor_text": "SpaCy", "paragraph_index": 1}, {"url": "https://en.wikipedia.org/wiki/Word_embedding", "anchor_text": "word embeddings", "paragraph_index": 1}, {"url": "https://en.wikipedia.org/wiki/Feature_hashing", "anchor_text": "1", "paragraph_index": 2}, {"url": "https://spacy.io/models/de#de_core_news_md", "anchor_text": "the one for German", "paragraph_index": 3}, {"url": "https://www.kaggle.com/toddcook/spacy-embeddings-compared-quality-implications", "anchor_text": "2", "paragraph_index": 6}, {"url": "https://scikit-learn.org/stable/modules/learning_curve.html#learning-curve", "anchor_text": "learning curve plot", "paragraph_index": 6}, {"url": "https://fasttext.cc/", "anchor_text": "FastText", "paragraph_index": 7}, {"url": "https://fasttext.cc/docs/en/crawl-vectors.html", "anchor_text": "word vectors for 157 languages using Wikipedia and Common Crawl data", "paragraph_index": 7}, {"url": "https://radimrehurek.com/gensim", "anchor_text": "Gensim", "paragraph_index": 9}, {"url": "https://nlp.stanford.edu/projects/glove/", "anchor_text": "GloVe", "paragraph_index": 9}, {"url": "https://fasttext.cc/docs/en/cheatsheet.html#obtaining-word-vectors", "anchor_text": "their example", "paragraph_index": 11}, {"url": "https://www.kaggle.com/toddcook/spacy-embeddings-compared-quality-implications", "anchor_text": "A few quick experiments", "paragraph_index": 15}, {"url": "https://en.wikipedia.org/wiki/Principal_component_analysis", "anchor_text": "PCA", "paragraph_index": 15}, {"url": "https://spacy.io/usage/processing-pipelines", "anchor_text": "5", "paragraph_index": 21}, {"url": "https://stanfordnlp.github.io/stanza/", "anchor_text": "Stanza", "paragraph_index": 21}], "all_paragraphs": ["Today, memory and disk space are cheap, yet many applications are still paying a price for using poor quality word vector representations. Why are non-English language models particularly susceptible to being handicapped with poor quality word embeddings?", "The SpaCy NLP library offers models in English and many other languages, namely: German, French, Spanish, Portuguese, Italian, Dutch, Greek, Norwegian Bokm\u00e5l, Lithuanian. However, there is a big discrepancy among the offerings. The English model is available in sizes small, medium and large \u2014 each growing in file size and memory demands. Each model has a fixed vocabulary, and a major component of the model\u2019s file size is the word embeddings vector dimensions. One way to cut down on the file size is to use the hashing trick.", "In machine learning, feature hashing, also known as the hashing trick (by analogy to the kernel trick), is a fast and space-efficient way of vectorizing features, i.e. turning arbitrary features into indices in a vector or matrix. It works by applying a hash function to the features and using their hash values as indices directly, rather than looking the indices up in an associative array. [1]", "In some situations, there\u2019s nothing wrong with feature hashing, but when the resulting space is too restricted, collisions increase and performance decreases. Employing the hashing trick can create smaller word vector models (think 200,000 keywords mapping to just 20,000 floating point vectors). It appears the SpaCy medium-sized models built their word vector representations using a hashing trick with a lot of collisions, or perhaps they simply reused slots randomly\u2014 the medium-sized model vocabulary is fixed, so it\u2019s hard to tell, but the effect is the same. Currently, there is only one SpaCy large model that uses full-sized embeddings \u2014 English. Meanwhile, all the non-English languages and medium-sized models save disk and memory space by reusing embedding slots. The tip-off is indicated in each medium-sized model specification, here\u2019s the one for German:", "Yes, that\u2019s 86.2% reuse! In practice this is a lot of hash collisions.", "How do Hashed Word Embeddings perform compared to Full-Sized Embeddings?", "Even with only 7,108 items of labeled data, using hashed word vectors resulted in 73% accuracy, instead of the 84% accuracy when using the full-sized embeddings (three classes for separation with a majority class baseline of 43%)[2]. The learning curve plot above shows how a machine learning model performs differently as it is trained on progressive amounts of data.", "Fortunately, other embeddings are available for non-English languages, notably FastText which has word vectors for 157 languages using Wikipedia and Common Crawl data. Of course, size and quality vary greatly among the languages:", "There is no easy fix for a lack of data, however, FastText has improved the problem of generating Word Vectors with a unique twist: by allowing the use of character level information.", "Traditional word vector building tools like Gensim, GloVe, or an Embedding layer of a neural net, etc, need many examples to generate an accurate representation for a single word. Thus, most traditional word vector models have a limited vocabulary, and thus cannot predict the value of a word they haven\u2019t seen before. In contrast, by considering the sequence of characters \u2014 sometimes called subword information \u2014 FastText can generate models that use the collective information of many similar words and subwords to generate embeddings for words it has never seen. However, there is a catch, to perform this functionality you have to use the binary form of the FastText model, not the Word Vector text format.", "The FastText binary model file format is very large on disk and in memory, e.g. 4.2 GB compressed for English. The more common Word Vector file format is a simple text file where every line starts with a word and then is followed by floating point numbers that represent the dimensions of that word in the global word-vector space. Here\u2019s a short excerpt of the GloVe file showing the common text formatting of the word vector file:", "In practice, NLP/ML applications use the WordVector text file as a dictionary and to turn words into numerical representations that are then fed into pipelines and models. Using the FastText executable, you can load the binary model files (.bin), request the vectors for each word you specify and write out the information in a text file in the common word vector format. The process is really that straightforward, here\u2019s their example:", "Obtaining word vectorsPrint word vectors for a text file queries.txt containing words.`$ ./fasttext print-word-vectors model.bin < queries.txt`", "Unfortunately, it does require a lot of disk space and memory, so depending on your model and vocabulary size you may need to use a machine with more memory. Generating custom word embeddings using the FastText binary format may be a better solution than periodically regenerating the word embeddings from scratch, and this might be a good question for practical research.", "A better word embedding doesn\u2019t necessarily require more dimensions", "A few quick experiments show that Principal Component Analysis (PCA) can reduce the SpaCy full-sized word embeddings from 300 to 50 dimensions, and only suffer an accuracy loss of one or two points. When regular/full-sized embeddings are compared with the hashed embeddings, we found a 10%+ difference in accuracy. That\u2019s right: using 83% less space, a 50-dimension word vector outperformed a 300-dimension hashed word vector by more than 10% . This was using three classes for separation and 7,108 labeled items; if you have more data or more classes \u2014 your mileage may vary \u2014 but the performance will likely be worse.", "How to boost the performance of your NLP Application", "We have shown how poorly hashed embeddings perform, and we have shown how to get full-sized word embeddings. Now you should be able to upgrade the word embeddings your application uses and get a good performance boost. There is much legacy code still in use, and a solid performance gain can be attained by simply avoiding the use of hashed embeddings and always using the highest quality word embeddings available.", "Quick side note: ELMO and BERT can also produce embeddings based on characters, but they are designed to produce contextual embeddings which are the product of many characters and words in a sentence taken together. So while BERT and ELMO could be used to generate traditional word embeddings, those applications are beyond the scope of this article. They are powerful tools; but there\u2019s a lot of code that already successfully uses word embeddings, or for which it would be cost prohibitive to substitute in BERT or ELMO for generating the embeddings.", "SpaCy is still a good NLP Library", "Although I am critical of SpaCy\u2019s choice to use hashed word vectors on its medium-sized models, it\u2019s still a great NLP library and I recommend it all aspiring NLP practitioners (from beginners to intermediate and beyond).", "You don\u2019t have to throw away any SpaCy code you have; you should be able to use the SpaCy models for other functionality such as tokenization, Part of Speech tagging, Named Entity Recognition and Text Categorization[5]. Additionally, some parts of the SpaCy architecture are modular and extensible which allows for many novel usages. However, users should also consider the alternatives. Stanza, Stanford\u2019s NLP library is now giving some strong competition, and it should be considered for efforts starting from scratch. Unlike SpaCy, Stanza does not provide word embeddings as part of a discrete vocabulary.", "Most importantly, be aware of what kind of word embeddings your systems are using. Insist on high standards to achieve high performance.", "(Thanks to Kyle P. Johnson and Josh Frazier for reviewing and commenting on a draft of this article.)"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fb05f38c7e095&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-hidden-costs-of-low-quality-word-embeddings-b05f38c7e095&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-hidden-costs-of-low-quality-word-embeddings-b05f38c7e095&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-hidden-costs-of-low-quality-word-embeddings-b05f38c7e095&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-hidden-costs-of-low-quality-word-embeddings-b05f38c7e095&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/@todd.g.cook?source=post_page-----b05f38c7e095--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b05f38c7e095--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@todd.g.cook?source=post_page-----b05f38c7e095--------------------------------", "anchor_text": "Todd Cook"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1cdf128a8cb2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-hidden-costs-of-low-quality-word-embeddings-b05f38c7e095&user=Todd+Cook&userId=1cdf128a8cb2&source=post_page-1cdf128a8cb2----b05f38c7e095---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----b05f38c7e095--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb05f38c7e095&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-hidden-costs-of-low-quality-word-embeddings-b05f38c7e095&user=Todd+Cook&userId=1cdf128a8cb2&source=-----b05f38c7e095---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb05f38c7e095&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-hidden-costs-of-low-quality-word-embeddings-b05f38c7e095&source=-----b05f38c7e095---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://en.wikipedia.org/wiki/Word_embedding", "anchor_text": "word embeddings"}, {"url": "https://spacy.io/", "anchor_text": "SpaCy"}, {"url": "https://en.wikipedia.org/wiki/Word_embedding", "anchor_text": "word embeddings"}, {"url": "https://en.wikipedia.org/wiki/Feature_hashing", "anchor_text": "1"}, {"url": "https://spacy.io/models/de#de_core_news_md", "anchor_text": "the one for German"}, {"url": "https://www.kaggle.com/toddcook/spacy-embeddings-compared-quality-implications", "anchor_text": "2"}, {"url": "https://scikit-learn.org/stable/modules/learning_curve.html#learning-curve", "anchor_text": "learning curve plot"}, {"url": "https://fasttext.cc/", "anchor_text": "FastText"}, {"url": "https://fasttext.cc/docs/en/crawl-vectors.html", "anchor_text": "word vectors for 157 languages using Wikipedia and Common Crawl data"}, {"url": "https://en.wikipedia.org/wiki/List_of_Wikipedias", "anchor_text": "3"}, {"url": "https://commoncrawl.github.io/cc-crawl-statistics/plots/crawlsize", "anchor_text": "4"}, {"url": "https://radimrehurek.com/gensim", "anchor_text": "Gensim"}, {"url": "https://nlp.stanford.edu/projects/glove/", "anchor_text": "GloVe"}, {"url": "https://fasttext.cc/docs/en/cheatsheet.html#obtaining-word-vectors", "anchor_text": "their example"}, {"url": "https://www.kaggle.com/toddcook/spacy-embeddings-compared-quality-implications", "anchor_text": "A few quick experiments"}, {"url": "https://en.wikipedia.org/wiki/Principal_component_analysis", "anchor_text": "PCA"}, {"url": "https://spacy.io/usage/processing-pipelines", "anchor_text": "5"}, {"url": "https://stanfordnlp.github.io/stanza/", "anchor_text": "Stanza"}, {"url": "https://en.wikipedia.org/wiki/Feature_hashing", "anchor_text": "https://en.wikipedia.org/wiki/Feature_hashing"}, {"url": "https://www.kaggle.com/toddcook/spacy-embeddings-compared-quality-implications", "anchor_text": "https://www.kaggle.com/toddcook/spacy-embeddings-compared-quality-implications"}, {"url": "https://en.wikipedia.org/wiki/List_of_Wikipedias", "anchor_text": "https://en.wikipedia.org/wiki/List_of_Wikipedias"}, {"url": "https://commoncrawl.github.io/cc-crawl-statistics/plots/crawlsize", "anchor_text": "https://commoncrawl.github.io/cc-crawl-statistics/plots/crawlsize"}, {"url": "https://spacy.io/usage/processing-pipelines", "anchor_text": "https://spacy.io/usage/processing-pipelines"}, {"url": "https://medium.com/tag/nlp?source=post_page-----b05f38c7e095---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/word-embeddings?source=post_page-----b05f38c7e095---------------word_embeddings-----------------", "anchor_text": "Word Embeddings"}, {"url": "https://medium.com/tag/spacy?source=post_page-----b05f38c7e095---------------spacy-----------------", "anchor_text": "Spacy"}, {"url": "https://medium.com/tag/fasttext?source=post_page-----b05f38c7e095---------------fasttext-----------------", "anchor_text": "Fasttext"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----b05f38c7e095---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb05f38c7e095&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-hidden-costs-of-low-quality-word-embeddings-b05f38c7e095&user=Todd+Cook&userId=1cdf128a8cb2&source=-----b05f38c7e095---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb05f38c7e095&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-hidden-costs-of-low-quality-word-embeddings-b05f38c7e095&user=Todd+Cook&userId=1cdf128a8cb2&source=-----b05f38c7e095---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb05f38c7e095&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-hidden-costs-of-low-quality-word-embeddings-b05f38c7e095&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/@todd.g.cook?source=post_page-----b05f38c7e095--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b05f38c7e095--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1cdf128a8cb2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-hidden-costs-of-low-quality-word-embeddings-b05f38c7e095&user=Todd+Cook&userId=1cdf128a8cb2&source=post_page-1cdf128a8cb2----b05f38c7e095---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fbaee942d363c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-hidden-costs-of-low-quality-word-embeddings-b05f38c7e095&newsletterV3=1cdf128a8cb2&newsletterV3Id=baee942d363c&user=Todd+Cook&userId=1cdf128a8cb2&source=-----b05f38c7e095---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/@todd.g.cook?source=post_page-----b05f38c7e095--------------------------------", "anchor_text": "Written by Todd Cook"}, {"url": "https://medium.com/@todd.g.cook/followers?source=post_page-----b05f38c7e095--------------------------------", "anchor_text": "44 Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----b05f38c7e095--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1cdf128a8cb2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-hidden-costs-of-low-quality-word-embeddings-b05f38c7e095&user=Todd+Cook&userId=1cdf128a8cb2&source=post_page-1cdf128a8cb2----b05f38c7e095---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fbaee942d363c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-hidden-costs-of-low-quality-word-embeddings-b05f38c7e095&newsletterV3=1cdf128a8cb2&newsletterV3Id=baee942d363c&user=Todd+Cook&userId=1cdf128a8cb2&source=-----b05f38c7e095---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-bert-determines-search-relevance-2a67a1575ac4?source=author_recirc-----b05f38c7e095----0---------------------66c5d7f5_a715_415d_bb9a_72c999819ed4-------", "anchor_text": ""}, {"url": "https://medium.com/@todd.g.cook?source=author_recirc-----b05f38c7e095----0---------------------66c5d7f5_a715_415d_bb9a_72c999819ed4-------", "anchor_text": ""}, {"url": "https://medium.com/@todd.g.cook?source=author_recirc-----b05f38c7e095----0---------------------66c5d7f5_a715_415d_bb9a_72c999819ed4-------", "anchor_text": "Todd Cook"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----b05f38c7e095----0---------------------66c5d7f5_a715_415d_bb9a_72c999819ed4-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/how-bert-determines-search-relevance-2a67a1575ac4?source=author_recirc-----b05f38c7e095----0---------------------66c5d7f5_a715_415d_bb9a_72c999819ed4-------", "anchor_text": "How BERT Determines Search RelevanceUnderstanding BERT\u2019s limitations and biases will help you better understand how BERT and Search views the world and your content."}, {"url": "https://towardsdatascience.com/how-bert-determines-search-relevance-2a67a1575ac4?source=author_recirc-----b05f38c7e095----0---------------------66c5d7f5_a715_415d_bb9a_72c999819ed4-------", "anchor_text": "10 min read\u00b7Aug 30, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2a67a1575ac4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-bert-determines-search-relevance-2a67a1575ac4&user=Todd+Cook&userId=1cdf128a8cb2&source=-----2a67a1575ac4----0-----------------clap_footer----66c5d7f5_a715_415d_bb9a_72c999819ed4-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-bert-determines-search-relevance-2a67a1575ac4?source=author_recirc-----b05f38c7e095----0---------------------66c5d7f5_a715_415d_bb9a_72c999819ed4-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2a67a1575ac4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-bert-determines-search-relevance-2a67a1575ac4&source=-----b05f38c7e095----0-----------------bookmark_preview----66c5d7f5_a715_415d_bb9a_72c999819ed4-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----b05f38c7e095----1---------------------66c5d7f5_a715_415d_bb9a_72c999819ed4-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----b05f38c7e095----1---------------------66c5d7f5_a715_415d_bb9a_72c999819ed4-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----b05f38c7e095----1---------------------66c5d7f5_a715_415d_bb9a_72c999819ed4-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----b05f38c7e095----1---------------------66c5d7f5_a715_415d_bb9a_72c999819ed4-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----b05f38c7e095----1---------------------66c5d7f5_a715_415d_bb9a_72c999819ed4-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----b05f38c7e095----1---------------------66c5d7f5_a715_415d_bb9a_72c999819ed4-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----66c5d7f5_a715_415d_bb9a_72c999819ed4-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----b05f38c7e095----1---------------------66c5d7f5_a715_415d_bb9a_72c999819ed4-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----b05f38c7e095----1-----------------bookmark_preview----66c5d7f5_a715_415d_bb9a_72c999819ed4-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----b05f38c7e095----2---------------------66c5d7f5_a715_415d_bb9a_72c999819ed4-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----b05f38c7e095----2---------------------66c5d7f5_a715_415d_bb9a_72c999819ed4-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----b05f38c7e095----2---------------------66c5d7f5_a715_415d_bb9a_72c999819ed4-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----b05f38c7e095----2---------------------66c5d7f5_a715_415d_bb9a_72c999819ed4-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----b05f38c7e095----2---------------------66c5d7f5_a715_415d_bb9a_72c999819ed4-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----b05f38c7e095----2---------------------66c5d7f5_a715_415d_bb9a_72c999819ed4-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----2-----------------clap_footer----66c5d7f5_a715_415d_bb9a_72c999819ed4-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----b05f38c7e095----2---------------------66c5d7f5_a715_415d_bb9a_72c999819ed4-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----b05f38c7e095----2-----------------bookmark_preview----66c5d7f5_a715_415d_bb9a_72c999819ed4-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----b05f38c7e095----3---------------------66c5d7f5_a715_415d_bb9a_72c999819ed4-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----b05f38c7e095----3---------------------66c5d7f5_a715_415d_bb9a_72c999819ed4-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----b05f38c7e095----3---------------------66c5d7f5_a715_415d_bb9a_72c999819ed4-------", "anchor_text": "Jacob Marks, Ph.D."}, {"url": "https://towardsdatascience.com/?source=author_recirc-----b05f38c7e095----3---------------------66c5d7f5_a715_415d_bb9a_72c999819ed4-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----b05f38c7e095----3---------------------66c5d7f5_a715_415d_bb9a_72c999819ed4-------", "anchor_text": "How I Turned My Company\u2019s Docs into a Searchable Database with OpenAIAnd how you can do the same with your docs"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----b05f38c7e095----3---------------------66c5d7f5_a715_415d_bb9a_72c999819ed4-------", "anchor_text": "15 min read\u00b76 days ago"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&user=Jacob+Marks%2C+Ph.D.&userId=f7dc0c0eae92&source=-----4f2d34bd8736----3-----------------clap_footer----66c5d7f5_a715_415d_bb9a_72c999819ed4-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----b05f38c7e095----3---------------------66c5d7f5_a715_415d_bb9a_72c999819ed4-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "20"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&source=-----b05f38c7e095----3-----------------bookmark_preview----66c5d7f5_a715_415d_bb9a_72c999819ed4-------", "anchor_text": ""}, {"url": "https://medium.com/@todd.g.cook?source=post_page-----b05f38c7e095--------------------------------", "anchor_text": "See all from Todd Cook"}, {"url": "https://towardsdatascience.com/?source=post_page-----b05f38c7e095--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://towardsdatascience.com/how-to-train-a-word2vec-model-from-scratch-with-gensim-c457d587e031?source=read_next_recirc-----b05f38c7e095----0---------------------b16b0198_14ec_4687_9cae_472dea562c63-------", "anchor_text": ""}, {"url": "https://medium.com/@theDrewDag?source=read_next_recirc-----b05f38c7e095----0---------------------b16b0198_14ec_4687_9cae_472dea562c63-------", "anchor_text": ""}, {"url": "https://medium.com/@theDrewDag?source=read_next_recirc-----b05f38c7e095----0---------------------b16b0198_14ec_4687_9cae_472dea562c63-------", "anchor_text": "Andrea D'Agostino"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----b05f38c7e095----0---------------------b16b0198_14ec_4687_9cae_472dea562c63-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/how-to-train-a-word2vec-model-from-scratch-with-gensim-c457d587e031?source=read_next_recirc-----b05f38c7e095----0---------------------b16b0198_14ec_4687_9cae_472dea562c63-------", "anchor_text": "How to Train a Word2Vec Model from Scratch with GensimIn this article we will explore Gensim, a very popular Python library for training text-based machine learning models, to train a Word2Vec\u2026"}, {"url": "https://towardsdatascience.com/how-to-train-a-word2vec-model-from-scratch-with-gensim-c457d587e031?source=read_next_recirc-----b05f38c7e095----0---------------------b16b0198_14ec_4687_9cae_472dea562c63-------", "anchor_text": "\u00b79 min read\u00b7Feb 6"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc457d587e031&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-train-a-word2vec-model-from-scratch-with-gensim-c457d587e031&user=Andrea+D%27Agostino&userId=4e8f67b0b09b&source=-----c457d587e031----0-----------------clap_footer----b16b0198_14ec_4687_9cae_472dea562c63-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-to-train-a-word2vec-model-from-scratch-with-gensim-c457d587e031?source=read_next_recirc-----b05f38c7e095----0---------------------b16b0198_14ec_4687_9cae_472dea562c63-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc457d587e031&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-train-a-word2vec-model-from-scratch-with-gensim-c457d587e031&source=-----b05f38c7e095----0-----------------bookmark_preview----b16b0198_14ec_4687_9cae_472dea562c63-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/generating-word-embeddings-from-text-data-using-skip-gram-algorithm-and-deep-learning-in-python-a8873b225ab6?source=read_next_recirc-----b05f38c7e095----1---------------------b16b0198_14ec_4687_9cae_472dea562c63-------", "anchor_text": ""}, {"url": "https://angeleastbengal.medium.com/?source=read_next_recirc-----b05f38c7e095----1---------------------b16b0198_14ec_4687_9cae_472dea562c63-------", "anchor_text": ""}, {"url": "https://angeleastbengal.medium.com/?source=read_next_recirc-----b05f38c7e095----1---------------------b16b0198_14ec_4687_9cae_472dea562c63-------", "anchor_text": "Angel Das"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----b05f38c7e095----1---------------------b16b0198_14ec_4687_9cae_472dea562c63-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/generating-word-embeddings-from-text-data-using-skip-gram-algorithm-and-deep-learning-in-python-a8873b225ab6?source=read_next_recirc-----b05f38c7e095----1---------------------b16b0198_14ec_4687_9cae_472dea562c63-------", "anchor_text": "Generating Word Embeddings from Text Data using Skip-Gram Algorithm and Deep Learning in PythonIntroduction to embeddings in natural language processing using Artificial Neural Network and Gensim"}, {"url": "https://towardsdatascience.com/generating-word-embeddings-from-text-data-using-skip-gram-algorithm-and-deep-learning-in-python-a8873b225ab6?source=read_next_recirc-----b05f38c7e095----1---------------------b16b0198_14ec_4687_9cae_472dea562c63-------", "anchor_text": "\u00b713 min read\u00b7Nov 9, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa8873b225ab6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerating-word-embeddings-from-text-data-using-skip-gram-algorithm-and-deep-learning-in-python-a8873b225ab6&user=Angel+Das&userId=8418ab50405a&source=-----a8873b225ab6----1-----------------clap_footer----b16b0198_14ec_4687_9cae_472dea562c63-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/generating-word-embeddings-from-text-data-using-skip-gram-algorithm-and-deep-learning-in-python-a8873b225ab6?source=read_next_recirc-----b05f38c7e095----1---------------------b16b0198_14ec_4687_9cae_472dea562c63-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "4"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa8873b225ab6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerating-word-embeddings-from-text-data-using-skip-gram-algorithm-and-deep-learning-in-python-a8873b225ab6&source=-----b05f38c7e095----1-----------------bookmark_preview----b16b0198_14ec_4687_9cae_472dea562c63-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----b05f38c7e095----0---------------------b16b0198_14ec_4687_9cae_472dea562c63-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----b05f38c7e095----0---------------------b16b0198_14ec_4687_9cae_472dea562c63-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----b05f38c7e095----0---------------------b16b0198_14ec_4687_9cae_472dea562c63-------", "anchor_text": "Alexander Nguyen"}, {"url": "https://levelup.gitconnected.com/?source=read_next_recirc-----b05f38c7e095----0---------------------b16b0198_14ec_4687_9cae_472dea562c63-------", "anchor_text": "Level Up Coding"}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----b05f38c7e095----0---------------------b16b0198_14ec_4687_9cae_472dea562c63-------", "anchor_text": "Why I Keep Failing Candidates During Google Interviews\u2026They don\u2019t meet the bar."}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----b05f38c7e095----0---------------------b16b0198_14ec_4687_9cae_472dea562c63-------", "anchor_text": "\u00b74 min read\u00b7Apr 13"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fgitconnected%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&user=Alexander+Nguyen&userId=a148fd75c2e9&source=-----dc8f865b2c19----0-----------------clap_footer----b16b0198_14ec_4687_9cae_472dea562c63-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----b05f38c7e095----0---------------------b16b0198_14ec_4687_9cae_472dea562c63-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "91"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&source=-----b05f38c7e095----0-----------------bookmark_preview----b16b0198_14ec_4687_9cae_472dea562c63-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/density-based-clustering-dbscan-vs-hdbscan-39e02af990c7?source=read_next_recirc-----b05f38c7e095----1---------------------b16b0198_14ec_4687_9cae_472dea562c63-------", "anchor_text": ""}, {"url": "https://thomasdorfer.medium.com/?source=read_next_recirc-----b05f38c7e095----1---------------------b16b0198_14ec_4687_9cae_472dea562c63-------", "anchor_text": ""}, {"url": "https://thomasdorfer.medium.com/?source=read_next_recirc-----b05f38c7e095----1---------------------b16b0198_14ec_4687_9cae_472dea562c63-------", "anchor_text": "Thomas A Dorfer"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----b05f38c7e095----1---------------------b16b0198_14ec_4687_9cae_472dea562c63-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/density-based-clustering-dbscan-vs-hdbscan-39e02af990c7?source=read_next_recirc-----b05f38c7e095----1---------------------b16b0198_14ec_4687_9cae_472dea562c63-------", "anchor_text": "Density-Based Clustering: DBSCAN vs. HDBSCANWhich algorithm to choose for your data"}, {"url": "https://towardsdatascience.com/density-based-clustering-dbscan-vs-hdbscan-39e02af990c7?source=read_next_recirc-----b05f38c7e095----1---------------------b16b0198_14ec_4687_9cae_472dea562c63-------", "anchor_text": "\u00b75 min read\u00b7Dec 5, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F39e02af990c7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdensity-based-clustering-dbscan-vs-hdbscan-39e02af990c7&user=Thomas+A+Dorfer&userId=7c54f9b62b90&source=-----39e02af990c7----1-----------------clap_footer----b16b0198_14ec_4687_9cae_472dea562c63-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/density-based-clustering-dbscan-vs-hdbscan-39e02af990c7?source=read_next_recirc-----b05f38c7e095----1---------------------b16b0198_14ec_4687_9cae_472dea562c63-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F39e02af990c7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdensity-based-clustering-dbscan-vs-hdbscan-39e02af990c7&source=-----b05f38c7e095----1-----------------bookmark_preview----b16b0198_14ec_4687_9cae_472dea562c63-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----b05f38c7e095----2---------------------b16b0198_14ec_4687_9cae_472dea562c63-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----b05f38c7e095----2---------------------b16b0198_14ec_4687_9cae_472dea562c63-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----b05f38c7e095----2---------------------b16b0198_14ec_4687_9cae_472dea562c63-------", "anchor_text": "The PyCoach"}, {"url": "https://artificialcorner.com/?source=read_next_recirc-----b05f38c7e095----2---------------------b16b0198_14ec_4687_9cae_472dea562c63-------", "anchor_text": "Artificial Corner"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----b05f38c7e095----2---------------------b16b0198_14ec_4687_9cae_472dea562c63-------", "anchor_text": "You\u2019re Using ChatGPT Wrong! Here\u2019s How to Be Ahead of 99% of ChatGPT UsersMaster ChatGPT by learning prompt engineering."}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----b05f38c7e095----2---------------------b16b0198_14ec_4687_9cae_472dea562c63-------", "anchor_text": "\u00b77 min read\u00b7Mar 17"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fartificial-corner%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&user=The+PyCoach&userId=fb44e21903f3&source=-----886a50dabc54----2-----------------clap_footer----b16b0198_14ec_4687_9cae_472dea562c63-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----b05f38c7e095----2---------------------b16b0198_14ec_4687_9cae_472dea562c63-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "276"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&source=-----b05f38c7e095----2-----------------bookmark_preview----b16b0198_14ec_4687_9cae_472dea562c63-------", "anchor_text": ""}, {"url": "https://billatnapier.medium.com/similarity-hashing-and-perceptial-hashes-963fba36c8b5?source=read_next_recirc-----b05f38c7e095----3---------------------b16b0198_14ec_4687_9cae_472dea562c63-------", "anchor_text": ""}, {"url": "https://billatnapier.medium.com/?source=read_next_recirc-----b05f38c7e095----3---------------------b16b0198_14ec_4687_9cae_472dea562c63-------", "anchor_text": ""}, {"url": "https://billatnapier.medium.com/?source=read_next_recirc-----b05f38c7e095----3---------------------b16b0198_14ec_4687_9cae_472dea562c63-------", "anchor_text": "Prof Bill Buchanan OBE"}, {"url": "https://billatnapier.medium.com/similarity-hashing-and-perceptial-hashes-963fba36c8b5?source=read_next_recirc-----b05f38c7e095----3---------------------b16b0198_14ec_4687_9cae_472dea562c63-------", "anchor_text": "Similarity Hashing and Perceptual HashesSean McKeown [here] and myself have just published a paper to arXiv that will be presented at DFRWS (Digital Forensics Research Conference)\u2026"}, {"url": "https://billatnapier.medium.com/similarity-hashing-and-perceptial-hashes-963fba36c8b5?source=read_next_recirc-----b05f38c7e095----3---------------------b16b0198_14ec_4687_9cae_472dea562c63-------", "anchor_text": "\u00b79 min read\u00b7Dec 17, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F963fba36c8b5&operation=register&redirect=https%3A%2F%2Fbillatnapier.medium.com%2Fsimilarity-hashing-and-perceptial-hashes-963fba36c8b5&user=Prof+Bill+Buchanan+OBE&userId=e680fcaf274b&source=-----963fba36c8b5----3-----------------clap_footer----b16b0198_14ec_4687_9cae_472dea562c63-------", "anchor_text": ""}, {"url": "https://billatnapier.medium.com/similarity-hashing-and-perceptial-hashes-963fba36c8b5?source=read_next_recirc-----b05f38c7e095----3---------------------b16b0198_14ec_4687_9cae_472dea562c63-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F963fba36c8b5&operation=register&redirect=https%3A%2F%2Fbillatnapier.medium.com%2Fsimilarity-hashing-and-perceptial-hashes-963fba36c8b5&source=-----b05f38c7e095----3-----------------bookmark_preview----b16b0198_14ec_4687_9cae_472dea562c63-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----b05f38c7e095--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----b05f38c7e095--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----b05f38c7e095--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----b05f38c7e095--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----b05f38c7e095--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----b05f38c7e095--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----b05f38c7e095--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----b05f38c7e095--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----b05f38c7e095--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----b05f38c7e095--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}