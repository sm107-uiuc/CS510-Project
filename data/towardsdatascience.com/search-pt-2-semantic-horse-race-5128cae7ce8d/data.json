{"url": "https://towardsdatascience.com/search-pt-2-semantic-horse-race-5128cae7ce8d", "time": 1683014114.74475, "path": "towardsdatascience.com/search-pt-2-semantic-horse-race-5128cae7ce8d/", "webpage": {"metadata": {"title": "Search (Pt 2) \u2014 A Semantic Horse Race | by Mihail Dungarov | Towards Data Science", "h1": "Search (Pt 2) \u2014 A Semantic Horse Race", "description": "In this non-technical article, we will compare contextual search to the keyword based approach. For the former, we will utilise some of the recent developments in NLP to search through a large corpus\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/search-pt-1-a-gentle-introduction-335656c0f814", "anchor_text": "Pt 1 \u2014 A gentle introduction", "paragraph_index": 2}, {"url": "https://medium.com/@mihail.dungarov/elastic-transformers-ae011e8f5b88", "anchor_text": "Pt 3 (Elastic Transformers)", "paragraph_index": 3}, {"url": "https://en.wikipedia.org/wiki/John_Rupert_Firth", "anchor_text": "Firth, J. R", "paragraph_index": 8}, {"url": "https://allennlp.org/elmo", "anchor_text": "ELMo", "paragraph_index": 9}, {"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "BERT", "paragraph_index": 9}, {"url": "http://jalammar.github.io/illustrated-bert/", "anchor_text": "here", "paragraph_index": 9}, {"url": "https://github.com/huggingface/transformers", "anchor_text": "transformers", "paragraph_index": 9}, {"url": "https://github.com/UKPLab/sentence-transformers", "anchor_text": "sentence-transformers", "paragraph_index": 10}, {"url": "https://arxiv.org/abs/1908.10084", "anchor_text": "SBERT", "paragraph_index": 10}, {"url": "https://arxiv.org/abs/1908.10084", "anchor_text": "paper", "paragraph_index": 10}, {"url": "https://arxiv.org/abs/1803.11175", "anchor_text": "USE", "paragraph_index": 10}, {"url": "https://arxiv.org/abs/1705.02364", "anchor_text": "InferSent", "paragraph_index": 10}, {"url": "https://arxiv.org/abs/2007.12603", "anchor_text": "IR-BERT", "paragraph_index": 12}, {"url": "http://trec-news.org/guidelines-2020.pdf", "anchor_text": "here", "paragraph_index": 12}, {"url": "https://www.kaggle.com/therohk/million-headlines", "anchor_text": "A Million News Headlines", "paragraph_index": 14}, {"url": "https://arxiv.org/pdf/2007.12603.pdf", "anchor_text": "IR-BERT", "paragraph_index": 18}, {"url": "https://en.wikipedia.org/wiki/Royal_Commission_into_Misconduct_in_the_Banking,_Superannuation_and_Financial_Services_Industry", "anchor_text": "Royal Commission", "paragraph_index": 19}, {"url": "https://medium.com/@mihail.dungarov/elastic-transformers-ae011e8f5b88", "anchor_text": "Pt 3", "paragraph_index": 22}, {"url": "https://github.com/PAIR-code/lit", "anchor_text": "released a tool", "paragraph_index": 23}, {"url": "https://medium.com/datadriveninvestor/releasing-corona-papers-an-ai-powered-search-engine-to-explore-covid-19-research-4b18d1259491", "anchor_text": "Corona Papers", "paragraph_index": 24}, {"url": "https://covid19-research-explorer.appspot.com/", "anchor_text": "COVID Research Explorer", "paragraph_index": 24}, {"url": "https://www.semanticscholar.org/cord19", "anchor_text": "CORD-19", "paragraph_index": 24}, {"url": "https://medium.com/@mihail.dungarov/elastic-transformers-ae011e8f5b88", "anchor_text": "Part 3", "paragraph_index": 28}, {"url": "https://www.linkedin.com/in/mihail-dungarov-cfa-a0291a88", "anchor_text": "LinkedIn", "paragraph_index": 29}], "all_paragraphs": ["In this non-technical article, we will compare contextual search to the keyword based approach. For the former, we will utilise some of the recent developments in NLP to search through a large corpus of news. We will focus on explaining the differences, pros and cons of the approach vs its traditional counterpart.", "This is a three part series on Search.", "In Pt 1 \u2014 A gentle introduction one we provided an overview of the basic building blocks of search.", "Finally, Pt 3 (Elastic Transformers) contains the purely technical considerations of how to build an index as an Elasticsearch engine with contextual text embeddings. For the current discussion, we will use some results from that search index.", "In the previous article we took a bird\u2019s eye perspective of how search works, building blocks and how they differ. Here, we will consider a practical comparison between contextual and keyword search. With contextual search, we mean specifically text embeddings using NLP transformers, which we will dive into in a bit.", "Traditional keyword search tends to use specific keyword frequencies to identify a good fit for a search query. This, however, can be limiting in some cases, specifically, if the keywords we are using are not representative of the documents we are searching. For instance, we might look for \u201cnatural disasters\u201d, but the available documents might contain many examples of specific disasters such as \u201churricane\u201d, \u201cearthquake\u201d, etc. without referring to those as our explicit word choice \u201cnatural disaster\u201d. This is where contextual embeddings can help.", "Text embeddings are the mathematical representations of words, sentences or documents as vectors. This means that we can represent text in ways that allow us to perform mathematical operations on them. The way that we can say that the number 2 is closer to 3 than it is to 10 and all of them are closer to each other than they are to 100. If we are able to encode the entire meaning of a document in such a way we can use those properties to find similarities between documents, group them together in meaningful clusters, etc.", "In most cases, the way such representations are \u2018learned\u2019 by machines is by giving them a lot of text to read and urge them to learn what words or sentences \u201cgo together\u201d. Famously:", "You shall know a word by the company it keeps \u2014 Firth, J. R", "Earlier tools on text embeddings used to be able to only capture single words mostly (Word2Vec, GloVe, etc), however, that was still problematic with examples where a phrase might have different meaning than the words within. \u2018Natural\u2019 and \u2018disaster\u2019 mean something very different when put together. Recent advances in NLP, have brought forth a fleet of contextual embedding models, such as ELMo, BERT, etc. (one good overview here). Often, however, these come as pre-trained tools which still need fine tuning for a specific task. Such a task may be: sentence sentiment identification, sentence similarity, question-answering, etc. Huggingface\u2019s transformers library made the use of a lot of these tools very accessible.", "Hare, we will use sentence-transformers \u2014 a Python library which (among others) brings us SBERT models pre-trained for the task of sentence semantic similarity, allowing us to compare the meaning of whole sentences. (definitely check out the paper too). This is the crucial enabler of what we do, as without powerful document level embeddings (in this case the news headline is the document) we are not able to extract meaningful comparisons between the query and documents searched. Note, that SBERT is not the only way to do this, some other approaches are, eg USE, InferSent, etc. Also this approach for search tries to match the query with results that mention similar words from the query. In this way it is similar to keyword search which looks for documents which use the same words of the query but does not replace \u2018question-answering\u2019, where we are looking for a specific answer to a question.", "Here, I will compare the results we get if we query the two approaches side-by-side. I will try to compare the differences and draw some conclusions from those.", "Note that a more formal evaluation of a similar task is done in IR-BERT (the task is to find the best supporting articles to a given (query) article, Task 1 here). The authors demonstrate that the contextual approach (SBERT) clearly outperforms the pure keyword approach (BM25), however, results are mixed when more advanced keyword weighing techniques are being used.", "Firstly, in many cases, simple keyword search will suffice. Consider searching for \u201cApple Inc\u201d. We do not need a semantic engine for that one. As are looking for is a \u201cnamed entity\u201d, exact matches make sense and approximate matches might be outright wrong. In a way, when we are very specific about what we are looking for, looking for specific names, dates, ids, etc., an exact match may do just fine. However, if we are missing the exact terms of what we need (see \u2018natural disaster\u2019 example before) we would benefit from a result based on context rather than exact matches.", "The setup: I compared top results between keyword and contextual search side by side on a few queries. In green I highlight the results clearly relevant for search, amber \u2014 ambiguous ones and red \u2014 irrelevant ones. Notice that we used A Million News Headlines \u2014 sourced from ABC news, i.e. the news have some Australian focus.", "Lets search (topically) for \u201cvirus threat\u201d.", "Both approaches yield good results. Note that result #5. \u201cWHO highlights dangers of vector borne diseases\u201d doesn\u2019t include any of our search terms but is highly relevant still. Here are more examples where contextual search results do not contain ANY of the keywords. We see relevant mentions of: outbreak, infection, parasite, etc", "The corpus has plenty of exact matches of \u201cNatural disaster\u201d so the first 10 mentions are pretty close to each other in both forms of search. However, consider the contextual results without exact matches to the keywords. We can see relevant mentions such as: flood, freak storm, catastrophic fire, etc", "Expanding the query further makes things more complicated. Firstly, about the query: the ask here is a bit vague as the user is obviously interested in banking reform and regulation but not specific about what, who, when. It is likely in the context of a wider research trying to compare and contrast cases. Hence, a good result (independent of search approach) would be looking for diverse examples. Coincidentally diversity of results is also a metric considered in the IR-BERT where contextual search outperforms", "Looking at contextual results, it seems to be clear the topic is banking, but narrowing down to regulatory / reform topics is leading to debatable results. There are mentions of RBA (Australia\u2019s Central Bank) which, however, are not always related to regulatory risks as well as mentions of the Royal Commission (is commission investigating a range of financial misconduct in financial services) which is relevant to finance but are not always concerned with banking. On the keyword search side, however, despite lack of good documents, the ones returned consistently will have at least 2 of the words in the query, which often seems to make the result \u201csemantically relevant\u201d \u2014 as long as a couple of the keywords are present, results are likely relevant. Notice also how explainability of keyword search helps us immediately rule out some cases. For instance, keyword result #8 \u201cGovt Internet Regulatory Plan Criticised\u201d \u2014 we know exactly why this was suggested and can therefore rule it out quickly. In contrast, contextual result #5 \u201cRBA considers cap on credit card surcharges\u201d is not clear as to why exactly the model considers it a good hit", "What can we say about the benefits and the shortcomings of contextual search when compared to keyword search", "Contextual bit \u2014 as we have seen, keyword search can be (sometimes) limiting. This is particularly relevant when doing research or just starting to explore a topic as we often do not relevant terms, keywords and entities for our search", "Side-by-side capabilities \u2014 in the current setup, we are able to easily switch between key-word and contextual search. Some work can further be done to get both results ranked side-by-side. Some assumptions are needed, however. Notice that keyword search can also be used as an initial filter for good candidates to improve the overall speed of the solution. More on speed considerations in Pt 3.", "Explainability \u2014 we cannot easily reverse-engineer why the model decided to show a specific result. Seeing our search words in the results helps make clear decisions on whether we agree with the results. This is an ongoing area of research and development. Recently, Google released a tool for interpretability of contextual embeddings. However, the use here would not be a straightforward application as we need to also assess what is the specific driver of a search match.", "Context is contextual \u2014 talking about semantic search and meaning can be highly domain specific to what we are actually looking for. For instance, we might be interested in \u201cvaccines\u201d in the context of our own health, or as investors deciding companies support, or as researchers \u2014 looking for technical details of trials, methods, etc. These aspects can partially be dealt with by refining the query, but the quality of results will be limited by both the available documents in the index as well as the way in which contextual embeddings have been trained. It is a particular concern that the system not equipped to indicate to the user that the quality of a domain-specific query may be poor \u2014 it will dutifully always provide lists of ranked results. In practice, domain specific search engines help solve this. One example would be a specialised tool for COVID searchers based on domain specific articles and research. A couple of examples of such tools are Corona Papers and Google\u2019s COVID Research Explorer both using CORD-19 (COVID-specific database of research articles) to contextualise the text embeddings used", "We have seen an outline of what are text embeddings and how they can help contextual search.", "We illustrated with some contrastive examples and saw how contextual search helps us find themes and terms which we did not anticipate with our query.", "Finally, we argued that some of the shortcomings of the approach \u2014 lack of explainability as well as that domain-specific queries may may not work well.", "All that remains is to show the technical implementation and considerations around building such a tool \u2014which we see in Part 3", "Hopefully, this was useful. Thank you for reading. If you feel like saying Hi do reach out via LinkedIn", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F5128cae7ce8d&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsearch-pt-2-semantic-horse-race-5128cae7ce8d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsearch-pt-2-semantic-horse-race-5128cae7ce8d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsearch-pt-2-semantic-horse-race-5128cae7ce8d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsearch-pt-2-semantic-horse-race-5128cae7ce8d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----5128cae7ce8d--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5128cae7ce8d--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@mihail.dungarov?source=post_page-----5128cae7ce8d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@mihail.dungarov?source=post_page-----5128cae7ce8d--------------------------------", "anchor_text": "Mihail Dungarov"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff882548c947b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsearch-pt-2-semantic-horse-race-5128cae7ce8d&user=Mihail+Dungarov&userId=f882548c947b&source=post_page-f882548c947b----5128cae7ce8d---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5128cae7ce8d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsearch-pt-2-semantic-horse-race-5128cae7ce8d&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5128cae7ce8d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsearch-pt-2-semantic-horse-race-5128cae7ce8d&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@noahsilliman?utm_source=medium&utm_medium=referral", "anchor_text": "Noah Silliman"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://towardsdatascience.com/search-pt-1-a-gentle-introduction-335656c0f814", "anchor_text": "Pt 1 \u2014 A gentle introduction"}, {"url": "https://medium.com/@mihail.dungarov/elastic-transformers-ae011e8f5b88", "anchor_text": "Pt 3 (Elastic Transformers)"}, {"url": "https://gifox.io/", "anchor_text": "gifox"}, {"url": "https://en.wikipedia.org/wiki/John_Rupert_Firth", "anchor_text": "Firth, J. R"}, {"url": "https://allennlp.org/elmo", "anchor_text": "ELMo"}, {"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "BERT"}, {"url": "http://jalammar.github.io/illustrated-bert/", "anchor_text": "here"}, {"url": "https://github.com/huggingface/transformers", "anchor_text": "transformers"}, {"url": "https://github.com/UKPLab/sentence-transformers", "anchor_text": "sentence-transformers"}, {"url": "https://arxiv.org/abs/1908.10084", "anchor_text": "SBERT"}, {"url": "https://arxiv.org/abs/1908.10084", "anchor_text": "paper"}, {"url": "https://arxiv.org/abs/1803.11175", "anchor_text": "USE"}, {"url": "https://arxiv.org/abs/1705.02364", "anchor_text": "InferSent"}, {"url": "https://arxiv.org/abs/2007.12603", "anchor_text": "IR-BERT"}, {"url": "http://trec-news.org/guidelines-2020.pdf", "anchor_text": "here"}, {"url": "https://www.kaggle.com/therohk/million-headlines", "anchor_text": "A Million News Headlines"}, {"url": "https://arxiv.org/pdf/2007.12603.pdf", "anchor_text": "IR-BERT"}, {"url": "https://en.wikipedia.org/wiki/Royal_Commission_into_Misconduct_in_the_Banking,_Superannuation_and_Financial_Services_Industry", "anchor_text": "Royal Commission"}, {"url": "https://medium.com/@mihail.dungarov/elastic-transformers-ae011e8f5b88", "anchor_text": "Pt 3"}, {"url": "https://github.com/PAIR-code/lit", "anchor_text": "released a tool"}, {"url": "https://medium.com/datadriveninvestor/releasing-corona-papers-an-ai-powered-search-engine-to-explore-covid-19-research-4b18d1259491", "anchor_text": "Corona Papers"}, {"url": "https://covid19-research-explorer.appspot.com/", "anchor_text": "COVID Research Explorer"}, {"url": "https://www.semanticscholar.org/cord19", "anchor_text": "CORD-19"}, {"url": "https://medium.com/@mihail.dungarov/elastic-transformers-ae011e8f5b88", "anchor_text": "Part 3"}, {"url": "https://www.linkedin.com/in/mihail-dungarov-cfa-a0291a88", "anchor_text": "LinkedIn"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----5128cae7ce8d---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/search?source=post_page-----5128cae7ce8d---------------search-----------------", "anchor_text": "Search"}, {"url": "https://medium.com/tag/transformers?source=post_page-----5128cae7ce8d---------------transformers-----------------", "anchor_text": "Transformers"}, {"url": "https://medium.com/tag/nlp?source=post_page-----5128cae7ce8d---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----5128cae7ce8d---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5128cae7ce8d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsearch-pt-2-semantic-horse-race-5128cae7ce8d&user=Mihail+Dungarov&userId=f882548c947b&source=-----5128cae7ce8d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5128cae7ce8d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsearch-pt-2-semantic-horse-race-5128cae7ce8d&user=Mihail+Dungarov&userId=f882548c947b&source=-----5128cae7ce8d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5128cae7ce8d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsearch-pt-2-semantic-horse-race-5128cae7ce8d&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5128cae7ce8d--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F5128cae7ce8d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsearch-pt-2-semantic-horse-race-5128cae7ce8d&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----5128cae7ce8d---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----5128cae7ce8d--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----5128cae7ce8d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----5128cae7ce8d--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----5128cae7ce8d--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----5128cae7ce8d--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----5128cae7ce8d--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----5128cae7ce8d--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----5128cae7ce8d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@mihail.dungarov?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@mihail.dungarov?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Mihail Dungarov"}, {"url": "https://medium.com/@mihail.dungarov/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "90 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff882548c947b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsearch-pt-2-semantic-horse-race-5128cae7ce8d&user=Mihail+Dungarov&userId=f882548c947b&source=post_page-f882548c947b--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Ff882548c947b%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsearch-pt-2-semantic-horse-race-5128cae7ce8d&user=Mihail+Dungarov&userId=f882548c947b&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}