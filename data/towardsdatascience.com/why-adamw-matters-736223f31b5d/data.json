{"url": "https://towardsdatascience.com/why-adamw-matters-736223f31b5d", "time": 1682993411.501968, "path": "towardsdatascience.com/why-adamw-matters-736223f31b5d/", "webpage": {"metadata": {"title": "Why AdamW matters. Adaptive optimizers like Adam have\u2026 | by Fabio M. Graetz | Towards Data Science", "h1": "Why AdamW matters", "description": "Ilya Loshchilov and Frank Hutter from the University of Freiburg in Germany recently published their article \u201cFixing Weight Decay Regularization in Adam\u201c in which they demonstrate that L2\u2026"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/abs/1711.05101", "anchor_text": "article", "paragraph_index": 0}, {"url": "http://neuralnetworksanddeeplearning.com/chap3.html#regularization", "anchor_text": "ebook", "paragraph_index": 16}, {"url": "https://arxiv.org/abs/1711.05101", "anchor_text": "article", "paragraph_index": 23}], "all_paragraphs": ["Ilya Loshchilov and Frank Hutter from the University of Freiburg in Germany recently published their article \u201cFixing Weight Decay Regularization in Adam\u201c in which they demonstrate that L2 regularization is significantly less effective for adaptive algorithms than for SGD. They propose an improved version of Adam called AdamW that yields models that generalize much better and is thus able to compete with SGD while training much faster.", "After reading this summary of the article, you will understand how 1) Adam works, 2) what L2 regularization is and why it is used and 3) why the improved version AdamW results in better generalizing models than standard Adam.", "Try to imagine minimizing a cost function f of a neural network like walking down a hillside in the mountains: You initialize the weights of your network randomly which translates to starting at a random point on the mountain. Your goal is to reach a good minimum of the cost function (the valley) as quickly as possible. Before each step you calculate the gradient \u2207 f (determine in which direction the hillside inclines the most) and take a step in the opposite direction: The new weight x(t) (following the notation of the article) is equal to the old weight x(t-1) minus the gradient times the learning rate \u03b1:", "Following this procedure you will eventually reach the valley (or at least a local minimum), however, you might want to take bigger, more daring steps when walking down a meadow where the gradient does not change much \u2014 or smaller steps when climbing down rocks where the gradient constantly changes. Adams does exactly that for you: Big steps when the gradients do not change much and small steps when they vary rapidly (adapting the step size for each weight individually).", "Let us understand how Adam works (ignore the colored parts for now):", "Adam keeps track of (exponential moving) averages of the gradient (called the first moment, from now on denoted as m) and the square of the gradients (called raw second moment, from now on denoted as v).", "In every time step the gradient g=\u2207 f[x(t-1)] is calculated, followed by calculating the moving averages:", "The parameters \u03b21 (i.e. 0.9) and \u03b22 (i.e. 0.999) control how quickly the averages decay meaning how \u201chow far into the past you average over the gradients (squared)\u201d. Read the equations the following way: \u201cThe new average is equal to 0.9 (or 0.999 for the squares of the gradients) times the old average plus 0.1 times the current gradient\u201d. With each time step, the old gradients are multiplied with 0.9 one additional time meaning that they contribute less and less to the moving average.", "Please note that in line 9 and 10 the averages are rescaled by (1-\u03b2^t) where t is the timestep. To understand why this is necessary, consider the first time step and remember that m(0) and v(0) are initialized as 0. This means that the average after the first time step is m(1) = 0.9 \u00b7 0 + 0.1 \u00b7 g(1) = 0.1 \u00b7 g(1). However, the average after the first time step should be exactly g(1), which you get if you divide m(1) by (1\u20130.9\u00b9)=0.1.", "We set \u03b7=1 for simplicity (learning rate schedule multiplier) and put everything together in line 12:", "When descending a step \u201cdown the hill\u201d the step size is adapted by multiplying the learning rate \u03b1 with m(t) and dividing with the root of v(t) (let us ignore the hat ^ at this point).", "Remember that the variance of a random variable x is defined as Var(x) = <x\u00b2>-<x>\u00b2 where < > is the expected value. The exponential moving average of the square of the gradients v(t) is called the uncentered variance because we did not subtract the square of the mean of the gradients.", "The variance quantifies how much the gradients vary around their means. If the gradients stay approximately constant because we are \u201cwalking down a meadow\u201d, the variance of the gradients is approximately 0 and the uncentered variance v(t) approximately equal to m(t)\u00b2. This means that m(t) / sqrt(v(t)) is around 1 and the step \u201cdown the hill\u201d is in the order of \u03b1.", "If on the other hand, the gradients are changing rapidly, sqrt(v(t)) is much larger than m(t) and the step \u201cdown the hill\u201d is therefore much smaller than \u03b1.", "Summing up, this means that Adam is able to adapt step sizes for each individual weight from estimating the first and second moments of the gradients. When the gradients do not change much and \u201cwe do not have to be careful walking down the hill\u201d, the step size is of the order of \u03b1, if they do and \u201cwe need to be careful not to walk in the wrong direction\u201d, the step size is much smaller.", "In the next section, I will explain what L2 regularization is and in the last section, I will summarize the authors\u2019 findings on why Adam with L2 regularization yields models that generalize worse than models trained with SGD and how they propose to fix this problem.", "The idea behind L2 regularization or weight decay is that networks with smaller weights (all other things being equal) are observed to overfit less and generalize better. I suggest that you read Michael Nielsen\u2019s great ebook if you are not familiar with the concept.", "Of course, large weights are still possible but only if they significantly reduce the loss. The rate of the weight decay per step w defines the relative importance of minimizing the original loss function (more important if small w is chosen) and finding small weights (more important if large w is chosen). If you compare the update of the weights as explained before (new weight is equal to old weight minus learning rate times gradient)", "to the version with weight decay", "you will notice the additional term -w x(t-1) that exponentially decays the weights x and thus forces the network to learn smaller weights.", "Often, instead of performing weight decay, a regularized loss function is defined (L2 regularization):", "If you calculate the gradient of this regularized loss function", "you will see that this is equivalent to weight decay if you define w\u2019 = w/\u03b1.", "Common deep learning libraries usually implement the latter L2 regularization. However, the article shows, that this equivalence only holds for SGD and not for adaptive optimizers like Adam!", "In the last section of this post, I will explain why L2 regularization is not equivalent to weight decay for Adam, what the differences between Adam and the proposed AdamW are and why using AdamW gives better generalizing models.", "Let us take another look at the Adam algorithm.", "The violet term in line 6 shows L2 regularization in Adam (not AdamW) as it is usually implemented in deep learning libraries. The regularization term is added to the cost function which is then derived to calculate the gradients g. However, if one adds the weight decay term at this point, the moving averages of the gradient and its square (m and v) keep track not only of the gradients of the loss function but also of the regularization term!", "If we insert line 6, 7 and 8 into line 12 (ignore the hat ^ for now because t is assumed to be large and thus \u03b2^t=0), the update of the weights looks like this:", "As you can see the weight decay is normalized by sqrt(v) as well. If the gradient of a certain weight is large (or is changing a lot), the corresponding v is large too and the weight is regularized less than weights with small and slowly changing gradients! This means that L2 regularization does not work as intended and is not as effective as with SGD which is why SGD yields models that generalize better and has been used for most state-of-the-art results.", "The authors, therefore, suggest an improved version of Adam called AdamW where the weight decay is performed only after controlling the parameter-wise step size (see the green term in line 12). The weight decay or regularization term does not end up in the moving averages and is thus only proportional to the weight itself. The authors show experimentally that AdamW yields better training loss and that the models generalize much better than models trained with Adam allowing the new version to compete with stochastic gradient descent with momentum. This means that in the future researchers and engineers might not have to switch between SGD and Adam as often. Keep this in mind, the next time you train a model :)", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Senior MLOps engineer at Recogni | Machine Learning | Kubernetes | Theoretical Astrophysicist | Bespoke Shoemaking | Berlin"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F736223f31b5d&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-adamw-matters-736223f31b5d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-adamw-matters-736223f31b5d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-adamw-matters-736223f31b5d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-adamw-matters-736223f31b5d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----736223f31b5d--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----736223f31b5d--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@fabiograetz?source=post_page-----736223f31b5d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@fabiograetz?source=post_page-----736223f31b5d--------------------------------", "anchor_text": "Fabio M. Graetz"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffb820388a7e9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-adamw-matters-736223f31b5d&user=Fabio+M.+Graetz&userId=fb820388a7e9&source=post_page-fb820388a7e9----736223f31b5d---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F736223f31b5d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-adamw-matters-736223f31b5d&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F736223f31b5d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-adamw-matters-736223f31b5d&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://arxiv.org/abs/1711.05101", "anchor_text": "article"}, {"url": "http://neuralnetworksanddeeplearning.com/chap3.html#regularization", "anchor_text": "ebook"}, {"url": "https://arxiv.org/abs/1711.05101", "anchor_text": "article"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----736223f31b5d---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----736223f31b5d---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----736223f31b5d---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/data-science?source=post_page-----736223f31b5d---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----736223f31b5d---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F736223f31b5d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-adamw-matters-736223f31b5d&user=Fabio+M.+Graetz&userId=fb820388a7e9&source=-----736223f31b5d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F736223f31b5d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-adamw-matters-736223f31b5d&user=Fabio+M.+Graetz&userId=fb820388a7e9&source=-----736223f31b5d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F736223f31b5d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-adamw-matters-736223f31b5d&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----736223f31b5d--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F736223f31b5d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-adamw-matters-736223f31b5d&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----736223f31b5d---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----736223f31b5d--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----736223f31b5d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----736223f31b5d--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----736223f31b5d--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----736223f31b5d--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----736223f31b5d--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----736223f31b5d--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----736223f31b5d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@fabiograetz?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@fabiograetz?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Fabio M. Graetz"}, {"url": "https://medium.com/@fabiograetz/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "849 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffb820388a7e9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-adamw-matters-736223f31b5d&user=Fabio+M.+Graetz&userId=fb820388a7e9&source=post_page-fb820388a7e9--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fc88e15df57a4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-adamw-matters-736223f31b5d&newsletterV3=fb820388a7e9&newsletterV3Id=c88e15df57a4&user=Fabio+M.+Graetz&userId=fb820388a7e9&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}