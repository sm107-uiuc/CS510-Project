{"url": "https://towardsdatascience.com/the-ingredients-of-machine-learning-algorithms-4d1ca9f5ceec", "time": 1683001276.9511209, "path": "towardsdatascience.com/the-ingredients-of-machine-learning-algorithms-4d1ca9f5ceec/", "webpage": {"metadata": {"title": "The \u2018Ingredients\u2019 of Machine Learning Algorithms | by Ekin Tiu | Towards Data Science", "h1": "The \u2018Ingredients\u2019 of Machine Learning Algorithms", "description": "What\u2019s a cost function, optimization, a model, or an algorithm? The esoteric nuances of machine learning algorithms and terminology can easily overwhelm the machine learning novice. As I was reading\u2026"}, "outgoing_paragraph_urls": [], "all_paragraphs": ["What\u2019s a cost function, optimization, a model, or an algorithm? The esoteric nuances of machine learning algorithms and terminology can easily overwhelm the machine learning novice.", "As I was reading the Deep Learning book by Yoshua Bengio, Aaron Courville, and Ian Goodfellow, I was ecstatic when I reached the section that explained the common \u201crecipe\u201d that almost all machine learning algorithms share \u2014 a dataset, a cost function, an optimization procedure, and a model.", "In this article, I summarize each universal \u2018ingredient\u2019 of machine learning algorithms by dissecting them into their simplest components.", "With these \u2018ingredients\u2019 in mind, you no longer have to view each new machine learning algorithm you encounter as an entity isolated from the others, but rather a unique combination of the four common elements described below.", "There are many types of machine learning algorithms. In this article, we will use the Linear Regression Algorithm to learn about each of the four components.", "The first component of a machine learning model is the dataset.", "Machine learning, as a type of applied statistics, is built on large quantities of data. As a result, your choice of data features, important data fed as input, can significantly influence the performance of your algorithm.", "The art of choosing data features is so important that it has its own term: feature engineering. See the article below for more on feature engineering.", "A dataset of a simple linear regression algorithm could look like this:", "In the Linear Regression example, our specified dataset would be our X values, and our y values (the predictors, and the observed data).", "The model can be thought of as the primary function that accepts your X (input) and returns your y-hat (predicted output).", "Although your model may not always be a function in the traditional mathematical sense, it is very intuitive to think of a model as a function because, given some input, the model will do something with the input to perform the Task (T).", "In the context of a simple linear regression, the model is:", "where y is the predicted output, x is the input, and m and b are model parameters.", "Every model has parameters, variables that help define a unique model, and whose values are estimated as a result of learning from data. For instance, if we had the following simple dataset from section 1,", "our optimal m and b in our linear model would be -2 and 8 respectively, to have a fitted model of y = -2x + 8. The specific values, -2 and 8 make our linear model unique to this dataset.", "Since our dataset is relatively simple, it is easy to determine the parameter values that would result in a model that minimizes error (in this case, the \u2018predicted\u2019 value is = to the \u2018actual value\u2019).", "Consider the dataset like the one below:", "The graph of Fig 2.0 is displayed below.", "Notice that finding the optimal m and b is no longer as straightforward as the previous example. In this case, we would have to estimate the best model parameters, m and b, that fit the data by optimizing a cost function.", "The next universal component is the cost function or loss function, usually denoted as J(\u0398).", "A machine learning algorithm must have some cost function that, when optimized, makes the predictions of the ML algorithm estimate the actual values to the best of its ability. The optimization of the cost function is the process of learning.", "In the most basic sense, a cost function is some function that measures the difference between the observed/actual values and the predicted values based on the model.", "This makes intuitive sense. If our function measures some distance between the observed and predicted values, then, if minimized, the difference between observed and predicted will steadily decrease as the model learns, meaning that our algorithm\u2019s prediction is becoming a better estimate of the actual value.", "Not all cost functions are able to be easily evaluated. However, we may use iterative numerical optimization (see Optimization Procedure) to optimize it.", "There are common cost functions for each type of Task (T).", "In our linear regression example, our cost function can be the mean squared error:", "This cost function measures the difference between the actual data (yi) and the values predicted by the model (mxi + b). We square this difference, and take the mean over the dataset by dividing by the number of data points. We can now use an optimization procedure to find the m and b that minimize the cost.", "Next is the optimization procedure, or the method that is used to minimize or maximize our cost function with respect to our model parameters. Through this optimization procedure, we are estimating the model parameters that make our model perform better.", "There are two main forms of optimization procedures:", "A function can be optimized in closed-form if we can find the exact minima (or maxima) using a finite number of \u2018operations\u2019.", "A very simple example only requires high-school calculus.", "If you have the function, J(w) = w\u00b2 +3w + 2 (shown above), then you can find the exact minima of this function with respect to w by taking the derivative of f(w), and setting it equal to 0 (which are a finite number of operations).", "Iterative numerical optimization is a technique that estimates the optima.", "It is the most common optimization procedure because it often has a lower computational cost than closed-form optimization methods. For this reason, many algorithms will trade 100% accuracy for faster, more efficient estimations of the minima or maxima. Furthermore, many cost functions do not have a closed-form solution!", "Using the same example from closed-form optimization, we can imagine we are trying to optimize the function J(w) = w\u00b2 + 3w + 2. We can imagine choosing a random point on this graph (the model parameters are randomly initialized, so the initial \u2018prediction\u2019 is random, and the initial value of the function is therefore random).", "In this case, we can use Stochastic Gradient Descent. See the following articles for more on SGD:", "It is best to think of this type of iterative optimization as a ball rolling down a hill/valley, as can be visualized in the image above.", "According to the Deep Learning book, \u201cother algorithms such as decision trees and k-means require special-case optimizers because their cost functions have flat regions\u2026 that are inappropriate for minimization by gradient-based optimizers.\u201d", "In our linear regression example, we could apply SGD to our MSE cost function in order to find the optimal m and b.", "Our algorithm would calculate the gradient of the MSE with respect to m and b, and iteratively update m and b until our model\u2019s performance has converged, or until it has reached a threshold of our choosing.", "This is analogous to calculating the derivative of our J(w) function shown in Fig 4.1, and moving w in the opposite direction of the sign of the derivative, bringing us closer to the minima. (slope is positive, w becomes more negative)", "Many have heard of the term backpropagation in the context of deep learning. A common misconception is that backpropagation itself is what makes the model learn. This is not the case. Backpropagation is not the optimization procedure.", "So where does backpropagation fit into the picture?", "Backpropagation is used as a step in the optimization procedure of Stochastic Gradient Descent. To be more precise, it is the technique used to estimate the gradients of the cost function with respect to the model parameters.", "In this article, we\u2019ve dissected the machine learning algorithm into common components.", "I hope you find comfort in the fact that most machine learning algorithms can be broken down into a common set of components. We can now view \u2018new\u2019 machine learning algorithms as mere variations or combinations of the \u2018recipe\u2019, as opposed to an entirely new concept.", "With that said, don\u2019t be afraid to tackle new ML algorithms, and perhaps experiment with your own unique combinations.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "CS @ Stanford University | Stanford ML Group"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F4d1ca9f5ceec&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-ingredients-of-machine-learning-algorithms-4d1ca9f5ceec&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-ingredients-of-machine-learning-algorithms-4d1ca9f5ceec&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-ingredients-of-machine-learning-algorithms-4d1ca9f5ceec&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-ingredients-of-machine-learning-algorithms-4d1ca9f5ceec&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----4d1ca9f5ceec--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4d1ca9f5ceec--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://ekintiu.medium.com/?source=post_page-----4d1ca9f5ceec--------------------------------", "anchor_text": ""}, {"url": "https://ekintiu.medium.com/?source=post_page-----4d1ca9f5ceec--------------------------------", "anchor_text": "Ekin Tiu"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F684d91ee6205&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-ingredients-of-machine-learning-algorithms-4d1ca9f5ceec&user=Ekin+Tiu&userId=684d91ee6205&source=post_page-684d91ee6205----4d1ca9f5ceec---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4d1ca9f5ceec&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-ingredients-of-machine-learning-algorithms-4d1ca9f5ceec&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4d1ca9f5ceec&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-ingredients-of-machine-learning-algorithms-4d1ca9f5ceec&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@danielcgold?utm_source=medium&utm_medium=referral", "anchor_text": "Dan Gold"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://towardsdatascience.com/feature-engineering-what-powers-machine-learning-93ab191bcc2d", "anchor_text": "Feature Engineering: What Powers Machine LearningExtracting Features for Supervised Learningtowardsdatascience.comm"}, {"url": "https://towardsdatascience.com/maximum-likelihood-estimation-984af2dcfcac", "anchor_text": "Maximum Likelihood EstimationFundamentals of Machine Learning (Part 2)towardsdatascience.com"}, {"url": "http://sebastianraschka.com/", "anchor_text": "http://sebastianraschka.com/"}, {"url": "https://towardsdatascience.com/understanding-gradient-descent-35a7e3007098", "anchor_text": "Understanding Gradient DescentThe fundamentals of this critical data science tooltowardsdatascience.com"}, {"url": "https://towardsdatascience.com/stochastic-gradient-descent-clearly-explained-53d239905d31", "anchor_text": "Stochastic Gradient Descent \u2014 Clearly Explained !!Stochastic gradient descent is a very popular and common algorithm used in various Machine Learning algorithms, most\u2026towardsdatascience.com"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----4d1ca9f5ceec---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----4d1ca9f5ceec---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/algorithms?source=post_page-----4d1ca9f5ceec---------------algorithms-----------------", "anchor_text": "Algorithms"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----4d1ca9f5ceec---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4d1ca9f5ceec&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-ingredients-of-machine-learning-algorithms-4d1ca9f5ceec&user=Ekin+Tiu&userId=684d91ee6205&source=-----4d1ca9f5ceec---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4d1ca9f5ceec&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-ingredients-of-machine-learning-algorithms-4d1ca9f5ceec&user=Ekin+Tiu&userId=684d91ee6205&source=-----4d1ca9f5ceec---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4d1ca9f5ceec&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-ingredients-of-machine-learning-algorithms-4d1ca9f5ceec&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4d1ca9f5ceec--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F4d1ca9f5ceec&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-ingredients-of-machine-learning-algorithms-4d1ca9f5ceec&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----4d1ca9f5ceec---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----4d1ca9f5ceec--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----4d1ca9f5ceec--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----4d1ca9f5ceec--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----4d1ca9f5ceec--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----4d1ca9f5ceec--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----4d1ca9f5ceec--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----4d1ca9f5ceec--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----4d1ca9f5ceec--------------------------------", "anchor_text": ""}, {"url": "https://ekintiu.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://ekintiu.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ekin Tiu"}, {"url": "https://ekintiu.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.1K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F684d91ee6205&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-ingredients-of-machine-learning-algorithms-4d1ca9f5ceec&user=Ekin+Tiu&userId=684d91ee6205&source=post_page-684d91ee6205--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F70d9799552f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-ingredients-of-machine-learning-algorithms-4d1ca9f5ceec&newsletterV3=684d91ee6205&newsletterV3Id=70d9799552f&user=Ekin+Tiu&userId=684d91ee6205&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}