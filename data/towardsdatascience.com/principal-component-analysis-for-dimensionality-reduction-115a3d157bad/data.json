{"url": "https://towardsdatascience.com/principal-component-analysis-for-dimensionality-reduction-115a3d157bad", "time": 1682996373.463671, "path": "towardsdatascience.com/principal-component-analysis-for-dimensionality-reduction-115a3d157bad/", "webpage": {"metadata": {"title": "Principal Component Analysis for Dimensionality Reduction | by Lorraine Li | Towards Data Science", "h1": "Principal Component Analysis for Dimensionality Reduction", "description": "Learn the concepts and mathematics behind the PCA algorithm and how to implement it both from scratch using Python and with scikit-learn!"}, "outgoing_paragraph_urls": [{"url": "https://c.next.tech/2vZveOh", "anchor_text": "here", "paragraph_index": 4}, {"url": "https://archive.ics.uci.edu/ml/datasets/wine", "anchor_text": "here", "paragraph_index": 14}, {"url": "https://c.next.tech/30EzSiV", "anchor_text": "sandbox", "paragraph_index": 15}, {"url": "https://c.next.tech/2vZveOh", "anchor_text": "here", "paragraph_index": 49}], "all_paragraphs": ["In the modern age of technology, increasing amounts of data are produced and collected. In machine learning, however, too much data can be a bad thing. At a certain point, more features or dimensions can decrease a model\u2019s accuracy since there is more data that needs to be generalized \u2014 this is known as the curse of dimensionality.", "Dimensionality reduction is way to reduce the complexity of a model and avoid overfitting. There are two main categories of dimensionality reduction: feature selection and feature extraction. Via feature selection, we select a subset of the original features, whereas in feature extraction, we derive information from the feature set to construct a new feature subspace.", "In this tutorial we will explore feature extraction. In practice, feature extraction is not only used to improve storage space or the computational efficiency of the learning algorithm, but can also improve the predictive performance by reducing the curse of dimensionality \u2014 especially if we are working with non-regularized models.", "Specifically, we will discuss the Principal Component Analysis (PCA) algorithm used to compress a dataset onto a lower-dimensional feature subspace with the goal of maintaining most of the relevant information. We will explore:", "This tutorial is adapted from Part 2 of Next Tech\u2019s Python Machine Learning series, which takes you through machine learning and deep learning algorithms with Python from 0 to 100. It includes an in-browser sandboxed environment with all the necessary software and libraries pre-installed, and projects using public datasets. You can get started for free here!", "Principal Component Analysis (PCA) is an unsupervised linear transformation technique that is widely used across different fields, most prominently for feature extraction and dimensionality reduction. Other popular applications of PCA include exploratory data analyses and de-noising of signals in stock market trading, and the analysis of genome data and gene expression levels in the field of bioinformatics.", "PCA helps us to identify patterns in data based on the correlation between features. In a nutshell, PCA aims to find the directions of maximum variance in high-dimensional data and projects it onto a new subspace with equal or fewer dimensions than the original one.", "The orthogonal axes (principal components) of the new subspace can be interpreted as the directions of maximum variance given the constraint that the new feature axes are orthogonal to each other, as illustrated in the following figure:", "In the preceding figure, x1 and x2 are the original feature axes, and PC1 and PC2 are the principal components.", "If we use PCA for dimensionality reduction, we construct a d x k\u2013dimensional transformation matrix W that allows us to map a sample vector x onto a new k\u2013dimensional feature subspace that has fewer dimensions than the original d\u2013dimensional feature space:", "As a result of transforming the original d-dimensional data onto this new k-dimensional subspace (typically k \u226a d), the first principal component will have the largest possible variance, and all consequent principal components will have the largest variance given the constraint that these components are uncorrelated (orthogonal) to the other principal components \u2014 even if the input features are correlated, the resulting principal components will be mutually orthogonal (uncorrelated).", "Note that the PCA directions are highly sensitive to data scaling, and we need to standardize the features prior to PCA if the features were measured on different scales and we want to assign equal importance to all features.", "Before looking at the PCA algorithm for dimensionality reduction in more detail, let\u2019s summarize the approach in a few simple steps:", "Let\u2019s perform a PCA step by step, using Python as a learning exercise. Then, we will see how to perform a PCA more conveniently using scikit-learn.", "We will be using the Wine dataset from The UCI Machine Learning Repository in our example. This dataset consists of 178 wine samples with 13 features describing their different chemical properties. You can find out more here.", "In this section we will tackle the first four steps of a PCA; later we will go over the last three. You can follow along with the code in this tutorial by using a Next Tech sandbox, which has all the necessary libraries pre-installed, or if you\u2019d prefer, you can run the snippets in your own local environment.", "Once your sandbox loads, we will start by loading the Wine dataset directly from the repository:", "Next, we will process the Wine data into separate training and test sets \u2014 using a 70:30 split \u2014 and standardize it to unit variance:", "After completing the mandatory preprocessing, let\u2019s advance to the second step: constructing the covariance matrix. The symmetric d x d-dimensional covariance matrix, where d is the number of dimensions in the dataset, stores the pairwise covariances between the different features. For example, the covariance between two features x_j and x_k on the population level can be calculated via the following equation:", "Here, \u03bc_j and \u03bc_k are the sample means of features j and k, respectively.", "Note that the sample means are zero if we standardized the dataset. A positive covariance between two features indicates that the features increase or decrease together, whereas a negative covariance indicates that the features vary in opposite directions. For example, the covariance matrix of three features can then be written as follows (note that \u03a3 stands for the Greek uppercase letter sigma, which is not to be confused with the sum symbol):", "The eigenvectors of the covariance matrix represent the principal components (the directions of maximum variance), whereas the corresponding eigenvalues will define their magnitude. In the case of the Wine dataset, we would obtain 13 eigenvectors and eigenvalues from the 13 x 13-dimensional covariance matrix.", "Now, for our third step, let\u2019s obtain the eigenpairs of the covariance matrix. An eigenvector v satisfies the following condition:", "Here, \u03bb is a scalar: the eigenvalue. Since the manual computation of eigenvectors and eigenvalues is a somewhat tedious and elaborate task, we will use the linalg.eig function from NumPy to obtain the eigenpairs of the Wine covariance matrix:", "Using the numpy.cov function, we computed the covariance matrix of the standardized training dataset. Using the linalg.eig function, we performed the eigendecomposition, which yielded a vector (eigen_vals) consisting of 13 eigenvalues and the corresponding eigenvectors stored as columns in a 13 x 13-dimensional matrix (eigen_vecs).", "Since we want to reduce the dimensionality of our dataset by compressing it onto a new feature subspace, we only select the subset of the eigenvectors (principal components) that contains most of the information (variance). The eigenvalues define the magnitude of the eigenvectors, so we have to sort the eigenvalues by decreasing magnitude; we are interested in the top k eigenvectors based on the values of their corresponding eigenvalues.", "But before we collect those k most informative eigenvectors, let\u2019s plot the variance explained ratios of the eigenvalues. The variance explained ratio of an eigenvalue \u03bb_j is simply the fraction of an eigenvalue \u03bb_j and the total sum of the eigenvalues:", "Using the NumPy cumsum function, we can then calculate the cumulative sum of explained variances, which we will then plot via matplotlib\u2019s step function:", "The resulting plot indicates that the first principal component alone accounts for approximately 40% of the variance. Also, we can see that the first two principal components combined explain almost 60% of the variance in the dataset.", "After we have successfully decomposed the covariance matrix into eigenpairs, let\u2019s now proceed with the last three steps of PCA to transform the Wine dataset onto the new principal component axes.", "We will sort the eigenpairs by descending order of the eigenvalues, construct a projection matrix from the selected eigenvectors, and use the projection matrix to transform the data onto the lower-dimensional subspace.", "We start by sorting the eigenpairs by decreasing order of the eigenvalues:", "Next, we collect the two eigenvectors that correspond to the two largest eigenvalues, to capture about 60% of the variance in this dataset. Note that we only chose two eigenvectors for the purpose of illustration, since we are going to plot the data via a two-dimensional scatter plot later in this subsection. In practice, the number of principal components has to be determined by a trade-off between computational efficiency and the performance of the classifier:", "By executing the preceding code, we have created a 13 x 2-dimensional projection matrix W from the top two eigenvectors.", "Using the projection matrix, we can now transform a sample x (represented as a 1 x 13-dimensional row vector) onto the PCA subspace (the principal components one and two) obtaining x\u2032, now a two-dimensional sample vector consisting of two new features:", "Similarly, we can transform the entire 124 x 13-dimensional training dataset onto the two principal components by calculating the matrix dot product:", "Lastly, let\u2019s visualize the transformed Wine training set, now stored as an 124 x 2-dimensional matrix, in a two-dimensional scatterplot:", "As we can see in the resulting plot, the data is more spread along the x-axis \u2014 the first principal component \u2014 than the second principal component (y-axis), which is consistent with the explained variance ratio plot that we created previously. However, we can intuitively see that a linear classifier will likely be able to separate the classes well.", "Although we encoded the class label information for the purpose of illustration in the preceding scatter plot, we have to keep in mind that PCA is an unsupervised technique that does not use any class label information.", "Although the verbose approach in the previous subsection helped us to follow the inner workings of PCA, we will now discuss how to use the PCA class implemented in scikit-learn. The PCA class is another one of scikit-learn\u2019s transformer classes, where we first fit the model using the training data before we transform both the training data and the test dataset using the same model parameters.", "Let\u2019s use the PCA class on the Wine training dataset, classify the transformed samples via logistic regression:", "Now, using a custom plot_decision_regions function, we will visualize the decision regions:", "By executing the preceding code, we should now see the decision regions for the training data reduced to two principal component axes.", "For the sake of completeness, let\u2019s plot the decision regions of the logistic regression on the transformed test dataset as well to see if it can separate the classes well:", "After we plotted the decision regions for the test set by executing the preceding code, we can see that logistic regression performs quite well on this small two-dimensional feature subspace and only misclassifies very few samples in the test dataset.", "If we are interested in the explained variance ratios of the different principal components, we can simply initialize the PCA class with the n_components parameter set to None, so all principal components are kept and the explained variance ratio can then be accessed via the explained_variance_ratio_ attribute:", "Note that we set n_components=None when we initialized the PCA class so that it will return all principal components in a sorted order instead of performing a dimensionality reduction.", "I hope you enjoyed this tutorial on principal component analysis for dimensionality reduction! We covered the mathematics behind the PCA algorithm, how to perform PCA step-by-step with Python, and how to implement PCA using scikit-learn. Other techniques for dimensionality reduction are Linear Discriminant Analysis (LDA) and Kernel PCA (used for non-linearly separable data).", "These other techniques and more topics to improve model performance, such as data preprocessing, model evaluation, hyperparameter tuning, and ensemble learning techniques are covered in Next Tech\u2019s Python Machine Learning (Part 2) course.", "You can get started here for free!", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F115a3d157bad&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprincipal-component-analysis-for-dimensionality-reduction-115a3d157bad&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprincipal-component-analysis-for-dimensionality-reduction-115a3d157bad&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprincipal-component-analysis-for-dimensionality-reduction-115a3d157bad&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprincipal-component-analysis-for-dimensionality-reduction-115a3d157bad&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----115a3d157bad--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----115a3d157bad--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@lorrli?source=post_page-----115a3d157bad--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@lorrli?source=post_page-----115a3d157bad--------------------------------", "anchor_text": "Lorraine Li"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F37b1fe31b4bf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprincipal-component-analysis-for-dimensionality-reduction-115a3d157bad&user=Lorraine+Li&userId=37b1fe31b4bf&source=post_page-37b1fe31b4bf----115a3d157bad---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F115a3d157bad&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprincipal-component-analysis-for-dimensionality-reduction-115a3d157bad&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F115a3d157bad&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprincipal-component-analysis-for-dimensionality-reduction-115a3d157bad&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://c.next.tech/2vZveOh", "anchor_text": "here"}, {"url": "https://archive.ics.uci.edu/ml/datasets/wine", "anchor_text": "here"}, {"url": "https://c.next.tech/30EzSiV", "anchor_text": "sandbox"}, {"url": "https://c.next.tech/2vZveOh", "anchor_text": "here"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----115a3d157bad---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/tutorial?source=post_page-----115a3d157bad---------------tutorial-----------------", "anchor_text": "Tutorial"}, {"url": "https://medium.com/tag/dimensionality-reduction?source=post_page-----115a3d157bad---------------dimensionality_reduction-----------------", "anchor_text": "Dimensionality Reduction"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----115a3d157bad---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/python?source=post_page-----115a3d157bad---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F115a3d157bad&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprincipal-component-analysis-for-dimensionality-reduction-115a3d157bad&user=Lorraine+Li&userId=37b1fe31b4bf&source=-----115a3d157bad---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F115a3d157bad&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprincipal-component-analysis-for-dimensionality-reduction-115a3d157bad&user=Lorraine+Li&userId=37b1fe31b4bf&source=-----115a3d157bad---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F115a3d157bad&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprincipal-component-analysis-for-dimensionality-reduction-115a3d157bad&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----115a3d157bad--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F115a3d157bad&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprincipal-component-analysis-for-dimensionality-reduction-115a3d157bad&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----115a3d157bad---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----115a3d157bad--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----115a3d157bad--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----115a3d157bad--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----115a3d157bad--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----115a3d157bad--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----115a3d157bad--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----115a3d157bad--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----115a3d157bad--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@lorrli?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@lorrli?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Lorraine Li"}, {"url": "https://medium.com/@lorrli/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "994 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F37b1fe31b4bf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprincipal-component-analysis-for-dimensionality-reduction-115a3d157bad&user=Lorraine+Li&userId=37b1fe31b4bf&source=post_page-37b1fe31b4bf--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Feeded4fba6ef&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprincipal-component-analysis-for-dimensionality-reduction-115a3d157bad&newsletterV3=37b1fe31b4bf&newsletterV3Id=eeded4fba6ef&user=Lorraine+Li&userId=37b1fe31b4bf&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}