{"url": "https://towardsdatascience.com/generalization-regularization-overfitting-bias-and-variance-in-machine-learning-aa942886b870", "time": 1683002390.504978, "path": "towardsdatascience.com/generalization-regularization-overfitting-bias-and-variance-in-machine-learning-aa942886b870/", "webpage": {"metadata": {"title": "Understanding Generalization, Regularization, Overfitting, Bias, and Variance in Machine Learning | by Andreas St\u00f6ffelbauer | Towards Data Science", "h1": "Understanding Generalization, Regularization, Overfitting, Bias, and Variance in Machine Learning", "description": "How do you know if a machine learning model is actually learning something useful? It\u2019s not as plain as it may seem, and it\u2019s definitely worth taking a closer look. To begin with, this post is about\u2026"}, "outgoing_paragraph_urls": [], "all_paragraphs": ["How do you know if a machine learning model is actually learning something useful? It\u2019s not as plain as it may seem, and it\u2019s definitely worth taking a closer look. To begin with, this post is about the kind of machine learning that is explained in, for example, the classic book Elements of Statistical Learning. These models usually learn by computing derivatives with respect to a loss function and moving its parameters step-by-step in the right direction, or some similar idea of statistical learning. In that context, \u201clearning\u201d has virtually nothing to do with human learning. Even if some algorithms were inspired by the human brain, they work in very different ways. Making that distinction is important because the word \u201clearn\u201d may falsely suggest that such a model can really \u201clearn to understand\u201d the data it\u2019s processing like human beings do \u2013 but it cannot. The only thing it may learn is statistical patterns, not more and not less.", "The ultimate goal of machine learning is to find statistical patterns in a training set that generalize to data outside the training set. Take the following simple NLP problem: Say you want to predict a word in a sequence given its preceding words.", "For example, the sequence \u201cthe cat ___\u201d may be followed by sleeps, enjoys, or wants. Your training sequences probably include many such noun-verb combinations, others might be\u201cshe has\u201d, \u201can expert says\u201d, \u201cit seems\u201d, and so on. Note that all these verbs have a third person -s because they refer to the subject of the sequence. A good language model can indeed pick up this pattern and make correct predictions even on sequences it was not trained on. Given as input \u201cthe plane ___\u201d, for instance, we would expect the model to predict departs with a higher probability than depart, and leaves with a higher probability than leave. In that case, the model has learned a pattern that is more generally true. Thumbs up. Yet the model does not understand the meaning of words, not to mention language in general. If we, for instance, provide \u201ctheee plaaaane ___\u201d as input to the model, it will totally screw up.", "It\u2019s usually a good idea to plot the learning process. First, take a look at the training loss. It should approach zero, or at least a reasonably low value (see graphic below). If not, your model is probably either not a good fit, is not flexible enough, or \u2014 and that is often forgotten \u2014 the output is not a function of the input data so there is no relationship to learn. Think about whether a sentence is really a function of words, for instance. It may not be true unconditionally, but can still be useful in some ways.", "If training loss in fact does decrease as expected, it doesn\u2019t automatically mean that whatever the model has learned is also useful. This is where the validation loss comes into play. Things look good if the validation loss decreases alongside the training loss. In that case, the learned patterns seem to generalize to the unseen validation data. The validation loss will typically be higher than the training loss, however, since not all patterns generalize, as you can see in the following graphic.", "Bias is defined as the average squared difference between predictions and true values. It\u2019s a measure of how good your model fits the data. Zero bias would mean that the model captures the true data generating process perfectly. Both your training and validation loss would go to zero. That is unrealistic, however, as data is almost always noisy in reality, so some bias is inevitable \u2014 called the irreducible error.", "Anyway, if losses do not decrease as expected, it probably signals that the model is not a good fit for the data. It would happen, for example, if you tried to fit an exponential relationship with a linear model \u2014 it can simply not adequately capture that relationship. Just try a different, more flexible model in that case.", "You may also call this underfitting, with a slightly different connotation, though. Unlike bias, underfitting would imply that the model has still capacity to learn, so you would simply train for more iterations or collect more data.", "Importantly, biases may also be hidden in the training data \u2014 which is easily overlooked. Your training loss may decrease as usual in that case. Only testing on real data can reveal any such bias.", "A model is said to have high variance if its predictions are sensitive to small changes in the input. In other words, you can think of it as the surface between the data points not being smooth but very wiggly. That is usually not what you want. High variance often means overfitting because the model seems to have captured random noise or outliers.", "Like high bias and underfitting, high variance and overfitting are related as well but are still not totally equivalent in meaning. See below.", "At some point during the training of a model, the validation loss usually levels out (and sometimes even starts to increase again) while the training loss continues to decrease. That signals overfitting. In other words, the model is still learning patterns but they do not generalize beyond the training set (see graphic below). Overfitting is particularly typical for models that have a large number of parameters, like deep neural networks.", "A large gap between training and validation loss is a hint that the model does not generalize well and you may want to try to narrow that gap (graphic below). The simplest solution to overfitting is early-stopping, that is to stop the training loop as soon as validation loss is beginning to level off. Alternatively, regularization may help (see below). Underfitting, on the other hand, may happen if you stop too early.", "Regularization is a method to avoid high variance and overfitting as well as to increase generalization. Without getting into details, regularization aims to keep coefficients close to zero. Intuitively, it follows that the function the model represents is simpler, less unsteady. So predictions are smoother and overfitting is less likely (graphic below). Regularization can be as simple as shrinking or penalizing large coefficients \u2014 often called weight decay. L1 and L2 regularization are two widely used methods. But you may also encounter different forms, such as dropout regularization in neural networks.", "To sum it all up, learning is well and good, but generalization is what we really want. For that matter, a good model is supposed to have both low bias and low variance. Overfitting and underfitting should both be avoided as well. And regularization may be part of the solution to all of that.", "Don\u2019t be surprised if real plots look very different from the ones presented here. They are only supposed to visualize the concepts and should help understand them. Real loss curves will sometimes deviate in some way or another. Nevertheless, it\u2019s good to know what to look out for when training a machine learning model.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "@ Microsoft | @ London School of Economics (LSE)"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Faa942886b870&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgeneralization-regularization-overfitting-bias-and-variance-in-machine-learning-aa942886b870&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgeneralization-regularization-overfitting-bias-and-variance-in-machine-learning-aa942886b870&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgeneralization-regularization-overfitting-bias-and-variance-in-machine-learning-aa942886b870&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgeneralization-regularization-overfitting-bias-and-variance-in-machine-learning-aa942886b870&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----aa942886b870--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----aa942886b870--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@andreas.stoeffelbauer?source=post_page-----aa942886b870--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@andreas.stoeffelbauer?source=post_page-----aa942886b870--------------------------------", "anchor_text": "Andreas St\u00f6ffelbauer"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb06af985ef2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgeneralization-regularization-overfitting-bias-and-variance-in-machine-learning-aa942886b870&user=Andreas+St%C3%B6ffelbauer&userId=b06af985ef2&source=post_page-b06af985ef2----aa942886b870---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Faa942886b870&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgeneralization-regularization-overfitting-bias-and-variance-in-machine-learning-aa942886b870&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Faa942886b870&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgeneralization-regularization-overfitting-bias-and-variance-in-machine-learning-aa942886b870&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@scottwebb?utm_source=medium&utm_medium=referral", "anchor_text": "Scott Webb"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----aa942886b870---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/generalization?source=post_page-----aa942886b870---------------generalization-----------------", "anchor_text": "Generalization"}, {"url": "https://medium.com/tag/regularization?source=post_page-----aa942886b870---------------regularization-----------------", "anchor_text": "Regularization"}, {"url": "https://medium.com/tag/overfitting?source=post_page-----aa942886b870---------------overfitting-----------------", "anchor_text": "Overfitting"}, {"url": "https://medium.com/tag/bias?source=post_page-----aa942886b870---------------bias-----------------", "anchor_text": "Bias"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Faa942886b870&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgeneralization-regularization-overfitting-bias-and-variance-in-machine-learning-aa942886b870&user=Andreas+St%C3%B6ffelbauer&userId=b06af985ef2&source=-----aa942886b870---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Faa942886b870&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgeneralization-regularization-overfitting-bias-and-variance-in-machine-learning-aa942886b870&user=Andreas+St%C3%B6ffelbauer&userId=b06af985ef2&source=-----aa942886b870---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Faa942886b870&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgeneralization-regularization-overfitting-bias-and-variance-in-machine-learning-aa942886b870&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----aa942886b870--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Faa942886b870&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgeneralization-regularization-overfitting-bias-and-variance-in-machine-learning-aa942886b870&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----aa942886b870---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----aa942886b870--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----aa942886b870--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----aa942886b870--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----aa942886b870--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----aa942886b870--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----aa942886b870--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----aa942886b870--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----aa942886b870--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@andreas.stoeffelbauer?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@andreas.stoeffelbauer?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Andreas St\u00f6ffelbauer"}, {"url": "https://medium.com/@andreas.stoeffelbauer/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "172 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb06af985ef2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgeneralization-regularization-overfitting-bias-and-variance-in-machine-learning-aa942886b870&user=Andreas+St%C3%B6ffelbauer&userId=b06af985ef2&source=post_page-b06af985ef2--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F5663ada0bace&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgeneralization-regularization-overfitting-bias-and-variance-in-machine-learning-aa942886b870&newsletterV3=b06af985ef2&newsletterV3Id=5663ada0bace&user=Andreas+St%C3%B6ffelbauer&userId=b06af985ef2&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}