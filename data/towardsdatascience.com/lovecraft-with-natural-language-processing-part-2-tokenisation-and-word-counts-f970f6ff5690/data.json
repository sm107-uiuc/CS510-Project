{"url": "https://towardsdatascience.com/lovecraft-with-natural-language-processing-part-2-tokenisation-and-word-counts-f970f6ff5690", "time": 1683008923.3749309, "path": "towardsdatascience.com/lovecraft-with-natural-language-processing-part-2-tokenisation-and-word-counts-f970f6ff5690/", "webpage": {"metadata": {"title": "Lovecraft with NLP: Tokenisation and Word Counts | Towards Data Science", "h1": "Lovecraft with Natural Language Processing \u2014 Part 2: Tokenisation and Word Counts", "description": "Using spaCy, a Python NLP library, to analyse word usage in H. P. Lovecraft's stories."}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/lovecraft-with-natural-language-processing-part-1-rule-based-sentiment-analysis-5727e774e524", "anchor_text": "first post", "paragraph_index": 0}, {"url": "https://arkhamarchivist.com/wordcount-lovecraft-favorite-words/", "anchor_text": "separate page dedicated", "paragraph_index": 3}, {"url": "https://spacy.io/", "anchor_text": "spaCy", "paragraph_index": 4}, {"url": "https://towardsdatascience.com/pdf-text-extraction-in-python-5b6ab9e92dd", "anchor_text": "text pre-work", "paragraph_index": 6}, {"url": "https://en.wikipedia.org/wiki/Apostrophe", "anchor_text": "apostrophes", "paragraph_index": 7}, {"url": "https://howtowritelike.com/2019/02/06/how-to-write-like-h-p-lovecraft/", "anchor_text": "almost never used contractions", "paragraph_index": 8}, {"url": "https://www.nltk.org/", "anchor_text": "NLTK", "paragraph_index": 9}, {"url": "https://spacy.io/", "anchor_text": "spaCy", "paragraph_index": 20}, {"url": "https://spacy.io/usage/models", "anchor_text": "models available", "paragraph_index": 21}, {"url": "https://spacy.io/api/doc#_title", "anchor_text": "Doc", "paragraph_index": 24}, {"url": "https://spacy.io/api/token#_title", "anchor_text": "Token", "paragraph_index": 25}, {"url": "https://spacy.io/api/token#attributes", "anchor_text": "here", "paragraph_index": 25}, {"url": "https://spacy.io/usage/visualizers", "anchor_text": "documentation", "paragraph_index": 29}, {"url": "https://spacy.io/api/doc#ents", "anchor_text": "ents", "paragraph_index": 36}, {"url": "https://github.com/MatePocs/lovecraft/blob/master/results/word_counts/most_frequent_words/most_frequent_nouns.csv", "anchor_text": "nouns", "paragraph_index": 58}, {"url": "https://github.com/MatePocs/lovecraft/blob/master/results/word_counts/most_frequent_words/most_frequent_adjs.csv", "anchor_text": "adjectives", "paragraph_index": 58}, {"url": "https://github.com/MatePocs/lovecraft/blob/master/results/word_counts/most_frequent_words/most_frequent_verbs.csv", "anchor_text": "verbs", "paragraph_index": 58}, {"url": "https://arkhamarchivist.com/wordcount-lovecraft-favorite-words/", "anchor_text": "Arkham Archivist", "paragraph_index": 67}, {"url": "https://en.wikipedia.org/wiki/N-gram#:~:text=In%20the%20fields%20of%20computational,a%20text%20or%20speech%20corpus.", "anchor_text": "n-grams", "paragraph_index": 68}], "all_paragraphs": ["This is the second blog post in my series in which I analyse the works of H. P. Lovecraft through the lens of Natural Language Processing. In the first post, we kicked things off by analysing the overall sentiment of the stories. We can\u2019t postpone this any longer, we have to talk about the basis of all NLP analysis: tokenisation. I thought it would be fun to pair this with a particular question, so in this post, we are going to find which words Lovecraft used the most in each story, and in his literary work combined.", "One of the things you remember the most about reading Lovecraft is the odd language. He tends to use lots of negative words, horror, madness, depression, especially with adjectives: unnamable, indescribable, unspeakable horrors everywhere, right? There is something about this section from The Outsider (1921):", "God knows it was not of this world \u2014 or no longer of this world \u2014 yet to my horror I saw in its eaten-away and bone-revealing outlines a leering, abhorrent travesty on the human shape; and in its mouldy, disintegrating apparel an unspeakable quality that chilled me even more.", "that screams Lovecraft. On Arkham Archivist (the website from where I downloaded the text), there is a separate page dedicated to word counts, people submitted suggestions and the person who collected the stories counted them. I was curious to see if this notion can be observed \u201cscientifically\u201d. Are the stories as negative as we thought? What are the most used adjectives, are they \u201chorrible\u201d and \u201cunknown\u201d and \u201cancient\u201d? Are verbs about knowledge and/or going crazy? Does he ever use the word \u201cwoman\u201d? Well, let\u2019s find out!", "We are going to start with a quick theory background, apply some practical prep-work on the text, have a look at a couple of examples in spaCy, and then finally get on with word counting.", "First, let\u2019s have a look at a few concepts that we are going to use in this project.", "When I first did the text pre-work for this project, I suspected that I will have to circle back and do some additional tweaking, which turned out to b true \u2014 as soon as I started playing with the tokenisation, I ran into problems. For example, would you guess that these two sentences:", "will be lemmatised differently? The reason is the apostrophes. Generally, you want the \u201cn\u2019t\u201d part of a word to be separated from the base. So \u201cisn\u2019t\u201d becomes \u201cis\u201d and \u201cnot\u201d. But it does not work with all apostrophes. Similar issues with hyphens, dashes, quotation marks \u2014 you wouldn\u2019t imagine how many different characters exist for the same exact purpose!", "(Note: Lovecraft almost never used contractions because he saw them as a sure sign of lack of proper education. However, he sprinkles them in sometimes in dialogues, to show \u2014 and also tell \u2014 how uneducated that person is. He did use a lot of dashes though.)", "So before we do the investigation, we need to hunt down these weird characters. I think it\u2019s important to talk a bit about these steps, even if it might seem like a bit boring, because (in my personal experience) a lot of NLP guides skip it entirely and just start with a clean text. There might be methods for this in large NLP libraries like NLTK, it might be a Unicode decoding issue, but to me, manually adjusting the text seemed like the most logical solution.", "I started with analysing the characters in one story, The Dunwich Horror (1928), assumed to be saved in the text variable. Excluded whitespace and made it lower-case:", "Then you can create a string of unique characters like so:", "If you want to print them as a string, you can simply join the set", "In the case of Dunwich, these were the unique characters:", "Then, I created a string called regular_characters, which included letters from the English alphabet, numbers, punctuation, brackets, regular quotation marks and apostrophes. We loop through them and remove the regular characters from the unique characters, so what we are left with are going to be the irregular characters.", "These are the characters that are left:", "is FALSE for some reason, and at that point, we already excluded what I thought to be the regular hyphen and dash! Similar issue with quotation marks, \u2018\u2016\u2019 being one of them. (That\u2019s not a capital I, those are two vertical lines that were quotation marks in the text.) The non-English Latin letters are OK, their usage is legit in the text, in case you are wondering:", "I repeated this process for all the stories and collected all the weird characters that I wanted to tweak. I ended up with this replace dictionary:", "And looped through all the text files, applied the replacements:", "and saved them as new TXT files. They should work properly. All right, on to the real thing now.", "We start by installing the NLP library we are going to work with: spaCy.", "We also need to separately install a language model that we are going to use. There are models available for a number of languages, three different ones for English. Most of the guides I found recommend starting with en_core_web_sm, and I figured that would do for a simple project, but after some trial and error, I decided to useen_core_web_lg. (We will see an example of why later on.)", "Please note that it might take some time to download and build the large model.", "Now, we can set up an nlp object that is going to be our NLP model, and create a doc object that is the processed version of the string input:", "doc is a Doc object, it\u2019s basically a container of tokens. So at this point, we already have our text tokenised! Doc keeps all the information from the original text like whitespace, so you can re-build the text from it.", "The tokens in a Doc object are Token objects, with a long list of attributes you can request, see the full list here. For now, we are going to use three attributes:", "To analyse our sentence, we can list the token attributes:", "It seems almost correct, falsely categorised \u201cdoorbell\u201d as a proper noun, as opposed to a noun. This was the large model. The small model performed worse, categorised \u201crang\u201d as a proper noun, and did not find the present tense of the verb. For some reason, this turned out to be a tricky sentence.", "You can also visualise a sentence with the display method, although I think this is mainly useful in an educational environment. If you are working in a Jupyter Notebook, you already have a browser window open, and you can use the render method:", "You can also save these visualisations as SVG files, you can read more about how to do that in the documentation.", "We saw how well spaCy works out of the box with a random sentence. What about really weird sentences with intentional misspellings like this one from Old Bugs (1919):", "Some swell guy, too\u2013\u2019s father\u2019s a big corp\u2019ration lawyer up in his burg, \u2019n\u2019 \u2018s mother\u2019s some lit\u2019ry genius.", "Well, the library struggles, as expected. It more or less gets the part of speech indexes of the tokens, but does not recognise that \u201c \u2019n\u2019 \u201d is supposed to be \u201cand\u201d, \u201c \u2018s \u201c is supposed to be \u201chis\u201d, etc\u2026", "One more thing I want to talk about in the spaCy library is how it recognises named entities in a text. To my surprise, it even works on fictional ones.", "Take the opening sentence from Herbert West \u2014 Reanimator (1922).", "Of Herbert West, who was my friend in college and in after life, I can speak only with extreme terror.", "We can loop through the ents in the Doc object for the named entities:", "Which I found to be a quite impressive feature. Let\u2019s see another sentence from the same story:", "After we list the entities in the same way, we get:", "Almost perfect, although it missed the \u201cMiskatonic\u201d part for some reason.", "This can also be visualised, this time we should set the style to \u201cent\u201d:", "That\u2019s all we need from spaCy for now.", "As a first step, we are concentrating on one story. There are many built-in methods in libraries that do bag-of-words for you automatically, I did not find one that did exactly what I needed, and fortunately, it\u2019s really simple to code it ourselves! Let\u2019s create a number of helper functions first for this project.", "First, we build a create_word_counts_by_pos function that takes in a raw text file, a list of part-of-speech tags, and optionally a word count dictionary in case we want to aggregate multiple stories this way. It will return one dictionary, where the keys are part-of-speech tags, and the values are smaller dictionaries with the words of the corresponding tag as values and their counts in the text as values\u2026 So the result dictionary of word_count_dict will have a word_count_dict[\u2018NOUN\u2019] dictionary inside, which will consist of key-value pairs like \u2018man\u2019: 4, \u2018house\u2019: 28, \u2026, similar structure for word_count_dict[\u2018ADJ\u2019], word_count_dict[\u2018VERB\u2019], etc.", "An important part of this process is that we want to exclude the stop words, which is really simple to do with spaCy, the tokens have an is_stop method that returns a boolean, we only need to consider tokens where this attribute is False.", "Assuming we have a word_count_dict in this format, we might want to filter down to words above a certain threshold. This is what filter_word_count_dict_to_frequent will do, keep the structure of the input dictionary, but delete all elements where the count was below the threshold.", "And finally, we will need some way to order the words so we can know which ones were the most frequent. This is what collect_most_frequent_words will be responsible for, you enter a word count dictionary with a number, and you get that many of the highest frequency words from each part-of-speech category.", "This is all the code we are going to need to accomplish what we just discussed:", "Now that we have these functions, getting the word counts in different structures is very easy. For now, we are only curious about the nouns, proper nouns, adjectives, and verbs:", "Assuming we have all the text in the text variable, to get the word_count_dict:", "word_count_dict will be a long collection of words, any noun, adjective, or verb that appears in the text even once will be represented there. We can filter it down to only words that appear more than 10 times:", "frequent_word_count_dict is concise enough that it does not seem excessive to simply copy it here:", "If you read the story / saw the film, you will probably find a lot of these satisfying. For those of you who did not: the story is basically about a mad scientist who creates zombies, and is on a never-ending quest to find fresh corpses.", "Let\u2019s take the adjectives, the top three adjectives are: \u201cfresh dead human\u201d could be the working title of the story. \u201cspecimen\u201d is used more often than \u201cman\u201d, apparently the narrator who is friends with Herbert West mostly refers to him as \u201cWest\u201d, not as \u201cHerbert\u201d, and the verb \u201cinject\u201d is used suspiciously often.", "Now let\u2019s do this for all the stories!", "I decided to omit the proper nouns from the analysis, and concentrate on the top 5 words per part-of-speech for each story. First, we need to set up some variables:", "Then, we can do a similar process to what we did with the individual story in the previous chapter. We are not using the frequent dictionaries, instead, request a list of the most frequent words. Assuming filenames contains the names of files we want to import, and they are saved in the txt_folder:", "After that\u2019s done, we should have everything we need in the two dictionaries, words and word_counts. I combined them with pandas, for the \u2018NOUN\u2019 group, we can do it like so:", "Repeat the same for adjectives and verbs, and now we have the top 5 words with their counts in the 3 part-of-speech groups for the 63 stories in the corpus. Copying them all here would be a bit much, you can find the data in CSV files here for nouns, adjectives, and verbs.", "Let me just grab a couple of examples that I found to be really interesting.", "The verbs are usually not that closely matching with the story, there are a lot of general verbs out there after all. Plus Lovecraft\u2019s style is very descriptive and distant, and there is hardly any action.", "All right, I\u2019m pretty satisfied so far, I think it\u2019s impressive how such a basic analysis managed to capture the essence of so many of the stories.", "And finally, back to the big question, which words did he use the most often ever? Following a similar process as above, but in every turn, we keep the word_count_dict, so it calculates a combined score. (Combining all the text does not work, it\u2019s roughly 2.5 times more than the memory limit in spaCy.)", "So, drumroll, these are the 20 nouns, adjectives, and verbs that Lovecraft used the most often:", "Once again, I think the verbs are the least exciting, although you can see how most of the verbs are about knowledge gathering or sharing. In case you are wondering, \u201cshew\u201d is the old-fashioned spelling of show.", "Nouns are better, \u201ctime\u201d, \u201cnight\u201d, \u201cstone\u201d, \u201cdream\u201d, \u201chorror\u201d \u2014 I would guess the writer is Lovecraft based on just these five words. I mean, horror is there in the top 20 nouns ever used, that is certainly reassuring!", "And finally, the adjectives, at least 2/3rd of them would generally be weird to have so high on the list. The most often used adjective is \u201cold\u201d, I mean, how awesome is that? And the list goes on to \u201cgreat\u201d, \u201cstrange\u201d, \u201cancient\u201d, \u201cunknown\u201d, \u201cterrible\u201d, etc.", "(You might notice that we have \u201chideous\u201d 213 times, but on the Arkham Archivist page that I mentioned in the intro, there are 260 counts of the word. The difference is due to the fact that we separate word usage by part-of-speech. He used the word 213 times as an adjective, but that does not include \u201chideousness\u201d which is a noun, or \u201chideously\u201d which is an adverb.)", "I think there are many more interesting analysis you could do with these simple NLP tools. We limited our scope to nouns, adjectives, and verbs, it would be interesting to see adverbs or proper nouns too. Analysing the relative portion of let\u2019s say verbs in different years would also be interesting. That way, we could see if the stories became more action-packed as time went by, or was it the other way around? There is also a very important concept we did not consider: the n-grams. A very similar analysis could be done to find the most frequently used expressions in his writings.", "We started with a very simple question: I wonder what is Lovecraft's most frequently used word? And for such a simple question this turned out to be more complicated than I imagined. It was really cool to see such a simple NLP representation to work surprisingly well.", "In the next post, I am going to write about how one can take word counts a step further and calculate the differences between documents using TF-IDF vectors.", "Hobson, L. & Cole, H. & Hannes, H. (2019). Natural Language Processing in Action: Understanding, Analyzing, and Generating Text with Python. Manning Publications, 2019.", "The Complete Works of H. P. Lovecraft:", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Ff970f6ff5690&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flovecraft-with-natural-language-processing-part-2-tokenisation-and-word-counts-f970f6ff5690&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flovecraft-with-natural-language-processing-part-2-tokenisation-and-word-counts-f970f6ff5690&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flovecraft-with-natural-language-processing-part-2-tokenisation-and-word-counts-f970f6ff5690&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flovecraft-with-natural-language-processing-part-2-tokenisation-and-word-counts-f970f6ff5690&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----f970f6ff5690--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f970f6ff5690--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://matepocs.medium.com/?source=post_page-----f970f6ff5690--------------------------------", "anchor_text": ""}, {"url": "https://matepocs.medium.com/?source=post_page-----f970f6ff5690--------------------------------", "anchor_text": "Mate Pocs"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F686b78ddcf4b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flovecraft-with-natural-language-processing-part-2-tokenisation-and-word-counts-f970f6ff5690&user=Mate+Pocs&userId=686b78ddcf4b&source=post_page-686b78ddcf4b----f970f6ff5690---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff970f6ff5690&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flovecraft-with-natural-language-processing-part-2-tokenisation-and-word-counts-f970f6ff5690&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff970f6ff5690&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flovecraft-with-natural-language-processing-part-2-tokenisation-and-word-counts-f970f6ff5690&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://pixabay.com/users/LUM3N-1066559/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=1535201", "anchor_text": "LUM3N"}, {"url": "https://pixabay.com/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=1535201", "anchor_text": "Pixabay"}, {"url": "https://towardsdatascience.com/lovecraft-with-natural-language-processing-part-1-rule-based-sentiment-analysis-5727e774e524", "anchor_text": "first post"}, {"url": "https://arkhamarchivist.com/wordcount-lovecraft-favorite-words/", "anchor_text": "separate page dedicated"}, {"url": "https://spacy.io/", "anchor_text": "spaCy"}, {"url": "https://en.wikipedia.org/wiki/Lexical_analysis#Tokenization", "anchor_text": "tokenisation"}, {"url": "https://en.wikipedia.org/wiki/Bag-of-words_model", "anchor_text": "bag-of-words"}, {"url": "https://en.wikipedia.org/wiki/Stemming", "anchor_text": "normalisation"}, {"url": "https://en.wikipedia.org/wiki/Lemmatisation", "anchor_text": "lemmatisation"}, {"url": "https://en.wikipedia.org/wiki/Lemma_(morphology)", "anchor_text": "lemma"}, {"url": "https://en.wikipedia.org/wiki/Stop_words", "anchor_text": "stop words"}, {"url": "https://towardsdatascience.com/pdf-text-extraction-in-python-5b6ab9e92dd", "anchor_text": "text pre-work"}, {"url": "https://en.wikipedia.org/wiki/Apostrophe", "anchor_text": "apostrophes"}, {"url": "https://howtowritelike.com/2019/02/06/how-to-write-like-h-p-lovecraft/", "anchor_text": "almost never used contractions"}, {"url": "https://www.nltk.org/", "anchor_text": "NLTK"}, {"url": "https://spacy.io/", "anchor_text": "spaCy"}, {"url": "https://spacy.io/usage/models", "anchor_text": "models available"}, {"url": "https://spacy.io/api/doc#_title", "anchor_text": "Doc"}, {"url": "https://spacy.io/api/token#_title", "anchor_text": "Token"}, {"url": "https://spacy.io/api/token#attributes", "anchor_text": "here"}, {"url": "https://universaldependencies.org/docs/u/pos/", "anchor_text": "part-of-speech"}, {"url": "https://spacy.io/usage/visualizers", "anchor_text": "documentation"}, {"url": "https://spacy.io/api/doc#ents", "anchor_text": "ents"}, {"url": "https://github.com/MatePocs/lovecraft/blob/master/results/word_counts/most_frequent_words/most_frequent_nouns.csv", "anchor_text": "nouns"}, {"url": "https://github.com/MatePocs/lovecraft/blob/master/results/word_counts/most_frequent_words/most_frequent_adjs.csv", "anchor_text": "adjectives"}, {"url": "https://github.com/MatePocs/lovecraft/blob/master/results/word_counts/most_frequent_words/most_frequent_verbs.csv", "anchor_text": "verbs"}, {"url": "https://arkhamarchivist.com/wordcount-lovecraft-favorite-words/", "anchor_text": "Arkham Archivist"}, {"url": "https://en.wikipedia.org/wiki/N-gram#:~:text=In%20the%20fields%20of%20computational,a%20text%20or%20speech%20corpus.", "anchor_text": "n-grams"}, {"url": "https://spacy.io/usage/linguistic-features", "anchor_text": "Linguistic Features \u00b7 spaCy Usage DocumentationProcessing raw text intelligently is difficult: most words are rare, and it's common for words that look completely\u2026spacy.io"}, {"url": "https://arkhamarchivist.com/free-complete-lovecraft-ebook-nook-kindle/", "anchor_text": "https://arkhamarchivist.com/free-complete-lovecraft-ebook-nook-kindle/"}, {"url": "https://medium.com/tag/python?source=post_page-----f970f6ff5690---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/nlp?source=post_page-----f970f6ff5690---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/lemmatization?source=post_page-----f970f6ff5690---------------lemmatization-----------------", "anchor_text": "Lemmatization"}, {"url": "https://medium.com/tag/lovecraft?source=post_page-----f970f6ff5690---------------lovecraft-----------------", "anchor_text": "Lovecraft"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff970f6ff5690&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flovecraft-with-natural-language-processing-part-2-tokenisation-and-word-counts-f970f6ff5690&user=Mate+Pocs&userId=686b78ddcf4b&source=-----f970f6ff5690---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff970f6ff5690&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flovecraft-with-natural-language-processing-part-2-tokenisation-and-word-counts-f970f6ff5690&user=Mate+Pocs&userId=686b78ddcf4b&source=-----f970f6ff5690---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff970f6ff5690&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flovecraft-with-natural-language-processing-part-2-tokenisation-and-word-counts-f970f6ff5690&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f970f6ff5690--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Ff970f6ff5690&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flovecraft-with-natural-language-processing-part-2-tokenisation-and-word-counts-f970f6ff5690&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----f970f6ff5690---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----f970f6ff5690--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----f970f6ff5690--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----f970f6ff5690--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----f970f6ff5690--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----f970f6ff5690--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----f970f6ff5690--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----f970f6ff5690--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----f970f6ff5690--------------------------------", "anchor_text": ""}, {"url": "https://matepocs.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://matepocs.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Mate Pocs"}, {"url": "https://matepocs.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "460 Followers"}, {"url": "http://linkedin.com/in/matepocs", "anchor_text": "linkedin.com/in/matepocs"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F686b78ddcf4b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flovecraft-with-natural-language-processing-part-2-tokenisation-and-word-counts-f970f6ff5690&user=Mate+Pocs&userId=686b78ddcf4b&source=post_page-686b78ddcf4b--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fea5ecc8b4148&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flovecraft-with-natural-language-processing-part-2-tokenisation-and-word-counts-f970f6ff5690&newsletterV3=686b78ddcf4b&newsletterV3Id=ea5ecc8b4148&user=Mate+Pocs&userId=686b78ddcf4b&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}