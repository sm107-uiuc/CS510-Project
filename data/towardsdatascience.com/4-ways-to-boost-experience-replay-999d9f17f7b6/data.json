{"url": "https://towardsdatascience.com/4-ways-to-boost-experience-replay-999d9f17f7b6", "time": 1683007052.6258078, "path": "towardsdatascience.com/4-ways-to-boost-experience-replay-999d9f17f7b6/", "webpage": {"metadata": {"title": "4 Ways to Boost Experience Replay | Towards Data Science", "h1": "4 Ways to Boost Experience Replay", "description": "Outlines 4 variants of experience replay for deep reinforcement learning, usually for deep neural network training."}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/pdf/1906.04009.pdf", "anchor_text": "Boosting Soft Actor-Critic: Emphasizing Recent Experience without Forgetting the Past", "paragraph_index": 36}, {"url": "https://openreview.net/pdf?id=r1lyTjAqYX", "anchor_text": "Recurrent Experience Replay in Distributed Reinforcement Learning", "paragraph_index": 37}], "all_paragraphs": ["Experience replay is an essential part of off-policy learning. It allows agents to get the most \u201cbang for their buck,\u201d squeezing out as much information as possible from past experiences. However, sampling uniformly from the replay has proven to have sub-par results compared to more involved sampling methods. In this article, we discuss four variations of experience replay, each of which can boost learning robustness and speed depending on the context.", "Context: Originally designed for the Double DQN algorithm to improve sample efficiency, but naturally adaptable to any RL algorithm.", "PER exploits the fact that agents can learn from certain experiences more than others. Some transitions may be rare but important, so it should be given more attention. Some transitions may be redundant and already learned, so the agent\u2019s time might be better spent learning other things.", "Not all experiences are created equal", "Intuitively, we want to sample points resulting in a higher loss for our model, because the higher the loss, the more room the agent has to learn. As a result, we use a transition\u2019s temporal difference (TD) error to measure its importance.", "However, we don\u2019t want to naively select the transitions with the highest TD error every iteration. Bear in mind that TD errors decrease slowly in iterative algorithms, so we would dedicate a large portion of our time to only a small subset of the replay, running the risk of overfitting. So, we instead propose a sampling method where transition sampling probabilities are proportional to its TD error.", "where alpha is a tunable parameter between 0 and 1. We also define the variable:", "Notice, for lower values of alpha, the closer we get to uniform sampling. Therefore, we can think of alpha as a parameter tuning \u201chow much we want to prioritize transitions with higher TD errors.\u201d", "There\u2019s one issue. Training the agent with these prioritized samples may lead to a bias error, resulting in possible training instability. We correct this by multiplying the transition\u2019s gradient update by a weight defined as:", "where N is the number of samples in the replay and beta is a tunable hyperparameter. Beta is a value between 0 and 1 that represents the extent to which we want to compensate for the bias error. The paper [1] recommends annealing beta to one because unbiased updates are more important towards the end of the training cycle than the beginning. The authors also recommend dividing each weight by the max of all update weights for a \u201cnormalization\u201d effect.", "Context: Distributed reinforcement learning approaches (both synchronous and asynchronous). Although originally proposed for distributed DQN and DPG variations called Ape-X, it naturally fits with any algorithms under the same umbrella.", "As a side note, PER has a variation adapted for distributed settings! Typically, distributed algorithms have several instantiations of the same environment. While we still train a single agent, we give each of these environments a copy of the agent. Then, the experiences from all these environments are pooled into a single replay. Lastly, we sample from the prioritized replay (similar to PER) and train the learning agent, occasionally updating each of the copies with the original\u2019s updated parameters.", "In this way, we can gather more experiences using different strategies. While there are [many] more nuances to Ape-X [2], the idea of using a single, shared experience replay provides many benefits in learning.", "Context: Primarily designed to tackle problems with sparse rewards by encouraging more meaningful exploration, not aimless wandering. Choice of RL algorithm to couple it with is arbitrary.", "Suppose that an agent is given a task with some goal g. Then, suppose that each transition that does not result in our goal state has a reward of -1 and 1 otherwise. This is an environment with sparse rewards (or environments where a large majority of transitions look similar regarding its returns). For vanilla RL algorithms, learning in these environments is incredibly difficult. HER tries to address this issue.", "Say we trained policies and value functions that not only took the state but also a goal. We define the reward function by returning a negative reward r if the goal is not achieved and some arbitrary higher reward otherwise. Then, the transition stored to the experience replay would look like this:", "However, we still have the issue of sparsity! If we rarely reach our goal, we rarely see a change in reward. Here\u2019s the idea behind HER: use subgoals to split the task. In other words, instead of only storing a transition with the overall goal g, we also store the same transition but replace the goal g with some other subgoals g\u2019! That way, the agent sees some reward differentiation more often.", "But, why does this help? How does this work in tasks where there\u2019s only one goal we care about? We can think of generating these subgoals as a way of \u201ccorralling\u201d our agent towards the end goal. We\u2019re leaving a cookie crumb trail for the agent to follow.", "Then, comes the question of how we choose our subgoals. We don\u2019t want to explicitly tell the agent what the subgoals are, because that would require domain-specific knowledge. Plus, it\u2019s not generalizable to multiple tasks.", "The paper proposes selecting k subgoals g\u2019 to replace the goal g. The paper [3] proposes various methods for selecting these subgoals:", "In other words, the entire process of storing transitions into the experience replay looks something like this:", "Testing the effects of HER in the context of robotic arm locomotion, the results for each of these selection schemes are shown in the graph above.", "Context: Originally designed for bolstering Soft Actor Critic (SAC) convergence speed. Can, arguably, be applied to most algorithms and tasks that inherently benefit from learning recent experiences more quickly (like tasks with multiple parts)", "Often, our agent becomes well-adjusted to transitions found earlier in the learning process. As a result, sampling uniformly from these early transitions does not benefit our policy. ERE creates a simple yet powerful sampling method that allows an agent to emphasize recent transitions while not neglecting learned policies from the past.", "Suppose that, in an update phase, we sample K mini-batches. Then, update our model\u2019s parameters for each mini-batch. ERE proposes a scheme where we sample from only a subset of the experience replay instead of the entire thing. In other words, for the kth mini-batch, we sample uniformly from the most recent c_k points:", "where c_min is the minimum number of most recent data points we can sample from, N is the size of our experience replay, and nu is a tuneable hyperparameter determining how much we prioritize recent observations.", "Notice how within an update consisting of K mini-batches, earlier samples allow training on earlier transitions while later samples put more emphasis on recent transitions. In other words, we learn more from recent experiences but never forget the past.", "The paper [4] suggests that setting nu to .996 was a good value for all environments. Furthermore, it is suggested to anneal the value of nu to one as training progresses. This is used to allow quicker training in earlier stages and encourage slower, careful training in later stages.", "Some may question: why not just use PER? Both of these methods seem to be doing similar things, training on data the agent would most benefit from. The beauty in ERE is its simplicity. While PER requires extra implementations (special data structures for computational efficiency), ERE does not. Even though it was shown that combining PER and ERE generally yielded better results, using solely ERE performed similarly or only marginally worse.", "Context: Originally designed for recurrent distributed agents, especially one\u2019s that involve partial observability. However, it can be applied to a large majority of value-based algorithms, whether or not it is distributed.", "Common in many methods, we can train Q-values with n-step lookaheads. In classic Q-learning, targets are produced by using a one-step lookahead; we glanced in the future by one timestep. For n greater than one, we collect experiences in groups of n and \u201cunroll\u201d the transition, getting a more explicit target value for our value update.", "This paper [5] proposes that the agent stores fixed-length (m = 80) sequences of state-action-reward observations. On top of that, we impose:", "Then, when we sample this sequence, we \u201cunroll\u201d value and target networks on the same set of states, generating value estimates and targets for training.", "However, how do we sample these sequences in the first place? This method takes inspiration from Prioritized Experience Replay (PER) but tweaks the criteria by using a weighted sum of two different values. Assuming we use a value n less than m, the first term is the max absolute n-step TD error contained within the m-length sequence. The second is the sequence\u2019s mean absolute n-step TD error:", "where nu is a tuneable hyperparameter between 0 and 1. The paper [5] suggests setting nu to .9, yielding a more aggressive sampling method. It intuitively makes sense to err on the greedier side as averaging tends to nullify and smooth out larger errors, making it difficult to pick out valuable transitions.", "Of course, there are many other sampling schemes and experience replay variations. While we only outline a few here, there are always other versions better acclimated to certain environments than others. We simply have to experiment, explore, and analyze. Even though experience replay isn\u2019t a \u201cmake-it-or-break-it\u201d deal, it can play a large part in the optimality, robustness, and convergent properties of our RL agents. It\u2019s definitely worth giving it a little bit of our attention.", "[4] C. Wang, K. Ross, Boosting Soft Actor-Critic: Emphasizing Recent Experience without Forgetting the Past (2019).", "[5] S. Kapturowski, G. Ostrovski, J. Quan, R. Munos, W. Dabney, Recurrent Experience Replay in Distributed Reinforcement Learning (2019).", "From the classic to state-of-the-art, here are related articles discussing both multi-agent and single-agent reinforcement learning:", "Part-time writer \u00b7 Full-time learner \u00b7 PhD Student @ University of Michigan"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F999d9f17f7b6&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F4-ways-to-boost-experience-replay-999d9f17f7b6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2F4-ways-to-boost-experience-replay-999d9f17f7b6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F4-ways-to-boost-experience-replay-999d9f17f7b6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2F4-ways-to-boost-experience-replay-999d9f17f7b6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/@austinnguyen517?source=post_page-----999d9f17f7b6--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----999d9f17f7b6--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@austinnguyen517?source=post_page-----999d9f17f7b6--------------------------------", "anchor_text": "Austin Nguyen"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6a1b6c5da9ab&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F4-ways-to-boost-experience-replay-999d9f17f7b6&user=Austin+Nguyen&userId=6a1b6c5da9ab&source=post_page-6a1b6c5da9ab----999d9f17f7b6---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----999d9f17f7b6--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F999d9f17f7b6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F4-ways-to-boost-experience-replay-999d9f17f7b6&user=Austin+Nguyen&userId=6a1b6c5da9ab&source=-----999d9f17f7b6---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F999d9f17f7b6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F4-ways-to-boost-experience-replay-999d9f17f7b6&source=-----999d9f17f7b6---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://unsplash.com/@m_b_m?utm_source=medium&utm_medium=referral", "anchor_text": "M. B. M."}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://unsplash.com/@trommelkopf?utm_source=medium&utm_medium=referral", "anchor_text": "Steve Harvey"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://unsplash.com/@sammcghee?utm_source=medium&utm_medium=referral", "anchor_text": "Sam McGhee"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://unsplash.com/@patuphotos?utm_source=medium&utm_medium=referral", "anchor_text": "Patrick Selin"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://unsplash.com/@rolzay?utm_source=medium&utm_medium=referral", "anchor_text": "Rolands Zilvinskis"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://arxiv.org/pdf/1511.05952.pdf", "anchor_text": "Prioritized Experience Replay"}, {"url": "https://arxiv.org/pdf/1803.00933.pdf", "anchor_text": "Distributed Experience Replay"}, {"url": "https://arxiv.org/pdf/1803.00933.pdf", "anchor_text": "Hindsight Experience Replay"}, {"url": "https://arxiv.org/pdf/1906.04009.pdf", "anchor_text": "Boosting Soft Actor-Critic: Emphasizing Recent Experience without Forgetting the Past"}, {"url": "https://openreview.net/pdf?id=r1lyTjAqYX", "anchor_text": "Recurrent Experience Replay in Distributed Reinforcement Learning"}, {"url": "https://towardsdatascience.com/hierarchical-reinforcement-learning-feudal-networks-44e2657526d7", "anchor_text": "Hierarchical Reinforcement Learning: FeUdal NetworksLetting computers see the bigger picturetowardsdatascience.com"}, {"url": "https://towardsdatascience.com/how-deepminds-unreal-agent-performed-9-times-better-than-experts-on-atari-9c6ee538404e", "anchor_text": "How DeepMind\u2019s UNREAL Agent Performed 9 Times Better Than Experts on AtariDeep reinforcement learning at its finesttowardsdatascience.com"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----999d9f17f7b6---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----999d9f17f7b6---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----999d9f17f7b6---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----999d9f17f7b6---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----999d9f17f7b6---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F999d9f17f7b6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F4-ways-to-boost-experience-replay-999d9f17f7b6&user=Austin+Nguyen&userId=6a1b6c5da9ab&source=-----999d9f17f7b6---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F999d9f17f7b6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F4-ways-to-boost-experience-replay-999d9f17f7b6&user=Austin+Nguyen&userId=6a1b6c5da9ab&source=-----999d9f17f7b6---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F999d9f17f7b6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F4-ways-to-boost-experience-replay-999d9f17f7b6&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/@austinnguyen517?source=post_page-----999d9f17f7b6--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----999d9f17f7b6--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6a1b6c5da9ab&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F4-ways-to-boost-experience-replay-999d9f17f7b6&user=Austin+Nguyen&userId=6a1b6c5da9ab&source=post_page-6a1b6c5da9ab----999d9f17f7b6---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F2cb997cef03c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F4-ways-to-boost-experience-replay-999d9f17f7b6&newsletterV3=6a1b6c5da9ab&newsletterV3Id=2cb997cef03c&user=Austin+Nguyen&userId=6a1b6c5da9ab&source=-----999d9f17f7b6---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/@austinnguyen517?source=post_page-----999d9f17f7b6--------------------------------", "anchor_text": "Written by Austin Nguyen"}, {"url": "https://medium.com/@austinnguyen517/followers?source=post_page-----999d9f17f7b6--------------------------------", "anchor_text": "208 Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----999d9f17f7b6--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6a1b6c5da9ab&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F4-ways-to-boost-experience-replay-999d9f17f7b6&user=Austin+Nguyen&userId=6a1b6c5da9ab&source=post_page-6a1b6c5da9ab----999d9f17f7b6---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F2cb997cef03c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F4-ways-to-boost-experience-replay-999d9f17f7b6&newsletterV3=6a1b6c5da9ab&newsletterV3Id=2cb997cef03c&user=Austin+Nguyen&userId=6a1b6c5da9ab&source=-----999d9f17f7b6---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/openais-multi-agent-deep-deterministic-policy-gradients-maddpg-9d2dad34c82?source=author_recirc-----999d9f17f7b6----0---------------------5f70bddd_c468_45b2_b437_7daa2e28c863-------", "anchor_text": ""}, {"url": "https://medium.com/@austinnguyen517?source=author_recirc-----999d9f17f7b6----0---------------------5f70bddd_c468_45b2_b437_7daa2e28c863-------", "anchor_text": ""}, {"url": "https://medium.com/@austinnguyen517?source=author_recirc-----999d9f17f7b6----0---------------------5f70bddd_c468_45b2_b437_7daa2e28c863-------", "anchor_text": "Austin Nguyen"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----999d9f17f7b6----0---------------------5f70bddd_c468_45b2_b437_7daa2e28c863-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/openais-multi-agent-deep-deterministic-policy-gradients-maddpg-9d2dad34c82?source=author_recirc-----999d9f17f7b6----0---------------------5f70bddd_c468_45b2_b437_7daa2e28c863-------", "anchor_text": "OpenAI\u2019s MADDPG AlgorithmAn actor-critic approach to multi-agent RL problems"}, {"url": "https://towardsdatascience.com/openais-multi-agent-deep-deterministic-policy-gradients-maddpg-9d2dad34c82?source=author_recirc-----999d9f17f7b6----0---------------------5f70bddd_c468_45b2_b437_7daa2e28c863-------", "anchor_text": "\u00b78 min read\u00b7May 26, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9d2dad34c82&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fopenais-multi-agent-deep-deterministic-policy-gradients-maddpg-9d2dad34c82&user=Austin+Nguyen&userId=6a1b6c5da9ab&source=-----9d2dad34c82----0-----------------clap_footer----5f70bddd_c468_45b2_b437_7daa2e28c863-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/openais-multi-agent-deep-deterministic-policy-gradients-maddpg-9d2dad34c82?source=author_recirc-----999d9f17f7b6----0---------------------5f70bddd_c468_45b2_b437_7daa2e28c863-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9d2dad34c82&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fopenais-multi-agent-deep-deterministic-policy-gradients-maddpg-9d2dad34c82&source=-----999d9f17f7b6----0-----------------bookmark_preview----5f70bddd_c468_45b2_b437_7daa2e28c863-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----999d9f17f7b6----1---------------------5f70bddd_c468_45b2_b437_7daa2e28c863-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----999d9f17f7b6----1---------------------5f70bddd_c468_45b2_b437_7daa2e28c863-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----999d9f17f7b6----1---------------------5f70bddd_c468_45b2_b437_7daa2e28c863-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----999d9f17f7b6----1---------------------5f70bddd_c468_45b2_b437_7daa2e28c863-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----999d9f17f7b6----1---------------------5f70bddd_c468_45b2_b437_7daa2e28c863-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----999d9f17f7b6----1---------------------5f70bddd_c468_45b2_b437_7daa2e28c863-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----5f70bddd_c468_45b2_b437_7daa2e28c863-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----999d9f17f7b6----1---------------------5f70bddd_c468_45b2_b437_7daa2e28c863-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----999d9f17f7b6----1-----------------bookmark_preview----5f70bddd_c468_45b2_b437_7daa2e28c863-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----999d9f17f7b6----2---------------------5f70bddd_c468_45b2_b437_7daa2e28c863-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----999d9f17f7b6----2---------------------5f70bddd_c468_45b2_b437_7daa2e28c863-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----999d9f17f7b6----2---------------------5f70bddd_c468_45b2_b437_7daa2e28c863-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----999d9f17f7b6----2---------------------5f70bddd_c468_45b2_b437_7daa2e28c863-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----999d9f17f7b6----2---------------------5f70bddd_c468_45b2_b437_7daa2e28c863-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----999d9f17f7b6----2---------------------5f70bddd_c468_45b2_b437_7daa2e28c863-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----2-----------------clap_footer----5f70bddd_c468_45b2_b437_7daa2e28c863-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----999d9f17f7b6----2---------------------5f70bddd_c468_45b2_b437_7daa2e28c863-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----999d9f17f7b6----2-----------------bookmark_preview----5f70bddd_c468_45b2_b437_7daa2e28c863-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-used-machine-learning-to-automatically-hand-draw-any-picture-7d024d0de997?source=author_recirc-----999d9f17f7b6----3---------------------5f70bddd_c468_45b2_b437_7daa2e28c863-------", "anchor_text": ""}, {"url": "https://medium.com/@austinnguyen517?source=author_recirc-----999d9f17f7b6----3---------------------5f70bddd_c468_45b2_b437_7daa2e28c863-------", "anchor_text": ""}, {"url": "https://medium.com/@austinnguyen517?source=author_recirc-----999d9f17f7b6----3---------------------5f70bddd_c468_45b2_b437_7daa2e28c863-------", "anchor_text": "Austin Nguyen"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----999d9f17f7b6----3---------------------5f70bddd_c468_45b2_b437_7daa2e28c863-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/how-i-used-machine-learning-to-automatically-hand-draw-any-picture-7d024d0de997?source=author_recirc-----999d9f17f7b6----3---------------------5f70bddd_c468_45b2_b437_7daa2e28c863-------", "anchor_text": "How I Used Machine Learning to Automatically Hand-Draw Any PictureSupervised and unsupervised learning made easy!"}, {"url": "https://towardsdatascience.com/how-i-used-machine-learning-to-automatically-hand-draw-any-picture-7d024d0de997?source=author_recirc-----999d9f17f7b6----3---------------------5f70bddd_c468_45b2_b437_7daa2e28c863-------", "anchor_text": "\u00b78 min read\u00b7Jun 1, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7d024d0de997&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-used-machine-learning-to-automatically-hand-draw-any-picture-7d024d0de997&user=Austin+Nguyen&userId=6a1b6c5da9ab&source=-----7d024d0de997----3-----------------clap_footer----5f70bddd_c468_45b2_b437_7daa2e28c863-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-used-machine-learning-to-automatically-hand-draw-any-picture-7d024d0de997?source=author_recirc-----999d9f17f7b6----3---------------------5f70bddd_c468_45b2_b437_7daa2e28c863-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "4"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7d024d0de997&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-used-machine-learning-to-automatically-hand-draw-any-picture-7d024d0de997&source=-----999d9f17f7b6----3-----------------bookmark_preview----5f70bddd_c468_45b2_b437_7daa2e28c863-------", "anchor_text": ""}, {"url": "https://medium.com/@austinnguyen517?source=post_page-----999d9f17f7b6--------------------------------", "anchor_text": "See all from Austin Nguyen"}, {"url": "https://towardsdatascience.com/?source=post_page-----999d9f17f7b6--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----999d9f17f7b6----0---------------------08635409_43c0_4669_86eb_350b7dd0b2e0-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----999d9f17f7b6----0---------------------08635409_43c0_4669_86eb_350b7dd0b2e0-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----999d9f17f7b6----0---------------------08635409_43c0_4669_86eb_350b7dd0b2e0-------", "anchor_text": "The PyCoach"}, {"url": "https://artificialcorner.com/?source=read_next_recirc-----999d9f17f7b6----0---------------------08635409_43c0_4669_86eb_350b7dd0b2e0-------", "anchor_text": "Artificial Corner"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----999d9f17f7b6----0---------------------08635409_43c0_4669_86eb_350b7dd0b2e0-------", "anchor_text": "You\u2019re Using ChatGPT Wrong! Here\u2019s How to Be Ahead of 99% of ChatGPT UsersMaster ChatGPT by learning prompt engineering."}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----999d9f17f7b6----0---------------------08635409_43c0_4669_86eb_350b7dd0b2e0-------", "anchor_text": "\u00b77 min read\u00b7Mar 17"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fartificial-corner%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&user=The+PyCoach&userId=fb44e21903f3&source=-----886a50dabc54----0-----------------clap_footer----08635409_43c0_4669_86eb_350b7dd0b2e0-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----999d9f17f7b6----0---------------------08635409_43c0_4669_86eb_350b7dd0b2e0-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "276"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&source=-----999d9f17f7b6----0-----------------bookmark_preview----08635409_43c0_4669_86eb_350b7dd0b2e0-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----999d9f17f7b6----1---------------------08635409_43c0_4669_86eb_350b7dd0b2e0-------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=read_next_recirc-----999d9f17f7b6----1---------------------08635409_43c0_4669_86eb_350b7dd0b2e0-------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=read_next_recirc-----999d9f17f7b6----1---------------------08635409_43c0_4669_86eb_350b7dd0b2e0-------", "anchor_text": "Wouter van Heeswijk, PhD"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----999d9f17f7b6----1---------------------08635409_43c0_4669_86eb_350b7dd0b2e0-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----999d9f17f7b6----1---------------------08635409_43c0_4669_86eb_350b7dd0b2e0-------", "anchor_text": "Proximal Policy Optimization (PPO) ExplainedThe journey from REINFORCE to the go-to algorithm in continuous control"}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----999d9f17f7b6----1---------------------08635409_43c0_4669_86eb_350b7dd0b2e0-------", "anchor_text": "\u00b713 min read\u00b7Nov 29, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fabed1952457b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fproximal-policy-optimization-ppo-explained-abed1952457b&user=Wouter+van+Heeswijk%2C+PhD&userId=33f45c9ab481&source=-----abed1952457b----1-----------------clap_footer----08635409_43c0_4669_86eb_350b7dd0b2e0-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----999d9f17f7b6----1---------------------08635409_43c0_4669_86eb_350b7dd0b2e0-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fabed1952457b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fproximal-policy-optimization-ppo-explained-abed1952457b&source=-----999d9f17f7b6----1-----------------bookmark_preview----08635409_43c0_4669_86eb_350b7dd0b2e0-------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----999d9f17f7b6----0---------------------08635409_43c0_4669_86eb_350b7dd0b2e0-------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk?source=read_next_recirc-----999d9f17f7b6----0---------------------08635409_43c0_4669_86eb_350b7dd0b2e0-------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk?source=read_next_recirc-----999d9f17f7b6----0---------------------08635409_43c0_4669_86eb_350b7dd0b2e0-------", "anchor_text": "Steve Roberts"}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----999d9f17f7b6----0---------------------08635409_43c0_4669_86eb_350b7dd0b2e0-------", "anchor_text": "State Values and Policy Evaluation in 5 minutesAn Introduction to Reinforcement Learning"}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----999d9f17f7b6----0---------------------08635409_43c0_4669_86eb_350b7dd0b2e0-------", "anchor_text": "\u00b75 min read\u00b7Jan 11"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2Ff3e00f3c1a50&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40tinkertytonk%2Fstate-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50&user=Steve+Roberts&userId=6b6735266652&source=-----f3e00f3c1a50----0-----------------clap_footer----08635409_43c0_4669_86eb_350b7dd0b2e0-------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----999d9f17f7b6----0---------------------08635409_43c0_4669_86eb_350b7dd0b2e0-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff3e00f3c1a50&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40tinkertytonk%2Fstate-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50&source=-----999d9f17f7b6----0-----------------bookmark_preview----08635409_43c0_4669_86eb_350b7dd0b2e0-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----999d9f17f7b6----1---------------------08635409_43c0_4669_86eb_350b7dd0b2e0-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----999d9f17f7b6----1---------------------08635409_43c0_4669_86eb_350b7dd0b2e0-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----999d9f17f7b6----1---------------------08635409_43c0_4669_86eb_350b7dd0b2e0-------", "anchor_text": "Alexander Nguyen"}, {"url": "https://levelup.gitconnected.com/?source=read_next_recirc-----999d9f17f7b6----1---------------------08635409_43c0_4669_86eb_350b7dd0b2e0-------", "anchor_text": "Level Up Coding"}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----999d9f17f7b6----1---------------------08635409_43c0_4669_86eb_350b7dd0b2e0-------", "anchor_text": "Why I Keep Failing Candidates During Google Interviews\u2026They don\u2019t meet the bar."}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----999d9f17f7b6----1---------------------08635409_43c0_4669_86eb_350b7dd0b2e0-------", "anchor_text": "\u00b74 min read\u00b7Apr 13"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fgitconnected%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&user=Alexander+Nguyen&userId=a148fd75c2e9&source=-----dc8f865b2c19----1-----------------clap_footer----08635409_43c0_4669_86eb_350b7dd0b2e0-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----999d9f17f7b6----1---------------------08635409_43c0_4669_86eb_350b7dd0b2e0-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "90"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&source=-----999d9f17f7b6----1-----------------bookmark_preview----08635409_43c0_4669_86eb_350b7dd0b2e0-------", "anchor_text": ""}, {"url": "https://medium.datadriveninvestor.com/finrl-meta-market-environments-and-benchmarks-for-data-driven-financial-reinforcement-learning-7af8e747c4bd?source=read_next_recirc-----999d9f17f7b6----2---------------------08635409_43c0_4669_86eb_350b7dd0b2e0-------", "anchor_text": ""}, {"url": "https://byfintech.medium.com/?source=read_next_recirc-----999d9f17f7b6----2---------------------08635409_43c0_4669_86eb_350b7dd0b2e0-------", "anchor_text": ""}, {"url": "https://byfintech.medium.com/?source=read_next_recirc-----999d9f17f7b6----2---------------------08635409_43c0_4669_86eb_350b7dd0b2e0-------", "anchor_text": "Bruce Yang ByFinTech"}, {"url": "https://medium.datadriveninvestor.com/?source=read_next_recirc-----999d9f17f7b6----2---------------------08635409_43c0_4669_86eb_350b7dd0b2e0-------", "anchor_text": "DataDrivenInvestor"}, {"url": "https://medium.datadriveninvestor.com/finrl-meta-market-environments-and-benchmarks-for-data-driven-financial-reinforcement-learning-7af8e747c4bd?source=read_next_recirc-----999d9f17f7b6----2---------------------08635409_43c0_4669_86eb_350b7dd0b2e0-------", "anchor_text": "FinRL-Meta: Market Environments and Benchmarks for Data-Driven Financial Reinforcement LearningNeurIPS 2022 Datasets and Benchmarks."}, {"url": "https://medium.datadriveninvestor.com/finrl-meta-market-environments-and-benchmarks-for-data-driven-financial-reinforcement-learning-7af8e747c4bd?source=read_next_recirc-----999d9f17f7b6----2---------------------08635409_43c0_4669_86eb_350b7dd0b2e0-------", "anchor_text": "\u00b79 min read\u00b7Nov 13, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fdatadriveninvestor%2F7af8e747c4bd&operation=register&redirect=https%3A%2F%2Fmedium.datadriveninvestor.com%2Ffinrl-meta-market-environments-and-benchmarks-for-data-driven-financial-reinforcement-learning-7af8e747c4bd&user=Bruce+Yang+ByFinTech&userId=a878fc45fb3f&source=-----7af8e747c4bd----2-----------------clap_footer----08635409_43c0_4669_86eb_350b7dd0b2e0-------", "anchor_text": ""}, {"url": "https://medium.datadriveninvestor.com/finrl-meta-market-environments-and-benchmarks-for-data-driven-financial-reinforcement-learning-7af8e747c4bd?source=read_next_recirc-----999d9f17f7b6----2---------------------08635409_43c0_4669_86eb_350b7dd0b2e0-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7af8e747c4bd&operation=register&redirect=https%3A%2F%2Fmedium.datadriveninvestor.com%2Ffinrl-meta-market-environments-and-benchmarks-for-data-driven-financial-reinforcement-learning-7af8e747c4bd&source=-----999d9f17f7b6----2-----------------bookmark_preview----08635409_43c0_4669_86eb_350b7dd0b2e0-------", "anchor_text": ""}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----999d9f17f7b6----3---------------------08635409_43c0_4669_86eb_350b7dd0b2e0-------", "anchor_text": ""}, {"url": "https://medium.com/@timothymugayi?source=read_next_recirc-----999d9f17f7b6----3---------------------08635409_43c0_4669_86eb_350b7dd0b2e0-------", "anchor_text": ""}, {"url": "https://medium.com/@timothymugayi?source=read_next_recirc-----999d9f17f7b6----3---------------------08635409_43c0_4669_86eb_350b7dd0b2e0-------", "anchor_text": "Timothy Mugayi"}, {"url": "https://betterprogramming.pub/?source=read_next_recirc-----999d9f17f7b6----3---------------------08635409_43c0_4669_86eb_350b7dd0b2e0-------", "anchor_text": "Better Programming"}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----999d9f17f7b6----3---------------------08635409_43c0_4669_86eb_350b7dd0b2e0-------", "anchor_text": "How To Build Your Own Custom ChatGPT With Custom Knowledge BaseFeed your ChatGPT bot with custom data sources"}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----999d9f17f7b6----3---------------------08635409_43c0_4669_86eb_350b7dd0b2e0-------", "anchor_text": "\u00b711 min read\u00b7Apr 7"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fbetter-programming%2F4e61ad82427e&operation=register&redirect=https%3A%2F%2Fbetterprogramming.pub%2Fhow-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e&user=Timothy+Mugayi&userId=34774d6cac27&source=-----4e61ad82427e----3-----------------clap_footer----08635409_43c0_4669_86eb_350b7dd0b2e0-------", "anchor_text": ""}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----999d9f17f7b6----3---------------------08635409_43c0_4669_86eb_350b7dd0b2e0-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "83"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4e61ad82427e&operation=register&redirect=https%3A%2F%2Fbetterprogramming.pub%2Fhow-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e&source=-----999d9f17f7b6----3-----------------bookmark_preview----08635409_43c0_4669_86eb_350b7dd0b2e0-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----999d9f17f7b6--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----999d9f17f7b6--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----999d9f17f7b6--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----999d9f17f7b6--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----999d9f17f7b6--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----999d9f17f7b6--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----999d9f17f7b6--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----999d9f17f7b6--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----999d9f17f7b6--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----999d9f17f7b6--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}