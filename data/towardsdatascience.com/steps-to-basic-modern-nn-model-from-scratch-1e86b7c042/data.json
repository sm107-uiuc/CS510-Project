{"url": "https://towardsdatascience.com/steps-to-basic-modern-nn-model-from-scratch-1e86b7c042", "time": 1683001262.447786, "path": "towardsdatascience.com/steps-to-basic-modern-nn-model-from-scratch-1e86b7c042/", "webpage": {"metadata": {"title": "Steps to basic modern NN model from scratch | by Rrohan.Arrora | Towards Data Science", "h1": "Steps to basic modern NN model from scratch", "description": "Today, we will learn the first step to building the Neural network, and it is the primary matrix multiplication. There are many ways to do so, and we will see each one and will compare them to get\u2026"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/abs/1502.01852", "anchor_text": "Delving Deep into Rectifiers", "paragraph_index": 22}, {"url": "https://towardsdatascience.com/cnn-resnets-a-more-liberal-understanding-a0143a3ddac9", "anchor_text": "convolutional networks", "paragraph_index": 55}, {"url": "https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.indexing.html#integer-array-indexing", "anchor_text": "integer array indexing", "paragraph_index": 77}, {"url": "https://en.wikipedia.org/wiki/LogSumExp", "anchor_text": "LogSumExp trick", "paragraph_index": 78}], "all_paragraphs": ["Today, we will learn the first step to building the Neural network, and it is the primary matrix multiplication. There are many ways to do so, and we will see each one and will compare them to get the best results.", "We require matrix multiplication in the linear layer of the Neural Networks. To do matrix multiplication, we need a dataset. Fastai is kind to provide various datasets, and we will use the MNIST dataset to do operations.", "We can do matrix multiplication using the simple python programs. But python programs take a lot of time to implement and let us see how.", "Pytorch provided us with an effortless way to do matrix multiplications, and it is known as element-wise operations. Let us understand it.", "Broadcasting is another way of matrix multiplication.", "As per the Scipy docs, the term broadcasting describes how numpy treats arrays with different shapes during arithmetic operations. Subject to certain constraints, the smaller array is \u201cbroadcast\u201d across the larger array so that they have compatible shapes. Broadcasting provides a means of vectorising array operations so that looping occurs in C instead of Python. It does this without making needless copies of data and usually leads to efficient algorithm implementations. Broadcasting happens a C speed and with CUDA speed on GPU.", "Einstein summation (einsum) is a compact representation for combining products and sums in a general way.", "From the numpy docs:\u201cThe subscripts string is a comma-separated list of subscript labels, where each label refers to a dimension of the corresponding operand.\u201d", "We can use PyTorch\u2019s function or operator directly for matrix multiplication.", "Thus, we can easily compare the timings of various codes. This is the reason also that we do not prefer to write the code in the Python language due to its slow performance. Therefore, most of the python libraries are implemented in the C.", "After we have defined the matrix multiplication strategy, its time to defined the ReLU function and the forward pass for the Neural Network. I would request the readers to go through the Part \u2014 1 of the series to get the background of the data used below.", "The Neural Network is defined as below:", "Let us explain the weights for the matrix multiplication.", "I will create a 2-layer neural network.", "Let us define the linear layer for the Neural Network and perform the operation.", "Now, the mean and standard deviation obtained after the linear operation is again non-normalized. Now, the problem is still pure. If it remains like this, more linear operations will lead to significant and substantial values, which will be challenging to handle. Thus, we want our activations after the linear operation to be normalized as well.", "To handle the non-normalized behaviour of linear neural network operation, we define weights to be Kaiming initialized. Though Kaiming Normalization or He initialisation is defined to handle ReLu/Leaky ReLu operation, we still can use it for linear operations.", "We divide our weights by math.sqrt(x) where x is the number of rows.", "After the above trivia, we get the normalized mean and SD.", "Let us define the ReLU layer for the Neural Network and perform the operation. Now, why we are defining the ReLU as a non-linear activation function, I hope you are aware of the Universal Approximation Theorem.", "Notice above that our standard deviation gets halved of the one obtained after the linear operation, and if it gets halved after one layer, imagine after eight layers it will get to 1/2\u00b2\u2078, which is very very small. And if our neural network has got 10000 layers \ud83d\ude35, forget about it.", "From PyTorch docs: a: the negative slope of the rectifier used after this layer (0 for ReLU by default)", "This was introduced in the paper that described the Imagenet-winning approach from Kaiming He and others: Delving Deep into Rectifiers, which was also the first paper that claimed \u201csuper-human performance\u201d on Imagenet (and, most importantly, it introduced ResNets!)", "Thus, following the same strategy, we will multiply our weights with math.sqrt(2/m) .", "Though we have better results, still the mean is not so good. As per the fastai docs, We could handle the mean by the below tweak.", "Let us combine the above all code and strategies and create the forward pass of our Neural Network. PyTorch has a defined method for Kaiming Normalization i.e kaiming_normal_ .", "The last to be defined for the forward pass is the Loss function: MSE.", "As per our previous knowledge, we generally use CrossEntroyLoss as the loss function for single-label classification functions. I will address the same later. For now, I am using MSE to understand the operation.", "Let us perform the above operations for the training dataset.", "To perform the MSE, we need the floats.", "After all of the above operation, one question is still not answered substantially and it is", "For Neural Networks, the primary step is the matrix multiplication and if we have a deep neural network with approx 100 layers, then let us see what will the standard deviation and mean of the activations obtained.", "We can easily see that mean, and a standard deviation is no longer a number. And it is justified as well. The computer is not able to store that large numbers; it cannot account for such large numbers. It has restricted the practitioners to train such deep neural networks for the same reason.", "The problem you\u2019ll get with that is activation explosion: very soon, your activations will go to nan. We can even ask the loop to break when that first happens:", "Thus, such problems lead to the invention of Kaiming Initialization. It surely took decades to come up with the idea finally.", "So, that\u2019s how we define the ReLu and backward pass for the neural network.", "Till now, we have understood the idea behind the matrix multiplication, the ReLu function, and the forward pass of the Neural Network. Now, we will discuss the backward pass.", "Before going into the backward pass, Lets us understand one question.", "This is the fancy term for the deep learning practitioners, especially when you are discussing with someone. But, in simpler terms, backpropagation is just calculating gradients through the chain rule. We find a gradient concerning the weights/parameters. It is this simple. Backpropagation is just the reverse of the gradient of the forward pass.", "So let us find the gradients step by step.", "\u2014 The gradient of the MSE layer", "\u2014 The gradient of the ReLU", "Now, for any value greater than 0, we have to replace it with 0, and for values smaller than 0, we have to keep them 0.", "\u2014 The gradient of the Linear layer", "I found the gradient of the linear layer more challenging to understand than the other layers. But I will try my best to simplify it.", "Now, let us combine both forward pass and backward pass.", "What if we compare the gradients we calculated with the gradients calculated by the PyTorch.", "Before comparison, we need to store our gradients.", "We cheat a little bit and use PyTorch autograd to check our results.", "Let us define the forward function to calculate the gradients using the PyTorch.", "Let us compare the w2 gradients.", "Now, there are so many ways in which we can refactor the above-written code, but that is not the concern. We need to understand the semantics behind the backward pass of the Neural Networks. This is how we define backwards pass in our model.", "In the previous part, we looked for the need of Kaiming Initialization for stabilising the effects of non-linear activation action. Now, the major for now is to see how kaiming initalization is used for Convolutional Networks. I will also tell you how it is implemented in PyTorch. So let us start the learning journey.", "From the last chapters, we have the below values.", "Let us look into the shape of the dataset.", "From our knowledge of convolutional networks, Let us create a simple convolutional neural network using PyTorch.", "When we are talking of Kaiming initialization, the first thing that comes to the mind is to calculate the mean and standard deviation of weights of the convolutional neural network.", "l1 has defined weights in it. Let\u2019s understand them before calculating the stats.", "We need to focus on the output of the convolutional neural network.", "But, we want the standard deviation of 1 instead of 0.6, though we have a mean of 0. So, let us apply kaiming initialization to the weights.", "Now, it is better. Mean is almost 0 and SD is around 1.", "But, kaiming initialization was introduced to handle the non-linear activation function. Let us define it.", "\u2014 Mean is not around 0, but SD is almost equal to 1.", "Without kaiming initialization, let us find the stats.", "Now, let us compare our results with the PyTorch. Before that, we need to see the PyTorch code.", "Let us understand the above methods.", "From all the above knowledge, we can create our kaiming_uniform as below.", "So, this is how kaiming concept is used in the Convolutional Neural Networks.", "Now, we have reached to the point where we need to know about the CrossEntropy loss because mainly CrossEntropy loss is used in single or multi-classification problems. Since we are using MNIST dataset, then we need to create a neural network which will predict for ten numbers, i.e. from 0 to 9. Earlier, we used the MSE loss and predicted the single outcome, which we generally do not do.", "So before learning the loss function deeply, let us create a neural network using PyTorch nn.module.", "Let us defined the weights again.", "You may observe the differences in the weight initialization. This time, we want ten predictions, one for each number in the output. That is why I initialized w2 to be (nh, 10).", "Again, before indulging into the Cross-Entropy loss, we need to take softmax of our predictions or activations. We do softmax in the case; we want single-label classification. In practice, we will need the log of the softmax when we calculate the loss because it helps further in calculating the cross-entropy loss.", "Since we have defined the log_softmax , let us take the log_softmax of our predictions.", "The cross-entropy loss is defined as below:", "In binary classification, where the number of classes M equals 2, cross-entropy can be calculated as:", "If M>2 (i.e. multiclass classification), we calculate a separate loss for each class label per observation and sum the result.", "Our problem is multiclass classification, so our cross-entropy loss function would be later one. The cross-entropy loss function for multiclass classification can also be done using numpy-style integer array indexing. Let us implement using it.", "Then, there is a way to compute the log of the sum of exponentials in a more stable way, called the LogSumExp trick. The idea is to use the following formula:", "So we can use it for our log_softmax function.", "Let us see the above implementation in PyTorch.", "In PyTorch, F.log_softmax and F.nll_loss are combined in one optimized function, F.cross_entropy.", "The training loop repeats over the following steps:", "Let us combine all the above concepts and create our loop.", "So, this is how we define the training loop in the neural loop. Now, in PyTorch, it has some syntax differences which you can understand now. I will add more steps to this series with time. Till then, feel free to explore fastai.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "demystifying the theories, construing the results."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F1e86b7c042&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsteps-to-basic-modern-nn-model-from-scratch-1e86b7c042&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsteps-to-basic-modern-nn-model-from-scratch-1e86b7c042&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsteps-to-basic-modern-nn-model-from-scratch-1e86b7c042&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsteps-to-basic-modern-nn-model-from-scratch-1e86b7c042&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----1e86b7c042--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1e86b7c042--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://rrohan-arrora.medium.com/?source=post_page-----1e86b7c042--------------------------------", "anchor_text": ""}, {"url": "https://rrohan-arrora.medium.com/?source=post_page-----1e86b7c042--------------------------------", "anchor_text": "Rrohan.Arrora"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdbb9cfaeb49a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsteps-to-basic-modern-nn-model-from-scratch-1e86b7c042&user=Rrohan.Arrora&userId=dbb9cfaeb49a&source=post_page-dbb9cfaeb49a----1e86b7c042---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1e86b7c042&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsteps-to-basic-modern-nn-model-from-scratch-1e86b7c042&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1e86b7c042&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsteps-to-basic-modern-nn-model-from-scratch-1e86b7c042&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@skraidantisdrambliukas?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Gertr\u016bda Valasevi\u010di\u016bt\u0117"}, {"url": "https://unsplash.com/s/photos/artificial-intelligence?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://arxiv.org/abs/1502.01852", "anchor_text": "Delving Deep into Rectifiers"}, {"url": "https://towardsdatascience.com/cnn-resnets-a-more-liberal-understanding-a0143a3ddac9", "anchor_text": "convolutional networks"}, {"url": "https://github.com/pytorch/pytorch/blob/fd4f22e4eaae17b8ee8c7de8b0b2b0e202fdf147/torch/nn/init.py#L60", "anchor_text": "PyTorch/PyTorchYou can\u2019t perform that action at this time. You signed in with another tab or window. You signed out in another tab or\u2026github.com"}, {"url": "https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.indexing.html#integer-array-indexing", "anchor_text": "integer array indexing"}, {"url": "https://en.wikipedia.org/wiki/LogSumExp", "anchor_text": "LogSumExp trick"}, {"url": "https://www.fast.ai/", "anchor_text": "fast.ai"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----1e86b7c042---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----1e86b7c042---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/fastai?source=post_page-----1e86b7c042---------------fastai-----------------", "anchor_text": "Fastai"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----1e86b7c042---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/convolutional-neural-net?source=post_page-----1e86b7c042---------------convolutional_neural_net-----------------", "anchor_text": "Convolutional Neural Net"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1e86b7c042&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsteps-to-basic-modern-nn-model-from-scratch-1e86b7c042&user=Rrohan.Arrora&userId=dbb9cfaeb49a&source=-----1e86b7c042---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1e86b7c042&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsteps-to-basic-modern-nn-model-from-scratch-1e86b7c042&user=Rrohan.Arrora&userId=dbb9cfaeb49a&source=-----1e86b7c042---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1e86b7c042&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsteps-to-basic-modern-nn-model-from-scratch-1e86b7c042&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1e86b7c042--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F1e86b7c042&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsteps-to-basic-modern-nn-model-from-scratch-1e86b7c042&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----1e86b7c042---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----1e86b7c042--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----1e86b7c042--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----1e86b7c042--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----1e86b7c042--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----1e86b7c042--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----1e86b7c042--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----1e86b7c042--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----1e86b7c042--------------------------------", "anchor_text": ""}, {"url": "https://rrohan-arrora.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://rrohan-arrora.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Rrohan.Arrora"}, {"url": "https://rrohan-arrora.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "122 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdbb9cfaeb49a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsteps-to-basic-modern-nn-model-from-scratch-1e86b7c042&user=Rrohan.Arrora&userId=dbb9cfaeb49a&source=post_page-dbb9cfaeb49a--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F1b79de439ecb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsteps-to-basic-modern-nn-model-from-scratch-1e86b7c042&newsletterV3=dbb9cfaeb49a&newsletterV3Id=1b79de439ecb&user=Rrohan.Arrora&userId=dbb9cfaeb49a&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}