{"url": "https://towardsdatascience.com/demystifying-generative-models-by-generating-passwords-part-2-38ad3c325a46", "time": 1683001151.9835148, "path": "towardsdatascience.com/demystifying-generative-models-by-generating-passwords-part-2-38ad3c325a46/", "webpage": {"metadata": {"title": "Demystifying Generative Models by Generating Passwords \u2014 Part 2 | by Andreas Pogiatzis | Towards Data Science", "h1": "Demystifying Generative Models by Generating Passwords \u2014 Part 2", "description": "Hello, once again this is the second part of the \u201cDemystifying Generative Models\u201d posts so if you haven\u2019t read Part 1 yet, I really urge you to do so here. In the previous post, we discussed the\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/generating-passwords-with-generative-models-from-probabilistic-to-deep-learning-approaches-54d41d8810e3", "anchor_text": "here.", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/nlp-extract-contextualized-word-embeddings-from-bert-keras-tf-67ef29f60a7b", "anchor_text": "here", "paragraph_index": 10}, {"url": "http://ufldl.stanford.edu/tutorial/unsupervised/Autoencoders/", "anchor_text": "excellent tutorial", "paragraph_index": 15}, {"url": "https://medium.com/@llionj/the-reparameterization-trick-4ff30fe92954", "anchor_text": "here", "paragraph_index": 39}, {"url": "http://cs229.stanford.edu/section/gaussians.pdf", "anchor_text": "http://cs229.stanford.edu/section/gaussians.pdf", "paragraph_index": 44}, {"url": "https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained", "anchor_text": "awesome post", "paragraph_index": 51}], "all_paragraphs": ["Hello, once again this is the second part of the \u201cDemystifying Generative Models\u201d posts so if you haven\u2019t read Part 1 yet, I really urge you to do so here.", "In the previous post, we discussed the differences between discriminative and generative models, took a peek to the fascinating world of probabilities and used that knowledge to develop a working Naive Bayes that generates passwords for us. Now, we will change our methodologies a little bit and explore how Deep Learning can help us when probabilities fail.", "Assuming that you have the context knowledge, of part 1, I will jump straight to the point. The Multinomial Naive Bayes model that was developed was based on the assumption that each feature is independent of each other and it worked quite well! But let\u2019s change the problem formulation slightly and observe how it behaves. Instead of having 4 features, fixed order, and a limited amount of choices per features, let\u2019s just define an alphabet with all the letters, digits and special characters that appear in our initial feature choices. These would be:", "To make this new problem as close to the previous one as possible, we will constrain the maximum length of the password to be 15 since this is the max length from our passwords dataset. Additionally, let\u2019s define another imaginary character called <NULL>. Since the model that will be described below will have to fill all 15 positions of the password the <NULL> character is just a way for the model to say put nothing here. Therefore from our perspective, any <NULL> characters will be ignored.", "With all these changes, the alphabet now contains a total of 19+8+6+4 + 1= 38 characters (uppercase + lowercase + digits + special characters + null char). As a result, there are 38\u00b9\u2075 possible combinations in our sample space! Of course, this is a significantly larger sample space than previously but let\u2019s observe how the Naive-Bayes model behaves in these circumstances.", "The list below illustrates passwords generated using the old Naive-Bayes model in the current problem context:", "Can you spot the issues here? Since the relies on the conditional independence assumption, each character position is inferred independently, therefore there is not a single word in the passwords that makes sense. This would make it extremely difficult to remember the password, and it\u2019s not what we want.", "Recall that in Part 1, there were constraints on the output format but also on the input. We specifically asked the model to do some sampling based on a finite set of choices which unavoidably makes sense and they were indeed independent! Nonetheless, this time, the sample space is incredibly huge. From all the possible combinations, only a tiny part resembles a word or follows the format that has been defined in the beginning since every letter is depended on the previous ones. To give you an example, the probability of having an \u201ce\u201d in the fourth position of our password sequence should be greater when the previous three letters are \u201ccaf\u201d rather than when they are \u201cpas\u201d! Hopefully, this somewhat explains why statistical models such as Naive-Bayes won\u2019t perform well on unstructured data.", "Yet, this does not mean that we can\u2019t work with unstructured data. Deep Learning has received an incredible amount of research and industry attention in recent years and it has been evident that unstructured data is an area that Deep Learning models excel at! Undoubtedly, this is because of their ability to model the underlying structure/dependencies of the input data.", "But how is this even possible you may wonder. Well, the deep-stacked layering of neurons in a trained deep learning model can be conceived as a mapping from a higher-dimensional space to a lower-dimensional latent space (dimensionality reduction). In such a space, it becomes easier to model the relationships amongst data points and it may be even possible to perform arithmetic operations. For example, a widely used application of latent space representation is the generation of word embeddings using deep learning models.", "You can learn more about word embeddings in my older blog post here.", "More formally, this is called representation learning and is something that as humans, we use it constantly without even noticing. Still, it is an extremely difficult task for a machine. As an example, recall the last time you attempted to describe a person to someone. Most likely, you mentioned hair colour, body type, clothing, eye colour, skin colour etc\u2026 and immediately the person that you had been talking to, could imagine a picture of the person you were describing! All the features that were used for this description are low dimensional features that our brain knows how to translate to a higher level dimension, or in other words, an image! In the same manner, the sensory input of the brain comes in a high-dimensional representation but the human cognitive ability allows us to translate those to lower-dimensional features! Isn\u2019t that exciting??", "Autoencoder[1] is a neural network architecture which can be more precisely described as an encoder and a decoder connected together. An encoder is a smaller neural network that maps high dimensional data to a smaller latent space while preserving the principal features of the original data. In the same manner, a decoder is simply a reversed encoder. It takes input from a latent space and attempts to recreate the original data.", "Notice that the narrowest part of the network is in the middle. This part is otherwise known as the \u201cbottleneck\u201d of the network and it is essentially what forces the autoencoder to discard some information and store only important features, rather than just passing along the information from input to output.", "In a generative modelling context, the network is trained as a whole, but only the decoder part is needed for generating data. The training is performed by minimizing the error of the original input with the reconstructed output.", "I will skip the formal description of how the network works in this post since my main purpose is to highlight the differences from probabilistic learning, but Stanford University has an excellent tutorial about autoencoders in case you would like to learn more.", "Let\u2019s revisit the password generation example, develop an autoencoder that can generate passwords for us and observer how that compares with the independent Naive Bayes model.", "Just a heads up, all the code for the next sections can be found here:", "So feel free to skip the rest if you are here for the code, but I would suggest opening the notebook up and following along, as the code will make much more sense if it is read along with the rest of the post.", "Initially, the passwords have to be converted to vectors in order to be passed as an input to any kind of deep learning model. One of the most well-known techniques for such a task is one-hot encoding. In general, one hot encoding is suitable for categorical data and in layman terms it represents an item as \u201cone-of-X\u201d where is X is an instance of all the possible categories. In the context of textual data, it is assumed that there is a fixed vocabulary and each token is represented as a sparse vector of the same length as the vocabulary where the number 1 indicated that the token is an instance of the class corresponding to related vector index.", "Let\u2019s see an example to clear any confusion. Assuming that my vocabulary includes only 5 characters (\u201ca\u201d, \u201cb\u201d, \u201cc\u201d, \u201cd\u201d, \u201ce\u201d), then the word \u201cbad\u201d in one hot encoding can be represented as:", "As described earlier, autoencoders usually follow a standard stacked architecture but there are many parameters that can be tweaked such as the number of stacked layers, the number of cells, the way of connecting the cells etc... In addition, autoencoders can adopt an asymmetric architecture where the decoder has a different structure than the encoder. Whatever the case, the concept remains the same.", "For our password generation scenario, I adopted a BiLSTM Autoencoder architecture as illustrated in the figure below. BiLSTM layers are simply the Bidirectional version of normal LSTM layers. The main reason behind this choice was to utilise a model that excels at capturing sequential dependencies, but any other sequential model is applicable to what is described here as well.", "For implementing the model above, Keras Functional API with Tensorflow backend was used. More precisely, the passwords are one hot encoded as described above and then pass through 3 BiLSTM layers with 16, 10 and 6 units respectively. Behind the scenes though, the output dimension is doubled since the first layer, for instance, outputs 16 units for one direction and 16 for the reverse direction. Finally, the input passes through a fully connected dense layer with 6 units which is the bottleneck. In other words, by the time a password reaches the bottleneck, it is represented as a 6-dimensional vector. Next, the decoder takes this 6-dimensional vector and attempts to reconstruct the password.", "The training of the model has been performed on a dataset of 1000 passwords that have the structure described in Part 1 of this tutorial. After training for 300 epochs, the model reached an accuracy of 95%.", "Note that the size of the bottleneck layer has a major role in how accurately the model can reconstruct the original data since it ultimately dictates how much information is maintained during the encoding stage. More complex data would require a larger bottleneck size in order to capture all the nuances that may carry. Nonetheless, considering that the end goal, in this case, is to generate new passwords, 6 dimensions seemed enough and indeed the reconstruction accuracy is acceptable.", "Inspecting the results more closely, it is evident that the model did a decent job reconstructing the passwords from just 6 values!", "Now is the time for the big act! Let\u2019s see our awesome model in action and generate some passwords. For this task, only the decoder of the model is required since we will effectively sample 6 random values and feed them into the decoder to get a password:", "The results are interesting. First of all, it is obvious that the Autoencoder does a better job generating passwords than the Naive Bayes model for sure!", "This should be the \u201cAha!\u201d moment realising how powerful deep learning can become when identifying hidden correlations in unstructured data!", "There are some pretty usable passwords in there even with words that were never in the original passwords such as haakerman11$.", "Yet, most of them are quite off. Passwords 1, 2, 3, 5, 6 and 7 are totally non-sensical. But what is the issue that\u2019s causing this really? Let\u2019s examine what is going on further bu visualizing the latent space.", "By using t-SNE (a technique for visualizing high dimensional data) on 200 passwords in the latent space, we can indeed observe that similar passwords cluster together (i.e. hackerman passwords are all in the top left corner, further below we have hyper then ninja and so forth\u2026).", "Apparently, the clusters don\u2019t really have a symmetric structure. More specifically, the top right part of the space is completely empty, making us wonder what a password would like if sampled from that region. Moreover, there are large gaps between clusters and the positive y-axis has a wider range of points rather than the negative y-axis. As a result, sampling can become really problematic because, when randomly sampling from the latent space the decoder would have no idea how to decode it if the point is nowhere near already seen points. Plus, how do we know where to sample from, since the points are not even placed around the origin.", "As a matter of fact, all of these issues is one of the major weaknesses of vanilla Autoencoders but there is a more modern type of Autoencoders called Variational Autoencoders that take an interesting approach to tackle that problem!", "Variational Autoencoders (VAE)[2], first introduced in 2013 is a special type of autoencoders which adopt a Bayesian approach to form a more general, inherently continuous latent space to stochastically sample new data points from.", "In layman\u2019s terms, it is almost the same model as the standard autoencoder but with an extra layer right in the middle. That is a fully connected dense layer which instead of mapping a password directly to a point in the latent space, it outputs a multivariate normal distribution. To be more precise, it maps to the parameters that define a multivariate normal distribution.", "Let\u2019s do a brief statistics refresher to elucidate those concepts. Normal (aka Gaussian) distribution is a probability distribution which is defined by two parameters. Namely, the mean (\u03bc) and variance (\u03c3\u00b2) (Also has standard deviation but that\u2019s just the square root of the variance). In formal terms is defined as:", "The normal distribution is symmetric about the mean and can be very easily recognised by its bell-shaped curve. To sample a point from a normal distribution you can use the equation:", "Where epsilon is sampled from a standard normal distribution (\u03bc = 0, \u03c3 = 1). This is also called the reparameterization trick and helps with backpropagation of the model as well. (See here)", "Furthermore, the normal distribution can be defined in many dimensions, where is called a Multivariate Normal Distribution. More formally, a Multivariate Normal Distribution of n dimensions is defined as:", "Where \u03bc is the mean vector and \u03a3 is the covariance matrix. In 2D they would look like illustrated below:", "and \u03c1 is the correlation between the dimensions.", "In the context of VAEs, the correlation factor \u03c1 is not relevant as it is assumed that there isn\u2019t any correlation amongst the dimensions. Strictly speaking, this means that the covariance matrix is diagonal and thus the multivariate normal distribution is the equivalent toa collection of independent normal distributions.", "Proof and more details about that can be found here: http://cs229.stanford.edu/section/gaussians.pdf", "So coming back to the password generation problem, as mentioned earlier, instead of encoding the password to a concrete point in latent space, we encode it to the parameters of a collection of Normal Distributions. Then a random variable is sampled from each one of them, to form a point in our latent space!", "An additionaly technicality to be stated is that in reality, we map it to the logarithm of the variance instead of the variance itself. This tweak allows variance to take any value from (-inf, inf) instead of just positive values.", "So far so good but how do all these resolve our problems, you may be wondering?", "Practically, this allows the VAE to have two very nice properties when it comes to new data generation. More specifically completeness and continuity! For example sampling from the probability distribution means that each input password maps to an area rather than a single point, thus forcing the decoder to generate similar data for points in the latent space that fall within the distribution. For instance, if let\u2019s say point (1,1) maps to the password \u201cmypass\u201d, the point (1.1, 1.1) should map to a similar password.", "Still, just adding a probability distribution to sample points in the latent space does not solve everything. There is one more thing that needs to be tweaked in order to perfect our variational autoencoder. That is, revising the loss function.", "So fat we have ensured that similar data points should be close together, but what about the data points which have completely different features? To explain further, the password \u201cMypass\u201d should not be directly next to \u201cSomethingRandom\u201d in the latent space. Yet, we would like to avoid having huge gaps between clusters of data points as shown in the image below. So in technical terms, the model should be penalized whenever that happens.", "This is achieved by introducing \u201cKullback-Leibler (KL)\u201d divergence in the loss function. In simple terms, KL divergence is a metric that measures how different two distributions are (i.e. how much they diverge). A more thorough explanation about KL can be found in this awesome post.", "In the case of VAEs we want to penalize any distribution that diverges from the standard normal distribution (\u03bc = 0, \u03c3 = 1). This rewards the model for having all the distributions evenly spaced around the origin of the latent space and punishes it otherwise.", "The sum performed over the means and variances of all dimensions in the latent space.", "In summary, the final loss function becomes:", "Alright, enough with the theory! Let\u2019s get things done! Just a reminder that the code for that is included in the notebook provided in the sections above.", "Of course, the model architecture is the same as earlier apart from the extra sampling layer which is implemented using the Lambda layer of Keras API. Again, the model is trained on a dataset of 1000 passwords, over 300 epochs reaching an accuracy of 85%. Note that accuracy is lower than standard autoencoder\u2019s accuracy. Keep in mind that accuracy here simply refers to categorical accuracy thus it only reflects how accurate the reconstructions it. It does not say anything about the structure of the latent space which is what VAEs are all about! But in general, the lower accuracy was expected, due to the more complex loss function which makes training harder.", "By observing the password reconstructions in the figure below we can see that the model still does a decent job decoding the passwords.", "This is probably the most exciting part of the post series. Let\u2019s use the finalized Variational Autoencoder to generate some passwords!", "In order to create new passwords, It is enough to just sample points from a Normal Distribution with 0 mean and variance from 1\u20133 since in theory all of our points in the latent space are distributed around the standard normal distribution.", "There you go! Much better than the conventional autoencoder! It is apparent that most of these are passwords that actually make sense! And remember\u2026 as opposed to Naive Bayes in the previous post we didn\u2019t enforce any sort of structure or dependencies amongst the letters in the model. Everything was derived by deep learning itself!", "Finally, we can visualize the latent space in a 2D plot using t-SNE to observe the changes in its structure.", "Admittedly, the variational visualization of the latent space was not quite what I was expecting and I am not sure why. I was expecting to see the points more densely placed in the latent space but instead, there is a big gap in the middle. It could be a small dataset, the t-SNE itself, or it may require more training but I can\u2019t be certain. So if any of you guys have an insight as to what could have been the case, please let me know in the comments below.", "Again to save you from scrolling all the way up. Here is the repository which contains all the relevant code for this post:", "This concludes the two-part series of \u201cDemystifying Generative modelling by Generating Passwords\u201d posts. In these articles, we explored the differences of Discriminative Modelling vs Generative Modelling and analysed some Generative Models to identify their inner workings, strengths, and weaknesses. Adopting a password generation scenario as the basis for applying the models in practice, we described, analysed, implemented, tested those models in Python using Keras API and use real practical cases to highlight problems in each case.", "In summary, we have seen than probabilistic models like Naive Bayes perform relatively well when the number of features is small or may require some form of feature engineering prior to processing. Yet, when the number of features is untractably large (i.e. working with unstructured data) Deep Learning massively outperforms statistical models as important features are identified as part of the training. More precisely, this post focused specifically on standard and variational Autoencoders to demonstrate the aptitudes of generative deep learning.", "By the end of the post, we had a working Variational Autoencoder implemented, capable of generating new passwords just by decoding 6 points from a Normal Distribution.", "I really hope you enjoyed the walkthroughs as much as I did and that these posts gave you an insight into generative modelling in simple terms. Any feedback is much appreciated.", "[2] Diederik P Kingma and Max Welling. Auto-encoding variational bayes, 2013.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "\u2630 PhD Candidate @ UoG \u25cf Combining Cyber Security with Data Science \u25cf Writing to Understand"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F38ad3c325a46&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-generative-models-by-generating-passwords-part-2-38ad3c325a46&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-generative-models-by-generating-passwords-part-2-38ad3c325a46&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-generative-models-by-generating-passwords-part-2-38ad3c325a46&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-generative-models-by-generating-passwords-part-2-38ad3c325a46&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----38ad3c325a46--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----38ad3c325a46--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://apogiatzis.medium.com/?source=post_page-----38ad3c325a46--------------------------------", "anchor_text": ""}, {"url": "https://apogiatzis.medium.com/?source=post_page-----38ad3c325a46--------------------------------", "anchor_text": "Andreas Pogiatzis"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd468d7b1e1a9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-generative-models-by-generating-passwords-part-2-38ad3c325a46&user=Andreas+Pogiatzis&userId=d468d7b1e1a9&source=post_page-d468d7b1e1a9----38ad3c325a46---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F38ad3c325a46&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-generative-models-by-generating-passwords-part-2-38ad3c325a46&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F38ad3c325a46&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-generative-models-by-generating-passwords-part-2-38ad3c325a46&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@isisfra?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Isis Fran\u00e7a"}, {"url": "https://unsplash.com/search/photos/gear?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://towardsdatascience.com/generating-passwords-with-generative-models-from-probabilistic-to-deep-learning-approaches-54d41d8810e3", "anchor_text": "here."}, {"url": "https://towardsdatascience.com/nlp-extract-contextualized-word-embeddings-from-bert-keras-tf-67ef29f60a7b", "anchor_text": "here"}, {"url": "https://www.jeremyjordan.me/autoencoders/", "anchor_text": "Introduction to Autoencoders"}, {"url": "http://ufldl.stanford.edu/tutorial/unsupervised/Autoencoders/", "anchor_text": "excellent tutorial"}, {"url": "https://github.com/apogiatzis/vae-lstm-autoencoder-password-generation", "anchor_text": "apogiatzis/vae-lstm-autoencoder-password-generationThis repository contains the relevant code for the Part 2 of the post series named \"Demystifying Generative Models by\u2026github.com"}, {"url": "https://medium.com/@llionj/the-reparameterization-trick-4ff30fe92954", "anchor_text": "here"}, {"url": "http://cs229.stanford.edu/section/gaussians.pdf", "anchor_text": "http://cs229.stanford.edu/section/gaussians.pdf"}, {"url": "https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained", "anchor_text": "awesome post"}, {"url": "https://github.com/apogiatzis/vae-lstm-autoencoder-password-generation", "anchor_text": "apogiatzis/vae-lstm-autoencoder-password-generationThis repository contains the relevant code for the Part 2 of the post series named \"Demystifying Generative Models by\u2026github.com"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----38ad3c325a46---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/generative-model?source=post_page-----38ad3c325a46---------------generative_model-----------------", "anchor_text": "Generative Model"}, {"url": "https://medium.com/tag/data-science?source=post_page-----38ad3c325a46---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/autoencoder?source=post_page-----38ad3c325a46---------------autoencoder-----------------", "anchor_text": "Autoencoder"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----38ad3c325a46---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F38ad3c325a46&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-generative-models-by-generating-passwords-part-2-38ad3c325a46&user=Andreas+Pogiatzis&userId=d468d7b1e1a9&source=-----38ad3c325a46---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F38ad3c325a46&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-generative-models-by-generating-passwords-part-2-38ad3c325a46&user=Andreas+Pogiatzis&userId=d468d7b1e1a9&source=-----38ad3c325a46---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F38ad3c325a46&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-generative-models-by-generating-passwords-part-2-38ad3c325a46&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----38ad3c325a46--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F38ad3c325a46&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-generative-models-by-generating-passwords-part-2-38ad3c325a46&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----38ad3c325a46---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----38ad3c325a46--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----38ad3c325a46--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----38ad3c325a46--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----38ad3c325a46--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----38ad3c325a46--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----38ad3c325a46--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----38ad3c325a46--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----38ad3c325a46--------------------------------", "anchor_text": ""}, {"url": "https://apogiatzis.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://apogiatzis.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Andreas Pogiatzis"}, {"url": "https://apogiatzis.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "788 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd468d7b1e1a9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-generative-models-by-generating-passwords-part-2-38ad3c325a46&user=Andreas+Pogiatzis&userId=d468d7b1e1a9&source=post_page-d468d7b1e1a9--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F4df0306d22cc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-generative-models-by-generating-passwords-part-2-38ad3c325a46&newsletterV3=d468d7b1e1a9&newsletterV3Id=4df0306d22cc&user=Andreas+Pogiatzis&userId=d468d7b1e1a9&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}