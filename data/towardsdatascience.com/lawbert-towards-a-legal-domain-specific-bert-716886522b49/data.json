{"url": "https://towardsdatascience.com/lawbert-towards-a-legal-domain-specific-bert-716886522b49", "time": 1683012829.970437, "path": "towardsdatascience.com/lawbert-towards-a-legal-domain-specific-bert-716886522b49/", "webpage": {"metadata": {"title": "LawBERT: Towards a Legal Domain-Specific BERT? | by Erin Zhang | Towards Data Science", "h1": "LawBERT: Towards a Legal Domain-Specific BERT?", "description": "Google\u2019s Bidirectional Encoder Representations from Transformers (BERT) is a large-scale pre-trained autoencoding language model developed in 2018. Its development has been described as the NLP\u2026"}, "outgoing_paragraph_urls": [], "all_paragraphs": ["Google\u2019s Bidirectional Encoder Representations from Transformers (BERT) is a large-scale pre-trained autoencoding language model developed in 2018. Its development has been described as the NLP community\u2019s \u201cImageNet moment\u201d, largely because of how adept BERT is at performing downstream NLP language understanding tasks with very little backpropagation and fine-tuning needed (usually only 2\u20134 epochs).", "For context, traditional word embeddings (e.g. word2vec and GloVe) are non-contextual. They represent each word token with a single static vector, and learn by word co-occurence instead of the sequential context of words. This may be problematic when words are polysemous (i.e. where the same word has multiple different meanings), which is very common in law.", "For instance, a single static context-free vector will be used to represent the word \u201clemon\u201d in the sentences \u201cthe lemon is sour\u201d and \u201cthat car is a lemon\u201d (i.e. an interesting reference to lemon laws, which protects consumers when new vehicles turn out to be defective).", "In contrast, BERT is a contextual model which generates context-specific representations based on the words surrounding each token. Interestingly, Ethayarajh (2019) shows that instead of creating one representation of \u201clemon\u201d per word sense (left option), BERT will create infinitely many representations of \u201clemon\u201d, each highly specific to its context (right option). This means that instead of generating a single dense vector to represent the token \u201clemon\u201d, the token is dynamically represented by \u201cthe ______ is sour\u201d and \u201cthat car is a _____\u201d.", "BERT\u2019s ability to capture linguistic polysemy is particularly relevant in the legal domain, where polysemy abounds. For instance, the word \u201cconsideration\u201d in law represents the idea of reciprocity in contractual relationships, and has a different meaning from the word\u2019s usual connotation of being considerate. Furthermore, the same term can have multiple definitions in statute and case law. For instance, the term \u201cworker\u201d has four different definitions in EU law, and can be used to connote different kinds of workers, even in the same document. The development of contextual language models is hence significant for the legal AI landscape.", "The technicalities of the BERT architecture are beyond the scope of this article (and has been extensively written about). Nevertheless, it is worth noting that one of BERT\u2019s key innovations is its bidirectionality, i.e. it overcomes the traditional problems of sequential text parsing. Notably, bidirectionality in this context does not mean bidirectional in a sequential sense (i.e. parsing from both left-to-right and right-to left at the same time) but bidirectional in a simultaneous sense (i.e. it learns information from both the left and right contexts of a token in all layers at the same time). This is done by using methods like Masked LMs (MLMs) and Next Sentence Prediction (NSP) to achieve better contextual word embedding results.", "While BERT has been very effective in performing general language representation tasks, a problem is that \u2014 being trained on unlabelled generic Wikipedia and open-source articles \u2014 it lacks domain-specific knowledge. This is an important caveat, as a language model can only be as good as its corpus. As an analogy, applying a vanilla BERT model to legal domain-specific issues might be equivalent to asking a liberal arts student to approach a legal problem, rather than a law student trained on years of legal material. This is problematic since there is a lot of disconnect between the language as found in general open-source corpora (e.g. Wikipedia and news articles) and legal language, which may be esoteric and Latin-based.", "While there has not yet been a legal domain-specific BERT model developed at the time of writing, examples of other domain-specific BERTs that have been developed include BioBERT (biomedical sciences), SciBERT (scientific publications), FinBERT (financial communciations), and ClinicalBERT (clinical notes). Although they all share the similarity of being domain-specific BERTs, their architecture exhibits many key differences.", "I will explore some of the approaches in training domain-specific BERTs:", "This involves completely re-doing BERT\u2019s pre-training process with large-scale unlabelled legal corpora (e.g. statutes, precedents). In this sense, the domain-specific knowledge would be injected during the pre-training process. This was the approach taken for the SCIVOCAB SciBERT.", "While this does significantly enhance BERT\u2019s performance on domain-specific tasks, the key problem is that the time, costs, and quantity of data required for complete re-training might simply be too massive to be feasible. BERT-Base itself is a neural network with 12 stacked layers and approximately 110 million weights/parameters, and its pre-training corpora required 3.3 billion tokens. Re-training a domain-specific BERT would require a similar amount of training data (e.g. SciBERT used 3.17 billion tokens). It would require a week or more to train the model (e.g.the SCIVOCAB SciBERTs took 1 week to train on a v3 TPU) and be extremely costly, which is simply not feasible for the average enterprise.", "As such, a compromise might be to not completely re-train BERT, but instead initialise weights from BERT and pre-train it further with legal domain-specific data. This has been shown to enhance BERT\u2019s performance, and Sun (2019) shows that models that were further pre-trained performed better across all seven datasets than the original BERT-Base model.", "This seems to be the most popular method, and was used for BioBERT, the BASEVOCAB SciBERT, and FinBERT. Instead of completely re-training from scratch, researchers initialised the new BERT model with learned weights from BERT-Base, then trained it on domain-specific texts (e.g. PubMed abstracts and PMC full-text articles for BioBERT). Lee et al. (2020) reported that BioBERT achieved higher F1, precision, and recall scores than BERT on all datasets. Similarly, Beltagy et al. (2019) reported that SciBert outperformed on biomedical, computer science, and multi-domain tasks.", "Another, even simpler, option is to use the pre-trained BERT but fine-tune it with legal domain-specific corpora in downstream NLP tasks. The concept of fine-tuning comes from the domain of transfer learning, which \u2014 in simple terms \u2014 means taking a model that was aimed to solve task x, and repurposing it to solve task y. In practice, it usually means taking the pre-trained BERT model and adding an additional output layer of untrained neurons at the end, which is then trained using labelled legal corpora (fine-tuning is usually a supervised task). After being fine-tuned on domain-specific data, the resulting model will have updated weights that more closely resembles the characteristics and vocabulary of the target domain", "The advantage of this approach is that it requires much less data and is significantly faster and cheaper to train. Given that BERT-Base\u2019s pre-training has already encoded knowledge of most words in the general domain, it only requires a few tweaks to adapt it to function in a legal context. Instead of a timespan of weeks, fine-tuning usually requires just 2\u20134 epochs (a matter of minutes). This approach also requires less data (although it usually requires labelled data), which makes it more feasible.", "Furthermore, if desired, fine-tuning can be done in conjunction with domain-specific pre-training, i.e. they are not mutually exclusive methods. For instance, BioBERT was first pre-trained with biomedical domain-specific corpora and then fine-tuned on biomedical text mining tasks like named entity recognition, relation extraction, and QA.", "With the recent hype around mega-scale models like OpenAI\u2019s GPT-3, the initial excitement surrounding BERT seems to have somewhat waned. Nevertheless, it bears remembering that BERT is still an incredibly powerful and agile model that arguably sees more practical application. As seen from the experience of domain-specific BERTs like SciBERT and BioBERT, fine-tuning and/or pre-training BERT on domain-specific corpora is likely to improve performance with regard to legal NLP tasks. To this end, depending on the researcher\u2019s time and monetary resources, there are various ways to improve and tailor BERT\u2019s performance.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Hi, I\u2019m Erin! I work as a trainee solicitor in London and am passionate about making legaltech and data science concepts accessible to everyone."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F716886522b49&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flawbert-towards-a-legal-domain-specific-bert-716886522b49&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flawbert-towards-a-legal-domain-specific-bert-716886522b49&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flawbert-towards-a-legal-domain-specific-bert-716886522b49&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flawbert-towards-a-legal-domain-specific-bert-716886522b49&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----716886522b49--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----716886522b49--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@erin.zhangyj?source=post_page-----716886522b49--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@erin.zhangyj?source=post_page-----716886522b49--------------------------------", "anchor_text": "Erin Zhang"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1f0b81ad9d97&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flawbert-towards-a-legal-domain-specific-bert-716886522b49&user=Erin+Zhang&userId=1f0b81ad9d97&source=post_page-1f0b81ad9d97----716886522b49---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F716886522b49&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flawbert-towards-a-legal-domain-specific-bert-716886522b49&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F716886522b49&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flawbert-towards-a-legal-domain-specific-bert-716886522b49&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.bl.uk/collection-guides/modern-law-reports-and-legal-cases", "anchor_text": "The British Library"}, {"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "Devlin et al. (2019)"}, {"url": "https://developers.google.com/machine-learning/crash-course/embeddings/translating-to-a-lower-dimensional-space", "anchor_text": "Google Developers"}, {"url": "https://www.aclweb.org/anthology/D19-1006.pdf", "anchor_text": "Ethayarajh (2019)"}, {"url": "https://www.britannica.com/story/what-is-the-difference-between-criminal-law-and-civil-law", "anchor_text": "Encyclopedia Britannica"}, {"url": "https://arxiv.org/ftp/arxiv/papers/1901/1901.08746.pdf", "anchor_text": "Lee et al. (2019)"}, {"url": "https://medium.com/tag/legaltech?source=post_page-----716886522b49---------------legaltech-----------------", "anchor_text": "Legaltech"}, {"url": "https://medium.com/tag/legal-ai?source=post_page-----716886522b49---------------legal_ai-----------------", "anchor_text": "Legal Ai"}, {"url": "https://medium.com/tag/nlp?source=post_page-----716886522b49---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/naturallanguageprocessing?source=post_page-----716886522b49---------------naturallanguageprocessing-----------------", "anchor_text": "Naturallanguageprocessing"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F716886522b49&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flawbert-towards-a-legal-domain-specific-bert-716886522b49&user=Erin+Zhang&userId=1f0b81ad9d97&source=-----716886522b49---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F716886522b49&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flawbert-towards-a-legal-domain-specific-bert-716886522b49&user=Erin+Zhang&userId=1f0b81ad9d97&source=-----716886522b49---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F716886522b49&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flawbert-towards-a-legal-domain-specific-bert-716886522b49&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----716886522b49--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F716886522b49&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flawbert-towards-a-legal-domain-specific-bert-716886522b49&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----716886522b49---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----716886522b49--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----716886522b49--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----716886522b49--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----716886522b49--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----716886522b49--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----716886522b49--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----716886522b49--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----716886522b49--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@erin.zhangyj?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@erin.zhangyj?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Erin Zhang"}, {"url": "https://medium.com/@erin.zhangyj/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "63 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1f0b81ad9d97&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flawbert-towards-a-legal-domain-specific-bert-716886522b49&user=Erin+Zhang&userId=1f0b81ad9d97&source=post_page-1f0b81ad9d97--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fc328a5522871&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flawbert-towards-a-legal-domain-specific-bert-716886522b49&newsletterV3=1f0b81ad9d97&newsletterV3Id=c328a5522871&user=Erin+Zhang&userId=1f0b81ad9d97&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}