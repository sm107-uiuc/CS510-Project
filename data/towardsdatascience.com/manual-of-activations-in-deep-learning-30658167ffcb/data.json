{"url": "https://towardsdatascience.com/manual-of-activations-in-deep-learning-30658167ffcb", "time": 1683015969.844424, "path": "towardsdatascience.com/manual-of-activations-in-deep-learning-30658167ffcb/", "webpage": {"metadata": {"title": "Definitive Guide of Activations in Machine Learning | by DJ | Towards Data Science", "h1": "Definitive Guide of Activations in Machine Learning", "description": "For any Machine Learning model, one of the most critical decisions is the choice of which activation to use. Let\u2019s go through all the activation functions you\u2019d ever wanna know about. The order of\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/@wilder.rodrigues/sinerelu-an-alternative-to-the-relu-activation-function-e46a6199997d", "anchor_text": "Medium article", "paragraph_index": 33}], "all_paragraphs": ["For any Machine Learning model, one of the most critical decisions is the choice of which activation to use. Let\u2019s go through all the activation functions you\u2019d ever wanna know about. The order of this list is in the Increasing Usage.", "Here\u2019s a quick list if you wanna jump right into one.", "With so much to cover, let\u2019s start now.", "The identity function is one that practically gives the input back as the output. Therefore,", "This function is very rarely used in recent times, and it is extremely doubtful that you\u2019d ever want to use this.", "The graph of the Identity function is simply like below.", "The binary step function returns 1 if the number is positive and zero otherwise. It is a rarely used function. Thus,", "Where the graph is like the below,", "This function was previously the most used in all of Machine Learning. This function maps a number to between 1 and 0, with a graph looking like the above.", "The sigmoid function is now limited to Logistic Regression and Neural Nets\u2019 output nodes for binary classification problems (outputs 0 or 1), although earlier it was also used in hidden units.", "The softmax function applies one-sum probabilities to individual components of a vector. Mathematically,", "It is also, thus called the Normalized Exponential operation.", "Individually, the following exponential graph is applied.", "It finds application in the output node of Neural Nets for multi-class classification problems.", "The TanH function was the successor of sigmoid as it consistently gave a better performance in the hidden layers of the Neural Nets. You may observe that the tanH graph is very similar to sigmoid, except that it stabilizes at -1 and 1, and centres at 0.", "Mathematically it is the ratio of the hyperbolic sine and cosine,", "Nowadays, the tanh function is also lesser-used for hidden layers, although some specific models use the tanh function too. Some problems with the output needs of range -1 and 1 use tanh in the output node.", "The SoftPlus function is the softer, or smoother version of the ReLU that you\u2019ll see later.Mathematically,", "When the ReLU gives zero gradients, the SoftPlus allows smooth gradients. Thus, in that case, you may use SoftPlus instead of ReLU. It should be noted that the SoftPlus function is computationally much more expensive.", "ELU or exponential linear unit is a new and highly accurate well-used activation function in hidden layers. It is a parameterized function, i.e. it has a parameter (it is technically a hyper-parameter or tunable parameter) called alpha, symbol \u03b1. The ELU returns the number itself if it\u2019s positive, and gives alpha multiplied by the exponentiated input subtracted by 1.", "where \u03b1 is usually a number between 0.1 and 0.3", "The ELU has the potential of getting better accuracy than the ReLU. However, it is more computationally expensive.", "The SELU or the Scaled Exponential Linear Unit is the modification of the ELU, which better aids in improving accuracy and normalizing. An additional hyperparameter lambda is added, symbol \u03bb. The SELU is given as,", "The SELU works better than the ELU, yet it is, due to another added multiplication, even more computationally expensive.", "One of the most commonly used activation functions nowadays is the Rectified Linear Unit or ReLU function. The thing that makes it so attractive is the sheer simplicity and its effectivity. This function simply eliminates a negative value by making its value zero. It retains the values of positive inputs.", "The ReLU, although having lesser performance than ELU, SELU or its modifications, is highly computationally efficient, and is thus the most used activation function.", "The ReLU activation function has the undesirable attribute of zeroing negative gradients, which lead to a problem called Dying ReLU. To solve this, the Leaky ReLU or LReLU diminishes negative values by multiplying them with a value of 0.01.", "Although the Leaky ReLU usually finds better optima, it is computationally more expensive, and thus takes more time. Therefore, it is lesser used for tasks with speed being the more critical criterion and where there are less computational resources.", "The PReLU or Parameterized ReLU turns the coefficient of x in the Leaky ReLU to a parameter, instead of a fixed number, which can be learned through backpropagation. This is written as alpha with the symbol \u03b1. When formulated,", "The PReLU usually finds even better optima than either of ReLU or Leaky ReLU, yet takes more epochs and more time than either. As it has an additional parameter alpha, it is computationally expensive.", "The SReLU or S-Shaped ReLU can learn both concave and convex functions. Specifically, it consists of three piecewise linear functions, which are formulated by four learnable parameters.", "The SReLU is a complicated function, thus it can learn complexities.", "The SReLU is the most computationally expensive function in this list of activations. It is also one of the best activation accuracy-wise on the list. The SReLU may be used when computational resources are high.", "The Sine ReLU is probably the newest function on the list. This was invented by Wilder Rodriguez. To learn about this function, you can see the Medium article. It gets great results, so it\u2019s worth trying. Its formulated as", "The epsilon mentioned can carry base values of 0.0025, although he uses 0.25 for Dense layers.", "In my opinion, the SineReLU, at some datasets, can get better results than all of the previous functions. This is another one of the functions you can try out. Also, it is computationally more expensive than Leaky-ReLU and ReLU, although it can outperform them.", "There are so many choices! Which one to choose is finally up to you, and its effectiveness would also vary with your application, so test them out! You\u2019d probably get some gut feelings, for what function to use when, with experience\u2026", "I really hope this helped you. If you have any suggestions, do give feedback in the comments.", "Note: All LateX equation images are by Author", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "I am an AI geek with 15k+ views, capable of programming in about 5 languages, also an avid Arduino fan!"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F30658167ffcb&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmanual-of-activations-in-deep-learning-30658167ffcb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmanual-of-activations-in-deep-learning-30658167ffcb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmanual-of-activations-in-deep-learning-30658167ffcb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmanual-of-activations-in-deep-learning-30658167ffcb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----30658167ffcb--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----30658167ffcb--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://dj-ai.medium.com/?source=post_page-----30658167ffcb--------------------------------", "anchor_text": ""}, {"url": "https://dj-ai.medium.com/?source=post_page-----30658167ffcb--------------------------------", "anchor_text": "DJ"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3c24cf63041b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmanual-of-activations-in-deep-learning-30658167ffcb&user=DJ&userId=3c24cf63041b&source=post_page-3c24cf63041b----30658167ffcb---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F30658167ffcb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmanual-of-activations-in-deep-learning-30658167ffcb&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F30658167ffcb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmanual-of-activations-in-deep-learning-30658167ffcb&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/@wilder.rodrigues/sinerelu-an-alternative-to-the-relu-activation-function-e46a6199997d", "anchor_text": "Medium article"}, {"url": "https://medium.com/@wilder.rodrigues/sinerelu-an-alternative-to-the-relu-activation-function-e46a6199997d", "anchor_text": "Medium article"}, {"url": "https://medium.com/tag/data-science?source=post_page-----30658167ffcb---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----30658167ffcb---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/activation-functions?source=post_page-----30658167ffcb---------------activation_functions-----------------", "anchor_text": "Activation Functions"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F30658167ffcb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmanual-of-activations-in-deep-learning-30658167ffcb&user=DJ&userId=3c24cf63041b&source=-----30658167ffcb---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F30658167ffcb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmanual-of-activations-in-deep-learning-30658167ffcb&user=DJ&userId=3c24cf63041b&source=-----30658167ffcb---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F30658167ffcb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmanual-of-activations-in-deep-learning-30658167ffcb&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----30658167ffcb--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F30658167ffcb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmanual-of-activations-in-deep-learning-30658167ffcb&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----30658167ffcb---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----30658167ffcb--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----30658167ffcb--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----30658167ffcb--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----30658167ffcb--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----30658167ffcb--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----30658167ffcb--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----30658167ffcb--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----30658167ffcb--------------------------------", "anchor_text": ""}, {"url": "https://dj-ai.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://dj-ai.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "DJ"}, {"url": "https://dj-ai.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "34 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3c24cf63041b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmanual-of-activations-in-deep-learning-30658167ffcb&user=DJ&userId=3c24cf63041b&source=post_page-3c24cf63041b--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fe9c709276ba&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmanual-of-activations-in-deep-learning-30658167ffcb&newsletterV3=3c24cf63041b&newsletterV3Id=e9c709276ba&user=DJ&userId=3c24cf63041b&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}