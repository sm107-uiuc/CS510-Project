{"url": "https://towardsdatascience.com/demystifying-maths-of-svm-13ccfe00091e", "time": 1682994499.872229, "path": "towardsdatascience.com/demystifying-maths-of-svm-13ccfe00091e/", "webpage": {"metadata": {"title": "Demystifying Maths of SVM \u2014 Part 1 | by Krishna Kumar Mahto | Towards Data Science", "h1": "Demystifying Maths of SVM \u2014 Part 1", "description": "So, three days into SVM, I was 40% frustrated, 30% restless, 20% irritated and 100% inefficient in terms of getting my work done. I was stuck with the Maths part of Support Vector Machine. I went\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.youtube.com/watch?v=s8B4A5ubw6c", "anchor_text": "Andrew Ng\u2019s Stanford lectures", "paragraph_index": 0}, {"url": "http://cs229.stanford.edu/notes/cs229-notes3.pdf", "anchor_text": "Andrew Ng\u2019s lecture notes", "paragraph_index": 41}, {"url": "https://www.youtube.com/watch?v=s8B4A5ubw6c", "anchor_text": "lecture series available on YouTube", "paragraph_index": 41}, {"url": "https://bit.ly/2BnYnpz", "anchor_text": "here", "paragraph_index": 43}, {"url": "http://linkedin.com/in/krishna-kumar-mahto", "anchor_text": "linkedin.com/in/krishna-kumar-mahto", "paragraph_index": 45}], "all_paragraphs": ["So, three days into SVM, I was 40% frustrated, 30% restless, 20% irritated and 100% inefficient in terms of getting my work done. I was stuck with the Maths part of Support Vector Machine. I went through a number of YouTube videos, a number of documents, PPTs and PDFs of lecture notes, but everything seemed too indistinct for me. Out of all these, I found Andrew Ng\u2019s Stanford lectures the most useful. Although he falls a little short in his ability to convey everything he intends to, his notes and derivations flow down very smoothly. Whatever I am going to discuss was inspired 50% by Andrew Ng\u2019s lectures and his notes, 20% by one of the ML courses I am taking, and 29% by everything else and the rest of 1% comes from the little work which I put together into building this up. At the end, it turns out that it is not at all difficult to understand how SVM came up as all it takes is high school coordinate and vector geometry. For the most part, finding the right dots to make a sensible map was what I found difficult. With this article, I have tried to lay down the mathematical derivation which I came up with by affixing ideas from different sources, along with the thought process.", "The diagram does not look to be too worrying if you know SVM at a high conceptual level (the Optimal Margin Classifier stuff). Although the cases of linearly separable datasets are not seen in real life, discussion throughout this article on SVM will be for this context only. I might do a separate post for a more general version of SVM.", "Hypothesis, w.r.t. a machine learning model is the model itself, which is nothing but our classifier (which, is a function).", "Class labels are denoted as -1 for negative class and +1 for positive class in SVM.", "The final optimization problem that we shall have derived at the end of this article and what SVM solves to fit the best parameters is:", "This is a convex optimization problem, with a convex optimization objective function and a set of constraints that define a convex set as the feasible region. Convex functions look like a bowl placed right-side-up. Convex set is a set of points in which a line joining any two points lies entirely within the set. I would have loved to talk on these in more detail, but it would be more convenient to just google the terms in italics.", "Before delving into the actual part, we should be familiar with two terms- Functional margin and Geometric margin.", "Following is how we are going to notate the hyperplane that separates the positive and negative examples throughout this article:", "Each training example is denoted as x, and superscript (i) denotes ith training example. In the following section y superscripted with (i) represents label corresponding to the ith training example.", "Functional margin of a hyperplane w.r.t. ith taining example is defined as:", "Functional margin of a hyperplane w.r.t. the entire dataset is defined as:", "Geometric margin of a hyperplane w.r.t. ith training example is defined as functional margin normalized by norm(w):", "Geometric margin for a hyperplane w.r.t. the entire dataset is defined as:", "Note: In the following discussion, if it is not specified whether the functional/geometric margin of a hyperplane is mentioned w.r.t. the entire dataset or some example, then it should be assumed to be in reference to the entire dataset, and not a single example.", "Just to make sure we are on the same page, lets discuss how SVM works. I have come across two interpretations of SVM (or more precisely, the goal that SVM aims at achieving). Both the interpretations as quoted below are just different ways of conveying the same thing as we shall see when we derive the optimization objective.", "SVM maximizes the margin (as drawn in fig. 1) by learning a suitable decision boundary/decision surface/separating hyperplane.", "SVM maximizes the geometric margin (as already defined, and shown below in figure 2) by learning a suitable decision boundary/decision surface/separating hyperplane.", "The way I have derived the optimization objective starts with using the concepts of functional and geometric margin; and after establishing that the two interpretations of SVM coexist with each other, the final optimization objective is derived.", "As said, we shall start with functional and geometric margin interpretation, and then establish the coexistence of the two interpretations of SVM.", "Could we have gone the other way round?", "I tried doing that, but it turned out to be not a better way to proceed by starting with the first interpretation. We shall discuss why that would not have worked out this well once we have derived the formulation of the optimization objective.", "As already discussed, SVM aims at maximizing the geometric margin and returns the corresponding hyperplane. What it means is that out of all possible hyperplanes (each hyperplane has a geometric margin w.r.t. the point closest to it which is the least of all other geometric margins defined w.r.t. all other points), SVM chooses that hyperplane which has the maximum geometric margin. In fig. 3, the red hyperplane is the best separating hyperplane.", "This can be mathematically written as,", "Apparently, the objective function is the geometric margin of the hyperplane (w, b). The constraints represent the fact that the objective function is the minimum of the set of geometric margins of the hyperplane (w, b) w.r.t. all the training examples.", "We can observe that the formulation of geometric margin ensures that for the optimal hyperplane, the value of the smallest of all geometric margins (computed w.r.t. all examples) is shared by atleast one pair of examples (one pair from +ve class and the other from -ve class). Such points are called as support vectors (fig.- 1).", "Therefore, the optimization problem as defined above is equivalent to the problem of maximizing the margin value (not geometric/functional margin values). Margin is defined as the distance between two hyperplanes, each of which is parallel to the separating hyperplane and passes through support vectors of each class (i.e., one hyperplane passes through support vectors of +ve class, while the other hyperplane passes through support vectors of -ve class, and both are parallel to the separating hyperplanes).", "At this point, therefore, we can establish that both the interpretations of SVM actually coexist, although we started with the second interpretation to come to this conclusion. Towards the end, I shall discuss that while going the other way round, we carry some redundancy in our formulation and therefore, start off with less cleaner ideation of the problem.", "Let us denote the respective hyperplanes as:", "Therefore, the optimization problem can be reformulated with the following objective function:", "The simplification till now has been done only in terms of writing smaller notations and smaller expressions. We have not cut down anything in terms of the computations to be done in comparison to where we had started (actually, it would be nice to verify this on your own).", "The idea that will be executed as the next step in developing the final form of the optimization problem of the SVM, does not appear to be as straightforward to ideate.", "We have been doing vector geometry since the beginning of this article. It turns out that SVM is actually geometrically motivated algorithm at its core. What is done to further simplify the objective function is not difficult apply or understand, but out of many operations that you could possibly do on an expression, zeroing down on the following idea feels non-trivial.", "One of the things about equation of a hyperplane is that scaling it does not change the hyperplane. It gives a new equation (w\u2019, b\u2019), where w\u2019 = k . w and b\u2019 = k . b (k is the scaling factor) but the hyperplane remains the same in space and the geometric margin therefore, also does not change (but functional margin does change). Leveraging this factor, what we can do is we can scale the w and b in our optimization objective function such that functional margin becomes 1.", "This makes our optimization problem as:", "This, in true sense, has reduced the optimization problem by reducing the number of computations to be performed!", "We have gone through almost the entire derivation, with the last part we just saw being the most crucial one.", "However, there is a problem with this formulation. The objective function is a non-convex function, which is preferred to be avoided (fig. 4).", "Since we apply gradient descent to optimize the function, non-convex functions may cause the algorithm to get stuck at a local minimum. But it turns out that squaring the norm(w) in the multiplicative inverse of the objective function gives a convex, differentiable function (inverse of the objective function is also convex, but is non-differentiable at w = 0, you can verify this by using an online plotting tool). We can use this as our objective function, with the optimization problem being a little modified now:", "In fact, this is the final objective function which is perfectly convex also (fig. 5)! We love convex optimization problems.", "Note that maximizing a function f is same as minimizing a function g = 1/f. That is why the optimization problem is now a minimization problem opposed to being a maximization problem as before.", "A few more points to conclude:", "Hopefully I have written something sensible with correct Maths. I used Andrew Ng\u2019s lecture notes and his lecture series available on YouTube (both from Stanford offline class notes, not Coursera), some ideas from a paid course that I am doing and a number of visits to stackoverflow.com, and then compiled them all into this. Hope you liked it.", "Also, kindly pardon my handwriting. If what is written in those images are not clear, do let me know. I shall replace them with new pictures.", "I have also written an article deriving the Soft-margin SVM. You can find it here. Thank you.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Software Engineer-Intern at EIG | Machine Learning Enthusiast | Python | VIT | linkedin.com/in/krishna-kumar-mahto"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F13ccfe00091e&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-maths-of-svm-13ccfe00091e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-maths-of-svm-13ccfe00091e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-maths-of-svm-13ccfe00091e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-maths-of-svm-13ccfe00091e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----13ccfe00091e--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----13ccfe00091e--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@krish.thorcode?source=post_page-----13ccfe00091e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@krish.thorcode?source=post_page-----13ccfe00091e--------------------------------", "anchor_text": "Krishna Kumar Mahto"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe1dae62fc758&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-maths-of-svm-13ccfe00091e&user=Krishna+Kumar+Mahto&userId=e1dae62fc758&source=post_page-e1dae62fc758----13ccfe00091e---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F13ccfe00091e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-maths-of-svm-13ccfe00091e&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F13ccfe00091e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-maths-of-svm-13ccfe00091e&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.youtube.com/watch?v=s8B4A5ubw6c", "anchor_text": "Andrew Ng\u2019s Stanford lectures"}, {"url": "https://www.researchgate.net/figure/Classification-of-data-by-support-vector-machine-SVM_fig8_304611323", "anchor_text": "https://www.researchgate.net/figure/Classification-of-data-by-support-vector-machine-SVM_fig8_304611323"}, {"url": "https://www.geogebra.org/", "anchor_text": "https://www.geogebra.org/"}, {"url": "https://www.geogebra.org", "anchor_text": "https://www.geogebra.org"}, {"url": "http://cs229.stanford.edu/notes/cs229-notes3.pdf", "anchor_text": "Andrew Ng\u2019s lecture notes"}, {"url": "https://www.youtube.com/watch?v=s8B4A5ubw6c", "anchor_text": "lecture series available on YouTube"}, {"url": "https://bit.ly/2BnYnpz", "anchor_text": "here"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----13ccfe00091e---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/classifcation-models?source=post_page-----13ccfe00091e---------------classifcation_models-----------------", "anchor_text": "Classifcation Models"}, {"url": "https://medium.com/tag/svm?source=post_page-----13ccfe00091e---------------svm-----------------", "anchor_text": "Svm"}, {"url": "https://medium.com/tag/supervised-learning?source=post_page-----13ccfe00091e---------------supervised_learning-----------------", "anchor_text": "Supervised Learning"}, {"url": "https://medium.com/tag/cost-function?source=post_page-----13ccfe00091e---------------cost_function-----------------", "anchor_text": "Cost Function"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F13ccfe00091e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-maths-of-svm-13ccfe00091e&user=Krishna+Kumar+Mahto&userId=e1dae62fc758&source=-----13ccfe00091e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F13ccfe00091e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-maths-of-svm-13ccfe00091e&user=Krishna+Kumar+Mahto&userId=e1dae62fc758&source=-----13ccfe00091e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F13ccfe00091e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-maths-of-svm-13ccfe00091e&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----13ccfe00091e--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F13ccfe00091e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-maths-of-svm-13ccfe00091e&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----13ccfe00091e---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----13ccfe00091e--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----13ccfe00091e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----13ccfe00091e--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----13ccfe00091e--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----13ccfe00091e--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----13ccfe00091e--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----13ccfe00091e--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----13ccfe00091e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@krish.thorcode?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@krish.thorcode?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Krishna Kumar Mahto"}, {"url": "https://medium.com/@krish.thorcode/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "211 Followers"}, {"url": "http://linkedin.com/in/krishna-kumar-mahto", "anchor_text": "linkedin.com/in/krishna-kumar-mahto"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe1dae62fc758&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-maths-of-svm-13ccfe00091e&user=Krishna+Kumar+Mahto&userId=e1dae62fc758&source=post_page-e1dae62fc758--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Fe1dae62fc758%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-maths-of-svm-13ccfe00091e&user=Krishna+Kumar+Mahto&userId=e1dae62fc758&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}