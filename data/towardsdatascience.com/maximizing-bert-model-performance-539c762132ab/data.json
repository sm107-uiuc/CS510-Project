{"url": "https://towardsdatascience.com/maximizing-bert-model-performance-539c762132ab", "time": 1683015988.438123, "path": "towardsdatascience.com/maximizing-bert-model-performance-539c762132ab/", "webpage": {"metadata": {"title": "Maximizing BERT model performance | by Ajit Rajasekharan | Towards Data Science", "h1": "Maximizing BERT model performance", "description": "Training a BERT model from scratch on a domain specific corpus such as biomedical space with a custom vocabulary generated specific to that space has proven to be critical to maximize model\u2026"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/pdf/1901.08746.pdf", "anchor_text": "(e.g. BioBERT. Pathway 1a \u2192 1b \u2192 1c \u21921d in Figure 1)", "paragraph_index": 1}, {"url": "https://arxiv.org/pdf/1903.10676.pdf", "anchor_text": "SciBERT", "paragraph_index": 1}, {"url": "https://towardsdatascience.com/unsupervised-ner-using-bert-2d7af5f90b8a", "anchor_text": "unsupervised NER", "paragraph_index": 7}, {"url": "https://towardsdatascience.com/unsupervised-creation-of-interpretable-sentence-representations-851e74921cf9", "anchor_text": "sentence representations", "paragraph_index": 7}, {"url": "https://arxiv.org/pdf/2007.15779.pdf", "anchor_text": "MS Models were pre-trained", "paragraph_index": 19}, {"url": "https://towardsdatascience.com/unsupervised-ner-using-bert-2d7af5f90b8a", "anchor_text": "unsupervised NER", "paragraph_index": 25}, {"url": "https://qr.ae/pNgh9D", "anchor_text": "Quora", "paragraph_index": 26}], "all_paragraphs": ["Training a BERT model from scratch on a domain specific corpus such as biomedical space with a custom vocabulary generated specific to that space has proven to be critical to maximize model performance in biomedical domain. This is largely because of language characteristics that are unique to biomedical space which is insufficiently represented in the original pre-trained models released by Google (self-supervised training of a BERT model is often called pre-training). These domain specific language characteristics are", "Training the original BERT models further on a domain specific corpus retaining the original vocabulary, often called continual pre-training, and then fine tuning the model further on a supervised task has been shown to increase model performance(e.g. BioBERT. Pathway 1a \u2192 1b \u2192 1c \u21921d in Figure 1). However, such a model\u2019s performance still lags behind a model that is pre-trained from scratch on a domain specific corpus with a domain specific vocabulary (e.g. SciBERT. Pathway 2a \u2192 2b \u2192 2d in Figure 1).", "Given the typical use of BERT models is to take an existing pre-trained model and fine tune it on a task (pathway 1a \u21921b \u21921d in Figure 1), an implicit assumption is often made in practice that the original pre-trained model is trained \u201cwell\u201d and performs adequately. Only the fine-tuned model is evaluated \u2014 the quality of the pre-trained model is taken for granted.", "However, when we pre-train a BERT model from scratch with a custom vocabulary to maximize performance, we need a way to evaluate the quality of the pre-trained model. BERT\u2019s pre-training model loss on masked language objective (predicting masked or corrupted tokens in a sentence) or next sentence objective alone may not suffice in practice.", "The performance checks listed below could be of value to examine a pre-trained model", "These two performance measures are used below to examine the quality of a few publicly released BERT pre-trained models.", "The result of this exercise, discussed below, underscores the importance of evaluating pre-training models to maximize model performance. It also helps us determine if we need to pre-train a publicly released model further, even if it is was pre-trained in the domain of interest to us, before we fine tune it for a domain specific task.", "Self-supervised learning in BERT is accomplished by masking and corrupting few words in each sentence and have the model learn by predicting those words. Though this form of learning (autoencoder models) limits the model\u2019s generative capacity to create new sentences in contrast to autoregressive models like GPT-2, a pre-trained BERT model can be used as is for unsupervised NER, sentence representations etc. without the need for any labeled data, leveraging off the learned vectors constituting BERT\u2019s vocabulary (context independent vectors) and the MLM (Masked Language Model) capability of the model \u2014 i.e. the \u201cfill in the blanks\u201d capability of BERT using context sensitive vectors (vectors that emerge out a model for a masked position \u2014 neighbors to these vectors in the original vocabulary are candidates for filling in the blank).", "However, the popular and typical use of a pre-trained BERT model is to fine tune it for a downstream supervised task. Fine tuning may be preceded by continual pre-training in some cases to increase model performance (Figure 2).", "Any form of model training, pre-training, continual pre-training or fine tuning, modifies both model weights as well as the vocabulary vectors \u2014 the different shades of same color model (shades of beige) as well as vocabulary(shades of blue) in the training stages from left to right illustrates this fact in figures 1 and 2.", "Evidence from examining the models below suggests that, regardless of the training pathways used (1a-1d or 2a-2d in Figure 1), the evaluation of a pre-trained model performance (steps 3 and 6 in Figure 2) is perhaps key to maximize model performance in downstream tasks.", "Three broad categories of \u201cfill in the blank\u201d tests (Masked Language Model predictions) are used to evaluate model performance. These tests are essentially sentences with a masked position. The model prediction scores (over the entire vocabulary) for the masked position is used to evaluate model performance.", "Domain specific entities and sentence structure test samples", "Basic language sentence structure test samples", "While the tests above are purely illustrative samples for the three broad categories, practitioners can use just the few qualitative tests like the ones above to detect a model is performing poorly while or after training a model. The two Microsoft pre-trained models are examples of this \u2014 they perform consistently poorly in all the tests. A poorly performing model in addition to inaccurate predictions, also exhibits other signs of inadequately/improperly pre-training \u2014 they have the same signature noise like terms for different/distinct input sentences.", "However, to determine if a model is pre-trained for maximum performance, one would have to create sufficient number of test cases across these categories and then score the performance based on the top model predictions for the blanked positions.", "In addition to context sensitive vectors output by a trained model exhibiting measurable signs of inadequate/poor training, vectors constituting BERT\u2019s vocabulary could also reveal measurable information about the training quality. For instance,", "Histogram of cosine distribution of various models", "The figures above do not reveal anything out of the ordinary for all 5 evaluated models, indicating there is no noticeable anomaly in the training of vectors constituting BERT\u2019s vocabulary. The cosine distance neighbor tests below for few sample words further confirms this.", "One curious property however of MS pre-trained models including the continually pre-trained model (figure 6) is the vocabulary vectors which are 768 dimensions (like bert-base models ) have a distribution profile that is like a 1024 BERT model with whole word masking (figure 7). It could very well be whole word masking causes this distribution profile (MS Models were pre-trained with whole word masking). If that is the case, it is not clear why whole word masking causes such a large number of vectors to be nearly orthogonal to other vectors ( implementation notes below elaborates this but does not answer this question yet) even after training \u2014 the y-intercept above of MS-pretrained models and BERT whole word (cased and uncased) are over a thousand \u2014 in stark contrast to BERT base and large models without whole word masking, which are under 500.", "Top 10 neighbors for few vocabulary test terms used earlier", "The neighbor test for the terms below, both of which are present in the vocabulary of all the evaluated models except BERT, shows that their neighbors are all quite good, even for the two MS pre-trained models and SciBERT. These three models performed poorly in the sentence test containing these terms however, suggesting that the training of the model layers were inadequate as opposed to the vocabulary vectors themselves. The average length of the distribution tail of MS models is relatively high, indicative of high quality neighbors, offering further evidence to well trained vocabulary vectors (implementation notes below has examples)", "Also when examining the clusters formed by the vectors of Microsoft pre-trained model (Abstract + full text) and its continually pre-trained version, where clustering is done with a fixed threshold (60 degrees \u2014 .5 cosine), 80% percent of the clusters were identical. This further confirms the fact, the vocabulary vectors were well trained even in the original pre-trained model. It was the model layers that required further training. Clustering of vocabulary vectors without a fixed threshold but with a cut-off determined on an individual term basis (largely based on single digit counts of neighbors for a particular cosine distance), shows long tails for MS models (example in implementation notes). This offers further evidence of well trained vocabulary vectors.", "Figure below captures the sequence flow for maximizing model performance during training and continual pre-training. The sequence below applies even in the case of a model outputting a set of checkpoints (as opposed to one final checkpoint) and we evaluate each checkpoint as illustrated in a flow. However, in this case, if a checkpoint fails MLM check we can discard it, if a more recent checkpoint was chosen as a candidate.", "Pre-trained or continually pre-trained on a domain specific corpus with a custom vocabulary has proven to be key for practitioners particularly when using the model for a variety of downstream tasks, be it supervised or unsupervised. Any approach not leveraging off a domain specific custom vocabulary has been proven to yield relatively suboptimal results, at least in biomedical domain. When pre-training or continually training, instead of the typical approach of just pre-training a certain number of steps or epochs and fine tuning on downstream tasks, evaluating the model along the lines discussed in this article may yield better performance in those downstream tasks.", "The utility of \u201cfill in the blanks\u201d(MLM) learning approach of BERT from a practitioner\u2019s perspective cannot be overstated, despite the fact that this learning approach limits the model from being a generative one like GPT-2. In addition to being useful for evaluating the quality of trained BERT model as illustrated in this article, \u201cfill in the blanks\u201d capability facilitates transforming a traditionally supervised task into an unsupervised one(e.g. unsupervised NER), when used in conjunction with the learned vocabulary vectors.", "This article was originally posted in Quora", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F539c762132ab&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaximizing-bert-model-performance-539c762132ab&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaximizing-bert-model-performance-539c762132ab&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaximizing-bert-model-performance-539c762132ab&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaximizing-bert-model-performance-539c762132ab&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----539c762132ab--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----539c762132ab--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://ajitrajasekharan.medium.com/?source=post_page-----539c762132ab--------------------------------", "anchor_text": ""}, {"url": "https://ajitrajasekharan.medium.com/?source=post_page-----539c762132ab--------------------------------", "anchor_text": "Ajit Rajasekharan"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffd04a90b4be7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaximizing-bert-model-performance-539c762132ab&user=Ajit+Rajasekharan&userId=fd04a90b4be7&source=post_page-fd04a90b4be7----539c762132ab---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F539c762132ab&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaximizing-bert-model-performance-539c762132ab&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F539c762132ab&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaximizing-bert-model-performance-539c762132ab&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://github.com/google-research/bert", "anchor_text": "bert-base/large-cased/uncased, or the tiny bert versions"}, {"url": "https://towardsdatascience.com/unsupervised-ner-using-bert-2d7af5f90b8a", "anchor_text": "largely skewed towards people, locations, organizations etc."}, {"url": "https://arxiv.org/pdf/1901.08746.pdf", "anchor_text": "(e.g. BioBERT. Pathway 1a \u2192 1b \u2192 1c \u21921d in Figure 1)"}, {"url": "https://arxiv.org/pdf/1903.10676.pdf", "anchor_text": "SciBERT"}, {"url": "https://towardsdatascience.com/unsupervised-ner-using-bert-2d7af5f90b8a", "anchor_text": "unsupervised NER"}, {"url": "https://towardsdatascience.com/unsupervised-creation-of-interpretable-sentence-representations-851e74921cf9", "anchor_text": "sentence representations"}, {"url": "https://towardsdatascience.com/unsupervised-ner-using-bert-2d7af5f90b8a", "anchor_text": "unsupervised NER"}, {"url": "https://towardsdatascience.com/unsupervised-creation-of-interpretable-sentence-representations-851e74921cf9", "anchor_text": "unsupervised sentence representations"}, {"url": "https://github.com/google-research/bert", "anchor_text": "11 model flavors of BERT \u2014 all trained on Wikipedia and Bookcorpus"}, {"url": "https://github.com/allenai/scibert", "anchor_text": "SciBERT trained on papers from Semantic Scholar"}, {"url": "https://microsoft.github.io/BLURB/models.html#page-top;", "anchor_text": "Microsoft PubmedBERT"}, {"url": "https://drive.google.com/file/d/1De5P6kwiAcu-8uU96gMuu8hZXpXnpdQc/view?usp=sharing", "anchor_text": "Continual pre-training of Microsoft PubmedBERT was done for this evaluation by me on clinical t"}, {"url": "https://drive.google.com/file/d/1De5P6kwiAcu-8uU96gMuu8hZXpXnpdQc/view", "anchor_text": "to evaluate"}, {"url": "https://arxiv.org/pdf/2007.15779.pdf", "anchor_text": "two of the three models"}, {"url": "https://arxiv.org/pdf/2007.15779.pdf", "anchor_text": "MS Models were pre-trained"}, {"url": "https://towardsdatascience.com/unsupervised-ner-using-bert-2d7af5f90b8a", "anchor_text": "unsupervised NER"}, {"url": "https://github.com/google-research/bert", "anchor_text": "BERT\u2019s Github page"}, {"url": "https://github.com/huggingface/transformers", "anchor_text": "HuggingFace"}, {"url": "https://github.com/google-research/bert.git", "anchor_text": "original BERT release code"}, {"url": "https://github.com/google-research/bert.git", "anchor_text": "original BERT release code"}, {"url": "https://github.com/google-research/bert.git", "anchor_text": "original BERT release code"}, {"url": "https://github.com/google-research/bert.git", "anchor_text": "original BERT release code"}, {"url": "https://huggingface.co/datasets/bookcorpus", "anchor_text": "Bookcorpus"}, {"url": "https://github.com/google-research/bert.git", "anchor_text": "original BERT release code"}, {"url": "https://towardsdatascience.com/unsupervised-ner-using-bert-2d7af5f90b8a", "anchor_text": "unsupervised NER"}, {"url": "https://github.com/ajitrajasekharan/bert_mask", "anchor_text": "MLM check"}, {"url": "https://github.com/ajitrajasekharan/bert_vector_clustering", "anchor_text": "vocab check"}, {"url": "https://huggingface.co/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext#list-files", "anchor_text": "Timestamp of MS Abstract + fulltext model used in this evaluation"}, {"url": "https://huggingface.co/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract#list-files", "anchor_text": "Timestamp of MS Abstract only model used in this evaluation"}, {"url": "https://towardsdatascience.com/swiss-army-knife-for-unsupervised-task-solving-26f9acf7c023", "anchor_text": "learn quality sentence representations that can be useful for a variety of tasks"}, {"url": "https://qr.ae/pNgh9D", "anchor_text": "Quora"}, {"url": "https://medium.com/tag/nlp?source=post_page-----539c762132ab---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/naturallanguageprocessing?source=post_page-----539c762132ab---------------naturallanguageprocessing-----------------", "anchor_text": "Naturallanguageprocessing"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----539c762132ab---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/ai?source=post_page-----539c762132ab---------------ai-----------------", "anchor_text": "AI"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----539c762132ab---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F539c762132ab&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaximizing-bert-model-performance-539c762132ab&user=Ajit+Rajasekharan&userId=fd04a90b4be7&source=-----539c762132ab---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F539c762132ab&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaximizing-bert-model-performance-539c762132ab&user=Ajit+Rajasekharan&userId=fd04a90b4be7&source=-----539c762132ab---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F539c762132ab&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaximizing-bert-model-performance-539c762132ab&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----539c762132ab--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F539c762132ab&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaximizing-bert-model-performance-539c762132ab&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----539c762132ab---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----539c762132ab--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----539c762132ab--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----539c762132ab--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----539c762132ab--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----539c762132ab--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----539c762132ab--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----539c762132ab--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----539c762132ab--------------------------------", "anchor_text": ""}, {"url": "https://ajitrajasekharan.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://ajitrajasekharan.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ajit Rajasekharan"}, {"url": "https://ajitrajasekharan.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "779 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffd04a90b4be7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaximizing-bert-model-performance-539c762132ab&user=Ajit+Rajasekharan&userId=fd04a90b4be7&source=post_page-fd04a90b4be7--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F974aed893170&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaximizing-bert-model-performance-539c762132ab&newsletterV3=fd04a90b4be7&newsletterV3Id=974aed893170&user=Ajit+Rajasekharan&userId=fd04a90b4be7&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}