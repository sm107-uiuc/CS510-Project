{"url": "https://towardsdatascience.com/the-definitive-guide-to-bidaf-part-2-word-embedding-character-embedding-and-contextual-c151fc4f05bb", "time": 1683000153.185364, "path": "towardsdatascience.com/the-definitive-guide-to-bidaf-part-2-word-embedding-character-embedding-and-contextual-c151fc4f05bb/", "webpage": {"metadata": {"title": "Word Embedding, Character Embedding and Contextual Embedding in BiDAF \u2014 an Illustrated Guide | by Meraldo Antonio | Towards Data Science", "h1": "Word Embedding, Character Embedding and Contextual Embedding in BiDAF \u2014 an Illustrated Guide", "description": "BiDAF is an NLP model for Question and Answering tasks. Here, I illustrate how it uses 3 embedding mechanisms to convert words into vectors."}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/abs/1611.01603", "anchor_text": "Bi-Directional Attention Flow (BiDAF)", "paragraph_index": 0}, {"url": "https://medium.com/@meraldo.antonio/the-definitive-guide-to-bi-directional-attention-flow-d0e96e9e666b", "anchor_text": "The first article", "paragraph_index": 2}, {"url": "https://nlp.stanford.edu/projects/glove/", "anchor_text": "GloVe", "paragraph_index": 5}, {"url": "http://mlexplained.com/2018/04/29/paper-dissected-glove-global-vectors-for-word-representation-explained/", "anchor_text": "excellent", "paragraph_index": 5}, {"url": "https://nlp.seas.harvard.edu/slides/aaai16.pdf", "anchor_text": "slides", "paragraph_index": 19}, {"url": "https://towardsdatascience.com/the-definitive-guide-to-bidaf-part-3-attention-92352bbdcb07", "anchor_text": "next article", "paragraph_index": 40}, {"url": "https://arxiv.org/abs/1611.01603", "anchor_text": "Bi-Directional Attention Flow for Machine Comprehension (Minjoon Seo et. al, 2017)", "paragraph_index": 41}, {"url": "https://www.linkedin.com/in/meraldo/", "anchor_text": "LinkedIn", "paragraph_index": 43}, {"url": "https://medium.com/@meraldo.antonio/membership", "anchor_text": "my referral link", "paragraph_index": 43}], "all_paragraphs": ["This article is the second in a series of four articles that aim to illustrate the working of Bi-Directional Attention Flow (BiDAF), a popular machine learning model for question and answering (Q&A).", "To recap, BiDAF is an closed-domain, extractive Q&A model. This means that to be able to answer a Query, BiDAF needs to consult the an accompanying text that contains the information needed to answer the Query. This accompanying text is called the Context. BiDAF works by extracting a substring of the Context that best answers the query \u2014 this is what what we refer to as the Answer to the Query. I intentionally capitalize the words Query, Context and Answer to signal that I am using them in their specialized technical capacities.", "The first article in the series provided a high-level overview of BiDAF. In this article, we will focus on the first portion of the BiDAF architecture \u2014 the first thing that takes place when the model receives an incoming Query and its accompanying Context. To facilitate your learning, a glossary containing the mathematical notations involved in these steps is provided at the end. Let\u2019s dive in!", "In BiDAF, the incoming Query and its Context are first tokenized, i.e. these two long strings are broken down into their constituent words. In the BiDAF paper, the symbols T and J are used to denote the number of words in Context and Query, respectively. Here is a depiction of the tokenization:", "The resulting words are then subjected to the embedding process, where they are converted into vectors of numbers. These vectors capture the grammatical function (syntax) and the meaning (semantics) of the words, enabling us to perform various mathematical operations on them. In BiDAF, embedding is done on three levels of granularity: on the character, word and contextual levels. Let\u2019s now focus on the first embedding layer \u2014 the word embedding.", "The word embedding algorithm used in the original BiDAF is GloVe. In this article, I will only give a brief overview of GloVe because there already exist several excellent resources that explain how the algorithm works. But if you are short on time, here is a very simplified summary of GloVe:", "GloVe is an unsupervised learning algorithm that uses co-occurrence frequencies of words in a corpus to generate the words\u2019 vector representations. These vector representations numerically represent various aspects of the words\u2019 meaning.", "As the numbers that make up GloVe vectors encapsulate semantic and syntactic information about the words they represent, we can perform some cool stuff using these vectors! For instance, we can use subtraction of GloVe vectors to find word analogies, as illustrated below.", "BiDAF uses pre-trained GloVe embeddings to get the vector representation of words in the Query and the Context. \u201cPre-trained\u201d means that the GloVe representations used here have already been trained; their values are frozen and won\u2019t be updated during training. Thus, you can think of BiDAF\u2019s word embedding step as a simple dictionary lookup step where we substitute words (the \u201ckeys\u201d of the GloVe \u201cdictionary\u201d) with vectors (the \u201cvalues\u201d of the \u201cdictionary\u201d).", "The output of the word embedding step is two matrices \u2014 one for the Context and one for the Query. The lengths of these matrices equal the number of words in the Context and the Query (T for the Context matrix and J for the Query matrix). Meanwhile, their height , d1, is a preset value that is equal to the vector dimension from GloVe; this can either be 50, 100, 200 or 300. The figure below depicts the word embedding step for the Context:", "Okay, so with GloVe, we obtain the vector representations of most words. However, the GloVe representations are not enough for our purpose!", "The pretrained GloVe \u201cdictionary\u201d is huge and contains millions of words; however, there will come a time where we encounter a word in our training set that is not present in GloVe\u2019s vocabulary. Such a word is called an out-of-vocabulary (OOV) word. GloVe deals with these OOV words by simply assigning them some random vector values. If not remedied, this random assignment would end up confusing our model.", "Therefore, we need another embedding mechanism that can handle OOV words. This is where the character level embedding comes in. Character level embedding uses one-dimensional convolutional neural network (1D-CNN) to find numeric representation of words by looking at their character-level compositions.", "You can think of 1D-CNN as a process where we have several scanners sliding through a word, character by character. These scanners can focus on several characters at a time. As these scanners sweep along, they extract the information from the characters they are focusing on. At the end of these scanning process, information from different scanners are collected to form the representation of a word.", "The output of the character embedding step is similar to the output of the word embedding step. We obtain two matrices, one for the Context and the other for the Query. The lengths of these matrices equal the number of words in the Context and in the Query \u2014 T and J \u2014 while their height depends on the number of convolutional filters used in 1D-CNN (to know what a \u201cconvolutional filter\u201d is, do read the next section). The height is denoted as d2 in the diagram. These two matrices will be concatenated with the matrices that we obtained from the word embedding step.", "The section above only presents a very conceptual overview of the workings of 1D-CNN. In this section, I will explain how 1D-CNN works in details. Strictly speaking, these details are not necessary to understand how BiDAF works; as such, feel free to jump ahead if you are short on time. However, if you are the type of person who can\u2019t sleep well without understanding every moving part of an algorithm you are learning about, this section is for you!", "The idea that motivates the use of 1D-CNN is that not only words as a whole have meanings \u2014 word parts can carry meaning, too!", "For example, if you know the meaning of the word \u201cunderestimate\u201d, you will understand the meaning of \u201cmisunderestimate\u201d, although the latter isn\u2019t actually a real word. Why? Because you know from your knowledge of the English language that the prefix \u201cmis-\u201d usually indicates the concept of \u201cmistaken\u201d; this allows you to deduce that \u201cmisunderestimate\u201d refers to \u201cmistakenly underestimate\u201d something.", "1D-CNN is an algorithm that mimics this human capability to understand word parts. More broadly speaking, 1D-CNN is an algorithm capable of extracting information from shorter segments of a long input sequence. This input sequence can be music, DNA, voice recording, weblogs, etc. In BiDAF, this \u201clong input sequence\u201d is words and the \u201cshorter segments\u201d are the letter combinations and morphemes that make up the words.", "To understand how 1D-CNN works, let\u2019s look at the series of illustrations below, which are taken from slides by Yoon Kim et. al., a group from Harvard University.", "2. Next, we create a convolutional filter H. This convolutional filter (also known as \u201ckernel\u201d) is a matrix with which we will \u201cscan\u201d the word. Its height, d, is the same as the height of C but its width w is a number that is smaller than l. The values within H are randomly initialized and will be adjusted during model training.", "3. We overlay H on the leftmost corner of C and take an element-wise product of H and its projection on C (a fancy word to describe this process is taking a Hadamard product of H and its projection on C). This process outputs a matrix that has the same dimension as H \u2014 a d x l matrix. We then sum up all the numbers in this output matrix to get a scalar. In our example, the scalar is 0.1. This scalar is set as the first element of a new vector called f.", "4. We then slide H one character to the right and perform the same operations (get the Hadamard product and sum up the numbers in the resulting matrix) to get another scalar, 0.7. This scalar is set as the second element of f.", "5. We repeat these operations character by character until we reach the end of the word. In each step, we add one more element to f and lengthen the vector until it reaches its maximum length which is l - w+1. The vector f is a numeric representation of the word \u201cabsurdity\u201d obtained when we look at this word three characters at a time. One thing to note is that the values within the convolution filter H don\u2019t change as H slides through the word. In fancier terms, we call H \u201cposition invariant\u201d. The position invariance of the convolutional filters enables us to capture the meaning of a certain letter combination no matter where in the word such combination appears.", "6. We record the maximum value in f. This maximum can be thought of as the \u201csummary\u201d of f. In our example, this number is 0.7. We shall refer to this number as the \u201csummary scalar\u201d of f. This process of taking a maximum value of the vector f is also referred to as \u201cmax-pooling\u201d.", "7. We then repeat all of the above steps with yet another convolutional filter (yet another H!). This convolutional filter might have a different width. In our example below, our second H, denoted H\u2019, has a width of 2. As with the first filter, we slide along H\u2019 across the word to get the vector f and then perform max-pooling on f (i.e. get its summary scalar).", "8. We repeat this scanning process several times with different convolutional filters, with each scanning process resulting in one summary scalar. Finally, the summary scalars from these different scanning processes are collected to form the character embedding of the word.", "So that\u2019s it \u2014 now we\u2019ve obtained a character-based representation of the word that can complement is word-based representation. That's the end of this little digression on 1D-CNN; now let's get back to talking about BiDAF.", "At this point, we have obtained two sets of vector representations for our words \u2014 one from the GloVe (word) embedding and the other from 1D-CNN (character) embedding. The next step is to vertically concatenate these representations.", "This concatenation produces two matrices, one for the Context and the other for the Query. Their height is d, which is the sum of d1 and d2. Meanwhile, their lengths are still the same as their predecessor matrices (T for the Context matrix and J for the Query matrix).", "These matrices are then passed through a so-called highway network. A highway network is very similar to a feed forward neural network. You guys are probably familiar with feed forward neural network already. To remind you, if we insert an input vector y into a single layer of feed forward neural network, three things will happen before the output z is produced:", "In a highway network, only a fraction of the input will be subjected to the three aforementioned steps; the remaining fraction is permitted to pass through the network untransformed. The ratio of these fractions is managed by t, the transform gate and by (1-t),the carry gate. The value of t is calculated using a sigmoid function and is always between 0 and 1. Now, our equation becomes as follows:", "Upon exiting the network, the transformed fraction of the input is summed with its untransformed fraction.", "The highway network\u2019s role is to adjust the relative contribution from the word embedding and the character embedding steps. The logic is that if we are dealing with an OOV word such as \u201cmisunderestimate\u201d, we would want to increase the relative importance of the word\u2019s 1D-CNN representation because we know that its GloVe representation is likely to be some random gibberish. On the other hand, when we are dealing with a common and unambiguous English word such as \u201ctable\u201d, we might want to have more equal contribution from GloVe and 1D-CNN.", "The outputs of the highway network are again two matrices, one for the Context (a d-by-T matrix) and one for the Query (a d-by-J matrix). They represent the adjusted vector representations of words in the Query and the Context from word and character embedding steps.", "It turns out that these representations are still insufficient for our purpose! The problem is that these word representations don\u2019t take into account the words\u2019 contextual meaning \u2014 the meaning derived from the words' surroundings. When we rely on word and character embedding alone, a pair of homonyms such as the words \u201ctear\u201d (watery excretion from the eyes) and \u201ctear\u201d (rip apart) will be assigned the exact same vector representation although these are actually different words. This might confuse our model and reduce its accuracy.", "Thus, we need an embedding mechanism that can understand a word in its context. This is where the contextual embedding layer comes in. The contextual embedding layer consists of Long-Short-Term-Memory (LSTM) sequences. Here is a quick introduction to LSTM:", "An LSTM is a neural network architecture that can memorize long-term dependencies. When we enter an input sequence (such as a string of text) into a normal forward LSTM layer, the output sequence for each timestep will encode information from that timestep as well as past timesteps. In other words, the output embedding for each word will contain contextual information from words that came before it.", "BiDAF employs a bidirectional-LSTM (bi-LSTM), which is composed of both forward- as well as backward-LSTM sequences. The combined output embeddings from the forward- and backward-LSTM simultaneously encode information from both past (backwards) and future (forward) states. Put differently, each word representation coming out of this layer now includes include contextual information about the surrounding phrases of the word.", "The output of the contextual embedding step is two matrices \u2014 one from the Context and the other from the Query. The BiDAF paper refers to these matrices as H and U, respectively (terminologies alert \u2014 this matrix H is distinct from the convolutional matrix H mentioned earlier; it is an unfortunate coincidence that the two sources use the same symbol for two different concepts!). The Context matrix H is a d-by-T matrix while the Query matrix U is a d-by-J matrix.", "So that\u2019s all there is to it about the embedding layers in BiDAF! Thanks to the contribution from the three embedding layers, the embedding outputs H and U carry within them the syntactic, semantic as well as contextual information from all words in the Query and the Context. We will use H and U in the next step \u2014 the attention step \u2014 in which we will fuse together the information from them. The attention step, which is the core technical innovation of BiDAF, will be the focus of the next article in the series \u2014 do check it out!", "[1] Bi-Directional Attention Flow for Machine Comprehension (Minjoon Seo et. al, 2017)", "All images and diagrams above belong to me unless indicated otherwise.", "If you have any comments about the article or would like to reach out to me, feel free to send me a connection through LinkedIn. Also, I\u2019d be very grateful if you could support me by becoming a Medium member through my referral link. As a member, you\u2019ll be able to read all my writings on data science and personal development and have full access to all stories on Medium.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Scientist @ DBS \ud83c\uddf8\ud83c\uddec. Enthusiast of quantitative finance, operations research, MLOps, and Cloud Computing."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fc151fc4f05bb&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-definitive-guide-to-bidaf-part-2-word-embedding-character-embedding-and-contextual-c151fc4f05bb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-definitive-guide-to-bidaf-part-2-word-embedding-character-embedding-and-contextual-c151fc4f05bb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-definitive-guide-to-bidaf-part-2-word-embedding-character-embedding-and-contextual-c151fc4f05bb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-definitive-guide-to-bidaf-part-2-word-embedding-character-embedding-and-contextual-c151fc4f05bb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----c151fc4f05bb--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c151fc4f05bb--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@meraldo.antonio?source=post_page-----c151fc4f05bb--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@meraldo.antonio?source=post_page-----c151fc4f05bb--------------------------------", "anchor_text": "Meraldo Antonio"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F65b4354e2ed8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-definitive-guide-to-bidaf-part-2-word-embedding-character-embedding-and-contextual-c151fc4f05bb&user=Meraldo+Antonio&userId=65b4354e2ed8&source=post_page-65b4354e2ed8----c151fc4f05bb---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc151fc4f05bb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-definitive-guide-to-bidaf-part-2-word-embedding-character-embedding-and-contextual-c151fc4f05bb&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc151fc4f05bb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-definitive-guide-to-bidaf-part-2-word-embedding-character-embedding-and-contextual-c151fc4f05bb&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/photos/7PYqjNzvrc4", "anchor_text": "Unsplash"}, {"url": "https://arxiv.org/abs/1611.01603", "anchor_text": "Bi-Directional Attention Flow (BiDAF)"}, {"url": "https://medium.com/@meraldo.antonio/the-definitive-guide-to-bi-directional-attention-flow-d0e96e9e666b", "anchor_text": "The first article"}, {"url": "https://nlp.stanford.edu/projects/glove/", "anchor_text": "GloVe"}, {"url": "http://mlexplained.com/2018/04/29/paper-dissected-glove-global-vectors-for-word-representation-explained/", "anchor_text": "excellent"}, {"url": "https://nlp.stanford.edu/projects/glove/", "anchor_text": "Stanford's NLP Group"}, {"url": "https://nlp.seas.harvard.edu/slides/aaai16.pdf", "anchor_text": "slides"}, {"url": "https://towardsdatascience.com/the-definitive-guide-to-bidaf-part-3-attention-92352bbdcb07", "anchor_text": "next article"}, {"url": "https://arxiv.org/abs/1611.01603", "anchor_text": "Bi-Directional Attention Flow for Machine Comprehension (Minjoon Seo et. al, 2017)"}, {"url": "https://arxiv.org/abs/1508.06615", "anchor_text": "[2] Character-Aware Neural Language Models (Yoon Kim et. al, 2015)"}, {"url": "https://www.linkedin.com/in/meraldo/", "anchor_text": "LinkedIn"}, {"url": "https://medium.com/@meraldo.antonio/membership", "anchor_text": "my referral link"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----c151fc4f05bb---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/nlp?source=post_page-----c151fc4f05bb---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----c151fc4f05bb---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/data-science?source=post_page-----c151fc4f05bb---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/computer-science?source=post_page-----c151fc4f05bb---------------computer_science-----------------", "anchor_text": "Computer Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc151fc4f05bb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-definitive-guide-to-bidaf-part-2-word-embedding-character-embedding-and-contextual-c151fc4f05bb&user=Meraldo+Antonio&userId=65b4354e2ed8&source=-----c151fc4f05bb---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc151fc4f05bb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-definitive-guide-to-bidaf-part-2-word-embedding-character-embedding-and-contextual-c151fc4f05bb&user=Meraldo+Antonio&userId=65b4354e2ed8&source=-----c151fc4f05bb---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc151fc4f05bb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-definitive-guide-to-bidaf-part-2-word-embedding-character-embedding-and-contextual-c151fc4f05bb&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c151fc4f05bb--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fc151fc4f05bb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-definitive-guide-to-bidaf-part-2-word-embedding-character-embedding-and-contextual-c151fc4f05bb&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----c151fc4f05bb---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----c151fc4f05bb--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----c151fc4f05bb--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----c151fc4f05bb--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----c151fc4f05bb--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----c151fc4f05bb--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----c151fc4f05bb--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----c151fc4f05bb--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----c151fc4f05bb--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@meraldo.antonio?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@meraldo.antonio?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Meraldo Antonio"}, {"url": "https://medium.com/@meraldo.antonio/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "367 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F65b4354e2ed8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-definitive-guide-to-bidaf-part-2-word-embedding-character-embedding-and-contextual-c151fc4f05bb&user=Meraldo+Antonio&userId=65b4354e2ed8&source=post_page-65b4354e2ed8--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Ff1745664aa6d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-definitive-guide-to-bidaf-part-2-word-embedding-character-embedding-and-contextual-c151fc4f05bb&newsletterV3=65b4354e2ed8&newsletterV3Id=f1745664aa6d&user=Meraldo+Antonio&userId=65b4354e2ed8&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}