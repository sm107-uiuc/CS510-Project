{"url": "https://towardsdatascience.com/cartpole-problem-using-tf-agents-build-your-first-reinforcement-learning-application-3e6006adeba7", "time": 1683008418.072366, "path": "towardsdatascience.com/cartpole-problem-using-tf-agents-build-your-first-reinforcement-learning-application-3e6006adeba7/", "webpage": {"metadata": {"title": "CartPole Problem Using TF-Agents \u2014 Build Your First Reinforcement Learning Application | by Suraj Regmi | Towards Data Science", "h1": "CartPole Problem Using TF-Agents \u2014 Build Your First Reinforcement Learning Application", "description": "Reinforcement learning is an emerging sub-field of artificial intelligence that is both cool and effective in some of the applications. Reinforcement learning has been touched in the first few\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/creating-a-custom-environment-for-tensorflow-agent-tic-tac-toe-example-b66902f73059", "anchor_text": "this", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/creating-a-custom-environment-for-tensorflow-agent-tic-tac-toe-example-b66902f73059", "anchor_text": "blog", "paragraph_index": 4}, {"url": "https://towardsdatascience.com/creating-a-custom-environment-for-tensorflow-agent-tic-tac-toe-example-b66902f73059", "anchor_text": "blog", "paragraph_index": 6}], "all_paragraphs": ["Reinforcement learning is an emerging sub-field of artificial intelligence that is both cool and effective in some of the applications. Reinforcement learning has been touched in the first few paragraphs of this article and there are hundreds of other introductory blog articles written on the subject. Assuming the reader is familiar with the concept of reinforcement learning, let\u2019s create our first reinforcement learning application using the CartPole problem.", "CartPole problem is the problem of balancing the CartPole. CartPole is the structure where a pole is attached to the cart and the cart is free to slide across the frictionless surface. By sliding the cart left or right, the CartPole is balanced.", "So, the objective of the CartPole is to keep it from falling or moving out of the range. Therefore, the failure conditions are:", "To keep the CartPole continuing on for infinitely many steps, we limit the number of steps to some value (200 in our case).", "Just so the difference between steps and episodes is clear, I am quoting the excerpt from the previously mentioned blog:", "It is worth mentioning here about episode and step. An episode is an instance of a game (or life of a game). If the game ends or life decreases, the episode ends. Step, on the other hand, is the time or some discrete value which increases monotonically in an episode. With each change in the state of the game, the value of step increases until the game ends.", "Generally speaking, the environment is the surrounding or setting on which the agent performs actions. Here, the CartPole environment is the CartPole and the setting on which CartPole operates. Environment consists of resetting function, next step generator (on the basis of current state and action), action specification, state specification, and other helper functions. We need not create a custom environment the way we did with simplified Tic-tac-toe in our previous blog. We use the environment available in OpenAI gym through TF-Agents. One versatile feature of TF-Agents is that it supports off-the-shelf environments of libraries like OpenAI Gym, Unity\u2019s ML-Agents, etc.", "Let\u2019s load the CartPole environment using suite_gym module.", "So, how do we represent the state in the CartPole environment? The CartPole is in some position (linear and angular) and it has some velocity (also linear and angular). So, the four variables \u2014 linear position, angular position, linear velocity, angular velocity \u2014 represent the state of the environment.", "Let\u2019s get the state of the CartPole after resetting the environment \u2014 initial state.", "It returns TimeStep object which has environment state along with some other information. We will get to that later but for now, we can see how it gives the current state.", "Action is the activity agent performs. Here, the agent can either make the cart go right or left. So, the actions are represented by 0 and 1. 0 means left and 1 means right.", "Vaguely speaking, reward is the advantage or encouragement the agent gets for performing good action. Just as how a student gets pat on his/her back on getting good grades, we should give a pat to our CartPole agent if it performs good action. Here, the CartPole not toppling down or going out of range should be rewarded and encouraged. So, for each instance of CartPole not toppling down or going out of range, we have a reward of +1.0.", "Let\u2019s now perform some actions on our environment and see how state changes, rewards are awarded, and episode ends. More importantly, let\u2019s understand what TimeStep is and what do the components of TimeStep mean.", "When we ran reset method of env we got TimeStep object. Let\u2019s understand what this object is and what it contains.", "TimeStep is the object which contains information about a state like current observation, type of the step, reward, and discount. Given that some action is performed on some state, it gives the new state, type of the new step (or state), discount, and reward achieved. In our CartPole environment, there are three types of steps \u2014 the start of the episode, the intermediate step in the episode, and the last step of the episode. When an episode starts, the step\u2019s type is 0 and it changes to 1 until the end of the episode. At the last step of the episode, the step\u2019s type changes to 2. This helps in knowing if the step is the last one. Discount is the parameter which gives ratio of importance given to the later reward than to the former reward. The reward gives reward achieved in reaching the current step.", "Initially, we run the reset method and get the TimeStep which gives the current observation. We see that step_type is 0 and it is obvious no reward is received.", "Let\u2019s apply action 1 (move right) to it continuously and see when the episode ends.", "The results are shown in the below pandas dataframe df.", "We see here that after 10 steps, the episode ends. As one can guess well, the policy of always moving right is not good.", "Let\u2019s see by randomly moving left or right. If an agent takes random action from the list of possible actions, the policy is called random policy.", "The average number of steps came around double than that of right-only policy we tried but nah, this is not good.", "Training architecture of TF-Agents training program usually has two parts: collection part and training part. The collection part uses driver to collect trajectories (experiences) and training part used agent to train some network. The network once trained updates collect policy. We will come to the collect policy while explaining the training architecture.", "We already discussed about environment, TimeStep, and action. Let\u2019s bite each component one by one relating to our CartPole problem.", "We create two different environments: train environment for training and evaluation environment for evaluation. This is normally the case because training might require multiple environments and more of the resources than the evaluation environment.", "Let\u2019s now create Deep Q-Network agent. The agent takes the batches of trajectories from the replay buffer and trains the Q-Network. We create a Q-Network (q_net) and pass it to the agent. Along with the network, we pass TimeStep and action specifications of the train environment, optimizer, loss, and train step counter.", "There are two types of policies in a deep reinforcement learning setting. Policy is the policy used to evaluate and deploy our model whereas collect policy is the policy used to create the trajectories/experiences. Based on the collect policy, the driver performs the action and generates the trajectories. Trajectories are simply experiences. They consist of current step, reward, and current next step. The batches of them are used to train the network.", "For populating the replay buffer with some trajectories so as to help for evaluation of the agent\u2019s policy once before training, we make use of the random policy and collect data beforehand.", "In addition, let\u2019s create utility functions to compute the average return and collect data in the replay buffer. Before that, let\u2019s run through the replay buffer. The replay buffer is the buffer where trajectories are stored. The batches of trajectories are pulled from it to train the Q-Network. When the batch is created in a random fashion from the replay buffer, the training trajectories are identical and independent so helping in the training process using the gradient descent algorithm.", "Now, the data from the replay buffer is converted to iterable tf.data.Dataset pipeline and it will be fed to the agent.", "Here, num_steps is set to 2 because we need two observations to compute the loss. Setting num_steps to 2 samples two adjacent rows for each item in the batch. Parallel calls and prefetching optimizes the dataset.", "As we have all the components built, let\u2019s now train our agent. Before training, let\u2019s define our hyperparameters.", "num_iterations is the number of iterations of training we are going to do. collect_steps_per_iteration is the number of collect steps we are going to do per an iteration of training. We set it to one so there is only a single step of the collection at the onset of training. replay_buffer_max_length is the maximum length of the replay buffer. It is kept on the basis of memory available. batch_size is the size of a batch of trajectories which is fed to the agent. learning_rate is the rate of learning for Adam optimizer. log_interval is the interval for logging the training loss and eval_interval is the interval for evaluating the algorithm using average return. num_eval_episodes is the number of episodes we take for calculating the average return.", "Hopefully, the average return kept increasing until saturating around 200 (which is the maximum number of steps allowed per episode).", "Congratulations! You trained your first reinforcement learning algorithm using TF-Agents. Grab a glass of your favorite drink. You deserve it!", "Let\u2019s visualize the training process now. We see the trend of average reward instead of the loss because the average reward is the target/goal for us, not the loss. Better learning of the policy and the loss might not always go hand in hand. When the agent is exploring new space, it might still be learning but the loss could be increasing. Similarly, when the agent is stuck in some space for a long time, the loss might be decreasing but the policy is not generalized one \u2014 i.e. overfitted.", "We see here that the average reward saturates around 200 after 5000 episodes. Woohoo!", "Now, let\u2019s see our agent in action using the optimum policy we just learned.", "As the training and evaluation were done in Jupyter Lab, code snippet for displaying evaluation video is given below.", "Now, the video of our agent performing in the CartPole environment is attached below.", "As we kept the maximum steps to 200, the agent does not care about balancing it in the way that it can stand for more than 200 steps. So, we see it tending to fall while nearing 200 steps. We can take care of that by either increasing the limit of the maximum steps or giving variable reward on the basis of linear and angular displacement.", "Hopefully, this article gave readers some ideas about reinforcement learning and its implementation using TF-Agents. TF-Agents made a lot of things easier by making the components of training architecture modular, flexible, and readily available. We will explore more reinforcement learning environments and problems in the coming days. Stay tuned!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "CS Graduate Student at UAH, Former Data Scientist at World Bank \u2014 the views and the content here represent my own and not of my employers."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F3e6006adeba7&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcartpole-problem-using-tf-agents-build-your-first-reinforcement-learning-application-3e6006adeba7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcartpole-problem-using-tf-agents-build-your-first-reinforcement-learning-application-3e6006adeba7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcartpole-problem-using-tf-agents-build-your-first-reinforcement-learning-application-3e6006adeba7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcartpole-problem-using-tf-agents-build-your-first-reinforcement-learning-application-3e6006adeba7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----3e6006adeba7--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3e6006adeba7--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://surajregmi.medium.com/?source=post_page-----3e6006adeba7--------------------------------", "anchor_text": ""}, {"url": "https://surajregmi.medium.com/?source=post_page-----3e6006adeba7--------------------------------", "anchor_text": "Suraj Regmi"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffd455fd5a9ab&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcartpole-problem-using-tf-agents-build-your-first-reinforcement-learning-application-3e6006adeba7&user=Suraj+Regmi&userId=fd455fd5a9ab&source=post_page-fd455fd5a9ab----3e6006adeba7---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3e6006adeba7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcartpole-problem-using-tf-agents-build-your-first-reinforcement-learning-application-3e6006adeba7&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3e6006adeba7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcartpole-problem-using-tf-agents-build-your-first-reinforcement-learning-application-3e6006adeba7&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/creating-a-custom-environment-for-tensorflow-agent-tic-tac-toe-example-b66902f73059", "anchor_text": "this"}, {"url": "https://towardsdatascience.com/creating-a-custom-environment-for-tensorflow-agent-tic-tac-toe-example-b66902f73059", "anchor_text": "blog"}, {"url": "https://towardsdatascience.com/creating-a-custom-environment-for-tensorflow-agent-tic-tac-toe-example-b66902f73059", "anchor_text": "blog"}, {"url": "https://unsplash.com/@whitney_wright?utm_source=medium&utm_medium=referral", "anchor_text": "Whitney Wright"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----3e6006adeba7---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----3e6006adeba7---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----3e6006adeba7---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/tensorflow?source=post_page-----3e6006adeba7---------------tensorflow-----------------", "anchor_text": "TensorFlow"}, {"url": "https://medium.com/tag/data-science?source=post_page-----3e6006adeba7---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3e6006adeba7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcartpole-problem-using-tf-agents-build-your-first-reinforcement-learning-application-3e6006adeba7&user=Suraj+Regmi&userId=fd455fd5a9ab&source=-----3e6006adeba7---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3e6006adeba7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcartpole-problem-using-tf-agents-build-your-first-reinforcement-learning-application-3e6006adeba7&user=Suraj+Regmi&userId=fd455fd5a9ab&source=-----3e6006adeba7---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3e6006adeba7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcartpole-problem-using-tf-agents-build-your-first-reinforcement-learning-application-3e6006adeba7&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3e6006adeba7--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F3e6006adeba7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcartpole-problem-using-tf-agents-build-your-first-reinforcement-learning-application-3e6006adeba7&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----3e6006adeba7---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----3e6006adeba7--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----3e6006adeba7--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----3e6006adeba7--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----3e6006adeba7--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----3e6006adeba7--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----3e6006adeba7--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----3e6006adeba7--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----3e6006adeba7--------------------------------", "anchor_text": ""}, {"url": "https://surajregmi.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://surajregmi.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Suraj Regmi"}, {"url": "https://surajregmi.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "135 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffd455fd5a9ab&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcartpole-problem-using-tf-agents-build-your-first-reinforcement-learning-application-3e6006adeba7&user=Suraj+Regmi&userId=fd455fd5a9ab&source=post_page-fd455fd5a9ab--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Feacad15e45f2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcartpole-problem-using-tf-agents-build-your-first-reinforcement-learning-application-3e6006adeba7&newsletterV3=fd455fd5a9ab&newsletterV3Id=eacad15e45f2&user=Suraj+Regmi&userId=fd455fd5a9ab&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}