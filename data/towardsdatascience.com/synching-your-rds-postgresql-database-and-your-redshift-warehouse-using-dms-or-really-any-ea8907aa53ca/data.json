{"url": "https://towardsdatascience.com/synching-your-rds-postgresql-database-and-your-redshift-warehouse-using-dms-or-really-any-ea8907aa53ca", "time": 1682995459.496202, "path": "towardsdatascience.com/synching-your-rds-postgresql-database-and-your-redshift-warehouse-using-dms-or-really-any-ea8907aa53ca/", "webpage": {"metadata": {"title": "Sync AWS RDS Postgres to Redshift using AWS DMS | by Axel Furlan | Towards Data Science", "h1": "Sync AWS RDS Postgres to Redshift using AWS DMS", "description": "When we first started to get to know AWS Redshift, we fell in love for the fast aggregated query processing. This strong advantage meant sky-rocketing our productivity and speed when performing\u2026"}, "outgoing_paragraph_urls": [{"url": "https://aws.amazon.com/es/blogs/big-data/performance-matters-amazon-redshift-is-now-up-to-3-5x-faster-for-real-world-workloads/", "anchor_text": "fast aggregated query processing", "paragraph_index": 1}, {"url": "https://www.stitchdata.com", "anchor_text": "Stitch", "paragraph_index": 1}, {"url": "https://www.stitchdata.com/pricing/", "anchor_text": "subscription plan", "paragraph_index": 2}, {"url": "https://increasecard.com/", "anchor_text": "IncreaseCard", "paragraph_index": 3}, {"url": "https://en.wikipedia.org/wiki/Historical_exchange_rates_of_Argentine_currency", "anchor_text": "lost a lot of its value against US dollars recently", "paragraph_index": 3}, {"url": "https://increasecard.com/conciliacion/", "anchor_text": "IncreaseConciliaci\u00f3n", "paragraph_index": 4}, {"url": "http://cassandra.apache.org/", "anchor_text": "Apache Cassandra database", "paragraph_index": 4}, {"url": "https://github.com/eulerto/wal2json", "anchor_text": "wal2json", "paragraph_index": 8}, {"url": "https://aws.amazon.com/cloudwatch/pricing/", "anchor_text": "billing", "paragraph_index": 26}, {"url": "https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.S3.html", "anchor_text": "here", "paragraph_index": 33}, {"url": "https://serverless.com/", "anchor_text": "Serverless", "paragraph_index": 40}, {"url": "https://realpython.com/python-csv/#parsing-csv-files-with-pythons-built-in-csv-library", "anchor_text": "link", "paragraph_index": 47}, {"url": "https://billyfung.com/blog/2017/06/psycopg2-multiple-insert/", "anchor_text": "here", "paragraph_index": 48}, {"url": "https://aws.amazon.com/es/athena/", "anchor_text": "AWS Athena and Glue", "paragraph_index": 50}, {"url": "https://www.linkedin.com/in/axelfurlan/", "anchor_text": "https://www.linkedin.com/in/axelfurlan/", "paragraph_index": 53}], "all_paragraphs": ["Disclaimer: this post assumes some understanding of programming.", "When we first started to get to know AWS Redshift, we fell in love for the fast aggregated query processing. This strong advantage meant sky-rocketing our productivity and speed when performing statistical studies or simply data-extractions. So, of course, we turned it into our main data warehouse for all the data sources we managed (Adwords, Salesforce, FBAds, Zendesk, etc.) \u2014 all of this, using Stitch as our main ETL tool.", "Stitch holds a nice subscription plan of $100, offering process capacity for 5M rows and $20 per additional million rows. Stitch logs and billing invoices tell us we barely reached $180 on a very busy month using all the data sources mentioned above. Most of those months were just plain $100 (no additional million rows used).", "Our company develops, maintains and sells its core SaaS product, IncreaseCard. Hosted in the AWS Cloud, our production database resides in AWS RDS, storing between 10 and 20 million new rows every month, and considerably growing.So typically, using Stitch to sync our production database to a Redshift warehouse, would turn out to be painfully expensive for a third-world startup company. Also considering our country\u2019s currency lost a lot of its value against US dollars recently.", "We also had another restriction. The company\u2019s developing its second major product, IncreaseConciliaci\u00f3n. This one uses the NoSQL Apache Cassandra database to store and process its huge data. The problem is, both products must be synced in order for Conciliaci\u00f3n to use transactions extracted by Card. In other words, we had to build a data-lake accessible for consumption by any service to perform syncing operations on-demand.", "Many things to take into account, right?", "Being a small team of 2 people, the mighty \u201cData Team\u201d, we get it easy to try and test new things, especially architectures.", "We started by using AWS Data Pipeline, a UI based service to build ETLs between a bunch of data sources. Although the process of building an ETL was rather easy, there were a bunch of workarounds that we had to take in order for it to be effective \u2014 remember that we have to update every change whether it be an insertion, a deletion or an update. Since you can\u2019t use code here, it became unmaintainable quickly. Furthermore, there isn\u2019t much detailed documentation or clear examples for this service IMO.", "We tried to set a Lambda to consume every 15 minutes the Wal log of a replication slot of our Postgres database, and send it to a Kinesis Firehose data stream. It seemed to be all safe and sound until a production process updated more rows than usually expected. We found out that, in these cases, the way the records came from the logical decoding were huge rows full of chunks of changes of the tables involved, causing the function to die of lack of memory every time it tried to load it. We solved this by setting true to the property write_in_chunks of the logical decoding plugin, we used (wal2json), enabling us to partition the incoming JSON log. Long story short, the function could still be terminated unsuccessfully due to not enough time to process the huge transaction. No bueno.", "Ok so, let\u2019s head to it.", "Our current architecture consists of the following:", "For this post to be more \u201cdemocratic\u201d, I\u2019ll divide it into 2 sections. The first one will be the steps to replicate changes directly to Redshift. The second one, building the S3 data-lake for other services to use. Feel free to read one or the other, or even better, both \ud83d\ude04.", "Since I don\u2019t consider myself smarter than people that write AWS documentation, I\u2019ll copy-paste some of their instructions below \ud83e\udd37\u200d\u2642.", "Find your RDS instance, look up the parameter group that this instance applies. Either duplicate or modify the parameter group directly with the following.", "1.- Set the rds.logical_replication static parameter to 1. As part of applying this parameter, we also set the parameters wal_level, max_wal_senders, max_replication_slots, andmax_connections. These parameter changes can increase WAL generation, so you should only set therds.logical_replication parameter when you are using logical slots.", "2.- Reboot the DB instance for the static rds.logical_replication parameter to take effect.", "After rebooting, we should be good to go. A good way to test if everything\u2019s gucci is by running the following line on your database console.", "First, we must create the resources that the task is going to use. Create the endpoints (source and target) and the replication instance (basically an EC2 instance that\u2019s in charge of all the dirty work).", "The process of creating a replication instance is super straightforward, so I won\u2019t go over it, just make sure to use a VPC that has access to the source and target you intend to use.", "For the source endpoint, tick the option that says something like \u201cSelect a RDS instance\u201d and effectively, select your source database and fill the fields for the credentials.", "For the target endpoint, select Redshift and fill in all the textboxes with Host, Port and credentials.", "Great, now we have everything we need, let\u2019s create the task that\u2019s going to migrate and replicate the stuff.", "In DMS, press the Create Task button, put it a fancy name and start selecting all the resources we created earlier. For Migration type choose Migrate existing data and replicate ongoing changes, this performs a full migration first and then starts replicating on-going CRUD operations on the target database.", "On the selection section, select whatever schema/table you want to use, in my case I\u2019ll just replicate every change within a schema \u2014 also, check if the tables have primary keys, DMS may mess up if they don\u2019t have a PK. Use % character as a wildcard (i.e \u201call tables containing the word \u2018foo\u2019 \u201d would be table = %foo%.", "The task can also have transformation rules, it enables you to, for example, change a schema or table name on the target destination.", "Ready to unleash it? Hit that create task button and wait for it to start \u2014 or start it if you chose not to start when created. The task now is going to fully load the tables into your destination and then start replicating changes. You should see Load complete, replication ongoing as its status when finished migrating.", "Voila, done!. Your Redshift warehouse is now enriched by your production RDS data, good job!. At this point, if anything went wrong you can always enable the logging feature of the DMS task to see what happened \ud83d\ude04, just take into account Cloudwatch billing.", "You can also check some useful charts that the DMS task presents to us.", "Next section is going to be about the creation of the S3 data-lake for other services to consume database changes. If you\u2019re leaving now, thanks for reading and I hope this post was useful to you.", "Same as before, create a task that instead of Redshift as a target destination, it\u2019s an S3 bucket.", "For the target endpoint, previously create an S3 bucket, and simply put the name of the bucket and the corresponding role to access it. Take into account the service role should have the following policy.", "On the additional parameters section, put the following addColumnName=true;, including the previous parameters or others that you may want to use.", "In this case, choose to Replicate data changes only as migration type, we don\u2019t want to infest our S3 bucket of previous data.", "For more info about using S3 as a target for DMS, click here.", "Ok, the moment of truth, run your little Frankenstein baby.", "The state of the task should be Ongoing replication. If otherwise, check the logs related to the replication instance for errors in the Cloudwatch service.", "From now on, if you\u2019re using a staging database instance (which I hope you are), create, update and delete some rows in order for changes to be replicated by the task. This will hopefully conclude in a csv file in the S3 bucket you specified in your destination endpoint.", "This file should have something like:", "If you\u2019re not, what here we call as salame, you\u2019d realize I stands for Insert, D for delete and U for Update.", "The moment you\u2019ve been waiting for (or not). This is where we get to code the function that\u2019s going to parse the CSV document that\u2019s being created on our bucket.", "For this part, I suggest you use Serverless, a nice tool to easily deploy Lambda functions using your AWS CLI credentials. It\u2019s as simple as writing a .yml file, hitting serverless deploy and voila.", "Now, let\u2019s create our Lambda function and later add the S3 bucket event to trigger the function on every object creation. Also, you may want to add a prefix if you\u2019ve specified that the files are going to be created within a specific folder, just put folder/ on the prefix textbox, and you\u2019re good to go.", "Let\u2019s code part of the function that\u2019s going to receive and extract the data from the file uploaded in the bucket.", "So, what we first want to do is build our handler function, you know, the usual main() but Lambdably speaking.", "This main function is going to receive as a parameter the JSON event that Cloudwatch handles to it, basically saying \u201cYo, there\u2019s a new file created in the bucket, here\u2019s its key to access it.\u201d", "Here\u2019s the sample event that AWS gives us.", "And below I paste the Python code that I use to get the final content of the file created in S3.", "Now you have the data of the CSV in the file_content variable. If you have experience with parsers, this should be a piece of cake, if that\u2019s not the case for you, I recommend you check this link.", "From now on, it\u2019s on your hands. Use a python driver to handle the CRUD operations, processing every row of the CSV impacting the changes on Redshift. For this case, I recommend using functions like execute_values() (psycopg2) to minimize execution time as explained here. Use Lambda environment variables to handle credential secrets for the function to use, remember it\u2019s not a good idea to hardcode them.", "What I present to you is just one of the thousand possibilities to achieve the goal of synchronizing databases. If you don\u2019t mind spending some hundreds \ud83d\udcb8 on services that handle the ETL process for you, go for it. No really, just go for it.", "This architecture serves us with 2 main positive side effects IMO. First one, having tracking information about the changes in your production database, this can never be needless. Nowadays with services like AWS Athena and Glue you can query that data directly via the console. The second one is the ability to connect/trigger any procedure via S3 bucket object creation event \u2014 in our case, the IncreaseConciliaci\u00f3n team replicating changes in their own Cassandra database.", "We\u2019ve come to an end, hopefully, you\u2019ve enjoyed my first article \ud83d\ude04", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Working as a Data Engineer. Data science amateur. Constantly wanting to travel far. https://www.linkedin.com/in/axelfurlan/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fea8907aa53ca&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsynching-your-rds-postgresql-database-and-your-redshift-warehouse-using-dms-or-really-any-ea8907aa53ca&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsynching-your-rds-postgresql-database-and-your-redshift-warehouse-using-dms-or-really-any-ea8907aa53ca&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsynching-your-rds-postgresql-database-and-your-redshift-warehouse-using-dms-or-really-any-ea8907aa53ca&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsynching-your-rds-postgresql-database-and-your-redshift-warehouse-using-dms-or-really-any-ea8907aa53ca&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----ea8907aa53ca--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ea8907aa53ca--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@axelfurlan_17621?source=post_page-----ea8907aa53ca--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@axelfurlan_17621?source=post_page-----ea8907aa53ca--------------------------------", "anchor_text": "Axel Furlan"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4b2ad5f659bd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsynching-your-rds-postgresql-database-and-your-redshift-warehouse-using-dms-or-really-any-ea8907aa53ca&user=Axel+Furlan&userId=4b2ad5f659bd&source=post_page-4b2ad5f659bd----ea8907aa53ca---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fea8907aa53ca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsynching-your-rds-postgresql-database-and-your-redshift-warehouse-using-dms-or-really-any-ea8907aa53ca&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fea8907aa53ca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsynching-your-rds-postgresql-database-and-your-redshift-warehouse-using-dms-or-really-any-ea8907aa53ca&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@hishahadat?utm_source=medium&utm_medium=referral", "anchor_text": "Shahadat Rahman"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://aws.amazon.com/es/blogs/big-data/performance-matters-amazon-redshift-is-now-up-to-3-5x-faster-for-real-world-workloads/", "anchor_text": "fast aggregated query processing"}, {"url": "https://www.stitchdata.com", "anchor_text": "Stitch"}, {"url": "https://www.stitchdata.com/pricing/", "anchor_text": "subscription plan"}, {"url": "https://increasecard.com/", "anchor_text": "IncreaseCard"}, {"url": "https://en.wikipedia.org/wiki/Historical_exchange_rates_of_Argentine_currency", "anchor_text": "lost a lot of its value against US dollars recently"}, {"url": "https://unsplash.com/@sandercrombach?utm_source=medium&utm_medium=referral", "anchor_text": "Sander Crombach"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://increasecard.com/conciliacion/", "anchor_text": "IncreaseConciliaci\u00f3n"}, {"url": "http://cassandra.apache.org/", "anchor_text": "Apache Cassandra database"}, {"url": "https://github.com/eulerto/wal2json", "anchor_text": "wal2json"}, {"url": "http://lucidchart.com/", "anchor_text": "Lucidchart"}, {"url": "https://docs.aws.amazon.com/AmazonS3/latest/dev/ways-to-add-notification-config-to-bucket.html?shortFooter=true", "anchor_text": "link"}, {"url": "https://pixabay.com/es/users/pasja1000-6355831/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=4369865", "anchor_text": "pasja1000"}, {"url": "https://pixabay.com/es/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=4369865", "anchor_text": "Pixabay"}, {"url": "https://aws.amazon.com/cloudwatch/pricing/", "anchor_text": "billing"}, {"url": "https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.S3.html", "anchor_text": "here"}, {"url": "https://pixabay.com/es/users/maraisea-2989330/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=1648250", "anchor_text": "Etienne Marais"}, {"url": "https://pixabay.com/es/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=1648250", "anchor_text": "Pixabay"}, {"url": "https://serverless.com/", "anchor_text": "Serverless"}, {"url": "https://realpython.com/python-csv/#parsing-csv-files-with-pythons-built-in-csv-library", "anchor_text": "link"}, {"url": "https://billyfung.com/blog/2017/06/psycopg2-multiple-insert/", "anchor_text": "here"}, {"url": "https://aws.amazon.com/es/athena/", "anchor_text": "AWS Athena and Glue"}, {"url": "https://medium.com/tag/aws?source=post_page-----ea8907aa53ca---------------aws-----------------", "anchor_text": "AWS"}, {"url": "https://medium.com/tag/redshift?source=post_page-----ea8907aa53ca---------------redshift-----------------", "anchor_text": "Redshift"}, {"url": "https://medium.com/tag/postgres?source=post_page-----ea8907aa53ca---------------postgres-----------------", "anchor_text": "Postgres"}, {"url": "https://medium.com/tag/data-engineering?source=post_page-----ea8907aa53ca---------------data_engineering-----------------", "anchor_text": "Data Engineering"}, {"url": "https://medium.com/tag/data?source=post_page-----ea8907aa53ca---------------data-----------------", "anchor_text": "Data"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fea8907aa53ca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsynching-your-rds-postgresql-database-and-your-redshift-warehouse-using-dms-or-really-any-ea8907aa53ca&user=Axel+Furlan&userId=4b2ad5f659bd&source=-----ea8907aa53ca---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fea8907aa53ca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsynching-your-rds-postgresql-database-and-your-redshift-warehouse-using-dms-or-really-any-ea8907aa53ca&user=Axel+Furlan&userId=4b2ad5f659bd&source=-----ea8907aa53ca---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fea8907aa53ca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsynching-your-rds-postgresql-database-and-your-redshift-warehouse-using-dms-or-really-any-ea8907aa53ca&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ea8907aa53ca--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fea8907aa53ca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsynching-your-rds-postgresql-database-and-your-redshift-warehouse-using-dms-or-really-any-ea8907aa53ca&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----ea8907aa53ca---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----ea8907aa53ca--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----ea8907aa53ca--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----ea8907aa53ca--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----ea8907aa53ca--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----ea8907aa53ca--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----ea8907aa53ca--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----ea8907aa53ca--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----ea8907aa53ca--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@axelfurlan_17621?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@axelfurlan_17621?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Axel Furlan"}, {"url": "https://medium.com/@axelfurlan_17621/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "226 Followers"}, {"url": "https://www.linkedin.com/in/axelfurlan/", "anchor_text": "https://www.linkedin.com/in/axelfurlan/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4b2ad5f659bd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsynching-your-rds-postgresql-database-and-your-redshift-warehouse-using-dms-or-really-any-ea8907aa53ca&user=Axel+Furlan&userId=4b2ad5f659bd&source=post_page-4b2ad5f659bd--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fce2814aca92&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsynching-your-rds-postgresql-database-and-your-redshift-warehouse-using-dms-or-really-any-ea8907aa53ca&newsletterV3=4b2ad5f659bd&newsletterV3Id=ce2814aca92&user=Axel+Furlan&userId=4b2ad5f659bd&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}