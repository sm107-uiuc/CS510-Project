{"url": "https://towardsdatascience.com/the-guide-to-multi-tasking-with-the-t5-transformer-90c70a08837b", "time": 1683009562.0900328, "path": "towardsdatascience.com/the-guide-to-multi-tasking-with-the-t5-transformer-90c70a08837b/", "webpage": {"metadata": {"title": "The Guide to Multi-Tasking with the T5 Transformer | by Thilina Rajapakse | Towards Data Science", "h1": "The Guide to Multi-Tasking with the T5 Transformer", "description": "The T5 (Text-To-Text Transfer Transformer) model was the product of a large-scale study (paper) conducted to explore the limits of transfer learning. It builds upon popular architectures like GPT\u2026"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/abs/1910.10683", "anchor_text": "paper", "paragraph_index": 0}, {"url": "https://github.com/ThilinaRajapakse/simpletransformers/tree/master/examples/t5/mixed_tasks", "anchor_text": "Github", "paragraph_index": 4}, {"url": "https://github.com/ThilinaRajapakse/simpletransformers", "anchor_text": "Simple Transformers", "paragraph_index": 18}, {"url": "https://github.com/huggingface/transformers", "anchor_text": "Transformers", "paragraph_index": 18}, {"url": "https://app.wandb.ai/thilina/T5%20mixed%20tasks%20-%20Binary,%20Multi-Label,%20Regression?workspace=user-thilina", "anchor_text": "here", "paragraph_index": 22}, {"url": "https://www.wandb.com/", "anchor_text": "W&B", "paragraph_index": 22}, {"url": "https://towardsdatascience.com/asking-the-right-questions-training-a-t5-transformer-model-on-a-new-task-691ebba2d72c", "anchor_text": "article", "paragraph_index": 25}, {"url": "http://www.linkedin.com/in/t-rajapakse/", "anchor_text": "www.linkedin.com/in/t-rajapakse/", "paragraph_index": 36}], "all_paragraphs": ["The T5 (Text-To-Text Transfer Transformer) model was the product of a large-scale study (paper) conducted to explore the limits of transfer learning. It builds upon popular architectures like GPT, BERT, and RoBERTa(to name only a few) models that utilized Transfer Learning with incredible success. While BERT-like models can be fine-tuned to perform a variety of tasks, the constraints of the architecture mean that each model can perform only one task.", "Typically, this is done by adding a task-specific layer on top of the Transformer model. For example, a BERT Transformer can be adapted for binary classification by adding a fully-connected layer with two output neurons (corresponding to each class). The T5 model departs from this tradition by reframing all NLP tasks as text-to-text tasks. This results in a shared framework for any NLP task as the input to the model and the output from the model is always a string. In the example of binary classification, the T5 model will simply output a string representation for the class (i.e. \"0\" or \"1\").", "Since the input and output formats are identical for any NLP task, the same T5 model can be taught to perform multiple tasks! To specify which task should be performed, we can simply prepend a prefix (string) to the input of the model. The animation (shown below) from the Google AI Blog article demonstrates this concept.", "In this article, we\u2019ll be using this technique to train a single T5 model capable of performing the 3 NLP tasks, binary classification, multi-label classification, and regression.", "All code can also be found on Github.", "The goal of binary classification in NLP is to classify a given text sequence into one of two classes. In our task, we will be using the Yelp Reviews dataset to classify the sentiment of the text as either positive ( \"1\" ) or negative ( \"0\" ).", "In multi-label classification, a given text sequence should be labeled with the correct subset of a set of pre-defined labels (note that the subset can include both the null set and the full set of labels itself). For this, we will be using the Toxic Comments dataset where each text can be labeled with any subset of the labels toxic, severe_toxic, obscene, threat, insult, identity_hate.", "In regression tasks, the target variable is a continuous value. In our task, we will use the STS-B (Semantic Textual Similarity Benchmark) dataset where the goal is to predict the similarity of two sentences. The similarity is denoted by a continuous value between 0 and 5.", "Since we are going to be working with 3 datasets, we\u2019ll put them in 3 separate subdirectories inside the data directory.", "As mentioned earlier, the inputs and outputs of a T5 model is always text. A particular task is specified by using a prefix text that lets the model know what it should do with the input.", "The input data format for a T5 model in Simple Transformers reflects this fact. The input is a Pandas dataframe with the 3 columns \u2014 prefix, input_text, andtarget_text. This makes it quite easy to train the model on multiple tasks as you just need to change the prefix.", "The notebook above loads each of the datasets, preprocesses them for T5, and finally combines them into a unified dataframe.", "This gives us a dataframe with 3 unique prefixes, namely binary classification, multilabel classification, and similarity. Note that the prefixes themselves are fairly arbitrary, the important thing is to ensure that each task has its own unique prefix. The input to the model will take the following format:", "The \": \" is automatically added when training.", "A few other things to note:", "As you can see from the way the different inputs and outputs are represented, the T5 model\u2019s text-to-text approach gives us a great deal of flexibility both in terms of representing various tasks and in terms of the actual tasks we can perform.", "The only limitation is imagination! (Well, imagination and compute resources but that\u2019s another story) \ud83d\ude05", "Getting back to the data, running the notebook should have given you a train.tsv and an eval.tsv file which we\u2019ll be using to train our model in the next section!", "We will be using the Simple Transformers library (based on the Hugging Face Transformers) to train the T5 model.", "The instructions given below will install all the requirements.", "As always, training the model with Simple Transformers is quite straightforward.", "Most of the arguments used here are fairly standard.", "Speaking of visualization, you can check my training progress here. Shoutout to W&B for their awesome library!", "Considering the fact that we are dealing with multiple tasks, it\u2019s a good idea to use suitable metrics to evaluate each task. With that in mind, we\u2019ll be using the following metrics;", "Note that a \": \u201c is inserted between the prefix and the input_text when preparing the data. This is done automatically when training but needs to be handled manually for prediction.", "If you\u2019d like to read more about the decoding arguments (num_beams, do_sample, max_length, top_k, top_p), please refer to this article.", "Time to see how our model did!", "The model performs quite well on each task, despite being trained on 3 separate tasks! We\u2019ll take a quick look at how we can try to improve the performance of the model even more in the next section.", "A potential issue that arises when mixing tasks is the discrepancy between the sizes of the datasets used for each task. We can see this issue in our dataset by taking a look at the training sample counts.", "The dataset is substantially unbalanced with the plight of the similarity task seeming particularly dire! This can be clearly seen in the evaluation scores where the similarity task lags behind the others (although it\u2019s important to note that we are not looking at the same metrics between the tasks).", "A possible remedy to this problem would be to oversample the similarity tasks so that the model.", "In addition to this, increasing the number of training epochs (and tuning other hyperparameters) is also likely to improve the model.", "Finally, tuning the decoding parameters could also lead to better results.", "The text-to-text format of the T5 model paves the way to apply Transformers and NLP to a wide variety of tasks with next to no customization necessary. The T5 model performs strongly even when the same model is used to perform multiple tasks!", "Hopefully, this will lead to many innovative applications in the near future.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "AI researcher, avid reader, fantasy and Sci-Fi geek, and fan of the Oxford comma. www.linkedin.com/in/t-rajapakse/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F90c70a08837b&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-guide-to-multi-tasking-with-the-t5-transformer-90c70a08837b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-guide-to-multi-tasking-with-the-t5-transformer-90c70a08837b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-guide-to-multi-tasking-with-the-t5-transformer-90c70a08837b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-guide-to-multi-tasking-with-the-t5-transformer-90c70a08837b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----90c70a08837b--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----90c70a08837b--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://chaturangarajapakshe.medium.com/?source=post_page-----90c70a08837b--------------------------------", "anchor_text": ""}, {"url": "https://chaturangarajapakshe.medium.com/?source=post_page-----90c70a08837b--------------------------------", "anchor_text": "Thilina Rajapakse"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6b1e2355088e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-guide-to-multi-tasking-with-the-t5-transformer-90c70a08837b&user=Thilina+Rajapakse&userId=6b1e2355088e&source=post_page-6b1e2355088e----90c70a08837b---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F90c70a08837b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-guide-to-multi-tasking-with-the-t5-transformer-90c70a08837b&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F90c70a08837b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-guide-to-multi-tasking-with-the-t5-transformer-90c70a08837b&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@mbeero?utm_source=medium&utm_medium=referral", "anchor_text": "Matt Bero"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://arxiv.org/abs/1910.10683", "anchor_text": "paper"}, {"url": "http://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html", "anchor_text": "Exploring Transfer Learning with T5: the Text-To-Text Transfer Transformer"}, {"url": "https://github.com/ThilinaRajapakse/simpletransformers/tree/master/examples/t5/mixed_tasks", "anchor_text": "Github"}, {"url": "https://s3.amazonaws.com/fast-ai-nlp/yelp_review_polarity_csv.tgz", "anchor_text": "Yelp Reviews Dataset"}, {"url": "https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/", "anchor_text": "Toxic Comments dataset"}, {"url": "http://ixa2.si.ehu.es/stswiki/index.php/STSbenchmark", "anchor_text": "STS-B dataset"}, {"url": "https://towardsdatascience.com/asking-the-right-questions-training-a-t5-transformer-model-on-a-new-task-691ebba2d72c", "anchor_text": "Asking the Right Questions: Training a T5 Transformer Model on a New taskThe T5 Transformer frames any NLP task as a text-to-text task enabling it to easily learn new tasks. Let\u2019s teach the\u2026towardsdatascience.com"}, {"url": "https://github.com/ThilinaRajapakse/simpletransformers", "anchor_text": "Simple Transformers"}, {"url": "https://github.com/huggingface/transformers", "anchor_text": "Transformers"}, {"url": "https://www.anaconda.com/distribution/", "anchor_text": "here"}, {"url": "https://simpletransformers.ai/docs/installation/#installation-steps", "anchor_text": "docs"}, {"url": "https://app.wandb.ai/thilina/T5%20mixed%20tasks%20-%20Binary,%20Multi-Label,%20Regression?workspace=user-thilina", "anchor_text": "here"}, {"url": "https://www.wandb.com/", "anchor_text": "W&B"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html", "anchor_text": "F1 score"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html", "anchor_text": "Accuracy score"}, {"url": "https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pearsonr.html", "anchor_text": "Pearson correlation coefficient"}, {"url": "https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.spearmanr.html", "anchor_text": "Spearman correlation"}, {"url": "https://towardsdatascience.com/asking-the-right-questions-training-a-t5-transformer-model-on-a-new-task-691ebba2d72c", "anchor_text": "article"}, {"url": "https://arxiv.org/abs/1910.10683", "anchor_text": "https://arxiv.org/abs/1910.10683"}, {"url": "https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html", "anchor_text": "https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----90c70a08837b---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/data-science?source=post_page-----90c70a08837b---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/nlp?source=post_page-----90c70a08837b---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----90c70a08837b---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F90c70a08837b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-guide-to-multi-tasking-with-the-t5-transformer-90c70a08837b&user=Thilina+Rajapakse&userId=6b1e2355088e&source=-----90c70a08837b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F90c70a08837b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-guide-to-multi-tasking-with-the-t5-transformer-90c70a08837b&user=Thilina+Rajapakse&userId=6b1e2355088e&source=-----90c70a08837b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F90c70a08837b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-guide-to-multi-tasking-with-the-t5-transformer-90c70a08837b&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----90c70a08837b--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F90c70a08837b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-guide-to-multi-tasking-with-the-t5-transformer-90c70a08837b&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----90c70a08837b---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----90c70a08837b--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----90c70a08837b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----90c70a08837b--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----90c70a08837b--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----90c70a08837b--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----90c70a08837b--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----90c70a08837b--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----90c70a08837b--------------------------------", "anchor_text": ""}, {"url": "https://chaturangarajapakshe.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://chaturangarajapakshe.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Thilina Rajapakse"}, {"url": "https://chaturangarajapakshe.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.6K Followers"}, {"url": "http://www.linkedin.com/in/t-rajapakse/", "anchor_text": "www.linkedin.com/in/t-rajapakse/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6b1e2355088e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-guide-to-multi-tasking-with-the-t5-transformer-90c70a08837b&user=Thilina+Rajapakse&userId=6b1e2355088e&source=post_page-6b1e2355088e--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fecf622989264&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-guide-to-multi-tasking-with-the-t5-transformer-90c70a08837b&newsletterV3=6b1e2355088e&newsletterV3Id=ecf622989264&user=Thilina+Rajapakse&userId=6b1e2355088e&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}