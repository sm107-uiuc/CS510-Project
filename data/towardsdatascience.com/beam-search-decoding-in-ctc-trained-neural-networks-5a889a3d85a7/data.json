{"url": "https://towardsdatascience.com/beam-search-decoding-in-ctc-trained-neural-networks-5a889a3d85a7", "time": 1682993497.957394, "path": "towardsdatascience.com/beam-search-decoding-in-ctc-trained-neural-networks-5a889a3d85a7/", "webpage": {"metadata": {"title": "Beam Search Decoding in CTC-trained Neural Networks | by Harald Scheidl | Towards Data Science", "h1": "Beam Search Decoding in CTC-trained Neural Networks", "description": "Neural networks (NN) consisting of convolutional NN layers and recurrent NN layers combined with a final connectionist temporal classification (CTC) layer are a good choice for (handwritten) text\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/3797e43a86c", "anchor_text": "\u201cAn Intuitive Explanation of Connectionist Temporal Classification\u201d", "paragraph_index": 3}, {"url": "https://github.com/githubharald/CTCDecoder", "anchor_text": "CTCDecoder", "paragraph_index": 27}, {"url": "https://githubharald.github.io", "anchor_text": "https://githubharald.github.io", "paragraph_index": 32}], "all_paragraphs": ["Neural networks (NN) consisting of convolutional NN layers and recurrent NN layers combined with a final connectionist temporal classification (CTC) layer are a good choice for (handwritten) text recognition.", "The output of the NN is a matrix containing character-probabilities for each time-step (horizontal position), an example is shown in Fig 1. This matrix must be decoded to get the final text. One algorithm to achieve this is beam search decoding which can easily integrate a character-level language model.", "We will start our discussion with a recap of CTC and best path decoding. Then we will discuss the building blocks (basic algorithm, CTC scoring, language model) of the CTC beam search decoding algorithm. Finally, I will point you to a Python implementation which you can use to do your own tests and experiments.", "Reading the article \u201cAn Intuitive Explanation of Connectionist Temporal Classification\u201d helps you to understand the following discussion. Here I will give a short recap.", "CTC allows training text recognition systems with pairs of images and ground truth texts. Text is encoded in the NN output matrix by paths, which contain one character per time-step, e.g. \u2018ab\u2019 or \u2018aa\u2019 are possible paths in Fig. 1. I will show text in double quotes \u201ctext\u201d and paths in single quotes \u2018path\u2019.", "A path encodes text in the following way: each character of a text can be repeated an arbitrary number of times. Further, an arbitrary number of CTC blanks (non-character, not to be confused with a white-space character, denoted as \u201c-\u201d in this article) can be inserted between characters. In the case of repeated characters (e.g. \u201cpizza\u201d), at least one blank must be placed in between these repeated characters on the path (e.g. \u2018piz-za\u2019).", "Here are examples of texts with corresponding paths:", "As you see, there may be more than one path corresponding to a text. When we are interested in the probability of a text, we have to sum over the probabilities of all corresponding paths. The probability of a single path is the product of the character-probabilities on this path, e.g. for the path \u2018aa\u2019 in Fig. 1 it is 0.2\u00b70.4=0.08.", "Best path decoding is the simplest method to decode the output matrix:", "Let\u2019s look at an example: the matrix is shown in Fig. 2. The highest-scoring character is blank for both time-steps t0 and t1. So, the best path is \u2018--\u2019. We then undo the encoding and get the text \u201c\u201d. Further, we can calculate the probability of the path by multiplying the character-probabilities, which is 0.8\u00b70.6=0.48 in this example.", "Best path decoding is fast, we only have to find the character with the highest score for each time-step. If we have C characters and T time-steps, the algorithm has a running time of O(T\u00b7C).", "Best path decoding is both fast and simple, which are of course nice properties. But it may fail in certain situations like the one shown in Fig 2. In Fig. 3 all paths corresponding to the text \u201ca\u201d are shown: \u2018aa\u2019, \u2018a-\u2019 and \u2018-a\u2019. The probability of the text \u201ca\u201d is the sum over all probabilities of these mentioned paths: 0.2\u00b70.4+0.2\u00b70.6+0.8\u00b70.4=0.52. So, \u201ca\u201d is more probable than \u201c\u201d (0.52>0.48). We need a better algorithm than best path decoding which can handle such situations.", "Beam search decoding iteratively creates text candidates (beams) and scores them. Pseudo-code for a basic version is shows in Fig 4.: the list of beams is initialized with an empty beam (line 1) and a corresponding score (2). Then, the algorithm iterates over all time-steps of the NN output matrix (3\u201315). At each time-step, only the best scoring beams from the previous time-step are kept (4). The beam width (BW) specifies the number of beams to keep. For each of these beams, the score at the current time-step is calculated (8). Further, each beam is extended by all possible characters from the alphabet (10) and again, a score is calculated (11). After the last time-step, the best beam is returned as a result (16).", "Let\u2019s visualize how the algorithm decodes our example NN output with BW 2 and alphabet {\u201ca\u201d, \u201cb\u201d}. Fig. 5 shows both the NN output to be decoded and the tree of beams. The algorithm starts with an empty beam \u201c\u201d, which corresponds to the root node of the tree. The beam is then both copied and extended by all possible characters from the alphabet. This gives us the beams \u201ca\u201d, \u201cb\u201d and \u201c\u201d. Later, we will take a closer look at how to calculate the beam-scores. For now, we use our intuition and see that there is only one path corresponding to each beam: \u2018a\u2019 with probability 0.2, \u2018b\u2019 with 0 and \u2018-\u2019 with 0.8.", "In the next iteration, we just keep the 2 best beams (according to BW) from the previous time-step, i.e. we throw away the beam \u201cb\u201d. Then, we again both copy and extend the surviving beams and get \u201caa\u201d, \u201cab\u201d, \u201ca\u201d, \u201ca\u201d, \u201cb\u201d, \u201c\u201d. If two beams are equal as it is the case for \u201ca\u201d, we simply merge them: we add up the scores and only keep one of the beams. We again use our intuition to compute the scores. Each beam containing a \u201cb\u201d has a probability of 0. \u201caa\u201d also has 0 probability because to encode a text with repeated characters, we have to put a blank in between (e.g. \u2018a-a\u2019), which is not possible for a path of length 2. Finally, what remains are the beams \u201ca\u201d and \u201c\u201d. We already calculated the probabilities for them: 0.52 and 0.48.", "We finished the last iteration and the final step of the algorithm is to return the beam with the highest score, which is \u201ca\u201d in this example.", "We didn\u2019t talk about how to score the beams yet. We split the beam-score into the score of paths ending with a blank (e.g. \u2018aa-\u2019) and paths ending with a non-blank (e.g. \u2018aaa\u2019). We denote the probability of all paths ending with a blank and corresponding to a beam b at time-step t by Pb(b, t) and by Pnb(b, t) for the non-blank case. The probability Ptot(b, t) of a beam b at time-step t is then simply the sum of Pb and Pnb, i.e. Ptot(b, t)=Pb(b, t)+Pnb(b, t).", "Fig. 6 shows what happens when we extend a path. There are three main cases: extend by blank, extend by repeating last character and extend by some other character. When we collapse the extended paths, we either get the unchanged (copied) beam (\u201ca\u201d \u2192 \u201ca\u201d), or we get an extended beam (\u201ca\u201d \u2192 \u201caa\u201d or \u201cab\u201d). We can use this information the other way round too: if we extend a beam, we know which paths we have to consider to calculate the score.", "Let\u2019s look at how to iteratively compute Pb and Pnb. Note that we are always adding instead of assigning the computed values (+= instead of =), this implicitly implements the beam merging discussed earlier. All Pb and Pnb values are initially set to 0.", "To copy a beam, we can extend corresponding paths by a blank and get paths ending with a blank: Pb(b, t)+=Ptot(b, t-1)\u00b7mat(blank, t).", "Further, we may extend paths ending with a non-blank by the last character (if the beam is non-empty): Pnb(b, t)+=Pnb(b, t-1)\u00b7mat(b[-1], t), where -1 indexes the last character in the beam.", "There are two cases. Either we extend the beam by a character c different from the last character, then there is no need for separating blanks in the paths: Pnb(b+c, t)+=Ptot(b, t-1)\u00b7mat(c, t).", "We don\u2019t have to care about Pb(b+c, t) because we added a non-blank character.", "A character-level language model (LM) scores a sequence of characters. We restrict our LM to score single characters (unigram LM) and pairs of characters (bigram LM). We denote a unigram probability of the character c as P(c) and the bigram probability of characters c1, c2 as P(c2|c1). The score of a text \u201chello\u201d is the probability of seeing a single \u201ch\u201d, and the probability of seeing a pair \u201ch\u201d and \u201ce\u201d next to each other, and a pair \u201ce\u201d and \u201cl\u201d next to each other, \u2026", "Training such a LM from a large text is easy: we simply count how often a character occurs and divide by the total number of characters to get the unigram probability. And we count how often a pair of characters occurs and normalize it to get the bigram probability.", "The CTC beam search algorithm is shown in Fig. 7. It is similar to the already shown basic version, but includes code to score the beams: copied beams (lines 7\u201310) and extended beams are scored (15\u201319). Further, the LM is applied when extending a beam b by a character c (line 14). In case of a single-character beam, we apply the unigram score P(c), while for longer beams, we apply the bigram score P(b[-1], c). The LM score for a beam b is put into the variable Ptxt(b). When the algorithm looks for the best scoring beams, it sorts them according to Ptot\u00b7Ptxt (line 4) and then takes the BW best ones.", "The running time can be derived from the pseudo code: the outer-most loop has T iterations. In each iteration, the N beams are sorted, which accounts for N\u00b7log(N). The BW best beams are selected and each of them is extended by C characters. Therefore we have N=BW\u00b7C beams and the overall running time is O(T\u00b7BW\u00b7C\u00b7log(BW\u00b7C)).", "A Python implementation of beam search decoding (and other decoding algorithms) can be found in the CTCDecoder repository: the relevant code is located in src/BeamSearch.py and src/LanguageModel.py. TensorFlow provides the ctc_beam_search_decoder operation, however, it does not include a LM.", "Decoding a NN on the IAM dataset gives a character error rate of 5.60% with best path decoding and 5.35% with beam search decoding. The running time increases from 12ms to 56ms per sample.", "Here is a sample from the IAM dataset (see Fig. 8) to get a better feeling for how beam search improves the results. Decoding is done with best path decoding and beam search with and without LM.", "CTC beam search decoding is a simple and fast algorithm and outperforms best path decoding. A character-level LM can easily be integrated.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Interested in computer vision, deep learning, C++ and Python. https://githubharald.github.io"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F5a889a3d85a7&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeam-search-decoding-in-ctc-trained-neural-networks-5a889a3d85a7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeam-search-decoding-in-ctc-trained-neural-networks-5a889a3d85a7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeam-search-decoding-in-ctc-trained-neural-networks-5a889a3d85a7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeam-search-decoding-in-ctc-trained-neural-networks-5a889a3d85a7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----5a889a3d85a7--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5a889a3d85a7--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://harald-scheidl.medium.com/?source=post_page-----5a889a3d85a7--------------------------------", "anchor_text": ""}, {"url": "https://harald-scheidl.medium.com/?source=post_page-----5a889a3d85a7--------------------------------", "anchor_text": "Harald Scheidl"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5ecdb9e8b3fc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeam-search-decoding-in-ctc-trained-neural-networks-5a889a3d85a7&user=Harald+Scheidl&userId=5ecdb9e8b3fc&source=post_page-5ecdb9e8b3fc----5a889a3d85a7---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5a889a3d85a7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeam-search-decoding-in-ctc-trained-neural-networks-5a889a3d85a7&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5a889a3d85a7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeam-search-decoding-in-ctc-trained-neural-networks-5a889a3d85a7&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/3797e43a86c", "anchor_text": "\u201cAn Intuitive Explanation of Connectionist Temporal Classification\u201d"}, {"url": "https://github.com/githubharald/CTCDecoder", "anchor_text": "CTCDecoder"}, {"url": "https://github.com/githubharald/CTCDecoder", "anchor_text": "Python implementation of decoders (best path, beam search, \u2026)"}, {"url": "https://github.com/githubharald/CTCDecoder/blob/master/doc/comparison.pdf", "anchor_text": "Comparison of decoders"}, {"url": "https://towardsdatascience.com/b051d28f3d2e", "anchor_text": "Word beam search decoding"}, {"url": "https://towardsdatascience.com/3797e43a86c", "anchor_text": "Introduction to CTC"}, {"url": "https://arxiv.org/abs/1601.06581", "anchor_text": "Hwang and Sung \u2014 Character-Level Incremental Speech Recognition with Recurrent Neural Networks"}, {"url": "http://proceedings.mlr.press/v32/graves14.pdf", "anchor_text": "Graves and Jaitly \u2014 Towards end-to-end speech recognition with recurrent neural networks"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----5a889a3d85a7---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/ocr?source=post_page-----5a889a3d85a7---------------ocr-----------------", "anchor_text": "Ocr"}, {"url": "https://medium.com/tag/text-recognition?source=post_page-----5a889a3d85a7---------------text_recognition-----------------", "anchor_text": "Text Recognition"}, {"url": "https://medium.com/tag/algorithms?source=post_page-----5a889a3d85a7---------------algorithms-----------------", "anchor_text": "Algorithms"}, {"url": "https://medium.com/tag/recurrent-neural-network?source=post_page-----5a889a3d85a7---------------recurrent_neural_network-----------------", "anchor_text": "Recurrent Neural Network"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5a889a3d85a7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeam-search-decoding-in-ctc-trained-neural-networks-5a889a3d85a7&user=Harald+Scheidl&userId=5ecdb9e8b3fc&source=-----5a889a3d85a7---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5a889a3d85a7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeam-search-decoding-in-ctc-trained-neural-networks-5a889a3d85a7&user=Harald+Scheidl&userId=5ecdb9e8b3fc&source=-----5a889a3d85a7---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5a889a3d85a7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeam-search-decoding-in-ctc-trained-neural-networks-5a889a3d85a7&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5a889a3d85a7--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F5a889a3d85a7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeam-search-decoding-in-ctc-trained-neural-networks-5a889a3d85a7&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----5a889a3d85a7---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----5a889a3d85a7--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----5a889a3d85a7--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----5a889a3d85a7--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----5a889a3d85a7--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----5a889a3d85a7--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----5a889a3d85a7--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----5a889a3d85a7--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----5a889a3d85a7--------------------------------", "anchor_text": ""}, {"url": "https://harald-scheidl.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://harald-scheidl.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Harald Scheidl"}, {"url": "https://harald-scheidl.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.1K Followers"}, {"url": "https://githubharald.github.io", "anchor_text": "https://githubharald.github.io"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5ecdb9e8b3fc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeam-search-decoding-in-ctc-trained-neural-networks-5a889a3d85a7&user=Harald+Scheidl&userId=5ecdb9e8b3fc&source=post_page-5ecdb9e8b3fc--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fb95d23199e63&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeam-search-decoding-in-ctc-trained-neural-networks-5a889a3d85a7&newsletterV3=5ecdb9e8b3fc&newsletterV3Id=b95d23199e63&user=Harald+Scheidl&userId=5ecdb9e8b3fc&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}