{"url": "https://towardsdatascience.com/binary-cross-entropy-and-logistic-regression-bf7098e75559", "time": 1683008387.561215, "path": "towardsdatascience.com/binary-cross-entropy-and-logistic-regression-bf7098e75559/", "webpage": {"metadata": {"title": "Binary cross-entropy and logistic regression | by Jean-Christophe B. Loiseau | Towards Data Science", "h1": "Binary cross-entropy and logistic regression", "description": "Although it finds its roots in statistics, logistic regression is a fairly standard approach to solve binary classification problems in machine learning. It is actually so standard that it is\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.ibm.com/analytics/spss-statistics-software", "anchor_text": "SPSS", "paragraph_index": 0}, {"url": "https://www.gnu.org/software/pspp/", "anchor_text": "PSPP", "paragraph_index": 0}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html", "anchor_text": "scikit-learn", "paragraph_index": 0}, {"url": "https://www.statsmodels.org/stable/generated/statsmodels.discrete.discrete_model.Logit.html", "anchor_text": "statsmodels", "paragraph_index": 0}, {"url": "https://en.wikipedia.org/wiki/Matrix_calculus", "anchor_text": "here", "paragraph_index": 17}, {"url": "https://arxiv.org/abs/1902.07399", "anchor_text": "Yedida & Saha", "paragraph_index": 22}, {"url": "https://en.wikipedia.org/wiki/Newton%27s_method", "anchor_text": "Newton-Raphson\u2019s method", "paragraph_index": 26}, {"url": "https://en.wikipedia.org/wiki/Isaac_Newton", "anchor_text": "Sir Isaac Newton", "paragraph_index": 26}, {"url": "https://en.wikipedia.org/wiki/Joseph_Raphson", "anchor_text": "Joseph Raphson", "paragraph_index": 26}, {"url": "https://loiseau-jc.medium.com/list/lowrank-structure-and-datadriven-modeling-8f39635a90ea", "anchor_text": "low-rank structure and data-driven modeling", "paragraph_index": 44}, {"url": "https://loiseau-jc.medium.com/list/machine-learning-basics-0baf10d8f8b5", "anchor_text": "Machine learning basics", "paragraph_index": 44}], "all_paragraphs": ["Although it finds its roots in statistics, logistic regression is a fairly standard approach to solve binary classification problems in machine learning. It is actually so standard that it is implemented in all major data analysis software (e.g. Excel, SPSS, or its open-source alternative PSPP) and libraries (e.g. scikit-learn, statsmodels, etc). Even if you are only mildly familiar with logistic regression, you may know that it relies on the minimization of the so-called binary cross-entropy", "where m is the number of samples, x\u1d62 is the i-th training example, y\u1d62 its class (i.e. either 0 or 1), \u03c3(z) is the logistic function and w is the vector of parameters of the model. You may also know that, for logistic regression, it is a convex function. As such, any minimum is a global minimum. But have you ever wondered why we use it, where it actually comes from or how you could find this minimum more efficiently than with plain gradient descent? We\u2019ll address these questions below and provide simple implementations in Python. But first, let us do a quick recap\u2019 about the logistic function!", "The logistic function \u03c3(z) is an S-shaped curve defined as", "It is also sometimes known as the expit function or the sigmoid. It is monotonic and is bounded between 0 and 1, hence its widespread usage as a model for probability. We moreover have", "Finally, you can easily show that its derivative with respect to z is given by", "This is pretty much all you need to know about this function (at least for this post). So, without further ado, let us get started!", "Let us consider a predictor x and a binary (or Bernoulli) variable y. Assuming there exist some relationship between x and y, an ideal model would predict", "By using logistic regression, this unknown probability function is modeled as", "Our goal is thus to find the parameters w such that the modeled probability function is as close as possible to the true one.", "One way to assess how good of a job our model is doing is to compute the so-called likelihood function. Given m examples, this likelihood function is defined as", "Ideally, we thus want to find the parameters w that maximizes \u2112(w). In practice, however, one usually does not work directly with this function but with its negative log for the sake of simplicity", "Because logarithm is a strictly monotonic function, minimizing the negative log-likelihood will result in the same parameters w as when maximizing directly the likelihood function. But how to compute P(y|x, w) when our logistic regression only models P(1|x, w)? Given that", "one can use a simple exponentiation trick to write", "Inserting this expression into the negative log-likelihood function (and normalizing by the number of examples), we finally obtain the desired normalized binary cross-entropy", "Finding the weights w minimizing the binary cross-entropy is thus equivalent to finding the weights that maximize the likelihood function assessing how good of a job our logistic regression model is doing at approximating the true probability distribution of our Bernoulli variable!", "As stated, our goal is to find the weights w that minimizes the binary cross-entropy. In the most general case, a function may however admit multiple minima, and finding the global one is considered a hard problem. It can be shown nonetheless that minimizing the binary cross-entropy for the logistic regression is a convex problem and, as such, any minimum is a global one. Let us prove quickly it is indeed a convex problem!", "Several approaches could be used to prove that a function is convex. A sufficient condition is however that its Hessian matrix (i.e. its matrix of second-order derivatives) is positive semi-definite for all possible values of w. To facilitate our derivation and subsequent implementation, let us consider the vectorized version of the binary cross-entropy, i.e.", "where each row of X is one of our training examples and we made use of some identities introduced along with the logistic function. Using some elements of matrix calculus (check here if you\u2019re not familiar with it), one can show that the gradient of our loss function with respect to w is given by", "From this point, one can easily show that", "Hence, the Hessian matrix is positive semi-definite for every possible w and the binary cross-entropy (for the logistic regression) is a convex function. Now that we know our optimization problem is well-behaved, let us turn our attention to how to solve it!", "Unlike linear regression, no closed-form solution exists for logistic regression. The binary cross-entropy being a convex function in the present case, any technique from convex optimization is nonetheless guaranteed to find the global minimum. We\u2019ll illustrate this point below using two such techniques, namely gradient descent with optimal learning rate and Newton-Raphson\u2019s method.", "In machine learning, variations of gradient descent are the workhorses of model training. In this framework, the weights w are iteratively updated following the simple rule", "until convergence is reached. Here, \u03b1 is known as the learning rate or step size. It is quite common to use a constant learning rate but how to choose it? By computing the expression of the Lipschitz constant of various loss functions, Yedida & Saha have recently shown that, for the logistic regression, the optimal learning rate is given by", "Below is a simple Python implementation of the corresponding algorithm. Assuming you are already familiar with Python, the code should be quite self-explanatory.", "For the sake of clarity and usability, I try throughout every single one of my posts to stick to the scikit-learn API.", "Gradient descent-based techniques are also known as first-order methods since they only make use of the first derivatives encoding the local slope of the loss function. When proving the binary cross-entropy for logistic regression was a convex function, we however also computed the expression of the Hessian matrix so let\u2019s use it!", "Having access to the Hessian matrix allows us to use second-order optimization methods. Such techniques use additional information about the local curvature of the loss function encoded by this Hessian matrix to adaptively estimate the optimal step size in each direction during the training procedure, thus enabling faster convergence (albeit at a larger computational cost). The most famous second-order technique is the Newton-Raphson\u2019s method, named after the illustrious Sir Isaac Newton and lesser-known English mathematician Joseph Raphson. Using this method, the update rule for the weights w is now given by", "where H(w) is the Hessian matrix evaluated for the current w. Note that the entries of the Hessian matrix depend explicitly on the current w. As such, it needs to be updated at every iteration and its inverse recomputed. Although it converges faster than plain gradient descent, Newton\u2019s method is thus more computationally expansive and memory intensive. For small to moderate-size problems, it may nonetheless still converge faster (in wall-clock time) than gradient descent. For larger problems, one may look at methods known as Quasi-Newton, the most famous one being the BFGS method. As before, a simple Python implementation of the corresponding algorithm is provided below.", "Logistic regression provides a fairly flexible framework for classification tasks. As such, numerous variants have been proposed over the years to overcome some of its limitations.", "By construction, logistic regression is a linear classifier. Just like linear regression can be extended to model nonlinear relationships, logistic regression can also be extended to classify points otherwise nonlinearly separable. Doing so may however require expert knowledge, a good understanding of the properties of the data, and feature engineering (which is more of a craft than exact science).", "When using vanilla logistic regression, we implicitly assume that the prevalence of the two classes in our samples is roughly the same (e.g. predicting whether an individual is male or female). There are however numerous real-life situations where this is not the case. This is particularly true in medical sciences wherein one may like to predict whether, given his/her medical record, a patient will die or not after say surgery. Hopefully, most patients already treated have survived and our training dataset thus only contains relatively few examples of patients who did die. This is known as a class imbalance.", "Different approaches have been proposed to handle this class imbalance problem such as up-sampling the minority class or down-sampling the majority one. Another approach is to use cost-sensitive training. To illustrate the latter, let us considered the following situation: we have 90 samples belonging to say class y = 0 (e.g. patient survived) and only 10 belonging to class y = 1 (e.g. patient died). If our model were to predict y = 0 all the time (i.e. patient will survive), it would have a remarkable accuracy of 90% but would be nowhere useful to predict if a given patient is likely to die or not. A simple trick to improve the model\u2019s usefulness and predictive capabilities are however to modify the binary cross-entropy loss as follows", "The weights \u03b1\u2080 and \u03b1\u2081 are usually chosen as the inverse frequency of each class in the training set. Back to our small example above, \u03b1\u2080 would be chosen as", "while \u03b1\u2081 would be set to", "Doing so, the model is more severely penalized (approximately 10 times more) when it misclassifies a patient likely to die than to survive. It requires only minor modifications of the algorithms presented before. Although this approach may increase the number of false-positive (i.e. patients that would survive wrongly classified as being likely to die), it reduces the number of false-negative (i.e. patients that would die wrongly classified as being likely to survive). Doctors can then focus their attention on patients who actually need it even though a few of them would have survived anyway.", "Albeit binary classification problems are ubiquitous in real-life applications, some problems may require a multiclass approach exemplified by handwritten digit recognition.", "In this problem, one tries to assign a label (from 0 to 9) characterizing which digit is presented in the image. Even though logistic regression is by design a binary classification model, it can solve this task using a One-vs-Rest approach. Ten different logistic regression models are trained independently :", "In the deployment phase, the label assigned to a new image is based on which of these models is the most confident about its prediction. This One-vs-Rest approach is however not free from limitations, the major three being :", "Despite these limitations, a One-vs-Rest logistic regression model is nonetheless a good baseline to use when tackling a multiclass problem and I encourage you to do so as a starting point. A more suitable approach, known as softmax regression, will be considered in an upcoming post.", "It is fairly common in machine learning to handle data characterized by a large number of features. Not all of these features may however be informative for prediction purposes and one may thus aim for a sparse logistic regression model. To do so, one can for instance use an \u2113\u2081-norm regularization of the model\u2019s weights. The modified loss function is then given by", "where CE(w) is a shorthand notation for the binary cross-entropy. It is now well known that using such a regularization of the loss function encourages the vector of parameters w to be sparse. The hyper-parameter \u03bb then controls the trade-off between how sparse the model should be and how important it is to minimize the cross-entropy. Although hyper-parameter optimization is a dedicated area of machine learning in itself and well beyond the scope of this post, let us finally mention that scikit-learn provides a simple heuristic based on grid-search and cross-validation to find good values for \u03bb.", "Its simplicity and flexibility, both from a mathematical and computational point of view, makes logistic regression by far the most commonly used technique for binary classification in real-life applications. If you are a newcomer to machine learning, I hope you now have a better understanding of the mathematics it relies on and how it can be used in practice. Note that there is a lot we did not cover such as:", "These should however come in a second step after you have mastered the basics. Furthermore, there are plenty of resources online that address these extra points. Do not hesitate to go through them to gain even better insights! Do not hesitate also to derive all of the mathematical results presented herein yourself and to play with the codes provided! And please, let me know if any of this has been useful to you or if you have found any typos :]", "In the next few posts, we\u2019ll address the following subjects :", "Want to read more of this content ? Check out my other articles on low-rank structure and data-driven modeling or simply my Machine learning basics!", "Assistant Professor in Fluid Mechanics and Applied Mathematics. Passionate about machine learning, physics and science outreach."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fbf7098e75559&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbinary-cross-entropy-and-logistic-regression-bf7098e75559&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbinary-cross-entropy-and-logistic-regression-bf7098e75559&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbinary-cross-entropy-and-logistic-regression-bf7098e75559&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbinary-cross-entropy-and-logistic-regression-bf7098e75559&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://loiseau-jc.medium.com/?source=post_page-----bf7098e75559--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----bf7098e75559--------------------------------", "anchor_text": ""}, {"url": "https://loiseau-jc.medium.com/?source=post_page-----bf7098e75559--------------------------------", "anchor_text": "Jean-Christophe B. Loiseau"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F147ab927857&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbinary-cross-entropy-and-logistic-regression-bf7098e75559&user=Jean-Christophe+B.+Loiseau&userId=147ab927857&source=post_page-147ab927857----bf7098e75559---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----bf7098e75559--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbf7098e75559&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbinary-cross-entropy-and-logistic-regression-bf7098e75559&user=Jean-Christophe+B.+Loiseau&userId=147ab927857&source=-----bf7098e75559---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbf7098e75559&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbinary-cross-entropy-and-logistic-regression-bf7098e75559&source=-----bf7098e75559---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://www.ibm.com/analytics/spss-statistics-software", "anchor_text": "SPSS"}, {"url": "https://www.gnu.org/software/pspp/", "anchor_text": "PSPP"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html", "anchor_text": "scikit-learn"}, {"url": "https://www.statsmodels.org/stable/generated/statsmodels.discrete.discrete_model.Logit.html", "anchor_text": "statsmodels"}, {"url": "https://en.wikipedia.org/wiki/Matrix_calculus", "anchor_text": "here"}, {"url": "https://arxiv.org/abs/1902.07399", "anchor_text": "Yedida & Saha"}, {"url": "https://en.wikipedia.org/wiki/Newton%27s_method", "anchor_text": "Newton-Raphson\u2019s method"}, {"url": "https://en.wikipedia.org/wiki/Isaac_Newton", "anchor_text": "Sir Isaac Newton"}, {"url": "https://en.wikipedia.org/wiki/Joseph_Raphson", "anchor_text": "Joseph Raphson"}, {"url": "https://www.youtube.com/watch?v=ErfnhcEV1O8", "anchor_text": "here"}, {"url": "https://loiseau-jc.medium.com/list/lowrank-structure-and-datadriven-modeling-8f39635a90ea", "anchor_text": "low-rank structure and data-driven modeling"}, {"url": "https://loiseau-jc.medium.com/list/machine-learning-basics-0baf10d8f8b5", "anchor_text": "Machine learning basics"}, {"url": "https://towardsdatascience.com/rosenblatts-perceptron-the-very-first-neural-network-37a3ec09038a", "anchor_text": "Rosenblatt\u2019s perceptron, the very first neural networkA quick introduction to deep learning.towardsdatascience.com"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----bf7098e75559---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----bf7098e75559---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/programming?source=post_page-----bf7098e75559---------------programming-----------------", "anchor_text": "Programming"}, {"url": "https://medium.com/tag/statistics?source=post_page-----bf7098e75559---------------statistics-----------------", "anchor_text": "Statistics"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----bf7098e75559---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbf7098e75559&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbinary-cross-entropy-and-logistic-regression-bf7098e75559&user=Jean-Christophe+B.+Loiseau&userId=147ab927857&source=-----bf7098e75559---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbf7098e75559&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbinary-cross-entropy-and-logistic-regression-bf7098e75559&user=Jean-Christophe+B.+Loiseau&userId=147ab927857&source=-----bf7098e75559---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbf7098e75559&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbinary-cross-entropy-and-logistic-regression-bf7098e75559&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://loiseau-jc.medium.com/?source=post_page-----bf7098e75559--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----bf7098e75559--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F147ab927857&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbinary-cross-entropy-and-logistic-regression-bf7098e75559&user=Jean-Christophe+B.+Loiseau&userId=147ab927857&source=post_page-147ab927857----bf7098e75559---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F961520fda2b6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbinary-cross-entropy-and-logistic-regression-bf7098e75559&newsletterV3=147ab927857&newsletterV3Id=961520fda2b6&user=Jean-Christophe+B.+Loiseau&userId=147ab927857&source=-----bf7098e75559---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://loiseau-jc.medium.com/?source=post_page-----bf7098e75559--------------------------------", "anchor_text": "Written by Jean-Christophe B. Loiseau"}, {"url": "https://loiseau-jc.medium.com/followers?source=post_page-----bf7098e75559--------------------------------", "anchor_text": "280 Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----bf7098e75559--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F147ab927857&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbinary-cross-entropy-and-logistic-regression-bf7098e75559&user=Jean-Christophe+B.+Loiseau&userId=147ab927857&source=post_page-147ab927857----bf7098e75559---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F961520fda2b6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbinary-cross-entropy-and-logistic-regression-bf7098e75559&newsletterV3=147ab927857&newsletterV3Id=961520fda2b6&user=Jean-Christophe+B.+Loiseau&userId=147ab927857&source=-----bf7098e75559---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/rosenblatts-perceptron-the-very-first-neural-network-37a3ec09038a?source=author_recirc-----bf7098e75559----0---------------------f5d7b426_46fa_4dbc_8aa5_29d9b171bb78-------", "anchor_text": ""}, {"url": "https://loiseau-jc.medium.com/?source=author_recirc-----bf7098e75559----0---------------------f5d7b426_46fa_4dbc_8aa5_29d9b171bb78-------", "anchor_text": ""}, {"url": "https://loiseau-jc.medium.com/?source=author_recirc-----bf7098e75559----0---------------------f5d7b426_46fa_4dbc_8aa5_29d9b171bb78-------", "anchor_text": "Jean-Christophe B. Loiseau"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----bf7098e75559----0---------------------f5d7b426_46fa_4dbc_8aa5_29d9b171bb78-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/rosenblatts-perceptron-the-very-first-neural-network-37a3ec09038a?source=author_recirc-----bf7098e75559----0---------------------f5d7b426_46fa_4dbc_8aa5_29d9b171bb78-------", "anchor_text": "Rosenblatt\u2019s perceptron, the very first neural networkA quick introduction to deep learning."}, {"url": "https://towardsdatascience.com/rosenblatts-perceptron-the-very-first-neural-network-37a3ec09038a?source=author_recirc-----bf7098e75559----0---------------------f5d7b426_46fa_4dbc_8aa5_29d9b171bb78-------", "anchor_text": "\u00b715 min read\u00b7Mar 11, 2019"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F37a3ec09038a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frosenblatts-perceptron-the-very-first-neural-network-37a3ec09038a&user=Jean-Christophe+B.+Loiseau&userId=147ab927857&source=-----37a3ec09038a----0-----------------clap_footer----f5d7b426_46fa_4dbc_8aa5_29d9b171bb78-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/rosenblatts-perceptron-the-very-first-neural-network-37a3ec09038a?source=author_recirc-----bf7098e75559----0---------------------f5d7b426_46fa_4dbc_8aa5_29d9b171bb78-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "5"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F37a3ec09038a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frosenblatts-perceptron-the-very-first-neural-network-37a3ec09038a&source=-----bf7098e75559----0-----------------bookmark_preview----f5d7b426_46fa_4dbc_8aa5_29d9b171bb78-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----bf7098e75559----1---------------------f5d7b426_46fa_4dbc_8aa5_29d9b171bb78-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----bf7098e75559----1---------------------f5d7b426_46fa_4dbc_8aa5_29d9b171bb78-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----bf7098e75559----1---------------------f5d7b426_46fa_4dbc_8aa5_29d9b171bb78-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----bf7098e75559----1---------------------f5d7b426_46fa_4dbc_8aa5_29d9b171bb78-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----bf7098e75559----1---------------------f5d7b426_46fa_4dbc_8aa5_29d9b171bb78-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----bf7098e75559----1---------------------f5d7b426_46fa_4dbc_8aa5_29d9b171bb78-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----f5d7b426_46fa_4dbc_8aa5_29d9b171bb78-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----bf7098e75559----1---------------------f5d7b426_46fa_4dbc_8aa5_29d9b171bb78-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----bf7098e75559----1-----------------bookmark_preview----f5d7b426_46fa_4dbc_8aa5_29d9b171bb78-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----bf7098e75559----2---------------------f5d7b426_46fa_4dbc_8aa5_29d9b171bb78-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----bf7098e75559----2---------------------f5d7b426_46fa_4dbc_8aa5_29d9b171bb78-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----bf7098e75559----2---------------------f5d7b426_46fa_4dbc_8aa5_29d9b171bb78-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----bf7098e75559----2---------------------f5d7b426_46fa_4dbc_8aa5_29d9b171bb78-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----bf7098e75559----2---------------------f5d7b426_46fa_4dbc_8aa5_29d9b171bb78-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----bf7098e75559----2---------------------f5d7b426_46fa_4dbc_8aa5_29d9b171bb78-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----2-----------------clap_footer----f5d7b426_46fa_4dbc_8aa5_29d9b171bb78-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----bf7098e75559----2---------------------f5d7b426_46fa_4dbc_8aa5_29d9b171bb78-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----bf7098e75559----2-----------------bookmark_preview----f5d7b426_46fa_4dbc_8aa5_29d9b171bb78-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/categorical-cross-entropy-and-softmax-regression-780e8a2c5e8c?source=author_recirc-----bf7098e75559----3---------------------f5d7b426_46fa_4dbc_8aa5_29d9b171bb78-------", "anchor_text": ""}, {"url": "https://loiseau-jc.medium.com/?source=author_recirc-----bf7098e75559----3---------------------f5d7b426_46fa_4dbc_8aa5_29d9b171bb78-------", "anchor_text": ""}, {"url": "https://loiseau-jc.medium.com/?source=author_recirc-----bf7098e75559----3---------------------f5d7b426_46fa_4dbc_8aa5_29d9b171bb78-------", "anchor_text": "Jean-Christophe B. Loiseau"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----bf7098e75559----3---------------------f5d7b426_46fa_4dbc_8aa5_29d9b171bb78-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/categorical-cross-entropy-and-softmax-regression-780e8a2c5e8c?source=author_recirc-----bf7098e75559----3---------------------f5d7b426_46fa_4dbc_8aa5_29d9b171bb78-------", "anchor_text": "Categorical cross-entropy and SoftMax regressionEver wondered how to implement a simple baseline model for multi-class problems ? Here is one example (code included)."}, {"url": "https://towardsdatascience.com/categorical-cross-entropy-and-softmax-regression-780e8a2c5e8c?source=author_recirc-----bf7098e75559----3---------------------f5d7b426_46fa_4dbc_8aa5_29d9b171bb78-------", "anchor_text": "\u00b79 min read\u00b7Feb 15, 2021"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F780e8a2c5e8c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcategorical-cross-entropy-and-softmax-regression-780e8a2c5e8c&user=Jean-Christophe+B.+Loiseau&userId=147ab927857&source=-----780e8a2c5e8c----3-----------------clap_footer----f5d7b426_46fa_4dbc_8aa5_29d9b171bb78-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/categorical-cross-entropy-and-softmax-regression-780e8a2c5e8c?source=author_recirc-----bf7098e75559----3---------------------f5d7b426_46fa_4dbc_8aa5_29d9b171bb78-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F780e8a2c5e8c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcategorical-cross-entropy-and-softmax-regression-780e8a2c5e8c&source=-----bf7098e75559----3-----------------bookmark_preview----f5d7b426_46fa_4dbc_8aa5_29d9b171bb78-------", "anchor_text": ""}, {"url": "https://loiseau-jc.medium.com/?source=post_page-----bf7098e75559--------------------------------", "anchor_text": "See all from Jean-Christophe B. Loiseau"}, {"url": "https://towardsdatascience.com/?source=post_page-----bf7098e75559--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://ai.plainenglish.io/l1-lasso-and-l2-ridge-regularizations-in-logistic-regression-53ab6c952f15?source=read_next_recirc-----bf7098e75559----0---------------------52d03279_4876_47a0_8658_059e89215098-------", "anchor_text": ""}, {"url": "https://medium.com/@peterkaras?source=read_next_recirc-----bf7098e75559----0---------------------52d03279_4876_47a0_8658_059e89215098-------", "anchor_text": ""}, {"url": "https://medium.com/@peterkaras?source=read_next_recirc-----bf7098e75559----0---------------------52d03279_4876_47a0_8658_059e89215098-------", "anchor_text": "Peter Karas"}, {"url": "https://ai.plainenglish.io/?source=read_next_recirc-----bf7098e75559----0---------------------52d03279_4876_47a0_8658_059e89215098-------", "anchor_text": "Artificial Intelligence in Plain English"}, {"url": "https://ai.plainenglish.io/l1-lasso-and-l2-ridge-regularizations-in-logistic-regression-53ab6c952f15?source=read_next_recirc-----bf7098e75559----0---------------------52d03279_4876_47a0_8658_059e89215098-------", "anchor_text": "L1 (Lasso) and L2 (Ridge) regularizations in logistic regressionLogistic regression , Lasso and Rigde regularizations, derivations, math"}, {"url": "https://ai.plainenglish.io/l1-lasso-and-l2-ridge-regularizations-in-logistic-regression-53ab6c952f15?source=read_next_recirc-----bf7098e75559----0---------------------52d03279_4876_47a0_8658_059e89215098-------", "anchor_text": "\u00b76 min read\u00b7Feb 2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fai-in-plain-english%2F53ab6c952f15&operation=register&redirect=https%3A%2F%2Fai.plainenglish.io%2Fl1-lasso-and-l2-ridge-regularizations-in-logistic-regression-53ab6c952f15&user=Peter+Karas&userId=2b327b985531&source=-----53ab6c952f15----0-----------------clap_footer----52d03279_4876_47a0_8658_059e89215098-------", "anchor_text": ""}, {"url": "https://ai.plainenglish.io/l1-lasso-and-l2-ridge-regularizations-in-logistic-regression-53ab6c952f15?source=read_next_recirc-----bf7098e75559----0---------------------52d03279_4876_47a0_8658_059e89215098-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F53ab6c952f15&operation=register&redirect=https%3A%2F%2Fai.plainenglish.io%2Fl1-lasso-and-l2-ridge-regularizations-in-logistic-regression-53ab6c952f15&source=-----bf7098e75559----0-----------------bookmark_preview----52d03279_4876_47a0_8658_059e89215098-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-does-xgboost-handle-multiclass-classification-6c76ba71f6f0?source=read_next_recirc-----bf7098e75559----1---------------------52d03279_4876_47a0_8658_059e89215098-------", "anchor_text": ""}, {"url": "https://medium.com/@guillaume.saupin?source=read_next_recirc-----bf7098e75559----1---------------------52d03279_4876_47a0_8658_059e89215098-------", "anchor_text": ""}, {"url": "https://medium.com/@guillaume.saupin?source=read_next_recirc-----bf7098e75559----1---------------------52d03279_4876_47a0_8658_059e89215098-------", "anchor_text": "Saupin Guillaume"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----bf7098e75559----1---------------------52d03279_4876_47a0_8658_059e89215098-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/how-does-xgboost-handle-multiclass-classification-6c76ba71f6f0?source=read_next_recirc-----bf7098e75559----1---------------------52d03279_4876_47a0_8658_059e89215098-------", "anchor_text": "How Does XGBoost Handle Multiclass Classification?It\u2019s crucial to understand the underlying workings of classification using this kind of model, as it impacts performance."}, {"url": "https://towardsdatascience.com/how-does-xgboost-handle-multiclass-classification-6c76ba71f6f0?source=read_next_recirc-----bf7098e75559----1---------------------52d03279_4876_47a0_8658_059e89215098-------", "anchor_text": "\u00b77 min read\u00b7Jan 7"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6c76ba71f6f0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-does-xgboost-handle-multiclass-classification-6c76ba71f6f0&user=Saupin+Guillaume&userId=891e27328f3a&source=-----6c76ba71f6f0----1-----------------clap_footer----52d03279_4876_47a0_8658_059e89215098-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-does-xgboost-handle-multiclass-classification-6c76ba71f6f0?source=read_next_recirc-----bf7098e75559----1---------------------52d03279_4876_47a0_8658_059e89215098-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6c76ba71f6f0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-does-xgboost-handle-multiclass-classification-6c76ba71f6f0&source=-----bf7098e75559----1-----------------bookmark_preview----52d03279_4876_47a0_8658_059e89215098-------", "anchor_text": ""}, {"url": "https://medium.com/@msayef/logistic-regression-with-gradient-descent-and-regularization-binary-multi-class-classification-cc25ed63f655?source=read_next_recirc-----bf7098e75559----0---------------------52d03279_4876_47a0_8658_059e89215098-------", "anchor_text": ""}, {"url": "https://medium.com/@msayef?source=read_next_recirc-----bf7098e75559----0---------------------52d03279_4876_47a0_8658_059e89215098-------", "anchor_text": ""}, {"url": "https://medium.com/@msayef?source=read_next_recirc-----bf7098e75559----0---------------------52d03279_4876_47a0_8658_059e89215098-------", "anchor_text": "Sayef"}, {"url": "https://medium.com/@msayef/logistic-regression-with-gradient-descent-and-regularization-binary-multi-class-classification-cc25ed63f655?source=read_next_recirc-----bf7098e75559----0---------------------52d03279_4876_47a0_8658_059e89215098-------", "anchor_text": "Logistic Regression with Gradient Descent and Regularization: Binary & Multi-class ClassificationLearn how to implement logistic regression with gradient descent optimization from scratch."}, {"url": "https://medium.com/@msayef/logistic-regression-with-gradient-descent-and-regularization-binary-multi-class-classification-cc25ed63f655?source=read_next_recirc-----bf7098e75559----0---------------------52d03279_4876_47a0_8658_059e89215098-------", "anchor_text": "\u00b713 min read\u00b7Apr 9"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2Fcc25ed63f655&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40msayef%2Flogistic-regression-with-gradient-descent-and-regularization-binary-multi-class-classification-cc25ed63f655&user=Sayef&userId=e05fb963b9fd&source=-----cc25ed63f655----0-----------------clap_footer----52d03279_4876_47a0_8658_059e89215098-------", "anchor_text": ""}, {"url": "https://medium.com/@msayef/logistic-regression-with-gradient-descent-and-regularization-binary-multi-class-classification-cc25ed63f655?source=read_next_recirc-----bf7098e75559----0---------------------52d03279_4876_47a0_8658_059e89215098-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcc25ed63f655&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40msayef%2Flogistic-regression-with-gradient-descent-and-regularization-binary-multi-class-classification-cc25ed63f655&source=-----bf7098e75559----0-----------------bookmark_preview----52d03279_4876_47a0_8658_059e89215098-------", "anchor_text": ""}, {"url": "https://ai.plainenglish.io/logistic-regression-543c8424595d?source=read_next_recirc-----bf7098e75559----1---------------------52d03279_4876_47a0_8658_059e89215098-------", "anchor_text": ""}, {"url": "https://medium.com/@peterkaras?source=read_next_recirc-----bf7098e75559----1---------------------52d03279_4876_47a0_8658_059e89215098-------", "anchor_text": ""}, {"url": "https://medium.com/@peterkaras?source=read_next_recirc-----bf7098e75559----1---------------------52d03279_4876_47a0_8658_059e89215098-------", "anchor_text": "Peter Karas"}, {"url": "https://ai.plainenglish.io/?source=read_next_recirc-----bf7098e75559----1---------------------52d03279_4876_47a0_8658_059e89215098-------", "anchor_text": "Artificial Intelligence in Plain English"}, {"url": "https://ai.plainenglish.io/logistic-regression-543c8424595d?source=read_next_recirc-----bf7098e75559----1---------------------52d03279_4876_47a0_8658_059e89215098-------", "anchor_text": "Logistic Regression in DepthLogistic regression, activation function, derivation, math"}, {"url": "https://ai.plainenglish.io/logistic-regression-543c8424595d?source=read_next_recirc-----bf7098e75559----1---------------------52d03279_4876_47a0_8658_059e89215098-------", "anchor_text": "\u00b76 min read\u00b7Jan 31"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fai-in-plain-english%2F543c8424595d&operation=register&redirect=https%3A%2F%2Fai.plainenglish.io%2Flogistic-regression-543c8424595d&user=Peter+Karas&userId=2b327b985531&source=-----543c8424595d----1-----------------clap_footer----52d03279_4876_47a0_8658_059e89215098-------", "anchor_text": ""}, {"url": "https://ai.plainenglish.io/logistic-regression-543c8424595d?source=read_next_recirc-----bf7098e75559----1---------------------52d03279_4876_47a0_8658_059e89215098-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F543c8424595d&operation=register&redirect=https%3A%2F%2Fai.plainenglish.io%2Flogistic-regression-543c8424595d&source=-----bf7098e75559----1-----------------bookmark_preview----52d03279_4876_47a0_8658_059e89215098-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/understanding-l1-regularisation-in-gradient-boosted-decision-trees-af4f0ba9d32a?source=read_next_recirc-----bf7098e75559----2---------------------52d03279_4876_47a0_8658_059e89215098-------", "anchor_text": ""}, {"url": "https://matepocs.medium.com/?source=read_next_recirc-----bf7098e75559----2---------------------52d03279_4876_47a0_8658_059e89215098-------", "anchor_text": ""}, {"url": "https://matepocs.medium.com/?source=read_next_recirc-----bf7098e75559----2---------------------52d03279_4876_47a0_8658_059e89215098-------", "anchor_text": "Mate Pocs"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----bf7098e75559----2---------------------52d03279_4876_47a0_8658_059e89215098-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/understanding-l1-regularisation-in-gradient-boosted-decision-trees-af4f0ba9d32a?source=read_next_recirc-----bf7098e75559----2---------------------52d03279_4876_47a0_8658_059e89215098-------", "anchor_text": "Understanding L1 Regularisation in Gradient Boosted Decision TreesA thorough look with an example in LightGBM and R"}, {"url": "https://towardsdatascience.com/understanding-l1-regularisation-in-gradient-boosted-decision-trees-af4f0ba9d32a?source=read_next_recirc-----bf7098e75559----2---------------------52d03279_4876_47a0_8658_059e89215098-------", "anchor_text": "\u00b716 min read\u00b7Nov 1, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Faf4f0ba9d32a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-l1-regularisation-in-gradient-boosted-decision-trees-af4f0ba9d32a&user=Mate+Pocs&userId=686b78ddcf4b&source=-----af4f0ba9d32a----2-----------------clap_footer----52d03279_4876_47a0_8658_059e89215098-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/understanding-l1-regularisation-in-gradient-boosted-decision-trees-af4f0ba9d32a?source=read_next_recirc-----bf7098e75559----2---------------------52d03279_4876_47a0_8658_059e89215098-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Faf4f0ba9d32a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-l1-regularisation-in-gradient-boosted-decision-trees-af4f0ba9d32a&source=-----bf7098e75559----2-----------------bookmark_preview----52d03279_4876_47a0_8658_059e89215098-------", "anchor_text": ""}, {"url": "https://medium.com/grabngoinfo/gradient-descent-vs-616ba269de8d?source=read_next_recirc-----bf7098e75559----3---------------------52d03279_4876_47a0_8658_059e89215098-------", "anchor_text": ""}, {"url": "https://medium.com/@AmyGrabNGoInfo?source=read_next_recirc-----bf7098e75559----3---------------------52d03279_4876_47a0_8658_059e89215098-------", "anchor_text": ""}, {"url": "https://medium.com/@AmyGrabNGoInfo?source=read_next_recirc-----bf7098e75559----3---------------------52d03279_4876_47a0_8658_059e89215098-------", "anchor_text": "Amy @GrabNGoInfo"}, {"url": "https://medium.com/grabngoinfo?source=read_next_recirc-----bf7098e75559----3---------------------52d03279_4876_47a0_8658_059e89215098-------", "anchor_text": "GrabNGoInfo"}, {"url": "https://medium.com/grabngoinfo/gradient-descent-vs-616ba269de8d?source=read_next_recirc-----bf7098e75559----3---------------------52d03279_4876_47a0_8658_059e89215098-------", "anchor_text": "Gradient Descent vs Stochastic Gradient Descent vs Batch Gradient Descent vs Mini-batch Gradient\u2026Data science interview questions and answers"}, {"url": "https://medium.com/grabngoinfo/gradient-descent-vs-616ba269de8d?source=read_next_recirc-----bf7098e75559----3---------------------52d03279_4876_47a0_8658_059e89215098-------", "anchor_text": "\u00b74 min read\u00b7Dec 16, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fgrabngoinfo%2F616ba269de8d&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fgrabngoinfo%2Fgradient-descent-vs-616ba269de8d&user=Amy+%40GrabNGoInfo&userId=ef6171ffb4ed&source=-----616ba269de8d----3-----------------clap_footer----52d03279_4876_47a0_8658_059e89215098-------", "anchor_text": ""}, {"url": "https://medium.com/grabngoinfo/gradient-descent-vs-616ba269de8d?source=read_next_recirc-----bf7098e75559----3---------------------52d03279_4876_47a0_8658_059e89215098-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F616ba269de8d&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fgrabngoinfo%2Fgradient-descent-vs-616ba269de8d&source=-----bf7098e75559----3-----------------bookmark_preview----52d03279_4876_47a0_8658_059e89215098-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----bf7098e75559--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----bf7098e75559--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----bf7098e75559--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----bf7098e75559--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----bf7098e75559--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----bf7098e75559--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----bf7098e75559--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----bf7098e75559--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----bf7098e75559--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----bf7098e75559--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}