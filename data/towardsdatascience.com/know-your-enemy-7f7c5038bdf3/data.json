{"url": "https://towardsdatascience.com/know-your-enemy-7f7c5038bdf3", "time": 1682994504.8047612, "path": "towardsdatascience.com/know-your-enemy-7f7c5038bdf3/", "webpage": {"metadata": {"title": "Know your enemy. How you can create and defend against\u2026 | by Oscar Knagg | Towards Data Science", "h1": "Know your enemy", "description": "The current driving force in machine learning is to produce increasingly more accurate models while less attention has been paid to the security and robustness of these models. As I covered in my\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/@oknagg/know-your-enemy-the-fascinating-implications-of-adversarial-examples-5936bccb24af", "anchor_text": "previous post", "paragraph_index": 0}, {"url": "https://github.com/oscarknagg/adversarial/blob/master/notebooks/Creating_And_Defending_From_Adversarial_Examples.ipynb", "anchor_text": "this Jupyter notebook", "paragraph_index": 1}, {"url": "https://en.wikipedia.org/wiki/Norm_(mathematics)", "anchor_text": "norm", "paragraph_index": 3}, {"url": "https://distill.pub/2017/momentum/", "anchor_text": "momentum", "paragraph_index": 5}, {"url": "https://arxiv.org/abs/1412.6980", "anchor_text": "Adam", "paragraph_index": 5}, {"url": "https://arxiv.org/pdf/1805.12152.pdf", "anchor_text": "Madry et al", "paragraph_index": 23}, {"url": "https://github.com/oscarknagg/adversarial/blob/master/notebooks/Adversarial_Training.ipynb", "anchor_text": "Juypter Notebook", "paragraph_index": 24}, {"url": "https://blog.keras.io/the-limitations-of-deep-learning.html", "anchor_text": "not to anthropomorphize ML models", "paragraph_index": 27}, {"url": "http://proceedings.mlr.press/v80/mirman18b/mirman18b.pdf", "anchor_text": "certify", "paragraph_index": 27}, {"url": "https://github.com/eth-sri/diffai", "anchor_text": "robustness", "paragraph_index": 27}, {"url": "https://towardsdatascience.com/know-your-enemy-the-fascinating-implications-of-adversarial-examples-5936bccb24af", "anchor_text": "previous post", "paragraph_index": 28}, {"url": "https://github.com/oscarknagg/adversarial/blob/master/notebooks/Creating_And_Defending_From_Adversarial_Examples.ipynb", "anchor_text": "this Jupyter notebook", "paragraph_index": 28}, {"url": "https://arxiv.org/pdf/1805.12152.pdf", "anchor_text": "Robustness May Be at Odds With Accuracy", "paragraph_index": 29}, {"url": "https://arxiv.org/pdf/1706.06083.pdf", "anchor_text": "Towards Deep Learning Models Resistant To Adversarial Attack", "paragraph_index": 30}], "all_paragraphs": ["The current driving force in machine learning is to produce increasingly more accurate models while less attention has been paid to the security and robustness of these models. As I covered in my previous post ML models such as image classifiers are vulnerable to tiny perturbations to their inputs that cause them to make the wrong decisions. The aim of this post is to inform you how to create and defend from a powerful white-box adversarial attack via the example of an MNIST digit classifier. Contents:", "Check out this Jupyter notebook which contains the code to produce all the figures in this post and train your own adversarially robust model.", "The PGD attack is a white-box attack which means the attacker has access to the model gradients i.e. the attacker has a copy of your model\u2019s weights. This threat model gives the attacker much more power than black box attacks as they can specifically craft their attack to fool your model without having to rely on transfer attacks that often result in human-visible perturbations. PGD can be considered the most \u201ccomplete\u201d white-box adversary as it lifts any constraints on the amount of time and effort the attacker can put into finding the best attack.", "The key to understanding the PGD attack is to frame finding an adversarial example as a contrained optimisation problem. PGD attempts to find the perturbation that maximises the loss of a model on a particular input while keeping the size of the perturbation smaller than a specified amount referred to as epsilon. This constraint is usually expressed as the L\u00b2 or L\u221e norm of the perturbation and it is added so the content of the adversarial example is the same as the unperturbed sample \u2014 or even such that the adversarial example is imperceptibly different to humans.", "Real world attacks that could be possible with PGD are:", "The PGD algorithm can be summarised with the 5 steps below although the attacker is free to apply any optimisation improvements such as momentum, Adam, multiple restarts etc\u2026", "\u201cProjecting into the L^P ball\u201d may be an unfamiliar term but simply means moving a point outside of some volume to the closest point inside that volume. In the case of the L\u00b2 norm in 2D this is moving a point to the corresponding closest point on the circle of a particular radius centered at the origin.", "For those of you who have a practical mindset the following PyTorch function is an implementation of PGD to generate targeted or untargeted adversarial examples for a batch of images.", "Running this code snippet on samples from MNIST produces the following. Bear in mind that adversarial examples for MNIST are much more visible than for datasets like ImageNet due to its lower dimension/resolution \u2014 however a non-robust classifier is comprehensively fooled by these images.", "However we\u2019re data scientists and can do better than just interesting pictures. The charts below quantify the accuracy of a non-robust classifier against adversarial perturbations of varying size, epsilon. You can see the performance is poor as L\u00b2 and L\u221e bounded attacks of the size shown above reduce the accuracy of our model to around the random-guessing level. Let\u2019s see what we can do about this!", "The current state of the art defense against this attack is adversarial training, this is not the same training scheme as generative adversarial networks (GANs) although adversarially trained classifiers to exhibit GAN-like properties as I\u2019ll demonstrate later.", "Adversarial training is simply putting the PGD attack inside your training loop. This is can be viewed as \u201cultimate data augmentation\u201d as instead of performing random transformations (rescaling, cropping, mirroring etc.) as a preprocessing step we create specific perturbations that best fool our model and indeed adversarially trained models do exhibit less overfitting when trained on small datasets.", "This seems like an obvious approach but it is not obvious that such a training method will actually converge. Here\u2019s why: in regular training we minimise the expected natural loss over our dataset {x, y}, w.r.t our model parameters, theta.", "However in adversarial training we are minimising the following loss function where \u0394 is a set of perturbations we want our model to be invariant to such as the L\u00b2 and L\u221e perturbations discussed earlier.", "Hence we are now solving a minimax AKA saddle-point problem that is guaranteed to be non-convex. Similarities can be drawn between this and the GAN loss function below that is also a two-player minimax game where the players are the discriminator and generator instead of adversary and network.", "However in practice this kind of training does converge (and more consistently than GAN training) at the cost of increased training times as we are solving a multistep optimisation problem (i.e. PGD) for each training step of our network. The following code snippet trains an MNIST model against an L\u221e adversary.", "The following plots quantify the adversarial accuracy of models trained against L\u00b2 and L\u221e adversaries \u2014 there\u2019s definitely an improvement in robustness.", "There are a few interesting points to note here:", "My hypothesis to explain the poor robust and non-robust performance of the L\u00b2 trained model is that L\u00b2 bounded perturbations are semantically relevant on MNIST. If you take a look at the adversarial loss defined earlier we are training our model to be invariant to perturbations in the set \u0394 i.e. L\u00b2 perturbations. If these are semantically relevant perturbations on MNIST then L\u00b2 adversarial training is actively obstructing our models ability to learn! (The alternative hypothesis is that I need to search more hyperparameters for L\u00b2 training\u2026)", "Adversarial training of an MNIST classifier has some unexpected benefits beyond just robustness to attacks. The most interesting of which is the ability to smoothly interpolate between classes using large-epsilon adversarial examples. These are produced using the PGD method described earlier except we allow the size of the adversarial perturbation to be much larger than that used in training.", "Shown below are targeted large-epsilon adversarial examples created using the PGD attack on the L\u00b2 trained model. There are some artifacts but the end results are quite clearly images of the desired class. This is possible because the gradients of the robust model in the input space align well with human perception, so following that gradient with PGD produces plausible images.", "Trying the same thing with the non-robust model results in garbage images that only bear a slight resemblance to the desired classes.", "Interestingly, untargeted adversarial attacks on the L\u00b2 robust model produce a trajectory from the most to least similar class. Shown below are the adversarial examples and prediction probabilities as we increase the L\u00b2 norm of the adversarial pertubation from 0 to 10.", "This phenomenon isn\u2019t just unique to MNIST as Madry et al were able to produce the same kind of interpolations on an ImageNet trained model.", "The L\u221e does not produce as interesting interpolations as the L\u00b2 model (check out my Juypter Notebook if you want to generate some) but it does have it\u2019s own unexpected benefits \u2014 namely very sparse weights.", "Sparse weights are considered useful for their own sake as they\u2019re more interpretable and are more amenable to pruning and hence model size reductions. Manual inspection of the 32 convolutional filters in the first layer of the L\u221e model shows some interesting traits.", "As the non-zero filters have only one weight they become just a rescaling of the original image. Combining rescaling with ReLu activation means that these are thresholding filters i.e. ReLu(ax-b), the activations of which will remain unchanged for any perturbations smaller than b. Thresholding/binarizing of input data is a well known adversarial defense as it destroys small perturbations \u2014 adversarial training has caused the model to learn this independently!", "As deep learning achieves more widespread adoption we must remain cautious not to anthropomorphize ML models as we have seen they can fail catastrophically in ways unlike humans. However some stellar research is being done to prevent these kinds of attacks and in some cases certify robustness to particular perturbations so perhaps truly robust deep networks could be just around the corner.", "Thanks for reading. Feel free to read my previous post which highlights some surprising results about adversarial examples or to check out this Jupyter notebook which contains the code to produce all the figures in this post and train your own adversarially robust model.", "Robustness May Be at Odds With Accuracy \u2014Tsipras et al 2018", "Towards Deep Learning Models Resistant To Adversarial Attack \u2014 Madry et al 2017", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "I like to build novel things"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F7f7c5038bdf3&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknow-your-enemy-7f7c5038bdf3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknow-your-enemy-7f7c5038bdf3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknow-your-enemy-7f7c5038bdf3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknow-your-enemy-7f7c5038bdf3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----7f7c5038bdf3--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----7f7c5038bdf3--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@oknagg?source=post_page-----7f7c5038bdf3--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@oknagg?source=post_page-----7f7c5038bdf3--------------------------------", "anchor_text": "Oscar Knagg"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc510ccc9027c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknow-your-enemy-7f7c5038bdf3&user=Oscar+Knagg&userId=c510ccc9027c&source=post_page-c510ccc9027c----7f7c5038bdf3---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7f7c5038bdf3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknow-your-enemy-7f7c5038bdf3&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7f7c5038bdf3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknow-your-enemy-7f7c5038bdf3&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/@oknagg/know-your-enemy-the-fascinating-implications-of-adversarial-examples-5936bccb24af", "anchor_text": "previous post"}, {"url": "https://github.com/oscarknagg/adversarial/blob/master/notebooks/Creating_And_Defending_From_Adversarial_Examples.ipynb", "anchor_text": "this Jupyter notebook"}, {"url": "https://en.wikipedia.org/wiki/Norm_(mathematics)", "anchor_text": "norm"}, {"url": "https://devblogs.nvidia.com/malware-detection-neural-networks/", "anchor_text": "ML"}, {"url": "https://ieeexplore.ieee.org/document/7399288", "anchor_text": "based"}, {"url": "https://arxiv.org/abs/1812.10360v1", "anchor_text": "detection"}, {"url": "https://www.theverge.com/2018/12/3/18123752/tumblr-adult-content-porn-ban-date-explicit-changes-why-safe-mode", "anchor_text": "Tumblr\u2019s porn ban"}, {"url": "https://towardsdatascience.com/https-medium-com-skuttruf-machine-learning-in-finance-algorithmic-trading-on-energy-markets-cb68f7471475", "anchor_text": "ML trading strategies"}, {"url": "https://distill.pub/2017/momentum/", "anchor_text": "momentum"}, {"url": "https://arxiv.org/abs/1412.6980", "anchor_text": "Adam"}, {"url": "https://en.wikipedia.org/wiki/Gradient_descent", "anchor_text": "gradient step"}, {"url": "https://arxiv.org/pdf/1805.12152.pdf", "anchor_text": "Madry et al"}, {"url": "https://arxiv.org/pdf/1805.12152.pdf", "anchor_text": "https://arxiv.org/pdf/1805.12152.pdf"}, {"url": "https://github.com/oscarknagg/adversarial/blob/master/notebooks/Adversarial_Training.ipynb", "anchor_text": "Juypter Notebook"}, {"url": "https://blog.keras.io/the-limitations-of-deep-learning.html", "anchor_text": "not to anthropomorphize ML models"}, {"url": "http://proceedings.mlr.press/v80/mirman18b/mirman18b.pdf", "anchor_text": "certify"}, {"url": "https://github.com/eth-sri/diffai", "anchor_text": "robustness"}, {"url": "https://towardsdatascience.com/know-your-enemy-the-fascinating-implications-of-adversarial-examples-5936bccb24af", "anchor_text": "previous post"}, {"url": "https://github.com/oscarknagg/adversarial/blob/master/notebooks/Creating_And_Defending_From_Adversarial_Examples.ipynb", "anchor_text": "this Jupyter notebook"}, {"url": "https://arxiv.org/pdf/1805.12152.pdf", "anchor_text": "Robustness May Be at Odds With Accuracy"}, {"url": "https://arxiv.org/pdf/1706.06083.pdf", "anchor_text": "Towards Deep Learning Models Resistant To Adversarial Attack"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----7f7c5038bdf3---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/adversarial-network?source=post_page-----7f7c5038bdf3---------------adversarial_network-----------------", "anchor_text": "Adversarial Network"}, {"url": "https://medium.com/tag/computer-vision?source=post_page-----7f7c5038bdf3---------------computer_vision-----------------", "anchor_text": "Computer Vision"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----7f7c5038bdf3---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----7f7c5038bdf3---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7f7c5038bdf3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknow-your-enemy-7f7c5038bdf3&user=Oscar+Knagg&userId=c510ccc9027c&source=-----7f7c5038bdf3---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7f7c5038bdf3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknow-your-enemy-7f7c5038bdf3&user=Oscar+Knagg&userId=c510ccc9027c&source=-----7f7c5038bdf3---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7f7c5038bdf3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknow-your-enemy-7f7c5038bdf3&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----7f7c5038bdf3--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F7f7c5038bdf3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknow-your-enemy-7f7c5038bdf3&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----7f7c5038bdf3---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----7f7c5038bdf3--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----7f7c5038bdf3--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----7f7c5038bdf3--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----7f7c5038bdf3--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----7f7c5038bdf3--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----7f7c5038bdf3--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----7f7c5038bdf3--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----7f7c5038bdf3--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@oknagg?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@oknagg?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Oscar Knagg"}, {"url": "https://medium.com/@oknagg/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "654 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc510ccc9027c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknow-your-enemy-7f7c5038bdf3&user=Oscar+Knagg&userId=c510ccc9027c&source=post_page-c510ccc9027c--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F3bb863a32894&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknow-your-enemy-7f7c5038bdf3&newsletterV3=c510ccc9027c&newsletterV3Id=3bb863a32894&user=Oscar+Knagg&userId=c510ccc9027c&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}