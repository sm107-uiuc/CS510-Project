{"url": "https://towardsdatascience.com/learning-pyspark-by-exploring-and-visualizing-google-app-data-60cfecacc701", "time": 1683010479.823675, "path": "towardsdatascience.com/learning-pyspark-by-exploring-and-visualizing-google-app-data-60cfecacc701/", "webpage": {"metadata": {"title": "Learning PySpark by Exploring and Visualizing Google App Data | by Bee Guan Teo | Towards Data Science", "h1": "Learning PySpark by Exploring and Visualizing Google App Data", "description": "Apache Spark is an indispensable data processing framework that everyone should know when dealing with big data. When we try to perform data analysis on big data, we might encounter a problem that\u2026"}, "outgoing_paragraph_urls": [{"url": "http://spark.apache.org/", "anchor_text": "Source", "paragraph_index": 2}, {"url": "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html", "anchor_text": "Pandas DataFrame", "paragraph_index": 4}, {"url": "https://www.kaggle.com/lava18/google-play-store-apps", "anchor_text": "Kaggle", "paragraph_index": 6}, {"url": "https://databricks.com/", "anchor_text": "Databricks", "paragraph_index": 9}, {"url": "https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/4212847372568791/2909369757549244/2027857480347661/latest.html", "anchor_text": "link", "paragraph_index": 26}, {"url": "https://github.com/teobeeguan/PySpark-Project", "anchor_text": "Github", "paragraph_index": 26}, {"url": "https://ko-fi.com/teobeeguan", "anchor_text": "https://ko-fi.com/teobeeguan", "paragraph_index": 69}], "all_paragraphs": ["Apache Spark is an indispensable data processing framework that everyone should know when dealing with big data. When we try to perform data analysis on big data, we might encounter a problem that your current computer cannot cater the need to process big data due to a limited processing power and memory resources in a single computer. While we can try to upgrade our computer to meet the need of big data processing but we will soon find the computer can easily reach its maximum capacity again when dealing with the ever increasing datasets.", "One way to resolve this issue is to fetch our big data to a distributed and parallel processing platform supported by a cluster of computers instead of relying on a single machine. This is where Apache Spark come into the picture in big data processing.", "\u201cApache Spark is basically a unified analytics engine for large-scale data processing in the parallel and batch systems.\u201d (Source).", "Apache Spark is originally written in Scala language but it also offers a Python API which is PySpark. The release of PySpark eases the job of the data science community who are deep rooted in Python programming to harness the powerful feature of Apache Spark without picking up another programming language such as Scala. One can just write Python script to access the features offered by Apache Spark and perform data exploratory analysis on big data.", "Besides, learning PySpark is not a formidable task especially if you have been using Pandas for a while in your existing data analysis work. Spark offers a DataFrame data structure which is very similar to Pandas DataFrame.", "In this article, I am going to walk through an example of how we can perform data exploration and visualization on a Google App dataset represented as the Spark Dataframe.", "The sample datasets that we are going to use can be downloaded from Kaggle. There are two csv files available on the website and we will only use one of them which is \u201cgoogleplaystore.csv\u201d. The datasets is about Google Play Store Apps that entails the information such as app name, category, rating, price, etc.", "Just imagine if you intend to build a mobile app and wish to know more about the market trend prior to the app development, exploration of this dataset can be useful.", "We will need a distributed computing platform to host our dataset and also process it using PySpark. In the early day, setting up a distributed computing platform was a highly complex and daunting task.", "Fortunately, the entire setup process has been greatly simplified to a few button clicks with existence of cloud services. We are going to use one of the cloud services here which is Databricks. We will setup a distributed computing environment via Databricks to go through the data exploratory tasks presented in the article.", "Step 2: Sign up a Databricks account. Just click on \u201cTRY DATABRICKS\u201d at the top right corner. We will be redirected to a page where we can proceed to fill up our details to register an account.", "Step 3: After completing registration, sign in the Community Edition. Just click the tiny link of \u201cSign in here\u201d.", "Databricks offers a Community Edition which is totally free of charge. The Community Edition offers us a cluster with 15.3 GB Memory, 2 Cores and 1 DBU. This is sufficient for learning and experimental purpose.", "However, please note that the Community Edition cluster will automatically terminate after an idle period of two hours. This means we have to re-build a new cluster again in Databricks from time to time.", "(Don\u2019t worry, rebuilding a new cluster in Databricks just expect few clicks of button which can be completed within 1 minute).", "Step 4: Setup a cluster. In the main page of Databricks, select the \u201cClusters\u201d from a panel at the left hand side.", "Next, fill up \u201cCluster Name\u201d field in the following page. We can just provide a cluster name based on our preference.", "Wait for around 2\u20133 minutes before Databricks allocate a cluster to us.", "Step 5: Upload dataset. Select \u201cData\u201d from the left hand panel.", "We can choose to either drop the Kaggle dataset or browse our directory to upload the dataset.", "Step 6: Create a blank notebook from the Databricks main page. Give a name to our notebook.", "The new Notebook will automatically be attached to the cluster that we have just created in the earlier step.", "With just several clicks of button, we have managed to setup a distributed computing platform in Databricks and upload the data onto the platform. Besides, we have also created a Notebook where we can write our Python script to perform the data analytical work. PySpark is already built in the Notebook and there is no further installation of framework required here.", "(Please note the Notebook in Databricks just like our commonly used Jupyter Notebook which offers an interactive programming interface to write our scripts and visualize the output)", "We are now ready to start our data exploration journey using PySpark.", "In this section, we are going to start writing Python script in the Databricks Notebooks to perform exploratory data analysis using PySpark. This section will be broken down into seven parts and some common PySpark methods will be introduced along the way.", "An online version of the Notebook with completed code can be accessed at this link or you may also download an offline Notebook from my Github.", "The first step started with importing prerequisite libraries/modules.", "Next, we get the data from an external source (a CSV file in this case).", "Once we read the raw data from the CSV file, we might be interested to know some basic details of the dataset by having a quick overview of some records.", "3.2 Display first several rows of records", "At the first glance of the raw data read from the CSV file, we might have noticed several issues:", "Data cleaning and transformation are needed here to permit easy data access and analysis.", "Prior to removing the null values, we need to identify the columns where null values can be found.", "The output shows that there is one null value in \u201cContent Rating\u201d, \u201cCurrent Ver\u201d and \u201cAndroid Ver\u201d columns. Next we are going to use dropna method to remove the null value from the columns.", "In PySpark, we can transform our data in a specified column into a format that is useful to us. To do so, we can use withColumn and translate methods.", "At this point we have managed to remove unwanted characters, \u201cM\u201d or \u201ck\u201d, from the \u201cSize\u201d column. However, there is still one more issue remained. There is existence of value \u201cVaries with device\u201d in that column. This is because the size of some apps can vary with device. Such string value is inconsistent with the rest of the values (numerical)in the column and therefore we have to remove them.", "4.4 Change data type of columns", "Remember that all the columns are still in string format even though we have gone through the data cleaning and transformation steps above. We have to convert some columns from string to numerical values.", "Now, we can proceed to rename the columns which we have just transformed their values to reflect the changes.", "Not all the columns are relevant in the study here and we can remove those irrelevant columns.", "At last, we manage to obtain a clean data in a usable format and we are now ready to delve deeper to explore our data. In this part, we will use filter method to perform data query based on different type of conditions.", "In general, we can set a condition within the filter method and this will return all the records that match the condition. Let\u2019s look at the examples below:", "PySpark offers a method, between, to enable us to search for records between a lower limit and upper limit.", "This is also possible to search for record based on the existence of some specific keywords that exist in a particular column. We can do so by one of the three methods: startswith, endswith and contains. Let\u2019s look at several examples below:", "We can use logical operator such as &, |, and ~ to join multiple search conditions in our data query.", "We can easily run a quick descriptive statistical study on our dataset using PySpark describe method.", "We are coming to an interesting part where we will see how PySpark offers some very user friendly features to enable user to create different type of charts to visualize their data.", "In this part, we will plot some charts using PySpark display function to address some questions related to app development. Let\u2019s look at some examples below.", "Let\u2019s say we are interested to know which category of app show the highest market share. One simple solution is to create a pie chart to show the total number of installations by category. From there, we can easily identify the most dominant category of app.", "(By default, the original size of the chart might be very small. We can adjust the size by dragging the bottom right corner of chart figure to enlarge the image size.)", "From the pie chart, this is obvious that the game almost occupy half of the app market and records the highest market share compared with the rest.", "As shown above, we don\u2019t need to write additional codes to generate the plot. Instead, we can just use the display function to process our dataframe and pick one of the plot options from the drop down list to present our data.", "Here come with another question that might intrigue our interest: Will size of app affect the installation rate?", "We might predict users commonly prefer a lightweight app which consume less storage resources from their mobile devices. Let\u2019s verify it by plotting a histogram.", "From the histogram, an app with less than 50 Megabytes are most welcome by the community. The size above 100 Megabytes tends to drive a large group of users away from using it.", "When developing an app, we tend to make sure our app can reach as large community group as possible. The acceptance of our app is highly dependent on the Android version that can support our app.", "This is the reason a survey on the current most widely supported Android platform is very helpful to us to make a better decision to set a minimum OS platform for our app. To do so, we can choose to plot a bar chart that shows the number of occurrences of app supported by different Android version (at minimum level).", "From the bar chart above, we learn that most current apps are supported in Android version 4.1, 4.0.3, 4.0 and 4.4 (at minimum level). Hence, if we intend to aim for a larger market, it is wise to have our app to be supported by Android version 4 and above.", "Now we wish to set a reasonable price for our app. While setting a price is highly dependent on the development and maintenance cost, another important factor which is worth to consider is the affordability of the users. A hefty price tag can deter many users from using our app even though our app are well developed and maintained.", "Here we create a stacked bar chart to show us some clues about the affordability of different user groups.", "Obviously, a price tag of $0.99 are most widely accepted by all age groups. A price tag above $10 can hardly gain a significant public market share.", "Finally, we are left with one more question: Will the app price affect an app\u2019s rating? If users paid more, will they put a higher expectation on the app?", "To address this question, let\u2019s create a series of box plot.", "The box plots do not show an obvious pattern that the higher the median price, the rating tend to be lower or vice versa. The apps with price at $0.99 can receive a rating ranged from 3.4 to 5.", "In this article, we have seen how we can perform data exploratory and visualization in a distributed computing environment using PySpark. If you are familiar with Pandas Dataframe, you can easily adapt to the PySpark Dataframe as there are lots of similarities between them except for some minor differences in syntax. However, this is important to learn that Pandas is not designed for parallel processing but it is based on a single thread operation. Hence, Pandas is not a desirable option to handle a very huge datasets in a big data context. On anther hand, PySpark also offers a very user friendly way to plot some basic graphs from its dataframe. Basically, the data visualization job can be done through a graphical user interface as presented above.", "This is also worth to mention that there are still lots of PySpark features which are not discussed in this article and two of them are Resilient Distributed Datasets (RDD) and Spark MLlib which are too broad to cover in an article. My intention here is to introduce PySpark by mainly focusing on its dataframe and I hope this can facilitate those of you who have already familiar with Pandas to migrate your data skills to PySpark.", "I hope you enjoy and benefit from this article.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Like problem solving with Python | teobguan2013@gmail.com | Buy me a coffee at https://ko-fi.com/teobeeguan"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F60cfecacc701&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-pyspark-by-exploring-and-visualizing-google-app-data-60cfecacc701&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-pyspark-by-exploring-and-visualizing-google-app-data-60cfecacc701&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-pyspark-by-exploring-and-visualizing-google-app-data-60cfecacc701&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-pyspark-by-exploring-and-visualizing-google-app-data-60cfecacc701&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://me.dm/@teobguan2013", "anchor_text": "Mastodon"}, {"url": "https://towardsdatascience.com/?source=post_page-----60cfecacc701--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----60cfecacc701--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://teobguan2013.medium.com/?source=post_page-----60cfecacc701--------------------------------", "anchor_text": ""}, {"url": "https://teobguan2013.medium.com/?source=post_page-----60cfecacc701--------------------------------", "anchor_text": "Bee Guan Teo"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd25adce173af&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-pyspark-by-exploring-and-visualizing-google-app-data-60cfecacc701&user=Bee+Guan+Teo&userId=d25adce173af&source=post_page-d25adce173af----60cfecacc701---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F60cfecacc701&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-pyspark-by-exploring-and-visualizing-google-app-data-60cfecacc701&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F60cfecacc701&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-pyspark-by-exploring-and-visualizing-google-app-data-60cfecacc701&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@rami_alzayat?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Rami Al-zayat"}, {"url": "https://unsplash.com/s/photos/technology?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "http://spark.apache.org/", "anchor_text": "Source"}, {"url": "https://www.pexels.com/@divinetechygirl?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels", "anchor_text": "Christina Morillo"}, {"url": "https://www.pexels.com/photo/woman-programming-on-a-notebook-1181359/?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels", "anchor_text": "Pexels"}, {"url": "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html", "anchor_text": "Pandas DataFrame"}, {"url": "https://www.kaggle.com/lava18/google-play-store-apps", "anchor_text": "Kaggle"}, {"url": "https://databricks.com/", "anchor_text": "Databricks"}, {"url": "https://databricks.com/", "anchor_text": "https://databricks.com/"}, {"url": "https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/4212847372568791/2909369757549244/2027857480347661/latest.html", "anchor_text": "link"}, {"url": "https://github.com/teobeeguan/PySpark-Project", "anchor_text": "Github"}, {"url": "http://spark.apache.org/", "anchor_text": "http://spark.apache.org/"}, {"url": "https://databricks.com/", "anchor_text": "https://databricks.com/"}, {"url": "https://medium.com/tag/data-science?source=post_page-----60cfecacc701---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/python?source=post_page-----60cfecacc701---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/pyspark?source=post_page-----60cfecacc701---------------pyspark-----------------", "anchor_text": "Pyspark"}, {"url": "https://medium.com/tag/data-visualization?source=post_page-----60cfecacc701---------------data_visualization-----------------", "anchor_text": "Data Visualization"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F60cfecacc701&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-pyspark-by-exploring-and-visualizing-google-app-data-60cfecacc701&user=Bee+Guan+Teo&userId=d25adce173af&source=-----60cfecacc701---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F60cfecacc701&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-pyspark-by-exploring-and-visualizing-google-app-data-60cfecacc701&user=Bee+Guan+Teo&userId=d25adce173af&source=-----60cfecacc701---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F60cfecacc701&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-pyspark-by-exploring-and-visualizing-google-app-data-60cfecacc701&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----60cfecacc701--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F60cfecacc701&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-pyspark-by-exploring-and-visualizing-google-app-data-60cfecacc701&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----60cfecacc701---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----60cfecacc701--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----60cfecacc701--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----60cfecacc701--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----60cfecacc701--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----60cfecacc701--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----60cfecacc701--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----60cfecacc701--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----60cfecacc701--------------------------------", "anchor_text": ""}, {"url": "https://teobguan2013.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://teobguan2013.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Bee Guan Teo"}, {"url": "https://teobguan2013.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.3K Followers"}, {"url": "https://ko-fi.com/teobeeguan", "anchor_text": "https://ko-fi.com/teobeeguan"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd25adce173af&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-pyspark-by-exploring-and-visualizing-google-app-data-60cfecacc701&user=Bee+Guan+Teo&userId=d25adce173af&source=post_page-d25adce173af--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fbb9e2c126c7f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-pyspark-by-exploring-and-visualizing-google-app-data-60cfecacc701&newsletterV3=d25adce173af&newsletterV3Id=bb9e2c126c7f&user=Bee+Guan+Teo&userId=d25adce173af&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}