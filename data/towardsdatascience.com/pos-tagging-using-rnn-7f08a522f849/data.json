{"url": "https://towardsdatascience.com/pos-tagging-using-rnn-7f08a522f849", "time": 1683013330.410951, "path": "towardsdatascience.com/pos-tagging-using-rnn-7f08a522f849/", "webpage": {"metadata": {"title": "POS Tagging Using RNN. Learn how to use RNNs to tag words in\u2026 | by Tanya | Towards Data Science", "h1": "POS Tagging Using RNN", "description": "The classical way of doing POS tagging is using some variant of Hidden Markov Model. Here we'll see how we could do that using Recurrent neural networks. The original RNN architecture has some\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.kaggle.com/tanyadayanand/pos-tagging-using-rnn", "anchor_text": "notebook", "paragraph_index": 2}, {"url": "https://www.kaggle.com/tanyadayanand/pos-tagging-using-rnn", "anchor_text": "here", "paragraph_index": 4}, {"url": "http://www.linkedin.com/in/tanyadayanand", "anchor_text": "LinkedIn", "paragraph_index": 47}], "all_paragraphs": ["The classical way of doing POS tagging is using some variant of Hidden Markov Model. Here we'll see how we could do that using Recurrent neural networks. The original RNN architecture has some variants too. It has a novel RNN architecture \u2014 the Bidirectional RNN which is capable of reading sequences in the \u2018reverse order\u2019 as well and has proven to boost performance significantly.", "Then two important cutting-edge variants of the RNN which have made it possible to train large networks on real datasets. Although RNNs are capable of solving a variety of sequence problems, their architecture itself is their biggest enemy due to the problems of exploding and vanishing gradients that occur during the training of RNNs. This problem is solved by two popular gated RNN architectures \u2014 the Long, Short Term Memory (LSTM) and the Gated Recurrent Unit (GRU). We\u2019ll look into all these models here with respect to POS tagging.", "The process of classifying words into their parts of speech and labeling them accordingly is known as part-of-speech tagging, or simply POS-tagging. The NLTK library has a number of corpora that contain words and their POS tag. I will be using the POS tagged corpora i.e treebank, conll2000, and brown from NLTK to demonstrate the key concepts. To get into the codes directly, an accompanying notebook is published on Kaggle.", "The following table provides information about some of the major tags:", "Let\u2019s begin with importing the necessary libraries and loading the dataset. This is a requisite step in every data analysis process(The complete code can be viewed here). We\u2019ll be loading the data first using three well-known text corpora and taking the union of those.", "As a part of preprocessing, we\u2019ll be performing various steps such as dividing data into words and tags, Vectorise X and Y, and Pad sequences.", "Let's look at the data first. For each of the words below, there is a tag associated with it.", "Since this is a many-to-many problem, each data point will be a different sentence of the corpora. Each data point will have multiple words in the input sequence. This is what we will refer to as X. Each word will have its corresponding tag in the output sequence. This what we will refer to as Y. Sample dataset:", "The next thing we need to figure out is how are we going to feed these inputs to an RNN. If we have to give the words as input to any neural networks then we essentially have to convert them into numbers. We need to create a word embedding or one-hot vectors i.e. a vector of numbers form of each word. To start with this we'll first encode the input and output which will give a blind unique id to each word in the entire corpus for input data. On the other hand, we have the Y matrix(tags/output data). We have twelve POS tags here, treating each of them as a class and each pos tag is converted into one-hot encoding of length twelve. We\u2019ll use the Tokenizer() function from Keras library to encode text sequence to integer sequence.", "Make sure that each sequence of input and output is of the same length.", "The sentences in the corpus are not of the same length. Before we feed the input in the RNN model we need to fix the length of the sentences. We cannot dynamically allocate memory required to process each sentence in the corpus as they are of different lengths. Therefore the next step after encoding the data is to define the sequence lengths. We need to either pad short sentences or truncate long sentences to a fixed length. This fixed length, however, is a hyperparameter.", "You know that a better way (than one-hot vectors) to represent text is word embeddings. Currently, each word and each tag is encoded as an integer. We\u2019ll use a more sophisticated technique to represent the input words (X) using what\u2019s known as word embeddings.", "However, to represent each tag in Y, we\u2019ll simply use one-hot encoding scheme since there are only 12 tags in the dataset and the LSTM will have no problems in learning its own representation of these tags.", "To use word embeddings, you can go for either of the following models:", "We\u2019re using the word2vec model for no particular reason. Both of these are very efficient in representing words. You can try both and see which one works better.", "The dimension of a word embedding is: (VOCABULARY_SIZE, EMBEDDING_DIMENSION)", "All the data preprocessing is now complete. Let\u2019s now jump to the modeling part by splitting the data to train, validation, and test sets.", "Before using RNN, we must make sure the dimensions of the data are what an RNN expects. In general, an RNN expects the following shape", "Now, there can be various variations in the shape that you use to feed an RNN depending on the type of architecture. Since the problem we\u2019re working on has a many-to-many architecture, the input and the output both include number of timesteps which is nothing but the sequence length. But notice that the tensor X doesn\u2019t have the third dimension, that is, number of features. That\u2019s because we\u2019re going to use word embeddings before feeding in the data to an RNN, and hence there is no need to explicitly mention the third dimension. That\u2019s because when you use the Embedding() layer in Keras, the training data will automatically be converted to (#samples, #timesteps, #features) where #features will be the embedding dimension (and note that the Embedding layer is always the very first layer of an RNN). While using the embedding layer we only need to reshape the data to (#samples, #timesteps) which is what we have done. However, note that you\u2019ll need to shape it to (#samples, #timesteps, #features) in case you don\u2019t use the Embedding() layer in Keras.", "Next, let\u2019s build the RNN model. We\u2019re going to use word embeddings to represent the words. Now, while training the model, you can also train the word embeddings along with the network weights. These are often called the embedding weights. While training, the embedding weights will be treated as normal weights of the network which are updated in each iteration.", "In the next few sections, we will try the following three RNN models:", "Let\u2019s start with the first experiment: a vanilla RNN with arbitrarily initialized, untrainable embedding. For this RNN we won\u2019t use the pre-trained word embeddings. We\u2019ll use randomly initialized embeddings. Moreover, we won\u2019t update the embeddings weights.", "We can see here, after ten epoch, it is giving fairly decent accuracy of approx 95%. Also, we are getting a healthy growth curve below.", "Next, try the second model \u2014 RNN with arbitrarily initialized, trainable embeddings. Here, we\u2019ll allow the embeddings to get trained with the network. All I am doing is changing the parameter trainable to true i.e trainable = True. Rest all remains the same as above. On checking the model summary we can see that all the parameters have become trainable. i.e trainable params are equal to total params.", "On fitting the model the accuracy has grown significantly. It has gone up to approx 98.95% by allowing the embedding weights to train. Therefore embedding has a significant effect on how the network is going to perform.", "we\u2019ll now try the word2vec embeddings and see if that improves our model or not.", "Let\u2019s now try the third experiment \u2014 RNN with trainable word2vec embeddings. Recall that we had loaded the word2vec embeddings in a matrix called \u2018embedding_weights\u2019. Using word2vec embeddings is just as easy as including this matrix in the model architecture.", "The network architecture is the same as above but instead of starting with an arbitrary embedding matrix, we\u2019ll use pre-trained embedding weights (weights = [embedding_weights]) coming from word2vec. The accuracy, in this case, has gone even further to approx 99.04%.", "The results improved marginally in this case. That\u2019s because the model was already performing very well. You\u2019ll see much more improvements by using pre-trained embeddings in cases where you don\u2019t have such a good model performance. Pre-trained embeddings provide a real boost in many applications.", "To solve the vanishing gradients problem, many attempts have been made to tweak the vanilla RNNs such that the gradients don\u2019t die when sequences get long. The most popular and successful of these attempts has been the long, short-term memory network, or the LSTM. LSTMs have proven to be so effective that they have almost replaced vanilla RNNs.", "Thus, one of the fundamental differences between an RNN and an LSTM is that an LSTM has an explicit memory unit which stores information relevant for learning some task. In the standard RNN, the only way the network remembers past information is through updating the hidden states over time, but it does not have an explicit memory to store information.", "On the other hand, in LSTMs, the memory units retain pieces of information even when the sequences get really long.", "Next, we\u2019ll build an LSTM model instead of an RNN. We just need to replace the RNN layer with LSTM layer.", "The LSTM model also provided some marginal improvement. However, if we use an LSTM model in other tasks such as language translation, image captioning, time series forecasting, etc. then you may see a significant boost in the performance.", "Keeping in mind the computational expenses and the problem of overfitting, researchers have tried to come up with alternate structures of the LSTM cell. The most popular one of these alternatives is the gated recurrent unit (GRU). GRU being a simpler model than LSTM, it's always easier to train. LSTMs and GRUs have almost completely replaced the standard RNNs in practice because they\u2019re more effective and faster to train than vanilla RNNs (despite the larger number of parameters).", "Let\u2019s now build a GRU model. We\u2019ll then also compare the performance of the RNN, LSTM, and the GRU model.", "There is a reduction in params in GRU as compared to LSTM. Therefore we do get a significant boost in terms of computational efficiency with hardly any decremental effect in the performance of the model.", "The accuracy of the model remains the same as the LSTM. But we saw that the time taken by an LSTM is greater than a GRU and an RNN. This was expected since the parameters in an LSTM and GRU are 4x and 3x of a normal RNN, respectively.", "For example, when you want to assign a sentiment score to a piece of text (say a customer review), the network can see the entire review text before assigning them a score. On the other hand, in a task such as predicting the next word given previous few typed words, the network does not have access to the words in the future time steps while predicting the next word.", "These two types of tasks are called offline and online sequence processing respectively.", "Now, there is a neat trick you can use with offline tasks \u2014 since the network has access to the entire sequence before making predictions, why not use this task to make the network \u2018look at the future elements in the sequence\u2019 while training, hoping that this will make the network learn better?", "This is the idea exploited by what is called bidirectional RNNs.", "By using bidirectional RNNs, it is almost certain that you\u2019ll get better results. However, bidirectional RNNs take almost double the time to train since the number of parameters of the network increase. Therefore, you have a tradeoff between training time and performance. The decision to use a bidirectional RNN depends on the computing resources that you have and the performance you are aiming for.", "Finally, let\u2019s build one more model \u2014 a bidirectional LSTM and compare its performance in terms of accuracy and training time as compared to the previous models.", "You can see the no of parameters has gone up. It does significantly shoot up the no of params.", "The bidirectional LSTM did increase the accuracy substantially (considering that the accuracy was already hitting the roof). This shows the power of bidirectional LSTMs. However, this increased accuracy comes at a cost. The time taken was almost double than of a normal LSTM network.", "Below is the quick summary of each of the four models we tried. We can see a trend here as we move from one model to another.", "If you have any questions, recommendations, or critiques, I can be reached via LinkedIn or the comment section.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "AI & ML Enthusiast | Reshaping with technology"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F7f08a522f849&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpos-tagging-using-rnn-7f08a522f849&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpos-tagging-using-rnn-7f08a522f849&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpos-tagging-using-rnn-7f08a522f849&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpos-tagging-using-rnn-7f08a522f849&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----7f08a522f849--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----7f08a522f849--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@tanyadayanand?source=post_page-----7f08a522f849--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@tanyadayanand?source=post_page-----7f08a522f849--------------------------------", "anchor_text": "Tanya"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff678f7de95fd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpos-tagging-using-rnn-7f08a522f849&user=Tanya&userId=f678f7de95fd&source=post_page-f678f7de95fd----7f08a522f849---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7f08a522f849&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpos-tagging-using-rnn-7f08a522f849&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7f08a522f849&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpos-tagging-using-rnn-7f08a522f849&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@angelekamp?utm_source=medium&utm_medium=referral", "anchor_text": "Ang\u00e8le Kamp"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://www.kaggle.com/tanyadayanand/pos-tagging-using-rnn", "anchor_text": "notebook"}, {"url": "https://www.kaggle.com/tanyadayanand/pos-tagging-using-rnn", "anchor_text": "POS Tagging Using RNNExplore and run machine learning code with Kaggle Notebooks | Using data from word-embeddingswww.kaggle.com"}, {"url": "https://www.kaggle.com/tanyadayanand/pos-tagging-using-rnn", "anchor_text": "here"}, {"url": "https://code.google.com/archive/p/word2vec/", "anchor_text": "word2vec model"}, {"url": "https://nlp.stanford.edu/projects/glove/", "anchor_text": "GloVe model"}, {"url": "http://www.linkedin.com/in/tanyadayanand", "anchor_text": "LinkedIn"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----7f08a522f849---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/pos-tagging?source=post_page-----7f08a522f849---------------pos_tagging-----------------", "anchor_text": "Pos Tagging"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----7f08a522f849---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/recurrent-neural-network?source=post_page-----7f08a522f849---------------recurrent_neural_network-----------------", "anchor_text": "Recurrent Neural Network"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----7f08a522f849---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7f08a522f849&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpos-tagging-using-rnn-7f08a522f849&user=Tanya&userId=f678f7de95fd&source=-----7f08a522f849---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7f08a522f849&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpos-tagging-using-rnn-7f08a522f849&user=Tanya&userId=f678f7de95fd&source=-----7f08a522f849---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7f08a522f849&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpos-tagging-using-rnn-7f08a522f849&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----7f08a522f849--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F7f08a522f849&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpos-tagging-using-rnn-7f08a522f849&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----7f08a522f849---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----7f08a522f849--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----7f08a522f849--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----7f08a522f849--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----7f08a522f849--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----7f08a522f849--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----7f08a522f849--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----7f08a522f849--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----7f08a522f849--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@tanyadayanand?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@tanyadayanand?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Tanya"}, {"url": "https://medium.com/@tanyadayanand/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "68 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff678f7de95fd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpos-tagging-using-rnn-7f08a522f849&user=Tanya&userId=f678f7de95fd&source=post_page-f678f7de95fd--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fc91183103b2b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpos-tagging-using-rnn-7f08a522f849&newsletterV3=f678f7de95fd&newsletterV3Id=c91183103b2b&user=Tanya&userId=f678f7de95fd&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}