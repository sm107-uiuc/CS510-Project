{"url": "https://towardsdatascience.com/understanding-bert-bidirectional-encoder-representations-from-transformers-45ee6cd51eef", "time": 1683012039.796761, "path": "towardsdatascience.com/understanding-bert-bidirectional-encoder-representations-from-transformers-45ee6cd51eef/", "webpage": {"metadata": {"title": "Understanding BERT \u2014 (Bidirectional Encoder Representations from Transformers) | by Sarthak Vajpayee | Towards Data Science", "h1": "Understanding BERT \u2014 (Bidirectional Encoder Representations from Transformers)", "description": "This is a 3 part series where we will be going through Transformers, BERT, and a hands-on Kaggle challenge \u2014 Google QUEST Q&A Labeling to see Transformers in action (top 4.4% on the leaderboard). In\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.kaggle.com/c/google-quest-challenge/", "anchor_text": "Google QUEST Q&A Labeling", "paragraph_index": 0}, {"url": "https://arxiv.org/pdf/1810.04805.pdf", "anchor_text": "paper", "paragraph_index": 2}, {"url": "https://arxiv.org/pdf/1802.05365.pdf", "anchor_text": "paper", "paragraph_index": 2}, {"url": "https://rajpurkar.github.io/SQuAD-explorer/", "anchor_text": "SQuAD v1.1", "paragraph_index": 11}, {"url": "https://huggingface.co/transformers/model_doc/bert.html", "anchor_text": "https://huggingface.co/transformers/model_doc/bert.html", "paragraph_index": 23}, {"url": "https://towardsdatascience.com/transformers-state-of-the-art-natural-language-processing-1d84c4c7462b?source=friends_link&sk=4ba3eb424ff59ce765c749819c6b5892", "anchor_text": "part 1/3", "paragraph_index": 31}, {"url": "https://towardsdatascience.com/hands-on-transformers-kaggle-google-quest-q-a-labeling-affd3dad7bcb?source=friends_link&sk=159fd259d7ae346bb4cfa07dc5180938", "anchor_text": "part 3/3", "paragraph_index": 32}, {"url": "https://www.kaggle.com/c/google-quest-challenge/", "anchor_text": "Google QUEST Q&A Labeling", "paragraph_index": 32}], "all_paragraphs": ["This is a 3 part series where we will be going through Transformers, BERT, and a hands-on Kaggle challenge \u2014 Google QUEST Q&A Labeling to see Transformers in action (top 4.4% on the leaderboard).In this part (2/3) we will be looking at BERT (Bidirectional Encoder Representations from Transformers) and how it became state-of-the-art in various modern natural language processing tasks.Since the architecture of BERT is based on Transformers, you might want to check the internals of a Transformer which you can find in Part 1/3.", "We are aware of the fact that how transfer learning has revolutionized the field of computer vision in the past few years. Pretrained networks like VGG, YOLO, UNET, RESNET, and many more have shown a groundbreaking performance in different areas of computer vision. We have also seen a similar approach in the field of NLP like Word2Vector, GloVe but BERT takes it to a whole another level.", "BERT was a paper published by researchers at Google AI Language in late 2018 and since then it has become a state of the art in many NLP tasks. BERT takes a different approach, it considers all the words of the input sentence simultaneously and then uses an attention mechanism to develop a contextual meaning of the words.This approach works well for many NLP tasks as shown in the ELMo (Embeddings from Language Models) paper recently.", "In this blog, we will cover BERT by going through 4 sub-topics-", "How is BERT different from other embedding generating algorithms like Word2Vector or GloVe?", "The main differences between BERT and W2V or GloVe are:", "Now that we have got to know BERT a bit, let\u2019s understand how it works.", "Taking a look under the hood", "The architecture of BERT is derived from transformers. Inside a BERT there are several stacked encoder cells, similar to what we saw in transformers. Remember that inside a transformer how the encoder cells were used to read the input sentence and the decoder cells were used to predict the output sentence (word by word) but in the case of BERT, since we only need a model that reads the input sentence and generates some features that can be used for various NLP tasks, only the encoder part of the transformer is used.The bi-directional part in BERT comes from the fact that it reads all the input words simultaneously.", "Like I said before, the encoder cells are similar to what we saw in the transformer. There are self-attention heads and then a feed-forward neural network. The attention heads and feed-forward neural networks are also parameters that define different kinds of BERT models.", "If we look at BERT base and BERT large which are the two BERT architectures, both BERT base, and BERT large take an input of 512-dimensions.", "On SQuAD v1.1, BERT achieves a 93.16% F1 score, surpassing even the human-level score of 91.2%: BERT also improves the state-of-the-art by 7.6% absolute on the very challenging GLUE benchmark, a set of 9 diverse Natural Language Understanding (NLU) tasks.", "*The paper calls encoder cells as transformer blocks.", "BERT is available as a pre-trained model. It was pre-trained on a large corpus of unlabelled text including the entire Wikipedia(that\u2019s 2,500 million words) and book corpus (800 million words). Let\u2019s see the 2 training methods that were used to train BERT.", "While training a BERT model, both of the approaches discussed above are used simultaneously.", "Having learned about the architecture and the training process of BERT, now let\u2019s understand how to generate the output using BERT given some input text.", "Special tokens: There are some special tokens or keywords that are used while generating the input for BERT. The main ones are [CLS] and [SEP].[CLS] is used as a very first token added at the beginning of the input sentence. [SEP] is used as a separator between different sentences when multiple input sentences are passed.Let\u2019s consider an example: Suppose we want to pass the two sentences \u201cI have a pen\u201d and \u201cThe pen is red\u201d to BERT. The tokenizer will first tokenize these sentences as: [\u2018[CLS]\u2019, \u2018I\u2019, \u2018have\u2019, \u2018a\u2019, \u2018pen\u2019, \u2018[SEP]\u2019, \u2018the\u2019, \u2018pen\u2019, \u2018is\u2019, \u2018red\u2019, \u2018[SEP]\u2019]and then convert into numerical tokens.", "BERT takes 3 types of input:", "Remember for each word in the input, the BERT base internally creates a 768-dimensional output but for tasks like classification, we do not actually require the output for all the embeddings. So by default, BERT considers only the output corresponding to the first token [CLS] and drops the output vectors corresponding to all the other tokens.", "This works pretty neatly for classification tasks like spam detection where for a given an input text, we need to predict if it is spam or not.We generate token_embeddings, segmentation_embeddings, and mask_tokens for the input sentence, pass them into BERT which generates a 768-dimensional output. Finally, we take this output and pass it into a feed-forward network like a Dense layer with 2 nodes and softmax as the activation function.", "The catch with the above output is that it does not work as good for tasks where we need to focus more on the semantic meaning of the sentence like machine translation.For tasks like these, it is advised to use the pooled or averaged output of the hidden states of encoders. This is sometimes also referred to as feature extraction.", "After much experimentation on which vector works best as a contextualized embedding, the paper mentions 6 choices.", "Turns out that for most of the tasks, concatenation of the hidden states from the last four encoders seems to work the best.", "To know more about the input parameters and the values returned my BERT you can check out the official documentation here: https://huggingface.co/transformers/model_doc/bert.html", "Finally, let\u2019s take a look at what all tasks BERT can perform as per the paper.", "(a). To classify a pair of sentences eg. a question-answer pair is relevant or not.", "(b). To classify a single sentence eg. detect if the input sentence is a spam or not.", "(c). To generate an answer to the given question title and paragraph.", "(d). Single sentence tagging tasks such as named entity recognition, a tag must be predicted for every word in the input.", "That was all to BERT in this blog, Hope the read was pleasant.I would like to thank all the creators for creating the awesome content I referred to for writing this blog.", "Thank you for reading the blog. I hope it was useful for some of you aspiring to do projects or learn some new concepts in NLP.", "In part 1/3 we covered how Transformers became state-of-the-art in various modern natural language processing tasks and their working.", "In part 3/3 we will go through a hands-on Kaggle challenge \u2014 Google QUEST Q&A Labeling to see Transformers in action (top 4.4% on the leaderboard).", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Learning something new epoch by epoch."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F45ee6cd51eef&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-bert-bidirectional-encoder-representations-from-transformers-45ee6cd51eef&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-bert-bidirectional-encoder-representations-from-transformers-45ee6cd51eef&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-bert-bidirectional-encoder-representations-from-transformers-45ee6cd51eef&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-bert-bidirectional-encoder-representations-from-transformers-45ee6cd51eef&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----45ee6cd51eef--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----45ee6cd51eef--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@itssarthakvajpayee?source=post_page-----45ee6cd51eef--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@itssarthakvajpayee?source=post_page-----45ee6cd51eef--------------------------------", "anchor_text": "Sarthak Vajpayee"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd850bda2ea8c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-bert-bidirectional-encoder-representations-from-transformers-45ee6cd51eef&user=Sarthak+Vajpayee&userId=d850bda2ea8c&source=post_page-d850bda2ea8c----45ee6cd51eef---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F45ee6cd51eef&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-bert-bidirectional-encoder-representations-from-transformers-45ee6cd51eef&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F45ee6cd51eef&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-bert-bidirectional-encoder-representations-from-transformers-45ee6cd51eef&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.kaggle.com/c/google-quest-challenge/", "anchor_text": "Google QUEST Q&A Labeling"}, {"url": "https://arxiv.org/pdf/1810.04805.pdf", "anchor_text": "paper"}, {"url": "https://arxiv.org/pdf/1802.05365.pdf", "anchor_text": "paper"}, {"url": "http://jalammar.github.io/illustrated-bert/", "anchor_text": "http://jalammar.github.io/illustrated-bert/"}, {"url": "http://jalammar.github.io/illustrated-bert/", "anchor_text": "http://jalammar.github.io/illustrated-bert/"}, {"url": "http://jalammar.github.io/illustrated-bert/", "anchor_text": "http://jalammar.github.io/illustrated-bert/"}, {"url": "https://towardsdatascience.com/understanding-bert-is-it-a-game-changer-in-nlp-7cca943cf3ad", "anchor_text": "https://medium.com/r/?url=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-bert-is-it-a-game-changer-in-nlp-7cca943cf3ad"}, {"url": "https://rajpurkar.github.io/SQuAD-explorer/", "anchor_text": "SQuAD v1.1"}, {"url": "https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270", "anchor_text": "https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270"}, {"url": "http://jalammar.github.io/illustrated-bert/", "anchor_text": "http://jalammar.github.io/illustrated-bert/"}, {"url": "http://jalammar.github.io/illustrated-bert/", "anchor_text": "http://jalammar.github.io/illustrated-bert/"}, {"url": "https://huggingface.co/transformers/model_doc/bert.html", "anchor_text": "https://huggingface.co/transformers/model_doc/bert.html"}, {"url": "http://jalammar.github.io/illustrated-bert/", "anchor_text": "http://jalammar.github.io/illustrated-bert/"}, {"url": "https://www.appliedaicourse.com/", "anchor_text": "https://www.appliedaicourse.com/"}, {"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "https://arxiv.org/abs/1810.04805"}, {"url": "https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270", "anchor_text": "https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270"}, {"url": "http://jalammar.github.io/illustrated-bert/", "anchor_text": "http://jalammar.github.io/illustrated-bert/"}, {"url": "https://www.youtube.com/watch?v=BhlOGGzC0Q0", "anchor_text": "https://www.youtube.com/watch?v=BhlOGGzC0Q0"}, {"url": "https://towardsdatascience.com/understanding-bert-is-it-a-game-changer-in-nlp-7cca943cf3ad", "anchor_text": "https://towardsdatascience.com/understanding-bert-is-it-a-game-changer-in-nlp-7cca943cf3ad"}, {"url": "https://towardsdatascience.com/transformers-state-of-the-art-natural-language-processing-1d84c4c7462b?source=friends_link&sk=4ba3eb424ff59ce765c749819c6b5892", "anchor_text": "part 1/3"}, {"url": "https://towardsdatascience.com/hands-on-transformers-kaggle-google-quest-q-a-labeling-affd3dad7bcb?source=friends_link&sk=159fd259d7ae346bb4cfa07dc5180938", "anchor_text": "part 3/3"}, {"url": "https://www.kaggle.com/c/google-quest-challenge/", "anchor_text": "Google QUEST Q&A Labeling"}, {"url": "http://www.linkedin.com/in/sarthak-vajpayee", "anchor_text": "www.linkedin.com/in/sarthak-vajpayee"}, {"url": "https://medium.com/tag/nlp?source=post_page-----45ee6cd51eef---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/deeplearing?source=post_page-----45ee6cd51eef---------------deeplearing-----------------", "anchor_text": "Deeplearing"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----45ee6cd51eef---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----45ee6cd51eef---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/education?source=post_page-----45ee6cd51eef---------------education-----------------", "anchor_text": "Education"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F45ee6cd51eef&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-bert-bidirectional-encoder-representations-from-transformers-45ee6cd51eef&user=Sarthak+Vajpayee&userId=d850bda2ea8c&source=-----45ee6cd51eef---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F45ee6cd51eef&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-bert-bidirectional-encoder-representations-from-transformers-45ee6cd51eef&user=Sarthak+Vajpayee&userId=d850bda2ea8c&source=-----45ee6cd51eef---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F45ee6cd51eef&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-bert-bidirectional-encoder-representations-from-transformers-45ee6cd51eef&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----45ee6cd51eef--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F45ee6cd51eef&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-bert-bidirectional-encoder-representations-from-transformers-45ee6cd51eef&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----45ee6cd51eef---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----45ee6cd51eef--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----45ee6cd51eef--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----45ee6cd51eef--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----45ee6cd51eef--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----45ee6cd51eef--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----45ee6cd51eef--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----45ee6cd51eef--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----45ee6cd51eef--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@itssarthakvajpayee?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@itssarthakvajpayee?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Sarthak Vajpayee"}, {"url": "https://medium.com/@itssarthakvajpayee/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "136 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd850bda2ea8c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-bert-bidirectional-encoder-representations-from-transformers-45ee6cd51eef&user=Sarthak+Vajpayee&userId=d850bda2ea8c&source=post_page-d850bda2ea8c--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fb72ebbfba4df&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-bert-bidirectional-encoder-representations-from-transformers-45ee6cd51eef&newsletterV3=d850bda2ea8c&newsletterV3Id=b72ebbfba4df&user=Sarthak+Vajpayee&userId=d850bda2ea8c&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}