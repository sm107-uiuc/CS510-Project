{"url": "https://towardsdatascience.com/an-intuitive-explanation-of-random-forest-and-extra-trees-classifiers-8507ac21d54b", "time": 1682997172.260182, "path": "towardsdatascience.com/an-intuitive-explanation-of-random-forest-and-extra-trees-classifiers-8507ac21d54b/", "webpage": {"metadata": {"title": "An Intuitive Explanation of Random Forest and Extra Trees Classifiers | by Frank Ceballos | Towards Data Science", "h1": "An Intuitive Explanation of Random Forest and Extra Trees Classifiers", "description": "Purpose: The purpose of this article is to provide the reader an intuitive understanding of Random Forest and Extra Trees classifiers. Materials and methods: We will use the Iris dataset which\u2026"}, "outgoing_paragraph_urls": [{"url": "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html", "anchor_text": "Random Forest", "paragraph_index": 0}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html", "anchor_text": "Extra Trees", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/i-want-to-be-the-very-best/installing-keras-tensorflow-using-anaconda-for-machine-learning-44ab28ff39cb", "anchor_text": "article", "paragraph_index": 3}, {"url": "https://towardsdatascience.com/scikit-learn-decision-trees-explained-803f3812290d", "anchor_text": "here", "paragraph_index": 3}, {"url": "https://en.wikipedia.org/wiki/Decision_stump", "anchor_text": "decision stump", "paragraph_index": 4}, {"url": "https://stats.stackexchange.com/questions/320156/hard-voting-versus-soft-voting-in-ensemble-based-methods", "anchor_text": "hard votes", "paragraph_index": 4}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html", "anchor_text": "BaggingClassifier", "paragraph_index": 5}, {"url": "https://towardsdatascience.com/scikit-learn-decision-trees-explained-803f3812290d", "anchor_text": "read this article", "paragraph_index": 12}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html", "anchor_text": "Voting Classifier", "paragraph_index": 20}, {"url": "https://medium.com/i-want-to-be-the-very-best/model-design-and-selection-with-scikit-learn-18a29041d02a?source=post_stats_page---------------------------", "anchor_text": "model design and selection using Scikit-learn", "paragraph_index": 20}, {"url": "https://www.linkedin.com/in/frank-ceballos/", "anchor_text": "LinkedIn", "paragraph_index": 21}, {"url": "https://www.frank-ceballos.com/", "anchor_text": "personal blog", "paragraph_index": 21}], "all_paragraphs": ["Purpose: The purpose of this article is to provide the reader an intuitive understanding of Random Forest and Extra Trees classifiers.", "Materials and methods: We will use the Iris dataset which contains features describing three species of flowers. In total there are 150 instances, each containing four features and labeled with one species of flower. We will investigate and report on the accuracy of Decision Trees, Random Forest, and Extra Trees.", "Hardware: We train and evaluate our models on an Apple workstation equipped with 8 GB 1600 MHz DDR3, with Inter(R)Core(TM) i7 with 2 CPUs @ 2.9 Ghz, and an Intel HD Graphics 4000 card. Let\u2019s kick some tail.", "Note: In the case you\u2019re starting from scratch, I advise you follow this article to install all the necessary libraries. Finally, it will be assumed that the reader is familiar with Python, Pandas, Scikit-learn, and decision trees. A detailed explanation about Scikit-learn decision trees can be found here.", "Suppose that we have a weak learner, a classifier whose accuracy is slightly better than a random decision, with a classification accuracy of 51 %. This could be a decision stump , a decision tree classifier with its depth set to one. At first instance, it would appear that one shouldn\u2019t bother which such a weak classifier; however, what if we consider putting together 1000 slightly different decision stumps (an ensemble), each with 51 % accuracy to make our final prediction? Intuitively, we can see that in average, 510 of these classifiers would correctly classify a test case and that 490 would misclassify it. If we collect the hard votes of each classifier, we could see that on average there would be about 20 more correct predictions; consequently, our ensemble would tend to have an accuracy higher than 51 %. Let\u2019s see this in practice.", "Here we will build a decision stump and compare its predictive performance to an ensemble of 1000 of them. The ensemble of decision trees is created using the Scikit-learn BaggingClassifier. The decision stump and the ensemble will be trained on the Iris dataset which contains four features and three classes. The data is randomly split to create a training and test set.", "Each decision stump will be built with the following criteria:", "First we import all the libraries that we will use for this article.", "Then, we load the data, split it, and train and compare a single stump vs an ensemble. The results are printed to the console.", "The results show that the ensemble of 1000 decision stumps obtained an accuracy of 55 %, showing that they are no better than a single decision stump. So what happened? Why are we not getting better results? Well we basically created 1000 decision stumps that were exactly the same. It\u2019s like we asked a single person what their favorite food was 1000 times and, not surprisingly, obtained the same answer 1000 times.", "In the previous section, we learned that having 1000 copies of the same decision stump in our ensemble is like having a single decision stump. Therefore, we will change the criteria for how we build each stump in order to introduce variation.", "Each decision stump will be built with the following criteria:", "What we just described was the criteria to create a Random Forest. However, a Random Forest uses decision trees with a depth of one or greater. The term random stems from the fact that we randomly sample the training set, and since we have a collection of trees, it\u2019s natural to call it a forest \u2014 hence Random Forest. To build the root node or any node in the tree, a random subset of features is selected. For each of these selected features, the algorithm searches for the optimal cutting point to determine the split for the given feature. The feature from the randomly selected subset that produces the purest split is then used to create the root node. The tree is grown to a depth of one, and the same process is repeated for all other nodes in the tree, until the desired depth of the tree is reached. Finally, it\u2019s important to note that each tree is built separately using a different bootstrap, which introduces variation among the trees. Consequently, each tree makes different mistakes and when combined a strong classifier can be built. If you\u2019re confused by all the jargon, read this article that explains most of what I just described in a single paragraph.", "What?! So by simply introducing variation, we were able to obtain an accuracy of 95 %. In other words, decision stumps with low accuracies were used to build a forest. Variation was introduced among the stumps by building them on bootstraps \u2014 created by sampling the training set with replacement and allowing the stumps to only search subsets of randomly selected features to split the root node. Individually, each stump would obtain a low accurracy. However, when used in an ensemble, we showed that their accuracy skyrocketed! That, my friends, is what is commonly referred to as the wisdom of the crowd.", "Just so that it sinks in, we can use weak classifiers as the base of an ensemble to obtain a high performing one.", "Similar to a Random Forest classifier we have the Extra Trees classifier \u2014 also known as Extremely Randomized Trees. To introduce more variation into the ensemble, we will change how we build trees.", "Each decision stump will be built with the following criteria:", "Notice that in an Extra Trees classifier, the features and splits are selected at random; hence, \u201cExtremely Randomized Tree\u201d. Since splits are chosen at random for each feature in the Extra Trees Classifier, it\u2019s less computationally expensive than a Random Forest.", "The Extra Trees classifier performed similarly to the Random Forest. However, there are performance differences that I would like to mention. Namely: Decision Trees show high variance, Random Forests show medium variance, and Extra Trees show low variance.", "If you reached the end of the article, by now you should understand the power of ensemble methods and know how Random Forest and and Extra Trees classifiers are built. I would like to mention that you should not use the Bagging classifier to build your Random Forest or Extra Trees classifier. More effective versions of these two classifiers are already built into Scikit-learn.", "Ensemble methods are not limited to having weak learners as their base estimator. For example, you can determine the best three classifiers for a given task and use Scikit-learn Voting Classifier to form an ensemble with these. You can optimize the weights of the ensemble and make predictions with them. Assuming that you have finely tuned your classifiers and all of them have similar performances, the Voting Classifier would gain an edge over any of them. To narrow your search of which classifiers to use, you can read this article that talks about model design and selection using Scikit-learn.", "You can find me on LinkedIn or visit my personal blog.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F8507ac21d54b&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-intuitive-explanation-of-random-forest-and-extra-trees-classifiers-8507ac21d54b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-intuitive-explanation-of-random-forest-and-extra-trees-classifiers-8507ac21d54b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-intuitive-explanation-of-random-forest-and-extra-trees-classifiers-8507ac21d54b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-intuitive-explanation-of-random-forest-and-extra-trees-classifiers-8507ac21d54b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----8507ac21d54b--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8507ac21d54b--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://frank-ceballos.medium.com/?source=post_page-----8507ac21d54b--------------------------------", "anchor_text": ""}, {"url": "https://frank-ceballos.medium.com/?source=post_page-----8507ac21d54b--------------------------------", "anchor_text": "Frank Ceballos"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F176ad7e37afe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-intuitive-explanation-of-random-forest-and-extra-trees-classifiers-8507ac21d54b&user=Frank+Ceballos&userId=176ad7e37afe&source=post_page-176ad7e37afe----8507ac21d54b---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8507ac21d54b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-intuitive-explanation-of-random-forest-and-extra-trees-classifiers-8507ac21d54b&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8507ac21d54b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-intuitive-explanation-of-random-forest-and-extra-trees-classifiers-8507ac21d54b&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@aperturevintage?utm_source=medium&utm_medium=referral", "anchor_text": "Aperture Vintage"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html", "anchor_text": "Random Forest"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html", "anchor_text": "Extra Trees"}, {"url": "https://towardsdatascience.com/i-want-to-be-the-very-best/installing-keras-tensorflow-using-anaconda-for-machine-learning-44ab28ff39cb", "anchor_text": "article"}, {"url": "https://towardsdatascience.com/scikit-learn-decision-trees-explained-803f3812290d", "anchor_text": "here"}, {"url": "https://en.wikipedia.org/wiki/Decision_stump", "anchor_text": "decision stump"}, {"url": "https://stats.stackexchange.com/questions/320156/hard-voting-versus-soft-voting-in-ensemble-based-methods", "anchor_text": "hard votes"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html", "anchor_text": "BaggingClassifier"}, {"url": "https://towardsdatascience.com/scikit-learn-decision-trees-explained-803f3812290d", "anchor_text": "read this article"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html", "anchor_text": "Voting Classifier"}, {"url": "https://medium.com/i-want-to-be-the-very-best/model-design-and-selection-with-scikit-learn-18a29041d02a?source=post_stats_page---------------------------", "anchor_text": "model design and selection using Scikit-learn"}, {"url": "https://www.linkedin.com/in/frank-ceballos/", "anchor_text": "LinkedIn"}, {"url": "https://www.frank-ceballos.com/", "anchor_text": "personal blog"}, {"url": "https://www.frank-ceballos.com/", "anchor_text": "Frank CeballosGraphswww.frank-ceballos.com"}, {"url": "https://www.linkedin.com/in/frank-ceballos/", "anchor_text": "Frank Ceballos - Postdoctoral Fellow - Medical College of Wisconsin | LinkedInI'm a postdoctoral fellow in the Medical College of Wisconsin where I analyze high-dimensional complex clinical data\u2026www.linkedin.com"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----8507ac21d54b---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/python?source=post_page-----8507ac21d54b---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/data-science?source=post_page-----8507ac21d54b---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/random-forest?source=post_page-----8507ac21d54b---------------random_forest-----------------", "anchor_text": "Random Forest"}, {"url": "https://medium.com/tag/decision-tree?source=post_page-----8507ac21d54b---------------decision_tree-----------------", "anchor_text": "Decision Tree"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8507ac21d54b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-intuitive-explanation-of-random-forest-and-extra-trees-classifiers-8507ac21d54b&user=Frank+Ceballos&userId=176ad7e37afe&source=-----8507ac21d54b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8507ac21d54b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-intuitive-explanation-of-random-forest-and-extra-trees-classifiers-8507ac21d54b&user=Frank+Ceballos&userId=176ad7e37afe&source=-----8507ac21d54b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8507ac21d54b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-intuitive-explanation-of-random-forest-and-extra-trees-classifiers-8507ac21d54b&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8507ac21d54b--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F8507ac21d54b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-intuitive-explanation-of-random-forest-and-extra-trees-classifiers-8507ac21d54b&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----8507ac21d54b---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----8507ac21d54b--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----8507ac21d54b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----8507ac21d54b--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----8507ac21d54b--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----8507ac21d54b--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----8507ac21d54b--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----8507ac21d54b--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----8507ac21d54b--------------------------------", "anchor_text": ""}, {"url": "https://frank-ceballos.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://frank-ceballos.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Frank Ceballos"}, {"url": "https://frank-ceballos.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "856 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F176ad7e37afe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-intuitive-explanation-of-random-forest-and-extra-trees-classifiers-8507ac21d54b&user=Frank+Ceballos&userId=176ad7e37afe&source=post_page-176ad7e37afe--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Ff3961b72dd61&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-intuitive-explanation-of-random-forest-and-extra-trees-classifiers-8507ac21d54b&newsletterV3=176ad7e37afe&newsletterV3Id=f3961b72dd61&user=Frank+Ceballos&userId=176ad7e37afe&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}