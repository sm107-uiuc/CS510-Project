{"url": "https://towardsdatascience.com/cleansing-and-transforming-schema-drifted-csv-files-into-relational-data-in-azure-databricks-519e82ea84ff", "time": 1683014355.218333, "path": "towardsdatascience.com/cleansing-and-transforming-schema-drifted-csv-files-into-relational-data-in-azure-databricks-519e82ea84ff/", "webpage": {"metadata": {"title": "Cleansing and transforming schema drifted CSV files into relational data in Azure Databricks | by Dhyanendra Singh Rathore | Towards Data Science", "h1": "Cleansing and transforming schema drifted CSV files into relational data in Azure Databricks", "description": "Using PySpark to incrementally processing and loading schema drifted CSV files to Azure Synapse Analytics data warehouse in Azure Databricks."}, "outgoing_paragraph_urls": [{"url": "https://portal.azure.com/", "anchor_text": "Azure Portal", "paragraph_index": 4}, {"url": "https://docs.databricks.com/notebooks/index.html", "anchor_text": "notebooks", "paragraph_index": 8}, {"url": "https://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data#daily-reports-csse_covid_19_daily_reports", "anchor_text": "GitHub Source", "paragraph_index": 17}, {"url": "https://www.linkedin.com/in/dhyans/", "anchor_text": "LinkedIn", "paragraph_index": 32}, {"url": "https://dhyanintech.medium.com/membership", "anchor_text": "Medium", "paragraph_index": 32}], "all_paragraphs": ["Data is the blood of a business. Data comes in varying shapes and sizes, making it a constant challenging task to find the means of processing and consumption, without which it holds no value whatsoever.", "This article looks at how to leverage Apache Spark\u2019s parallel analytics capabilities to iteratively cleanse and transform schema drifted CSV files into queryable relational data to store in a data warehouse. We will work in a Spark environment and write code in PySpark to achieve our transformation goal.", "Caution: Microsoft Azure is a paid service, and following this article can cause financial liability to you or your organization.", "If you don\u2019t have prerequisites set up yet, refer to our previous articles to get started:", "Sign in to the Azure Portal, locate and open your Azure Databricks instance and click on \u2018Launch Workspace.\u2019 Our Databricks instance will open up in a new browser tab; wait for Azure AD SSO to sign you in automatically.", "Next, we need to create a cluster of nodes to leverage Apache Spark\u2019s unparalleled parallel processing (pun intended) capabilities to process, cleanse, and transform our semi-structured data.", "Select Clusters on the left menu to begin creating a new cluster. Start by selecting + Create Cluster and proceed as shown. Two essential things to pay attention to here are the Databricks runtime version and the minimum and the maximum number of worker nodes. Our cluster will scale automatically between these nodes to accommodate the load. Wait for the creation process to finish.", "Click on Start to start your cluster. It might take a few minutes for Azure to provision and set up your cluster resources. Keep an eye on the cluster status indicator to see the real-time status.", "The real magic of Databricks takes place in notebooks. Azure Databricks supports notebooks written in Python, Scala, SQL, and R. In our project, we will use Python and PySpark to code all the transformation and cleansing activities. Let\u2019s get spinning by creating a Python notebook.", "A notebook is a web-based interface to a document that contains runnable code, narrative text, and visualizations.", "PySpark is a Python API for Apache Spark. Apache Spark is written in Scala. PySpark has been released to support the collaboration of Apache Spark and Python.", "Select the Workspace in the left menu and follow the steps as shown. Your notebook will open up after creation; take a minute to look around to familiarize yourself with the UI and various options available for us.", "The first few lines in a notebook should tell Databricks where our data is and how to access it. We will mount our storage account to the Databricks filesystem and access it as local storage.", "Please head over to our article for detailed steps on mounting and accessing ADLS Gen2 storage in Azure Databricks. We will keep it short.", "Our end goal is to load the data into a data warehouse to derive insights from the data and build reports to make decisions. Let\u2019s set up the connectivity before proceeding.", "Our connections are all set; let\u2019s get on with cleansing the CSV files we just mounted. We will briefly explain the purpose of statements and, in the end, present the entire code.", "First off, let\u2019s read a file into PySpark and determine the schema. We will set some options to tell PySpark about the type and structure of the columns.", "This file has lesser columns than we expected from our GitHub Source and column names differ as well. We\u2019re interested in highlighted columns for our final Power BI visualization.", "Let\u2019s read a newer file and check the structure.", "This file\u2019s structure is closer to our source\u2019s description. The difference in schema doesn\u2019t make things easy for us. If all our files have the same schema, we can load and cleanse all the files at once. Ours is a classic case of schema drift, and we must handle it appropriately; otherwise, our ELT (Extract, Load, and Transform) process will fail. We will design our transformation to account for this drift and make it unerring to schema changes.", "Schema drift is the case where a source often changes metadata. Fields, columns, and, types are subject to change, addition, or removal.", "We will start cleansing by renaming the columns to match our table's attributes in the database to have a one-to-one mapping between our table and the data. We will achieve this by converting all letters to lowercase and removing space, forwards slash (\u2018/\u2019), and underscore (\u2018_\u2019).", "Our column names look much better now. We will add a few new columns to deal with our missing column situation; active, longitude, latitude, and sourcefile. We will use the file name as the value for the sourcefile column. This column will be useful to set up the incremental load of our data into our database.", "First, we will rename lat and long column names to latitude and longitude if they exist in the data. Next, we will use lit()from PySpark to add missing active, latitude, and longitude columns with null values and sourcefile with the file name as the column value.", "Let\u2019s take a look at the data from the two files we viewed at the beginning of our cleansing activity using display(DATAFRAME)", "Both files now give us formatted data in a fixed desired structure and are ready to be inserted in our database. We have dealt with our drifted schema successfully.", "So far, we ran our code for two files manually; we should automate this to process files one after the other. We can use Databricks file system utilities to iterate through all the files.", "Further reading on Databricks file system utilities:", "We only need to process the files that haven\u2019t been loaded to our database yet (an incremental load). We can find out the name of the last file we loaded by querying the database and tweak our iterator code to ignore the files we have already loaded.", "Combining and restructuring all the code we\u2019ve written so far will allow us to cleanse our schema drifted files with an incremental load to our database. Give it a try. You can find the entire notebook from GitHub at the end of the article for any troubleshooting purposes.", "We looked at our CSV files and realized that they have different schemas and need divergent processing methods before we can load them into our data warehouse. We used PySpark to make a creative solution to process our files incrementally and designed a solution to fit our needs.", "If you\u2019re following our series on turning CSV data into Power BI visuals or are interested in learning how to add and execute Databricks notebook in your Data Factory pipeline, please head to our next article to continue the journey.", "Let\u2019s be friends! You can find me on LinkedIn or join me on Medium.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Analytics Expert. Data and BI Professional. Owner of Everyday BI. Private consultation - dhyan.singh@everydaybi.com"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F519e82ea84ff&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcleansing-and-transforming-schema-drifted-csv-files-into-relational-data-in-azure-databricks-519e82ea84ff&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcleansing-and-transforming-schema-drifted-csv-files-into-relational-data-in-azure-databricks-519e82ea84ff&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcleansing-and-transforming-schema-drifted-csv-files-into-relational-data-in-azure-databricks-519e82ea84ff&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcleansing-and-transforming-schema-drifted-csv-files-into-relational-data-in-azure-databricks-519e82ea84ff&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----519e82ea84ff--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----519e82ea84ff--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://dhyanintech.medium.com/?source=post_page-----519e82ea84ff--------------------------------", "anchor_text": ""}, {"url": "https://dhyanintech.medium.com/?source=post_page-----519e82ea84ff--------------------------------", "anchor_text": "Dhyanendra Singh Rathore"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb038ca7c0471&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcleansing-and-transforming-schema-drifted-csv-files-into-relational-data-in-azure-databricks-519e82ea84ff&user=Dhyanendra+Singh+Rathore&userId=b038ca7c0471&source=post_page-b038ca7c0471----519e82ea84ff---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F519e82ea84ff&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcleansing-and-transforming-schema-drifted-csv-files-into-relational-data-in-azure-databricks-519e82ea84ff&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F519e82ea84ff&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcleansing-and-transforming-schema-drifted-csv-files-into-relational-data-in-azure-databricks-519e82ea84ff&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@grimstad", "anchor_text": "H\u00e5kon Grimstad"}, {"url": "https://unsplash.com/?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://dhyanintech.medium.com/disclaimer-disclosure-terms-of-use-fb3bfbd1e0e5", "anchor_text": "https://dhyanintech.medium.com/disclaimer-disclosure-terms-of-use-fb3bfbd1e0e5"}, {"url": "https://medium.com/@dhyanintech/a-definitive-guide-to-turn-csv-files-into-power-bi-visuals-using-azure-4483cf406eab", "anchor_text": "A definitive guide to turn CSV files into Power BI visuals using AzureA step-by-step guide to turning COVID-19 data into stunning Power BI visuals using Microsoft Azure offerings.medium.com"}, {"url": "https://medium.com/@dhyanintech/using-azure-data-factory-to-incrementally-copy-files-based-on-url-pattern-over-http-569476b625fc", "anchor_text": "Using Azure Data Factory to incrementally copy files based on URL pattern over HTTPAn innovative Azure Data Factory pipeline to copy multiple files, incrementally, over HTTP from a third-party web\u2026medium.com"}, {"url": "https://portal.azure.com/", "anchor_text": "Azure Portal"}, {"url": "https://spark.apache.org/", "anchor_text": "Apache Spark\u2122 \u2014 Unified Analytics Engine for Big DataLightning-fast unified analytics engine Apache Spark\u2122 is a unified analytics engine for large-scale data processing\u2026spark.apache.org"}, {"url": "https://docs.databricks.com/notebooks/index.html", "anchor_text": "notebooks"}, {"url": "https://medium.com/@dhyanintech/mounting-accessing-adls-gen2-in-azure-databricks-using-service-principal-and-secret-scopes-96e5c3d6008b", "anchor_text": "Mounting & accessing ADLS Gen2 in Azure Databricks using Service Principal and Secret ScopesA guide on accessing Azure Data Lake Storage Gen2 from Databricks with Azure Key Vault-backed Secret Scopes and Service\u2026medium.com"}, {"url": "https://medium.com/@dhyanintech/a-credential-safe-way-to-connect-and-access-azure-synapse-analytics-in-azure-databricks-1b008839590a", "anchor_text": "A credential-safe way to connect and access Azure Synapse Analytics in Azure DatabricksA guide on how to setup SQL Server firewall and connect from Databricks using Secret Scopes in PySparkmedium.com"}, {"url": "https://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data#daily-reports-csse_covid_19_daily_reports", "anchor_text": "GitHub Source"}, {"url": "https://docs.databricks.com/dev-tools/databricks-utils.html#dbutilsfsls-command", "anchor_text": "Databricks UtilitiesDatabricks Utilities (DBUtils) make it easy to perform powerful combinations of tasks. You can use the utilities to\u2026docs.databricks.com"}, {"url": "https://medium.com/@dhyanintech/executing-azure-databricks-notebook-in-azure-data-factory-pipeline-using-access-tokens-3326b8703432", "anchor_text": "Executing Azure Databricks notebook in Azure Data Factory pipeline using Access TokensA guide on how to add and execute a Databricks notebook in Data Factory pipeline with Azure Key Vault safe Access\u2026medium.com"}, {"url": "https://www.linkedin.com/in/dhyans/", "anchor_text": "LinkedIn"}, {"url": "https://dhyanintech.medium.com/membership", "anchor_text": "Medium"}, {"url": "https://medium.com/tag/pyspark?source=post_page-----519e82ea84ff---------------pyspark-----------------", "anchor_text": "Pyspark"}, {"url": "https://medium.com/tag/schema-drift?source=post_page-----519e82ea84ff---------------schema_drift-----------------", "anchor_text": "Schema Drift"}, {"url": "https://medium.com/tag/azure-databricks?source=post_page-----519e82ea84ff---------------azure_databricks-----------------", "anchor_text": "Azure Databricks"}, {"url": "https://medium.com/tag/azure-synapse-analytics?source=post_page-----519e82ea84ff---------------azure_synapse_analytics-----------------", "anchor_text": "Azure Synapse Analytics"}, {"url": "https://medium.com/tag/data-cleansing?source=post_page-----519e82ea84ff---------------data_cleansing-----------------", "anchor_text": "Data Cleansing"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F519e82ea84ff&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcleansing-and-transforming-schema-drifted-csv-files-into-relational-data-in-azure-databricks-519e82ea84ff&user=Dhyanendra+Singh+Rathore&userId=b038ca7c0471&source=-----519e82ea84ff---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F519e82ea84ff&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcleansing-and-transforming-schema-drifted-csv-files-into-relational-data-in-azure-databricks-519e82ea84ff&user=Dhyanendra+Singh+Rathore&userId=b038ca7c0471&source=-----519e82ea84ff---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F519e82ea84ff&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcleansing-and-transforming-schema-drifted-csv-files-into-relational-data-in-azure-databricks-519e82ea84ff&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----519e82ea84ff--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F519e82ea84ff&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcleansing-and-transforming-schema-drifted-csv-files-into-relational-data-in-azure-databricks-519e82ea84ff&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----519e82ea84ff---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----519e82ea84ff--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----519e82ea84ff--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----519e82ea84ff--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----519e82ea84ff--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----519e82ea84ff--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----519e82ea84ff--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----519e82ea84ff--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----519e82ea84ff--------------------------------", "anchor_text": ""}, {"url": "https://dhyanintech.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://dhyanintech.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Dhyanendra Singh Rathore"}, {"url": "https://dhyanintech.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "258 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb038ca7c0471&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcleansing-and-transforming-schema-drifted-csv-files-into-relational-data-in-azure-databricks-519e82ea84ff&user=Dhyanendra+Singh+Rathore&userId=b038ca7c0471&source=post_page-b038ca7c0471--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fbd8cfa4cf694&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcleansing-and-transforming-schema-drifted-csv-files-into-relational-data-in-azure-databricks-519e82ea84ff&newsletterV3=b038ca7c0471&newsletterV3Id=bd8cfa4cf694&user=Dhyanendra+Singh+Rathore&userId=b038ca7c0471&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}