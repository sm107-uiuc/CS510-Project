{"url": "https://towardsdatascience.com/support-vector-machine-803884d967e3", "time": 1683011427.261882, "path": "towardsdatascience.com/support-vector-machine-803884d967e3/", "webpage": {"metadata": {"title": "Support Vector Machine. A dive into the math behind the SVM\u2026 | by Victor Varaschin | Towards Data Science", "h1": "Support Vector Machine", "description": "In this post, we\u2019ll discuss the use of support vector machines (SVM) as a classification model. We will start by exploring the idea behind it, translate this idea into a mathematical problem and use\u2026"}, "outgoing_paragraph_urls": [{"url": "https://github.com/victorvrv/blog-resources/blob/master/2-SVM/models.ipynb", "anchor_text": "repo", "paragraph_index": 41}, {"url": "https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html", "anchor_text": "SciPy\u2019s minimize", "paragraph_index": 41}, {"url": "https://www.csie.ntu.edu.tw/~cjlin/libsvm/", "anchor_text": "libsvm", "paragraph_index": 51}, {"url": "https://cvxopt.org/userguide/coneprog.html#quadratic-programming", "anchor_text": "cvxopt", "paragraph_index": 53}, {"url": "https://github.com/victorvrv/blog-resources/blob/master/2-SVM/models.ipynb", "anchor_text": "repo", "paragraph_index": 59}, {"url": "https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions", "anchor_text": "conditions", "paragraph_index": 74}, {"url": "https://www.csie.ntu.edu.tw/~cjlin/libsvm/", "anchor_text": "libsvm", "paragraph_index": 90}, {"url": "https://www.csie.ntu.edu.tw/~cjlin/papers/quadworkset.pdf", "anchor_text": "paper", "paragraph_index": 90}, {"url": "https://en.wikipedia.org/wiki/Mercer%27s_theorem#Mercer's_condition", "anchor_text": "Mercer\u2019s conditions", "paragraph_index": 105}, {"url": "https://gist.github.com/mblondel/586753", "anchor_text": "this repo", "paragraph_index": 107}, {"url": "https://github.com/victorvrv/blog-resources/blob/master/2-SVM/models.ipynb", "anchor_text": "here", "paragraph_index": 108}, {"url": "https://www.springer.com/gp/book/9780387310732", "anchor_text": "link", "paragraph_index": 109}, {"url": "https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/", "anchor_text": "link", "paragraph_index": 110}, {"url": "http://learningml.dev", "anchor_text": "learningml.dev", "paragraph_index": 112}], "all_paragraphs": ["In this post, we\u2019ll discuss the use of support vector machines (SVM) as a classification model. We will start by exploring the idea behind it, translate this idea into a mathematical problem and use quadratic programming (QP) to solve it.", "Let\u2019s start by analyzing the intuition behind the model. A linear classification algorithm\u2019s goal is to divide the input space in regions labeled by the different classes. That way, we can predict the class of any new data point based on which region it belongs to. Different statistical models diverge in how they find this decision boundary and in this post we are gonna talk about the strategy used by the SVM.", "Below we have a dataset with two classes. Here we show two decision boundaries represented by line 1 and 2. It is easy to see that there are many other lines that can divide the input space in a way that all observations are correctly classified \u2014 in fact, there are an infinite number of lines.", "So how does the SVM choose its line? There are two main ideas. Notice how point A is very far from line 1. It seems intuitive to conclude that, based on the decision boundary defined by line 1, we are more confident in predicting that point A belongs to the circle class than to say the same about point B. The SVM aims to maximize the confidence of its predictions within the training set. Also notice that there are many points very close to line 2: this means that any tweak to the decision boundary\u2019s parameters could result in incorrectly predicting the classes of some of these points. These two ideas combined form the intuition behind the SVM.", "Note: throughout this post, you\u2019ll see the word hyperplane being used. A hyperplane is just a fancy name for a subset with one less dimension than its ambient space, which is the case for the SVM decision boundary. That means that if the input space is in \u211d\u1d56, the decision boundary has (p \u2014 1) dimensions. For example, in the image below, the input space is in \u211d\u00b2, so the SVM decision boundary is one dimensional: a line.", "The SVM is a linear classification model. For an output y \u2208 {-1, 1}, we can write the hypothesis function as a linear combination of the inputs:", "It seems intuitive that the further away the hypothesis value is from zero, the more confident we are in our predictions. If h(x) = 10, we are more confident in predicting y=1 than when h(x) = 0.1.", "That could lead to the idea that we want to find the parameters (w, b) that will maximize the values of h when y\u207d\u2071\u207e = 1, and minimize the values of h when y\u207d\u2071\u207e = -1. In other words, we might feel inclined to maximize the functional margin \u03b3\u0302:", "But there is a problem with that: notice how the predicted class depends only on the sign of h. That means that we can scale the parameters, for example (w, b) \u2192 (10w, 10b), without changing the predicted classes. This would scale the values of h by a factor of 10 and give the false idea that our model is 10 times more confident in its predictions.", "This is where the concept of the geometric margin comes in. The geometric margin \u03b3\u0302 is defined as the distance of the i-th observation to the decision boundary. Unlike the functional margin, this measure is invariant to the scaling of parameters. After all, the hyperplane defined by w\u1d40x + b = 0 is exactly the same as the one defined by 10w\u1d40x + 10b = 0.", "The SVM looks for the hyperplane that is as far as possible from the closest member of each class. Looking at the figure below, we can see that P\u2081 and P\u2082 are the closest observations from each class. The SVM decision boundary is equidistant to P\u2081 and P\u2082, that is, d\u2081 = d\u2082. Last and more importantly, of all decision boundaries that correctly classify every observation in the training set, the SVM is the one with the greatest minimum distance min(d\u2081, d\u2082).", "In other words, if we define \u03b3 = min \u03b3\u207d\u2071\u207e, the SVM looks for the hyperplane with the largest value of \u03b3.", "The SVM aims to maximize the minimum geometric margin.", "Before moving on, we need to talk about one very important thing when dealing with a SVM: feature scaling. Since the SVM optimization goal is based on geometric distances, axis with very different scaling makes the model favor the direction with the largest values.", "For example, on the image below, we can see that before scaling the features, the SVM looks for a decision boundary such that the distance vector d\u2081 has the greatest vertical component as possible. This is why we should always apply feature scaling before fitting a SVM.", "Always scale the features before fitting an SVM", "As we said before, the SVM is a linear model \u2014 that means it is good at finding linear relationships in the data. But as you imagine, most real world datasets are not linear. And while we can\u2019t change the nature of the SVM itself, we can change the inputs that we feed to the model. Let\u2019s see why and how this would help.", "The image below is a good illustration of how changing the inputs can help the SVM model non-linear relationships. On the left, the data is clearly not linearly separable, that is, we can\u2019t find a point that would have one class to its right, and the other class to its left.", "But let\u2019s see what happens when we add one extra feature to this dataset: namely, the distance of each observation to the point (0). That means that a point defined by (-3) on the original input space is now represented by the pair (-3, 3); similarly, a point represented by (5) becomes (5, 5). We are remapping the input space from one dimension to two dimensions. And as you can see on the image to the right, this powerful and simple trick makes the data linearly separable in \u211d\u00b2 and, consequentially, a better fit for the SVM.", "We can circumvent the linear limitations of the model by applying non-linear transformations to the original features.", "One common way to implement this idea is by adding higher degree polynomials to the training set. The image below shows a dataset with two features (x\u2081, x\u2082) that is clearly not linearly separable in \u211d\u00b2.", "We need to transform the inputs in a way that the data becomes linearly separable (don\u2019t forget to scale it afterwards!). We will add polynomial features up to a third degree, remapping the input space from \u211d\u00b2 to \u211d\u00b9\u2070, such that (x\u2081, x\u2082) becomes (1, x\u2081, x\u2082, x\u2081x\u2082, x\u2081\u00b2, x\u2082\u00b2, x\u2081\u00b2x\u2082, x\u2082\u00b2x\u2081, x\u2081\u00b3, x\u2082\u00b3). Unfortunately we can\u2019t see the cool 10-dimensional plot, so we settle down for the 2D representation with the original features. Note that to make predictions about new data points, we need to apply the same polynomial transformation before feeding it to the SVM.", "Another way to work around the linear limitations of the SVM is by using similarity measures, which are simply functions that quantify the similarity between two data points. One of the most common similarity functions is the Gaussian Radial Basis Function (RBF), which is defined as:", "We can choose a few landmark points and remap the inputs to a vector containing the distance to each of the landmarks. So if we have three landmarks points (l\u2081, l\u2082, l\u2083) and we are using the RBF similarity function \u03d5, an observation originally represented by x becomes (\u03d5(x, l\u2081), \u03d5(x,l\u2082), \u03d5(x, l\u2083)).", "But how can we pick the data points to be used as landmarks? One strategy is to just pick every point in the training set. This obviously doesn\u2019t scale very well since we create an extra feature for every new observation, such that a training set with 500 observations and three features becomes a training set with 500 observations and 500 features (assuming we drop the original features).", "Later on this post, we\u2019ll introduce a way to implement this idea in a much more elegant and efficient way. For now, we can see below a simpler implementation of this concept of using every observation in the training set as a landmark and how that affects the SVM.", "As we said before, the SVM is a linear classification model, and we can write its hypothesis function as a linear combination of the inputs:", "Now it is time to translate the SVM goal of finding the largest geometric margin into a mathematical problem. Given a point x\u207d\u2071\u207e, what is its distance to the decision boundary defined by w\u1d40x + b = 0? Well, we know that w is the normal vector of this hyperplane, which means that it is perpendicular to the plane. And if w is the normal vector, then w/||w|| is a unit vector perpendicular to the plane.", "Now let \u03b3 be the distance from x to the hyperplane \u2014 we can then say that x - \u03b3 * (w/||w||) lands on the hyperplane (represented by p in the image above). Last since p is a vector on the hyperplane, this means that it must satisfy the hyperplane\u2019s equation w\u1d40p + b = 0. That gives us the system of equations below:", "And if we solve it for \u03b3, we find that the distance of a point x\u207d\u2071\u207e to the SVM decision boundary is given by \u03b3\u207d\u2071\u207e = (w\u1d40x\u207d\u2071\u207e + b)/ ||w||. Notice how this equation is only valid for points in the same direction as the normal vector w since we subtracted it from x; otherwise we would have to add \u03b3 w /w||. With a little tweak, we can write the generalized geometric margin \u03b3 as:", "We can again verify that, as we said before, the geometric margin is invariant to the scaling of parameters. Also notice that if ||w||= 1, then the geometric margin and the functional margin are the same.", "The hard margin SVM works on the assumption that the data is linearly separable, and it forces the model to correctly classify every observation on the training set. The i-th observation is correctly classified if its functional margin is greater than zero:", "Now remember how we said that the predicted classes depend only on the sign of h and that the geometric margins are invariant to the scaling of parameters? That means that we can add arbitrary constraints to the parameters without affecting the model. For any fitted SVM, we can impose a constraint like ||w|| = 1, then all we need to do is rescale its parameters and the model remains unchanged.", "We can use this result along with the fact that the geometric and the functional margins are equal when ||w|| = 1 to write the optimization objective of the hard margin SVM:", "The first constraint forces every observation to be correctly classified. The second constraint forces \u03b3 to not only be a lower bound for the functional margin, but also for the geometric margin. This is precisely the hard margin SVM optimization objective:", "to maximize the minimum geometric margin without any misclassifications", "If we had tools to solve this constrained optimization problem, we could stop here. But unfortunately, the ||w|| = 1 is a non-convex constraint, so we will need to make some changes to get this problem into a more friendly format.", "We can get rid of the nasty constraint by dividing the objective by the norm \u2014 if \u03b3 is a lower bound for the functional margin, then \u03b3/||w|| is a lower bound for the geometric margin. So our new problem can be written as:", "Now it is the objective function that is non-convex, but we are one step closer. Remember one last time that we can add arbitrary constraints to the parameters. So we can impose \u03b3 = 1 and again, this does not change the model and it can be satisfied by simply rescaling the parameters.", "Our new optimization objective is then to maximize 1/||w||, which is equivalent to minimize ||w||. Last, since||w|| is not differentiable at 0, we\u2019ll minimize (1/2)*||w||\u00b2 instead, whose derivative is just w. Optimization algorithms work much better on differentiable functions.", "Finally, we define the hard margin SVM optimization objective as:", "Now that we have the SVM optimization goal defined in a way that we can efficiently estimate it (a quadratic objective and linear constraints), we can write code to solve it. Read through the code below (or in this repo) or try to implement it yourself using SciPy\u2019s minimize function. If you read the documentation, you\u2019ll see that the objective function to be minimized takes a list as the argument, so we\u2019ll put both w and b in the same vector. It also takes a _constraints_ argument, which can represent an equality (the constraint function must return 0) or an inequality (the constraint function must return non-negative values). We\u2019ll rewrite our generalized constraint as y\u207d\u2071\u207e(w\u1d40x\u207d\u2071\u207e + b) \u2212 1 \u2265 0 to fit the _SciPy_\u2019s spec.", "Now we can see how our model perform on a side-by-side comparison with _SKLearn_\u2019s implementation. Not too bad!", "The hard margin SVM has two very important limitations:", "- it only works on linearly separable data;", "- it is very sensible to outliers.", "If we want more flexibility, we need to introduce a way for the model to allow for misclassifications, and we do that using the concept of slack variables. Every observation has its own slack measure, which represents how much that particular data point can violate the margin. Our new constraint can be rewritten as: y\u207d\u2071\u207e(w\u1d40x\u207d\u2071\u207e + b) \u2265 1 - \u03f5\u207d\u2071\u207e. Notice that we now need to estimate one extra variable for every observation.", "There is just one problem now. If we are still trying to minimize ||w||, we can simply let \u03f5 \u2192 \u221e and be sure that the new constraints will be respected. We need the new objective function to represent the soft margin SVM conflicting objectives: we still want the margins to be as wide as possible, and we want the slack variables to be as small as possible to prevent margins\u2019 violations. This gives us the soft margin SVM classifier objective:", "We introduced a new hyperparameter C that lets us control the tradeoff between the conflicting objectives. As C grows, it forces the optimization algorithm to find smaller values for \u03f5. Below you can see how different values of C affect the SVM.", "Before we code our own implementation of the soft margin SVM, we need to introduce a new concept.", "Quadratic programming (QP) is a system to solve a particular type of optimization problems where the objective function is quadratic, and the constraints are linear \u2014 precisely the type of problems we have been dealing with!", "Exactly how different algorithms solve QP optimizations is beyond the scope of this post. We instead are gonna focus in deriving the SVM optimization problem that is solved by libraries like libsvm.", "Given the QP problem defined as:", "All we need to do is figure out what values of P, q, G, h that represent the SVM problem, and we can use a solver (we\u2019ll use cvxopt later on) to get the parameters\u2019 estimates.", "Notice that we will need to rewrite the SVM constraints to fit the QP constraints\u2019 shape. They will be:", "So for the soft margin SVM, let X be the training set represented by the m\u00d7(n+1) matrix (m observations and n features) left-padded with a 1 column-vector for the intercept. Then the SVM QP problem is defined with:", "- the P matrix uses the identity to get w back and zeros to cancel b and \u03f5;", "- the q vector multiplies b and w by 0, and multiplies \u03f5 by C, so that we have the second part of the optimization objective C \u2211\u1d62\u208c\u2081\u1d50 \u03b5\u207d\u2071\u207e;", "- last the h vector has -1 and 0 just like the constraints (\u2264 -1 and \u2264 0).", "Now all we have to do is give these values to a QP software and we have the parameters of a soft margin SVM. You can check the code to implement this idea below (or in this repo). We made a lot of progress already, but we are not done yet.", "Remember we talked about similarity measures earlier on this post as a way to work around the linear limitations of the SVM? It turns out there is a much more elegant and efficient solution to this problem than explicitly making every observation a landmark and transforming the inputs.", "We\u2019ll next talk about Lagrange duality. This will lead us to a different representation of the soft margin SVM optimization problem (called its dual form). We will be able to apply non-linear transformations over the input space in a much more efficient way, allowing the SVM to work well even in very high dimensions.", "The general idea of the Lagrange method is to transform a constrained optimization problem (primal form) into an unconstrained one (dual form), by moving the constraints into the objective function. There are two main reasons for writing the SVM optimization problem in its dual form:", "- kernel trick: the training and predictions of the SVM will depend on the data points only through their inner product. This powerful result allows us to apply any transformations on the training set (even remapping it to an infinite-dimensional space!) as long as we can calculate that inner product;", "- support vectors: the predictions will depend only on a few points on the training set called support vectors. The dual form gives us an easy way to identify these points.", "Hopefully, these reasons will become clear by the end of this post. For now, let\u2019s leave the SVM aside for a moment and think about solving constrained optimization problems like the one below.", "For simplicity, we will analyze a problem with only inequality constraints (like the SVM), but the results could be easily extended to equalities. One way to rewrite this problem is by using an infinite step function g(u):", "We can then rewrite the original problem as:", "Notice that we penalize J(x) with a positive infinity term if any of the original constraints are not satisfied. And when all the constraints are met, then g(f\u1d62(x)) = 0 \u2200 i and J(x) = f\u2080(x). So minimizing J(x) is equivalent to the original problem, but we can\u2019t really solve this new optimization yet \u2014 the step function is neither continuous nor differentiable.", "What if instead of the infinite step function, we used a linear function like g(u) = \u03b1 u? If we enforce \u03b1 \u2265 0, then the penalty is, at least, in the right direction when a constraint is not met (\u03b1 u > 0). Furthermore, g is now continuously differentiable: a much better fit for optimization problems.", "So if we substitute our new linear penalty in the J(x) equation above, we get a function of x and \u03b1 that is known as the Lagrangian:", "Notice that if we maximize the Lagrangian with respect to \u03b1, we get J(x) back: \u03b1\u1d62 = 0 if f\u1d62(x) < 0 (since \u03b1\u1d62 \u2264 0) and \u03b1\u1d62 \u2192 \u221e if f\u1d62(x) > 0. So the original problem can now be written as a function of the Lagrangian:", "Last, since the min max f \u2265 max min f for any function f, we have that:", "The min\u2093 L(x, \u03b1) is called the dual function and its maximum is a lower bound for the original (primal) optimization problem. Moreover, it can be shown that, under certain conditions:", "so that we can solve the dual problem as opposed to the primal problem. Luckily the SVM satisfy these conditions (specifically the Slater\u2019s condition): the objective and the constraint functions are convex and continuously differentiable.", "Since the SVM satisfy the regularity conditions, if there is a solution for the primal problem, it will necessarily be among the stationary points (x*, \u03b1*) of the Lagrangian that respect the Karush\u2013Kuhn\u2013Tucker (KKT) conditions. Furthermore, in the case of the SVM (convex differentiable), the KKT conditions are not just necessary, but also sufficient for optimality.", "What are the KKT conditions? If the primal problem has a solution x*, and \u03b1* is a solution for the dual problem, such that L(x*, \u03b1*) = f\u2080(x*). Then the KKT conditions must be met:", "Below we have the soft margin SVM optimization problem:", "If we rewrite both constraints as:", "The Lagrangian can be written as:", "Remember that since the SVM optimization problem satisfy some special conditions (objective and constraints functions are convex and continuously differentiable), any solution to the Lagrangian is a solution to the original optimization and it must satisfy the KKT conditions. From the KKT we have:", "From the dual feasibility condition, we have:", "So based on the partial derivative on b, on the gradient over w, and on the dual feasibility condition, we derive the constraints for the dual problem:", "We are almost there! Back to the Lagrangian, let\u2019s use the result we got from the w gradient (w = \u2211\u1d62\u208c\u2081\u1d50 \u03b1\u207d\u2071\u207e y\u207d\u2071\u207e x\u207d\u2071\u207e) and substitute it on the last term of the Lagrangian sum:", "then plugging everything back into the Lagrangian we have:", "Last, since \u2211 \u03b1\u207d\u2071\u207e y\u207d\u2071\u207e = C \u2212 \u03bb\u207d\u2071\u207e - \u03b1\u207d\u2071\u207e = 0, we finally have the dual form of the soft margin SVM (after multiplying by -1 and turning the maximization into a minimization problem):", "Now we just need to get back the vector of weights w and the bias term b from the Lagrange multipliers \u03b1. For the weights, we can simply use the fact that w = \u2211 \u03b1\u207d\u2071\u207e y\u207d\u2071\u207e x\u207d\u2071\u207e. Notice that we only care about the indices where \u03b1\u207d\u2071\u207e > 0.", "To get b back, let\u2019s take one last look at the KKT conditions \u2014 from the complimentary slackness, we have:", "From the first equation, we see that when \u03b1\u1d62* > 0, the i-th constraint must be active, that is, it must hold by equality that:", "where M is the subset of the training sample that satisfy 0 < \u03b1\u207d\u2071\u207e < C and n\u2098 is its size. Last, to make a prediction about a new observation x we need to calculate:", "That was a long ride, but we just derived the exact optimization problem that the popular library libsvm solves! We will use a standard QP solver later on, but if you are interested in how they solve it more efficiently, you can check out this paper, where they present the sequential minimal optimization (SMO) algorithm that the _libsvm_ library is based on.", "There are two very important results that need to be highlighted:", "1. The SVM predictions depend only on the observations where \u03b1\u207d\u2071\u207e > 0. These points represent a very special subset of the training sample and are called support vectors.", "Once the model is trained, we can usually discard a large portion of the training set and only retain the support vectors.", "2. During training and to make predictions, we depend on the data points only through their inner product (x\u207d\u2071\u207e)\u1d40 x\u207d\u02b2\u207e. This beautiful and important result is what allows the kernel trick.", "In the non-linear classification section, we talked about applying non-linear transformations over the original features before fitting a SVM. This simple trick allowed the linear SVM to capture non-linear relationship in the data.", "For example, let \u03d5 be a feature mapping function, like the one below:", "Rather than fitting the SVM using the original features (x\u2081, x\u2082), we want to use \u03d5(x). We\u2019ve already seen that when using the dual form, the SVM depend on the data points only through their inner product. This means we only need to calculate:", "This is where kernels come in. In machine learning, a kernel is a function that calculates the inner product \u03d5(x\u207d\u2071\u207e)\u1d40 \u03d5(x\u207d\u02b2\u207e) based only on the original vectors (x\u207d\u2071\u207e, x\u207d\u02b2\u207e). No need to compute the transformation \u03d5(x) or even know about it!", "Let\u2019s look at a quick example. Let \u03d5(x) be the feature mapping function defined below:", "Then the inner product of two vectors a = [a\u2081, a\u2082] and b=[b\u2081, b\u2082] after being applied the transformation \u03d5 is:", "That is a very interesting result. The inner product of the transformed vectors is equal to the square of the inner product of the original vectors. You don\u2019t need to apply the transformation at all!", "If we were fitting an SVM to this data, we could simply take the square of the inner products of the original vectors and we would be, in fact, fitting a model to the transformed vectors.", "Another example of a kernel is the RBF function that we talked about earlier. It turns out that the transformation embedded in the RBF kernel maps each training instance to an infinite-dimensional space.", "This is a perfect example of how kernels can be useful: it allows the SVM to learn in a high (or infinite!) dimensional feature space, while only having to calculate the kernel function of the original vectors.", "This all begs the question: can every function be used as a kernel? It can be shown that if a function K(a, b) respects a few conditions (Mercer\u2019s conditions), then there exists a function \u03d5 that maps a and b into another space, such that: K(a, b) = \u03d5(a)\u1d40 \u03d5(b). We can then use K as a kernel because we know that \u03d5 exists. So as long as a function meets the Mercer\u2019s conditions, it can be used as a kernel.", "One last note: if we are using a kernel, we usually can\u2019t get the weight vector w from the Lagrange multipliers. That happens because, after transforming the inputs, w = \u2211\u1d62 \u03b1\u207d\u2071\u207e y\u207d\u2071\u207e \u03d5(x\u207d\u2071\u207e) and \u03d5 is not necessarily known. But this presents no problem because everywhere else we only need to compute \u03d5(x\u207d\u2071\u207e)\u03d5(x\u207d\u02b2\u207e) = K(x\u207d\u2071\u207e, x\u207d\u02b2\u207e).", "Now that we know how to derive and solve the SVM optimization problem, we are ready to understand the code behind it. The original code is available in this repo \u2014 the only change made was to estimate b by averaging over the instances where 0 < \u03b1\u207d\u2071\u207e < C (instead of 0 < \u03b1\u207d\u2071\u207e).", "That is it! I hope you learned as much as I did by writing this. You can find a notebook to play with the three models we coded in this post here.", "- Bishop, Christopher \u2014 Pattern Recognition And Machine Learning (link)", "- G\u00e9ron, Aur\u00e9lien \u2014 Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition (link)", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "A web dev learning ML. I write at learningml.dev"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F803884d967e3&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsupport-vector-machine-803884d967e3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsupport-vector-machine-803884d967e3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsupport-vector-machine-803884d967e3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsupport-vector-machine-803884d967e3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----803884d967e3--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----803884d967e3--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@vvaraschin?source=post_page-----803884d967e3--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@vvaraschin?source=post_page-----803884d967e3--------------------------------", "anchor_text": "Victor Varaschin"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F345e4f5e63ed&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsupport-vector-machine-803884d967e3&user=Victor+Varaschin&userId=345e4f5e63ed&source=post_page-345e4f5e63ed----803884d967e3---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F803884d967e3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsupport-vector-machine-803884d967e3&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F803884d967e3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsupport-vector-machine-803884d967e3&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://github.com/victorvrv/blog-resources/blob/master/2-SVM/models.ipynb", "anchor_text": "repo"}, {"url": "https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html", "anchor_text": "SciPy\u2019s minimize"}, {"url": "https://www.csie.ntu.edu.tw/~cjlin/libsvm/", "anchor_text": "libsvm"}, {"url": "https://cvxopt.org/userguide/coneprog.html#quadratic-programming", "anchor_text": "cvxopt"}, {"url": "https://github.com/victorvrv/blog-resources/blob/master/2-SVM/models.ipynb", "anchor_text": "repo"}, {"url": "https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions", "anchor_text": "conditions"}, {"url": "https://www.csie.ntu.edu.tw/~cjlin/libsvm/", "anchor_text": "libsvm"}, {"url": "https://www.csie.ntu.edu.tw/~cjlin/papers/quadworkset.pdf", "anchor_text": "paper"}, {"url": "https://en.wikipedia.org/wiki/Mercer%27s_theorem#Mercer's_condition", "anchor_text": "Mercer\u2019s conditions"}, {"url": "https://gist.github.com/mblondel/586753", "anchor_text": "this repo"}, {"url": "https://github.com/victorvrv/blog-resources/blob/master/2-SVM/models.ipynb", "anchor_text": "here"}, {"url": "http://cs229.stanford.edu/notes/cs229-notes3.pdf", "anchor_text": "http://cs229.stanford.edu/notes/cs229-notes3.pdf"}, {"url": "https://www-cs.stanford.edu/people/davidknowles/lagrangian_duality.pdf", "anchor_text": "https://www-cs.stanford.edu/people/davidknowles/lagrangian_duality.pdf"}, {"url": "https://gist.github.com/mblondel/586753", "anchor_text": "https://gist.github.com/mblondel/586753"}, {"url": "https://www.springer.com/gp/book/9780387310732", "anchor_text": "link"}, {"url": "https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/", "anchor_text": "link"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----803884d967e3---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/python?source=post_page-----803884d967e3---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/kernel-trick?source=post_page-----803884d967e3---------------kernel_trick-----------------", "anchor_text": "Kernel Trick"}, {"url": "https://medium.com/tag/data-science?source=post_page-----803884d967e3---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/support-vector-machine?source=post_page-----803884d967e3---------------support_vector_machine-----------------", "anchor_text": "Support Vector Machine"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F803884d967e3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsupport-vector-machine-803884d967e3&user=Victor+Varaschin&userId=345e4f5e63ed&source=-----803884d967e3---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F803884d967e3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsupport-vector-machine-803884d967e3&user=Victor+Varaschin&userId=345e4f5e63ed&source=-----803884d967e3---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F803884d967e3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsupport-vector-machine-803884d967e3&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----803884d967e3--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F803884d967e3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsupport-vector-machine-803884d967e3&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----803884d967e3---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----803884d967e3--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----803884d967e3--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----803884d967e3--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----803884d967e3--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----803884d967e3--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----803884d967e3--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----803884d967e3--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----803884d967e3--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@vvaraschin?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@vvaraschin?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Victor Varaschin"}, {"url": "https://medium.com/@vvaraschin/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "6 Followers"}, {"url": "http://learningml.dev", "anchor_text": "learningml.dev"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F345e4f5e63ed&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsupport-vector-machine-803884d967e3&user=Victor+Varaschin&userId=345e4f5e63ed&source=post_page-345e4f5e63ed--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F345e4f5e63ed%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsupport-vector-machine-803884d967e3&user=Victor+Varaschin&userId=345e4f5e63ed&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}