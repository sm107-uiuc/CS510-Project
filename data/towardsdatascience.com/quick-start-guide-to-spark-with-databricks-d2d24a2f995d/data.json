{"url": "https://towardsdatascience.com/quick-start-guide-to-spark-with-databricks-d2d24a2f995d", "time": 1683002156.439868, "path": "towardsdatascience.com/quick-start-guide-to-spark-with-databricks-d2d24a2f995d/", "webpage": {"metadata": {"title": "Quick start guide to Spark with Databricks | by Scott Johnson | Towards Data Science", "h1": "Quick start guide to Spark with Databricks", "description": "How to create a Spark cluster, import data, query, aggregate and export results in just a few minutes"}, "outgoing_paragraph_urls": [{"url": "http://community.cloud.databricks.com", "anchor_text": "community.cloud.databricks.com", "paragraph_index": 9}, {"url": "https://towardsdatascience.com/from-scikit-learn-to-spark-ml-f2886fb46852", "anchor_text": "other projects", "paragraph_index": 14}], "all_paragraphs": ["Apache Spark is a powerful framework for processing large volumes of data, probably the most powerful and most efficient. Any tool so powerful will be necessarily complex and may be difficult for beginners to use.", "This is most clearly the case when attempting to get started with Spark with Scala. It\u2019s easy to find a bit of sample code that will load up a file and run a few queries. But try actually running that sample code somewhere.", "You could do it in Scala with the sbt build tool. That is not a bad way to go, but if you are not already setup to build and run Scala applications with sbt, with Scala and sbt installed and your computer, and probably an IDE, and know how to produce sbt projects from scratch, it may not be so easy. If you are a Scala developer, this may not be that hard, although even then it is still harder than it needs to be.", "Moreover, in this day and age, when we want to write some code and get some quick results we usually don\u2019t want to create an entire sbt or Maven or Gradle project, we just want to pull up a Jupyter notebook, copy and paste a few lines and hit shift return.", "But try doing this with Scala and Spark.", "Your first option may be Amazon Web Services\u2019 SageMaker, which is great if you are working with Python and scikit-learn. But good luck if you are doing the exact same thing \u2014 using virtually the exact same code \u2014 with Spark. There is an option for Spark in SageMaker \u2014 but out of the box it doesn\u2019t work! At least the last time I tried I got a cryptic error about something, and then it turned out after a bit of Googling that the Spark SageMaker option requires you to create a Spark cluster and attach it to the notebook. Do you know how to do that? Do you want to? It certainly is not obvious from the notebook or from the error.", "Finally, you could install Jupyter notebooks on your laptop, then install the sparkmagic extension (which has multiple components) then hope that all works and all the pieces are pointed to each other in the right way. I got this to work, but it took days of head scratching on multiple fronts. Fortunately, they also have a Docker option, so if you have Docker installed on your laptop then you can (probably) get this going a lot quicker. Just make sure to mount a local directory to the running Docker instance so you don\u2019t lose your work every time you restart your machine! A lesson you will probably learn the hard way if you are not already savvy with the Docker CLI \u2014 something you may not want to think about if you are just trying to run some Spark sample code.", "There has to be a better way to run a few Spark commands on some local data, and fortunately there is.", "Databricks offers the \u201ceasy as SageMaker\u201d option for Spark that AWS decided not to provide. Even better, the Community Edition is free \u2014 and ironically it runs on AWS, which on its own will charge you to do the exact same thing with more work on your part!", "First, go to community.cloud.databricks.com and create an account. Click \u201cTry Databricks\u201d and choose the Community option.", "You will have to create a (free) cluster, but it only requires a couple of clicks and it is very obvious how to do it. Click on the \u201cCluster\u201d icon on the left, create a cluster and give it a name, any name. Since this is your first (and for now, only) cluster, it does not really matter that much what the name is. You aren\u2019t going to have to go digging for it in some file system with the Linux command line, it is all point-and-click.", "Now, click on the \u201cdatabricks\u201d icon on the left, then create a \u201cNew Notebook.\u201d Choose the Scala option (unless you want Python) and then select the cluster you already created. It\u2019s the only one there, so it should be pretty easy to choose it. You may need to click the \u201cWorkspace\u201d icon on the left to open the notebook, but that\u2019s about it. Now, you have a notebook running Scala with Spark built-in. No, really. The only pain is that you will have to recreate a cluster (with a few clicks) every day, so long as you stick with the free option.", "In order to do anything with Spark, you need a SparkSession. Don\u2019t worry if you don\u2019t know what this means \u2014 you can read more in depth about this as you become more familiar with Spark, but for now it is just an object that points to your cluster that allows you to run Spark commands. In Databricks, it is just called spark. You can verify that it is alive in the notebook, by typing \u201cspark\u201d and hitting shifter-enter.", "If you see something like this, then you have Spark running and you are good to go. It is surprising how much work it can take just to get here if you are trying this outside of Databricks.", "Now, let\u2019s do something useful. Let\u2019s load some data, extract some insights, and save them back elsewhere for later use. I am going to use a data set of San Francisco housing prices that I have used for other projects. This just a simple CSV file and you can really use any CSV file for this.", "First, upload the file into the notebook by clicking the \u201cData\u201d icon on the left, then the \u201cAdd data\u201d button, then upload the file. Select and upload your file. Note that the file you upload will be stored in the Databricks system at /FileStore/tables/[file].", "We can now read the file.", "All I have done here is told the SparkSession to read a file, infer the schema (the types of data, eg string or integer), noted that the CSV has a header in the first line (and not data in the first line), and gave the path to the file. After running this command we can use Databricks\u2019 display function to get a quick look at our data.", "We can now perform queries against our CSV file. No need to write parsing functions, no need to loop through each row line by line. We can just perform SQL queries against our CSV.", "First, let\u2019s register our dataset as a temporary table in Spark:", "Now we can run SQL queries directly against our CSV file, as though it were a database:", "This is just a sample of what can be done with Spark SQL, but the point here is that we are now up and running with SQL, in our own sandbox created in just a few minutes, without having to find (or provision) a relational database.", "On the other hand, if you have ever spent months or years of your professional life writing SQL queries, you (like me) may have many bad memories of stringing together ever more complicated strings of variables based on user input. These are ugly and cumbersome and extremely difficult to support and scale. This is where the power of Spark comes in. We can do the exact same queries above without writing any actual SQL queries. Instead, we can write Spark code in Scala by executing Spark query functions on our DataFrame:", "This will display the exact same results as the previous raw SQL query. Also, note that the display() function is specific to DataFrames. When using straight Spark then the .show function will display results (although without all the bells and whistles), eg:", "Now, let\u2019s query for more specific information. Say we want the average price of two bedrooms homes (which could be a house, condo, etc.) in the zip code 94109. In Spark, this is easy:", "Let\u2019s break this query down a bit.", "First, we select the lastsoldprice field in our DataFrame. Next, we filter our DataFrame so that first, it only contains values where the zipcode is 94109 and second where it only contains values where the number of bedrooms is 2. In each case we use === because we are comparing the value with a Column of values, not a single variable. Then query for the average lastsoldprice. Note that avg function has to be imported. There are a few subtlties here you may have to remember (or lookup) but the code itself is pretty self-explanatory.", "The result is a new DataFrame, which we then display using the display function.", "Now, let\u2019s take a look at some more complicated queries. First, let\u2019s create a DataFrame that contains only the fields we want:", "From here on out, every query is going to be slightly more efficient because we are only querying and returning the fields we actually need.", "Next, we can create DataFrames for subsets of our data. For example, say we want a DataFrame that only contains info for single family homes in the Mission district:", "We might want to find the average price of these homes sold in the year 2015. We can start with our mission DataFrame and query further from there to create a new DataFrame:", "Finally, this all looks very nice, but how do we actually get this data out of here? Spark generally assumes that you are going to do everything in Spark, and when you are done, you are going to write it out somewhere. So we will write our final DataFrame out to another CSV file:", "This should all be self-explanatory, except repartition(1) . That simply states that we want to write out all of the partitions in our cluster as a single partition. Otherwise, we might end up creating multiple files (one per partition) if that is how our cluster is setup.", "Having said that, Spark still creates multiple files with this write command, which you can see if you run the following:", "But if you look closely at the results of the above, it is clear that there is only one CSV file. The rest is stuff that Spark uses for its own housekeeping.", "Now, how do we get this file? This is as easy as it ought to be. Display the DataFrame, and download it:", "Note the download button at the bottom \u2014 this is at the bottom of every DataFrame displayed using the display command. Just click that and you will have your CSV.", "You could probably do all this using various filters and queries in Excel if you really wanted to. But with Spark, you can create a pipeline of these transformations (one for each combination of neighborhood, number of bedrooms, etc. that you want) which will query and produce results (aggregates like the average price, or new DataFrames/CSV files altogether), and do this for datasets of terabytes or more of data. Just scale your underlying cluster to support this volume of data, no code change necessary. The code will already be set to run and can be automated as a nightly our hourly job if that is what you want.", "That is not something that can be managed quite so easily in Excel or MySQL.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Software developer and data engineer based in the San Francisco Bay Area"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fd2d24a2f995d&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fquick-start-guide-to-spark-with-databricks-d2d24a2f995d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fquick-start-guide-to-spark-with-databricks-d2d24a2f995d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fquick-start-guide-to-spark-with-databricks-d2d24a2f995d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fquick-start-guide-to-spark-with-databricks-d2d24a2f995d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----d2d24a2f995d--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d2d24a2f995d--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@scottdonaldjohnson?source=post_page-----d2d24a2f995d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@scottdonaldjohnson?source=post_page-----d2d24a2f995d--------------------------------", "anchor_text": "Scott Johnson"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2459d2d9ebf7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fquick-start-guide-to-spark-with-databricks-d2d24a2f995d&user=Scott+Johnson&userId=2459d2d9ebf7&source=post_page-2459d2d9ebf7----d2d24a2f995d---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd2d24a2f995d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fquick-start-guide-to-spark-with-databricks-d2d24a2f995d&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd2d24a2f995d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fquick-start-guide-to-spark-with-databricks-d2d24a2f995d&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "http://community.cloud.databricks.com", "anchor_text": "community.cloud.databricks.com"}, {"url": "https://towardsdatascience.com/from-scikit-learn-to-spark-ml-f2886fb46852", "anchor_text": "other projects"}, {"url": "https://medium.com/tag/spark?source=post_page-----d2d24a2f995d---------------spark-----------------", "anchor_text": "Spark"}, {"url": "https://medium.com/tag/scala?source=post_page-----d2d24a2f995d---------------scala-----------------", "anchor_text": "Scala"}, {"url": "https://medium.com/tag/databricks?source=post_page-----d2d24a2f995d---------------databricks-----------------", "anchor_text": "Databricks"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd2d24a2f995d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fquick-start-guide-to-spark-with-databricks-d2d24a2f995d&user=Scott+Johnson&userId=2459d2d9ebf7&source=-----d2d24a2f995d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd2d24a2f995d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fquick-start-guide-to-spark-with-databricks-d2d24a2f995d&user=Scott+Johnson&userId=2459d2d9ebf7&source=-----d2d24a2f995d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd2d24a2f995d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fquick-start-guide-to-spark-with-databricks-d2d24a2f995d&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d2d24a2f995d--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fd2d24a2f995d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fquick-start-guide-to-spark-with-databricks-d2d24a2f995d&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----d2d24a2f995d---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----d2d24a2f995d--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----d2d24a2f995d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----d2d24a2f995d--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----d2d24a2f995d--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----d2d24a2f995d--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----d2d24a2f995d--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----d2d24a2f995d--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----d2d24a2f995d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@scottdonaldjohnson?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@scottdonaldjohnson?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Scott Johnson"}, {"url": "https://medium.com/@scottdonaldjohnson/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "84 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2459d2d9ebf7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fquick-start-guide-to-spark-with-databricks-d2d24a2f995d&user=Scott+Johnson&userId=2459d2d9ebf7&source=post_page-2459d2d9ebf7--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F2459d2d9ebf7%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fquick-start-guide-to-spark-with-databricks-d2d24a2f995d&user=Scott+Johnson&userId=2459d2d9ebf7&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}