{"url": "https://towardsdatascience.com/function-approximation-in-reinforcement-learning-85a4864d566", "time": 1682996329.8039029, "path": "towardsdatascience.com/function-approximation-in-reinforcement-learning-85a4864d566/", "webpage": {"metadata": {"title": "Function Approximation in Reinforcement Learning | by Ziad SALLOUM | Towards Data Science", "h1": "Function Approximation in Reinforcement Learning", "description": "Update: The best way of learning and practicing Reinforcement Learning is by going to http://rl-lab.com In tabular methods like DP and Monte Carlo we have seen that the representation of the states\u2026"}, "outgoing_paragraph_urls": [{"url": "http://rl-lab.com/", "anchor_text": "http://rl-lab.com", "paragraph_index": 0}, {"url": "https://medium.com/@zsalloum/math-behind-reinforcement-learning-the-easy-way-1b7ed0c030f4", "anchor_text": "Math Behind Reinforcement Learning", "paragraph_index": 15}, {"url": "https://towardsdatascience.com/td-in-reinforcement-learning-the-easy-way-f92ecfa9f3ce", "anchor_text": "Temporal Difference", "paragraph_index": 17}, {"url": "https://towardsdatascience.com/gradient-descent-the-easy-way-5240ca9a08da", "anchor_text": "Gradient Descent", "paragraph_index": 24}, {"url": "https://towardsdatascience.com/back-propagation-the-easy-way-part-1-6a8cde653f65", "anchor_text": "Part 1", "paragraph_index": 34}, {"url": "https://towardsdatascience.com/back-propagation-the-easy-way-part-2-bea37046c897", "anchor_text": "Part 2", "paragraph_index": 34}, {"url": "https://towardsdatascience.com/back-propagation-the-easy-way-part-3-cc1de33e8397", "anchor_text": "Part 3", "paragraph_index": 34}], "all_paragraphs": ["Update: The best way of learning and practicing Reinforcement Learning is by going to http://rl-lab.com", "In tabular methods like DP and Monte Carlo we have seen that the representation of the states is actually a memorisation of each state.Now let\u2019s recall what is exactly a state.A state is the combination of observable features or variables. Which means every time a feature or a variable has a new value, it results in a a new state.", "let\u2019s take a concrete example. Suppose an agent is in a 4x4 grid, so the location of the of the agent on the grit is a feature. This gives 16 different locations meaning 16 different states.", "But that\u2019s not all, suppose the orientation (north, south, east, west) is also a feature. This gives 4 possibilities for each location, which makes the number of states to 16*4 = 64. Furthermore if the agent has the possibility of using 5 different tools (including \u201cno tool\u201d case), this will grow the number of states to 64 * 5 = 320.You got the idea\u2026", "One way to represent those states is by creating a multidimensional array such as V[row, column, direction, tool]. Then we either query or compute a state.For example V[1, 2, north, torch] represents the state where the agent is at row 1, column 2, looking north and holding a torch. The value inside this array cell tells how valuable this state is.Another example would be V[4, 1, west, nothing], which is an agent at 4th row, 1st column, heading west and holding nothing.", "Let\u2019s also consider game of Chess.The situation of the board after each move is a state. The estimated number of states is about 10^120 !One representation of a state in Chess could be:", "Where each dimension represent a square on the board and its value is one of the black or white pieces as well as none.", "So once we have the set of states we can assign a value-state function for each state.", "Needless to say that the amount of memory needed to accommodate the number of state is huge and the amount to time needed to compute the value of each state is also prohibitive.", "Logically this pushes us to find better and more suitable solutions.", "It is always useful to keep in mind what we are trying to do, because with all the details we might lose sight.The idea is that we want to find the value of each state/action, in an environment, so that the agent follows the optimum path that collects the maximum rewards.", "In the previous section we have shown that when the state space becomes too large, the tabular methods become insufficient and unsuitable.In order to address this shortcoming, we can adopt a new approach based on the features of each state. The aim is to use these set of features to generalise the estimation of the value at states that have similar features.", "We used the word estimation to indicate that this approach will never find the true value of a state, but an approximation of it. Despite this seemingly inconvenient result, however this will achieve faster computation and much more generalisations.", "The methods that compute these approximations are called Function Approximators.", "Since we will use gradient descent in order to find the best result, the function approximators must be differentiable, which leads us to Linear combinations of features and Neural Networks.", "Let\u2019s now delve into the details of this Linear Function Approximator method.But first let\u2019s remember this formula from Math Behind Reinforcement Learning article:", "This is a recursive formula which computes the value of a state s based on the values of the next states s\u2019 and so on\u2026The problem with this formula is that each time we need to compute V(s) we need to compute all future states. Even worse, if at a time we encounter a state similar to one that we have already seen in the past, we have no way of recognizing it.", "Also remember from the Temporal Difference article that with Q learning formulas we have a way of estimating what future states can be:", "We will use those formulas to derive some interesting solutions.Let\u2019s redefine V(s) such as it reflects the features it contains. The state-value function can be expressed by its weighted sum of its features:V(s) = W1 . F1(s) + W2 . F2(s) + \u2026 +Wn . Fn(s) or in shorter terms:", "\ud835\udf6b(s) is a vector of features at state s, and \ud835\udf3d\u1d40 is a transpose matrix of weights applied to the features, in such a manner that some features are valued more than others at any state s.", "Now let\u2019s define \ud835\udf39, also called TD error, as follows:", "By replacing V(s) in \ud835\udf39 by its new definition we obtain:", "Remember that by definition V(St) = R(t\u208a\u2081) + \ud835\udefeV(St\u208a\u2081) which means that \ud835\udf39 should be zero. However we are not in a deterministic environment which means we are not sure that the same actions always lead to the same results, and the learning process is not 100% accurate, meaning what we learn for V(s) does not necessarily equates to R(t\u208a\u2081) + \ud835\udefeV(St\u208a\u2081). All this causes \ud835\udf39 to have non-zero value.So now our aim is to compute \ud835\udf3d such as to minimize \ud835\udf39. A convenient way to achieve this by minimizing J(\ud835\udf3d).", "The J(\ud835\udf3d) like in any regression computes the difference between the actual result and the true result. However our problem is that we don\u2019t know what is the true result! Instead we will compute the difference between the actual result and an estimated result. After each iteration we have a new estimation of the true result, which makes it appear as if we are aiming at a moving target.Still the idea is to keep refining the estimation until the difference between the two becomes small enough.It has been proven that this approach has enough guaranties of convergence.", "The minimization of the cost function can be done using the gradient descent method that is detailed in the Gradient Descent article.The Gradient Descent will update \ud835\udf3d step by step towards the value that minimizes J(\ud835\udf3d), the update formula is:", "IMPORTANT NOTE: Actually the derivation of J(\ud835\udf3d) relative to \ud835\udf3d is \ud835\udf39 [\ud835\udf6b(St)-\ud835\udf6b(St+1)], but in practice this algorithm has worse results.", "We have established the function approximation for state-value function, now let\u2019s extend this notion to the action-value functions.", "Let \ud835\udf6b(s,a) be the features at state s, and action a, what we want to do is to estimate the value of Q(s,a) according to a policy \u03c0.", "Similarly to state value, the action value function is:", "and the update of weights \ud835\udf3d is:", "The main drawback of linear function approximation compared to non-linear function approximation, such as the neural network, is the need for good hand-picked features, which may require domain knowledge.", "In the non-linear function approximator we will redefine once again the state and action value function V and Q such as:", "So now V and Q are non-linear functions that take \ud835\udf6b(s) / \ud835\udf6b(s, a) and \ud835\udf3d as parameters.What we should note in this formulation is that the number of features and the size of the \ud835\udf3d vector are not necessarily the same (just imagine a neural network).", "From the above we can now compute the TD error \ud835\udf39 and the update of the weights \ud835\udf3d", "Note that \ud835\udf39 is computed using the previously know values of \ud835\udf3d, referred here as \ud835\udf3d\u207b, and the newly computed one.Details of Neural Network computations can be found in Back Propagation articles Part 1, Part 2, and Part 3.", "In summary the function approximation helps finding the value of a state or an action when similar circumstances occur, whereas in computing the real values of V and Q requires a full computation and does not learn from past experience. Furthermore function approximation saves computation time and memory space."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F85a4864d566&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffunction-approximation-in-reinforcement-learning-85a4864d566&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffunction-approximation-in-reinforcement-learning-85a4864d566&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffunction-approximation-in-reinforcement-learning-85a4864d566&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffunction-approximation-in-reinforcement-learning-85a4864d566&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://zsalloum.medium.com/?source=post_page-----85a4864d566--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----85a4864d566--------------------------------", "anchor_text": ""}, {"url": "https://zsalloum.medium.com/?source=post_page-----85a4864d566--------------------------------", "anchor_text": "Ziad SALLOUM"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1f2b933522e2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffunction-approximation-in-reinforcement-learning-85a4864d566&user=Ziad+SALLOUM&userId=1f2b933522e2&source=post_page-1f2b933522e2----85a4864d566---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----85a4864d566--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F85a4864d566&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffunction-approximation-in-reinforcement-learning-85a4864d566&user=Ziad+SALLOUM&userId=1f2b933522e2&source=-----85a4864d566---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F85a4864d566&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffunction-approximation-in-reinforcement-learning-85a4864d566&source=-----85a4864d566---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://unsplash.com/@qwitka?utm_source=medium&utm_medium=referral", "anchor_text": "Maksym Kaharlytskyi"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "http://rl-lab.com/", "anchor_text": "http://rl-lab.com"}, {"url": "https://medium.com/@zsalloum/math-behind-reinforcement-learning-the-easy-way-1b7ed0c030f4", "anchor_text": "Math Behind Reinforcement Learning"}, {"url": "https://towardsdatascience.com/td-in-reinforcement-learning-the-easy-way-f92ecfa9f3ce", "anchor_text": "Temporal Difference"}, {"url": "https://towardsdatascience.com/gradient-descent-the-easy-way-5240ca9a08da", "anchor_text": "Gradient Descent"}, {"url": "https://towardsdatascience.com/back-propagation-the-easy-way-part-1-6a8cde653f65", "anchor_text": "Part 1"}, {"url": "https://towardsdatascience.com/back-propagation-the-easy-way-part-2-bea37046c897", "anchor_text": "Part 2"}, {"url": "https://towardsdatascience.com/back-propagation-the-easy-way-part-3-cc1de33e8397", "anchor_text": "Part 3"}, {"url": "https://towardsdatascience.com/policy-gradient-step-by-step-ac34b629fd55", "anchor_text": "Policy Gradient Step by Step"}, {"url": "https://towardsdatascience.com/policy-based-reinforcement-learning-the-easy-way-8de9a3356083", "anchor_text": "Policy Based Reinforcement Learning"}, {"url": "https://towardsdatascience.com/introduction-to-actor-critic-7642bdb2b3d2", "anchor_text": "Introduction to Actor Critic in Reinforcement Learning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----85a4864d566---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----85a4864d566---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----85a4864d566---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----85a4864d566---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/q-learning?source=post_page-----85a4864d566---------------q_learning-----------------", "anchor_text": "Q Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F85a4864d566&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffunction-approximation-in-reinforcement-learning-85a4864d566&user=Ziad+SALLOUM&userId=1f2b933522e2&source=-----85a4864d566---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F85a4864d566&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffunction-approximation-in-reinforcement-learning-85a4864d566&user=Ziad+SALLOUM&userId=1f2b933522e2&source=-----85a4864d566---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F85a4864d566&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffunction-approximation-in-reinforcement-learning-85a4864d566&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://zsalloum.medium.com/?source=post_page-----85a4864d566--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----85a4864d566--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1f2b933522e2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffunction-approximation-in-reinforcement-learning-85a4864d566&user=Ziad+SALLOUM&userId=1f2b933522e2&source=post_page-1f2b933522e2----85a4864d566---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F408fc441c93b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffunction-approximation-in-reinforcement-learning-85a4864d566&newsletterV3=1f2b933522e2&newsletterV3Id=408fc441c93b&user=Ziad+SALLOUM&userId=1f2b933522e2&source=-----85a4864d566---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://zsalloum.medium.com/?source=post_page-----85a4864d566--------------------------------", "anchor_text": "Written by Ziad SALLOUM"}, {"url": "https://zsalloum.medium.com/followers?source=post_page-----85a4864d566--------------------------------", "anchor_text": "845 Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----85a4864d566--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://rl-lab.com", "anchor_text": "https://rl-lab.com"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1f2b933522e2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffunction-approximation-in-reinforcement-learning-85a4864d566&user=Ziad+SALLOUM&userId=1f2b933522e2&source=post_page-1f2b933522e2----85a4864d566---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F408fc441c93b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffunction-approximation-in-reinforcement-learning-85a4864d566&newsletterV3=1f2b933522e2&newsletterV3Id=408fc441c93b&user=Ziad+SALLOUM&userId=1f2b933522e2&source=-----85a4864d566---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/policy-based-reinforcement-learning-the-easy-way-8de9a3356083?source=author_recirc-----85a4864d566----0---------------------23ea7511_59f0_4545_9f0a_3e5a7e31cd91-------", "anchor_text": ""}, {"url": "https://zsalloum.medium.com/?source=author_recirc-----85a4864d566----0---------------------23ea7511_59f0_4545_9f0a_3e5a7e31cd91-------", "anchor_text": ""}, {"url": "https://zsalloum.medium.com/?source=author_recirc-----85a4864d566----0---------------------23ea7511_59f0_4545_9f0a_3e5a7e31cd91-------", "anchor_text": "Ziad SALLOUM"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----85a4864d566----0---------------------23ea7511_59f0_4545_9f0a_3e5a7e31cd91-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/policy-based-reinforcement-learning-the-easy-way-8de9a3356083?source=author_recirc-----85a4864d566----0---------------------23ea7511_59f0_4545_9f0a_3e5a7e31cd91-------", "anchor_text": "Policy Based Reinforcement Learning, the Easy WayStep by step approach to understanding Policy Based methods in Reinforcement Learning"}, {"url": "https://towardsdatascience.com/policy-based-reinforcement-learning-the-easy-way-8de9a3356083?source=author_recirc-----85a4864d566----0---------------------23ea7511_59f0_4545_9f0a_3e5a7e31cd91-------", "anchor_text": "8 min read\u00b7Feb 8, 2019"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8de9a3356083&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpolicy-based-reinforcement-learning-the-easy-way-8de9a3356083&user=Ziad+SALLOUM&userId=1f2b933522e2&source=-----8de9a3356083----0-----------------clap_footer----23ea7511_59f0_4545_9f0a_3e5a7e31cd91-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/policy-based-reinforcement-learning-the-easy-way-8de9a3356083?source=author_recirc-----85a4864d566----0---------------------23ea7511_59f0_4545_9f0a_3e5a7e31cd91-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8de9a3356083&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpolicy-based-reinforcement-learning-the-easy-way-8de9a3356083&source=-----85a4864d566----0-----------------bookmark_preview----23ea7511_59f0_4545_9f0a_3e5a7e31cd91-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----85a4864d566----1---------------------23ea7511_59f0_4545_9f0a_3e5a7e31cd91-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----85a4864d566----1---------------------23ea7511_59f0_4545_9f0a_3e5a7e31cd91-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----85a4864d566----1---------------------23ea7511_59f0_4545_9f0a_3e5a7e31cd91-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----85a4864d566----1---------------------23ea7511_59f0_4545_9f0a_3e5a7e31cd91-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----85a4864d566----1---------------------23ea7511_59f0_4545_9f0a_3e5a7e31cd91-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----85a4864d566----1---------------------23ea7511_59f0_4545_9f0a_3e5a7e31cd91-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----23ea7511_59f0_4545_9f0a_3e5a7e31cd91-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----85a4864d566----1---------------------23ea7511_59f0_4545_9f0a_3e5a7e31cd91-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----85a4864d566----1-----------------bookmark_preview----23ea7511_59f0_4545_9f0a_3e5a7e31cd91-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----85a4864d566----2---------------------23ea7511_59f0_4545_9f0a_3e5a7e31cd91-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----85a4864d566----2---------------------23ea7511_59f0_4545_9f0a_3e5a7e31cd91-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----85a4864d566----2---------------------23ea7511_59f0_4545_9f0a_3e5a7e31cd91-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----85a4864d566----2---------------------23ea7511_59f0_4545_9f0a_3e5a7e31cd91-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----85a4864d566----2---------------------23ea7511_59f0_4545_9f0a_3e5a7e31cd91-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----85a4864d566----2---------------------23ea7511_59f0_4545_9f0a_3e5a7e31cd91-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----2-----------------clap_footer----23ea7511_59f0_4545_9f0a_3e5a7e31cd91-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----85a4864d566----2---------------------23ea7511_59f0_4545_9f0a_3e5a7e31cd91-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----85a4864d566----2-----------------bookmark_preview----23ea7511_59f0_4545_9f0a_3e5a7e31cd91-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/monte-carlo-tree-search-in-reinforcement-learning-b97d3e743d0f?source=author_recirc-----85a4864d566----3---------------------23ea7511_59f0_4545_9f0a_3e5a7e31cd91-------", "anchor_text": ""}, {"url": "https://zsalloum.medium.com/?source=author_recirc-----85a4864d566----3---------------------23ea7511_59f0_4545_9f0a_3e5a7e31cd91-------", "anchor_text": ""}, {"url": "https://zsalloum.medium.com/?source=author_recirc-----85a4864d566----3---------------------23ea7511_59f0_4545_9f0a_3e5a7e31cd91-------", "anchor_text": "Ziad SALLOUM"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----85a4864d566----3---------------------23ea7511_59f0_4545_9f0a_3e5a7e31cd91-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/monte-carlo-tree-search-in-reinforcement-learning-b97d3e743d0f?source=author_recirc-----85a4864d566----3---------------------23ea7511_59f0_4545_9f0a_3e5a7e31cd91-------", "anchor_text": "Monte Carlo Tree Search in Reinforcement LearningA recipe of the search algorithm at the heart of Deep Mind\u2019s Alpha Zero AI."}, {"url": "https://towardsdatascience.com/monte-carlo-tree-search-in-reinforcement-learning-b97d3e743d0f?source=author_recirc-----85a4864d566----3---------------------23ea7511_59f0_4545_9f0a_3e5a7e31cd91-------", "anchor_text": "\u00b76 min read\u00b7Feb 17, 2019"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb97d3e743d0f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmonte-carlo-tree-search-in-reinforcement-learning-b97d3e743d0f&user=Ziad+SALLOUM&userId=1f2b933522e2&source=-----b97d3e743d0f----3-----------------clap_footer----23ea7511_59f0_4545_9f0a_3e5a7e31cd91-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/monte-carlo-tree-search-in-reinforcement-learning-b97d3e743d0f?source=author_recirc-----85a4864d566----3---------------------23ea7511_59f0_4545_9f0a_3e5a7e31cd91-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb97d3e743d0f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmonte-carlo-tree-search-in-reinforcement-learning-b97d3e743d0f&source=-----85a4864d566----3-----------------bookmark_preview----23ea7511_59f0_4545_9f0a_3e5a7e31cd91-------", "anchor_text": ""}, {"url": "https://zsalloum.medium.com/?source=post_page-----85a4864d566--------------------------------", "anchor_text": "See all from Ziad SALLOUM"}, {"url": "https://towardsdatascience.com/?source=post_page-----85a4864d566--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----85a4864d566----0---------------------f3b127df_a8f7_4102_837d_926299e6f3a2-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData?source=read_next_recirc-----85a4864d566----0---------------------f3b127df_a8f7_4102_837d_926299e6f3a2-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData?source=read_next_recirc-----85a4864d566----0---------------------f3b127df_a8f7_4102_837d_926299e6f3a2-------", "anchor_text": "Andrew Austin"}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----85a4864d566----0---------------------f3b127df_a8f7_4102_837d_926299e6f3a2-------", "anchor_text": "AI Anyone Can Understand Part 1: Reinforcement LearningReinforcement learning is a way for machines to learn by trying different things and seeing what works best. For example, a robot could\u2026"}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----85a4864d566----0---------------------f3b127df_a8f7_4102_837d_926299e6f3a2-------", "anchor_text": "\u00b74 min read\u00b7Dec 11, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F6c3b3d623a2d&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40MoneyAndData%2Fai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d&user=Andrew+Austin&userId=42d388912d13&source=-----6c3b3d623a2d----0-----------------clap_footer----f3b127df_a8f7_4102_837d_926299e6f3a2-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----85a4864d566----0---------------------f3b127df_a8f7_4102_837d_926299e6f3a2-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6c3b3d623a2d&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40MoneyAndData%2Fai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d&source=-----85a4864d566----0-----------------bookmark_preview----f3b127df_a8f7_4102_837d_926299e6f3a2-------", "anchor_text": ""}, {"url": "https://medium.com/@anandmishra.kanha/deep-reinforcement-learning-current-state-of-art-383190b14464?source=read_next_recirc-----85a4864d566----1---------------------f3b127df_a8f7_4102_837d_926299e6f3a2-------", "anchor_text": ""}, {"url": "https://medium.com/@anandmishra.kanha?source=read_next_recirc-----85a4864d566----1---------------------f3b127df_a8f7_4102_837d_926299e6f3a2-------", "anchor_text": ""}, {"url": "https://medium.com/@anandmishra.kanha?source=read_next_recirc-----85a4864d566----1---------------------f3b127df_a8f7_4102_837d_926299e6f3a2-------", "anchor_text": "Anand Mishra"}, {"url": "https://medium.com/@anandmishra.kanha/deep-reinforcement-learning-current-state-of-art-383190b14464?source=read_next_recirc-----85a4864d566----1---------------------f3b127df_a8f7_4102_837d_926299e6f3a2-------", "anchor_text": "Deep reinforcement learning \u2014 current state of artCurrent"}, {"url": "https://medium.com/@anandmishra.kanha/deep-reinforcement-learning-current-state-of-art-383190b14464?source=read_next_recirc-----85a4864d566----1---------------------f3b127df_a8f7_4102_837d_926299e6f3a2-------", "anchor_text": "5 min read\u00b7Dec 15, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F383190b14464&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40anandmishra.kanha%2Fdeep-reinforcement-learning-current-state-of-art-383190b14464&user=Anand+Mishra&userId=86f86a9a5573&source=-----383190b14464----1-----------------clap_footer----f3b127df_a8f7_4102_837d_926299e6f3a2-------", "anchor_text": ""}, {"url": "https://medium.com/@anandmishra.kanha/deep-reinforcement-learning-current-state-of-art-383190b14464?source=read_next_recirc-----85a4864d566----1---------------------f3b127df_a8f7_4102_837d_926299e6f3a2-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F383190b14464&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40anandmishra.kanha%2Fdeep-reinforcement-learning-current-state-of-art-383190b14464&source=-----85a4864d566----1-----------------bookmark_preview----f3b127df_a8f7_4102_837d_926299e6f3a2-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----85a4864d566----0---------------------f3b127df_a8f7_4102_837d_926299e6f3a2-------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=read_next_recirc-----85a4864d566----0---------------------f3b127df_a8f7_4102_837d_926299e6f3a2-------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=read_next_recirc-----85a4864d566----0---------------------f3b127df_a8f7_4102_837d_926299e6f3a2-------", "anchor_text": "Wouter van Heeswijk, PhD"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----85a4864d566----0---------------------f3b127df_a8f7_4102_837d_926299e6f3a2-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----85a4864d566----0---------------------f3b127df_a8f7_4102_837d_926299e6f3a2-------", "anchor_text": "Proximal Policy Optimization (PPO) ExplainedThe journey from REINFORCE to the go-to algorithm in continuous control"}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----85a4864d566----0---------------------f3b127df_a8f7_4102_837d_926299e6f3a2-------", "anchor_text": "\u00b713 min read\u00b7Nov 29, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fabed1952457b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fproximal-policy-optimization-ppo-explained-abed1952457b&user=Wouter+van+Heeswijk%2C+PhD&userId=33f45c9ab481&source=-----abed1952457b----0-----------------clap_footer----f3b127df_a8f7_4102_837d_926299e6f3a2-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----85a4864d566----0---------------------f3b127df_a8f7_4102_837d_926299e6f3a2-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fabed1952457b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fproximal-policy-optimization-ppo-explained-abed1952457b&source=-----85a4864d566----0-----------------bookmark_preview----f3b127df_a8f7_4102_837d_926299e6f3a2-------", "anchor_text": ""}, {"url": "https://medium.com/dsckiit/reinforcement-learning-guide-and-introduction-467c6a2ed25e?source=read_next_recirc-----85a4864d566----1---------------------f3b127df_a8f7_4102_837d_926299e6f3a2-------", "anchor_text": ""}, {"url": "https://aniruddhamukh.medium.com/?source=read_next_recirc-----85a4864d566----1---------------------f3b127df_a8f7_4102_837d_926299e6f3a2-------", "anchor_text": ""}, {"url": "https://aniruddhamukh.medium.com/?source=read_next_recirc-----85a4864d566----1---------------------f3b127df_a8f7_4102_837d_926299e6f3a2-------", "anchor_text": "Aniruddha Mukherjee"}, {"url": "https://medium.com/dsckiit?source=read_next_recirc-----85a4864d566----1---------------------f3b127df_a8f7_4102_837d_926299e6f3a2-------", "anchor_text": "GDSC KIIT"}, {"url": "https://medium.com/dsckiit/reinforcement-learning-guide-and-introduction-467c6a2ed25e?source=read_next_recirc-----85a4864d566----1---------------------f3b127df_a8f7_4102_837d_926299e6f3a2-------", "anchor_text": "Reinforcement Learning: An Introduction and Guide to its FundamentalsPolicies, Rewards, the Bellman Equation, and the Markov Decision Process (MDP)"}, {"url": "https://medium.com/dsckiit/reinforcement-learning-guide-and-introduction-467c6a2ed25e?source=read_next_recirc-----85a4864d566----1---------------------f3b127df_a8f7_4102_837d_926299e6f3a2-------", "anchor_text": "5 min read\u00b7Apr 14"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fdsckiit%2F467c6a2ed25e&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fdsckiit%2Freinforcement-learning-guide-and-introduction-467c6a2ed25e&user=Aniruddha+Mukherjee&userId=68f97387c191&source=-----467c6a2ed25e----1-----------------clap_footer----f3b127df_a8f7_4102_837d_926299e6f3a2-------", "anchor_text": ""}, {"url": "https://medium.com/dsckiit/reinforcement-learning-guide-and-introduction-467c6a2ed25e?source=read_next_recirc-----85a4864d566----1---------------------f3b127df_a8f7_4102_837d_926299e6f3a2-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F467c6a2ed25e&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fdsckiit%2Freinforcement-learning-guide-and-introduction-467c6a2ed25e&source=-----85a4864d566----1-----------------bookmark_preview----f3b127df_a8f7_4102_837d_926299e6f3a2-------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----85a4864d566----2---------------------f3b127df_a8f7_4102_837d_926299e6f3a2-------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk?source=read_next_recirc-----85a4864d566----2---------------------f3b127df_a8f7_4102_837d_926299e6f3a2-------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk?source=read_next_recirc-----85a4864d566----2---------------------f3b127df_a8f7_4102_837d_926299e6f3a2-------", "anchor_text": "Steve Roberts"}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----85a4864d566----2---------------------f3b127df_a8f7_4102_837d_926299e6f3a2-------", "anchor_text": "State Values and Policy Evaluation in 5 minutesAn Introduction to Reinforcement Learning"}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----85a4864d566----2---------------------f3b127df_a8f7_4102_837d_926299e6f3a2-------", "anchor_text": "\u00b75 min read\u00b7Jan 11"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2Ff3e00f3c1a50&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40tinkertytonk%2Fstate-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50&user=Steve+Roberts&userId=6b6735266652&source=-----f3e00f3c1a50----2-----------------clap_footer----f3b127df_a8f7_4102_837d_926299e6f3a2-------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----85a4864d566----2---------------------f3b127df_a8f7_4102_837d_926299e6f3a2-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff3e00f3c1a50&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40tinkertytonk%2Fstate-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50&source=-----85a4864d566----2-----------------bookmark_preview----f3b127df_a8f7_4102_837d_926299e6f3a2-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/please-stop-drawing-neural-networks-wrong-ffd02b67ad77?source=read_next_recirc-----85a4864d566----3---------------------f3b127df_a8f7_4102_837d_926299e6f3a2-------", "anchor_text": ""}, {"url": "https://medium.com/@amaster_37400?source=read_next_recirc-----85a4864d566----3---------------------f3b127df_a8f7_4102_837d_926299e6f3a2-------", "anchor_text": ""}, {"url": "https://medium.com/@amaster_37400?source=read_next_recirc-----85a4864d566----3---------------------f3b127df_a8f7_4102_837d_926299e6f3a2-------", "anchor_text": "Aaron Master"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----85a4864d566----3---------------------f3b127df_a8f7_4102_837d_926299e6f3a2-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/please-stop-drawing-neural-networks-wrong-ffd02b67ad77?source=read_next_recirc-----85a4864d566----3---------------------f3b127df_a8f7_4102_837d_926299e6f3a2-------", "anchor_text": "Please Stop Drawing Neural Networks WrongThe Case for GOOD Diagrams"}, {"url": "https://towardsdatascience.com/please-stop-drawing-neural-networks-wrong-ffd02b67ad77?source=read_next_recirc-----85a4864d566----3---------------------f3b127df_a8f7_4102_837d_926299e6f3a2-------", "anchor_text": "12 min read\u00b7Mar 21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fffd02b67ad77&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplease-stop-drawing-neural-networks-wrong-ffd02b67ad77&user=Aaron+Master&userId=31905cfe67ce&source=-----ffd02b67ad77----3-----------------clap_footer----f3b127df_a8f7_4102_837d_926299e6f3a2-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/please-stop-drawing-neural-networks-wrong-ffd02b67ad77?source=read_next_recirc-----85a4864d566----3---------------------f3b127df_a8f7_4102_837d_926299e6f3a2-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "33"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fffd02b67ad77&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplease-stop-drawing-neural-networks-wrong-ffd02b67ad77&source=-----85a4864d566----3-----------------bookmark_preview----f3b127df_a8f7_4102_837d_926299e6f3a2-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----85a4864d566--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----85a4864d566--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----85a4864d566--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----85a4864d566--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----85a4864d566--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----85a4864d566--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----85a4864d566--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----85a4864d566--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----85a4864d566--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----85a4864d566--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}