{"url": "https://towardsdatascience.com/states-actions-rewards-the-intuition-behind-reinforcement-learning-33d4aa2bbfaa", "time": 1683003877.082062, "path": "towardsdatascience.com/states-actions-rewards-the-intuition-behind-reinforcement-learning-33d4aa2bbfaa/", "webpage": {"metadata": {"title": "States, Actions, Rewards \u2014 The Intuition behind Reinforcement Learning | by Deepak Dilipkumar | Towards Data Science", "h1": "States, Actions, Rewards \u2014 The Intuition behind Reinforcement Learning", "description": "What exactly is reinforcement learning, and how does an RL algorithm work in practice?"}, "outgoing_paragraph_urls": [{"url": "http://umichrl.pbworks.com/w/page/7597597/Successes%20of%20Reinforcement%20Learning", "anchor_text": "types of tasks", "paragraph_index": 6}, {"url": "http://brianshourd.com/posts/2012-11-06-tilt-number-of-tic-tac-toe-boards.html", "anchor_text": "593", "paragraph_index": 8}, {"url": "https://web.stanford.edu/group/pdplab/pdphandbook/handbookch10.html", "anchor_text": "Temporal Difference learning", "paragraph_index": 15}, {"url": "https://math.stackexchange.com/questions/1406919/how-many-legal-states-of-chess-exists", "anchor_text": "10\u2074\u2075", "paragraph_index": 16}, {"url": "http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/FA.pdf", "anchor_text": "value function approximation", "paragraph_index": 19}, {"url": "https://deepakdilipkumar.github.io/alphago/", "anchor_text": "https://deepakdilipkumar.github.io", "paragraph_index": 23}], "all_paragraphs": ["In 2014, Google acquired a British startup named DeepMind for half a billion dollars. A steep price, but the investment seems to have paid off many times over just from the publicity that DeepMind generates. ML researchers know DeepMind for its frequent breakthroughs in the field of deep reinforcement learning. But the company has also captured the attention of the general public, particularly due to its successes in building an algorithm to play the game of Go. Considering the frequency with which DeepMind makes progress in the field \u2014 AlphaGo, AlphaGo Zero and AlphaZero in the last couple of years \u2014 it becomes hard to keep track of what exactly is happening, both at a technical level and from the point of view of implications. I plan to do just that \u2014 provide a high-level view of DeepMind\u2019s successes in Go, and explain the distinctions between the different versions of AlphaGo that they have produced.", "Machine learning is a field that is ripe with comparisons to human cognition. This is not a coincidence of course. Many of the most popular tasks in the field (vision, speech and natural language processing) have typically been the domain of human (or natural) intelligence. As \u201cwhat\u201d the algorithm is doing is emulating humans, it is natural to think of \u201chow\u201d the algorithm works as emulating humans as well. Hence the abundance of statements like \u201cNeural networks are inspired by the human brain\u201d (In truth, that statement is only as true as \u201cAeroplanes are inspired by birds\u201d)", "Reinforcement learning is particularly opportune for such comparisons. At its core, any reinforcement learning task is defined by three things \u2014 states, actions and rewards. States are a representation of the current world or environment of the task. Actions are something an RL agent can do to change these states. And rewards are the utility the agent receives for performing the \u201cright\u201d actions. So the states tell the agent what situation it is in currently, and the rewards signal the states that it should be aspiring towards. The aim, then, is to learn a \u201cpolicy\u201d, something which tells you which action to take from each state so as to try and maximize reward. This broad definition can be used to fit a number of diverse tasks that we perform every day.", "When you drive, the state is the position and speed of your car and the neighbouring cars. The actions you can take are turning the steering wheel and pressing the accelerator or brake. The reward is dependant on how quickly you reach your destination while respecting traffic rules.", "When you play a video game (say, The Legend of Zelda), the state is whatever information you have on the screen \u2014 your location, the presence of nearby characters or monsters, the weapons and items you possess. Your actions can involve moving or attacking. Your reward depends on the number of lives you have remaining and any money or items you gain from defeating an enemy.", "Let\u2019s look at a more abstract example \u2014 applying for grad school. Your state is your current research/work profile, and the information you have about programmes and professors at different schools. Your actions may be deciding who to ask for a letter of recommendation, what to put on your SoP, and which schools to apply to. The reward, of course, is an acceptance letter (or a negative reward due to a rejection letter).", "These examples show that it is often possible to frame myriad tasks at different levels of \u201cconcreteness\u201d in terms of a reinforcement learning setup. However, it is important to note that the scope of the state, action and reward set may be very different for different tasks. For instance, the action set in a Zelda game, while extensive, is clearly an order of magnitude smaller than the action set for applying to grad school. This suggests that framing tasks as reinforcement learning works well when you have clearly defined states and rewards and restricted action sets. This can be seen in the types of tasks that RL has shown success on.", "One common approach to solving RL tasks is called \u201cvalue-based\u201d. We try to assign a number to each state (or each (state, next action) pair ) based on which states are likely to lead to high rewards. This entity connecting each state to a particular numerical value is called a value function. If we learn an appropriate value function in this way, then from a particular state, we can simply choose the action that is likely to lead to a next state with a high value. So we\u2019ve now reduced our task to the problem of learning this value function.", "To understand what this learning process may look like, let\u2019s look at a more concrete example \u2014 tic tac toe. The state is the current board position, the actions are the different places in which you can place an \u2018X\u2019 or \u2018O\u2019, and the reward is +1 or -1 depending on whether you win or lose the game. The \u201cstate space\u201d is the total number of possible states in a particular RL setup. Tic tac toe has a small enough state space (one reasonable estimate being 593) that we can actually remember a value for each individual state, using a table. This is called a tabular method for this reason. This isn\u2019t practical in most applications (imagine listing out all possible configurations of a chessboard and assigning a value to each one), but I\u2019ll come back to how to deal with that later.", "We start by assigning some initial values to each state, say 0 for all states (there are better initialization strategies, but this will do for now). So initially, our table looks like this (let\u2019s say we have some way of numbering or indexing the states):", "We then start playing games of tic tac toe against an opponent, where we follow the general rule that the action taken in the current state is the one that leads to a next state with as high a value as possible. In the beginning, as all states have equal values, this would mean just picking randomly from the set of legal moves. But after a few games, you notice that when you reach this position (you being \u2018X\u2019):", "You get a reward of +1. The value associated with that final state must be +1 then. So you update your table (say the final winning position has an arbitrary index of 123):", "Now, in the next few games, you see that the positions that you lead you to 123 (let\u2019s say these are 121 and 122 respectively) are also likely to end with a high reward, as long as you play the right move:", "So you update the table again, noting that this penultimate state doesn\u2019t necessarily have the full reward of +1, as it is still possible to make the wrong move and end up with a loss or draw:", "I won\u2019t go into the actual equations involved, but at a high-level we see that the reward is sort of \u201cflowing\u201d backwards from the final (terminal) state, and giving a concrete value to the states that lead up to it. These terminal states are thus extremely important in ensuring that the algorithm learns the right value function. I once had a bug where everything was working as expected except the flag indicating these terminal states, and the algorithm ended up learning almost nothing.", "So, given enough training games, the reward will flow all the way back to the very first states, and you have a good strategy from beginning to finish! For instance, one common way to do this \u201cbootstrapping\u201d, where the value of one state is learnt from the value of the states that come immediately after it, is called Temporal Difference learning.", "How well does this strategy work for, say, chess? Tic tac toe has around 600 states. While it is difficult to calculate the exact number of states in chess, a good upper bound seems to be 10\u2074\u2075. There isn\u2019t enough memory in all of the computers in the world to store that many states, so putting all the values in a table isn\u2019t a good idea anymore. How do we handle this issue?", "Let\u2019s think about we play chess. As we practice more, we start getting an intuitive understanding of how \u201cgood\u201d certain states are. For instance, try to evaluate this state (white to play):", "You can probably tell that white has the better position. It\u2019s likely that you\u2019ve never seen this exact board state before in your life. Nevertheless, you\u2019ve built intuition about what constitutes \u201cgoodness\u201d of a state in chess. You probably counted the number of pieces that each side has left, and noted that while black has 2 extra pawns, white has an extra knight and rook, giving that side the advantage. If you\u2019re a more experienced player, you might have looked at possible next moves (white bishop could take the black bishop for instance) and the sections of the board that each side controls.", "So you\u2019re definitely not memorizing the potential value of each possible chess position \u2014 you\u2019ve learnt some kind of a general function, mapping the state of the board to the value of the game for each player. This is exactly what we would want a smart chess algorithm for RL to do as well, through an idea called value function approximation.", "First, we figure out a way to represent the game state. For tic-tac-toe, we could just enumerate all the states, meaning that each game state was represented by a single number. For chess, one simple way of doing this would be to represent each piece with a number (pawn \u2192 1, rook \u2192 2 \u2026) and then have a list of length 64 that encodes the presence of a particular piece on each position (with, say, 0 for the empty positions). We could feed this into the value function that our RL algorithm has learnt, and it would spit out a number (representing \u201cvalue\u201d) or maybe a probability (representing the chance that black wins).", "Now the actual RL part is very similar to what we did for tic-tac-toe. We play lots of games, reward the algorithm for winning and penalize it for losing, and over time it will (hopefully) learn a good function representing the value of a state. The only change is that in place of looking up a value from a table, we pass an input through our function and get the corresponding value as output. Having this, we can derive a policy to play chess by looking at what state we\u2019d reach by taking each possible legal move, and pass each of these states through our function to see which one has the highest value!", "This was a fairly naive way of approaching the chess problem with RL, and there are some sophisticated ideas we can use to speed up computation, improve learning and eventually make better moves. This is going to be particularly important in the case of Go, where the state space for the game is even larger than for chess! This was all probably a lot to take in, so I\u2019ll stop here and discuss some of these interesting ideas in the context of Go in the next blog post. Thanks for reading!", "Originally published at https://deepakdilipkumar.github.io on March 3, 2018.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F33d4aa2bbfaa&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstates-actions-rewards-the-intuition-behind-reinforcement-learning-33d4aa2bbfaa&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstates-actions-rewards-the-intuition-behind-reinforcement-learning-33d4aa2bbfaa&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstates-actions-rewards-the-intuition-behind-reinforcement-learning-33d4aa2bbfaa&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstates-actions-rewards-the-intuition-behind-reinforcement-learning-33d4aa2bbfaa&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----33d4aa2bbfaa--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----33d4aa2bbfaa--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://deepakdilipkumar.medium.com/?source=post_page-----33d4aa2bbfaa--------------------------------", "anchor_text": ""}, {"url": "https://deepakdilipkumar.medium.com/?source=post_page-----33d4aa2bbfaa--------------------------------", "anchor_text": "Deepak Dilipkumar"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3a6bca9167fd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstates-actions-rewards-the-intuition-behind-reinforcement-learning-33d4aa2bbfaa&user=Deepak+Dilipkumar&userId=3a6bca9167fd&source=post_page-3a6bca9167fd----33d4aa2bbfaa---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F33d4aa2bbfaa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstates-actions-rewards-the-intuition-behind-reinforcement-learning-33d4aa2bbfaa&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F33d4aa2bbfaa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstates-actions-rewards-the-intuition-behind-reinforcement-learning-33d4aa2bbfaa&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "http://umichrl.pbworks.com/w/page/7597597/Successes%20of%20Reinforcement%20Learning", "anchor_text": "types of tasks"}, {"url": "http://brianshourd.com/posts/2012-11-06-tilt-number-of-tic-tac-toe-boards.html", "anchor_text": "593"}, {"url": "https://web.stanford.edu/group/pdplab/pdphandbook/handbookch10.html", "anchor_text": "Temporal Difference learning"}, {"url": "https://math.stackexchange.com/questions/1406919/how-many-legal-states-of-chess-exists", "anchor_text": "10\u2074\u2075"}, {"url": "http://chess.com", "anchor_text": "chess.com"}, {"url": "http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/FA.pdf", "anchor_text": "value function approximation"}, {"url": "https://deepakdilipkumar.github.io/alphago/", "anchor_text": "https://deepakdilipkumar.github.io"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----33d4aa2bbfaa---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----33d4aa2bbfaa---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----33d4aa2bbfaa---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F33d4aa2bbfaa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstates-actions-rewards-the-intuition-behind-reinforcement-learning-33d4aa2bbfaa&user=Deepak+Dilipkumar&userId=3a6bca9167fd&source=-----33d4aa2bbfaa---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F33d4aa2bbfaa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstates-actions-rewards-the-intuition-behind-reinforcement-learning-33d4aa2bbfaa&user=Deepak+Dilipkumar&userId=3a6bca9167fd&source=-----33d4aa2bbfaa---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F33d4aa2bbfaa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstates-actions-rewards-the-intuition-behind-reinforcement-learning-33d4aa2bbfaa&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----33d4aa2bbfaa--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F33d4aa2bbfaa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstates-actions-rewards-the-intuition-behind-reinforcement-learning-33d4aa2bbfaa&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----33d4aa2bbfaa---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----33d4aa2bbfaa--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----33d4aa2bbfaa--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----33d4aa2bbfaa--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----33d4aa2bbfaa--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----33d4aa2bbfaa--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----33d4aa2bbfaa--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----33d4aa2bbfaa--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----33d4aa2bbfaa--------------------------------", "anchor_text": ""}, {"url": "https://deepakdilipkumar.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://deepakdilipkumar.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Deepak Dilipkumar"}, {"url": "https://deepakdilipkumar.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "86 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3a6bca9167fd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstates-actions-rewards-the-intuition-behind-reinforcement-learning-33d4aa2bbfaa&user=Deepak+Dilipkumar&userId=3a6bca9167fd&source=post_page-3a6bca9167fd--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F3a61b5478539&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstates-actions-rewards-the-intuition-behind-reinforcement-learning-33d4aa2bbfaa&newsletterV3=3a6bca9167fd&newsletterV3Id=3a61b5478539&user=Deepak+Dilipkumar&userId=3a6bca9167fd&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}