{"url": "https://towardsdatascience.com/all-the-way-from-information-theory-to-log-loss-in-machine-learning-c78488dade15", "time": 1683014278.431787, "path": "towardsdatascience.com/all-the-way-from-information-theory-to-log-loss-in-machine-learning-c78488dade15/", "webpage": {"metadata": {"title": "All the Way from Information Theory to Log Loss in Machine Learning | by Soner Y\u0131ld\u0131r\u0131m | Towards Data Science", "h1": "All the Way from Information Theory to Log Loss in Machine Learning", "description": "In 1948, Claude Shannon introduced the information theory in his 55-page-long paper called \u201cA Mathematical Theory of Communication\u201d. The information theory is where we start the discussion that will\u2026"}, "outgoing_paragraph_urls": [{"url": "http://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf", "anchor_text": "A Mathematical Theory of Communication", "paragraph_index": 0}, {"url": "http://linkedin.com/in/soneryildirim/", "anchor_text": "linkedin.com/in/soneryildirim/", "paragraph_index": 35}, {"url": "http://twitter.com/snr14", "anchor_text": "twitter.com/snr14", "paragraph_index": 35}], "all_paragraphs": ["In 1948, Claude Shannon introduced the information theory in his 55-page-long paper called \u201cA Mathematical Theory of Communication\u201d. The information theory is where we start the discussion that will lead us to the log loss which is a widely-used cost function in machine learning and deep learning models.", "The goal of the information theory is to efficiently deliver messages from a sender to a receiver. In the digital age, the information is represented by bits, 0 and 1. According to Shannon, one bit of information sent to the recipient means to reduce the uncertainty of the recipient by a factor of two. Thus, information is proportional to the uncertainty reduction.", "Consider the case of flipping a fair coin. The probability of heads being the side facing up, P(Heads), is 0.5. After you (the recipient) are told that the heads is up, P(Heads) becomes 1. Thus, 1 bit of information is sent to you and the uncertainty is reduced by a factor of two. The amount of information we get is the reduction in uncertainty which is the inverse of the probability of events.", "The number of bits of information can easily be calculated by taking log (base2) of the reduction in uncertainty.", "Let\u2019s go over a slightly more complicated case. Two of your friends go to a store to buy a particular t-shirt and there are 4 different colors available.", "Your friend Julia is a little indecisive and she tells you that she can pick any color. Your other friend John tells you that he likes the color blue and he is very much likely to buy a blue t-shirt.", "You definitely have more uncertainty about the decision of Julia than that of John. Entropy is a measure that quantifies the uncertainty. To be more precise, it is the average amount of information received from samples within a probability distribution.", "The following table shows the probability distributions of the events that Julia and John buying a t-shirt.", "Let\u2019s start with Julia. If Julia picks blue, the uncertainty is reduced by 4 (1/0.25). It is equal to 2 bits in log base 2 (The base unit of entropy is a bit). Thus, in the case of blue, the amount of information we get is 2 bits. Since entropy is the average amount of information of samples, we repeat the same calculations for other colors. They result in the same number of bits since the probabilities are the same. For Julia, the entropy is calculated as follows:", "For John, the steps are the same but the result is different.", "The entropy is more in the case with Julia so we have more uncertainty about the decision of Julia which we expected in the beginning.", "We have calculated the entropy. It is time to introduce the formula:", "Note: We did not include the minus sign in our calculations because it has been eliminated by taking the inverse of probability (1 / p ).", "We have two events with 4 outcomes. The first event is Julia buying a t-shirt, The second event is John buying a t-shirt. The entropies are 2 bits and 1.19 bits, respectively. In other words, on average, we receive 2 bits of information about the first event and 1.19 bits of information about the second one.", "We are building our way towards the concepts used in machine learning. The next topic is the cross-entropy which is the message length on average.", "The color that your friend picks is transmitted to you digitally (i.e. with bits). The following table represents two different encodings used to transfer information about the choice of John.", "In case 1, two bits are used for every color. Thus, the average message length is 2.", "This encoding is acceptable for Julia but not for John. The entropy of the probability distribution of John\u2019s choices is 1.19 bits so using 2 bits on average to send information about his choice is not an optimal way.", "In case 2, cross-entropy turns out to be 1.3 bits. It is still more than 1.19 but definitely a better way than case 1.", "But, where does the word \u201ccross\u201d come from? When calculating the cross-entropy, we are actually comparing two different probability distributions. One is the actual probability distribution of the variable and the other is the predicted one with the choice of bits.", "The cross-entropy can be expressed as a function of the true and predicted distributions as follows:", "If you take a look at the calculations we have done to find the cross-entropy, you will notice that the steps overlap with this formula.", "We can now start our discussion on how cross-entropy is used in the field of machine learning. Cross-entropy loss (i.e. log loss) is a widely-used cost function for machine learning and deep learning models.", "Cross-entropy quantifies the comparison of two probability distributions. In supervised learning tasks, we have a target variable that we are trying to predict. The actual distribution of the target variable and our predictions are compared using the cross-entropy. The result is the cross-entropy loss, also known as log loss.", "There is a slight difference between the cross-entropy and the cross-entropy loss. When calculating the loss, natural log is usually used instead of log base 2.", "Let\u2019s do an example. We have a classification problem with 4 classes. The prediction of our model for a particular observations is as below:", "Since we know the true probability distribution, it is 100% for the true class and zero for all others. According to our model, the class that this observation belongs to is class 1with 80% probability. The cross-entropy loss for this particular observation is calculated as below:", "Since the true probability is zero for all classes except for the actual class, only the predicted probability of the actual class contributes to the cross-entropy loss.", "Please keep in mind that this is the loss on a particular observation. The loss on the training or test set is the average of the cross-entropies of all observations in that set.", "You may wonder why the log loss is used instead of classification accuracy as a cost function.", "The following table shows the predictions of two different models on a relatively small set that consists of 5 observations.", "Both models correctly classified 4 observations out of 5. Thus, in terms of classification accuracy, these models have the same performance. However, the probabilities reveal that Model 1 is more certain in the predictions. Thus, it is likely to perform better in general.", "Log loss (i.e. cross-entropy loss) provide a more robust and accurate evaluation of classification models.", "Thank you for reading. Please let me know if you have any feedback.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Scientist | Top 10 Writer in AI and Data Science | linkedin.com/in/soneryildirim/ | twitter.com/snr14"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fc78488dade15&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fall-the-way-from-information-theory-to-log-loss-in-machine-learning-c78488dade15&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fall-the-way-from-information-theory-to-log-loss-in-machine-learning-c78488dade15&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fall-the-way-from-information-theory-to-log-loss-in-machine-learning-c78488dade15&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fall-the-way-from-information-theory-to-log-loss-in-machine-learning-c78488dade15&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----c78488dade15--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c78488dade15--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://sonery.medium.com/?source=post_page-----c78488dade15--------------------------------", "anchor_text": ""}, {"url": "https://sonery.medium.com/?source=post_page-----c78488dade15--------------------------------", "anchor_text": "Soner Y\u0131ld\u0131r\u0131m"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2cf6b549448&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fall-the-way-from-information-theory-to-log-loss-in-machine-learning-c78488dade15&user=Soner+Y%C4%B1ld%C4%B1r%C4%B1m&userId=2cf6b549448&source=post_page-2cf6b549448----c78488dade15---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc78488dade15&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fall-the-way-from-information-theory-to-log-loss-in-machine-learning-c78488dade15&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc78488dade15&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fall-the-way-from-information-theory-to-log-loss-in-machine-learning-c78488dade15&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@markusspiske?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Markus Spiske"}, {"url": "https://unsplash.com/s/photos/information?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "http://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf", "anchor_text": "A Mathematical Theory of Communication"}, {"url": "http://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf", "anchor_text": "http://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf"}, {"url": "https://www.youtube.com/watch?v=ErfnhcEV1O8", "anchor_text": "https://www.youtube.com/watch?v=ErfnhcEV1O8"}, {"url": "https://medium.com/tag/data-science?source=post_page-----c78488dade15---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----c78488dade15---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----c78488dade15---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/predictive-analytics?source=post_page-----c78488dade15---------------predictive_analytics-----------------", "anchor_text": "Predictive Analytics"}, {"url": "https://medium.com/tag/classification?source=post_page-----c78488dade15---------------classification-----------------", "anchor_text": "Classification"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc78488dade15&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fall-the-way-from-information-theory-to-log-loss-in-machine-learning-c78488dade15&user=Soner+Y%C4%B1ld%C4%B1r%C4%B1m&userId=2cf6b549448&source=-----c78488dade15---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc78488dade15&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fall-the-way-from-information-theory-to-log-loss-in-machine-learning-c78488dade15&user=Soner+Y%C4%B1ld%C4%B1r%C4%B1m&userId=2cf6b549448&source=-----c78488dade15---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc78488dade15&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fall-the-way-from-information-theory-to-log-loss-in-machine-learning-c78488dade15&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c78488dade15--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fc78488dade15&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fall-the-way-from-information-theory-to-log-loss-in-machine-learning-c78488dade15&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----c78488dade15---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----c78488dade15--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----c78488dade15--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----c78488dade15--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----c78488dade15--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----c78488dade15--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----c78488dade15--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----c78488dade15--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----c78488dade15--------------------------------", "anchor_text": ""}, {"url": "https://sonery.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://sonery.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Soner Y\u0131ld\u0131r\u0131m"}, {"url": "https://sonery.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "21K Followers"}, {"url": "http://linkedin.com/in/soneryildirim/", "anchor_text": "linkedin.com/in/soneryildirim/"}, {"url": "http://twitter.com/snr14", "anchor_text": "twitter.com/snr14"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2cf6b549448&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fall-the-way-from-information-theory-to-log-loss-in-machine-learning-c78488dade15&user=Soner+Y%C4%B1ld%C4%B1r%C4%B1m&userId=2cf6b549448&source=post_page-2cf6b549448--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F7cdf5377373a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fall-the-way-from-information-theory-to-log-loss-in-machine-learning-c78488dade15&newsletterV3=2cf6b549448&newsletterV3Id=7cdf5377373a&user=Soner+Y%C4%B1ld%C4%B1r%C4%B1m&userId=2cf6b549448&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}