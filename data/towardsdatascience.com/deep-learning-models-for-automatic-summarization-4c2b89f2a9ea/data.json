{"url": "https://towardsdatascience.com/deep-learning-models-for-automatic-summarization-4c2b89f2a9ea", "time": 1683007268.342128, "path": "towardsdatascience.com/deep-learning-models-for-automatic-summarization-4c2b89f2a9ea/", "webpage": {"metadata": {"title": "Deep Learning Models for Automatic Summarization | by Pirmin Lemberger | Towards Data Science", "h1": "Deep Learning Models for Automatic Summarization", "description": "For over a quarter of century we have been able to search the web by querying a search engine using a couple of relevant keywords. Without such a tool the internet would be nothing but useless\u2026"}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/PageRank", "anchor_text": "PageRank algorithm", "paragraph_index": 0}, {"url": "https://searchengineland.com/welcome-bert-google-artificial-intelligence-for-understanding-search-queries-323976", "anchor_text": "semantic processing", "paragraph_index": 0}, {"url": "https://arxiv.org/abs/1807.08000", "anchor_text": "short product descriptions", "paragraph_index": 1}, {"url": "https://blog.frase.io/20-applications-of-automatic-summarization-in-the-enterprise/", "anchor_text": "here", "paragraph_index": 1}, {"url": "https://arxiv.org/abs/1906.03741", "anchor_text": "1", "paragraph_index": 2}, {"url": "https://arxiv.org/abs/1906.01351", "anchor_text": "2", "paragraph_index": 2}, {"url": "https://arxiv.org/abs/1704.04368", "anchor_text": "3", "paragraph_index": 2}, {"url": "https://arxiv.org/abs/1905.06566", "anchor_text": "4", "paragraph_index": 2}, {"url": "https://arxiv.org/abs/1705.04304", "anchor_text": "5", "paragraph_index": 2}, {"url": "https://rxnlp.com/how-rouge-works-for-evaluation-of-summarization-tasks/#.Xqbe75MzbUJ", "anchor_text": "6", "paragraph_index": 4}, {"url": "https://github.com/abisee/cnn-dailymail", "anchor_text": "CNN / Daily Mail data set", "paragraph_index": 8}, {"url": "https://arxiv.org/abs/1906.03741", "anchor_text": "1", "paragraph_index": 8}, {"url": "https://arxiv.org/abs/1906.03741", "anchor_text": "BigPatent dataset", "paragraph_index": 8}, {"url": "https://arxiv.org/abs/1906.03741", "anchor_text": "1", "paragraph_index": 8}, {"url": "https://arxiv.org/abs/1906.01351", "anchor_text": "TalkSumm method", "paragraph_index": 9}, {"url": "https://arxiv.org/abs/1906.01351", "anchor_text": "2", "paragraph_index": 9}, {"url": "https://guillaumegenthial.github.io/sequence-to-sequence.html", "anchor_text": "Seq2Seq architectures", "paragraph_index": 11}, {"url": "https://colah.github.io/posts/2015-08-Understanding-LSTMs/", "anchor_text": "LSTM recurrent neural networks", "paragraph_index": 11}, {"url": "http://jalammar.github.io/illustrated-bert/", "anchor_text": "BERT", "paragraph_index": 11}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "Transformer", "paragraph_index": 11}, {"url": "https://arxiv.org/abs/1409.0473", "anchor_text": "attention mechanism", "paragraph_index": 11}, {"url": "https://arxiv.org/abs/1704.04368", "anchor_text": "3", "paragraph_index": 14}, {"url": "https://arxiv.org/abs/1704.04368", "anchor_text": "3", "paragraph_index": 15}, {"url": "http://jalammar.github.io/illustrated-bert/", "anchor_text": "BERT model", "paragraph_index": 21}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "Transformer", "paragraph_index": 21}, {"url": "https://arxiv.org/abs/1905.06566", "anchor_text": "HIBERT", "paragraph_index": 21}, {"url": "https://arxiv.org/abs/1905.06566", "anchor_text": "4", "paragraph_index": 21}, {"url": "https://arxiv.org/abs/1705.04304", "anchor_text": "5", "paragraph_index": 29}, {"url": "http://incompleteideas.net/book/the-book-2nd.html", "anchor_text": "13", "paragraph_index": 31}, {"url": "http://incompleteideas.net/book/the-book-2nd.html", "anchor_text": "13", "paragraph_index": 33}, {"url": "https://arxiv.org/abs/1705.04304", "anchor_text": "5", "paragraph_index": 34}, {"url": "https://arxiv.org/abs/1704.04368", "anchor_text": "3", "paragraph_index": 39}, {"url": "https://recital.ai/en/", "anchor_text": "ReciTAL", "paragraph_index": 40}, {"url": "https://arxiv.org/abs/1909.01610", "anchor_text": "14", "paragraph_index": 40}, {"url": "https://arxiv.org/abs/1906.07901", "anchor_text": "15", "paragraph_index": 40}, {"url": "https://www.linkedin.com/in/tscialom/?originalSubdomain=fr", "anchor_text": "Thomas Scialom", "paragraph_index": 41}, {"url": "https://recital.ai/en/", "anchor_text": "ReciTAL", "paragraph_index": 41}, {"url": "https://github.com/recitalAI/summarizing_summarization", "anchor_text": "Summarizing Summarization", "paragraph_index": 41}, {"url": "https://github.com/recitalAI/summarizing_summarization", "anchor_text": "16", "paragraph_index": 41}], "all_paragraphs": ["For over a quarter of century we have been able to search the web by querying a search engine using a couple of relevant keywords. Without such a tool the internet would be nothing but useless garbage dump of data. In 1998 Google\u2019s PageRank algorithm redefined what we can expect as far as relevance of search results is concerned. More recently some semantic processing has been added to the wizardry that helps the engine to interpret a query that was expressed in plain language. In a not too distant future we may perhaps pinpoint documents by engaging a short Q\\&A kind of conversation with a search engine, just as we would with a bookseller. There is an important difference though between a bookseller and a search engine. If you are hesitant about which book you should read you could try to ask the bookseller to summarize it for you in few of sentences.", "This kind summarization task has long seemed totally out of reach within the classic rule-based NLP approaches and neither was it not considered realistic in foreseeable future. But, slowly, things are now changing with recent progress in Deep Learning models for NLP. For the moment just imagine you had a drop down list next to the input field of your favorite search engine that would allow you to set the length of an automatic summary for a given document. Say, a 1 sentence, a 10 sentences or a one page summary. Would that be helpful? Actually it is quite possible that it could quickly prove so useful that it could become ubiquitous. Besides improving document search it could also help in a multitude of other tasks. For instance it could help scientists keep up with a dizzying flow of publications in fields like medicine or AI. More prosaically it could help producing short product descriptions for online stores with catalogues too large to be handled manually. Many more examples of applications of automatic summarization are described for instance here.", "For larger documents with several hundreds of pages like novels such generic summarization tools still belong to the realm of science fiction. However, thanks to the ever surprising flexibility of Deep Learning models, the wait may not be that long for tools that could summarize one- or two-page documents in a few sentences, at least within specific areas of knowledge. The aim of this article is to describe recent data sets [1, 2] and Deep Learning architectures [3, 4, 5] that have brought us a little closer to the goal.", "The summarizing task is difficult for a number of reasons, some of which are common with other NLP tasks like translation for instance:", "Human evaluation of a summary is subjective and involves judgments like style, coherence, completeness and readability. Unfortunately no score is currently known which is both easy to compute and faithful to human judgment. The ROUGE score [6] is the best we have but it has obvious shortcomings as we shall see. ROUGE simply counts the number of words, or n-grams, that are common to the summary produced by a machine and a reference summary written by a human. More precisely it reports a combination of the corresponding recall:", "The combination reported in ROUGE-n is their geometric mean (known as the F1 score). Although the ROUGE score does not faithfully reflect a human judgment it has the advantage of computational simplicity and it takes into account some of the flexibility associated with the multiple summaries that could result by rearranging words within a valid summary.", "There are two types of summarization systems:", "We will describe instances of both kinds below.", "Until recently the main data set used for training summarization models was the CNN / Daily Mail data set which contains 300,000 examples of news article paired with their multiline summary. A detailed examination [1], however, has revealed various limitations in this data set that could bias the evaluation of the ability of a system to perform text summarization. It turned out for instance that useful information is spread unevenly across the source, namely mostly at the beginning of the documents. Moreover, many summaries contain large fragments of the source. This is certainly not the best way for teaching a system how to produce good abstractive summaries. But things have changed recently. The BigPatent dataset [1] for instance contains 1.3 millions patent documents together with their summaries that alleviate most of the above shortcomings.", "A novel approach to produce ever growing data sets for training summarization models uses video transcripts of talks given at international scientific conferences. The basic assumption here is that these transcripts make a good starting point for producing high quality summaries of scientific papers. The transcript itself is not directly the summary of a paper. Rather, the authors of the TalkSumm method [2] propose to create a summary by retrieving a sequence of relevant sentences from the paper presented in the talk. A sentence is deemed relevant depending on how many words the speaker uses to describe it in her talk, assuming that she has a given sentence of the paper in mind at any given point in time.", "In this section we describe 3 neural network models that have be developed recently for the summarization task. The aim here is not completeness of course but merely to illustrate the diversity of ideas which have been proposed to tackle this fundamental NLP problem.", "The basic neural network architectures that make it possible to learn this kind of task are the Seq2Seq architectures, the LSTM recurrent neural networks (RNN), the BERT and the Transformer models as well as the attention mechanism.", "For the readers unfamiliar with any of these topics we recommend the above links which will provide excellent introductions to each of them. Figure 1 represents the Seq2Seq architecture which converts a sequence of tokens into another sequence with a possibly different length. It defines the vectors we will refer to when talking about Seq2Seq.", "Figure 2 sketches a Transformer network with the self-attention dependencies between embeddings an hidden vectors. Roughly speaking a transformer converts a sequence of token embeddings x_i into another sequence of context aware embeddings h_i. The input vectors x_i also typically include positional information. This is needed in contrast to RNN networks because of the permutation symmetry of inputs in a Transformer.", "The first architecture we present addresses the abstractive summarization task [3]. Early attempts to apply vanilla Seq2Seq architectures to the summarization revealed a number of issues with this straightforward approach:", "Figure 3 shows examples of these unwanted behaviors. The authors in [3] propose two improvements over the vanilla Seq2Seq with attention mechanism to mitigate these shortcomings.", "First, to overcome the finite vocabulary limitation they allow the network to copy a word directly from the source and use it in the summary when needed. The precise mechanism to do this is called a pointer network. Remember that in a vanilla Seq2Seq network the decoder computes a probability distribution p^t_vocab(w), at each time step t, over the words w in a fixed finite vocabulary. As usual p^t_vocab is computed with a softmax layer that takes the attention context vector h^t and the decoder state s_t as inputs. In a pointer network an additional copy probability p_copy is computed which represents the probability that a word should be copied from the source rather than generated by the decoder. The probability p_copy is computed using a sigmoid layer having h^t, s_t and x_t vectors as inputs (see figure 1). Which word should actually be copied is determined by the attention weights a_i^t that the decoder puts at time t to each word w_i in the source. Putting it all together, the full probability for the model to produce the word w is thus given by the following mixture:", "Second, to avoid repeating the same segments the authors define a coverage vector c^t at each time step t which estimates the amount of attention that each word w_i in the source has received from the decoder until time t:", "This coverage vector is then used in two different places within the network. First it is used to inform the attention mechanism in charge of computing the attention weights a^t_i (in addition to the usual dependence on the encoder context vector h_i for the word w_i and the decoder state s_t. The decoder is thus aware of the words it has already been paying attention to. Second it is used to correct the loss function. Remember that at time step t the weight a^t_i is the attention put on word w_i while c^t_i is the attention this same word has received in the past. If the word w_i receives more attention at time t than it has already received in the past, that is if a^t_i > c^t_i, then the cost function should penalize large values of c^t_i and also the other way around. To penalize attention to repeated words one defines an additional term in the loss function at time step t as a sum over input tokens:", "This is then added (with an additional hyperparameter) to the usual negative log likelihood of the target word w^*_t in the train set:", "Results with and without these additional tricks are shown in figure 3.", "Our next example illustrates recent ideas that defined a new SOTA for the extractive summary task. It builds directly upon a key idea that lead to the BERT model in 2018, namely that of transfer learning based on a clever pretraining task for a Transformer encoder. Let\u2019s go into a little more detail and summarize the HIBERT architecture for document summarization [4].", "The basic observation is that extractive classification can be cast as a sentence tagging problem: simply train a model to identify which sentence in a document should be kept to make up summary! For this purpose the HIBERT architecture uses two nested encoder Transformers as illustrated in figure 4.", "The first Transformer encoder at the bottom is a classic sentence encoder that transforms the sequence of words (w_0^k, w_1^k,\u2026, w_j^k) that make up the kth sentence of the document to be summarized into a sentence embedding h_k. This vector is conventionally identified as the context vector above the end of sentence token <EOS>.", "The second Transformer encoder which sits on the top is a document encoder that transforms the sequence of sentence embeddings (h_1, h_2,\u2026, h_D) into a sequence of document aware sentence embeddings (d_1, d_2,\u2026, d_D). These embeddings are in turn converted into a sequence of probabilities (p_1, p_2,\u2026, p_D) where p_j is the probability that the jth sentence should be part of the summary.", "Training such a complex hierarchical network from scratch is impractical because it would require an unrealistic amount of document-summary pairs. As is well known, the best strategy to train such a complex network with a limited amount of data is to use transfer learning. For this purpose the HIBERT architecture is first pretrained on an auxiliary task which consists in predicting sentences that are randomly masked (15% of them) within in a large corpus of documents:", "Figure 5 shows the architecture used for this masked sentence prediction task. It adds a Transformer decoder on top of the HIBERT architecture in order to convert the document aware sentence embedding d_k into the sequence of words (w_0^k, w_2^k,\u2026, w_j^k) of the kth sentence which was masked. To generate the word at step i the decoder uses both its context h_i and the document aware sentence embedding d_k from the document encoder.", "Trained this way the network gathers a large amount of semantic knowledge without requiring any expansive labeling procedure. In a second stage, leveraging what it learned during the pretraining task, the network is fine-tuned on the actual target task, namely summarization as a sentence binary tagging task as in figure 4 describes.", "This masked sentence prediction task is obviously reminiscent, on a sentence level, of the masked language model (MLM) used for pretraining the original BERT model. Remember that the MLM task consisted in recovering randomly masked words within sentences.", "As we explained earlier one central issue with the summarization task is the lack of a unique best summary. The ROUGE score takes this into account, up to some level, because it ignores the order of the words (or n-grams) within the generated summary. Therefore the cost function we would actually like to minimize should be something like this ROUGE score, or at least the final loss function should include such a term. This is the strategy that was followed in the last work [5] we present here which, again, concerns abstractive summarization.", "The problem with a score like ROUGE is that for any sequence of words (w_1,\u2026, w_j) generated by the decoder, it is constant with respect to the parameters theta of the network, thus making backpropagation impossible. The situation is not hopeless though because the expectation of the ROUGE score for sentences (w_1,\u2026, w_j), sampled from the joint probability distribution p_theta(w_1,\u2026, w_j) defined by the generator is indeed a differentiable function of those parameters! The way to go is clear then. Just minimize the loss L_RL defined by that expectation:", "Actually we can view the generator of a Seq2Seq model as a Reinforcement Learning (RL) agent whose action at time step t is to generates a word w_t depending on an inner state s_t which encapsulates the history from previous actions. From here on we just need to open a book on RL [13] to learn how to minimize L_RL. A basic result in RL, known as the Policy Gradient Theorem, states that the gradient of L_RL:", "and the last index j is that of the <EOS> token. The REINFORCE algorithm approximates the above expectation with a single sample (w_1,\u2026, w_j) from the distribution p_theta(w_1,\u2026, w_j) computed by the generator:", "In practice scores like ROUGE can have a large variance which hinders convergence of the gradient descent. Fortunately, we can enhance the speed of convergence by comparing ROUGE(w_1,\u2026, w_j) to a baseline b which is independent of (w_1,\u2026, w_j). This does not change the gradient of L_RL as can readily be verified but it can considerably reduce the variance [13] and thus dramatically improve convergence:", "The main point thus is to find an appropriate baseline. The idea in the work we are discussing [5] is to take the baseline b equal to the ROUGE score of the sequence of words the generator actually generates at inference time. Remember that this is the sequence of words that successively maximize the conditional probabilities as computed by the softmax of the decoder at each step t:", "This choice for the baseline b is called self-critical sequence training (SCST). Altogether the reinforcement loss term thus reads:", "This loss term as we can see prompts p_theta to generate word sequences (w_1,\u2026,w_j) whose ROUGE score is larger than that of the sequence that was currently generated by the decoder.", "There are two benefits for including such a SCST reinforcement learning term L_RL in the loss function. The first, which motivated the construction L_RL of in the first place, is that it makes it possible to use a non-differentiable score like ROUGE within a stochastic gradient descent training procedure. The second benefit is that it also cures the so called exposure bias. Exposure bias results from the classic teacher forcing procedure that is typically used to train a Seq2Seq model. This procedure trains the decoder RNN using the ground truth words (w*_1,\u2026,w*_j) from the train set while at inference time the decoder must of course use its own generated tokens which could therefore result in an accumulation of errors. The SCST choice for the baseline b amounts to train the decoder using the distribution it will actually see at inference time.", "The final loss function used is a weighted sum of the reinforcement learning loss L_RL and a standard maximum likelihood objective L_ML. The former takes into account the non-uniqueness of summaries, at least up to some point, but by itself it is certainly not an incentive for the model to produce readable messages. The latter on the other hand favors readable sentences as it is basically defines a language model.", "In order to avoid repetitions, the authors also use an enhanced attention mechanism that involves a pointer network similar to the one we described in the first example [3].", "The three models we described in the previous section all use Deep Learning and therefore implement a purely statistical approach to the summarization task. Recent research also tries to find better loss functions. Researchers at ReciTAL for instance explore the interesting idea that a good summary should answer questions as well as the original text allows [14]. On the whole, these models work indeed surprisingly well for short documents. But can we reasonably expect to build a system that could summarize a 300 pages novel in a page using techniques that only rely on crunching huge amounts of textual data? This is far from obvious. Abstract summarization should in principle be able to leverage real world knowledge to make sense of a document or a book to be summarized. It is unlikely though that language models alone, even when initialized with clever pretraining tasks, can ever capture such common sense which is more likely collected by sensory experience. One short term possibility for building useful summarization tools could be to narrow their scope down to specific areas of expertise where knowledge basis or ontologies are already available. A more radical step towards building system with better \u201creal world understanding\u201d could arise from multimodal learners designed to aggregate audio, video and text modalities, from movies from instance. Promising results have already been obtained along this path [15].", "I would like here to thank Thomas Scialom, researcher at ReciTAL, who kindly share his knowledge with me by pointing my attention to his Summarizing Summarization page on GitHub [16]. This helped me kick start my exploration of Deep Learning summarization models.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "I am currently scientific director at onepoint. My main interests are in Deep Learning, NLP and general Data Science. I have a PhD in theoretical physics."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F4c2b89f2a9ea&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-models-for-automatic-summarization-4c2b89f2a9ea&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-models-for-automatic-summarization-4c2b89f2a9ea&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-models-for-automatic-summarization-4c2b89f2a9ea&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-models-for-automatic-summarization-4c2b89f2a9ea&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----4c2b89f2a9ea--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4c2b89f2a9ea--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@pirminlemberger?source=post_page-----4c2b89f2a9ea--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@pirminlemberger?source=post_page-----4c2b89f2a9ea--------------------------------", "anchor_text": "Pirmin Lemberger"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Faa1fd8f69eaf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-models-for-automatic-summarization-4c2b89f2a9ea&user=Pirmin+Lemberger&userId=aa1fd8f69eaf&source=post_page-aa1fd8f69eaf----4c2b89f2a9ea---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4c2b89f2a9ea&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-models-for-automatic-summarization-4c2b89f2a9ea&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4c2b89f2a9ea&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-models-for-automatic-summarization-4c2b89f2a9ea&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://photos.app.goo.gl/79pnDc5Yqx1EuoTP9", "anchor_text": "source"}, {"url": "https://arxiv.org/abs/2005.11988", "anchor_text": "PDF version on arXiv"}, {"url": "https://en.wikipedia.org/wiki/PageRank", "anchor_text": "PageRank algorithm"}, {"url": "https://searchengineland.com/welcome-bert-google-artificial-intelligence-for-understanding-search-queries-323976", "anchor_text": "semantic processing"}, {"url": "https://arxiv.org/abs/1807.08000", "anchor_text": "short product descriptions"}, {"url": "https://blog.frase.io/20-applications-of-automatic-summarization-in-the-enterprise/", "anchor_text": "here"}, {"url": "https://arxiv.org/abs/1906.03741", "anchor_text": "1"}, {"url": "https://arxiv.org/abs/1906.01351", "anchor_text": "2"}, {"url": "https://arxiv.org/abs/1704.04368", "anchor_text": "3"}, {"url": "https://arxiv.org/abs/1905.06566", "anchor_text": "4"}, {"url": "https://arxiv.org/abs/1705.04304", "anchor_text": "5"}, {"url": "https://rxnlp.com/how-rouge-works-for-evaluation-of-summarization-tasks/#.Xqbe75MzbUJ", "anchor_text": "6"}, {"url": "https://github.com/abisee/cnn-dailymail", "anchor_text": "CNN / Daily Mail data set"}, {"url": "https://arxiv.org/abs/1906.03741", "anchor_text": "1"}, {"url": "https://arxiv.org/abs/1906.03741", "anchor_text": "BigPatent dataset"}, {"url": "https://arxiv.org/abs/1906.03741", "anchor_text": "1"}, {"url": "https://arxiv.org/abs/1906.01351", "anchor_text": "TalkSumm method"}, {"url": "https://arxiv.org/abs/1906.01351", "anchor_text": "2"}, {"url": "https://guillaumegenthial.github.io/sequence-to-sequence.html", "anchor_text": "Seq2Seq architectures"}, {"url": "https://colah.github.io/posts/2015-08-Understanding-LSTMs/", "anchor_text": "LSTM recurrent neural networks"}, {"url": "http://jalammar.github.io/illustrated-bert/", "anchor_text": "BERT"}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "Transformer"}, {"url": "https://arxiv.org/abs/1409.0473", "anchor_text": "attention mechanism"}, {"url": "https://arxiv.org/abs/1704.04368", "anchor_text": "3"}, {"url": "https://arxiv.org/abs/1704.04368", "anchor_text": "3"}, {"url": "https://arxiv.org/abs/1704.04368", "anchor_text": "source"}, {"url": "http://jalammar.github.io/illustrated-bert/", "anchor_text": "BERT model"}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "Transformer"}, {"url": "https://arxiv.org/abs/1905.06566", "anchor_text": "HIBERT"}, {"url": "https://arxiv.org/abs/1905.06566", "anchor_text": "4"}, {"url": "https://arxiv.org/abs/1705.04304", "anchor_text": "5"}, {"url": "http://incompleteideas.net/book/the-book-2nd.html", "anchor_text": "13"}, {"url": "http://incompleteideas.net/book/the-book-2nd.html", "anchor_text": "13"}, {"url": "https://arxiv.org/abs/1705.04304", "anchor_text": "5"}, {"url": "https://arxiv.org/abs/1704.04368", "anchor_text": "3"}, {"url": "https://recital.ai/en/", "anchor_text": "ReciTAL"}, {"url": "https://arxiv.org/abs/1909.01610", "anchor_text": "14"}, {"url": "https://arxiv.org/abs/1906.07901", "anchor_text": "15"}, {"url": "https://www.linkedin.com/in/tscialom/?originalSubdomain=fr", "anchor_text": "Thomas Scialom"}, {"url": "https://recital.ai/en/", "anchor_text": "ReciTAL"}, {"url": "https://github.com/recitalAI/summarizing_summarization", "anchor_text": "Summarizing Summarization"}, {"url": "https://github.com/recitalAI/summarizing_summarization", "anchor_text": "16"}, {"url": "https://arxiv.org/abs/1906.03741", "anchor_text": "BIGPATENT: A Large-Scale Dataset for Abstractive and Coherent Summarization"}, {"url": "https://arxiv.org/abs/1906.01351", "anchor_text": "TalkSumm: A Dataset and Scalable Annotation Method for Scientific Paper Summarization Based on Conference Talks"}, {"url": "https://arxiv.org/abs/1704.04368", "anchor_text": "Get To The Point: Summarization with Pointer-Generator Networks"}, {"url": "https://arxiv.org/abs/1905.06566", "anchor_text": "Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization"}, {"url": "https://arxiv.org/abs/1705.04304", "anchor_text": "A Deep Reinforced Model for Abstractive Summarization"}, {"url": "https://www.aclweb.org/anthology/W04-1013.pdf", "anchor_text": "ROUGE A Package for Automatic Evaluation of Summaries"}, {"url": "https://arxiv.org/abs/1612.00563", "anchor_text": "Self-critical Sequence Training for Image Captioning"}, {"url": "https://guillaumegenthial.github.io/sequence-to-sequence.html", "anchor_text": "Seq2Seq with Attention and Beam Search"}, {"url": "https://colah.github.io/posts/2015-08-Understanding-LSTMs/", "anchor_text": "Understanding LSTM Networks"}, {"url": "http://jalammar.github.io/illustrated-bert/", "anchor_text": "The Illustrated BERT, ELMo and Co."}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "The Illustrated Transformer"}, {"url": "https://arxiv.org/abs/1409.0473", "anchor_text": "Neural Machine Translation by Jointly Learning to Align and Translate"}, {"url": "http://incompleteideas.net/book/the-book-2nd.html", "anchor_text": "Reinforcement Learning: An Introduction"}, {"url": "https://arxiv.org/abs/1909.01610", "anchor_text": "Answers Unite! Unsupervised Metrics for Reinforced Summarization Models"}, {"url": "https://arxiv.org/abs/1906.07901", "anchor_text": "Multimodal Abstractive Summarization for How2 Videos"}, {"url": "https://github.com/recitalAI/summarizing_summarization", "anchor_text": "Summarizing Summarization"}, {"url": "https://medium.com/tag/nlp?source=post_page-----4c2b89f2a9ea---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/summarization?source=post_page-----4c2b89f2a9ea---------------summarization-----------------", "anchor_text": "Summarization"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----4c2b89f2a9ea---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----4c2b89f2a9ea---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4c2b89f2a9ea&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-models-for-automatic-summarization-4c2b89f2a9ea&user=Pirmin+Lemberger&userId=aa1fd8f69eaf&source=-----4c2b89f2a9ea---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4c2b89f2a9ea&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-models-for-automatic-summarization-4c2b89f2a9ea&user=Pirmin+Lemberger&userId=aa1fd8f69eaf&source=-----4c2b89f2a9ea---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4c2b89f2a9ea&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-models-for-automatic-summarization-4c2b89f2a9ea&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4c2b89f2a9ea--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F4c2b89f2a9ea&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-models-for-automatic-summarization-4c2b89f2a9ea&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----4c2b89f2a9ea---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----4c2b89f2a9ea--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----4c2b89f2a9ea--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----4c2b89f2a9ea--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----4c2b89f2a9ea--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----4c2b89f2a9ea--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----4c2b89f2a9ea--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----4c2b89f2a9ea--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----4c2b89f2a9ea--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@pirminlemberger?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@pirminlemberger?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Pirmin Lemberger"}, {"url": "https://medium.com/@pirminlemberger/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "86 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Faa1fd8f69eaf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-models-for-automatic-summarization-4c2b89f2a9ea&user=Pirmin+Lemberger&userId=aa1fd8f69eaf&source=post_page-aa1fd8f69eaf--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F4e1867e29c1b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-models-for-automatic-summarization-4c2b89f2a9ea&newsletterV3=aa1fd8f69eaf&newsletterV3Id=4e1867e29c1b&user=Pirmin+Lemberger&userId=aa1fd8f69eaf&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}