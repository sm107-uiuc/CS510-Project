{"url": "https://towardsdatascience.com/customer-churn-prediction-with-pyspark-on-ibm-watson-studio-aws-and-databricks-de57a2ffb25b", "time": 1682994862.365961, "path": "towardsdatascience.com/customer-churn-prediction-with-pyspark-on-ibm-watson-studio-aws-and-databricks-de57a2ffb25b/", "webpage": {"metadata": {"title": "Customer Churn Prediction with PySpark on IBM Watson Studio, AWS and Databricks | by Josh Xin Jie Lee | Towards Data Science", "h1": "Customer Churn Prediction with PySpark on IBM Watson Studio, AWS and Databricks", "description": "According to the article by Harvard Business Review, acquiring a new customer can be 5 to 25 times more expensive than retaining an existing one. In fact, a research conducted by Frederick Reichheld\u2026"}, "outgoing_paragraph_urls": [{"url": "https://github.com/joshxinjie/sparkify_customer_churn", "anchor_text": "here", "paragraph_index": 0}, {"url": "https://hbr.org/2014/10/the-value-of-keeping-the-right-customers", "anchor_text": "article", "paragraph_index": 2}, {"url": "http://www2.bain.com/Images/BB_Prescription_cutting_costs.pdf", "anchor_text": "research", "paragraph_index": 3}, {"url": "https://github.com/joshxinjie/sparkify_customer_churn", "anchor_text": "GitHub", "paragraph_index": 18}, {"url": "https://dataplatform.cloud.ibm.com/", "anchor_text": "home", "paragraph_index": 23}, {"url": "https://aws.amazon.com/emr/", "anchor_text": "EMR", "paragraph_index": 45}, {"url": "https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-jupyterhub-install-kernels-libs.html", "anchor_text": "link", "paragraph_index": 66}, {"url": "https://www.youtube.com/watch?v=J6CFXgyKWQ4&list=WL&index=36&t=56s", "anchor_text": "video", "paragraph_index": 66}, {"url": "https://databricks.com/product/faq/community-edition", "anchor_text": "here", "paragraph_index": 68}, {"url": "https://community.cloud.databricks.com/login.html", "anchor_text": "login", "paragraph_index": 73}, {"url": "https://github.com/joshxinjie/sparkify_customer_churn", "anchor_text": "code", "paragraph_index": 137}, {"url": "https://github.com/joshxinjie/sparkify_customer_churn", "anchor_text": "here", "paragraph_index": 149}], "all_paragraphs": ["The code accompanying this article can be found here.", "Customer churn refers to the situation when customers stop doing business with a company.", "According to the article by Harvard Business Review, acquiring a new customer can be 5 to 25 times more expensive than retaining an existing one.", "In fact, a research conducted by Frederick Reichheld of Bain & Company shown that increasing customer retention rates by 5% can increase profits by 25% to 95%.", "Hence, it should be a company\u2019s priority to minimize the rate of customer churn. If we can successfully predict customers who will churn in advance, we can entice them to stay by providing discounts and incentives.", "In this article, we will tackle a customer churn prediction problem for a fictitious digital music service called Sparkify. We will train our predictive model on a large dataset (~12GB) of customer\u2019s activities on the service and attempt to predict customers who will churn based on their past behaviors.", "Since this dataset is too large to fit on a single computer, we will be using Apache Spark to help us analyze this dataset.", "Apache Spark is one of the most popular big data distributed processing frameworks (using multiple computers) in the world. Spark is significantly faster than Hadoop MapReduce, and it has a user-friendly API that can be accessed through a number of popular languages: Scala, Java, Python and R.", "For this task, we will be using Spark\u2019s Python API, PySpark.", "PySpark provides two approaches to manipulate data frames: first of which is similar to Python\u2019s Pandas library, and the other using SQL queries.", "Spark ML is the data frame based API for Spark\u2019s Machine Learning library, and it provides users with popular machine learning algorithms such as Linear Regression, Logistic Regression, Random Forests, Gradient-Boosted Tress, etc.", "Setting up PySpark on your local computer can be tricky. Running PySpark on cloud services will simplify the setup process, but may incur some costs in the process.", "Ideally, you should create the model\u2019s prototype on your local machine with a smaller dataset to save cost, and then transfer the code to a cloud-based service when you are ready to analyze the larger dataset. Ultimately, you will need a cloud service to take full advantage of Spark\u2019s distributed computing framework.", "An alternative approach will be to set up an account with Databricks and use their Community Edition to build your model\u2019s prototype. The Databricks Community Edition provides a free 6GB micro-cluster as well as a cluster manager and notebook environment. The best part is that the access is not time-limited!", "If you are interested in reproducing the code or trying out the dataset, I have included the setup instructions for three cloud services, IBM Studio Watson, Amazon AWS and Databricks.", "Sparkify has a free-tier and a premium subscription plan, and customers can cancel or downgrade from premium to free-tier at any time.", "We will define a customer churn event as downgrading from premium to free tier or a cancellation of service.", "Ideally, we want to predict churn events using data from the past to avoid any look-ahead biases. There are two months of customer activity data available. Hence, we will build a model to predict if users will churn in the second month using their behavioral data from the first month.", "The full dataset is 12 GB. Alternatively, you can try out smaller instances of the dataset, and I have included the datasets\u2019 download links on my GitHub page.", "I will now cover the setup instructions for the three cloud computing services: IBM Watson Studio, AWS and Databricks. Feel free to skip them and continue reading on \u2018Dataset\u2019 if you wish.", "One of the easiest way to set up and run Spark will be through the IBM Watson Studio platform. It has a user-friendly interface and there is a free \u201cLite Plan\u201d available.", "You will be given 50 capacity units per month with the \u201cLite Plan\u201d. The default Spark Python 3.5 environment will consume 1.5 capacity units per hour, giving you approximately 33 hours to work on a project.", "To setup IBM Watson Studio, you need to register for an IBM Cloud account if you do not have one.", "Next, log on to the IBM Watson Studio home page and log in.", "After logging in, you will be taken to this page. Select \u201cCreate a project\u201d.", "Next, hover around \u201cData Science\u201d and click \u201cCreate Project\u201d.", "Enter a name for your project, and select \u201cCreate\u201d.", "Select \u201cNotebook\u201d for your asset type.", "Give a name to the notebook and select \u201cDefault Spark Python 3.5XS (Driver with 1 vCPU and 4 GB RAM, 2 executors with 1 vCPU and 4 GB RAM each)\u201d. Next, click \u201cCreate Notebook\u201d.", "This will create a new notebook where you can start coding.", "To insert a data file, select \u201cFind and add data\u201d icon in the top right hand corner. Simply drag and drop your desired data file into the box.", "To create a new Spark Session and read in the data file, select \u201cinsert to code\u201d and click on \u201cInsert SparkSession DataFrame\u201d.", "This will generate a pre-written cell.", "You can uncomment the last two lines to read in the data file. Feel free to change the name of the data frame.", "You can now build your project!", "Should your current cluster terminate for any reason, you can always reconnect your pre-existing notebook to a new cluster and continue your work.", "After you have completed your project, you should stop your environment and delete your notebook and data files to avoid any unexpected charges.", "To stop your environment, click on \u201cEnvironment\u201d tab on top of your project page. Click on the three dots on the right of your active environment and select \u201cStop\u201d.", "Next, head over to the \u201cAssets\u201d tab on top of your project page. Click on the three dots to the right of your data file and select \u201cRemove\u201d. Repeat this for your notebook as well.", "You can check your billing information by selecting \u201cManage\u201d and then \u201cBilling and usage\u201d.", "An advantage of using Spark on the IBM Watson Studio platform is that it comes pre-installed with commonly used libraries, such as Pandas and Matplotlib, etc. This is in contrast with AWS EMR service, which does not have these libraries pre-installed.", "If your IBM hosted notebook ceased running or your internet browser crashed (yes it did happen), it probably meant that your current setup on IBM Watson Studio have insufficient memory to tackle the task. You might have to choose a simpler model, apply dimensionality reduction to your dataset, or purchase a paid plan to access a more powerful instance.", "I will now share the setup instructions for Amazon Web Services Elastic MapReduce (EMR).", "To begin. you need to sign up for an AWS account. You will be required to provide a credit card when signing up, but you would not be charged for anything yet.", "You will need to choose a support plan, and the free Basic Support Plan should suffice.", "Next, go to the Amazon EMR console and click \u201cGet Started with Amazon EMR\u201d. Once you have signed in with your account, you are ready to create your clusters.", "Choose the appropriate location on the top right hand corner (Pick the one closest to you). Select \u201cClusters\u201d on the menu on the left and then click \u201cCreate cluster\u201d.", "Configure your clusters with the following settings:", "If you want to run the code as it is, you are recommended to use 6 instances of m3.xlarge. Of course, you can try a lower number of instances, (3 for instance). However, if you encounter errors such as \u201cSession isn\u2019t active\u201d, it probably means that your current cluster setup have insufficient memory for the task. You will need to create a new cluster with larger instances.", "The rest of the settings can be kept at their default values, and you can finalize the setup by clicking \u201cCreate cluster\u201d.", "Next, you will be brought to a page that shows that the cluster is \u201cStarting\u201d. The status will then change to \u201cRunning\u201d after a few minutes. Finally, it will change to \u201cWaiting\u201d. The whole process can take somewhere between 3 to 10 minutes. At this point, you can move on to the next step.", "Finally, you can create your notebook.", "Next, wait for the status of the notebook to change from \u201cStarting\u201d or \u201cPending\u201d to \u201cReady\u201d. At this point, you can \u201copen\u201d the notebook.", "The starter code to create a new Spark session and read in the full dataset is provided below:", "This is what you will see after running the code.", "If you terminated your cluster and stopped your notebook, and you want to re-run the notebook, you can create a new cluster and re-connect your notebook to the newly cluster.", "Follow the previously given instructions to setup a new cluster and then return to the \u201cNotebooks\u201d option in the left menu. Click on the existing notebook you have created, select \u201cChange cluster\u201d and choose the newly created cluster. Finally select \u201cChange cluster and start notebook\u201d.", "To avoid any unexpected charges on AWS, terminate your clusters and delete your notebook when you have completed your analysis. You can check this in the \u201cClusters\u201d and \u201cNotebooks\u201d options in the left menu.", "If you have setup clusters on multiple locations, make sure to check on all of these locations as each location will have its own list of clusters!", "You can check your billing details under the \u201cMy Billing Dashboard\u201d option which can be found under your account name.", "When running the code on Amazon EMR, you might encounter these errors or exceptions:", "Do not fret if you see these errors. Generally, they do not mean that your code is faulty, and their occurrence will not affect the execution of your code. If they occur while you are executing a block of code, you can ignore these error messages. The code will still be executed.", "It costs me around US$20 to run the analysis on AWS EMR. That is taking into account of the fact that I had to re-run the analysis numerous times as my AWS hosted notebook crashed due to insufficient memory (I initially tried working with 3,4 and 5 instances of m3.xlarge).", "Hence, if you start with a sufficiently large cluster (6 instances of m3.xlarge for this project), you might be able to run the code once without encountering any issues. This could help lower your cost.", "In addition, I have developed the model\u2019s prototype on my local machine, so that helped lower the eventual cost.", "One side note is that AWS EMR clusters do not have libraries such as Scikit-Learn, Pandas, Matplotlib pre-installed. You should be able to complete the project without these libraries, although you would not be able to perform any data visualizations.", "If you do want to use these libraries, you will need to install them by following the instructions provided by this link. This video would also be helpful.", "It is a somewhat complicated procedure that vaguely involves writing a bash script to install these libraries, uploading the script to an S3 bucket, and then instantiate a cluster with the script. Admittedly, I have not tried it out at the time of writing. If I do get around in installing these libraries, I might update this article in the future.", "In order to register for a Databricks account, you will need an AWS account. Databricks\u2019 platform depends on AWS for the cloud infrastructure. You can register for a Databricks account here.", "The registration process for Databricks is slightly longer than the other two aforementioned cloud services and it involves shuttling between your AWS account and Databricks account setup page.", "However, once you have completed the set up and activated your account, it is relatively easy to set up the work environment in Databricks.", "In addition, the Community Edition offers you free unlimited access to a 6 GB cluster (no need to worry about time limit!), making it a perfect environment to build your model\u2019s prototype.", "Likewise with IBM Watson Studio, the work environment of Databricks comes with commonly used libraries such as Pandas, Matplotlib pre-installed.", "Once you have created your account, head over to the login page of Databricks Community Edition.", "You will be greeted by a similar page.", "Select \u201cClusters\u201d on the left menu.", "Select \u201c+ Create Cluster\u201d on the top left hand corner of the page.", "Give a name to your cluster. You can leave the Databricks Runtime Version unmodified. Select the Python version you want to work with. I chose Python 3 for my project. You can leave the Availability Zone field blank. Select \u201cCreate Cluster\u201d when you are done.", "Wait for the status to change from \u201cPending\u201d to \u201cRunning\u201d.", "Next, select \u201cWorkspace\u201d on the left menu, then click on \u201cUsers\u201d. Select the arrow next to your email and click on \u201cCreate\u201d then \u201cNotebook\u201d.", "Give a name to your notebook, and you can keep the rest of the settings unmodified.", "Next, you will want to upload your dataset. Select \u201cData\u201d on the left menu, and click on \u201cAdd Data\u201d.", "Simply drag and drop the file you want to upload into the grey box. Do not leave the page when the file is uploading, otherwise you will have to re-do the whole process.", "When the upload is completed, select \u201cCreate table in Notebook\u201d.", "This will open an example notebook with a pre-written cell that has the code to read in your data. Copy the code in this cell and go back to the original notebook you created earlier and paste the code there.", "If you want to save the file, click on \u201cFile\u201d, then \u201cExport\u201d, then \u201ciPython Notebook\u201d.", "If you want to change a code cell into a markdown cell, type \u201c%md\u201d in the first line of the cell.", "If you have terminated your cluster, you can always create a new cluster and reconnect an existing notebook to the new cluster.", "Even though the Community Edition is a free service, it is still a good idea to terminate your clusters and delete your notebooks to avoid any unexpected AWS charges. You can check your clusters\u2019 status by clicking on \u201cClusters\u201d on the left menu.", "Do note that if you choose to use the full Databricks platform instead of the Community Edition, you will incur AWS charges.", "One of the key column to look out for in this dataset is the \u2018page\u2019 column. The \u2018page\u2019 column keeps track of what pages a user visits.", "This is a list of the pages that users can visit:", "Of these, the pages which we should pay attention to are perhaps: \u2018Next Song\u2019 which tracks the songs played by a user, \u2018Submit Downgrade\u2019 which tracks when a user submits a downgrade request and \u2018Cancellation Confirmation\u2019 which tracks when a user\u2019s cancellation request is confirmed.", "We will know when a customer churn by searching for a \u2018Submit Downgrade\u2019 or \u2018Cancellation Confirmation\u2019 page.", "Note that a user can visit the \u2018Downgrade\u2019 and \u2018Cancel\u2019 page but not submit a downgrade or cancellation request.", "The rest of the pages are relatively straightforward. They indicate that the user has visited the relevant page.", "As mentioned earlier, there are 2 months of data available.", "Reading in the data is slightly different depending on the cloud computing service you are using. Each service\u2019s method of reading in the dataset are shared above in the setup instructions.", "If you are reading in the data on your local computer, this is the code for the small dataset:", "This code assumes that the dataset is located in the same file directory as your code. You might need to change the path of the dataset depending on where you store your dataset.", "There are a couple of columns with null values. Notably, entries missing firstName, gender, lastName, location, registration and userAgent belong to users who did not log in or did not register.", "Since we do not know which users these entries are tracking and most of these entries are associated with the login or home page, these entries provide little value. Hence, we can discard them from our analysis.", "Pages that are not \u2018NextSong\u2019 have null values for artist, length and sessionId. All of these variables are only valid when songs are played (they are only non-null when page=\u2019NextSong\u2019), hence we can leave them unmodified.", "First, we transform the timestamps to datetime. The original timestamps are in milliseconds, so we have to divide them by 1000 before converting them.", "Next, we can label the months from 0 to N where N represents the total number of months available in the dataset. We can approximate the start date of the analysis as \u20182018\u201310\u201301 00:00:00\u2019.", "Replace the string variables for gender and level with binary (0 or 1) variables.", "We can define churn events as whenever users visit a \u2018Cancellation Confirmation\u2019 or \u2018Submit Downgrade\u2019 pages.", "Generating monthly statistics for each user:", "Concatenating these monthly statistics into a new data-frame:", "Next, generate 1-month lagged features. These will be used as input features for the model, rather than the current month statistics, since we do not want any look-ahead biases.", "The generated features that will be used for our predictive model are:", "All missing values are imputed with 0 since missing values generally indicate no page visits, no songs played, etc.", "The following exploratory data analysis was obtained from the medium-sized instance of the dataset. Similar trends should be observed in both the small and large instances of the dataset.", "Shown below are the boxplots of various user statistics from the first month grouped by whether these users churned during the second month.", "We are interested to see if there are any discernable patterns in past user behavior that could indicate whether a user will churn in the near future.", "A surprising observation is that users who churned in the second month seem to be far more active on the digital music service during their first month, as compared to users who did not churn. Churned customers consistently have higher page visits, played more songs, listened to more singers, gave more thumbs up and thumbs down, etc.", "Perhaps, this suggests that users who are likely to churn next month will often try to make full use of their service in the current month.", "In addition, we note that the number of registrations, the number of visits to the cancel page and the number of logins were missing.", "When a user registers or login, the system does not know the user\u2019s id. Hence no information on these activities were recorded for the customers.", "As for the missing statistic regarding page visits to the cancel page, I note that none of the users staying with the service in the second month visited the cancel page during the first month. It is likely that any user who visited the cancel page during the first month probably churned during the month, hence none of them were with the service during the second month.", "Premium users seem far more likely to churn as compared to free users. Note that churn in this case could mean either downgrading from premium to free tier or cancelling their account altogether.", "In addition, it seems that male users are slightly less likely to churn as compared to female users.", "The number of users who churned in the second month are far lower than the number of users who did not churn. Hence, we will need to use the appropriate metrics to evaluate our model to ensure that our results are not deceiving. We could also employ techniques to handle imbalanced datasets, such as SMOTE, to improve our model\u2019s ability to predict users who will churn.", "In conclusion, it is likely that most churn resulted from users downgrading from premium to free tier. I hypothesized that there was a lower introductory premium pricing plan for new premium users that did not require any long-term commitment. Many of these new premium users were likely unwilling to make a long-term commitment to the music service, and wanted to maximize their usage of the service during the month when they had premium service.", "Alternatively, it could also be that the premium users who churned found the service unsatisfactory after prolonged usage, and thus wanted to downgrade their subscription.", "From the exploratory data analysis, we note that past customer activities can provide a relatively good indication of whether they will churn in the near future. We will now build predictive models, with the aid of Spark ML library, to predict when these customers will potentially churn.", "Spark ML library requires input features to be numeric, so we have to convert all string variables to integers or floats.", "You can check the datatypes of the features by running all_data.persist(). In addition, ensure that there are no null or \u2018NaN\u2019 values in your input features.", "Next, we drop entries from month 0, since we do not have previous months\u2019 statistics to work with. We will drop the current month statistics and keep last month\u2019s lagged features and the churn labels.", "We will store the input features and the labels in a data frame model_data.", "As a recap, we will be predicting customer churn during the second month using their activity data from the first month. Hence, we will perform a simple train-test split on the second month to obtain the training and test datasets.", "Note that all input features for a data point have to be placed in a single vector called features. It is recommended to scale the input features as well. The column churn should be renamed as label. Spark ML library will look for the features and label vectors, so use this naming convention.", "We will be trying 3 types of models: logistic regression, linear support vector classifier and gradient-boosted trees (GBT).", "The test accuracy, f1-score, precision and recall will be reported. F1-score will be preferred since the labels are imbalanced.", "The precision tells us how precise our predictions are (of those predicted to churn, how many did churn), while recall indicates how well our model recalled (how many of the original churned customer did the model manage to find).", "The f1-score can be viewed as the weighted average of precision and recall.", "Since the number of customers who churned is smaller than the number of customers who did not churn, f1-score will provide a more accurate representation of the model\u2019s performance, as opposed to accuracy.", "A word of caution when using PySpark\u2019s MulticlassClassificationEvaluator to report the f1-score for a binary classification task, is that the in-built function will treat both labels 0 and 1 as separate classes and return the weighted f1-score of both classes. This will produce an overly optimistic score that can be deceiving. Hence, I opted to define my own function to calculate the f1-score. More details can be found in the code.", "We will be performing k-fold cross-validation with 5 folds, and the models will be optimized using the area under the PR curve.", "Using the area under the PR curve is preferred over using the area under the ROC curve, since the imbalance of classes in the dataset meant that using the area under the ROC curve will lead to an overly optimistic picture.", "The following results are obtained from training our model on the full dataset.", "The GBT model achieved the highest f1-score and the highest recall among the 3 models for the test dataset. However, it also has the lowest precision. Nonetheless, the GBT model maintained the best balance between precision and recall.", "On the other hand, the linear support vector classifier has the highest precision, but the lowest f1-score, mostly attributed to it\u2019s low recall .", "Ultimately, the model chosen for the task of predicting customer churn will depend on your company\u2019s needs.", "If you prioritize customer retention, and do not mind spending a little more on discounts and incentives to retain customers, you can opt for the GBT model since it is the best at recalling customers who will churn.", "In addition, the GBT model will also be ideal if you want to maintain a better balance between customer retention and the amount spent on discounts and incentives to retain them, since it also has the highest f1-score.", "However, if you want to minimize the amount spent on discounts and incentives to retain customers, the linear support vector classifier will be ideal since it\u2019s predictions are the most precise.", "In this article, we implemented a single model to predict when customers churn (downgrading from premium to free tier or canceling their accounts). With this single model, we managed to achieve an f1-score of approximately 0.4, a respectable score.", "An alternative approach that could improve f1-scores, would be to build separate models to predict the two events. There is the possibility that both events have different signals, and hence the use of separate models would lead to better results.", "The code accompanying this article can be found here.", "Thank you for reading this article! If you have any thoughts or feedback, leave a comment below or send me an email at leexinjie@gmail.com. I\u2019d love to hear from you!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Aspiring ML practitioner @ AI Singapore -> The only true reward is existence and non-existence, everything else is a corollary of that \ud83c\udf0e"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fde57a2ffb25b&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcustomer-churn-prediction-with-pyspark-on-ibm-watson-studio-aws-and-databricks-de57a2ffb25b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcustomer-churn-prediction-with-pyspark-on-ibm-watson-studio-aws-and-databricks-de57a2ffb25b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcustomer-churn-prediction-with-pyspark-on-ibm-watson-studio-aws-and-databricks-de57a2ffb25b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcustomer-churn-prediction-with-pyspark-on-ibm-watson-studio-aws-and-databricks-de57a2ffb25b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----de57a2ffb25b--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----de57a2ffb25b--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@joshxinjielee?source=post_page-----de57a2ffb25b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@joshxinjielee?source=post_page-----de57a2ffb25b--------------------------------", "anchor_text": "Josh Xin Jie Lee"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa26bab2a7024&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcustomer-churn-prediction-with-pyspark-on-ibm-watson-studio-aws-and-databricks-de57a2ffb25b&user=Josh+Xin+Jie+Lee&userId=a26bab2a7024&source=post_page-a26bab2a7024----de57a2ffb25b---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fde57a2ffb25b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcustomer-churn-prediction-with-pyspark-on-ibm-watson-studio-aws-and-databricks-de57a2ffb25b&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fde57a2ffb25b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcustomer-churn-prediction-with-pyspark-on-ibm-watson-studio-aws-and-databricks-de57a2ffb25b&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://github.com/joshxinjie/sparkify_customer_churn", "anchor_text": "here"}, {"url": "https://hbr.org/2014/10/the-value-of-keeping-the-right-customers", "anchor_text": "article"}, {"url": "http://www2.bain.com/Images/BB_Prescription_cutting_costs.pdf", "anchor_text": "research"}, {"url": "https://github.com/joshxinjie/sparkify_customer_churn", "anchor_text": "GitHub"}, {"url": "https://dataplatform.cloud.ibm.com/", "anchor_text": "home"}, {"url": "https://aws.amazon.com/emr/", "anchor_text": "EMR"}, {"url": "https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-jupyterhub-install-kernels-libs.html", "anchor_text": "link"}, {"url": "https://www.youtube.com/watch?v=J6CFXgyKWQ4&list=WL&index=36&t=56s", "anchor_text": "video"}, {"url": "https://databricks.com/product/faq/community-edition", "anchor_text": "here"}, {"url": "https://community.cloud.databricks.com/login.html", "anchor_text": "login"}, {"url": "https://github.com/joshxinjie/sparkify_customer_churn", "anchor_text": "code"}, {"url": "https://github.com/joshxinjie/sparkify_customer_churn", "anchor_text": "here"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----de57a2ffb25b---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----de57a2ffb25b---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/big-data?source=post_page-----de57a2ffb25b---------------big_data-----------------", "anchor_text": "Big Data"}, {"url": "https://medium.com/tag/predictive-analytics?source=post_page-----de57a2ffb25b---------------predictive_analytics-----------------", "anchor_text": "Predictive Analytics"}, {"url": "https://medium.com/tag/data-analytics?source=post_page-----de57a2ffb25b---------------data_analytics-----------------", "anchor_text": "Data Analytics"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fde57a2ffb25b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcustomer-churn-prediction-with-pyspark-on-ibm-watson-studio-aws-and-databricks-de57a2ffb25b&user=Josh+Xin+Jie+Lee&userId=a26bab2a7024&source=-----de57a2ffb25b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fde57a2ffb25b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcustomer-churn-prediction-with-pyspark-on-ibm-watson-studio-aws-and-databricks-de57a2ffb25b&user=Josh+Xin+Jie+Lee&userId=a26bab2a7024&source=-----de57a2ffb25b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fde57a2ffb25b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcustomer-churn-prediction-with-pyspark-on-ibm-watson-studio-aws-and-databricks-de57a2ffb25b&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----de57a2ffb25b--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fde57a2ffb25b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcustomer-churn-prediction-with-pyspark-on-ibm-watson-studio-aws-and-databricks-de57a2ffb25b&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----de57a2ffb25b---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----de57a2ffb25b--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----de57a2ffb25b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----de57a2ffb25b--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----de57a2ffb25b--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----de57a2ffb25b--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----de57a2ffb25b--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----de57a2ffb25b--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----de57a2ffb25b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@joshxinjielee?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@joshxinjielee?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Josh Xin Jie Lee"}, {"url": "https://medium.com/@joshxinjielee/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "221 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa26bab2a7024&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcustomer-churn-prediction-with-pyspark-on-ibm-watson-studio-aws-and-databricks-de57a2ffb25b&user=Josh+Xin+Jie+Lee&userId=a26bab2a7024&source=post_page-a26bab2a7024--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F896a0c77555f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcustomer-churn-prediction-with-pyspark-on-ibm-watson-studio-aws-and-databricks-de57a2ffb25b&newsletterV3=a26bab2a7024&newsletterV3Id=896a0c77555f&user=Josh+Xin+Jie+Lee&userId=a26bab2a7024&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}