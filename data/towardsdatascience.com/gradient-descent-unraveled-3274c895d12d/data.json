{"url": "https://towardsdatascience.com/gradient-descent-unraveled-3274c895d12d", "time": 1683016452.808715, "path": "towardsdatascience.com/gradient-descent-unraveled-3274c895d12d/", "webpage": {"metadata": {"title": "Gradient Descent Unraveled. Understanding how gradient descent\u2026 | by Manpreet Singh Minhas | Towards Data Science", "h1": "Gradient Descent Unraveled", "description": "In the current era of Deep Learning you might have heard the term gradient descent before. If you didn\u2019t understand what it is and how it works, this post is for you."}, "outgoing_paragraph_urls": [{"url": "https://docs.sympy.org/latest/tutorial/basic_operations.html", "anchor_text": "sympy", "paragraph_index": 35}, {"url": "https://github.com/msminhas93/GradientDescentTutorial", "anchor_text": "https://github.com/msminhas93/GradientDescentTutorial", "paragraph_index": 40}, {"url": "https://medium.com/@lipeng2/cyclical-learning-rates-for-training-neural-networks-4de755927d46", "anchor_text": "here.", "paragraph_index": 58}, {"url": "https://github.com/msminhas93", "anchor_text": "https://github.com/msminhas93", "paragraph_index": 62}, {"url": "https://www.linkedin.com/in/msminhas93", "anchor_text": "https://www.linkedin.com/in/msminhas93", "paragraph_index": 62}], "all_paragraphs": ["In the current era of Deep Learning, you might have heard the term gradient descent before. If you didn\u2019t understand what it is and how it works, this post is for you. In this post, I\u2019ll be explaining what is it and how it works.", "First, let us begin with the concepts of maxima, minima, global and local.", "I\u2019ll explain these concepts for functions of a single variable because they are easy to visualize. However, they extend to multivariate cases.", "Let us start with a few definitions. [1]", "Graphics tend to make the concepts easier to understand. I\u2019ve summarized these four type of points in the following figure.", "As the name suggests minimum is the lowest value in a set and maximum is the highest value. Global means it is true for the entire set and local means it is true in some vicinity. A function can have multiple local maxima and minima. However there can be only one global maximum as well as minimum. Note that for Figures (a) and (b) the function domain is restricted to the values you are seeing. If it were to be infinite then there is no global minimum for the graph in Figure (a).", "Now that we understand these concepts, the next step is how to find these extremum points.", "Turns out derivatives in calculus are useful for finding these points. I won\u2019t be going into the details of derivatives. However, I\u2019ll explain enough to understand the following discussion.", "Derivative gives you a rate of change of something with respect to something. For example, how quickly a medicine would be absorbed by your system can be modeled and analysed using calculus.", "Now, let us understand what is a critical point.", "So we know that at these critical points there will be a either a local or global maximum or minimum. The next step is to identify which category it belongs to.", "You can use either of the two tests i.e. the first and second derivative test to classify the maximum and minimum values. When I was in my high school I used to find the second derivative test faster since I\u2019d calculate only one value (without using a calculator). I\u2019ll show you one example of how it is actually done.", "For finding whether the point is global you\u2019ll have to evaluate the function at all the critical points and see which point is the lowest. In our examples, we have seen a polynomial function. It is smooth and differentiable. There were limited points to test for and evaluating the function is easy if you have the equation.", "However, now let us move to the real world. We never know the actual equation of the real life processes that we deal with. Additionally, there are several variables involved in the equation. These tests won\u2019t be useful in those cases. For training a neural network you need to minimize the loss with respect to the network parameters. This is a multi-dimensional surface and multiple factors come into play. And the tests I discussed above won\u2019t be effective. So we turn to optimization for this task.", "Maximizing or minimizing some function relative to some set, often representing a range of choices available in a certain situation. The function allows a comparison of the different choices for determining which might be \u201cbest.\u201d", "Common applications: Minimal cost, maximal profit, minimal error, optimal design, optimal management, variational principles.", "In mathematics, computer science and operations research, mathematical optimization or mathematical programming is the selection of the best element (with regard to some criterion) from some set of available alternatives.", "Optimization is a vast ocean in itself and is extremely interesting. In the context of deep learning the optimization objective is to minimize the cost function with respect to the model parameters i.e. the weight matrices.", "Gradient: In vector calculus, the gradient is the multi-variable generalization of the derivative. The gradient of a scalar function f(x\u2081, x\u2082, x\u2083, \u2026., x\u2099) [hereafter referred to as f] is denoted by \u2207 f, where \u2207 (the nabla symbol) is known as the del operator. It packages all the partial derivatives information into a vector.", "\u2207 f is a vector valued function and points in the direction of steepest ascent. Imagine if you are standing a some point in the input space of f, the gradient vector tells you which direction you should travel, to increase the value of f rapidly.", "This is a type of optimization technique where you move towards the minimum in an iterative manner.", "The steps involved are as follows:", "In equation form for deep learning applications it can be written as:", "\u03c1 is the learning rate and governs how much you dampen the gradient value while taking the step.", "One more point to mention here is that unless the function is convex the algorithm can get stuck at a local minimum instead of converging to the global minimum.", "In mathematics, a real-valued function defined on an n-dimensional interval is called convex (or convex downward or concave upward) if the line segment between any two points on the graph of the function lies above or on the graph. This means that the function itself has one minimum for the strictly convex case.", "In summary, the gradient descent is an optimization method that finds the minimum of an objective function by incrementally updating its parameters in the negative direction of the gradient of the function which is the direction of steepest descent.", "Let us look at few examples. We will start with a 1D case, since it is easiest to visualize. The equation used is the same as the one I used earlier in the maxima and minima section.", "I\u2019ll show how to manually implement the code in python using numpy and matplotlib.", "The graphs of various combinations of the start point and learning rates are shown in the following figure.", "The equation under consideration has two minima. We want to find the global minimum using the gradient descent algorithm. I\u2019ll be explaining the figures filled by row. The number of iterations is kept as 10 for all the examples. I\u2019ve varied the learning rate and the initial start point to show you how it affects the algorithm. First five plots show that if start point and the number of iterations is kept the same, larger the learning rate faster the algorithm descends. But in those cases, the algorithm got stuck at a local minimum. In plot seven we see that due to the large learning rate coupled with the steep slope at the start point the algorithm took a large jump to the global minimum.", "Even though large learning rates can cause faster convergence, there are chances of your algorithm moving away from the minimum because of the large effective step size. Take a look at the last figure, the starting point enabled the algorithm to go to the global minimum but the large learning rate caused the algorithm to diverge. One more case is shown in the second last figure. The point was initialized at a critical point which has a zero derivative at that point. No matter how large your learning rate or how many steps you take the algorithm won\u2019t move from that point.", "From the above examples, we see that learning rate has to be chosen such that it is not too low so that the gradient is over-damped or too high for the gradient to blow up the step size. The initialization of the start point is crucial can get you the global minimum or the local. The number of steps to be taken also matter.", "Now let us walk through a few 2D examples.", "The first examples are of a paraboloid and its equation is as follows.", "I\u2019ll be using sympy for this tutorial. This allows you to do symbolic computations on equations. Even if you don\u2019t know calculus, you can use the code to try out different equations and experiment around.", "Following code creates a function f1 using the symbolic representation of sympy and calculates the partial derivatives w.r.t. x and y to calculate the gradient. Which is then used to perform gradient descent by updating the x and y values using the appropriate components.", "The following figure shows two views of the 3D plot.", "One thing immediately evident from the plot is as the algorithm reaches near the minimum the gradient value decreases and therefore the step size decreases. As I mentioned at the start of the post, the technique remains the same for n dimensional case. The observations that we draw in lower dimensions can be almost always extended to the higher dimensional case with little or no modifications.", "To give you a flavor for different surfaces, following are two more examples of 2D gradient descent. As discussed earlier the initialization matters and will ultimately decide where you end up at the end.", "I\u2019ve made a python utility that lets you play around with the learning rate, a number of iterations and initial start point to see how these affect the algorithm performance for the 1D example. The code for this, as well as the 2D function plots, is available at the following repository.https://github.com/msminhas93/GradientDescentTutorial", "So far we\u2019ve seen the algorithm and taken 1D and 2D examples to analyse how their choice affects the convergence.", "In the context of deep learning gradient descent can be classified into following categories.", "Stochastic Gradient Descent: Suppose you want to minimize an objective function that is written as a sum of differentiable functions.", "Each term Q\u1d62 is usually associated with the i\u1d57\u02b0 data point.", "Standard gradient descent (batch gradient descent) is given as follows.", "where \u03b7 is the learning rate (step size).", "Stochastic Gradient Descent (SGD) considers only a subset of summand functions at every iteration. This can be effective for large-scale problems. The gradient of Q(w) is approximated by a gradient at a single example:", "This update needs to be done for each training example. Several passes might be necessary over the training set until the algorithm converges.", "Let us consider the following equation:", "The update rule for SGD then is as follows.", "Few points on the three variants.", "Choice of mini-batch size parameter b depends on your GPU, data-set and the model. A rule of thumb is to choose a size in multiples of 8. Generally, 32 is a good number to start with. If you choose your number too high or low then the algorithm can become slower. In the former case, computations could become slower because you are putting a lot of load on the GPU. While in the latter case, lower mini-batch size underutilizes your GPU. Finding the right balance for your case is important.", "If \u03b7 is too high, the algorithm diverges. If \u03b7 is too low, it makes the algorithm slow to converge.", "A common practice is to make \u03b7\u2099 a decreasing function of iteration number n. Following are two examples.", "One can use interval based learning rate schedule too. For example:", "where n is the iteration number.", "The first iterations cause large changes in w, while the later ones do only fine tuning.", "There is something called as cyclical learning rates where you use a periodic function for scheduling. You can read more about it here.", "This wraps up the discussion for Gradient Descent.", "Hope this was useful. Your suggestions, feedback and comments are welcome.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "DL/CV Research Engineer | MASc UWaterloo | Follow and subscribe for DL/ML content | https://github.com/msminhas93 | https://www.linkedin.com/in/msminhas93"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F3274c895d12d&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-unraveled-3274c895d12d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-unraveled-3274c895d12d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-unraveled-3274c895d12d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-unraveled-3274c895d12d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----3274c895d12d--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3274c895d12d--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://manpreetsinghminhas.medium.com/?source=post_page-----3274c895d12d--------------------------------", "anchor_text": ""}, {"url": "https://manpreetsinghminhas.medium.com/?source=post_page-----3274c895d12d--------------------------------", "anchor_text": "Manpreet Singh Minhas"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F35b4b2dadc4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-unraveled-3274c895d12d&user=Manpreet+Singh+Minhas&userId=35b4b2dadc4&source=post_page-35b4b2dadc4----3274c895d12d---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3274c895d12d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-unraveled-3274c895d12d&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3274c895d12d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-unraveled-3274c895d12d&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/gradient-and-directional-derivatives/v/gradient", "anchor_text": "https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/gradient-and-directional-derivatives/v/gradient"}, {"url": "https://docs.sympy.org/latest/tutorial/basic_operations.html", "anchor_text": "sympy"}, {"url": "https://github.com/msminhas93/GradientDescentTutorial", "anchor_text": "https://github.com/msminhas93/GradientDescentTutorial"}, {"url": "https://medium.com/@lipeng2/cyclical-learning-rates-for-training-neural-networks-4de755927d46", "anchor_text": "here."}, {"url": "https://en.wikipedia.org/wiki/Maxima_and_minima", "anchor_text": "https://en.wikipedia.org/wiki/Maxima_and_minima"}, {"url": "https://www.youtube.com/playlist?list=PLEAYkSg4uSQ0MAdcjYsKFEo3guPTuUH66", "anchor_text": "https://www.youtube.com/playlist?list=PLEAYkSg4uSQ0MAdcjYsKFEo3guPTuUH66"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----3274c895d12d---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/gradient-descent?source=post_page-----3274c895d12d---------------gradient_descent-----------------", "anchor_text": "Gradient Descent"}, {"url": "https://medium.com/tag/optimization?source=post_page-----3274c895d12d---------------optimization-----------------", "anchor_text": "Optimization"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----3274c895d12d---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/editors-pick?source=post_page-----3274c895d12d---------------editors_pick-----------------", "anchor_text": "Editors Pick"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3274c895d12d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-unraveled-3274c895d12d&user=Manpreet+Singh+Minhas&userId=35b4b2dadc4&source=-----3274c895d12d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3274c895d12d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-unraveled-3274c895d12d&user=Manpreet+Singh+Minhas&userId=35b4b2dadc4&source=-----3274c895d12d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3274c895d12d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-unraveled-3274c895d12d&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3274c895d12d--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F3274c895d12d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-unraveled-3274c895d12d&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----3274c895d12d---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----3274c895d12d--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----3274c895d12d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----3274c895d12d--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----3274c895d12d--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----3274c895d12d--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----3274c895d12d--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----3274c895d12d--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----3274c895d12d--------------------------------", "anchor_text": ""}, {"url": "https://manpreetsinghminhas.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://manpreetsinghminhas.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Manpreet Singh Minhas"}, {"url": "https://manpreetsinghminhas.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "343 Followers"}, {"url": "https://github.com/msminhas93", "anchor_text": "https://github.com/msminhas93"}, {"url": "https://www.linkedin.com/in/msminhas93", "anchor_text": "https://www.linkedin.com/in/msminhas93"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F35b4b2dadc4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-unraveled-3274c895d12d&user=Manpreet+Singh+Minhas&userId=35b4b2dadc4&source=post_page-35b4b2dadc4--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fffc3b82d74a1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-unraveled-3274c895d12d&newsletterV3=35b4b2dadc4&newsletterV3Id=ffc3b82d74a1&user=Manpreet+Singh+Minhas&userId=35b4b2dadc4&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}