{"url": "https://towardsdatascience.com/comprehensive-introduction-to-turing-learning-and-gans-part-1-81f6d02e644d", "time": 1682996066.584534, "path": "towardsdatascience.com/comprehensive-introduction-to-turing-learning-and-gans-part-1-81f6d02e644d/", "webpage": {"metadata": {"title": "Introduction to Turing Learning and GANs | by Matthew Stewart, PhD | Towards Data Science", "h1": "Introduction to Turing Learning and GANs", "description": "\u201cGenerative Adversarial Networks is the most interesting idea in the last 10 years in Machine Learning.\u201d \u2014 Yann LeCun, Director of AI Research at Facebook AI This three-part tutorial continues my\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/generating-images-with-autoencoders-77fd3a8dd368", "anchor_text": "here", "paragraph_index": 3}, {"url": "https://towardsdatascience.com/generating-images-with-autoencoders-77fd3a8dd368", "anchor_text": "Comprehensive Introduction to Autoencoders", "paragraph_index": 13}, {"url": "https://en.wikipedia.org/wiki/Turing_test", "anchor_text": "Turing test", "paragraph_index": 16}, {"url": "https://en.wikipedia.org/wiki/Universal_approximation_theorem", "anchor_text": "universal approximation theorem", "paragraph_index": 17}, {"url": "https://en.wikipedia.org/wiki/Ke_Jie", "anchor_text": "Ke Jie", "paragraph_index": 26}, {"url": "https://github.com/dribnet/plat", "anchor_text": "https://github.com/dribnet/plat", "paragraph_index": 74}, {"url": "https://arxiv.org/abs/1609.05158", "anchor_text": "https://arxiv.org/abs/1609.05158", "paragraph_index": 77}, {"url": "https://mpstewart.io", "anchor_text": "https://mpstewart.io", "paragraph_index": 95}], "all_paragraphs": ["\u201cGenerative Adversarial Networks is the most interesting idea in the last 10 years in Machine Learning.\u201d \u2014 Yann LeCun, Director of AI Research at Facebook AI", "Part 2 is up and can be found here:", "Part 3 is up and can be found here:", "This three-part tutorial continues my series on deep generative models. This topic on Turing learning and GANs is a natural extension to the previous topic on variational autoencoders (found here). We will see that GANs are largely superior to variational autoencoders, but are notoriously difficult to work with.", "Throughout this tutorial, we will tackle the following topics:", "There will be no coding in part 1 of the tutorial (otherwise this tutorial would be extremely long), part 2 will act as a continuation to the current tutorial and will go into the more advanced aspects of GANs, with a simple coding implementation used to generate celebrity faces. The third part of the tutorial will be a coding tutorial for applying VAEs, GANs, and VAE-GANs to generate celebrity faces, as well as anime images. Part two and part three will be published in the next week.", "GANs is a fast-moving topic, this tutorial covers the state-of-the-art advances in GANs as of April 2019. If you are reading after this date then beware, there have likely been developments in the field and changes to the rules of thumb.", "My aim is for this to be the most comprehensive and accessible tutorial on GANs available, if you have any recommendations for improving this article, please let me know.", "All related code can now be found in my GitHub repository:", "Hopefully, you are reading this because you know nothing (or relatively little) about GANs and how they work. In this section, I hope to get you excited about the potential of GANs and how they can be used to solve real-world problems, as well as to have a lot of fun generating fake celebrities, anime characters, etc.", "In previous articles, we focused on generating data using autoencoders. However, the images produced by this procedure were not of very high resolution. In this article, we look at a completely different approach to generating data that is like the training data.", "This technique lets us generate types of data that go far beyond what a VAE offers. The GAN is based on a smart idea where two different networks are pitted against one another, with the goal of getting one network to create new samples that are different from the training data, but still close enough that the other network cannot differentiate which are synthetic and which belong to the original training set.", "As before, we would like to construct our generative model, which we would like to train to generate lightcurves like these from scratch. A generative model, in this case, could be one large neural network that outputs lightcurves: samples from the model.", "If you are unfamiliar with the idea of generative models or variational autoencoders, you may first want to read my previous article, Comprehensive Introduction to Autoencoders.", "To give you an idea of how important GANs are right now in the academic world, look at the below figure showing the increasing amount of papers published in the field each month.", "It is highly likely that you have heard of generative adversarial networks before, but may not have heard of Turing learning. Essentially, Turing learning is the generalization of the procedure underlying a GAN.", "The word \u2018Turing\u2019 comes from the similarities to the Turing test, in which a computer tries to fool the system into thinking that it is a human. As we will see, this is analogous to the goals of the generator in a GAN, which tries to fool its \u2018adversary\u2019, the discriminator. The need for having a generalization of GANs stems from the fact that Turing learning can be performed with any form of generator or discriminator, not necessarily a neural network.", "The main reason that using neural networks is commonplace within Turing learning is the fact that a neural network is a universal function approximator. That is, we are able to use a neural network (assuming it has sufficient capacity, i.e., a large number of nodes) to \u2018learn\u2019 a non-linear mapping between the input and the output. This gives neural networks much more freedom than most methods, as they are guaranteed to converge for any non-linear function (given an infinite network capacity and infinite training data \u2014 see the universal approximation theorem for more information).", "There are no real constraints on what form the generator or discriminator takes, they do not even need to be of the same form. However, using anything other than a neural network may increase the bias of the model. As an example, one could use support vector machines for both the generator and discriminator; similarly, a support vector machine for the generator and a neural network for the discriminator.", "A large part of this tutorial (mostly in part 2) will look at generating anime images similar to those below, using a VAE, followed by a GAN, followed by a VAE-GAN (more on this one later).", "Now we will dive into the structure of the GAN more specifically. As we have discussed, the job of the generator is to make fake images (in the case of image analysis, at least) that look reminiscent of the training set. The discriminator looks at this fake image and tries to identify whether it is real or not. The loss function of both the generator and discriminator is highly dependent on how well the discriminator performs its job.", "After sufficient training, the generator will become better, and the images will begin to look more photorealistic.", "Schematically, we can represent the generator and discriminator as black box models, which are abstractions of some form of function. This function can (as always in machine learning) be approximated using our catch-all function approximators, neural networks.", "The input to the generator is noize z, and the generated sample will be the output of our generator function, G(z). This generated image is then added arbitrarily to our input data to the discriminator, which then performs a binary classification (i.e. fake or not fake) and is assigned a score based on whether the image was, in fact, fake or not.", "The loss functions for the generator and the discriminator look a little bit intimidating at first, but they are actually very simple. G(z) is the output of our generator, i.e. the fake image, D(G(z)) is the prediction from the discriminator on our fake data and m is the number of samples. We use the logarithm because it is more numerically stable as a loss function and we take the gradient of the loss function with respect to the parameters so that we can apply stochastic gradient descent.", "If this sounds like gibberish to you right now then fear not, a large part of this tutorial will go into explaining the updating and refinement of the generator and discriminator.", "The entire idea of GANs is predicated on game theory. For those of you unaware, game theory analyzes games in order to come up with ideal strategies for how to win. It has become relatively intertwined with artificial learning and is how computers were able to beat world champions in pretty much every board game that exists. The most recent and impressive of which is probably Go, where the AI AlphaGo was able to beat world champion Ke Jie.", "In some games, there are unbounded resources. For example, in a game of poker, the pot can theoretically get larger and larger without limit. For many games, the resources are bounded, meaning that a player can only win at another player\u2019s expense; this is known as a zero-sum game.", "Zero-sum game: Players compete for a fixed and limited pool of resources. Players compete for resources, claiming them and each player\u2019s total number of resources can change, but the total number of resources remain constant.", "In zero-sum games, each player can try to set things up so that the other player\u2019s best move is of as little advantage as possible. This is called a minimax, or minmax, technique.", "Our goal in training the GAN is to produce two networks that are each as good as they can be. In other words, we don\u2019t end up with a \u201cwinner.\u201d", "Instead, both networks have reached their peak ability given the other network\u2019s abilities to thwart it. Game theorists call this state a Nash equilibrium, where each network is at its best configuration with respect to the other. This idea is illustrated below.", "To see this what is happening in terms of the Nash equilibrium in the latent representation, below we have plotted the generator and discriminator, as well as their distributions, as a function of epoch. We see that there is a gradual convergence of the distributions.", "Spam filtering is a great way to think about how the generative adversarial network works. This is similar to how I described the variational autoencoder, but not exactly the same.", "Imagine you have a marketer called Gary who is trying to get spam emails through David\u2019s spam filter. David is allowed to classify emails as spam or not after they have been OK\u2019ed by the spam filter. Gary wants to get through as many spam emails as possible, and David would like as few as possible to get through. Ideally, we will eventually reach a Nash equilibrium from such a scenario (although I am sure most people would prefer not spam emails!).", "After receiving a bunch of emails, David can check to see how well the spam filter did, and can \u2018punish\u2019 the spam filter by telling it when it got false positives or false negatives.", "Assuming that Gary also knows which of his spam emails got through (perhaps he also sends them to himself to validate the success) then both David and Gary can see how well they did at their respective tasks in the form of a confusion matrix (below).", "After this, both of them can learn what went wrong and then learn from their mistakes. Gary will try a different approach which makes use of his prior successes, and David will see where the spam filter went wrong to try and improve the filtering mechanism.", "We can continuously repeat this procedure until we obtain some form of Nash equilibrium (or one of the two finds out the perfect way to win and \u2018spams\u2019 this method, resulting in modal collapse \u2014 more on that later).", "We can consider the confusion matrix and use this as the basis to improve our generator and our discriminator. For example, if the email is, in fact, a spam email and it is classified as fake, the generator is doing a poor job and must do better. The discriminator does not need to do anything in this sense, it has done its job.", "In the case of a false negative (the email was not spam but it was classified as spam), it is the discriminator that has been fooled. The discriminator must do better in this case, whereas the generator has done its job correctly and does not need to be improved.", "In the case of a false positive (classified as real email when it is, in fact, a spam email), it is once again the discriminator that is at fault. The discriminator must then be updated whilst the generator does not do anything.", "For a true negative (the email was not a spam email and it was not classified as a spam email), neither the generator and discriminator need to update, as neither did anything incorrect.", "The discriminator is very simple. It takes a sample as input, and its output is a single value that reports the network\u2019s confidence that the input is from the training set, rather than being a fake. There are not many restrictions on what the discriminator is.", "The generator takes as input a bunch of random numbers. If we build our generator to be deterministic, then the same input will always produce the same output. In that sense, we can think of the input values as latent variables. But here the latent variables weren\u2019t discovered by analyzing the input, as they were for the VAE. The random noise is not \u201crandom\u201d but represents (an email in our example) in the \u201clatent\u201d space.", "The process \u2014 known as a learning round \u2014 accomplishes three jobs:", "[1] The discriminator learns to identify features that characterize a real sample.", "[2] The discriminator learns to identify features that reveal a fake sample.", "[3] The generator learns how to avoid including the features that the discriminator has learned to spot.", "The final network will look something like the one below. To briefly summarise how this works, a random sample is taken from some prior distribution, which is fed into the generator to make some fake image. This fake image, along with the real data, is fed into the discriminator network, which then decides which data comes from the real data set, and which comes from the fake data generated from the prior distribution.", "We will now move onto network training to see more quantitatively and explicitly the training is performed.", "In terms of our networks, there are two networks we need to train. This becomes interesting because both networks have the same overall value function, but slightly different loss functions. The discriminator is trying to maximize the overall value function, whereas the generator seeks to minimize the discriminator\u2019s value function.", "The method of training involves the following:", "False negative (I: Real/D: Fake): In this case, we feed reals to the discriminator. The generator is not involved in this step at all. The error function here only involves the discriminator and if it makes a mistake the error drives a backpropagation step through the discriminator, updating its weights so that it will get better at recognizing reals.", "True negative (I: Fake/D: Fake): We start with random numbers going into the generator. The generator\u2019s output is fake. The error function gets a large value if this fake is correctly identified as fake, meaning that the generator got caught. Backpropagation goes through the discriminator (which is frozen) to the generator. The generator is then updated, so it can better learn how to fool the discriminator.", "False positives (I:Fake/D:Real): Here we generate a fake and punish the discriminator if it classifies it as real.", "To illustrate the training in a less abstract form, we will go through another example which is slightly more involved than the spam filtering example.", "We have our generated (fake) distribution which is produced by our generative model, and we have a known true distribution. There is an associated KL-divergence between the two because they are not identical distributions, meaning that our loss function is non-zero.", "The discriminator then sees the input from the generated and true distributions. If the discriminator decides the data is from the generator, this generates a loss function value which propagates back to the generator and is used to update the weights. Importantly, only one of the two networks is ever trained at the same time.", "The generator has now improved, and the data looks more reminiscent of the true distribution.", "However, the data is still not quite good enough to fool the discriminator, and so the generator weights are updated once again.", "The generated distribution has once again been updated, and now the discriminator has been fooled, it thinks the generated data is from the true distribution. Time to update the discriminator!", "The loss function is used to update the discriminator weights through backpropagation.", "This process continues (theoretically) until the generated distribution is indistinguishable from the true distribution and the networks reach Nash equilibrium.", "Once our network is built and trained, we can use the generator to produce images that are indistinguishable from the training images, such as the following example of a DC-GAN used on the standard MNIST dataset.", "One very interesting application with GANs is the addition or removal of different attributes, illustrated below where smiles are \u2018added\u2019 to images without changing other attributes. This can also be done in video editing, and in the future, it may even be possible to post-edit videos to remove or add different actors in a similar manner. Similar things are already being done in the world of DeepFakes (although many applications of this could be considered malicious in nature).", "This can also be done with other traits, such as sunglasses.", "The above idea is essentially how the horses to zebra transition images were obtained at the start of this article.", "To give you an idea of just how much this area has improved in the past few years, look at the evolution of GANs from 2014 to 2017 from the images below.", "Also, to give you an idea for how many \u2018flavors\u2019 on GAN exist, there are a lot. The GAN that we are producing by the end of the tutorial will be a DCGAN, although I will describe the Wasserstein GAN (WGAN) in more detail in part 2. I will also outline how GANs can be used for generating time series, not just images.", "The two main types of networks to construct are either fully connected (FC) GANs or Deep Convolutional GANs (DC-GANs). Which you use will depend on the training data you are submitting to the network. If you are using single data points, an FC network is more appropriate, and if you are using images, a DC-GAN is more appropriate. The difference architectures for the two networks are shown below.", "Some of the rules of thumb to consider when using GANs are:", "We will discuss these more in the following section on GANHACKs.", "[1] Normalize the inputs \u2014 Normalize the images between -1 and 1, and make sure to use tanh as the last layer of the generator output.", "[2] Use Spherical Z \u2014 Don\u2019t sample from a uniform distribution. When doing interpolations, do the interpolation via a great circle, rather than a straight line from point A to point B. I recommend looking at Tom White\u2019s Sampling Generative Networks reference code https://github.com/dribnet/plat which has more details about this.", "[3] Batch Normalization \u2014 Construct different mini-batches for real and fake images, i.e. each mini-batch needs to contain only all real images or all generated images. However, when batch normalization is not an option, an alternative is to use instance normalization (for each sample, subtract mean and divide by standard deviation).", "[4] Avoid Sparse Gradients: ReLU, MaxPool \u2014 the stability of the GAN game suffers (a lot) if you have sparse gradients. In general, leaky ReLU is good (in both the generator and discriminator).", "If you are not familiar with PixelShuffle, there is an entire paper about it that you can read here: https://arxiv.org/abs/1609.05158.", "[5] Use Soft and Noisy Labels \u2014 Label smoothing, i.e. if you have two target labels: Real=1 and Fake=0, then for each incoming sample, if it is real, then replace the label with a random number between 0.7 and 1.2, and if it is a fake sample, replace it with 0.0 and 0.3 (for example). This is a recommendation from Salimans et. al. 2016. An alternative is to make the labels noisy for the discriminator: occasionally flip the labels when training the discriminator.", "There are a lot of problems with GANs, but I will touch on the main ones in this section and will discuss these more in part 2.", "[1] Sensitivity \u2014 The biggest challenge to using GANs in practice is their sensitivity to both structure and parameters. If either the discriminator or generator gets better than the other too quickly, the other will never be able to catch up. Finding the right combination can be very challenging. Following the rules of thumb we discussed above is generally recommended when we\u2019re building a new GAN or DC-GAN.", "[2] Convergence \u2014 There is no proof that a GAN will converge. GANs do seem to perform very well most of the time when we find the right parameters, but there\u2019s no guarantee beyond that. The more complicated the network gets, the more finicky the convergence becomes and the more difficult hyperparameter selection becomes.", "[3] Big Samples \u2014 Trying to train a GAN generator to produce large images, such as 1000x1000 pixels can be problematic. The problem is that with large images, it\u2019s easy for the discriminator to tell the generated fakes from the real images. Many pixels can lead to error gradients that cause the generator\u2019s output to move in almost random directions, rather than getting closer to matching the inputs. The best procedure for training GANs on large images is:", "This process takes much less time to complete than if we\u2019d trained with only the full-sized images from the start (and is more likely to converge).", "[4] Computation \u2014 Compute power, memory, and time to process large numbers is already very high. Running the networks until realistic images are produced can require many hours or days of training, even with high-performance GPUs (this is why our final images are sub-standard compared to those in some papers). This is further exacerbated for more complicated networks, larger training sets, and larger images.", "[5] Modal Collapse \u2014 This is possibly the most frustrating problem that we encounter in GANs (apart from the 10-hour training times). Let\u2019s say I would like to use GAN to produce faces like the ones below from NVIDIA (shown below).", "However, when training our network, the generator somehow finds one image that fools the discriminator.", "A generator could then just produce that image every time independently of the input noise. The discriminator will always say it is real, so the generator has accomplished its goal and stops learning. However, the problem is that every sample made by the generator is identical.", "This problem of producing just one successful output over and over is called modal collapse.", "This is much more common when the system produces the same few outputs or minor variations of them. This is called partial modal collapse.", "This was a very long article but I hope you now have a very good intuition for how these networks work. As a reward for making it this far in the article, here is a Tweet from Ian Goodfellow, the creator of the original GAN, showing you an interesting situation where GANs can fail when trained on cat images, some of which are memes!", "If you are hungry for more, you can continue to part 2 of the tutorial:", "Part 3 of the tutorial can also be found here:", "Below is some further reading which includes code, interactive exercises, and some seminal papers in the field of GANs. Feel free to reach out to me if you would like more information, resources, etc.", "For updates on new blog posts and extra content, sign up for my newsletter.", "ML Postdoc @Harvard | Environmental + Data Science PhD @Harvard | ML consultant @Critical Future | Blogger @TDS | Content Creator @EdX. https://mpstewart.io"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F81f6d02e644d&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcomprehensive-introduction-to-turing-learning-and-gans-part-1-81f6d02e644d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcomprehensive-introduction-to-turing-learning-and-gans-part-1-81f6d02e644d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcomprehensive-introduction-to-turing-learning-and-gans-part-1-81f6d02e644d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcomprehensive-introduction-to-turing-learning-and-gans-part-1-81f6d02e644d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/@matthew_stewart?source=post_page-----81f6d02e644d--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----81f6d02e644d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@matthew_stewart?source=post_page-----81f6d02e644d--------------------------------", "anchor_text": "Matthew Stewart, PhD"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb89dbc0712c4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcomprehensive-introduction-to-turing-learning-and-gans-part-1-81f6d02e644d&user=Matthew+Stewart%2C+PhD&userId=b89dbc0712c4&source=post_page-b89dbc0712c4----81f6d02e644d---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----81f6d02e644d--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F81f6d02e644d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcomprehensive-introduction-to-turing-learning-and-gans-part-1-81f6d02e644d&user=Matthew+Stewart%2C+PhD&userId=b89dbc0712c4&source=-----81f6d02e644d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F81f6d02e644d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcomprehensive-introduction-to-turing-learning-and-gans-part-1-81f6d02e644d&source=-----81f6d02e644d---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/comprehensive-introduction-to-turing-learning-and-gans-part-2-fd8e4a70775", "anchor_text": "Advanced Topics in GANsWant to turn horses into zebras? Make DIY anime characters or celebrities? Generative adversarial networks (GANs) are\u2026towardsdatascience.com"}, {"url": "https://medium.com/@matthew_stewart/gans-vs-autoencoders-comparison-of-deep-generative-models-985cf15936ea", "anchor_text": "GANs vs. Autoencoders: Comparison of Deep Generative ModelsWant to turn horses into zebras? Make DIY anime characters or celebrities? Generative adversarial networks (GANs) are\u2026medium.com"}, {"url": "https://towardsdatascience.com/generating-images-with-autoencoders-77fd3a8dd368", "anchor_text": "here"}, {"url": "https://github.com/mrdragonbear/GAN-Tutorial", "anchor_text": "mrdragonbear/GAN-TutorialGitHub is home to over 50 million developers working together to host and review code, manage projects, and build\u2026github.com"}, {"url": "https://towardsdatascience.com/generating-images-with-autoencoders-77fd3a8dd368", "anchor_text": "Comprehensive Introduction to Autoencoders"}, {"url": "https://en.wikipedia.org/wiki/Turing_test", "anchor_text": "Turing test"}, {"url": "https://en.wikipedia.org/wiki/Universal_approximation_theorem", "anchor_text": "universal approximation theorem"}, {"url": "https://en.wikipedia.org/wiki/Ke_Jie", "anchor_text": "Ke Jie"}, {"url": "https://arxiv.org/pdf/1511.06434v2.pdf", "anchor_text": "https://arxiv.org/pdf/1511.06434v2.pdf"}, {"url": "https://arxiv.org/pdf/1511.06434v2.pdf", "anchor_text": "https://arxiv.org/pdf/1511.06434v2.pdf"}, {"url": "https://github.com/dribnet/plat", "anchor_text": "https://github.com/dribnet/plat"}, {"url": "https://arxiv.org/abs/1609.05158", "anchor_text": "https://arxiv.org/abs/1609.05158"}, {"url": "https://github.com/soumith/ganhacks", "anchor_text": "https://github.com/soumith/ganhacks"}, {"url": "https://towardsdatascience.com/comprehensive-introduction-to-turing-learning-and-gans-part-2-fd8e4a70775", "anchor_text": "Advanced Topics in GANsWant to turn horses into zebras? Make DIY anime characters or celebrities? Generative adversarial networks (GANs) are\u2026towardsdatascience.com"}, {"url": "https://medium.com/@matthew_stewart/gans-vs-autoencoders-comparison-of-deep-generative-models-985cf15936ea", "anchor_text": "GANs vs. Autoencoders: Comparison of Deep Generative ModelsWant to turn horses into zebras? Make DIY anime characters or celebrities? Generative adversarial networks (GANs) are\u2026medium.com"}, {"url": "https://mailchi.mp/6304809e49e7/matthew-stewart", "anchor_text": "Newsletter SubscriptionEnrich your academic journey by joining a community of scientists, researchers, and industry professionals to obtain\u2026mailchi.mp"}, {"url": "https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/biggan_generation_with_tf_hub.ipynb", "anchor_text": "https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/biggan_generation_with_tf_hub.ipynb"}, {"url": "https://www.jessicayung.com/explaining-tensorflow-code-for-a-convolutional-neural-network/", "anchor_text": "https://www.jessicayung.com/explaining-tensorflow-code-for-a-convolutional-neural-network/"}, {"url": "https://lilianweng.github.io/lil-log/2017/08/20/from-GAN-to-WGAN.html", "anchor_text": "https://lilianweng.github.io/lil-log/2017/08/20/from-GAN-to-WGAN.html"}, {"url": "https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html", "anchor_text": "https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html"}, {"url": "https://github.com/tensorlayer/srgan", "anchor_text": "https://github.com/tensorlayer/srgan"}, {"url": "https://junyanz.github.io/CycleGAN/", "anchor_text": "https://junyanz.github.io/CycleGAN/"}, {"url": "https://affinelayer.com/pixsrv/", "anchor_text": "https://affinelayer.com/pixsrv/"}, {"url": "https://tcwang0509.github.io/pix2pixHD/", "anchor_text": "https://tcwang0509.github.io/pix2pixHD/"}, {"url": "https://arxiv.org/pdf/1511.06434v2.pdf", "anchor_text": "https://arxiv.org/pdf/1511.06434v2.pdf"}, {"url": "https://arxiv.org/pdf/1701.07875.pdf", "anchor_text": "https://arxiv.org/pdf/1701.07875.pdf"}, {"url": "https://arxiv.org/pdf/1411.1784v1.pdf", "anchor_text": "https://arxiv.org/pdf/1411.1784v1.pdf"}, {"url": "https://arxiv.org/pdf/1506.05751.pdf", "anchor_text": "https://arxiv.org/pdf/1506.05751.pdf"}, {"url": "https://arxiv.org/pdf/1609.04802.pdf", "anchor_text": "https://arxiv.org/pdf/1609.04802.pdf"}, {"url": "https://arxiv.org/pdf/1703.10593.pdf", "anchor_text": "https://arxiv.org/pdf/1703.10593.pdf"}, {"url": "https://arxiv.org/pdf/1606.03657", "anchor_text": "https://arxiv.org/pdf/1606.03657"}, {"url": "https://arxiv.org/pdf/1704.00028.pdf", "anchor_text": "https://arxiv.org/pdf/1704.00028.pdf"}, {"url": "https://arxiv.org/pdf/1701.07875.pdf", "anchor_text": "https://arxiv.org/pdf/1701.07875.pdf"}, {"url": "https://arxiv.org/pdf/1609.03126.pdf", "anchor_text": "https://arxiv.org/pdf/1609.03126.pdf"}, {"url": "https://arxiv.org/pdf/1512.09300.pdf", "anchor_text": "https://arxiv.org/pdf/1512.09300.pdf"}, {"url": "https://arxiv.org/pdf/1605.09782v6.pdf", "anchor_text": "https://arxiv.org/pdf/1605.09782v6.pdf"}, {"url": "https://arxiv.org/pdf/1612.04357.pdf", "anchor_text": "https://arxiv.org/pdf/1612.04357.pdf"}, {"url": "https://arxiv.org/pdf/1710.10916.pdf", "anchor_text": "https://arxiv.org/pdf/1710.10916.pdf"}, {"url": "https://arxiv.org/pdf/1612.07828v1.pdf", "anchor_text": "https://arxiv.org/pdf/1612.07828v1.pdf"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----81f6d02e644d---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----81f6d02e644d---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----81f6d02e644d---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----81f6d02e644d---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----81f6d02e644d---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F81f6d02e644d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcomprehensive-introduction-to-turing-learning-and-gans-part-1-81f6d02e644d&user=Matthew+Stewart%2C+PhD&userId=b89dbc0712c4&source=-----81f6d02e644d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F81f6d02e644d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcomprehensive-introduction-to-turing-learning-and-gans-part-1-81f6d02e644d&user=Matthew+Stewart%2C+PhD&userId=b89dbc0712c4&source=-----81f6d02e644d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F81f6d02e644d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcomprehensive-introduction-to-turing-learning-and-gans-part-1-81f6d02e644d&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/@matthew_stewart?source=post_page-----81f6d02e644d--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----81f6d02e644d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb89dbc0712c4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcomprehensive-introduction-to-turing-learning-and-gans-part-1-81f6d02e644d&user=Matthew+Stewart%2C+PhD&userId=b89dbc0712c4&source=post_page-b89dbc0712c4----81f6d02e644d---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F20066c159638&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcomprehensive-introduction-to-turing-learning-and-gans-part-1-81f6d02e644d&newsletterV3=b89dbc0712c4&newsletterV3Id=20066c159638&user=Matthew+Stewart%2C+PhD&userId=b89dbc0712c4&source=-----81f6d02e644d---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/@matthew_stewart?source=post_page-----81f6d02e644d--------------------------------", "anchor_text": "Written by Matthew Stewart, PhD"}, {"url": "https://medium.com/@matthew_stewart/followers?source=post_page-----81f6d02e644d--------------------------------", "anchor_text": "6.5K Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----81f6d02e644d--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://mpstewart.io", "anchor_text": "https://mpstewart.io"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb89dbc0712c4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcomprehensive-introduction-to-turing-learning-and-gans-part-1-81f6d02e644d&user=Matthew+Stewart%2C+PhD&userId=b89dbc0712c4&source=post_page-b89dbc0712c4----81f6d02e644d---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F20066c159638&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcomprehensive-introduction-to-turing-learning-and-gans-part-1-81f6d02e644d&newsletterV3=b89dbc0712c4&newsletterV3Id=20066c159638&user=Matthew+Stewart%2C+PhD&userId=b89dbc0712c4&source=-----81f6d02e644d---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/simple-guide-to-hyperparameter-tuning-in-neural-networks-3fe03dad8594?source=author_recirc-----81f6d02e644d----0---------------------fd089416_78b2_45fd_b1cb_ec96406ff79f-------", "anchor_text": ""}, {"url": "https://medium.com/@matthew_stewart?source=author_recirc-----81f6d02e644d----0---------------------fd089416_78b2_45fd_b1cb_ec96406ff79f-------", "anchor_text": ""}, {"url": "https://medium.com/@matthew_stewart?source=author_recirc-----81f6d02e644d----0---------------------fd089416_78b2_45fd_b1cb_ec96406ff79f-------", "anchor_text": "Matthew Stewart, PhD"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----81f6d02e644d----0---------------------fd089416_78b2_45fd_b1cb_ec96406ff79f-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/simple-guide-to-hyperparameter-tuning-in-neural-networks-3fe03dad8594?source=author_recirc-----81f6d02e644d----0---------------------fd089416_78b2_45fd_b1cb_ec96406ff79f-------", "anchor_text": "Simple Guide to Hyperparameter Tuning in Neural NetworksA step-by-step Jupyter notebook walkthrough on hyperparameter optimization."}, {"url": "https://towardsdatascience.com/simple-guide-to-hyperparameter-tuning-in-neural-networks-3fe03dad8594?source=author_recirc-----81f6d02e644d----0---------------------fd089416_78b2_45fd_b1cb_ec96406ff79f-------", "anchor_text": "15 min read\u00b7Jul 9, 2019"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3fe03dad8594&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimple-guide-to-hyperparameter-tuning-in-neural-networks-3fe03dad8594&user=Matthew+Stewart%2C+PhD&userId=b89dbc0712c4&source=-----3fe03dad8594----0-----------------clap_footer----fd089416_78b2_45fd_b1cb_ec96406ff79f-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/simple-guide-to-hyperparameter-tuning-in-neural-networks-3fe03dad8594?source=author_recirc-----81f6d02e644d----0---------------------fd089416_78b2_45fd_b1cb_ec96406ff79f-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "8"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3fe03dad8594&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimple-guide-to-hyperparameter-tuning-in-neural-networks-3fe03dad8594&source=-----81f6d02e644d----0-----------------bookmark_preview----fd089416_78b2_45fd_b1cb_ec96406ff79f-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----81f6d02e644d----1---------------------fd089416_78b2_45fd_b1cb_ec96406ff79f-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----81f6d02e644d----1---------------------fd089416_78b2_45fd_b1cb_ec96406ff79f-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----81f6d02e644d----1---------------------fd089416_78b2_45fd_b1cb_ec96406ff79f-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----81f6d02e644d----1---------------------fd089416_78b2_45fd_b1cb_ec96406ff79f-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----81f6d02e644d----1---------------------fd089416_78b2_45fd_b1cb_ec96406ff79f-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----81f6d02e644d----1---------------------fd089416_78b2_45fd_b1cb_ec96406ff79f-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----fd089416_78b2_45fd_b1cb_ec96406ff79f-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----81f6d02e644d----1---------------------fd089416_78b2_45fd_b1cb_ec96406ff79f-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----81f6d02e644d----1-----------------bookmark_preview----fd089416_78b2_45fd_b1cb_ec96406ff79f-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----81f6d02e644d----2---------------------fd089416_78b2_45fd_b1cb_ec96406ff79f-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----81f6d02e644d----2---------------------fd089416_78b2_45fd_b1cb_ec96406ff79f-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----81f6d02e644d----2---------------------fd089416_78b2_45fd_b1cb_ec96406ff79f-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----81f6d02e644d----2---------------------fd089416_78b2_45fd_b1cb_ec96406ff79f-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----81f6d02e644d----2---------------------fd089416_78b2_45fd_b1cb_ec96406ff79f-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----81f6d02e644d----2---------------------fd089416_78b2_45fd_b1cb_ec96406ff79f-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----2-----------------clap_footer----fd089416_78b2_45fd_b1cb_ec96406ff79f-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----81f6d02e644d----2---------------------fd089416_78b2_45fd_b1cb_ec96406ff79f-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----81f6d02e644d----2-----------------bookmark_preview----fd089416_78b2_45fd_b1cb_ec96406ff79f-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-actual-difference-between-statistics-and-machine-learning-64b49f07ea3?source=author_recirc-----81f6d02e644d----3---------------------fd089416_78b2_45fd_b1cb_ec96406ff79f-------", "anchor_text": ""}, {"url": "https://medium.com/@matthew_stewart?source=author_recirc-----81f6d02e644d----3---------------------fd089416_78b2_45fd_b1cb_ec96406ff79f-------", "anchor_text": ""}, {"url": "https://medium.com/@matthew_stewart?source=author_recirc-----81f6d02e644d----3---------------------fd089416_78b2_45fd_b1cb_ec96406ff79f-------", "anchor_text": "Matthew Stewart, PhD"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----81f6d02e644d----3---------------------fd089416_78b2_45fd_b1cb_ec96406ff79f-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-actual-difference-between-statistics-and-machine-learning-64b49f07ea3?source=author_recirc-----81f6d02e644d----3---------------------fd089416_78b2_45fd_b1cb_ec96406ff79f-------", "anchor_text": "The Actual Difference Between Statistics and Machine LearningNo, they are not the same. If machine learning is just glorified statistics, then architecture is just glorified sand-castle construction."}, {"url": "https://towardsdatascience.com/the-actual-difference-between-statistics-and-machine-learning-64b49f07ea3?source=author_recirc-----81f6d02e644d----3---------------------fd089416_78b2_45fd_b1cb_ec96406ff79f-------", "anchor_text": "15 min read\u00b7Mar 24, 2019"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F64b49f07ea3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-actual-difference-between-statistics-and-machine-learning-64b49f07ea3&user=Matthew+Stewart%2C+PhD&userId=b89dbc0712c4&source=-----64b49f07ea3----3-----------------clap_footer----fd089416_78b2_45fd_b1cb_ec96406ff79f-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-actual-difference-between-statistics-and-machine-learning-64b49f07ea3?source=author_recirc-----81f6d02e644d----3---------------------fd089416_78b2_45fd_b1cb_ec96406ff79f-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "35"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F64b49f07ea3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-actual-difference-between-statistics-and-machine-learning-64b49f07ea3&source=-----81f6d02e644d----3-----------------bookmark_preview----fd089416_78b2_45fd_b1cb_ec96406ff79f-------", "anchor_text": ""}, {"url": "https://medium.com/@matthew_stewart?source=post_page-----81f6d02e644d--------------------------------", "anchor_text": "See all from Matthew Stewart, PhD"}, {"url": "https://towardsdatascience.com/?source=post_page-----81f6d02e644d--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----81f6d02e644d----0---------------------171ca7f1_710c_44b6_a44f_8aba2dcdef6f-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----81f6d02e644d----0---------------------171ca7f1_710c_44b6_a44f_8aba2dcdef6f-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----81f6d02e644d----0---------------------171ca7f1_710c_44b6_a44f_8aba2dcdef6f-------", "anchor_text": "The PyCoach"}, {"url": "https://artificialcorner.com/?source=read_next_recirc-----81f6d02e644d----0---------------------171ca7f1_710c_44b6_a44f_8aba2dcdef6f-------", "anchor_text": "Artificial Corner"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----81f6d02e644d----0---------------------171ca7f1_710c_44b6_a44f_8aba2dcdef6f-------", "anchor_text": "You\u2019re Using ChatGPT Wrong! Here\u2019s How to Be Ahead of 99% of ChatGPT UsersMaster ChatGPT by learning prompt engineering."}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----81f6d02e644d----0---------------------171ca7f1_710c_44b6_a44f_8aba2dcdef6f-------", "anchor_text": "\u00b77 min read\u00b7Mar 17"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fartificial-corner%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&user=The+PyCoach&userId=fb44e21903f3&source=-----886a50dabc54----0-----------------clap_footer----171ca7f1_710c_44b6_a44f_8aba2dcdef6f-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----81f6d02e644d----0---------------------171ca7f1_710c_44b6_a44f_8aba2dcdef6f-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "275"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&source=-----81f6d02e644d----0-----------------bookmark_preview----171ca7f1_710c_44b6_a44f_8aba2dcdef6f-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/using-transformers-for-computer-vision-6f764c5a078b?source=read_next_recirc-----81f6d02e644d----1---------------------171ca7f1_710c_44b6_a44f_8aba2dcdef6f-------", "anchor_text": ""}, {"url": "https://wolfecameron.medium.com/?source=read_next_recirc-----81f6d02e644d----1---------------------171ca7f1_710c_44b6_a44f_8aba2dcdef6f-------", "anchor_text": ""}, {"url": "https://wolfecameron.medium.com/?source=read_next_recirc-----81f6d02e644d----1---------------------171ca7f1_710c_44b6_a44f_8aba2dcdef6f-------", "anchor_text": "Cameron R. Wolfe"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----81f6d02e644d----1---------------------171ca7f1_710c_44b6_a44f_8aba2dcdef6f-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/using-transformers-for-computer-vision-6f764c5a078b?source=read_next_recirc-----81f6d02e644d----1---------------------171ca7f1_710c_44b6_a44f_8aba2dcdef6f-------", "anchor_text": "Using Transformers for Computer VisionAre Vision Transformers actually useful?"}, {"url": "https://towardsdatascience.com/using-transformers-for-computer-vision-6f764c5a078b?source=read_next_recirc-----81f6d02e644d----1---------------------171ca7f1_710c_44b6_a44f_8aba2dcdef6f-------", "anchor_text": "\u00b713 min read\u00b7Oct 5, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6f764c5a078b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-transformers-for-computer-vision-6f764c5a078b&user=Cameron+R.+Wolfe&userId=28aa6026c553&source=-----6f764c5a078b----1-----------------clap_footer----171ca7f1_710c_44b6_a44f_8aba2dcdef6f-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/using-transformers-for-computer-vision-6f764c5a078b?source=read_next_recirc-----81f6d02e644d----1---------------------171ca7f1_710c_44b6_a44f_8aba2dcdef6f-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "4"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6f764c5a078b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-transformers-for-computer-vision-6f764c5a078b&source=-----81f6d02e644d----1-----------------bookmark_preview----171ca7f1_710c_44b6_a44f_8aba2dcdef6f-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----81f6d02e644d----0---------------------171ca7f1_710c_44b6_a44f_8aba2dcdef6f-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----81f6d02e644d----0---------------------171ca7f1_710c_44b6_a44f_8aba2dcdef6f-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----81f6d02e644d----0---------------------171ca7f1_710c_44b6_a44f_8aba2dcdef6f-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----81f6d02e644d----0---------------------171ca7f1_710c_44b6_a44f_8aba2dcdef6f-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----81f6d02e644d----0---------------------171ca7f1_710c_44b6_a44f_8aba2dcdef6f-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----81f6d02e644d----0---------------------171ca7f1_710c_44b6_a44f_8aba2dcdef6f-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----0-----------------clap_footer----171ca7f1_710c_44b6_a44f_8aba2dcdef6f-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----81f6d02e644d----0---------------------171ca7f1_710c_44b6_a44f_8aba2dcdef6f-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----81f6d02e644d----0-----------------bookmark_preview----171ca7f1_710c_44b6_a44f_8aba2dcdef6f-------", "anchor_text": ""}, {"url": "https://jehillparikh.medium.com/u-nets-with-attention-c8d7e9bf2416?source=read_next_recirc-----81f6d02e644d----1---------------------171ca7f1_710c_44b6_a44f_8aba2dcdef6f-------", "anchor_text": ""}, {"url": "https://jehillparikh.medium.com/?source=read_next_recirc-----81f6d02e644d----1---------------------171ca7f1_710c_44b6_a44f_8aba2dcdef6f-------", "anchor_text": ""}, {"url": "https://jehillparikh.medium.com/?source=read_next_recirc-----81f6d02e644d----1---------------------171ca7f1_710c_44b6_a44f_8aba2dcdef6f-------", "anchor_text": "Jehill Parikh"}, {"url": "https://jehillparikh.medium.com/u-nets-with-attention-c8d7e9bf2416?source=read_next_recirc-----81f6d02e644d----1---------------------171ca7f1_710c_44b6_a44f_8aba2dcdef6f-------", "anchor_text": "U-Nets with attentionU-Net are popular NN architecture which are employed for many applications and were initially developed for medical image segmentation."}, {"url": "https://jehillparikh.medium.com/u-nets-with-attention-c8d7e9bf2416?source=read_next_recirc-----81f6d02e644d----1---------------------171ca7f1_710c_44b6_a44f_8aba2dcdef6f-------", "anchor_text": "\u00b72 min read\u00b7Nov 15, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2Fc8d7e9bf2416&operation=register&redirect=https%3A%2F%2Fjehillparikh.medium.com%2Fu-nets-with-attention-c8d7e9bf2416&user=Jehill+Parikh&userId=c972081b627e&source=-----c8d7e9bf2416----1-----------------clap_footer----171ca7f1_710c_44b6_a44f_8aba2dcdef6f-------", "anchor_text": ""}, {"url": "https://jehillparikh.medium.com/u-nets-with-attention-c8d7e9bf2416?source=read_next_recirc-----81f6d02e644d----1---------------------171ca7f1_710c_44b6_a44f_8aba2dcdef6f-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc8d7e9bf2416&operation=register&redirect=https%3A%2F%2Fjehillparikh.medium.com%2Fu-nets-with-attention-c8d7e9bf2416&source=-----81f6d02e644d----1-----------------bookmark_preview----171ca7f1_710c_44b6_a44f_8aba2dcdef6f-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----81f6d02e644d----2---------------------171ca7f1_710c_44b6_a44f_8aba2dcdef6f-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----81f6d02e644d----2---------------------171ca7f1_710c_44b6_a44f_8aba2dcdef6f-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----81f6d02e644d----2---------------------171ca7f1_710c_44b6_a44f_8aba2dcdef6f-------", "anchor_text": "Alexander Nguyen"}, {"url": "https://levelup.gitconnected.com/?source=read_next_recirc-----81f6d02e644d----2---------------------171ca7f1_710c_44b6_a44f_8aba2dcdef6f-------", "anchor_text": "Level Up Coding"}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----81f6d02e644d----2---------------------171ca7f1_710c_44b6_a44f_8aba2dcdef6f-------", "anchor_text": "Why I Keep Failing Candidates During Google Interviews\u2026They don\u2019t meet the bar."}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----81f6d02e644d----2---------------------171ca7f1_710c_44b6_a44f_8aba2dcdef6f-------", "anchor_text": "\u00b74 min read\u00b7Apr 13"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fgitconnected%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&user=Alexander+Nguyen&userId=a148fd75c2e9&source=-----dc8f865b2c19----2-----------------clap_footer----171ca7f1_710c_44b6_a44f_8aba2dcdef6f-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----81f6d02e644d----2---------------------171ca7f1_710c_44b6_a44f_8aba2dcdef6f-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "89"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&source=-----81f6d02e644d----2-----------------bookmark_preview----171ca7f1_710c_44b6_a44f_8aba2dcdef6f-------", "anchor_text": ""}, {"url": "https://medium.com/mlearning-ai/understanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f?source=read_next_recirc-----81f6d02e644d----3---------------------171ca7f1_710c_44b6_a44f_8aba2dcdef6f-------", "anchor_text": ""}, {"url": "https://medium.com/@martin-thissen?source=read_next_recirc-----81f6d02e644d----3---------------------171ca7f1_710c_44b6_a44f_8aba2dcdef6f-------", "anchor_text": ""}, {"url": "https://medium.com/@martin-thissen?source=read_next_recirc-----81f6d02e644d----3---------------------171ca7f1_710c_44b6_a44f_8aba2dcdef6f-------", "anchor_text": "Martin Thissen"}, {"url": "https://medium.com/mlearning-ai?source=read_next_recirc-----81f6d02e644d----3---------------------171ca7f1_710c_44b6_a44f_8aba2dcdef6f-------", "anchor_text": "MLearning.ai"}, {"url": "https://medium.com/mlearning-ai/understanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f?source=read_next_recirc-----81f6d02e644d----3---------------------171ca7f1_710c_44b6_a44f_8aba2dcdef6f-------", "anchor_text": "Understanding and Coding the Attention Mechanism \u2014 The Magic Behind TransformersIn this article, I\u2019ll give you an introduction to the attention mechanism and show you how to code the attention mechanism yourself."}, {"url": "https://medium.com/mlearning-ai/understanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f?source=read_next_recirc-----81f6d02e644d----3---------------------171ca7f1_710c_44b6_a44f_8aba2dcdef6f-------", "anchor_text": "\u00b712 min read\u00b7Dec 6, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fmlearning-ai%2Ffe707a85cc3f&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fmlearning-ai%2Funderstanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f&user=Martin+Thissen&userId=f99c73950195&source=-----fe707a85cc3f----3-----------------clap_footer----171ca7f1_710c_44b6_a44f_8aba2dcdef6f-------", "anchor_text": ""}, {"url": "https://medium.com/mlearning-ai/understanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f?source=read_next_recirc-----81f6d02e644d----3---------------------171ca7f1_710c_44b6_a44f_8aba2dcdef6f-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffe707a85cc3f&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fmlearning-ai%2Funderstanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f&source=-----81f6d02e644d----3-----------------bookmark_preview----171ca7f1_710c_44b6_a44f_8aba2dcdef6f-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----81f6d02e644d--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----81f6d02e644d--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----81f6d02e644d--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----81f6d02e644d--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----81f6d02e644d--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----81f6d02e644d--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----81f6d02e644d--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----81f6d02e644d--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----81f6d02e644d--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----81f6d02e644d--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}