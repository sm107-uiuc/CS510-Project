{"url": "https://towardsdatascience.com/autoencoders-for-the-compression-of-stock-market-data-28e8c1a2da3e", "time": 1682994703.255181, "path": "towardsdatascience.com/autoencoders-for-the-compression-of-stock-market-data-28e8c1a2da3e/", "webpage": {"metadata": {"title": "Autoencoders for the compression of stock market time series | by Gerard Mart\u00ednez | Towards Data Science", "h1": "Autoencoders for the compression of stock market time series", "description": "Stock market data space is highly dimensional and, as such, algorithms that try to exploit potential patterns or structure in the price formation can suffer from the so-called \u201ccurse of\u2026"}, "outgoing_paragraph_urls": [{"url": "https://cryptodatum.io", "anchor_text": "CryptoDatum.io", "paragraph_index": 17}, {"url": "https://cryptodatum.io.", "anchor_text": "https://cryptodatum.io", "paragraph_index": 17}, {"url": "http://CryptoDatum.io", "anchor_text": "CryptoDatum.io", "paragraph_index": 19}], "all_paragraphs": ["Stock market data space is highly dimensional and, as such, algorithms that try to exploit potential patterns or structure in the price formation can suffer from the so-called \u201ccurse of dimensionality\u201d. In this short article, we will explore the potential of 4 different types of autoencoders to capture the dynamic information of stock market prices in a lower and traceable dimension space. To do so, we will use the Python programming language and, as an example, we will apply these algorithms to the compression of Bitcoin price time series. The code to build the neural network models (using the Keras library) and the full Jupyter notebook used is available at the end of the article.", "An autoencoder is a type of neural network in which the input and the output data are the same. As such, it is part of the so-called unsupervised learning or self-supervised learning because, unlike supervised learning, it requires no human intervention such as data labeling. The architecture of an autoencoder may vary, as we will see, but generally speaking it includes an encoder, that transforms the input into a lower dimensional representation, and a decoder, which tries to reconstruct the original input from the lower dimensional representation. Therefore, these models present some some sort of \u201cbottle neck\u201d in the middle that forces the network to learn how to compress the data in a lower dimensional space. When training these algorithms, the objective is to be able to reconstruct the original input with the minimum amount of information loss. Once the model is trained, we can compress data at will by only using the encoder component of the autoencoder.", "The data we are going to use is the Bitcoin time series consisting of 1-hour candlestick close prices of the Coindesk Bitcoin Price Index starting from 01/01/2015 until today. Specifically, we will use the first 93% of the data as a training dataset and the final 7% as test dataset. The bitcoin prices will be transformed to log returns (i.e. the log of the difference between the price x+1 and price x) and windows of 10 consecutive returns will be produced. Each of these windows of consecutive returns will be normalized with a MinMaxScaler to the range [0,1].", "The objective for the different autoencoder models is to be able to compress the input which is 10-dimensional to a 3-dimensional latent space. This constitutes a reduction factor of 3.3, which should be attainable with reasonably good accuracy.", "For each model tried we will show a summary of the model, the loss for the training and test datasets at each stage of the training epochs and, finally, the input and output of the autoencoder for 10 randomly selected price return windows extracted from the test dataset (i.e. the model has not seen these data points). The selected test windows will intentionally remain the same across all models to be able to compare which kind of features each model may be learning.", "The model used is super simple but the comparison between the input and the output reveal the ability of the network to abstract few important features such as peaks and lows. Interestingly, we can see that the some of the outputs are almost identical between each other even though the inputs are reasonably different.", "Despite the few more parameters we seem to reach a similar accuracy when looking at the train/test loss. However, the input/output examples show a different type of plots, the majority of them containing one single low or high peak unlike the previous results, which were much more variant in the middle range.", "This third model gets kind of interesting. In this model we are using convolutions with kernel size of 3 and the idea is that these convolutions should look at patterns occurring in groups of 3 returns. The results were surprising to me. In most of them, we can see the main \u201cevent\u201d very well represented while the overall reconstruction is very smooth as we had applied a moving average to the returns.", "While recurrent neural networks such as Long-Short Term Memory (LSTM) models are particularly suitable to tackle time series, we can see that their performance as autoencoders is very poor. Goodfellow et al. explain it succinctly well in their book \u201cDeep Learning\u201d:", "When the recurrent network is trained to perform a task that requires predicting the future from the past, the network typically learns to use h(t) [the compressed representation] as a kind of lossy summary of the task-relevant aspects of the past sequence of inputs up to t. This summary is in general necessarily lossy, since it maps an arbitrary length sequence to a fixed length vector h(t).", "The most demanding situation is when we ask h(t) to be rich enough to allow one to approximately recover the sequence, as in autoencoder frameworks.", "The idea behind an autoencoder is to reduce an original high dimensionality to a lower dimensionality. In our case, the values of this high dimensional space are continuous between 0 and 1 thanks to the normalization scheme that we are applying. However, one can think that if we discretize this 0 to 1 range to, say, 10 bins, all the sudden we are kind of categorizing down the whole 0\u20131 spectrum to 10 simple categories. Now, if our windows are 10 returns-long, using this \u201cdiscretized\u201d continuous space one could easily generate the 10 to the power of 10 different existing combinations or \u201cdiscretized time series\u201d. This \u201csynthethic\u201d dataset could be used as training dataset to almost de facto enrich our model and teach it to understand parts of the price space that were undersampled in the Bitcoin time series. What a nice idea, isn\u2019t it?", "Or\u2026 is it? The results are bittersweet. Some of the progressions improve a lot (compare for instance the first column to previous models) but others are really bad (for instance the 6th column). This brings me to think that perhaps sampling the whole space of possibilities equiprobably is not an optimal idea. By sampling the whole space equally we are forcing the network to learn to compress the whole space equally, indifferently from whether the space is actually relevant to represent Bitcoin or stock prices. We must keep in mind that an autoencoder, like all the neural networks, is a function approximator and as such it tries to globally approximate all the data points we use in the training. This global optimization inherently means that in order to approximate some values better it will have to necessarily loose performance in approximating others. This suggests that in order for this idea to work, we should find smarter ways to sample only the relevant space so that the network gets at its best only on relevant compressions.", "We have seen that autoencoders can be useful to compress the time series of stocks returns. If the objective was solely to compress the data, it would be interesting to try other classic dimensionality reduction algorithms such as PCA, which may well prove better at this specific task.", "However, the advantage of using autoencoders is that some of its components, such as the encoder, can be separately trained on several independent stock market returns and then re-used in other end-to-end neural-networks while still keeping the potential to be globally optimized by back-propagation.", "Some of the next ideas I\u2019d like to try next are:", "As usual, here\u2019s the Jupyter notebook to reproduce my work:", "This project is part of our research at CryptoDatum.io, a cryptocurrency data API that aims to provide plug-and-play datasets to train machine learning algorithms. If you liked the data we showed in this article, get your free API key and play with it yourself at https://cryptodatum.io", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Trading strategy developer at Primer Quant Firm \u2014 Founder of CryptoDatum.io"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F28e8c1a2da3e&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautoencoders-for-the-compression-of-stock-market-data-28e8c1a2da3e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautoencoders-for-the-compression-of-stock-market-data-28e8c1a2da3e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautoencoders-for-the-compression-of-stock-market-data-28e8c1a2da3e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautoencoders-for-the-compression-of-stock-market-data-28e8c1a2da3e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----28e8c1a2da3e--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----28e8c1a2da3e--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@gerardmartnez?source=post_page-----28e8c1a2da3e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@gerardmartnez?source=post_page-----28e8c1a2da3e--------------------------------", "anchor_text": "Gerard Mart\u00ednez"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa21c8bce638f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautoencoders-for-the-compression-of-stock-market-data-28e8c1a2da3e&user=Gerard+Mart%C3%ADnez&userId=a21c8bce638f&source=post_page-a21c8bce638f----28e8c1a2da3e---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F28e8c1a2da3e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautoencoders-for-the-compression-of-stock-market-data-28e8c1a2da3e&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F28e8c1a2da3e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautoencoders-for-the-compression-of-stock-market-data-28e8c1a2da3e&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://cryptodatum.io", "anchor_text": "CryptoDatum.io"}, {"url": "https://cryptodatum.io.", "anchor_text": "https://cryptodatum.io"}, {"url": "https://cryptodatum.io", "anchor_text": ""}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----28e8c1a2da3e---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----28e8c1a2da3e---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/bitcoin?source=post_page-----28e8c1a2da3e---------------bitcoin-----------------", "anchor_text": "Bitcoin"}, {"url": "https://medium.com/tag/stock-market?source=post_page-----28e8c1a2da3e---------------stock_market-----------------", "anchor_text": "Stock Market"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----28e8c1a2da3e---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F28e8c1a2da3e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautoencoders-for-the-compression-of-stock-market-data-28e8c1a2da3e&user=Gerard+Mart%C3%ADnez&userId=a21c8bce638f&source=-----28e8c1a2da3e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F28e8c1a2da3e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautoencoders-for-the-compression-of-stock-market-data-28e8c1a2da3e&user=Gerard+Mart%C3%ADnez&userId=a21c8bce638f&source=-----28e8c1a2da3e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F28e8c1a2da3e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautoencoders-for-the-compression-of-stock-market-data-28e8c1a2da3e&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----28e8c1a2da3e--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F28e8c1a2da3e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautoencoders-for-the-compression-of-stock-market-data-28e8c1a2da3e&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----28e8c1a2da3e---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----28e8c1a2da3e--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----28e8c1a2da3e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----28e8c1a2da3e--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----28e8c1a2da3e--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----28e8c1a2da3e--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----28e8c1a2da3e--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----28e8c1a2da3e--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----28e8c1a2da3e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@gerardmartnez?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@gerardmartnez?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Gerard Mart\u00ednez"}, {"url": "https://medium.com/@gerardmartnez/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.4K Followers"}, {"url": "http://CryptoDatum.io", "anchor_text": "CryptoDatum.io"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa21c8bce638f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautoencoders-for-the-compression-of-stock-market-data-28e8c1a2da3e&user=Gerard+Mart%C3%ADnez&userId=a21c8bce638f&source=post_page-a21c8bce638f--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F4ae84817a9e0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautoencoders-for-the-compression-of-stock-market-data-28e8c1a2da3e&newsletterV3=a21c8bce638f&newsletterV3Id=4ae84817a9e0&user=Gerard+Mart%C3%ADnez&userId=a21c8bce638f&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}