{"url": "https://towardsdatascience.com/text-based-graph-convolutional-network-for-semi-supervised-bible-book-classification-c71f6f61ff0f", "time": 1682996298.0966022, "path": "towardsdatascience.com/text-based-graph-convolutional-network-for-semi-supervised-bible-book-classification-c71f6f61ff0f/", "webpage": {"metadata": {"title": "Text-based Graph Convolutional Network \u2014 Bible Book Classification | by Wee Tee Soh | Towards Data Science", "h1": "Text-based Graph Convolutional Network \u2014 Bible Book Classification", "description": "In this article, I will walk you through the details of text-based Graph Convolutional Network (GCN) and its implementation using PyTorch and standard libraries. The text-based GCN model is an\u2026"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/abs/1809.05679", "anchor_text": "recently", "paragraph_index": 0}, {"url": "https://arxiv.org/abs/1609.02907", "anchor_text": "Kipf et al", "paragraph_index": 0}, {"url": "https://neptune.ai/blog/graph-neural-network-and-some-of-gnn-applications", "anchor_text": "https://neptune.ai/blog/graph-neural-network-and-some-of-gnn-applications", "paragraph_index": 4}, {"url": "https://github.com/scrollmapper/bible_databases", "anchor_text": "https://github.com/scrollmapper/bible_databases", "paragraph_index": 5}, {"url": "https://github.com/plkmo/Bible_Text_GCN", "anchor_text": "https://github.com/plkmo/Bible_Text_GCN", "paragraph_index": 6}, {"url": "https://github.com/plkmo/NLP_Toolkit", "anchor_text": "https://github.com/plkmo/NLP_Toolkit", "paragraph_index": 7}, {"url": "https://arxiv.org/abs/1902.10191", "anchor_text": "Recurrent Neural Networks (RNNs)", "paragraph_index": 28}, {"url": "https://arxiv.org/abs/1812.04206", "anchor_text": "Long Short-Term Memory (LSTMs)", "paragraph_index": 28}, {"url": "https://arxiv.org/abs/1801.07455", "anchor_text": "dynamic pose estimation of the human skeleton", "paragraph_index": 28}, {"url": "https://t.me/followai_bot", "anchor_text": "https://t.me/followai_bot", "paragraph_index": 30}], "all_paragraphs": ["In this article, I will walk you through the details of text-based Graph Convolutional Network (GCN) and its implementation using PyTorch and standard libraries. The text-based GCN model is an interesting and novel state-of-the-art semi-supervised learning concept that was proposed recently (expanding upon the previous GCN idea by Kipf et al. on non-textual data) which is able to very accurately infer the labels of some unknown textual data given related known labeled textual data.", "At the highest level, it does so by embedding the entire corpus into a single graph with documents (some labelled and some unlabelled) and words as nodes, with each document-word & word-word edges having some predetermined weights based on their relationships with each other (eg. Tf-idf). A GCN is then trained on this graph with documents nodes that have known labels, and the trained GCN model is then used to infer the labels of unlabelled documents.", "We implement text-based GCN here using the Holy Bible as the corpus, which is chosen because it is one of the most read book in the world and contains a rich structure of text. The Holy Bible (Protestant) consists of 66 Books (Genesis, Exodus, etc) and 1189 Chapters. The semi-supervised task here is to train a language model that is able to correctly classify the Book that some unlabelled Chapters belong to, given the known labels of other Chapters. (Since we actually do know the exact labels of all Chapters, we will intentionally mask the labels of some 10\u201320 % of the Chapters, which will be used as test set during model inference to measure the model accuracy)", "To solve this task, the language model needs to be able to distinguish between the contexts associated with the various Books (eg. Book of Genesis talks more about Adam & Eve while the Book of Ecclesiastes talks about the life of King Solomon). The obtained good results of the text-GCN model, as we shall see below, show that the graph structure is able to capture such context relatively well, where the document (Chapter)-word edges encode the context within Chapters, while the word-word edges encode the relative context between Chapters.", "For a nice, detailed explanation of graph neural networks in general, you can check out this great article https://neptune.ai/blog/graph-neural-network-and-some-of-gnn-applications", "The Bible text used here (BBE version) is obtained courtesy of https://github.com/scrollmapper/bible_databases.", "The source codes for the implementation can be found in my GitHub repository (https://github.com/plkmo/Bible_Text_GCN)", "If you would like to try text-GCN on your own corpus, I have created an NLP toolkit of state-of-the-art models, one of which implements text-GCN for easy text classification (among other tasks). You can use it here (https://github.com/plkmo/NLP_Toolkit).", "Following the paper, in order to allow GCN to capture the Chapter contexts, we build a graph with nodes and edges that represent the relationships between Chapters and words. The nodes will consist of all 1189 Chapters (documents) plus the whole vocabulary (words), with weighted document-word and word-word edges between them. Their weights A_ij are given by:", "where PMI is the Point-wise Mutual Information between pairs of co-occurring words over a sliding window #W that we fix to be of 10-words length. #W(i) is the number of sliding windows in a corpus that contain word i, #W(i,j) is the number of sliding windows that contain both word i and j, and #W is the total number of sliding windows in the corpus. TF-IDF is the usual term frequency-inverse document frequency of the word in the document. Intuitively, a high positive PMI between pairs of words means that they have a high semantic correlation, conversely we do not build edges between words with negative PMI. Overall, TF-IDF-weighted document-word edges capture within-document context, while PMI-weighted word-word edges (which can span across documents) capture across-document contexts.", "In comparison, for non-graph based models, such across-document context information are not easily provided as input features, and the model would have to learn them by itself \u201cfrom scratch\u201d based on the labels. Since additional information on the relationship between documents is provided in GCN which is definitely relevant in NLP tasks, one would expect that GCN would perform better.", "Calculating TF-IDF is relatively straightforward. We know the math and understand how it works, so we simply use sklearn\u2019s TfidfVectorizer module on our 1189 documents texts, and store the result in a dataframe. This will be used for the document-word weights when we create the graph later.", "2. Calculating Point-wise Mutual Information between words", "Calculating PMI between words is more tricky. First, we need to find the co-occurrences between words i, j within a sliding window of 10 words, stored as a square matrix in a dataframe where rows and columns represent the vocabulary. From this, we can then calculate the PMI using the definition earlier. The annotated code for the calculation is shown above.", "Now that we have all the weights for the edges, we are ready to build the graph G. We use the networkx module to build it. Here, its noteworthy to point out that most of the heavy-lifting computation for data processing in this whole project is spent on building the word-word edges, as we need to iterate over all possible pairwise word combinations for a vocabulary of about 6500 words. Luckily, there\u2019s an efficient way to implement this and it only took around 3 mins to run. The code snippet for our computation is shown below.", "In convolutional neural networks for image-related tasks, we have convolution layers or filters (with learnable weights) that \u201cpass over\u201d a bunch of pixels to generate feature maps that are learned by training. Now imagine that these bunch of pixels are your graph nodes, we will similarly have a bunch of filters with learnable weights W that \u201cpass over\u201d these graph nodes in GCN.", "However, there is a big problem: graph nodes do not really have a clear notion of physical space and distance as pixels have (we can\u2019t really say that a node is to the right or left of another). As such, in order to meaningfully convolve nodes with our filter W, we have to first find feature representations for each node that best captures the graph structure. For the advanced readers, the authors solved this problem by projecting both the filter weights W and feature space X for each node into the Fourier space of the graph, so that convolution becomes just a point-wise multiplication of nodes with features. For a deep dive into the derivation, the original paper by Kipf et al. is a good starting point. Otherwise, the readers can just make do with this intuitive explanation and proceed on.", "We are going to use a two-layer GCN (features are convolved twice) here as, according to their paper, it gives the best results. The convoluted output feature tensor after the two-layer GCN is given by:", "Here, A is the adjacency matrix of graph G (with diagonal elements being 1 to represent self-connection of nodes) and D is the degree matrix of G. W_0 and W_1 are the learnable filter weights for the first and second GCN layer respectively, which is to be trained. X is the input feature matrix which we take to be a diagonal square matrix (of ones) of the same dimension as the number of nodes, which simply means that the input is a one-hot encoding of each of the graph nodes. The final output is then fed into a softmax layer with a cross entropy loss function for classification with 66 different labels corresponding to each of the 66 books.", "The implementation of the two-layer GCN architecture in PyTorch is given below.", "Out of a total of 1189 Chapters, we will mask the labels of 111 of them (about 10 %) during training. As the class label distribution over 1189 Chapters are quite skewed (above figure), we will not mask any of the class labels of those Chapters in which their total count is less than 4, to ensure that the GCN can learn representations from all 66 unique class labels.", "We train the GCN model to minimize the cross entropy losses of the unmasked labels. After training the GCN for 7000 epochs, we will then use the model to infer the Book labels of the 111 masked Chapters and analyze the results.", "From the Loss vs Epoch graph above, we see that training proceeds pretty well and starts to saturate at around 2000 epochs.", "As training proceeds, the training accuracy as well as the inference accuracy (of the masked nodes) are seen to increase together, until about 2000 epochs where the inference accuracy starts to saturate at around 50%. Considering that we have 66 classes, which would have a baseline accuracy of 1.5 % if we assume that the model predicts by pure chance, thus 50% inference accuracy seems pretty good already. This tells us that the GCN model is able to correctly infer the Book which the given unlabelled Chapter belongs to about 50 % of the time, after being trained properly on labelled Chapters.", "The GCN model is able to capture the within-document and between-document contexts pretty well, but what about the misclassified Chapters? Does it mean that the GCN model failed on those? Lets look at a few of them to find out.", "In this case, Chapter 27 from the book of Matthew has been wrongly classified to be from the book of Luke. From above, we see that this Chapter is about Jesus being put to death by the chief priests and dying for our sins, as well as Judas\u2019s guilt after his betrayal of Jesus. Now, these events are also mentioned in Luke! (as well as in Mark and John) This is most likely why the model classified it as Luke, as they share similar context.", "Here, Chapter 12 from the Book of Isaiah is wrongly inferred to be from the Book of Psalms. It is clear from this passage that the narrator in Isaiah Chapter 12 talks about giving and singing praises to God, who is his comforter and source of salvation. This context of praising God and looking to Him for comfort is exactly the whole theme of the Book of Psalms, where David pens down his praises and prayers to God throughout his successes, trials and tribulations! Hence, it is no wonder that the model would classify it as Psalms, as they share similar context.", "The text-based Graph Convolutional Network is indeed a powerful model especially for semi-supervised learning, as it is able to strongly capture the textual context between and across words and documents, and infer the unknown given the known.", "The applications of GCNs are actually quite robust and far-reaching, and this article has only provided a glimpse of what it can do. In general other than for the task presented here, GCN can be used whenever one wants to combine the power of graph representations with deep learning. To provide a few interesting examples for further reading, GCN has been used in combination with Recurrent Neural Networks (RNNs)/Long Short-Term Memory (LSTMs) for dynamic network/node/edge predictions. It has also been successfully applied for dynamic pose estimation of the human skeleton by modelling human joints as graph nodes and the relationships between and within human body structures and time-frames as graph edges.", "Thanks for reading and I hope that this article has helped much to explain its inner workings.", "To keep up-to-date with the latest AI/data science trends, papers & news, check out my @followai_bot (https://t.me/followai_bot), your personalized AI/data science Telegram bot.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fc71f6f61ff0f&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-based-graph-convolutional-network-for-semi-supervised-bible-book-classification-c71f6f61ff0f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-based-graph-convolutional-network-for-semi-supervised-bible-book-classification-c71f6f61ff0f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-based-graph-convolutional-network-for-semi-supervised-bible-book-classification-c71f6f61ff0f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-based-graph-convolutional-network-for-semi-supervised-bible-book-classification-c71f6f61ff0f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----c71f6f61ff0f--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c71f6f61ff0f--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@weeteesoh345?source=post_page-----c71f6f61ff0f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@weeteesoh345?source=post_page-----c71f6f61ff0f--------------------------------", "anchor_text": "Wee Tee Soh"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F836f30b28f75&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-based-graph-convolutional-network-for-semi-supervised-bible-book-classification-c71f6f61ff0f&user=Wee+Tee+Soh&userId=836f30b28f75&source=post_page-836f30b28f75----c71f6f61ff0f---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc71f6f61ff0f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-based-graph-convolutional-network-for-semi-supervised-bible-book-classification-c71f6f61ff0f&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc71f6f61ff0f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-based-graph-convolutional-network-for-semi-supervised-bible-book-classification-c71f6f61ff0f&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.quora.com/Whats-the-most-beautiful-graph-you-have-ever-seen", "anchor_text": "https://www.quora.com/Whats-the-most-beautiful-graph-you-have-ever-seen"}, {"url": "https://arxiv.org/abs/1809.05679", "anchor_text": "recently"}, {"url": "https://arxiv.org/abs/1609.02907", "anchor_text": "Kipf et al"}, {"url": "https://neptune.ai/blog/graph-neural-network-and-some-of-gnn-applications", "anchor_text": "https://neptune.ai/blog/graph-neural-network-and-some-of-gnn-applications"}, {"url": "https://github.com/scrollmapper/bible_databases", "anchor_text": "https://github.com/scrollmapper/bible_databases"}, {"url": "https://arxiv.org/abs/1809.05679", "anchor_text": "https://arxiv.org/abs/1809.05679"}, {"url": "https://github.com/plkmo/Bible_Text_GCN", "anchor_text": "https://github.com/plkmo/Bible_Text_GCN"}, {"url": "https://github.com/plkmo/NLP_Toolkit", "anchor_text": "https://github.com/plkmo/NLP_Toolkit"}, {"url": "https://arxiv.org/abs/1902.10191", "anchor_text": "Recurrent Neural Networks (RNNs)"}, {"url": "https://arxiv.org/abs/1812.04206", "anchor_text": "Long Short-Term Memory (LSTMs)"}, {"url": "https://arxiv.org/abs/1801.07455", "anchor_text": "dynamic pose estimation of the human skeleton"}, {"url": "https://github.com/plkmo/Bible_Text_GCN", "anchor_text": "https://github.com/plkmo/Bible_Text_GCN"}, {"url": "https://github.com/plkmo/NLP_Toolkit", "anchor_text": "https://github.com/plkmo/NLP_Toolkit"}, {"url": "https://arxiv.org/search/cs?searchtype=author&query=Kipf%2C+T+N", "anchor_text": "Thomas N. Kipf"}, {"url": "https://arxiv.org/search/cs?searchtype=author&query=Welling%2C+M", "anchor_text": "Max Welling"}, {"url": "https://arxiv.org/abs/1609.02907", "anchor_text": "https://arxiv.org/abs/1609.02907"}, {"url": "https://arxiv.org/search/cs?searchtype=author&query=Yao%2C+L", "anchor_text": "Liang Yao"}, {"url": "https://arxiv.org/search/cs?searchtype=author&query=Mao%2C+C", "anchor_text": "Chengsheng Mao"}, {"url": "https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+Y", "anchor_text": "Yuan Luo"}, {"url": "https://arxiv.org/abs/1809.05679", "anchor_text": "https://arxiv.org/abs/1809.05679"}, {"url": "https://t.me/followai_bot", "anchor_text": "https://t.me/followai_bot"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----c71f6f61ff0f---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/graph?source=post_page-----c71f6f61ff0f---------------graph-----------------", "anchor_text": "Graph"}, {"url": "https://medium.com/tag/convolutional-neural-net?source=post_page-----c71f6f61ff0f---------------convolutional_neural_net-----------------", "anchor_text": "Convolutional Neural Net"}, {"url": "https://medium.com/tag/nlp?source=post_page-----c71f6f61ff0f---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----c71f6f61ff0f---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc71f6f61ff0f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-based-graph-convolutional-network-for-semi-supervised-bible-book-classification-c71f6f61ff0f&user=Wee+Tee+Soh&userId=836f30b28f75&source=-----c71f6f61ff0f---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc71f6f61ff0f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-based-graph-convolutional-network-for-semi-supervised-bible-book-classification-c71f6f61ff0f&user=Wee+Tee+Soh&userId=836f30b28f75&source=-----c71f6f61ff0f---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc71f6f61ff0f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-based-graph-convolutional-network-for-semi-supervised-bible-book-classification-c71f6f61ff0f&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c71f6f61ff0f--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fc71f6f61ff0f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-based-graph-convolutional-network-for-semi-supervised-bible-book-classification-c71f6f61ff0f&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----c71f6f61ff0f---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----c71f6f61ff0f--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----c71f6f61ff0f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----c71f6f61ff0f--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----c71f6f61ff0f--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----c71f6f61ff0f--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----c71f6f61ff0f--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----c71f6f61ff0f--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----c71f6f61ff0f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@weeteesoh345?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@weeteesoh345?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Wee Tee Soh"}, {"url": "https://medium.com/@weeteesoh345/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "216 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F836f30b28f75&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-based-graph-convolutional-network-for-semi-supervised-bible-book-classification-c71f6f61ff0f&user=Wee+Tee+Soh&userId=836f30b28f75&source=post_page-836f30b28f75--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F95d486611d9b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-based-graph-convolutional-network-for-semi-supervised-bible-book-classification-c71f6f61ff0f&newsletterV3=836f30b28f75&newsletterV3Id=95d486611d9b&user=Wee+Tee+Soh&userId=836f30b28f75&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}