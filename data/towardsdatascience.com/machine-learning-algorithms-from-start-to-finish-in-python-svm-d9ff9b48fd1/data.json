{"url": "https://towardsdatascience.com/machine-learning-algorithms-from-start-to-finish-in-python-svm-d9ff9b48fd1", "time": 1683016434.674585, "path": "towardsdatascience.com/machine-learning-algorithms-from-start-to-finish-in-python-svm-d9ff9b48fd1/", "webpage": {"metadata": {"title": "Machine Learning Algorithms from Start to Finish in Python: SVM | by Vagif Aliyev | Towards Data Science", "h1": "Machine Learning Algorithms from Start to Finish in Python: SVM", "description": "Support Vector Machines are very versatile Machine Learning algorithms. The main reason for their popularity is for their ability to perform both linear and non-linear classification and regression\u2026"}, "outgoing_paragraph_urls": [{"url": "https://github.com/Vagif12/ML-Algorithms-From-Scratch/blob/main/SVM%20from%20Scratch.py", "anchor_text": "Github Code", "paragraph_index": 106}, {"url": "http://Upword.ai", "anchor_text": "Upword.ai", "paragraph_index": 108}], "all_paragraphs": ["Support Vector Machines are very versatile Machine Learning algorithms. The main reason for their popularity is for their ability to perform both linear and non-linear classification and regression using what is known as the kernel trick; if you don\u2019t know what that is, don\u2019t worry. By the end of this article, you will be able to :", "So, without further ado, let\u2019s dive in!", "With so many other algorithms out there(Linear Regression, Logistic Regression, Neural Networks, etc..) You may be wondering why you need to have another one in your toolkit! Perhaps these questions can be answered with the aid of a diagram:", "Here we see three potential decision boundaries for classifying the data: H1, H2, and H3. First off, H1 does not separate the classes at all, so it is not a good hyperplane. H2 does separate the classes, however notice how the margin(or street) between the points is so small, and this classifier is very unlikely to perform well on unseen instances.", "The third hyperplane, H3, represents the decision boundary of the SVM classifier; this line not only separates the two classes but also keeps the widest distance between the most extreme points of the two classes.", "You can think of the SVM as fitting the widest possible margin between the two classes. This is known as large margin classification.", "Like I said, a large margin SVM Classifier essentially tries to fit the widest possible street(shown by the dashed parallel lines) between two classes. It is important to note that adding more instances that are \u201coff the street\u201d(not on the dashed line) will not affect the decision boundary.", "The decision boundary is fully determined(or supported) by the most extreme instances of the class, or, in other words, the instances located on the edge of the street. These are called support vectors(they are circled in black in the diagram).", "So essentially, a hard margin SVM basically tries to fit a decision boundary that maximises the distance between the support vectors of the two classes. However, there are a few issues with this model:", "These two concepts can be clearly highlighted in this visualisation:", "Note how the red point is an extreme outlier, and hence the SVM algorithm uses it as a support vector. Because the Hard Margin classifier finds the maximum distance between the support vectors, it uses the red outlier and the blue support vectors to set a decision boundary.", "This results in a very poor decision boundary that is likely overfitting and will fail to predict new classes well.", "In this example, we can clearly observe that there is no possible linear classifier that will separate the classes. Additionally, there is a major outlier. So, the question is, how can an SVM separate non-linearly separable data?", "One approach is to find a good balance between keeping the streets as wide as possible(maximising the margin) and limiting the margin violations(these are instances that end up in the middle of the street or even on the wrong side of the street). This is called a soft margin SVM.", "Essentially you are controlling the trade-off between two objectives:", "This trade-off is usually controlled by a hyperparameter that can be denoted by \u03bb, or, more commonly(in scikit-learn) the C parameter. This essentially controls the misclassification cost. Concretely,", "Although this approach can work, we have to figure out the optimal C parameter using cross-validation techniques. This can take a considerable amount of time. Additionally, one may want to create an optimal model and not have any \u201cslack\u201d variables that cross margin violations. So what is our solution now?", "While Linear SVM\u2019s work well in most cases, it is extremely rare to have a dataset that is linearly separable. One approach to combat this is to add more features, such as polynomial features(theses essentially transform your features by raising the values to an N degree polynomial(think X\u00b2,X\u00b3, etc..)).", "For instance, let\u2019s say we have the following data:", "Clearly, this dataset isn\u2019t linearly separable.", "However, when we apply a polynomial transformation by raising the root to a power of 20:", "We get a dataset that is linearly separable.", "However, this is not feasible for large datasets; the computational complexity and the time it will take for the polynomial transformation to happen would be simply too long and computationally expensive.", "Additionally, using high order polynomial degrees creates a huge number of features, making the model too slow.", "That\u2019s where the beauty of SVM\u2019s come to play. More specifically, the beauty of the kernel trick", "Essentially, kernels are different functions that calculates the relationships between non-linearly separable data points and maps them into higher dimensions. It then fits a standard Support Vector Classifier. It effectively maps features from being in a relatively low dimension to a relatively high dimension.", "However, kernel functions only calculate the high dimensional relationships between the data points as if they were in a higher dimension; they do not actually do the transformation, meaning that the kernel function does not add any features, but we get the same results as if we id.", "This trick(calculating the high dimensional relationships between the data points without actually creating or transforming them) is known as the Kernel Trick.", "The kernel trick reduces the computational complexity of the SVM by avoiding the math that transforms features from low dimensions to high dimensions!", "Let\u2019s look at two common Kernel functions:", "Essentially, this uses a Polynomial Kernel to calculate the high dimensional relationships between the data points and map the data into a higher dimension without adding any features.", "The formula for the polynomial kernel is the following(I do apologise for hitting you with the maths without warning!):", "To give a concrete example, let\u2019s suppose we have data that looks like this:", "Clearly, this data is not linearly separable. However, if we were to use an SVM with a polynomial kernel, we would get the following high dimension mapping:", "Again, the important idea to take from this is that the kernel function only calculates the high dimensional relationship between the points as if they were in high dimensions, but does not create or transform new features.", "The following code implements a polynomial kernel with the SVC class in scikit-learn:", "Obviously if your model is overfitting, you may need to reduce the degree of the polynomial. You may have noticed a few parameters here. Let me explain them briefly:", "Another SVM kernel that is extremely popular is the Gaussian Radial Based Function(Gaussian RBF). Essentially, this is a similarity function that computes the distance between instance and a landmark. The formula for the kernel function is given below:", "To clarify a few notations(NOTE: anything that is unclear will be explained shortly):", "The function itself follows a bell shaped curve(Hence why it\u2019s Gaussian) ranging from 0 (very far away from the landmark) to 1(at the landmark).", "I\u2019m sure this is still fuzzy in your mind, so let\u2019s clear up the confusion the only way I know; with an example!", "Observe the diagram of the 1D data below:", "A landmark is essentially a point in the dataset that we will use to get the similarity between. Here we have 2 landmarks, X2 and X3.", "We are now ready to compute new features. For example, Let\u2019s look at the instance X, which is equal to -1. It is located at a distance of 1 from the first landmark and a distance 2 from the second landmark. Therefore, its new mapped features would be:", "Which is plotted in the following diagram:", "And now we simply use a normal SVC to classify the points!", "You may be thinking, cool! But:", "So True. Well, to address the first question, usually you create a landmark at the location of each and every instance in the dataset. This creates many dimensions and thus increases the chances that the transformed training set will be linearly separable.", "However, once again, just like polynomial transformations, this is computationally expensive and requires a lot of features to be added, Just imagine if you had a training set with m instances and n features gets transformed into a training set with m instances and m features (assuming you drop the original features).", "In other words, if you had 20 instances with 20 features, calculating these transformations will results in 400 features!", "Luckily for us, the kernel trick does its magic; it makes it possible to compute these higher dimensional relationships without actually transforming or creating new features, and still getting the same result as if you had!", "Now, gamma is a special hyperparameter that is a specific to rbf kernels. Referring back to our plot above of the rbf function, gamma controls the width of each bell-shaped function.", "Concretely, a large value of gamma will shorten the width of the bell-shaped curve and reduce the range of influence of each instance and resulting in a more irregular decision boundary that wiggles between individual data points. Conversely, a small value of gamma increases the range of influence of each data point, and results in a much smoother decision boundary.", "The scikit-learn implementation of an SVC with an rbf kernel would be the following:", "NOTE: In this article, I will be only coding a soft and hard margin SVM. but in the future, I will be writing articles on how to implement the kernel trick in SVM, so be sure to stay tuned for that in the future", "Yes I\u2019m sorry but you do unfortunately need to understand the math in order to code it. If you really hate math, feel free to skip this section, but I highly advise at least trying to understand what is going on to give you a better sense of the problem at hand.", "Before I actually get to the math, let me give you a step by step guide as to how the SVM works:", "Now, let me explain some of those key terms before we dive into the SVM Math.", "A cost function is essentially a formula that measures the loss, or the \u201ccost\u201d of your model. If you have ever done any Kaggle competitions, you may have come across some of them. A few common ones include:", "The cost function that we will be using is called the hinge loss. The formula for the function is the following:", "Graphically, the hinge loss looks like this:", "In this plot, the blue represents the loss for correct classification, and the green represents the loss for misclassification.", "Note that the hinge loss penalises predictions where data points are inside the margin, even if we classified them correctly.", "Essentially, we will be using this to measure our algorithm\u2019s performance and make sure we reach our goal(maximising the margin)", "Optimisation is usually defined as the process of improving something so that it operates at its full potential. This is also applicable in Machine Learning. In the world of ML, optimisation is essentially trying to find the best combination of parameters for a certain dataset. This is essentially the \u201clearning\u201d bit of Machine Learning.", "While may optimisation algorithms exist, I will discuss two of the most common ones: Gradient Descent and The Normal Equation.", "Gradient Descent is an optimisation algorithm that aims to find the minimum of a function. It achieves this goal by iteratively taking steps in the negative direction of the slope. In our example, gradient descent would continuously update the weights by moving in the slope of the tangential line to the function. Well fantastic, sound great. English please? :)", "To better illustrate Gradient Descent, Let\u2019s go through a simple example. Imagine a human is at the top of a mountain, and he/she want to get to the bottom. What they might do is look around and see in what direction they should take a step in in order to get down quicker. Then, they might take a step in that direction and now they are closer to their goal. However, they have to be careful when coming down as they might get stuck at a certain point, so we have to make sure to choose our step sizes accordingly.", "Similarly, The objective of gradient descent is to minimise a function. In our case, it is to minimise the cost of our model. It does this by finding the tangential line to the function and moving in that direction. The size of the \u201cstep\u201d of the algorithm is defined by what is known as a learning rate. This essentially controls how far we move down. With this parameter, we have to be careful of two cases:", "We also have a parameter that controls the number of times the algorithm iterates over the dataset.", "Visually, the algorithm would do something like this:", "Because this algorithm is so vital to know in Machine Learning, let\u2019s recap what it does:", "This process is shown mathematically like so:", "\u03b1: this is the sign for the learning rate(remember: the size of the step)", "m: the number of training examples", "\u03b8n: the nth coefficient of our algorithm", "Ok, I know you really want to get onto to the actual coding, but this last part is the most important part of the math for SVM, so hang in there!", "This is the condition of the hinge loss for positive classification. This is represented by the blue line on the plot of the hinge loss we saw earlier. Basically, this checks whether the given instance was correctly or incorrectly classified.", "This is our formula for correct classification. To clarify:", "w: the weight of the algorithm", "\u03b1: the learning rate for gradient descent that we talked about earlier", "\u03bb: the regularisation parameter(this is equivalent to the C parameter)", "This is the formula for incorrect classification. Note how we also adjust the bias term.", "Ok, so finally, ladies and gentlemen, the main event: SVM from Scratch!", "Let\u2019s finally begin! First, let\u2019s do some basic imports:", "Yes, no sklearn models! This is going to be coded in plain numpy!", "Next, let\u2019s make a basic dataset and split our data into train and test:", "I\u2019m going to make a synthetic dataset for this tutorial, and set the class_sep to 0.7, meaning that if should be relatively simple to classify the data.", "Now, to adhere to software engineering principles, I will make an SVM class and build it to make the code look cleaner and make it easier to share:", "So here, we essentially initialise our learning rate, regularisation parameter, the number of iterations, and we set the weights and biased equal to zero.", "Next, we define our fit method:", "Once we are given training features and a target vector, we can randomly initialise our weights to be a vector of the number of features. Note how we convert the 0 values in our dataset to equal -1 so that we can use the hinge loss.", "Now, we move on to the core of the method:", "So, we are essentially doing the following:", "Next, we translate our formula for correct classification into code:", "and our formula for incorrect classification into code:", "Finally, we define our predict function:", "We make sure to convert our labels that equal -1 back to zero.", "Now, we simply call our function and get the accuracy of our model on the test set:", "I have added a visualise_svm() function to help visualise the SVM which can be accessed from the Github repo I have added at the end of this article. Nevertheless, running the function outputs the following:", "Now, if you have not guessed, we have just implemented a soft margin SVM. We set the C value to be 0.001, and we can clearly see that the decision boundary allowed some points to be in the margin and on the wrong side, but resulted in a better hyperplane.", "Now, when we change the C value to 0.9(very little regularisation), we get the following plot:", "And our accuracy has decreased from 0.82 to 0.7", "I want you to try out some tasks:", "I really appreciate everyone who motivates me to write good articles. I thank my loyal followers and everybody who has decided to read my work. I assure you, it does not go unnoticed. I hope to always produce interesting and fascinating work for readers.", "I hope you learned something new, and possibly refreshed some old knowledge. Be sure to stay tuned for more, and I wish you all the best!", "PS: here is the link to the Github Code", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "19 y/o student ex-founder of Snapstudy (acquired), founding engineer at Upword.ai"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fd9ff9b48fd1&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-algorithms-from-start-to-finish-in-python-svm-d9ff9b48fd1&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-algorithms-from-start-to-finish-in-python-svm-d9ff9b48fd1&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-algorithms-from-start-to-finish-in-python-svm-d9ff9b48fd1&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-algorithms-from-start-to-finish-in-python-svm-d9ff9b48fd1&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----d9ff9b48fd1--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d9ff9b48fd1--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://vagifaliyev.medium.com/?source=post_page-----d9ff9b48fd1--------------------------------", "anchor_text": ""}, {"url": "https://vagifaliyev.medium.com/?source=post_page-----d9ff9b48fd1--------------------------------", "anchor_text": "Vagif Aliyev"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6b4e6b48584&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-algorithms-from-start-to-finish-in-python-svm-d9ff9b48fd1&user=Vagif+Aliyev&userId=6b4e6b48584&source=post_page-6b4e6b48584----d9ff9b48fd1---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd9ff9b48fd1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-algorithms-from-start-to-finish-in-python-svm-d9ff9b48fd1&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd9ff9b48fd1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-algorithms-from-start-to-finish-in-python-svm-d9ff9b48fd1&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@mikloru?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Mike Lorusso"}, {"url": "https://unsplash.com/s/photos/street?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://unsplash.com/@paul_nic?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Paolo Nicolello"}, {"url": "https://unsplash.com/s/photos/confused?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://en.wikipedia.org/wiki/Support_vector_machine", "anchor_text": "Wikipedia"}, {"url": "https://en.wikipedia.org/wiki/Support_vector_machine", "anchor_text": "Wikipedia"}, {"url": "https://unsplash.com/@ludo_photos?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Ludovic Charlet"}, {"url": "https://unsplash.com/s/photos/limit?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://stackoverflow.com/questions/4629505/svm-hard-or-soft-margins/4630731", "anchor_text": "StackOverflow"}, {"url": "https://en.wikipedia.org/wiki/File:Kernel_trick_idea.svg", "anchor_text": "Wikipedia"}, {"url": "https://unsplash.com/@juliusdrost?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Julius Drost"}, {"url": "https://unsplash.com/s/photos/magic?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://unsplash.com/@chrislawton?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Chris Lawton"}, {"url": "https://unsplash.com/s/photos/transformation?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://unsplash.com/@scw1217?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Suzanne D. Williams"}, {"url": "https://unsplash.com/s/photos/transformation?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://en.wikipedia.org/wiki/Radial_basis_function_kernel", "anchor_text": "Wikipedia"}, {"url": "https://unsplash.com/@anniespratt?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Annie Spratt"}, {"url": "https://unsplash.com/s/photos/maths?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://unsplash.com/@timbatec?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Pepi Stojanovski"}, {"url": "https://unsplash.com/s/photos/cost?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://en.wikipedia.org/wiki/Hinge_loss", "anchor_text": "Wikipedia"}, {"url": "https://en.wikipedia.org/wiki/Hinge_loss", "anchor_text": "Wikipedia"}, {"url": "https://unsplash.com/@lux17?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Lucas Clara"}, {"url": "https://unsplash.com/s/photos/mountain?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://en.wikipedia.org/wiki/Gradient_descent", "anchor_text": "Wikipedia"}, {"url": "https://unsplash.com/@cdr6934?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Chris Ried"}, {"url": "https://unsplash.com/s/photos/code?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://github.com/Vagif12/ML-Algorithms-From-Scratch/blob/main/SVM%20from%20Scratch.py", "anchor_text": "Github Code"}, {"url": "https://unsplash.com/@kellysikkema?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Kelly Sikkema"}, {"url": "https://unsplash.com/s/photos/thank-you?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----d9ff9b48fd1---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----d9ff9b48fd1---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-visualization?source=post_page-----d9ff9b48fd1---------------data_visualization-----------------", "anchor_text": "Data Visualization"}, {"url": "https://medium.com/tag/data-science?source=post_page-----d9ff9b48fd1---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/data?source=post_page-----d9ff9b48fd1---------------data-----------------", "anchor_text": "Data"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd9ff9b48fd1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-algorithms-from-start-to-finish-in-python-svm-d9ff9b48fd1&user=Vagif+Aliyev&userId=6b4e6b48584&source=-----d9ff9b48fd1---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd9ff9b48fd1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-algorithms-from-start-to-finish-in-python-svm-d9ff9b48fd1&user=Vagif+Aliyev&userId=6b4e6b48584&source=-----d9ff9b48fd1---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd9ff9b48fd1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-algorithms-from-start-to-finish-in-python-svm-d9ff9b48fd1&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d9ff9b48fd1--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fd9ff9b48fd1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-algorithms-from-start-to-finish-in-python-svm-d9ff9b48fd1&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----d9ff9b48fd1---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----d9ff9b48fd1--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----d9ff9b48fd1--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----d9ff9b48fd1--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----d9ff9b48fd1--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----d9ff9b48fd1--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----d9ff9b48fd1--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----d9ff9b48fd1--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----d9ff9b48fd1--------------------------------", "anchor_text": ""}, {"url": "https://vagifaliyev.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://vagifaliyev.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Vagif Aliyev"}, {"url": "https://vagifaliyev.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "209 Followers"}, {"url": "http://Upword.ai", "anchor_text": "Upword.ai"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6b4e6b48584&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-algorithms-from-start-to-finish-in-python-svm-d9ff9b48fd1&user=Vagif+Aliyev&userId=6b4e6b48584&source=post_page-6b4e6b48584--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F6f056d3d77d3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-algorithms-from-start-to-finish-in-python-svm-d9ff9b48fd1&newsletterV3=6b4e6b48584&newsletterV3Id=6f056d3d77d3&user=Vagif+Aliyev&userId=6b4e6b48584&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}