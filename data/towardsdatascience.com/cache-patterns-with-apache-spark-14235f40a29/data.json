{"url": "https://towardsdatascience.com/cache-patterns-with-apache-spark-14235f40a29", "time": 1683003728.287093, "path": "towardsdatascience.com/cache-patterns-with-apache-spark-14235f40a29/", "webpage": {"metadata": {"title": "Cache Patterns with Apache Spark. Balancing the choice between disk, ram\u2026 | by Scott Haines | Towards Data Science", "h1": "Cache Patterns with Apache Spark", "description": "Apache Spark provides a few very simple mechanisms for caching in-process computations that can help to alleviate cumbersome and inherently complex workloads. This article aims at providing an\u2026"}, "outgoing_paragraph_urls": [{"url": "http://spark.apache.org/", "anchor_text": "Apache Spark", "paragraph_index": 0}, {"url": "https://en.wikipedia.org/wiki/Extract,_transform,_load", "anchor_text": "ETL", "paragraph_index": 1}, {"url": "https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-DataFrameReader.html", "anchor_text": "DataFrameReader", "paragraph_index": 10}, {"url": "http://spark.apache.org/docs/latest/cluster-overview.html#components", "anchor_text": "See the Spark Docs for more details", "paragraph_index": 15}, {"url": "https://www.oreilly.com/library/view/debugging-apache-spark/9781492039174/", "anchor_text": "debug-ability skills", "paragraph_index": 16}, {"url": "https://spark.apache.org/docs/2.4.5/api/java/index.html?org/apache/spark/storage/StorageLevel.html", "anchor_text": "StorageLevel", "paragraph_index": 23}, {"url": "https://twitter.com/holdenkarau", "anchor_text": "Holden Karau", "paragraph_index": 27}, {"url": "https://www.twilio.com/voice/insights", "anchor_text": "Voice Insights team at Twilio", "paragraph_index": 29}, {"url": "https://medium.com/@newfrontcreative/streaming-trend-discovery-real-time-discovery-in-a-sea-of-events-76bc03367e6f", "anchor_text": "Streaming Trend Discovery", "paragraph_index": 30}], "all_paragraphs": ["Apache Spark provides a few very simple mechanisms for caching in-process computations that can help to alleviate cumbersome and inherently complex workloads. This article aims at providing an approachable mental-model to break down and re-think how to frame your Apache Spark computations. Whether your next application is a batch or streaming job the aim here is to provide you with the necessary prerequisite to tackle your next Apache Spark project with gusto!", "Let me first start by stating that a simple workload should fit into memory across a relatively low number of executors \u2014 because simple workloads are inherently simple: like loading one set of data and joining it with another in a common ETL flow. The required overhead here tends to be low as you need to only fit both datasets in memory for a portion of the total processing time\u2014 if tuned correctly. This workload is the Data Engineer\u2019s bread and butter.", "Above is a simple example of a batch based Extract Transform Load (ETL) workload. Batch workloads are typically more of the traditional Map-Reduce style jobs that made Hadoop popular back in the day!", "Complex workloads will more often than not never fit entirely in memory due to cost of the underlying data, or due to the nature of the job like a streaming pipeline job. Complex workloads tends to bite you in very interesting ways, mainly due to the fact that what works locally, in dev or stage environments, rarely scales well on the first go around in production.", "Production can be a scary place where data rates (events per second, etc) bloom and grow way outside of a traditional normal distribution. Things that are uncertain, especially in the realm of humongous data, tend to cause pipelines to scream to a halt, spark jobs to die and for customers to get angry \u2014 all because we all must balance between cost of operation and the unfortunate Out of Memory Exception.", "Examples of complex workloads would be the case of a typical time-series ingestion pipeline with streaming statistical aggregations for real-time metrics or operational dashboards, or the good ol\u2019 real-time machine learning pipeline (see example below). These workloads are typically a joint effort between Data Engineers and an applied Machine Learning or Statistical / Performance Engineer.", "The example above is a fake use case using what is called a Stream-Stream join using Apache Spark Structured Streaming. This takes things further by including Ad Targeting to this streaming pipeline via an expensive real-time machine learning transformation, secondary streaming join, a filtered post-join query as well as downstream publishing to an Ad Recommendation stream. In theory a pipeline like this one could be sending you the ads you see everyday online.", "Now that we have a loose understanding of the differences between simple and complex workloads we can learn how to solve complex problems in more intelligent ways. Let\u2019s go.", "Interestingly enough the difference between most simple workloads and a complex workload tend to come down to intelligent patterns of decomposition, given that a complex workload can be looked at as a series of simple workloads with a distributed finish line that all processes are independently racing towards together, and whom can all be run either fully independently (eg: non deterministic workloads) or deterministically where prior phases of execution reduce a problem into more concise layers.", "Next we will look at how Spark distributes work. Trust me, this is all leading somewhere!", "In the spark world everything begins with a resource. This resource has a Source and describes how data can be read from that given Source. In the simplest file based spark applications we begin with a DataFrameReader.", "The DataFrameReader is available for most of the simpler data reading tasks like json, csv, orc and others enables us to quickly load data into Spark to be evaluated as a series of transformations leading up to a final outcome (or action). It is this (load)->(transform)->(write) directed-acyclic-computational-graph (DAG) which is what makes Spark so damn fun to work with. Starting with a simple file like we see below.", "We are able to easily read json data into spark memory as a DataFrame. This DataFrame wraps a powerful, but almost hidden gem within the more recent versions of Apache Spark. That is the RDD.", "The RDD is how spark beat Map-Reduce at its own game. It stands for Resilient Distributed Datasets. These datasets are are partitioned into a number of logical partitions. For example, say you have an Apache Kafka topic with 32 partitions. Apache Spark would automatically turn the read stream on the Kafka resource into an RDD with 32 partitions to honor and preserve the original Kafka partitioning scheme.", "This means that the following is true of RDDs.", "An executor is Spark\u2019s nomenclature for a distributed compute process which is simply a JVM process running on a Spark Worker. The sole job of an executor is to be dedicated fully to the processing of work described as tasks, within stages of a job (See the Spark Docs for more details).", "I\u2019ve always considered caching as a typical component of tuning and given that this post is all about caching, then let us just assume that you will take home a little tuning and debug-ability skills for free!", "Using cache appropriately within Apache Spark allows you to be a master over your available resources. Memory is not free, although it can be cheap, but in many cases the cost to store a DataFrame in memory is actually more expensive in the long run than going back to the source of truth dataset. If you have taken a look at the Spark UI (runs on port 4040 when spark.ui.enabled is set to true) and have determined that you can\u2019t squeeze performance out of the system, then you are a candidate for applying caching.", "Mainly if the most time taken during your computations is loading data from System A or B or C (say HDFS or MySQL or Redis) and you can\u2019t speed up anymore by scaling out. Scaling out with spark means adding more CPU cores across more RAM across more Machines. Then you can start to look at selectively caching portions of your most expensive computations.", "The above configuration would allow you to take up 8 machines running 8 cores by 12gb ram. *We use c5d.2xl machines from amazon and allocate 2gb for off-heap and 2gb for Operating System as well. Hence the 12g in the example above.", "The notion of the StorageLevel comes up when you are caching off the lineage of your DataFrame.", "Above we have an example of loading, transforming and joining data with an additional data frame. Given that this operation is lazy, in that nothing will be computed until you provide an action, we can then cheat and cache this lineage by calling head on our DataFrame to push Spark into action.", "Once you have cached your computations, in this case calling persist with the option of StorageLevel.MEMORY_ONLY_SER you can proceed with as many operations against the results of your initial computations without having to restart each action at the source or your initial read command.", "StorageLevel has options which range from DISK to OFF_HEAP to NONE. I suggest taking a look at the options there and testing different options that help to speed up your Spark Applications.", "When you are all done with your cached data, you can remove it from Spark really easily by calling unpersist. * tap with wand and say mischief managed \u2014 for the Harry Potter fans out there.", "This will remove the cache either immediately or you can also use the blocking option.", "Then you will free up your resources for the rest of your necessary computations.", "The Cross Product of Big Data and Big Data is Out of Memory Exceptions \u2014 Holden Karau", "The approach I take to caching and persistence with Spark is to treat it more like the JVM garbage collector. If I need to access the same data more than once then I store it in cache. This allows me to reduce round-trips to my source of truth for the data which is either s3a:// or hdfs://. Given the type of data I work with, which is mainly parquet and other FileSystem based data, is heavy on the IO and the total loaded files side. Once I have my data loaded it is often better to cache things off before even making another decision so I don\u2019t inadvertently make needless trips back to hdfs or s3 to load the same data again.", "I work on the Voice Insights team at Twilio where we are ingesting millions of events every few seconds across a slew of Spark Streaming applications all running within our data lineage pipeline. We are a hybrid Data Platform / Data Engineering and Data Sciences team working in California.", "See my other post on Streaming Trend Discovery to see how we handled some of the more complex use cases with Spark.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Distinguished Software Engineer @ Nike. I write about all things data, my views are my own."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F14235f40a29&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcache-patterns-with-apache-spark-14235f40a29&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcache-patterns-with-apache-spark-14235f40a29&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcache-patterns-with-apache-spark-14235f40a29&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcache-patterns-with-apache-spark-14235f40a29&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----14235f40a29--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----14235f40a29--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://newfrontcreative.medium.com/?source=post_page-----14235f40a29--------------------------------", "anchor_text": ""}, {"url": "https://newfrontcreative.medium.com/?source=post_page-----14235f40a29--------------------------------", "anchor_text": "Scott Haines"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3b4cab6af83e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcache-patterns-with-apache-spark-14235f40a29&user=Scott+Haines&userId=3b4cab6af83e&source=post_page-3b4cab6af83e----14235f40a29---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F14235f40a29&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcache-patterns-with-apache-spark-14235f40a29&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F14235f40a29&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcache-patterns-with-apache-spark-14235f40a29&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/photos/tMMapPBWK6o", "anchor_text": "Credit"}, {"url": "http://spark.apache.org/", "anchor_text": "Apache Spark"}, {"url": "https://en.wikipedia.org/wiki/Extract,_transform,_load", "anchor_text": "ETL"}, {"url": "https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-DataFrameReader.html", "anchor_text": "DataFrameReader"}, {"url": "https://unsplash.com/photos/lQT_bOWtysE", "anchor_text": "Unsplash"}, {"url": "https://en.wikipedia.org/wiki/Cyclic_redundancy_check", "anchor_text": "crc hash"}, {"url": "http://spark.apache.org/docs/latest/cluster-overview.html#components", "anchor_text": "Photo Credit"}, {"url": "http://spark.apache.org/docs/latest/cluster-overview.html#components", "anchor_text": "See the Spark Docs for more details"}, {"url": "https://unsplash.com/photos/lYxQ5F9xBDM", "anchor_text": "Credit"}, {"url": "https://www.oreilly.com/library/view/debugging-apache-spark/9781492039174/", "anchor_text": "debug-ability skills"}, {"url": "https://spark.apache.org/docs/2.4.5/api/java/index.html?org/apache/spark/storage/StorageLevel.html", "anchor_text": "StorageLevel"}, {"url": "https://twitter.com/holdenkarau", "anchor_text": "Holden Karau"}, {"url": "https://www.twilio.com/voice/insights", "anchor_text": "Voice Insights team at Twilio"}, {"url": "https://medium.com/@newfrontcreative/streaming-trend-discovery-real-time-discovery-in-a-sea-of-events-76bc03367e6f", "anchor_text": "Streaming Trend Discovery"}, {"url": "https://medium.com/tag/apache-spark?source=post_page-----14235f40a29---------------apache_spark-----------------", "anchor_text": "Apache Spark"}, {"url": "https://medium.com/tag/data-engineering?source=post_page-----14235f40a29---------------data_engineering-----------------", "anchor_text": "Data Engineering"}, {"url": "https://medium.com/tag/big-data-analytics?source=post_page-----14235f40a29---------------big_data_analytics-----------------", "anchor_text": "Big Data Analytics"}, {"url": "https://medium.com/tag/big-data?source=post_page-----14235f40a29---------------big_data-----------------", "anchor_text": "Big Data"}, {"url": "https://medium.com/tag/optimization?source=post_page-----14235f40a29---------------optimization-----------------", "anchor_text": "Optimization"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F14235f40a29&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcache-patterns-with-apache-spark-14235f40a29&user=Scott+Haines&userId=3b4cab6af83e&source=-----14235f40a29---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F14235f40a29&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcache-patterns-with-apache-spark-14235f40a29&user=Scott+Haines&userId=3b4cab6af83e&source=-----14235f40a29---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F14235f40a29&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcache-patterns-with-apache-spark-14235f40a29&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----14235f40a29--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F14235f40a29&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcache-patterns-with-apache-spark-14235f40a29&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----14235f40a29---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----14235f40a29--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----14235f40a29--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----14235f40a29--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----14235f40a29--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----14235f40a29--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----14235f40a29--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----14235f40a29--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----14235f40a29--------------------------------", "anchor_text": ""}, {"url": "https://newfrontcreative.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://newfrontcreative.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Scott Haines"}, {"url": "https://newfrontcreative.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "385 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3b4cab6af83e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcache-patterns-with-apache-spark-14235f40a29&user=Scott+Haines&userId=3b4cab6af83e&source=post_page-3b4cab6af83e--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Ffcdc220c6b1a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcache-patterns-with-apache-spark-14235f40a29&newsletterV3=3b4cab6af83e&newsletterV3Id=fcdc220c6b1a&user=Scott+Haines&userId=3b4cab6af83e&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}