{"url": "https://towardsdatascience.com/hbase-working-principle-a-part-of-hadoop-architecture-fbe0453a031b", "time": 1682996332.982114, "path": "towardsdatascience.com/hbase-working-principle-a-part-of-hadoop-architecture-fbe0453a031b/", "webpage": {"metadata": {"title": "HBase Working Principle: A part Hadoop Architecture | by Sahil Dhankhad | Towards Data Science", "h1": "HBase Working Principle: A part Hadoop Architecture", "description": "HBase is a high-reliability, high-performance, column-oriented, scalable distributed storage system that uses HBase technology to build large-scale structured storage clusters on inexpensive PC\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.linkedin.com/in/sahil-dhankhad-303350135/", "anchor_text": "LinkedIn", "paragraph_index": 57}], "all_paragraphs": ["HBase is a high-reliability, high-performance, column-oriented, scalable distributed storage system that uses HBase technology to build large-scale structured storage clusters on inexpensive PC Servers. The goal of HBase is to store and process large amounts of data, specifically to handle large amounts of data consisting of thousands of rows and columns using only standard hardware configurations.", "Different from MapReduce\u2019s offline batch computing framework, HBase is random access storage and retrieval data platform, which makes up for the shortcomings of HDFS that cannot access data randomly.", "It is suitable for business scenarios where real-time requirements are not very high \u2014 HBase stores Byte arrays, which don\u2019t mind data types, allowing dynamic, flexible data models.", "The figure above depicts the various layers of the Hadoop 2.0 ecosystem \u2014 Hbase located on the structured storage layer.", "HDFS provides high-reliability low-level storage support for HBase.", "MapReduce provides high-performance batch processing capability for HBase. ZooKeeper provides stable services and failover mechanism for HBase. Pig and Hive provide HBase for high-level language support for data statistics processing, Sqoop provides HDB with available RDBMS data import function, which makes it very convenient to migrate business data from a traditional database to HBase.", "2.1 Design IdeaHBase is a distributed database that uses ZooKeeper to manage clusters and HDFS as the underlying storage.", "At the architectural level, it consists of HMaster (Leader elected by Zookeeper) and multiple HRegionServers.", "The underlying architecture is shown in the following figure:", "In the concept of HBase, HRegionServer corresponds to one node in the cluster, one HRegionServer is responsible for managing multiple HRegions, and one HRegion represents a part of the data of a table.", "In HBase, a table may require a lot of HRegions to store data, and the data in each HRegion is not disorganized.", "When HBase manages HRegion, it will define a range of Rowkey for each HRegion. The data falling within a defined scope will be handed over to a specific Region, thus distributing the load to multiple nodes, thus taking advantage of the advantages of distributed and characteristic.", "Also, HBase will automatically adjust the location of the Region. If an HRegionServer is overheated, that is, a large number of requests fall on the HRegion managed by the HRegionServer, HBase will move the HRegion to other nodes that are relatively idle, ensuring that the cluster environment is fully utilized.", "HBase consists of HMaster and HRegionServer and also follows the master-slave server architecture. HBase divides the logical table into multiple data blocks, HRegion, and stores them in HRegionServer.", "HMaster is responsible for managing all HRegionServers. It does not store any data itself, but only stores the mappings (metadata) of data to HRegionServer.", "All nodes in the cluster are coordinated by Zookeeper and handle various issues that may be encountered during HBase operation. The basic architecture of HBase is shown below:", "Client : Use HBase\u2019s RPC mechanism to communicate with HMaster and HRegionServer, submit requests and get results. For management operations, the client performs RPC with HMaster. For data read and write operations, the client performs RPC with HRegionServer.", "Zookeeper: By registering the status information of each node in the cluster to ZooKeeper, HMaster can sense the health status of each HRegionServer at any time, and can also avoid the single point problem of HMaster.", "HMaster: Manage all HRegionServers, tell them which HRegions need to be maintained, and monitor the health of all HRegionServers. When a new HRegionServer logs in to HMaster, HMaster tells it to wait for data to be allocated. When an HRegion dies, HMaster marks all HRegions it is responsible for as unallocated and then assigns them to other HRegionServers. HMaster does not have a single point problem. HBase can start multiple HMasters. Through the Zookeeper\u2019s election mechanism, there is always one HMaster running in the cluster, which improves the availability of the cluster.", "HRegion: When the size of the table exceeds the preset value, HBase will automatically divide the table into different areas, each of which contains a subset of all the rows in the table. For the user, each table is a collection of data, distinguished by a primary key (RowKey). Physically, a table is split into multiple blocks, each of which is an HRegion. We use the table name + start/end primary key to distinguish each HRegion. One HRegion will save a piece of continuous data in a table. A complete table data is stored in multiple HRegions.", "HRegionServer: All data in HBase is generally stored in HDFS from the bottom layer. Users can obtain this data through a series of HRegionServers. Generally, only one HRegionServer is running on one node of the cluster, and the HRegion of each segment is only maintained by one HRegionServer. HRegionServer is mainly responsible for reading and writing data to the HDFS file system in response to user I/O requests. It is the core module in HBase. HRegionServer internally manages a series of HRegion objects, each HRegion corresponding to a continuous data segment in the logical table. HRegion is composed of multiple HStores. Each HStore corresponds to the storage of one column family in the logical table. It can be seen that each column family is a centralized storage unit. Therefore, to improve operational efficiency, it is preferable to place columns with common I/O characteristics in one column family.", "HStore: It is the core of HBase storage, which consists of MemStore and StoreFiles. MemStore is a memory buffer. The data written by the user will first be put into MemStore. When MemStore is full, Flush will be a StoreFile (the underlying implementation is HFile). When the number of StoreFile files increases to a certain threshold, the Compact merge operation will be triggered, merge multiple StoreFiles into one StoreFile, and perform version merge and data delete operations during the merge process. Therefore, it can be seen that HBase only adds data, and all update and delete operations are performed in the subsequent Compact process, so that the user\u2019s write operation can be returned as soon as it enters the memory, ensuring the high performance of HBaseI/O. When StoreFiles Compact, it will gradually form a larger and larger StoreFile. When the size of a single StoreFile exceeds a certain threshold, the Split operation will be triggered. At the same time, the current HRegion will be split into 2 HRegions, and the parent HRegion will go offline. The two sub-HRegions are assigned to the corresponding HRegionServer by HMaster so that the load pressure of the original HRegion is shunted to the two HRegions.", "HLog: Each HRegionServer has an HLog object, which is a pre-written log class that implements the Write Ahead Log. Each time a user writes data to MemStore, it also writes a copy of the data to the HLog file. The HLog file is periodically scrolled and deleted, and the old file is deleted (data that has been persisted to the StoreFile). When HMaster detects that an HRegionServer is terminated unexpectedly by the Zookeeper, HMaster first processes the legacy HLog file, splits the HLog data of different HRegions, puts them into the corresponding HRegion directory, and then redistributes the invalid HRegions. In the process of loading HRegion, HRegionServer of these HRegions will find that there is a history HLog needs to be processed so the data in Replay HLog will be transferred to MemStore, then Flush to StoreFiles to complete data recovery.", "All HRegion metadata of HBase is stored in the .META. table. As HRegion increases, the data in the .META table also increases and splits into multiple new HRegions.", "To locate the location of each HRegion in the .META table, the metadata of all HRegions in the .META the table is stored in the -ROOT-table, and finally, the location information of the ROOT-table is recorded by Zookeeper.", "Before all clients access user data, they need to first access Zookeeper to obtain the location of -ROOT-, then access the -ROOT-table to get the location of the .META table, and finally determine the location of the user data according to the information in the META table, as follows: The figure shows.", "The -ROOT-table is never split. It has only one HRegion, which guarantees that any HRegion can be located with only three jumps. To speed up access, all regions of the .META table are kept in memory.", "The client caches the queried location information, and the cache does not actively fail. If the client still does not have access to the data based on the cached information, then ask the Region server of the relevant .META table to try to obtain the location of the data. If it still fails, ask where the .META table associated with the -ROOT-table is.", "Finally, if the previous information is all invalid, the data of HRegion is relocated by ZooKeeper. So if the cache on the client is entirely invalid, you need to go back and forth six times to get the correct HRegion.", "HBase is a distributed database similar to BigTable. It is sparse long-term storage (on HDFS), multi-dimensional, and sorted mapping tables. The index of this table is the row keyword, column keyword, and timestamp. HBase data is a string, no type.", "Think of a table as a large mapping. You can locate specific data by row key, row key + timestamp or row key + column (column family: column modifier). Since HBase is sparsely storing data, some columns can be blank. The above table gives the logical storage logical view of the com.cnn.www website. There is only one row of data in the table.", "The unique identifier of the row is \u201ccom.cnn.www\u201d, and there is a time for each logical modification of this row of data. The stamp corresponds to the corresponding.", "There are four columns in the table: contents: HTML, anchor:cnnsi.com, anchor:my.look.ca, mime: type and each column give the column family to which it belongs.", "The row key (RowKey) is the unique identifier of the data row in the table and serves as the primary key for retrieving records.", "There are only three ways to access rows in a table in HBase: access via a row key, range access for a given row key, and full table scan.", "The row key can be any string (maximum length 64KB) and stored in lexicographical order. For rows that are often read together, the fundamental values \u200b\u200bneed to be carefully designed so that they can be stored collectively.", "The figure below is the HRegionServer data storage relationship diagram. As mentioned above, HBase uses MemStore and StoreFile to store updates to the table. The data is first written to HLog and MemStore when it is updated. The data in the MemStore is sorted.", "When the MemStore accumulates to a certain threshold, a new MemStore is created, and the old MemStore is added to the Flush queue, and a separate thread is flushed to the disk to become a StoreFile. At the same time, the system will record a CheckPoint in Zookeeper, indicating that the data changes before this time have been persisted. When an unexpected system occurs, the data in the MemStore may be lost.", "In this case, HLog is used to recover the data after CheckPoint.", "StoreFile is read-only and cannot be modified once created. Therefore, the update of HBase is an additional operation. When the StoreFile in a store reaches a certain threshold, a merge operation is performed, and the modifications of the same key are merged to form a large StoreFile. When the size of the StoreFile reaches a certain threshold, the StoreFile is split and divided into two StoreFiles.", "Step 1: The client sends a write data request to the HRegionServer through the scheduling of the Zookeeper, and writes the data in the HRegion.", "Step 2: The data is written to the MemStore of HRegion until the MemStore reaches the preset threshold.", "Step 3: The data in MemStore is Flushed into a StoreFile.", "Step 4: As the number of StoreFile files increases, when the number of the StoreFile files increases to a certain threshold, the Compact merge operation is triggered, and multiple StoreFiles are merged into one StoreFile, and version merge and data deletion are performed at the same time.", "Step 5: StoreFiles gradually forms a larger and larger StoreFile through the continuous Compact operation.", "Step 6: After the size of a single StoreFile exceeds a certain threshold, the Split operation is triggered to split the current HRegion into two new HRegions. The parent HRegion will go offline, and the two sub-HRegions from the new Split will be assigned to the corresponding HRegionServer by HMaster so that the pressure of the original HRegion can be shunted to the two HRegions.", "Step 1: The client accesses Zookeeper, finds the -ROOT-table, and obtains the .META. table information.", "Step 2: Search from the .META. table to obtain the HRegion information of the target data, to find the corresponding HRegionServer.", "Step 3: Obtain the data you need to find through HRegionServer.", "Step 4: The memory of the HRegionserver is divided into two parts: MemStore and BlockCache. MemStore is mainly used to write data, and BlockCache is mainly used to read data. Read the request first to the MemStore to check the data, check the BlockCache check, and then check the StoreFile, and put the read result into the BlockCache.", "Semi-structured or unstructured data: For data structure fields that are not well defined or cluttered, it is difficult to extract data according to a concept suitable for HBase. If more fields are stored as the business grows, the RDBMS needs to be down to maintain the change table structure, and HBase supports dynamic additions.", "The records are very sparse: how many columns of the RDBMS row are fixed, and empty columns waste storage space. Columns with empty HBase are not stored, which saves space and improves read performance.", "Multi-version data: Values \u200b\u200bthat are located according to the RowKey and column identifiers can have any number of version values \u200b\u200b(timestamps are different), so it is very convenient to use HBase for data that needs to store the change history.", "A large amount of data: When the amount of data is getting larger and larger, the RDBMS database can\u2019t hold up, and there is a read-write separation strategy. Through a master, it is responsible for write operations, and multiple slaves are responsible for reading operations, and the server cost is doubled. As the pressure increases, the Master can\u2019t hold it. At this time, the library is divided, and the data with little correlation is deployed separately. Some join queries cannot be used, and the middle layer needs to be used. As the amount of data increases further, the record of a table becomes larger and larger, and the query becomes very slow.", "Therefore, it is necessary to divide the table, for example, by modulo the ID into multiple tables to reduce the number of records of a single table. People who have experienced these things know how to toss the process.", "HBase is simple, just add new nodes to the cluster, HBase will automatically split horizontally, and seamless integration with Hadoop ensures data reliability (HDFS) and high performance of massive data analysis (MapReduce ).", "The relationship between Table and Region in HBase is somewhat similar to the relationship between File and Block in HDFS. Since HBase provides APIs for interacting with MapReduce, such as TableInputFormat and TableOutputFormat, HBase data tables can be directly used as input and output of Hadoop MapReduce, which facilitates the development of MapReduce applications, and does not need to pay attention to the processing of HBase system itself Detail.", "If you liked this topic, you can have a look of the few other topics that I have written down about the Hadoop. If you find any mistakes or have any suggestion, please feel free to contact me on my LinkedIn."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Ffbe0453a031b&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhbase-working-principle-a-part-of-hadoop-architecture-fbe0453a031b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhbase-working-principle-a-part-of-hadoop-architecture-fbe0453a031b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhbase-working-principle-a-part-of-hadoop-architecture-fbe0453a031b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhbase-working-principle-a-part-of-hadoop-architecture-fbe0453a031b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/@sahildhankhad?source=post_page-----fbe0453a031b--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----fbe0453a031b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@sahildhankhad?source=post_page-----fbe0453a031b--------------------------------", "anchor_text": "Sahil Dhankhad"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe99d0a3d60ad&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhbase-working-principle-a-part-of-hadoop-architecture-fbe0453a031b&user=Sahil+Dhankhad&userId=e99d0a3d60ad&source=post_page-e99d0a3d60ad----fbe0453a031b---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----fbe0453a031b--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffbe0453a031b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhbase-working-principle-a-part-of-hadoop-architecture-fbe0453a031b&user=Sahil+Dhankhad&userId=e99d0a3d60ad&source=-----fbe0453a031b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffbe0453a031b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhbase-working-principle-a-part-of-hadoop-architecture-fbe0453a031b&source=-----fbe0453a031b---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://www.linkedin.com/in/sahil-dhankhad-303350135/", "anchor_text": "LinkedIn"}, {"url": "https://towardsdatascience.com/a-brief-summary-of-apache-hadoop-a-solution-of-big-data-problem-and-hint-comes-from-google-95fd63b83623", "anchor_text": "A Brief Summary of Apache Hadoop: A Solution of Big Data Problem and Hint comes from GoogleWelcome to the introduction of Big data and Hadoop where we are going to talk about Apache Hadoop and problems that big\u2026towardsdatascience.com"}, {"url": "https://towardsdatascience.com/new-in-hadoop-you-should-know-the-various-file-format-in-hadoop-4fcdfa25d42b", "anchor_text": "New in Hadoop: You should know the Various File Format in Hadoop.A Beginners\u2019 Guide to Hadoop File Formatstowardsdatascience.com"}, {"url": "https://medium.com/tag/big-data?source=post_page-----fbe0453a031b---------------big_data-----------------", "anchor_text": "Big Data"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----fbe0453a031b---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/data-science?source=post_page-----fbe0453a031b---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/science?source=post_page-----fbe0453a031b---------------science-----------------", "anchor_text": "Science"}, {"url": "https://medium.com/tag/data?source=post_page-----fbe0453a031b---------------data-----------------", "anchor_text": "Data"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffbe0453a031b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhbase-working-principle-a-part-of-hadoop-architecture-fbe0453a031b&user=Sahil+Dhankhad&userId=e99d0a3d60ad&source=-----fbe0453a031b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffbe0453a031b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhbase-working-principle-a-part-of-hadoop-architecture-fbe0453a031b&user=Sahil+Dhankhad&userId=e99d0a3d60ad&source=-----fbe0453a031b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffbe0453a031b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhbase-working-principle-a-part-of-hadoop-architecture-fbe0453a031b&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/@sahildhankhad?source=post_page-----fbe0453a031b--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----fbe0453a031b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe99d0a3d60ad&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhbase-working-principle-a-part-of-hadoop-architecture-fbe0453a031b&user=Sahil+Dhankhad&userId=e99d0a3d60ad&source=post_page-e99d0a3d60ad----fbe0453a031b---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Ff800a3871395&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhbase-working-principle-a-part-of-hadoop-architecture-fbe0453a031b&newsletterV3=e99d0a3d60ad&newsletterV3Id=f800a3871395&user=Sahil+Dhankhad&userId=e99d0a3d60ad&source=-----fbe0453a031b---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/@sahildhankhad?source=post_page-----fbe0453a031b--------------------------------", "anchor_text": "Written by Sahil Dhankhad"}, {"url": "https://medium.com/@sahildhankhad/followers?source=post_page-----fbe0453a031b--------------------------------", "anchor_text": "298 Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----fbe0453a031b--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "http://www.linkedin.com/in/sahil-dhankhad-303350135", "anchor_text": "www.linkedin.com/in/sahil-dhankhad-303350135"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe99d0a3d60ad&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhbase-working-principle-a-part-of-hadoop-architecture-fbe0453a031b&user=Sahil+Dhankhad&userId=e99d0a3d60ad&source=post_page-e99d0a3d60ad----fbe0453a031b---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Ff800a3871395&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhbase-working-principle-a-part-of-hadoop-architecture-fbe0453a031b&newsletterV3=e99d0a3d60ad&newsletterV3Id=f800a3871395&user=Sahil+Dhankhad&userId=e99d0a3d60ad&source=-----fbe0453a031b---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/forget-apis-do-python-scraping-using-beautiful-soup-import-data-file-from-the-web-part-2-27af5d666246?source=author_recirc-----fbe0453a031b----0---------------------ab274fc3_49c8_4400_bdde_edb104d5e8b8-------", "anchor_text": ""}, {"url": "https://medium.com/@sahildhankhad?source=author_recirc-----fbe0453a031b----0---------------------ab274fc3_49c8_4400_bdde_edb104d5e8b8-------", "anchor_text": ""}, {"url": "https://medium.com/@sahildhankhad?source=author_recirc-----fbe0453a031b----0---------------------ab274fc3_49c8_4400_bdde_edb104d5e8b8-------", "anchor_text": "Sahil Dhankhad"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----fbe0453a031b----0---------------------ab274fc3_49c8_4400_bdde_edb104d5e8b8-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/forget-apis-do-python-scraping-using-beautiful-soup-import-data-file-from-the-web-part-2-27af5d666246?source=author_recirc-----fbe0453a031b----0---------------------ab274fc3_49c8_4400_bdde_edb104d5e8b8-------", "anchor_text": "Forget APIs Do Python Scraping Using Beautiful Soup, Import Data File from the web: Part 2APIs are not all there for you, but Beautiful Soup is going to stay with you forever."}, {"url": "https://towardsdatascience.com/forget-apis-do-python-scraping-using-beautiful-soup-import-data-file-from-the-web-part-2-27af5d666246?source=author_recirc-----fbe0453a031b----0---------------------ab274fc3_49c8_4400_bdde_edb104d5e8b8-------", "anchor_text": "\u00b78 min read\u00b7Apr 1, 2019"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F27af5d666246&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fforget-apis-do-python-scraping-using-beautiful-soup-import-data-file-from-the-web-part-2-27af5d666246&user=Sahil+Dhankhad&userId=e99d0a3d60ad&source=-----27af5d666246----0-----------------clap_footer----ab274fc3_49c8_4400_bdde_edb104d5e8b8-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/forget-apis-do-python-scraping-using-beautiful-soup-import-data-file-from-the-web-part-2-27af5d666246?source=author_recirc-----fbe0453a031b----0---------------------ab274fc3_49c8_4400_bdde_edb104d5e8b8-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F27af5d666246&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fforget-apis-do-python-scraping-using-beautiful-soup-import-data-file-from-the-web-part-2-27af5d666246&source=-----fbe0453a031b----0-----------------bookmark_preview----ab274fc3_49c8_4400_bdde_edb104d5e8b8-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----fbe0453a031b----1---------------------ab274fc3_49c8_4400_bdde_edb104d5e8b8-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----fbe0453a031b----1---------------------ab274fc3_49c8_4400_bdde_edb104d5e8b8-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----fbe0453a031b----1---------------------ab274fc3_49c8_4400_bdde_edb104d5e8b8-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----fbe0453a031b----1---------------------ab274fc3_49c8_4400_bdde_edb104d5e8b8-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----fbe0453a031b----1---------------------ab274fc3_49c8_4400_bdde_edb104d5e8b8-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----fbe0453a031b----1---------------------ab274fc3_49c8_4400_bdde_edb104d5e8b8-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----ab274fc3_49c8_4400_bdde_edb104d5e8b8-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----fbe0453a031b----1---------------------ab274fc3_49c8_4400_bdde_edb104d5e8b8-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----fbe0453a031b----1-----------------bookmark_preview----ab274fc3_49c8_4400_bdde_edb104d5e8b8-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----fbe0453a031b----2---------------------ab274fc3_49c8_4400_bdde_edb104d5e8b8-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----fbe0453a031b----2---------------------ab274fc3_49c8_4400_bdde_edb104d5e8b8-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----fbe0453a031b----2---------------------ab274fc3_49c8_4400_bdde_edb104d5e8b8-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----fbe0453a031b----2---------------------ab274fc3_49c8_4400_bdde_edb104d5e8b8-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----fbe0453a031b----2---------------------ab274fc3_49c8_4400_bdde_edb104d5e8b8-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----fbe0453a031b----2---------------------ab274fc3_49c8_4400_bdde_edb104d5e8b8-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----2-----------------clap_footer----ab274fc3_49c8_4400_bdde_edb104d5e8b8-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----fbe0453a031b----2---------------------ab274fc3_49c8_4400_bdde_edb104d5e8b8-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----fbe0453a031b----2-----------------bookmark_preview----ab274fc3_49c8_4400_bdde_edb104d5e8b8-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/new-in-hadoop-you-should-know-the-various-file-format-in-hadoop-4fcdfa25d42b?source=author_recirc-----fbe0453a031b----3---------------------ab274fc3_49c8_4400_bdde_edb104d5e8b8-------", "anchor_text": ""}, {"url": "https://medium.com/@sahildhankhad?source=author_recirc-----fbe0453a031b----3---------------------ab274fc3_49c8_4400_bdde_edb104d5e8b8-------", "anchor_text": ""}, {"url": "https://medium.com/@sahildhankhad?source=author_recirc-----fbe0453a031b----3---------------------ab274fc3_49c8_4400_bdde_edb104d5e8b8-------", "anchor_text": "Sahil Dhankhad"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----fbe0453a031b----3---------------------ab274fc3_49c8_4400_bdde_edb104d5e8b8-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/new-in-hadoop-you-should-know-the-various-file-format-in-hadoop-4fcdfa25d42b?source=author_recirc-----fbe0453a031b----3---------------------ab274fc3_49c8_4400_bdde_edb104d5e8b8-------", "anchor_text": "New in Hadoop: You should know the Various File Format in Hadoop.A Beginners\u2019 Guide to Hadoop File Formats"}, {"url": "https://towardsdatascience.com/new-in-hadoop-you-should-know-the-various-file-format-in-hadoop-4fcdfa25d42b?source=author_recirc-----fbe0453a031b----3---------------------ab274fc3_49c8_4400_bdde_edb104d5e8b8-------", "anchor_text": "\u00b75 min read\u00b7Apr 22, 2019"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4fcdfa25d42b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnew-in-hadoop-you-should-know-the-various-file-format-in-hadoop-4fcdfa25d42b&user=Sahil+Dhankhad&userId=e99d0a3d60ad&source=-----4fcdfa25d42b----3-----------------clap_footer----ab274fc3_49c8_4400_bdde_edb104d5e8b8-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/new-in-hadoop-you-should-know-the-various-file-format-in-hadoop-4fcdfa25d42b?source=author_recirc-----fbe0453a031b----3---------------------ab274fc3_49c8_4400_bdde_edb104d5e8b8-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4fcdfa25d42b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnew-in-hadoop-you-should-know-the-various-file-format-in-hadoop-4fcdfa25d42b&source=-----fbe0453a031b----3-----------------bookmark_preview----ab274fc3_49c8_4400_bdde_edb104d5e8b8-------", "anchor_text": ""}, {"url": "https://medium.com/@sahildhankhad?source=post_page-----fbe0453a031b--------------------------------", "anchor_text": "See all from Sahil Dhankhad"}, {"url": "https://towardsdatascience.com/?source=post_page-----fbe0453a031b--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----fbe0453a031b----0---------------------91593b63_55c9_4e46_a022_b65321d1c407-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----fbe0453a031b----0---------------------91593b63_55c9_4e46_a022_b65321d1c407-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----fbe0453a031b----0---------------------91593b63_55c9_4e46_a022_b65321d1c407-------", "anchor_text": "Alexander Nguyen"}, {"url": "https://levelup.gitconnected.com/?source=read_next_recirc-----fbe0453a031b----0---------------------91593b63_55c9_4e46_a022_b65321d1c407-------", "anchor_text": "Level Up Coding"}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----fbe0453a031b----0---------------------91593b63_55c9_4e46_a022_b65321d1c407-------", "anchor_text": "Why I Keep Failing Candidates During Google Interviews\u2026They don\u2019t meet the bar."}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----fbe0453a031b----0---------------------91593b63_55c9_4e46_a022_b65321d1c407-------", "anchor_text": "\u00b74 min read\u00b7Apr 13"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fgitconnected%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&user=Alexander+Nguyen&userId=a148fd75c2e9&source=-----dc8f865b2c19----0-----------------clap_footer----91593b63_55c9_4e46_a022_b65321d1c407-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----fbe0453a031b----0---------------------91593b63_55c9_4e46_a022_b65321d1c407-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "89"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&source=-----fbe0453a031b----0-----------------bookmark_preview----91593b63_55c9_4e46_a022_b65321d1c407-------", "anchor_text": ""}, {"url": "https://avinashnavlani.medium.com/apache-hive-hands-on-13342286b96a?source=read_next_recirc-----fbe0453a031b----1---------------------91593b63_55c9_4e46_a022_b65321d1c407-------", "anchor_text": ""}, {"url": "https://avinashnavlani.medium.com/?source=read_next_recirc-----fbe0453a031b----1---------------------91593b63_55c9_4e46_a022_b65321d1c407-------", "anchor_text": ""}, {"url": "https://avinashnavlani.medium.com/?source=read_next_recirc-----fbe0453a031b----1---------------------91593b63_55c9_4e46_a022_b65321d1c407-------", "anchor_text": "Avinash Navlani"}, {"url": "https://avinashnavlani.medium.com/apache-hive-hands-on-13342286b96a?source=read_next_recirc-----fbe0453a031b----1---------------------91593b63_55c9_4e46_a022_b65321d1c407-------", "anchor_text": "Apache Hive Hands-onIn this tutorial, we will focus on Hadoop Hive for processing big data."}, {"url": "https://avinashnavlani.medium.com/apache-hive-hands-on-13342286b96a?source=read_next_recirc-----fbe0453a031b----1---------------------91593b63_55c9_4e46_a022_b65321d1c407-------", "anchor_text": "\u00b77 min read\u00b7Nov 22, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F13342286b96a&operation=register&redirect=https%3A%2F%2Favinashnavlani.medium.com%2Fapache-hive-hands-on-13342286b96a&user=Avinash+Navlani&userId=82bb8547197f&source=-----13342286b96a----1-----------------clap_footer----91593b63_55c9_4e46_a022_b65321d1c407-------", "anchor_text": ""}, {"url": "https://avinashnavlani.medium.com/apache-hive-hands-on-13342286b96a?source=read_next_recirc-----fbe0453a031b----1---------------------91593b63_55c9_4e46_a022_b65321d1c407-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F13342286b96a&operation=register&redirect=https%3A%2F%2Favinashnavlani.medium.com%2Fapache-hive-hands-on-13342286b96a&source=-----fbe0453a031b----1-----------------bookmark_preview----91593b63_55c9_4e46_a022_b65321d1c407-------", "anchor_text": ""}, {"url": "https://billatnapier.medium.com/similarity-hashing-and-perceptial-hashes-963fba36c8b5?source=read_next_recirc-----fbe0453a031b----0---------------------91593b63_55c9_4e46_a022_b65321d1c407-------", "anchor_text": ""}, {"url": "https://billatnapier.medium.com/?source=read_next_recirc-----fbe0453a031b----0---------------------91593b63_55c9_4e46_a022_b65321d1c407-------", "anchor_text": ""}, {"url": "https://billatnapier.medium.com/?source=read_next_recirc-----fbe0453a031b----0---------------------91593b63_55c9_4e46_a022_b65321d1c407-------", "anchor_text": "Prof Bill Buchanan OBE"}, {"url": "https://billatnapier.medium.com/similarity-hashing-and-perceptial-hashes-963fba36c8b5?source=read_next_recirc-----fbe0453a031b----0---------------------91593b63_55c9_4e46_a022_b65321d1c407-------", "anchor_text": "Similarity Hashing and Perceptual HashesSean McKeown [here] and myself have just published a paper to arXiv that will be presented at DFRWS (Digital Forensics Research Conference)\u2026"}, {"url": "https://billatnapier.medium.com/similarity-hashing-and-perceptial-hashes-963fba36c8b5?source=read_next_recirc-----fbe0453a031b----0---------------------91593b63_55c9_4e46_a022_b65321d1c407-------", "anchor_text": "\u00b79 min read\u00b7Dec 17, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F963fba36c8b5&operation=register&redirect=https%3A%2F%2Fbillatnapier.medium.com%2Fsimilarity-hashing-and-perceptial-hashes-963fba36c8b5&user=Prof+Bill+Buchanan+OBE&userId=e680fcaf274b&source=-----963fba36c8b5----0-----------------clap_footer----91593b63_55c9_4e46_a022_b65321d1c407-------", "anchor_text": ""}, {"url": "https://billatnapier.medium.com/similarity-hashing-and-perceptial-hashes-963fba36c8b5?source=read_next_recirc-----fbe0453a031b----0---------------------91593b63_55c9_4e46_a022_b65321d1c407-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F963fba36c8b5&operation=register&redirect=https%3A%2F%2Fbillatnapier.medium.com%2Fsimilarity-hashing-and-perceptial-hashes-963fba36c8b5&source=-----fbe0453a031b----0-----------------bookmark_preview----91593b63_55c9_4e46_a022_b65321d1c407-------", "anchor_text": ""}, {"url": "https://blog.devgenius.io/difference-between-hadoop-and-spark-aa03c3cec416?source=read_next_recirc-----fbe0453a031b----1---------------------91593b63_55c9_4e46_a022_b65321d1c407-------", "anchor_text": ""}, {"url": "https://medium.com/@ansam.yousry?source=read_next_recirc-----fbe0453a031b----1---------------------91593b63_55c9_4e46_a022_b65321d1c407-------", "anchor_text": ""}, {"url": "https://medium.com/@ansam.yousry?source=read_next_recirc-----fbe0453a031b----1---------------------91593b63_55c9_4e46_a022_b65321d1c407-------", "anchor_text": "Ansam Yousry"}, {"url": "https://blog.devgenius.io/?source=read_next_recirc-----fbe0453a031b----1---------------------91593b63_55c9_4e46_a022_b65321d1c407-------", "anchor_text": "Dev Genius"}, {"url": "https://blog.devgenius.io/difference-between-hadoop-and-spark-aa03c3cec416?source=read_next_recirc-----fbe0453a031b----1---------------------91593b63_55c9_4e46_a022_b65321d1c407-------", "anchor_text": "Apache Spark vs HadoopMain differences between Hadoop and Spark"}, {"url": "https://blog.devgenius.io/difference-between-hadoop-and-spark-aa03c3cec416?source=read_next_recirc-----fbe0453a031b----1---------------------91593b63_55c9_4e46_a022_b65321d1c407-------", "anchor_text": "\u00b77 min read\u00b7Jan 1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fdev-genius%2Faa03c3cec416&operation=register&redirect=https%3A%2F%2Fblog.devgenius.io%2Fdifference-between-hadoop-and-spark-aa03c3cec416&user=Ansam+Yousry&userId=d215c0b84e88&source=-----aa03c3cec416----1-----------------clap_footer----91593b63_55c9_4e46_a022_b65321d1c407-------", "anchor_text": ""}, {"url": "https://blog.devgenius.io/difference-between-hadoop-and-spark-aa03c3cec416?source=read_next_recirc-----fbe0453a031b----1---------------------91593b63_55c9_4e46_a022_b65321d1c407-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Faa03c3cec416&operation=register&redirect=https%3A%2F%2Fblog.devgenius.io%2Fdifference-between-hadoop-and-spark-aa03c3cec416&source=-----fbe0453a031b----1-----------------bookmark_preview----91593b63_55c9_4e46_a022_b65321d1c407-------", "anchor_text": ""}, {"url": "https://medium.com/fintechexplained/what-is-event-driven-microservices-architecture-278d9e5adf8a?source=read_next_recirc-----fbe0453a031b----2---------------------91593b63_55c9_4e46_a022_b65321d1c407-------", "anchor_text": ""}, {"url": "https://medium.com/@farhadmalik?source=read_next_recirc-----fbe0453a031b----2---------------------91593b63_55c9_4e46_a022_b65321d1c407-------", "anchor_text": ""}, {"url": "https://medium.com/@farhadmalik?source=read_next_recirc-----fbe0453a031b----2---------------------91593b63_55c9_4e46_a022_b65321d1c407-------", "anchor_text": "Farhad Malik"}, {"url": "https://medium.com/fintechexplained?source=read_next_recirc-----fbe0453a031b----2---------------------91593b63_55c9_4e46_a022_b65321d1c407-------", "anchor_text": "FinTechExplained"}, {"url": "https://medium.com/fintechexplained/what-is-event-driven-microservices-architecture-278d9e5adf8a?source=read_next_recirc-----fbe0453a031b----2---------------------91593b63_55c9_4e46_a022_b65321d1c407-------", "anchor_text": "What Is Event-Driven Microservices Architecture?Designing Modern Event-Driven Microservices Applications With Kafka And Docker Containers Suitable For All Levels"}, {"url": "https://medium.com/fintechexplained/what-is-event-driven-microservices-architecture-278d9e5adf8a?source=read_next_recirc-----fbe0453a031b----2---------------------91593b63_55c9_4e46_a022_b65321d1c407-------", "anchor_text": "\u00b713 min read\u00b7Dec 5, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ffintechexplained%2F278d9e5adf8a&operation=register&redirect=https%3A%2F%2Fmedium.com%2Ffintechexplained%2Fwhat-is-event-driven-microservices-architecture-278d9e5adf8a&user=Farhad+Malik&userId=d9b237bc89f0&source=-----278d9e5adf8a----2-----------------clap_footer----91593b63_55c9_4e46_a022_b65321d1c407-------", "anchor_text": ""}, {"url": "https://medium.com/fintechexplained/what-is-event-driven-microservices-architecture-278d9e5adf8a?source=read_next_recirc-----fbe0453a031b----2---------------------91593b63_55c9_4e46_a022_b65321d1c407-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F278d9e5adf8a&operation=register&redirect=https%3A%2F%2Fmedium.com%2Ffintechexplained%2Fwhat-is-event-driven-microservices-architecture-278d9e5adf8a&source=-----fbe0453a031b----2-----------------bookmark_preview----91593b63_55c9_4e46_a022_b65321d1c407-------", "anchor_text": ""}, {"url": "https://medium.com/@SantalTech/no-leetcode-the-stripe-interview-experience-cf1b29e6f55d?source=read_next_recirc-----fbe0453a031b----3---------------------91593b63_55c9_4e46_a022_b65321d1c407-------", "anchor_text": ""}, {"url": "https://medium.com/@SantalTech?source=read_next_recirc-----fbe0453a031b----3---------------------91593b63_55c9_4e46_a022_b65321d1c407-------", "anchor_text": ""}, {"url": "https://medium.com/@SantalTech?source=read_next_recirc-----fbe0453a031b----3---------------------91593b63_55c9_4e46_a022_b65321d1c407-------", "anchor_text": "Santal Tech"}, {"url": "https://medium.com/@SantalTech/no-leetcode-the-stripe-interview-experience-cf1b29e6f55d?source=read_next_recirc-----fbe0453a031b----3---------------------91593b63_55c9_4e46_a022_b65321d1c407-------", "anchor_text": "No More Leetcode: The Stripe Interview ExperienceIn light of the recent layoffs, which I think Stripe handled well (CEO\u2019s note to employees), I wanted to share my interview experience with\u2026"}, {"url": "https://medium.com/@SantalTech/no-leetcode-the-stripe-interview-experience-cf1b29e6f55d?source=read_next_recirc-----fbe0453a031b----3---------------------91593b63_55c9_4e46_a022_b65321d1c407-------", "anchor_text": "\u00b75 min read\u00b7Nov 8, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2Fcf1b29e6f55d&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40SantalTech%2Fno-leetcode-the-stripe-interview-experience-cf1b29e6f55d&user=Santal+Tech&userId=99f97ad40c6a&source=-----cf1b29e6f55d----3-----------------clap_footer----91593b63_55c9_4e46_a022_b65321d1c407-------", "anchor_text": ""}, {"url": "https://medium.com/@SantalTech/no-leetcode-the-stripe-interview-experience-cf1b29e6f55d?source=read_next_recirc-----fbe0453a031b----3---------------------91593b63_55c9_4e46_a022_b65321d1c407-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "8"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcf1b29e6f55d&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40SantalTech%2Fno-leetcode-the-stripe-interview-experience-cf1b29e6f55d&source=-----fbe0453a031b----3-----------------bookmark_preview----91593b63_55c9_4e46_a022_b65321d1c407-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----fbe0453a031b--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----fbe0453a031b--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----fbe0453a031b--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----fbe0453a031b--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----fbe0453a031b--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----fbe0453a031b--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----fbe0453a031b--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----fbe0453a031b--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----fbe0453a031b--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----fbe0453a031b--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}