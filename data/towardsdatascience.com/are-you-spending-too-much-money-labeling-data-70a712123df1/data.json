{"url": "https://towardsdatascience.com/are-you-spending-too-much-money-labeling-data-70a712123df1", "time": 1683005143.7254138, "path": "towardsdatascience.com/are-you-spending-too-much-money-labeling-data-70a712123df1/", "webpage": {"metadata": {"title": "Are you spending too much money labeling data? | by Jennifer Prendki | Towards Data Science", "h1": "Are you spending too much money labeling data?", "description": "Data storage became cheap enough that companies started hoarding data without even knowing quite what to do with it. Data collection became ubiquitous, thanks in no small part to the internet of\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/alectio/can-you-lie-to-your-model-1-3-e03309e41291?source=collection_home---4------3-----------------------", "anchor_text": "we were able to determine that some classes were more sensitive than others to labeling noise", "paragraph_index": 28}], "all_paragraphs": ["Technologists will remember the 2010s as the decade of Big Data.", "Data storage became cheap enough that companies started hoarding data without even knowing quite what to do with it. Data collection became ubiquitous, thanks in no small part to the internet of things, which allowed for entire new streams of valuable, actionable data. And data processing benefited immensely from the emerging power of GPUs and TPUs to train more robust deep learning models.", "Having a preponderance of data to power business operations is, generally speaking, a really positive thing. But with 90% of the ML models in production today using a supervised learning approach, the success of these projects is hugely dependent on a company\u2019s ability to label its data accurately and efficiently.", "And that\u2019s easier said than done.", "If you\u2019re unfamiliar with the way labels are typically collected, you might assume that labeling is an easy task. Those whose careers have been dedicated to data labeling know this couldn\u2019t be further from the truth.", "Of course, the concept of annotation in itself is deceptively simple. Typically, data is annotated by people tasked with generating what is called \u201cground truth\u201d by the experts. That generated label (we talk about annotation if the label is more sophisticated than a simple value, like in the case of segmentation or bounding box annotations) is the class or value that we want models trained on that dataset to predict for that specific data point. For a simple example, an annotator might look at an image and label an object from a pre-existing ontology of classes. The label is fed to the machine for the process of learning. In a sense, labeling is the injection of human knowledge into the machine, which makes it a critically important step in developing a high-performance ML model. At the risk of being reductive, good labels drive good models.", "The problem is, in the real world, just because these labels seem easy on their face doesn\u2019t mean they\u2019re easy in practice.", "Here, we have a simple image with some simple instructions. A human annotator is asked to draw a rectangle around any people he sees in the raw, unlabeled image. Easy, right? Here, the image is clear; there is only one person and the person is not occluded. Yet there are many ways to get it wrong: for instance, drawing a box that is too large could cause the model to overfit on background noise and lose its ability to determine relevant patterns. Draw a box that\u2019s too small, and you might miss the interesting patterns. And that doesn\u2019t even account for the cases where the annotator, pressured to process data faster or to make more money, either makes a mistake (honest or not!) or fails to understand the instructions properly.", "Think now that this is actually an easy use case, as the image doesn\u2019t contain too many objects (real life cases actually look much messier) and the task at hand is objective. Images with multiple, partially occluded people are tougher. Same goes for something like sentiment analysis, where there is inherent subjectivity present.", "And we could go on. But the point is: even simple labeling tasks are fraught with potential errors. Those errors amount to bad data you\u2019re feeding your models. And chances are, you know where that leads.", "With all those challenges lurking, it can feel a lot harder to ever gather labels of sufficiently high quality to train a decent ML model. And yet supervised machine learning still has made a huge jump ahead over the last few years. So what gives?", "The \u201ctrick\u201d is that while it is unwise to trust a single human annotator, relying on a larger group actually typically gets us pretty far. That\u2019s because if one annotator makes a mistake, it\u2019s fairly unlikely that another one would make that exact same mistake. In other words, by collecting labels from multiple people for every single record, we are likely to get rid of most outliers and will ensure a higher likelihood that the label is the actual ground truth.", "For instance, if there is a 10% chance for an annotator to make a mistake on a binary use case, the probability for two annotators to get it wrong already drops at 0.1 x 0.1 = 0.01 = 1%, which is definitely better, but not quite enough for every (or even most) applications. Three or four annotators drops that number even further.", "The next question is whether you can combat inaccurately labeled training data with just more and more training data. In other words, can quantity fight poor quality? We discussed some of those tradeoffs elsewhere but one thing we were able to show was that some classes were more likely to be sensitive to bad labels than other classes and that the amount of additional data needed to fix the effect of bad labels varied immensely across the dataset. And it didn\u2019t really matter what model was being used. Bad labels pollute training data more and are significantly more harmful than just having less data that\u2019s well-labeled.", "In other words, not all data should be given the same attention when it comes to labeling. We also proved that sensitivity to labeling noise is nearly independent of the model. That is great news because it means a generic model can be used to identify the problematic clusters of data requiring a higher labeling accuracy, and rely on that information strategically when labeling the data.", "In other words, having a smart labeling strategy is much more important than you might think. Admittedly, it\u2019s not as sexy as researching the newest model, but it\u2019s oftentimes more vital to pushing a successful model to production. And if you ask any data labeling expert what comes to mind when discussing how to reduce their labeling budgets, they\u2019ll often say active learning.", "Active learning is a category of ML algorithms, a subset of semi-supervised learning which relies on an incremental learning approach and offers an elegant framework to allow ML scientists to work with datasets that are only partially labeled.", "Active learning is based on the simple yet powerful assumption that not all data is equally impactful on the model and that all data is not learned at the same pace by the model. That\u2019s both because datasets often contain a lot of duplicative information (which means that some records can individually be valuable but offer very little incremental improvement when used in conjunction with similar records) and also because some data does not contain any relevant information.", "An example? Say you want to train an OCR (Optical Character Recognition) algorithm but have a dataset where a large fraction of records do not contain any text at all. By incrementally adding more training records, active learning dynamically finds the records that are the hardest for the model to learn (or those that are the most beneficial) allowing the ML scientist to focus on the most important data first. Which is to say active learning selects the \u201cright\u201d records to label. And that\u2019s something budget holders can appreciate.", "But active learning misses something important. It cannot necessarily surmise the number of annotations that need to be collected for a given record.", "Generally, companies have always considered labeling frequency (or number of annotations per record) as a static parameter, which had to be predetermined as a function of their labeling budget and the required labeling accuracy. In other words: you label each image five times or each sentence three times. There really was no theory behind how to choose an optimal frequency, and no framework to dynamically tune that frequency as a function of a record\u2019s criticality to the model, its tolerance to noise, and how difficult it is to label it.", "Our recent study changes that. We now have data to model the impact of labeling accuracy to the model\u2019s accuracy and build strategies to optimize how to spend our labeling budget. Understanding which records need more labels and which don\u2019t opens the door to smartly optimized labeling budgeting strategies.", "So: labeling accuracy is a function of the number of annotations per record and the probability of mislabeling a record while model accuracy is a function of the accuracy of the labeling process and the size of the training set. When you combine those? You can start to figure out exactly how to marry a model\u2019s accuracy to its labeling budget.", "We simulated the situation of a 250,000 sample dataset for which each annotation would cost 25\u00a2, assumed a mislabeling probability of 40% across the dataset, and got the following relationships between the budget spent on labeling and the model\u2019s accuracy for 5 different volumes of training data:", "Right away, you can see that the strategic criteria to optimize the budget is actually not the volume of data but rather the number of annotations per record.", "For instance, with a budget of $50K, the customer is better off labeling 50K records 4 times each. This strategy is 16% more accurate compared to labeling 200K records once.", "However, if the customer needs her model to reach an 86% model accuracy, she would be making a better choice to use 200K training records 3 times; this would save her about $50K (more than 25%) compared to a strategy of 5 labels per record on a training set of size 150K records.", "If we now represent the same simulated data but group it by number of annotations per record, it\u2019s very clear that labeling the data once per record is simply a bad idea. Even two annotations grossly outperforms a single, fallible labeler. Annotating 3 times per record seems a safe bet most of the time, while at much above that you may have some issue justifying that cost.", "Now, it\u2019s important to note that so far, we have still chosen to use a constant number of annotations for all records. But since recently we were able to determine that some classes were more sensitive than others to labeling noise, it seems reasonable to try to exploit that fact to further tweak and optimize our labeling budget.", "In another simulation, where we now have a binary classification image classification problem on a balanced 500K dataset, assuming a more favorable mislabeling probability of 25% and modeling one class with a significantly higher sensitivity to noise than the other. We analyzed four different labeling strategies for this problem:", "We can see that strategy 3 applied to the entire dataset leads to the strongest accuracy; however, it is barely better than strategy 1, which is significantly cheaper. Strategy 1 seems to be the best for really low labeling budgets, while strategy 4 is better for medium budgets.", "Tuning the labeling strategy at the class level leads of course to a chicken-and-egg problem, as the labels are necessary to classify the data; however, the same sensitivity study can be combined with an unsupervised approach and applied on clusters instead of classes; this research is one of our current focus areas.", "So where does this all leave us? For starters, we hope you\u2019ve internalized that more data isn\u2019t always a good thing. Cleaner, better-labeled, more accurate data, on the other hand, is.", "But really, what\u2019s most important to take away from our experiments is this: you\u2019re likely spending too much money labeling data. If you\u2019re like a lot of ML practitioners, you\u2019re likely labeling too much without digging into the quality and how much both your good and bad labels are affecting your models. Remember: different classes and different problems require different labeling schemas. Leveraging active learning to understand which classes need extra attention from labelers holds the promise of building more accurate models at a fraction of the cost and time needed now. And that\u2019s something that should get everyone in your company on board.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F70a712123df1&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fare-you-spending-too-much-money-labeling-data-70a712123df1&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fare-you-spending-too-much-money-labeling-data-70a712123df1&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fare-you-spending-too-much-money-labeling-data-70a712123df1&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fare-you-spending-too-much-money-labeling-data-70a712123df1&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----70a712123df1--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----70a712123df1--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@jlprendki?source=post_page-----70a712123df1--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@jlprendki?source=post_page-----70a712123df1--------------------------------", "anchor_text": "Jennifer Prendki"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4df45fc50788&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fare-you-spending-too-much-money-labeling-data-70a712123df1&user=Jennifer+Prendki&userId=4df45fc50788&source=post_page-4df45fc50788----70a712123df1---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F70a712123df1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fare-you-spending-too-much-money-labeling-data-70a712123df1&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F70a712123df1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fare-you-spending-too-much-money-labeling-data-70a712123df1&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@clemono2?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Clem Onojeghuo"}, {"url": "https://unsplash.com/?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://www.cs.toronto.edu/~kriz/cifar.html", "anchor_text": "CIFAR-10"}, {"url": "https://medium.com/alectio/can-you-lie-to-your-model-1-3-e03309e41291?source=collection_home---4------3-----------------------", "anchor_text": "we were able to determine that some classes were more sensitive than others to labeling noise"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----70a712123df1---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-labeling?source=post_page-----70a712123df1---------------data_labeling-----------------", "anchor_text": "Data Labeling"}, {"url": "https://medium.com/tag/active-learning?source=post_page-----70a712123df1---------------active_learning-----------------", "anchor_text": "Active Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F70a712123df1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fare-you-spending-too-much-money-labeling-data-70a712123df1&user=Jennifer+Prendki&userId=4df45fc50788&source=-----70a712123df1---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F70a712123df1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fare-you-spending-too-much-money-labeling-data-70a712123df1&user=Jennifer+Prendki&userId=4df45fc50788&source=-----70a712123df1---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F70a712123df1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fare-you-spending-too-much-money-labeling-data-70a712123df1&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----70a712123df1--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F70a712123df1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fare-you-spending-too-much-money-labeling-data-70a712123df1&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----70a712123df1---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----70a712123df1--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----70a712123df1--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----70a712123df1--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----70a712123df1--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----70a712123df1--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----70a712123df1--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----70a712123df1--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----70a712123df1--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@jlprendki?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@jlprendki?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Jennifer Prendki"}, {"url": "https://medium.com/@jlprendki/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "44 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4df45fc50788&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fare-you-spending-too-much-money-labeling-data-70a712123df1&user=Jennifer+Prendki&userId=4df45fc50788&source=post_page-4df45fc50788--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Ff73f0c7cc82d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fare-you-spending-too-much-money-labeling-data-70a712123df1&newsletterV3=4df45fc50788&newsletterV3Id=f73f0c7cc82d&user=Jennifer+Prendki&userId=4df45fc50788&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}