{"url": "https://towardsdatascience.com/using-data-science-for-customer-acquisition-2001525792f", "time": 1683005254.423245, "path": "towardsdatascience.com/using-data-science-for-customer-acquisition-2001525792f/", "webpage": {"metadata": {"title": "Using Data Science for Customer Acquisition | by Jonathan Cabreira | Towards Data Science", "h1": "Using Data Science for Customer Acquisition", "description": "With the aim of bringing new customers to their brands, companies have been developing their customer acquisition processes and strategies. It\u2019s not easy though! The first step of any basic customer\u2026"}, "outgoing_paragraph_urls": [{"url": "https://github.com/jmcabreira/Data-Science-Projects/tree/master/Customer_Segmentation_Report", "anchor_text": "Github", "paragraph_index": 4}, {"url": "http://linkedin.com/in/cabreirajm", "anchor_text": "linkedin.com/in/cabreirajm", "paragraph_index": 51}], "all_paragraphs": ["With the aim of bringing new customers to their brands, companies have been developing their customer acquisition processes and strategies. It\u2019s not easy though! The first step of any basic customer acquisition plan is to identify quality potential customers and, guess what! Data Science can help us with that! This project is all about strategies that can be used in order to understand customers' characteristics and possibly, acquire them.", "This project applies Machine Learning algorithms in order to get information about potential customers out of demographic data. It is the Capstone project of the Udacity Machine Learning Engineer Nanodegree program which is provided by Arvato Financial Solutions, a Bertelsmann subsidiary. I chose this project mainly because the data provided is real and almost no cleaning has been done to it. In addition to that, I really enjoy understanding business and customer needs in order to provide the best experience for both the company and those who make use of a business product or service.", "The first part of the project consists of building a customer segmentation report based on the Arvato Financial Solutions' existing customers and the general population of Germany. To do that so I used unsupervised learning techniques to identify parts of the population that best describe the core customer base of the company.", "The second part consists in building a customer marketing campaign response predictor in order to identify individuals that are most likely to convert into becoming a customer. In order to do that, I tried out a few supervised learning algorithms and end up with the one that has given me the best ROC AUC score.", "You can check all the code I develop on my Github page.", "As mentioned above, the data for this project has been provided by Arvato Financial Solutions, a subsidiary of Bertelsmann in Germany. There are six data files associated with this project as follows:", "Part II ) Marketing Campaign Response Prediction", "Note: The DIAS Information Levels and the DIAS Attributes xlsx files have been translated to the data_info.csv file, which contains information regarding the features and their respective possible values.", "In order to feed the machine learning algorithm a couple of clean and preprocessing step has to be executed. In particular, the data provided needed to pass through 8 steps of preprocessing and feature engineering. Do you know the 80\u201320 rule? Well, many people say that about 80% of the time of a data science project is spent on data preparation (preprocessing) and 20% is spent on data analysis and machine learning approaches. That was exactly what happened on this particular project. I built a function that does all the required process as you can see below:", "The cleaning function drops 35 features ( columns) due to their number of missing values. Those columns have more than 40% of nan data. This lack of information may affect negatively the model and because of that, I decided to get rid of them. Apart from that, for the customer dataset I also the function also drops three extra columns (\u2018CUSTOMER_GROUP\u2019, \u2018ONLINE_PURCHASE\u2019, \u2018PRODUCT_GROUP\u2019).", "I created the data_info.csv file that summarizes information from both the DIAS Information Levels \u2014 Attributes 2017.xlsx and DIAS Attributes \u2014 Values 2017.xlsx files. In addition to that, I also included the missing value code of each feature so that I will be able to identify all missing values per feature and convert then to NAN.", "This is how the data_indo dataframe looks like:", "Step 3 \u2014 Drop rows and data imputation", "In order to analyze the number of missing values per row, I split the dataframe into different sets. The first with a high amount of missing values ( more than 250 ) and others with less than 250 missing values. By doing that I could investigate the distribution of a few features and compare both dataframes. I realized that the distributions (for many features) are different, so that makes me decide to keep working on the data frame with less missing value and drop all rows that have more than 250 nan variables.", "Although the majority of the features are either categorical or ordinal, I assumed all variables as being categorical. Knowing that so, I replaced all missing values (nan variables) with the most frequent value of the feature in order to compensate for missing values in the data set.", "The unsupervised learning algorithm that will be used to build the customer segmentation, requires numerical values. Because of that, all the data must be numerically encoded so that the model can proceed the way it is supposed to proceed.", "After analysing all of those features, I decided to do as follows:", "In order to avoid sparsity issues, I limited the one hot encode to the 44 and 33 most frequent labels of the features. In other words, I built one binary variable for each of the 44 and 33 most frequent features categories only and considered the rest as noise. I will set the top 44 categories of variables CAMEO_DEU_2015 and CAMEO_INTL_2015 and top 33 of D19_LETZTER_KAUF_BRANCHE.", "I used the top 44 and top 33 most frequent categories because that way I can guarantee that all top components of azdias datafram are equal to the top components of the customers' dataframe.", "Since we have a large dataset, we will perform a dimensional reduction. To do so, we need to perform a feature scaling so that the principal component vector will not be influenced by the difference in scale of each variable. This approach is really important when dealing with distance-based algorithms, which is our case.", "I used StandarScale that scale each feature to mean 0 and standard deviation equals to 1.", "After scaling the data frame, we can apply dimensional reduction. To do that, I used the principal component approach in order to find the vectors of maximal variance in the data. To start off, I used all of the features of the dataframe . By doing that we can see the general trend in variability. The graph below shows the ratio of variance explained by each component and the cumulative variance explained.", "After looking at the variance explained by each component in the screen plots above, I decided to select 215 components, which account for about 89% of the variance explained.", "We can plot the elbow curve to better determine the number of clusters to be used. The method takes a value for k (cluster) and looks at the average distance of each of the data points to the cluster centers. We can notice that the value decreases as k increases and we choose the number of k where this stops decreasing at a significant rate (the elbow).", "I started by using the MiniBatchKMeans which is a variant of the KMeans algorithm which uses mini-batches to reduce the computation time. The results of this algorithm is slightly worse than the standard KMeans and because of that, after determining the number of clusters, I used the regular one.", "The elbow curve above shows that the score rapidly decreases for the first 9 clusters. Even though it increases in a few points, after the cluster 11 the decrease becomes less and less noticeable. Because of that, I decided to choose 11 as the number of clusters.", "After building the PCA and the cluster models onto the general population data, we can apply the same process onto the customer data and compare the clusters distributions. The idea is to figure out where the most robust customer base for the company lies. In other words, we have to find out which clusters are either over-represented or underrepresented. How would that help? Well, let\u2019s suppose that 15% of people are assigned to a particular cluster for the general population and 25% of the customers also \u2018felt\u2019 close to that cluster\u2019s centroid. That way, the proportion of consumers will be greater than the proportion of the general population. This suggests that those people from the general population might be a target audience for the company.", "The bar plot below shows the distribution of people assigned to each of the 11 clusters. Based on the previous assumption, we can notice that the most over-represented and underrepresented clusters are clusters numbers 7 and 6 respectively.", "In order to understand few characteristics of those clusters, I decided to investigate a few features that describe their \u201cfinancial types\u201d and plot their distributions.", "The first thing we can notice is that there is a cluster that is overrepresented and clusters that are underrepresented by the customers' population.", "we can also notice that the three clusters with the largest amount of over-representation are cluster 7, cluster 11 and cluster 3. Because of that, This suggests that those clusters are more likely to be the target audience for the company.", "On the other hand, the three clusters that are most underrepresented are cluster 1, cluster 6, cluster 8", "In order to understand a bit more about the populations of each cluster, we have to check the distribution of each feature. In my case, I\u2019ve selected features that represent individual finance types of each of them.", "The cluster that had over-represented customer data, consisted of individuals that are more likely to:", "Those in the underrepresented cluster are more likely to:", "Now that we\u2019ve found which parts of the population are more likely to be customers of the mail-order company, we can build a supervised model that can predict whether a person will respond to a marketing campaign or not.", "Each of the rows in the \u201cmailout.train\u201d and mailout.test represents an individual that was targeted for a mailout campaign. The data sets contain approximately 43 000 data rows each and the train data set includes a column, \u201cRESPONSE\u201d, that states whether or not a person became a customer of the company following the campaign.", "I started by using the clean_df function to process all those steps from the preprocessing section. After that, I noticed that the data set is imbalanced in terms of classes. In fact, 98,76 % of the observations correspond to people that have not responded to the campaign. Therefore, the accuracy metric does not seem to be a good choice to evaluate the models. I will be using the ROC AUC metric due to its ability to not be affected by the imbalance.", "Since the second model is a classification problem, I will be using the AUC-ROC as the metric. The ROC (Receiver Operating Characteristics) represents the probability curve and it\u2019s created by plotting the true positive rate(TPR) or Recall against the False Positive Rate ( 1 \u2014 Specificity). The AUC gives us the degree of separability. By using the AUC-ROC we are able to know how our model is performing in classifying both classes. The higher the AUC (area under the curve), the better the model is at predicting the success of the campaign ( in our case).", "I built a function that takes in the model or list of models to be performed, the parameter names for feeding the GridSearchCV and tuning the classifier, the X_train data, the Y_train data and an optional parameter in case I want to plot a bar plot and compare different models.", "Whenever I am building a classification model, I like to start off by using the logistic regression algorithm in its basic form. At first, I deleted the column that represents the id of each individual and got a poor result. Surprisingly, after including the row I got a 0.7 result, which is I find good for a baseline model.", "I tested a couple of different supervised learning models on the data. Note that I did not scale the data because most I was expecting better results from the three based models and as we know, these models can handle non scaled data pretty well.", "The bar plot above shows that the most promising classifier is the gradientBoosting. Because of that, I chose this algorithm to be tuned used.", "I did an executive trial and error tuning, tweaking parameters such as n_estimators, loss, learning_rate, n_samples_split, min_samples_leaf, max_depth, max_features, and subsample.", "I end up with a model with the characteristics below:", "The feature importance according to the model:", "The main goal of this project was to understand customer groups and predict whether a person will respond to a marketing campaign or not. That way, we can help a German mail-order company develop a more efficient way of targeting potential customers.", "The first part consisted of build unsupervised models such as PCA and k-means clustering to perform customer segmentation. We did that by using 215 components and 11 clusters. After applying these approaches I could compare the over-represented and the underrepresented clusters and draw some conclusions. For example, I noticed that the majority of individuals of the over-represented group ( people from the general population might be a target audience for the company) have a high income, are investors and are good money savers. On the other hand, the general population have an average income, do not invest their money and have an average purchase power.", "In the second part, I built a couple of different supervised learning models and compare them. The Gradient Boosting algorithm gave provided me with the best ROC AUC score and because of that it was selected and tuned. After an exhaustive parameter tweaking, I end up with a score of 0.78794 on the test data.", "This project was really challenging but I learned a lot from it. I would like to thank Udacity for all the support (especially my mentor, Mofetoluwa A.)and Bertelsmann Arvato Analytics for providing all those data.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "\u2022 Electrical Engineer turned Data Scientist \u2022 LinkedIn : linkedin.com/in/cabreirajm"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F2001525792f&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-data-science-for-customer-acquisition-2001525792f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-data-science-for-customer-acquisition-2001525792f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-data-science-for-customer-acquisition-2001525792f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-data-science-for-customer-acquisition-2001525792f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----2001525792f--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2001525792f--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@cabreirajm?source=post_page-----2001525792f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@cabreirajm?source=post_page-----2001525792f--------------------------------", "anchor_text": "Jonathan Cabreira"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa5044f2be32e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-data-science-for-customer-acquisition-2001525792f&user=Jonathan+Cabreira&userId=a5044f2be32e&source=post_page-a5044f2be32e----2001525792f---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2001525792f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-data-science-for-customer-acquisition-2001525792f&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2001525792f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-data-science-for-customer-acquisition-2001525792f&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://github.com/jmcabreira/Data-Science-Projects/tree/master/Customer_Segmentation_Report", "anchor_text": "Github"}, {"url": "https://medium.com/tag/data-science?source=post_page-----2001525792f---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/customer-service?source=post_page-----2001525792f---------------customer_service-----------------", "anchor_text": "Customer Service"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----2001525792f---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data?source=post_page-----2001525792f---------------data-----------------", "anchor_text": "Data"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2001525792f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-data-science-for-customer-acquisition-2001525792f&user=Jonathan+Cabreira&userId=a5044f2be32e&source=-----2001525792f---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2001525792f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-data-science-for-customer-acquisition-2001525792f&user=Jonathan+Cabreira&userId=a5044f2be32e&source=-----2001525792f---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2001525792f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-data-science-for-customer-acquisition-2001525792f&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2001525792f--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F2001525792f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-data-science-for-customer-acquisition-2001525792f&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----2001525792f---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----2001525792f--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----2001525792f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----2001525792f--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----2001525792f--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----2001525792f--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----2001525792f--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----2001525792f--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----2001525792f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@cabreirajm?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@cabreirajm?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Jonathan Cabreira"}, {"url": "https://medium.com/@cabreirajm/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "138 Followers"}, {"url": "http://linkedin.com/in/cabreirajm", "anchor_text": "linkedin.com/in/cabreirajm"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa5044f2be32e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-data-science-for-customer-acquisition-2001525792f&user=Jonathan+Cabreira&userId=a5044f2be32e&source=post_page-a5044f2be32e--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Ff22c720b7fc8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-data-science-for-customer-acquisition-2001525792f&newsletterV3=a5044f2be32e&newsletterV3Id=f22c720b7fc8&user=Jonathan+Cabreira&userId=a5044f2be32e&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}