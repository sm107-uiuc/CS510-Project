{"url": "https://towardsdatascience.com/statistics-in-spark-sql-explained-22ec389bf71b", "time": 1683016817.759896, "path": "towardsdatascience.com/statistics-in-spark-sql-explained-22ec389bf71b/", "webpage": {"metadata": {"title": "Statistics in Spark SQL explained | by David Vrba | Towards Data Science", "h1": "Statistics in Spark SQL explained", "description": "Spark SQL optimizer uses two types of optimizations: rule-based and cost-based. The former relies on heuristic rules while the latter can use some statistical properties of the data. In this article\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/about-joins-in-spark-3-0-1e0ea083ea86", "anchor_text": "article", "paragraph_index": 21}], "all_paragraphs": ["Spark SQL optimizer uses two types of optimizations: rule-based and cost-based. The former relies on heuristic rules while the latter can use some statistical properties of the data. In this article, we will explain how these statistics are used in Spark under the hood and we will see in which situations they are useful and how to take advantage of them.", "Most of the optimizations that Spark does are based on some heuristic rules that do not take into account the properties of the data that are being processed. For example, the PredicatePushDown rule is based on a heuristic rule which assumes that it is better to first reduce the data by filtering and then apply some computation on it. There are however some situations in which Spark can also use some statistical information about the data in order to come up with yet a better plan and this is often referred to as the cost-based optimization or CBO. In this article, we will explore it more in detail.", "To see the statistics of a table we first need to compute them by running a SQL statement (notice that all the SQL statements can be executed in Spark using the sql() function spark.sql(sql_statement_as_string)):", "After this, the table level statistics are computed and saved in metastore and we can see them by calling", "This is going to show us some properties of the table including the table-level statistics. There are two metrics available, namely the rowCount and sizeInBytes:", "Apart from these table-level statistics there are also column-level statistics and to compute and see them we can call:", "This is going to show us a table like this (the column used in this example is user_id):", "So as you can see, there are various metrics for the column, such as min/max values, number of null values, number of distinct values (which is approximated), and some others.", "Since Spark 3.0 there is one more option on how to display the stats and see it not only for the table but for the actual query that we want to run. This can be done by using the new mode argument of the explain function:", "This is going to show us two query plans, namely the physical plan and also the optimized logical plan which will now contain also the information about the statistics as you can see in the image below:", "The point is that now you can see the statistics for each operator in the plan, so you can see what are the estimates of the stats after various transformations. The stats are first computed by the Relation operator which is a so-called leaf node and each leaf node is responsible to compute the stats somehow, and then they are propagated through the plan according to some rules.", "In the next, we will see how the leaf node computes the stats and how the propagation works.", "There are three ways of how the leaf node can compute the statistics. The first (and best) way is that the stats are taken from the metastore. The second option is that Spark will use InMemoryFileIndex which calls Hadoop API under the hood to gather the size of each file in the datasource and sum it up to get the total sizeInBytes (in this option only this one metric would be computed). And finally, the last way is that Spark would use a default value for the sizeInBytes metric which is given by the spark.sql.defaultSizeInBytes configuration setting and the default of this setting is 8 EiB, so basically Spark will overestimate the size for the Relation as much as possible (and again only this one metric will be used). Which of these three options takes place can be described by this diagram:", "This diagram is a tree where each node is a condition and if the condition is true, we go in the direction of T (True) otherwise we go in the direction of F (False). The leaves represent the actual way how the stats will be computed, so for example InMemoryFI means that only sizeInBytes will be computed by calling the Hadoop API. On the other hand, Stats from M means that the statistics will be taken from the metastore, however, notice that on the left side of the tree all the stats will be taken from the metastore but on the right side only one metric sizeInBytes will be taken. The leaf CatalogFileIndex represents the last case where the default value 8 EiB for the sizeInBytes will be used.", "In the diagram we have altogether four conditions, the first determines how the data is accessed: if we read the data as a table df=spark.table(table_name) then we go to the left, otherwise, we go to the right. The next condition is whether the cost-based optimizer (CBO) is turned On or Off. This is given by the configuration setting spark.sql.cbo.enabled and the default value is False (still in Spark 3.0). The third condition asks if the stats were computed in metastore by running the analyze table command (ATC) and finally the last condition is whether the table is partitioned or not.", "The best scenario occurs if we read the data as a table, if the cost-based optimizer is On, and if we first run ATC, in that case, all the statistics will be taken from the metastore (except for the sizeInBytes which is computed from the rowCount as described below). On the other hand, the worst situation with the worst estimates happens when we also access the data as a table, but if the ATC didn\u2019t run and moreover the table is partitioned, in this case, the default sizeInBytes is taken from the configuration property and the estimates will be very imprecise. Notice that for the worst-case scenario it is irrelevant whether the CBO is On or Off. Also notice that if the table is not partitioned, Spark will actually compute at least the sizeInBytes using the Hadoop API so the table partitioning has a direct impact on how the stats are computed in the leaf node of the query plan.", "Once the statistics are computed in the leaf node, they can be propagated to other nodes in the query plan. There are two ways of how this propagation can happen. The first way (let\u2019s call it the Old way) is rather basic and only one metric, namely sizeInBytes, is propagated and here the rules by which the metric is adjusted on various operators are very basic. For instance, the Filter operator doesn\u2019t adjust the value at all as you can see in this example:", "In this query, we filter for all records where the user_id is negative, and there are actually no such records, but Spark doesn\u2019t have this information available because that would require to use also column-level statistics and they are not used in this Old way. So as you can see from the query plan, there is only sizeInBytes propagated and it stays the same on both operators. In other words, we can say that Spark thinks that after applying the Filter, the size of the data doesn\u2019t change:", "The second way of stats propagation (let\u2019s call it the New way) is more mature, it is available since Spark 2.2 and it requires having the CBO turned ON. It also requires to have the stats computed in metastore with ATC. Here all the stats are propagated and if we provide also the column level metrics, Spark can compute the selectivity for the Filter operator and compute a better estimate:", "As you can see, the stats on the Filter operator changed, the rowCount is now zero and the sizeInBytes is 1B which is the minimum value. From the column level stats of the user_id column, Spark was able to see that there are no records with the negative value of user_id and is able to reflect that in the query plan.", "In this New way, to compute the sizeInBytes, Spark first computes the size of a single row based on the data types information for each column and then multiplies by the rowCount to get the final sizeInBytes. If the rowCount is zero, the sizeInBytes is set to 1 to avoid division by zero in some other stats computation. This is also how the sizeInBytes is adjusted on the Project operator (Spark knows what columns will be projected, so again it computes the size of a single row first).", "At this moment we have an idea of how the stats are computed and propagated through the plan, let\u2019s now finally see how they are also used during the query planning to achieve a more optimal plan. There are two places where these statistics are used. The first one is the JoinSelection strategy where Spark decides which algorithm will be used for joining two DataFrames (see my other article where I describe this more in detail). The basic logic is that if one of the DataFrames is smaller than a certain threshold, Spark will use BroadcastHashJoin (BHJ), because it is a very efficient algorithm if the broadcasted DataFrame is very small. The threshold is given by spark.sql.autoBroadcastJoinThreshold configuration setting which default value is 10 MB. So here having a good estimate for the size of the DataFrame can help to choose a more optimal algorithm for the join.", "The second place where stats are used is also related to joins and namely, it is the joinReorder rule. Using this rule Spark can find the most optimal order in which DataFrames will be joined (if you join more than two DataFrames). This rule is by default turned off and if you want to use it you have to enable it by this configuration setting:", "because the default value is False. The maximum number of DataFrames for which this can be used is controlled by this property:", "where the default value of n is 12.", "Well, we have seen that if a table is partitioned and we don\u2019t run ATC, Spark will overestimate the size by using the default value (which is very large \u2014 8 EiB). So in a situation where we join lots of tables and some of these tables are partitioned and rather small, so they could be good candidates for BHJ, in that case, it makes sense to run for them ATC. Also, we have to remember that if a table is appended or overwritten, the stats will be deleted, so we have to run ATC again. In some environments, having the stats in metastore up to date might be complicated. A partial solution to this problem provides adaptive query execution \u2014 a new feature in Spark 3.0.", "In Spark 3.0 a new feature Adaptive Query Execution (AQE) was released and it uses statistics in an even more enhanced way. If the AQE is enabled (by default it is not), the statistics are recomputed after each stage is executed during runtime. This leads to more precise stats estimates and a better ability to decide whether BHJ can be used or not. AQE itself is quite a big topic and we can cover it in a separate article.", "In this article, we have seen how statistical information about data is used by Spark in order to achieve a more optimal execution plan. We have seen that there are three ways how the stats are computed in the first node of the query plan (in a so-called leaf node) and then how this information can be propagated through the plan. We pointed out a special caveat related to partitioned tables which are small and may be good candidates for broadcasting in joins. If we don\u2019t run the analyze table command for them, Spark will overestimate the size of these tables and the broadcast join will be hardly achieved. We also showed that having better propagation of the statistics through the plan requires having the cost-based optimizer enabled and column statistics computed especially for those columns that are used in filtering conditions.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Senior ML Engineer at Sociabakers and Apache Spark trainer and consultant. I lecture Spark trainings, workshops and give public talks related to Spark."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F22ec389bf71b&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstatistics-in-spark-sql-explained-22ec389bf71b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstatistics-in-spark-sql-explained-22ec389bf71b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstatistics-in-spark-sql-explained-22ec389bf71b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstatistics-in-spark-sql-explained-22ec389bf71b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----22ec389bf71b--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----22ec389bf71b--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@vrba.dave?source=post_page-----22ec389bf71b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@vrba.dave?source=post_page-----22ec389bf71b--------------------------------", "anchor_text": "David Vrba"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb7f216c64e33&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstatistics-in-spark-sql-explained-22ec389bf71b&user=David+Vrba&userId=b7f216c64e33&source=post_page-b7f216c64e33----22ec389bf71b---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F22ec389bf71b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstatistics-in-spark-sql-explained-22ec389bf71b&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F22ec389bf71b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstatistics-in-spark-sql-explained-22ec389bf71b&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/about-joins-in-spark-3-0-1e0ea083ea86", "anchor_text": "article"}, {"url": "https://medium.com/tag/spark?source=post_page-----22ec389bf71b---------------spark-----------------", "anchor_text": "Spark"}, {"url": "https://medium.com/tag/optimization?source=post_page-----22ec389bf71b---------------optimization-----------------", "anchor_text": "Optimization"}, {"url": "https://medium.com/tag/sql?source=post_page-----22ec389bf71b---------------sql-----------------", "anchor_text": "Sql"}, {"url": "https://medium.com/tag/data-science?source=post_page-----22ec389bf71b---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/data-engineering?source=post_page-----22ec389bf71b---------------data_engineering-----------------", "anchor_text": "Data Engineering"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F22ec389bf71b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstatistics-in-spark-sql-explained-22ec389bf71b&user=David+Vrba&userId=b7f216c64e33&source=-----22ec389bf71b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F22ec389bf71b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstatistics-in-spark-sql-explained-22ec389bf71b&user=David+Vrba&userId=b7f216c64e33&source=-----22ec389bf71b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F22ec389bf71b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstatistics-in-spark-sql-explained-22ec389bf71b&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----22ec389bf71b--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F22ec389bf71b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstatistics-in-spark-sql-explained-22ec389bf71b&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----22ec389bf71b---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----22ec389bf71b--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----22ec389bf71b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----22ec389bf71b--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----22ec389bf71b--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----22ec389bf71b--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----22ec389bf71b--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----22ec389bf71b--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----22ec389bf71b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@vrba.dave?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@vrba.dave?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "David Vrba"}, {"url": "https://medium.com/@vrba.dave/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "2K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb7f216c64e33&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstatistics-in-spark-sql-explained-22ec389bf71b&user=David+Vrba&userId=b7f216c64e33&source=post_page-b7f216c64e33--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F83cdb92c0d8c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstatistics-in-spark-sql-explained-22ec389bf71b&newsletterV3=b7f216c64e33&newsletterV3Id=83cdb92c0d8c&user=David+Vrba&userId=b7f216c64e33&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}