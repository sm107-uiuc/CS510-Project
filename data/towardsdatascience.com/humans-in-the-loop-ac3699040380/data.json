{"url": "https://towardsdatascience.com/humans-in-the-loop-ac3699040380", "time": 1683016334.57116, "path": "towardsdatascience.com/humans-in-the-loop-ac3699040380/", "webpage": {"metadata": {"title": "Humans in the loop. Dylan Hadfield-Menell on the TDS\u2026 | by Jeremie Harris | Towards Data Science", "h1": "Humans in the loop", "description": "Editor\u2019s note: This episode is part of our podcast series on emerging problems in data science and machine learning, hosted by Jeremie Harris. Apart from hosting the podcast, Jeremie helps run a data\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/emerging-problems-in-data-science-and-machine-learning-36d37f6531a8", "anchor_text": "emerging problems in data science and machine learning", "paragraph_index": 0}, {"url": "http://sharpestminds.com", "anchor_text": "SharpestMinds", "paragraph_index": 0}, {"url": "https://twitter.com/dhadfieldmenell", "anchor_text": "follow Dylan on Twitter here", "paragraph_index": 4}, {"url": "https://twitter.com/jeremiecharris", "anchor_text": "follow me on Twitter here", "paragraph_index": 4}, {"url": "http://shorturl.at/jtMN0", "anchor_text": "shorturl.at/jtMN0", "paragraph_index": 123}], "all_paragraphs": ["Editor\u2019s note: This episode is part of our podcast series on emerging problems in data science and machine learning, hosted by Jeremie Harris. Apart from hosting the podcast, Jeremie helps run a data science mentorship startup called SharpestMinds. You can listen to the podcast below:", "Human beings are collaborating with artificial intelligences on an increasing number of high-stakes tasks. I\u2019m not just talking about robot-assisted surgery or self-driving cars here \u2014 every day, social media apps recommend content to us that quite literally shapes our worldviews and our cultures. And very few of us even have a basic idea of how these all-important recommendations are generated.", "As time goes on, we\u2019re likely going to become increasingly dependent on our machines, outsourcing more and more of our thinking to them. If we aren\u2019t thoughtful about the way we do this, we risk creating a world that doesn\u2019t reflect our current values or objectives. That\u2019s why the domain of human/AI collaboration and interaction is so important \u2014 and it\u2019s the reason I wanted to speak to Berkeley AI researcher Dylan Hadfield-Menell for this episode of the Towards Data Science podcast. Dylan\u2019s work is focused on designing algorithms that could allow humans and robots to collaborate more constructively, and he\u2019s one of a small but growing cohort of AI researchers focused on the area of AI ethics and AI alignment.", "Here were some of my favourite take-homes from our conversation:", "You can follow Dylan on Twitter here, and you can follow me on Twitter here.", "Jeremie (00:00:00):Hey everyone, Jeremie here, host of course of the podcast and also part of the team over at the Sharpest Minds Data Science Mentorship Program. I\u2019m here today with a really exciting episode actually, we\u2019re talking to Dylan, who is a researcher at UC Berkeley. He\u2019s actually wrapping up his PhD there, where he\u2019s been studying different kinds of human machine interactions, human AI interactions, and his goal really is to figure out how humans and AIs can interact together in a way that\u2019s more positive some.", "Jeremie (00:00:30):One of the concerns that comes up a lot whenever humans need to convey their preferences to AIs, is that their preferences can be misinterpreted. The more powerful an AI system is, the more damage the more harm that misinterpretation can do. Dylan\u2019s focused on schemes that allow humans to communicate very efficiently with AI systems, and exploring different ways in which you can set up systems so that they\u2019re maximally robust. The hope here being that AIs will learn to some degree to have a healthy dose of humility and uncertainty, especially when they\u2019re going in to carry out really important actions, really consequential actions.", "Jeremie (00:01:05):Dylan\u2019s working on that problem, he\u2019s also more broadly interested in what\u2019s known as the AI alignment problem, how we get AIs to behave in the way that we want them to. I\u2019m really excited to talk to him. He\u2019s got all kinds of great insights, including fresh perspectives on what AI alignment really means and is, as well as how to build systems that are properly aligned with our values. Really looking forward to diving into this one. I hope you enjoyed the conversation, and I\u2019ll see you on the flip side.", "Jeremie (00:01:35):I\u2019m really excited to have you here. You\u2019re one among a growing but still pretty small group of researchers focused on the question of AI safety. You have one specialization, of course, one sub domain that you\u2019re focused on, I\u2019d love to get a sense from you, first off of how you got there, how you came to focus on AI safety, and then what your current research looks like today.", "Dylan (00:01:57):My research origin story, if you will, starts off in integrated task and motion planning for robotics. I was interested on basically how to get robots to help you with your laundry. This involves long horizon planning and a lot of details of actually systems building on that side of things. One of the key things you have to do in building that system is in solving a problem called motion planning. There\u2019s a bunch of different ways you can solve it. A method that we\u2019ve used is trajectory optimization. But motion planning is the idea of, you\u2019ve got your robot in one particular configuration, and you\u2019ve got a target configuration that you want to move it towards, and you need to figure out a sequence of joint controls that actually take you from the initial position to the target position without hitting anything.", "Dylan (00:02:49):It turns out, this is a surprisingly difficult thing, and it\u2019s one of the examples of human intelligence that we take for granted in our daily lives. To make a long story short, in applying trajectory optimization to this, what you do is you specify a cost function that ranks different trajectories, different ways of getting from point A to point B. It does things like saying going into collision is very bad. But also things like getting there faster is good.", "Dylan (00:03:20):There\u2019s these trade offs that you end up having to specify, there\u2019s these weights. In some of my talks, I have this example of an interface that that shows a little bit of this kind of design process that you go through, where you\u2019re tweaking trade offs between how good it is to avoid collision versus how fast you want to go, let\u2019s say.", "Dylan (00:03:40):Ultimately, what I came to realize is that these weights are a representation of what kinds of trajectories that you want. In specifying these goals, that\u2019s really a type of programming process, where we specify what types of behaviors that we want. You can see this type of paradigm in a whole bunch of different contexts for AI, where we reduce the problem of specifying what we want to some kind of reward function or score function. In the case of reinforcement learning, this is something like a metric or a reward function. In the case of supervised learning, this is an empirical loss function on a data set. There\u2019s lots of other examples that you can come up with, where we\u2019re really encoding our preferences into these ranking functions, these types of utility functions, and then using general purpose methods to optimize them.", "Dylan (00:04:39):It\u2019s after working on those problems and becoming familiar with this paradigm after about two years or so that I began to pivot my research into studying actually what this problem looks like, and what does it mean to actually do it well, and what are the pitfalls that we can try to avoid? That is what I view as the value alignment problem in artificial intelligence.", "Jeremie (00:04:58):Okay, so broadly understood coming\u2026 Your picture of the value alignment problem includes coming up with the parameters that we want optimized. Is that fair to say? It includes that loss function.", "Dylan (00:05:12):Right. In a way you can think about loss functions as a type of programming language, where we can see optimization as a compiler, where you go from one representation of, say, a classification behavior, a mapping from pixels to binary 01, camera dog. Ultimately, there\u2019s some set of behaviors, some set of weights that represent classifying things appropriately. We don\u2019t know how to program that by hand, that would be crazy. But at the same time, we also don\u2019t know how to directly program in machine code anymore, either.", "Dylan (00:05:50):I think there\u2019s a nice analogy that you can make between this fast executable representation of behaviors, which would be something like the weights of a neural network, and the goals that we optimize in order to get those behaviors, which would be the data set that captures our notion of what a picture of a cat is.", "Jeremie (00:06:11):Interesting. I guess one of the interesting consequences of this sort of analogy to is, it maps on well to the subjective experience of somebody who\u2019s trying to program for the first time, and somebody who\u2019s trying to do a kind of naive machine learning. When I program for the first time, I would get really frustrated with the machine, because I would tell it to do things, and my code would break, the machine would be telling me absolutely, unambiguously, \u201cHey, you made a mistake.\u201d I couldn\u2019t accept that. I kept thinking like, well, no, I didn\u2019t make a mistake. Clearly, you\u2019re not interpreting what I asked you to do correctly.", "Jeremie (00:06:44):Of course, I now recognize that as a silly category of error. But it seems like we\u2019re making similar kinds of or at least it\u2019s possible for us to make similar kinds of mistakes on machine learning, where we\u2019re taking literally and not seriously, let\u2019s say by the machine. Would that be a fair mapping between the two in terms of where the alignment problem comes from?", "Dylan (00:07:06):Yeah, I think so. I think it\u2019s like before you have really good compilers, you work on benchmark code, or you don\u2019t know what programs people are interested in writing, and you write compilers that work on your best representations of them. There are particular programs that are meant to replicate certain types of loads that you might need to compile for, like fast data access in memory management or something like that. But ultimately, they\u2019re synthetic problems. If you look at AI that\u2026 Well, there is a general genuine, I think scientific interest in what is intelligence and those types of questions. But, practically, if you think about what we\u2019re actually using these systems for, we have datasets that are our representation of here are the kinds of things we\u2019d like to be able to do, which is good, and really, really useful. But it doesn\u2019t help us practice the process of, here\u2019s how we take a really loose idea of what we want, and then agree on how to represent that in some way that we can actually train a system to do what we want with it.", "Jeremie (00:08:15):What kinds of best practices are now emerging or maybe what kinds of strategies are you working on to make that process easier? How do we help humans better specify what they want so that robots don\u2019t go off and do the wrong thing?", "Dylan (00:08:30):Honestly, I think a big part of it is redefining what the rule is of someone working in machine learning or, working as a machine learning engineer, let\u2019s say. If you think about what you learn on your Intro ML course. You walk in, it\u2019s like day one, standard, supervised learning. I remember this pretty well, for me, and the first day, it\u2019s like, well, a supervised learning problem consists of a data set, a set of labels, a loss function, a hypothesis class, right? That\u2019s what defines the problem.", "Dylan (00:09:11):That\u2019s really good, but it\u2019s like saying, well, a program is defined by a set of terms that come from some grammar, and it\u2019s your job to convert that faithfully into machine code. It is accurate, but it doesn\u2019t focus on the aspect of actually making these systems work, which are often related to how you actually get your data, making sure that the data labeling processes are actually tied to the causal mechanisms in the world that you care about. Monitoring a data stream and making sure that the data doesn\u2019t shift over time.", "Dylan (00:09:47):I think ultimately, there\u2019s this question of how do you build a system with humans and robots as a part of it, that ultimately ensures that the behavior the system exhibits represents something that we can tie towards the person\u2019s goals. That doesn\u2019t sound like a super concrete recommendation. But I think that part of it is just a recognition of how many problems can be solved by being more careful about where you get your data, how you label your data, and how you actually approach problems.", "Dylan (00:10:22):One of my classic examples here has to do with hiring scenarios, where you can look at this problem, and you can say, I think, well, what does it mean to hire well? Well, I guess I want people that do well in performance reviews. I want to learn to predict who\u2019s going to get good performance reviews. I think this sounds reasonable. If you\u2019re not thinking super carefully about what that represents, and how that relates to the history of your company or your country or different contexts, that\u2019s a perfectly fine approach. I think, if you think about it a little more, what you start to realize is that actually encodes to, how can we hire the people that we would have wanted to hire in the past?", "Dylan (00:11:13):Which is a really interesting question, but probably not what you should optimize in order to hire the people that you want to hire now, or certainly something you should think really carefully about assuming transfers between the past and now, things arise in terms of hiring biases, which you have companies that had existing biased systems for evaluating candidate employees. The companies then went and invested a ton of time in, who they hired and how they did their training, and how they did their hiring processes in order to remove this aspect of bias.", "Dylan (00:11:51):Then, at the same time, you bring in systems trained to replicate that bias, and introduce that in a new way. I think that\u2019s a failure to really think through the important part of what actually is the problem that I\u2019m solving, rather than, say, focusing on what\u2019s the AUC that I can get with my model? Which is ultimately where you\u2019re going to get to, and, quite frankly, is an easier thing to measure. If you\u2019re an engineer within a company, and you\u2019re trying to make a case for promotion, that\u2019s going to\u2026 Having something like that to make your case with is certainly what you want to aim for, and it\u2019s much harder to describe and quantify the work of making sure that we actually are solving the problem we really wanted to solve in the first place.", "Jeremie (00:12:39):Well, this very much seems to invite some thought about Goodhart\u2019s law, and just the idea that, the moment that you define a metric that you want to optimize, it ceases to be a metric if it\u2019s worth optimizing for, because people find clever hacks around it. I guess, one concern I have with respect to the application of Goodhart\u2019s law here is does it imply that this is an intractable problem of even trying to identify a loss function that we should be striving towards in the long term?", "Dylan (00:13:12):I think that\u2019s a really good question. I think\u2026 At a high level\u2026 Should we talk about what Goodhart\u2019s law is-", "Jeremie (00:13:27):To my understanding, Goodhart\u2019s law is this idea that, for example, the moment let\u2019s say you pick a metric that you want to optimize for, like followers on Twitter, you might think on day one, that this is going to cause you to engage in the kinds of behaviors that you would associate with the people you admire. If I have a lot of followers on Twitter, I will become a person that I admire. But then you realize that the more you think about it, the more time and energy you spend on trying to optimize the crap out of that metric, you start to realize they\u2019re a little clever hacks that you can apply. You can play a follow, follow back game, or you can say outrageous things on Twitter that get you more followers, even if it doesn\u2019t help you move the needle on the thing you originally wanted to do.", "Jeremie (00:14:07):I guess the idea here is AI systems, as they become more sophisticated, as they in particular start to completely outstrip human performance on solving almost arbitrary tasks, we\u2019ll be able to put in compute cycles that just dwarf human ability to overfit to particular metrics. Eventually, you get to the point where you get outrageous solutions, almost no matter what loss function you specify. No matter how much time you might put into thinking of an objective function that you\u2019d like to optimize, the machine will find a way to do it so well, that it transcends the initial purpose of the program.", "Dylan (00:14:41):Right. I think that\u2019s a good summary, and I think covers a pretty broad spectrum. Let\u2019s put the relationship between Goodhart\u2019s law and AI to the side first, and just focus on kind of Goodhart\u2019s law and actually how that shows up with people, because that is ultimately where it arises from. It was formulated because Goodhart was actually talking about the inability to have effective monetary policy, which is a little bit strange. But it was basically saying that in some ways people have their own goals and objectives. If you change a metric, you\u2019re unlikely to actually meaningfully shift their behavior, and they will just adapt. So, you\u2019ll see the smallest possible shift to make your metric go up, that doesn\u2019t actually change what they\u2019re doing.", "Dylan (00:15:43):I think, as it\u2019s been interpreted more broadly by people within AI, safety in particular, and AI systems, and social science, generally, there\u2019s an observation that it\u2019s a particular statement, which is that an observed statistical correlation when put under optimization pressure, or I may be getting the details of it wrong won\u2019t cease to exist.", "Dylan (00:16:11):I think if you take that in its more general form, you get this observation that writing down a reward function or specifying your objectives is brittle. I think, in many ways, this is something that we deal with all the time in our lives, to some extent, we all have different incentive schemes that we\u2019re either subject to or that depending on your situation that you might impose on others, if you teach students you think a lot about incentive schemes that you put in place, and the implications of those.", "Dylan (00:16:52):I also think if you have employees, and you think about how you measure their performance, that also brings up similar questions. You can think about the implications of different ways of measurement, and the ways that people will work around them. There\u2019s some default bias in what they want, and they\u2019ll shift their behavior somewhat. But in many cases, if someone trusts you, you can shift their behavior towards what you want without having to use incentives. Incentives are very brittle, is my summary. There\u2019s a way that you can\u2026 A small amount of error in an objective that you specify can somehow seemingly be amplified.", "Jeremie (00:17:41):That\u2019s interesting. Would you say that the amplification of the error, or the fact that the error is put in sharper and sharper relief, as you approach a peak in parameter space? I guess, the way I\u2019m trying to imagine this, and please correct me if I\u2019m completely obvious here, but I\u2019m imagining an artificially intelligent system, which is trying to\u2026 Basically, it\u2019s an optimizer, it\u2019s trying to climb a peak in parameter space. We hope that the apex of that peak is going to coincide with an outcome that we actually genuinely deeply want.", "Jeremie (00:18:15):I tell my AI, I want you make to make the stock market go up, for example. Might be the last function that I assign it. But what I really want is I want it to make all humans really, really happy.", "Jeremie (00:18:27):Initially in the dark ages, when everything is absolute garbage, it really seems like the peak of that stock market landscape coincides with making people happy, because from far away, the difference between make the stock market really high and make all human beings really happy, seems to be pretty small in relative terms, but it\u2019s only as we start to climb that we notice, whoa, there\u2019s this big divergence starts to appear between these two goals. Is that an accurate framing?", "Dylan (00:18:54):I think that somewhat is there, although I\u2019m not sure that that helps for building intuition. One of the big things about make all people happy as a goal is that there\u2019s lots of people and people disagree about even the ways that you could come to agreement over some way to represent what that means. I think that brings in a layer of complexity that we might want to sidestep for now, let\u2019s say. But I think there is just a nature of, if you imagine teaching to the test, is probably one of the more intuitive things of to some extent studying for the SATs does help you become a little bit smarter or become better at certain types of math. There are certain things you have to get better at overall. But once you hit a certain point, which I think most people trying to apply to colleges hit in some way, studying for that does not help make you better at anything other than being good at that test.", "Dylan (00:20:06):I think it\u2019s that feeling that really drives these things. If we want to think about AI systems, there are some really clear examples that we\u2019ve actually really dealt with already as a field. For me, I think the biggest one is overfitting. If we think about what overfitting is, it\u2019s the observation that, well, you would like to optimize for true risk. You don\u2019t have true risk, though, you have empirical risk. For a while, optimizing empirical risk is helpful, and after a while, it\u2019s not, and after a while, it becomes entirely unrelated from true risk.", "Dylan (00:20:55):If you think about it, a lot of the work that we\u2019ve done on regularization, in prediction is about coping for and overcoming this misalignment between empirical and true risk. We have theorems about convergence that tell us, here\u2019s a class of reward functions that we assume we can sample from iteratively by drawing new samples from a fixed distribution, and we know that that class is in the limit trends towards the objective that we care about. In this sense, it\u2019s a set of objectives that ultimately trends towards the objective that you care about in a rigorous, mathematical way.", "Jeremie (00:21:40):This would be in the limit of large amounts of data that are-", "Dylan (00:21:44):Exactly precisely. Large amounts of labeled data, which is a really important distinction here.", "Jeremie (00:21:51):Yeah, and this seems like it really brings us to some of your research in an interesting sideways sort of way, we\u2019re bandwidth constrained, I think it\u2019s fair to say, in terms of our interaction with machines, it would be great if humans could provide feedback on how machines are performing the tasks that we ask them to do in a super fast way so that we could make sure every task that they do was perfectly supervised, we can\u2019t. I guess a lot of research is involved in answering the question, how can we make this happen more efficiently? How can we get humans in the loop in more constructive ways? Can you speak to that a little bit?", "Dylan (00:22:30):Sure. My research is about looking at a human and a robot working on a team. I think, when we do this work, and when we look at hypothetical agents, there\u2019s this thing you have to do, which is to draw the line around what counts as the agent, and what counts is the environment? A lot of our modeling for AI systems and a lot of the way we think about it, the agent is a robot of some kind, it\u2019s some prediction system. My research asked the question, what does it look like if we draw that box around the person and the robot? This is modeling a particular type of human robot system where you have partial information about what the true goal is, about how to actually accomplish that, and you have to communicate that through some sort of noisy channel.", "Dylan (00:23:23):In general actions a person could be taking. This could be you communicate it through demonstrations of behavior, which have to get imitated in new context. You could also imagine this would be natural language, or it could be a labeled data set of some kind.", "Dylan (00:23:40):My research looks at how can you combine those systems in a way that efficiently accomplish those goals? Now, what would this actually look like in practice? One particular idea that I\u2019ve looked at is one called the inverse reward design, which takes, I think, the simplest version of this type of interaction that we use and tries to look at one additional layer of complexity in how the human robot interact.", "Dylan (00:24:09):The idea here is you observe a reward function. You know two things really, one is that this reward function is not actually the true goal. There\u2019s some way in which it\u2019s wrong and which is aligned. Two, it is yet a really good source of information about what you should be doing at any point in time.", "Dylan (00:24:36):Exactly. How do you actually recognize the types of errors that are going to be likely? The first thing that you look at in this problem is recognize that on its own, you can\u2019t answer this. If all I tell you is here\u2019s a reward function, it could be wrong, and you don\u2019t know anything else, there\u2019s nothing you can do with that. What I did in this work is we brought in the idea of context for a metric. You have an existing set of options that you\u2019re choosing between, and you have a metric that you\u2019re selecting. What\u2019s your commitment, basically, to the system is, this metric leads to good behavior in these sets of environments or contexts.", "Dylan (00:25:16):Now, this gives you some information that\u2019s more structured. What you can do is actually say that this metric is a way of specifying a ranking over different behaviors, and I can treat this as an observed ranking of behaviors. From that, infer the likelihood of new behaviors in different settings, or more usefully actually, to figure out when the distribution over how those behaviors extrapolate to new settings is, if it\u2019s very, very tight, you have high confidence that, Oh, this is a similar environment to the one that the system designer had in mind, and so I can behave appropriately. Or if it\u2019s very broad, you can say, ah, this is bad, I should probably not go over there. Choose to either go get more information, or to pick an alternative, perhaps safer course of action.", "Dylan (00:26:12):An example that we use for this is we call\u2026 It\u2019s a setting where you\u2019ve got a robot that\u2019s trying to drive between some 2D environment, you\u2019ve got some different types of phenomenon that you want it to go observe. Imagine there are something like pots of gold throughout the environment that you want to seek out. For the environments you have in mind, there\u2019s really two types of terrain, there\u2019s grass and dirt. There\u2019s dirt paths, and then there\u2019s lawns and things, and you want it to mostly stay on dirt, find these objects, and cut across the lawns, where it has to.", "Dylan (00:26:45):Now, you would do this by collecting examples of each of these types of terrain, examples of the pots of gold that you want to go and seek out, training classifiers and then specifying a reward function that uses those classifiers, basically.", "Dylan (00:26:59):The problem is, if you forgot that your robot was going to be deployed in Hawaii, and so there\u2019s actually lava as well, everything goes haywire. You won\u2019t have a consistent prediction for what it should do with lava, because there\u2019s no clear implication of that based off the data you\u2019ve seen, or probably not. It all depends on the feature space and biases of the learning algorithms that you\u2019re using. But the point is, it\u2019s arbitrary what it does in that situation.", "Dylan (00:27:33):You\u2019ll get a bunch of metrics, and they all would say the same thing in a world with no lava, and wildly different things in a world with lava. That gives you an idea that perhaps my metric is not very well adapted to this environment.", "Jeremie (00:27:51):This seems like it introduces almost a dose of humility and introspection to the system where you first look at your environment and say, \u201cOkay, well, how much can I really map my Hawaii experience to my San Francisco experience?\u201d I guess, what conclusions does the robot or the AI tend to draw based on that? Would it tend to say, okay, I\u2019m just not going to take any action because I\u2019m uncertain maybe if I\u2019m in a new environment that doesn\u2019t map well on to what I\u2019ve seen before? Or is that left as an exercise to the developer?", "Dylan (00:28:27):I think in a very humble way, it\u2019s left as an exercise to the developer. In the sense that it\u2019s not my place to specify. I think, actually, one of the important things that\u2019s come out of this work is what things should you be careful about specifying as the developer? I think one of the things that actually falls out of the math in this case, in a really nice way, is specifying a fallback behavior for when you are too uncertain. I don\u2019t know what level of mathematical detail your listeners will want.", "Jeremie (00:29:06):Yeah, we might keep it reasonably high level, I guess, just so that\u2026 But if you have a-", "Dylan (00:29:11):I think that the high level version of it is\u2026 Would you say that people would be familiar with the idea of identifiability within a probabilistic model? Perhaps?", "Dylan (00:29:31):Identifiability is the question of what can you actually figure out about latent variables within a model from data? The classic example of this is if you have a mixture of Gaussians that are drawn from some\u2026 You have some latent variables that are the mean and variances of those Gaussians. When you do clustering, you observe this data, and then you fit these latent variables. In the limit of infinite data, you can prove that you will identify the correct means and covariances on average. With infinite amounts of data, you will eventually get there. However-", "Jeremie (00:30:11):Is this\u2026 Can I reconstruct the parameters of a neural network based on the outputs, the predictions that I-", "Dylan (00:30:17):Sort of. The thing is the values of the means are identifiable, but the ordering of them in that vector of latent variables is not.", "Dylan (00:30:30):There\u2019s a bunch of parallel hypotheses that will all get equal weight in your final inference.", "Jeremie (00:30:36):Correct me if I\u2019m wrong, that intuitively makes sense in the sense that I would imagine there would be, in fact, a potentially infinite number of neural networks that would produce the same input output profile.", "Dylan (00:30:54):You could use anything in here. But the point is, if you have two different settings of the latent variables that produce the same distribution over data, you can never figure out from observation, which one is actually the case. This is a very general concept known as identifiability. It\u2019s the study of what can you figure out based off of assumed causal structures in your data?", "Jeremie (00:31:22):Interesting, that seems almost\u2026 It seems to point to a problem, in that, I could imagine, let\u2019s say, two different neural networks, one of which gives exactly the results I want. Maximizes human value, or whatever I mean by that, and then the other of which is, really pathologically awful, but in ways that I just can\u2019t detect with the particular samples that I feed it. It\u2019s possible for an arbitrarily large set of those algorithms to exist, most of which will be pathologically, but I can only ever check their behavior on a finite number of samples, right? Does that speak to the verifiability, almost of alignment?", "Dylan (00:32:05):I think that\u2019s getting into questions like the no free lunch theorem with prediction, right? If you are asking questions about out of distribution performance, we don\u2019t have good ways about giving guarantees without additional assumptions. You know that those assumptions will help in some cases, and they must necessarily hurt than others. I think this is just the same kind of thing.", "Dylan (00:32:28):Now, I think my overall perception is that rationality, and the idea that behaviors should be optimized towards an objective is actually a choice, and it\u2019s a tool for how we design compilers that take in these representations of goals, and these representations of desired behaviors and communicate them. But I think it doesn\u2019t need to be the only property that we have.", "Dylan (00:33:01):Well, I think the first thing to say is, if you think about rationality, it\u2019s a high level heuristic of someone wrote down and objective. Well, it turns out, most behaviors you could do are dumb. Most neural nets don\u2019t do anything useful. Bringing in this idea of optimizing towards accomplishing some task, this paradigm basically builds in a heuristic of at least do something. This reduces the space of programs that you have to work with to represent what you want. Because rather than needing to distinguish between all of the little details of how things are worked out, you can say, well do this in a way that\u2019s reasonable or tries to optimize some set of goals.", "Dylan (00:33:50):I think you can also say, well, we don\u2019t really want purely rational behavior at the end of the day, we\u2019d like to have more regularized behavior. I think that\u2019s something to include within a spec. If you think about it, we don\u2019t write supervised learning systems that optimize empirical risk directly by default. In some settings, it works if you have enough data, and you can still rely on other physical properties to regularize the ultimate behavior.", "Dylan (00:34:22):In many ways, we do actually rely on the computational restraints of our systems to regularize their behavior. In theory, there is a really, really good strategy for every RL environment ever, which is to somehow do like a buffer overflow attack and write really large number to wherever in memory that reward calculation comes from, right? The reason why our systems don\u2019t do that is on the one hand, really obvious, but very hard to explain. It\u2019s not clear that there are easy strategies within a particular set of weights that would do that. I think we\u2019re confident that actually there are properties of the types of optimization techniques we use that mean that that wouldn\u2019t be what we wind up at.", "Dylan (00:35:26):I don\u2019t think we know how to describe them. I don\u2019t think anyone\u2019s realistically worried that an optimization that you\u2019re running currently is going to somehow go off the rails.", "Jeremie (00:35:41):Yeah. I\u2019ve seen people explore the possibility of scaled language models, for example, as potential risk factors in the sense that, take for a GPT 10 or whatever, and, you keep making the neural network larger and deeper, it starts by learning low levels of abstraction, and basic how to put letters together and whatnot, eventually grammar and so on, and then eventually develops a model of the world of some kind. That model may eventually come to include a recognition of the model\u2019s own place within the world that might go, oh, look at that, because it\u2019s instrumentally useful to predicting the next word in a sentence, I\u2019m going to realize that, hey, I\u2019m actually embedded in this world, and therefore along with that, start to get greedy\u2026 There are all kinds of [inaudible 00:36:31]", "Dylan (00:36:31):Right, this is like changing the world so that it\u2019s easier to predict the next word.", "Jeremie (00:36:38):Right. I guess this plays with what you\u2019ve been talking about is this distinction between the agent and the environment. This would be a moment in which the algorithm realizes, hey, I\u2019m embedded in the environment here, and it\u2019s time to start blurring those lines.", "Dylan (00:36:56):I think these perspectives, I think, are both worth taking seriously, but also very hard to actually refute. I think that\u2019s something to take seriously, and you should bring an appropriate level of skepticism to them, I think in that way. For example, there\u2019s an assumption that, basically, if the only character is a\u2026 Well, let me let put it this way, we have lots of examples from economics that rational behavior is weird and counterintuitive, and fundamentally, not human, I think in a lot of ways, right? Homo economicus is a different type of being.", "Dylan (00:38:00):I think if the only assumption you make about AI systems is that they are an instantiation of homo economicus with an arbitrary utility function, that\u2019s bad. You will end up with bad outcomes, because you are hypothesizing a scenario that\u2026 I think, basically, by assumption, you can\u2019t work out very well.", "Jeremie (00:38:30):That\u2019s a shame. Well, I guess it\u2019d be almost impossible by definition to figure out exactly where\u2026 If we could figure out where that assumption failed, we\u2019d know a whole bunch more things. But to my mind, the reason homo economicus doesn\u2019t work is that evolution has programmed us with wants and desires that are\u2026 Well, we\u2019re not random, but very misaligned with economic incentives. We want to do things\u2026 We\u2019re jealous of people, sometimes. In pathological modes of human existence, we sometimes value other people\u2019s suffering more than our own happiness, if we get vengeful, or whatever, and that throws off the whole model.", "Jeremie (00:39:16):In the case of, I guess, a machine learning model, or say, a deep neural network, that\u2019s essentially unconstrained by those evolutionary conditions, I guess you might say, effectively, it\u2019s undergoing a process of evolutionary optimization. Maybe that-", "Dylan (00:39:37):I don\u2019t think anyone can tell me that we can\u2019t figure out how to program something that fits that behavior profile, or that is well modeled by something like an optimal or rational agent or something like that. I\u2019m certainly not saying I think that\u2019s impossible. I\u2019m not saying that I think if we did build that, I don\u2019t think it would be good in a bunch of ways. I think that some people can talk about that through existential risk. I think you don\u2019t necessarily even need to get to that to really talk about pretty just bad outcomes that something like that would lead to.", "Jeremie (00:40:30):I totally agree. This is something that\u2026 To be clear, I actually framed this as an extremely or think of this as an extremely dangerous category of outcomes when we start to talk about breakout risk and that sort of thing. Yeah.", "Dylan (00:40:42):But I think that that is\u2026 The thing that I push back on is that that\u2019s what AI systems or the outcome of AI research has to be. There\u2019s a step that arguments like this take, which is to say, that is a model of what future AI systems will be. I think if you will go with me again, on this compiler analogy, at some point along the way, there was an innovation in building these systems, which was to bring in these models of optimal behavior, and to say, we should make our compiler more likely to output things that at least do something useful. That has been really useful for making progress within AI research and AI systems. But that doesn\u2019t mean that we have to push that line of research through to its eventual completion or that AI systems necessarily need to work in the direction of individually rational economic agents.", "Dylan (00:41:44):I think there are a lot of people who, quite frankly, want to build that, and I think that\u2019s not great. I think that comes out of a certain type of mindset. But I don\u2019t think it\u2019s inevitable. I think that\u2019s a choice, and actually, I think the outcome of looking at these arguments about risk is what actually are the appropriate directions to go in and what are useful alternatives there?", "Dylan (00:42:10):This is where actually feel kind of hopeful, because the problems that AI systems are running into as we try to deploy them in the world right now, to some extent, are these types of issues. They\u2019re not necessarily issues of prediction error, although I think you could argue that they are that way. But they\u2019re issues of not being able to appropriately represent the minimum viable product of goals for lots of important settings, and not producing practitioners who are trained in doing the hard work of thinking deeply about what those goals are as a process of representing them inside of a machine learning system.", "Jeremie (00:42:55):I definitely agree that it would be great if we could avoid the kind of circumstance in which people mindlessly move towards this super juiced up model. I guess my concern is that economically, it seems like there\u2019s a powerful forcing function in that direction, and that if we take the example\u2026 Okay, this toy example of a super scaled up deep neural network that\u2019s more massive than anything we\u2019ve ever built, the people who are trying to make, I won\u2019t call it GPTN, because that may sound like I\u2019m singling out open AI. I think they\u2019re approaching this with a lot of the risks in mind. But somebody who tries to make whatever that massive model looks like, might actually\u2026 They might not be trying to overfit. They might be trying to do the best job they can at coming up with a reasonable loss function that produces just a really powerful model, and not know, at what level of training, at what level of abstraction does the model decide to break out?", "Jeremie (00:43:57):They might even be approaching it with a safety conscious mindset, but because we have no way of really predicting at what point that abstraction leap will be made, it becomes an inherently dangerous thing to push in that direction.", "Dylan (00:44:13):Right. To be clear, I\u2019m not agreeing with that concern on a material level. I think we can be concerned that that is an issue and I have not seeing any genuine arguments that convince me that that\u2019s not likely to happen. I think there are intuitive arguments, there are people\u2019s perceptions of what\u2019s possible based off of either experience building systems or intuition from some other way. But I don\u2019t think there are any actual, solid arguments you could point me to, to convince me that that\u2019s not an issue.", "Dylan (00:44:59):Quite frankly, if that exists and someone knows about it, I would love to see it. I think my point is at the same time, I think we can recognize that, recognize that that is a challenge, and then also wonder what is a more serious form of proof that that is something to be concerned about with current systems? All of the arguments that I\u2019ve seen suggesting risk, are four different ways of things like\u2026 We will talk about ideas like breakout or something like that. That\u2019s not really how I try to think about these things, because I don\u2019t know how to make that idea precise, I don\u2019t know how to define that idea, and I don\u2019t know how to do anything useful with that representation.", "Dylan (00:45:48):Again, this is something where if someone can figure out a good formalism, a good way to represent this, to model this, I think that\u2019s good. But I\u2019m not sure where it comes in. The arguments for it rely on rational behavior as the axiom for how your AI system functions. The claim is that it would break out because that\u2019s better for the objective, as you have it represented within the system. On the one hand, I think that\u2019s true, if you accept the assumption that it\u2019s good to model this as this homo economicus. But, I also want to know, okay, are there any other assumptions that actually do lead to that? Mathematically, are there ways to build systems that are not actually purely optimizing their objectives?", "Dylan (00:46:42):I think in practice, this is certainly true. We have regularization. We can train things that make the best use of an empirical data set to make predictions on the true distribution of data that led to that, and we can tailor that to the data set size. That\u2019s something that you have to do. No one can tell you what number to set your regularization parameter at or the particular details, but we recognize this as a problem, and we know that overcoming it is a central part of building an AI system.", "Dylan (00:47:16):I think there is exactly the same kind of thing that shows up maybe one additional layer of abstraction higher, which is related to okay, what is the actual human cognitive process that you want to try to imitate, in the case of supervised learning? Here we are heavily, heavily biased as practitioners and as a field towards what is available, rather than what do we need?", "Jeremie (00:47:49):Yeah, and it\u2019s interesting how much of an interaction there is between the safety of the system, and then its overall abilities, because it\u2019d be wonderful to be able to fully leverage all of the capacity of a neural network towards some objective. But at the end of the day, anytime we talk about safety, that has to mean imposing some kind of constraint. The moment you start to let go of that harness, you venture into-", "Dylan (00:48:16):I would question that assumption, almost. In the sense that, okay, if we can assume a perfect world, you\u2019d like to squeeze every last bit of performance out of the system. But there\u2019s the Warren Buffet line about good employees. It\u2019s, you want people with some\u2026 I\u2019m sure I\u2019m getting it wrong, but it\u2019s like you want people with intelligence, grit and trustworthiness. If they don\u2019t have the last one, watch out for the first two.", "Dylan (00:48:49):I think that that does actually apply here in practical senses, not for far off systems, but for current systems. You would be okay with, except\u2026 For instance, if you went to YouTube right now, and you told them, \u201cI have a way for you to\u2026 You will do a worse job of predicting how likely someone is to click on this particular video, but I can guarantee you that it doesn\u2019t have a negative impact on the well being of teenagers as measured by Institute A, B and C.\u201d", "Dylan (00:49:33):If you went to them, and were able to specify that, that\u2019s an incredibly valuable thing, I think. That\u2019s something that you are not going to be able to fix by just simply relabeling the data that you have. You might be able to get at it by fundamentally redesigning the types of interactions that people have with your video recommendation system. But, there\u2019s a problem that they want to be solving, but they\u2019re unable to solve, they\u2019re unable to specify, and the barriers between being able to effectively make use of this technology are not really, I think, improves prediction performance, but it\u2019s actually improved ability to easily specify goals.", "Jeremie (00:50:23):Right, which I guess, yeah, that\u2019s the thing. It\u2019s like because you haven\u2019t specified the exact optimization objective that you want, you\u2019re then forced to take steps to limit the behavior of the system as a whole, using more coarse grained instruments. Basically finding a hammer to beat your neural network with a little bit.", "Dylan (00:50:47):I think that\u2019s kind of right. It\u2019s like you have this large set of behaviors, which consist of different parameterizations of a neural network, let\u2019s say, and you\u2019re trying to figure out how to pick out the right one, that\u2019s something that a person can\u2019t do on their own. We figured out that specifying some sort of metric, and then optimizing these predictive losses is a good way to actually prune out a whole bunch of these weight configurations that are just bad.", "Dylan (00:51:27):But not all of the wrong ones, and it actually turns out that if you focus too much, then you have a very high probability of getting something wrong in a different way. There\u2019s one end of the spectrum you have useless random behavior, and at the other end of the spectrum you have\u2026 With probability one, you probably don\u2019t actually have the behavior you want.", "Jeremie (00:51:50):This is something that\u2026 The idea that there may not actually exist an objective loss function, or an objective function, rather that humanity might want to optimize towards is\u2026 I don\u2019t know, for people who believe in something like AI absolutism, or the fact that eventually AI will be doing pretty well everything, a little concerning.", "Jeremie (00:52:14):Sam Harris, for example, has this book, The Moral Landscape, and he talks there about, essentially, what is an objective function that he believes exists out in the wild. But it always seems underspecified to me, anytime somebody speaks about morality in that way. Do you think that they\u2019re actually may be something that we could optimize to, I don\u2019t even know what I\u2019m going to say here, make people happy, maximize human flourishing?", "Dylan (00:52:42):I don\u2019t necessarily subscribe to that set of beliefs about where things will end up. What I\u2019m interested in is, what are the systems that we can set up to recreate some of the balancing of incentives that we have in human societies and to incorporate that into AI systems in general? I think, to some extent, this is a question of, what are the processes that we can, as people, put in place to argue about those things? To some extent, I don\u2019t know, to try to replicate the processes that we\u2019ve built and designed as a species in order to regulate group behavior and coordinate ourselves at scale?", "Dylan (00:53:37):I think there is this aspect of, we figured out how to balance a lot of different incentives within us individually, and as societies and structures. There are ways of having a trusted third party or the idea of having an agent. An agent is already a job, someone can be your agent, you can hire someone in the real estate market to be your agent and represent your interests. That didn\u2019t come out of nowhere, someone invented that as a way to manage the complexity of the world.", "Dylan (00:54:16):We can also be doing that with AI systems. We need to figure out what that looks like. What is your agent that can be trusted to act on your behalf in setting your settings on Facebook for what types of content you want to see, and when you want to see different kinds of things?", "Jeremie (00:54:34):Do you think this picture of robot-human interaction can be maintained in a world where robots, machines are doing things that are so complex, that human beings wouldn\u2019t even be able to understand the implications of those decisions?", "Dylan (00:54:54):I think that\u2019s a really tough question. I think the more mundane version of it is, how do you present people information in appropriate context, to give them the best way to make decisions that they can? This includes, how do you provide an appropriate context about what the consequences of that decision will be?", "Dylan (00:55:23):That is something that we don\u2019t know how to do. I\u2019ve been thinking a lot about how value alignment relates to recommendation systems in general, and content recommendation systems like YouTube and Facebook specifically. That\u2019s why it seems like I keep coming back to those examples. That\u2019s part of why. But we\u2019re not very good at figuring out how to get someone into a state where they can think reflectively about what type of things they want to engage with on the internet. My perspective on this question is, well, it is true that we could imagine possible systems in a far off future and talk about that. But realistically, it\u2019s like, okay, for something that\u2019s a new thing that exists, which is this weird type of content feed that\u2019s algorithmically filtered and selected for you, what is\u2026 Most people are not even aware of the appropriate terminology for what that is, what the recommendations are, it\u2019s really interesting for me when I go out, and occasionally run\u2026 It doesn\u2019t happen too much anymore. But you occasionally run into people. I would often ask them about their recommendations.", "Dylan (00:56:38):Everyone has different theories of ways of anthropomorphizing the recommendations that they get, or the rankings that they get, and what it\u2019s figured out about them and how it works. They\u2019re always really convinced about them, and they\u2019re usually wrong in some way, which is fine, because no one-", "Dylan (00:56:56):It\u2019s pretty complicated what\u2019s actually happening behind the scenes. As someone who\u2019s not at either of those companies, I\u2019m not aware of what\u2019s happening behind the scenes really. The fact that there is something like your information filter, or this ranking algorithm that\u2019s selecting what you see, and that has implications for the person that you will be, the types of news consumption that you\u2019ll have, what kinds of things should you care about in that? Should you care about balance, fairness? There\u2019s a whole bunch of different values that you might care about having represented there.", "Dylan (00:57:32):The question is, really, how do you begin to have that conversation? How do you\u2026 This is a combination of public education, helping people know that there actually is a problem there, and designing systems around providing context, helping people recognize problems they should be solving, helping them recognize the value of the cognitive work you\u2019re actually asking them to do.", "Dylan (00:57:54):I think one of the big ironies here is there actually are a ton of tools and controls you have. It\u2019s very interesting to go and see people say on Twitter that they wish Twitter had a way to see things less often. I\u2019ve seen this many times from Twitter power users, people with check marks and lots of followers and things. The reality is Twitter has that feature. It\u2019s there. I don\u2019t know how long it\u2019s been there, but it\u2019s there. The thing is, just no one knows what it actually is going to do, or how to trust that it\u2019s actually going to help them in the future.", "Jeremie (00:58:33):It\u2019s also I guess, this interesting question of, even if you knew exactly how this recommender system worked, and you knew exactly everything was open source or whatever, you still had this really complicated moral question about who you want to be. I can spot a YouTube video that is pregnant with the potential of turning me into a brilliant\u2026 I don\u2019t know, like a marine biologist, and I could just follow that rabbit hole and keep following videos.", "Jeremie (00:59:05):There\u2019s probably, in fairness, a sequence of videos that I could watch that would actually do that. But am I in a position to know which version of me I would want to be, if I\u2019m faced with that option, or another option that takes me to machine learning engineer as a final end state, or whatever\u2026 Or one that takes me to become a Nazi, or a communist or whatever else, it seems like\u2026 I don\u2019t know who the future versions of me would want me to be if they knew what all the different future versions of me would want. I don\u2019t even know where this is going.", "Dylan (00:59:37):This is where you get into some pretty complex philosophy and you can talk about ways to think about it. I\u2019m certainly not the right person to answer those questions for anyone but myself. But I think the observation that I think most people would agree on is it is cognitively taxing to answer those questions. There is a fundamental, cognitive work that goes into answering the questions of who do I want to be?", "Dylan (01:00:14):In some ways, you have to figure out how to design the system to overcome, not just the informational problem of you don\u2019t know who that person wants to be, or to make it more concrete, what content they want to see, which is related to who they want to be. The fact you can influence this, if you could just not influence who someone\u2026 Say, I\u2019m not going to influence who someone is tomorrow, based off what I show them. Again, that might be something that you would actually want. But you don\u2019t know what that is as the designer of a recommendation system. But beyond that, the person doesn\u2019t often know the answer to that question, perhaps doesn\u2019t even know how to really have that type of conversation or doesn\u2019t like that type of reflective thinking of who I want to be as a particular style of thinking, but not everyone likes to engage in. Or for me in a way, I think about it is that it is costly, and differentially costly for different people.", "Dylan (01:01:18):How do you convince people to go through this costly process, when all they wanted to do is disappear into some cat videos for a moment?", "Dylan (01:01:27):At the same time, it\u2019s clear that there would be huge positive externalities. If I could wave a magic wand for society right now, it would probably be to get everyone to budget 15 minutes a week to think reflectively about how they consume information, and the types of information they want to consume. Then to spend five minutes doing something that sets them up for success, based off that. I think if everyone did that, it would be a huge, huge positive externality for society, because there would be the individual change that you would have for your personal information consumption.", "Dylan (01:02:09):But the compounding effects of that actually shifting the overall, and actually steering and including certain types of cognitive processes into steering the information diet of a population.", "Jeremie (01:02:25):I can\u2019t get over\u2026 My most productive days are always the ones I don\u2019t spend on Twitter, for example. It\u2019s definitely the case that\u2026 The way it feels right now is that the amount of time that we have to step away to do information processing, relative to the amount of time we spend consuming information, the balance has just completely flipped in the last 10 years. It used to be, you\u2019d see one big news story in the day, you might spend the rest of the day chewing on it, maybe talking to a handful of people about that same focus story, in which case, the reliability of that story mattered a lot more. Whereas now it\u2019s just this nonstop, input, input, input and very little time for cognitive processing. An interesting phase shift in the way things are happening.", "Dylan (01:03:12):If you have a world where information is more scarce, or harder to get requires that you actually\u2026 It sounds a little silly, but if you have to choose to go and grab a newspaper, there\u2019s something about that action and that choice that commits you to it a little bit, it gets you into a slightly different state. Whereas, if you just want to pull out\u2026 If instead you\u2019re going to get information from, I don\u2019t know what else to do, there\u2019s a scroll of things for distraction. That\u2019s a very different kind of thing.", "Jeremie (01:03:40):Well on that note, I\u2019m going to hypocritically ask if you have a Twitter link that you want to share, if people want to follow you there to see more of your work?", "Dylan (01:03:49):Yeah, definitely. I\u2019m dhadfieldmenell. First initial, last name, no punctuation or anything else like that. I can\u2019t promise to tweet all that often. I\u2019m currently actually taking a break. But I\u2019ll definitely be posting paper updates, and I tend to tweet on occasional AI ethics and general value alignment topics.", "Jeremie (01:04:13):Awesome. Well, really appreciate your time and really fascinating, almost all encompassing discussion today. So, I really appreciate it. We\u2019ll be posting a link to your Twitter and I think also your academic website as well, if that\u2019s okay, in the blog post that will come with the podcast.", "Dylan (01:04:26):Yep, that would be great. Thank you so much. It\u2019s a real opportunity and I appreciate the chance to talk to you guys.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Co-founder of Gladstone AI \ud83e\udd16 an AI safety company. Author of Quantum Mechanics Made Me Do It (preorder: shorturl.at/jtMN0)."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fac3699040380&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhumans-in-the-loop-ac3699040380&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhumans-in-the-loop-ac3699040380&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhumans-in-the-loop-ac3699040380&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhumans-in-the-loop-ac3699040380&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----ac3699040380--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ac3699040380--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@JeremieHarris?source=post_page-----ac3699040380--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@JeremieHarris?source=post_page-----ac3699040380--------------------------------", "anchor_text": "Jeremie Harris"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F59564831d1eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhumans-in-the-loop-ac3699040380&user=Jeremie+Harris&userId=59564831d1eb&source=post_page-59564831d1eb----ac3699040380---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fac3699040380&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhumans-in-the-loop-ac3699040380&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fac3699040380&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhumans-in-the-loop-ac3699040380&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://podcasts.apple.com/ca/podcast/towards-data-science/id1470952338?mt=2", "anchor_text": "APPLE"}, {"url": "https://www.google.com/podcasts?feed=aHR0cHM6Ly9hbmNob3IuZm0vcy8zNmI0ODQ0L3BvZGNhc3QvcnNz", "anchor_text": "GOOGLE"}, {"url": "https://open.spotify.com/show/63diy2DtpHzQfeNVxAPZgU", "anchor_text": "SPOTIFY"}, {"url": "https://anchor.fm/towardsdatascience", "anchor_text": "OTHERS"}, {"url": "https://towardsdatascience.com/tagged/tds-podcast", "anchor_text": "TDS podcast"}, {"url": "https://youtu.be/NfElDoZ5O4Y", "anchor_text": "here"}, {"url": "https://towardsdatascience.com/emerging-problems-in-data-science-and-machine-learning-36d37f6531a8", "anchor_text": "emerging problems in data science and machine learning"}, {"url": "http://sharpestminds.com", "anchor_text": "SharpestMinds"}, {"url": "https://podcasts.apple.com/ca/podcast/towards-data-science/id1470952338?mt=2", "anchor_text": "Apple"}, {"url": "https://www.google.com/podcasts?feed=aHR0cHM6Ly9hbmNob3IuZm0vcy8zNmI0ODQ0L3BvZGNhc3QvcnNz", "anchor_text": "Google"}, {"url": "https://open.spotify.com/show/63diy2DtpHzQfeNVxAPZgU", "anchor_text": "Spotify"}, {"url": "https://twitter.com/dhadfieldmenell", "anchor_text": "follow Dylan on Twitter here"}, {"url": "https://twitter.com/jeremiecharris", "anchor_text": "follow me on Twitter here"}, {"url": "https://medium.com/tag/ai-alignment?source=post_page-----ac3699040380---------------ai_alignment-----------------", "anchor_text": "Ai Alignment"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----ac3699040380---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/tds-podcast?source=post_page-----ac3699040380---------------tds_podcast-----------------", "anchor_text": "Tds Podcast"}, {"url": "https://medium.com/tag/ai-alignment-and-safety?source=post_page-----ac3699040380---------------ai_alignment_and_safety-----------------", "anchor_text": "Ai Alignment And Safety"}, {"url": "https://medium.com/tag/editors-pick?source=post_page-----ac3699040380---------------editors_pick-----------------", "anchor_text": "Editors Pick"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fac3699040380&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhumans-in-the-loop-ac3699040380&user=Jeremie+Harris&userId=59564831d1eb&source=-----ac3699040380---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fac3699040380&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhumans-in-the-loop-ac3699040380&user=Jeremie+Harris&userId=59564831d1eb&source=-----ac3699040380---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fac3699040380&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhumans-in-the-loop-ac3699040380&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ac3699040380--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fac3699040380&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhumans-in-the-loop-ac3699040380&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----ac3699040380---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----ac3699040380--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----ac3699040380--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----ac3699040380--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----ac3699040380--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----ac3699040380--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----ac3699040380--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----ac3699040380--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----ac3699040380--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@JeremieHarris?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@JeremieHarris?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Jeremie Harris"}, {"url": "https://medium.com/@JeremieHarris/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "122K Followers"}, {"url": "http://shorturl.at/jtMN0", "anchor_text": "shorturl.at/jtMN0"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F59564831d1eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhumans-in-the-loop-ac3699040380&user=Jeremie+Harris&userId=59564831d1eb&source=post_page-59564831d1eb--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F15c61aaa3274&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhumans-in-the-loop-ac3699040380&newsletterV3=59564831d1eb&newsletterV3Id=15c61aaa3274&user=Jeremie+Harris&userId=59564831d1eb&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://www.amazon.ca/Quantum-Physics-Made-Fundamental-Everything/dp/0735244138", "anchor_text": "Quantum Physics Made Me Do It: A Simple Guide to the Fundamental Nature of Everything2023"}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}