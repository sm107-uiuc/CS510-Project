{"url": "https://towardsdatascience.com/an-involved-introduction-to-supervised-learning-for-the-common-human-6338d9559748", "time": 1682994063.959264, "path": "towardsdatascience.com/an-involved-introduction-to-supervised-learning-for-the-common-human-6338d9559748/", "webpage": {"metadata": {"title": "A Comprehensive Introductory Guide to Supervised Learning for the Non-Mathematician | by Faris Nolan | Towards Data Science", "h1": "A Comprehensive Introductory Guide to Supervised Learning for the Non-Mathematician", "description": "Artificial intelligence (A.I.): a robot tribe waging war on humanity. Technological apocalypse. The year 3000. Artificial intelligence (A.I.): paintings created by computers. Meaningful conversations\u2026"}, "outgoing_paragraph_urls": [], "all_paragraphs": ["Artificial intelligence (A.I.): a robot tribe waging war on humanity. Technological apocalypse. The year 3000.", "Well, not quite. Here\u2019s a more accurate introduction.", "Artificial intelligence (A.I.): paintings created by computers. Meaningful conversations with machines. Self-driving cars. And even, much to our misery, highly individualized, targeted advertisements.", "These are demonstrative of merely a fraction of the massively innovative applications of artificial intelligence.", "Welcome to the era of smart machines. A.I. is now upon us and fortunately, it is not looking to end mankind. When most people discuss artificial intelligence, they are referring to machine learning, commonly abbreviated as M.L., the largest field within the computer intelligence industry. In this guide I will be talking exclusively about machine learning and its inner workings, specifically linear regression: a vanilla form of supervised learning.", "Throughout the teachings to follow, multiple mathematical models are depicted and explained; keep in mind that these computations are all to be carried out inside of machines.", "Ready for some learning? If so, grab a flashlight \u2014 we\u2019re headed into the caverns of artificial intelligence!", "In tackling a complex subject, a strong foundation of the topic is massively important. Let\u2019s get an overview of what machine learning is doing before we dive into the mechanics of it.", "When we teach a computer to do something, such as recognize photos of objects, or predict tomorrow\u2019s housing prices, we use supervised learning. Supervised learning is a method in which we provide the machine with data, sorted into examples, as well as correct outputs for that data. The machine tweaks itself until it gains an acceptable prediction accuracy.", "Say we want to teach our computer to predict housing prices. We would begin by giving our machine the prices of other houses in the area, as well as information about each house (the size, number of bedrooms, number of floors).", "This mass of information is called the training set. The data we provide for any one of the given houses is called a training example. This is denoted by x(i), meaning data pertaining to house (i) (x(2) = training example 2).", "Each distinct bit of information contained within a training example is called a feature. In data representing housing attributes, the size of the house would likely be a feature, as would the number of stories and the number of bedrooms.", "Every training example contains the same features in the same order, meaning that we get the same information about every house, or example, in the training set.", "Features are to be represented by j. That is, x(i)j is feature n of example i.", "The price for a given house would be, in this case, the desired output or the label. This is represented by y(i), meaning the correct output for house (i) (y(2) = correct output for example 2).", "Note: Medium does not support subscripts and superscripts. x(i)j is normally written as x *superscript* (i), *subscript* j. y(i) should be y *superscript* (i).", "Now that\u2019s all good and well, but we still don\u2019t know how the computer actually comes up with its output.", "Linear regression is one of the most basic machine learning models and is certainly a go-to for learning. Nonetheless, it is very frequently used in large scale applications. Symbolically, this is a common representation of the model:", "Meet the hypothesis. Don\u2019t be afraid, he doesn\u2019t bite.", "I\u2019ll break the ice for you.", "h\u03b8(x) simply represents the hypothesis function. It says that the outputted value varies based on the input x. That is to say, based on the value x between parentheses, the prediction is made, meaning that h\u03b8(x(i)) symbolizes the prediction made by the hypothesis on example x(i).", "The funky Greek symbols you see (\u03b8 \u2014 these guys) are theta values, more professionally known as parameters. These can be positive or negative numbers.", "Each feature k from the data set is associated with parameter k from the hypothesis. You will soon see what is meant when I say \u2018associated with\u2019. \u03b80, however, has no respective feature, due to convention. For now, don\u2019t concern yourself with theta zero too much. Just know that it is called a bias unit.", "Note: The hypothesis pictured above only has one feature (x) and two parameters (\u03b80 and \u03b81). In real supervised learning applications, there are usually a very large number of parameters and features.", "Now that all the parts are in place, let\u2019s run through the prediction process! When we feed our hypothesis a training example, each feature is multiplied by its respective theta value. The output is the sum of these products, added to the bias unit.", "For clarification, let\u2019s go back to our housing prices example. Let\u2019s pretend we\u2019re using training example 1. Our example will use only one feature for this explanation: the size of the house in square feet. This will be represented by x(1)1, meaning feature 1 of training example 1, and will be equal to 2,500 (square feet). Let\u2019s say our hypothesis believes that each square foot adds $50 of value to the home and that housing prices start at $200,000 in the area. \u03b8(0) would then have a value of 200,000 and \u03b8(1) would then be 50.", "Let\u2019s check out why that is.", "We know that a prediction is formed by summing the bias unit with the product of features and their respective parameters.", "In real applications of machine learning, there would be between 10 and 10,000 features. Each theta value linked to a feature would hold a positive or negative weight, impacting the way in which each feature affects the final output.", "Wow! That\u2019s the process of predicting a value using a linear regression model.", "Before moving on, take a look at what a prediction may look like if it is graphed:", "Here is a hypothesis which has been fit to data with one feature. The x axis shows the value of that feature for each training example. The y axis measures the value of the desired output for that example. In the image above, \u03b80 has a value of 5 and \u03b81 an approximate value of 1/7. As you can see, \u03b80 is the y-intercept and \u03b81 is the slope of the model.", "The model we derived to predict housing prices could be graphed in the same way.", "Okay, now we understand what supervised learning is and how linear regression models make predictions based on their training examples. But where\u2019s the learning happening!?", "Don\u2019t worry we\u2019ll get there very shortly. Before we can do so, we must understand how to measure the efficacy of our hypothesis. Concretely, we need a numerical representation, specifically a function, demonstrating how well our parameters theta suit our data. This would show us how accurately our model makes predictions.", "There it is, in all its glory! The squared error cost function!", "Intimidated? Don\u2019t be. You\u2019ll have a clear image of what\u2019s going on here in no time.", "Just as with our hypothesis, we have a function handle: J(\u03b8). It serves to represent the function, as well as its outputted value. So,when you see J(\u03b8), recall that you are dealing with the cost function (sometimes called the loss function), or the output thereby associated.", "In machine learning, the letter m is the number of training examples. If we have data about 5,000 houses, m = 5,000.", "What about our newest Greek friend sigma (\u03a3)? This guy denotes a summation term, which loops over an iterator and sums a specified term. The small equation i = 1 underneath of \u03a3 shows what the initial value for the iterator is, as well as a variable (i). (i) denotes the current iteration in the summation loop. Above \u03a3, is m, the terminal value.", "That\u2019s a very abstract definition. In plain in English, the variable i takes on every value between 1 and m. For each stage of this cycle, we add the value of whatever is to the right of sigma to a running sum.", "So, what is that expression to the right of \u03a3?!", "(h\u03b8(x(i)) - y(i))\u00b2 is called the squared error term. Its contents should look familiar. For every iteration, the squared error term uses the hypothesis to predict an output for using example x(i), and subtracts the desired output for that example y(i) from it. This is the difference, or error between x(i) and y(i). This value is then raised to the power of two. Hence squared error.", "Seeing as it is in a summation, where (i) takes on all values between 1 and the number of training examples, we complete this computation for the entire data set. All of the squared errors are added together, one after another, as the algorithm progresses. Once the summation is complete the sum of all error for the hypothesis has been calculated.", "Finally, we multiply our total error by 1/2m, receiving the output of the cost function. In dividing by 2m, the result is half of the average error of the model. Dividing by 2m instead of m should be thought of as another machine learning convention. There is mathematical reason behind it, but that explanation is beyond the scope of this article.", "Given any coordinate on the cost function, the following information can be extracted: All the values of the independent variables (\u03b8 values), as well as the cost for those parameters. The cost is the dependent variable.", "And that sums up what there is to know about the cost function. Let\u2019s have a look at a couple of visualizations of it.", "Take note of how the cost function is parabolic due to it being a quadratic. It is also important to see that the cost, shown on the y-axis, changes with regard to the theta values.", "Another variation of the loss function is pictured above. It is measuring a hypothesis containing only one parameter and is therefore a two-dimensional visualization.", "For every theta value, a spatial dimension is added to the function. For that reason, we cannot graph any loss representations which deal with data containing more than 2 features (the third dimension being for the cost, on the y-axis). It may be helpful for you visual learners out there to keep this in mind.", "Cost functions frequently assume fancy shapes and odd forms. Aside from being mathematically and visually interesting, the loss function also plays a crucial role in the actual learning process for machines. The following sections will outline this critical characteristic.", "Derivatives are one of the fundamentals of calculus. I understand that calculus incites fear in many people. Please don\u2019t run away.", "Just like everything we\u2019ve already covered, I\u2019ll be explaining this concept in a simple and organized fashion.", "Here\u2019s what a derivative is one sentence: an expression representing the relationship between a function and the slope of that function at any point, given the independent variable.", "The derivative of x\u00b2 is 2x. This means that at a value of x on the function x\u00b2, the slope at that point is 2 * x.", "Knowing that, we can understand that derivatives represent instantaneous rate of change, as they reveal what the slope is at a single point on a function, rather than between two separate points.", "When we evaluate a derivative, by plugging the independent variable into the derivative equation, its output is the slope at that point on the function. For this reason, a derivative can be drawn as a tangent line to a graph.", "Three things to note about derivatives:", "Keep this in mind, as it is important for the next section.", "The gradient of a function, just like a derivative, represents the slope at any given point on the function. Gradients, however, can be applied to multivariate functions.", "A gradient is, in essence, a series of derivatives, each representing the slope in one dimension of the graph. These derivatives, each expressing the rate of change relative to one variable of a multivariate function, are called partial derivatives. When partial derivatives are combined to form a directional unit (a vector), they are then known as a gradient.", "A gradient, in the format of a vector, may be presented in the following format: [3; -4; 5] This would be gradient for a three dimensional graph. The information presented here is as follows:", "Above is a partial derivative on a 3-dimensional function. The pictured graph demonstrates what the partial derivative represents: the rate of change in one dimension of the function.", "And that\u2019s really all you need to know about gradients and derivatives to understand how machines learn!", "The extraction of derivatives and gradients from an equation can be rather intensive and usually requires the entirety of a text book to explain. Therefore, you will unfortunately not be getting an in depth lesson of derivations in this article. I just ask that you take me at my word when I conjure a derivative.", "Furthermore, don\u2019t fret if you didn\u2019t manage to absorb everything in this last section. The most important thing to take away is that a gradient represents the slope on a function. A partial derivative tells us the slope with respect to one variable (or dimension) of said function.", "I feel very strongly that you have already conquered the hardest part of machine learning. That is, wrapping your head around the foreign notation, and learning how hypotheses are formed and evaluated. If you understood the fundamental concept for each of the previous topics, making sense of gradient descent should come with relative ease.", "That being said, let\u2019s jump into it.", "The objective of gradient descent is to minimize the cost function. This means that we want to determine the lowest cost for our model, as doing so would find the hypothesis with the smallest margin of error. This value would be represented as the lowest point on the function, seeing as the graph\u2019s height depicts the loss.", "If this were to be represented in relation to the hypothesis, we would want a model which minimizes the sum of vertical differences between all data points and the function itself, seeing as error is calculated based on predicted output vs actual output, and is measured on the y-axis.", "The analogy always used for explaining gradient descent is as follows:", "You are standing at the top of a hill, surrounding by valleys. You want to get to the lowest elevation in close proximity to you by taking the most efficient route. You look all around and take a step in the steepest direction downhill. You repeat this process until you are at the bottom of a valley.", "The hills and valleys symbolize the cost function. The steps that are taken downhill represent the process of gradient descent.", "In the algorithm, said steps are made by incrementally altering theta values, which are initialized randomly, to better represent the training data. Once the model has been fit to the data, it should be able to generalize to examples it has not yet seen and provide reasonably accurate predictions on new information.", "The following is the formula for gradient descent. Once again, don\u2019t feel overwhelmed. I\u2019ll be covering its inner workings in detail.", "The algorithm says: for every value of j between 0 and n (the number of parameters), set \u03b8(j) equal to \u03b8(j) minus the partial derivative of the cost function with respect to \u03b8(j) at the current coordinate, multiplied by a value alpha. Repeat until the cost is minimized.", "That\u2019s probably an overwhelming amount of information packed into one sentence. I get that. Keep on reading and I\u2019ll clarify.", "As we know, derivatives point in the direction of steepest change in a function, and the value of a derivative is the slope of the line at that evaluated coordinate. By taking the negative of a derivative, we receive the direction opposite to that of steepest change. This direction is the one by which we adjust the theta values. The reason for this will soon become apparent.", "For every one of the parameters theta, we adjust theta by a scaled value of the negative partial derivative of said parameter. I say scaled, as we multiply the derivative by a variable alpha before performing the subtraction. I will explain the importance of alpha soon. This changes theta such that it gets closer to the value of itself which allows the cost function to have a minimum value. A single change to theta is called a step.", "In essence, if theta is too large, it will fall to the right side of the minimum. The partial derivative will be positive. When we subtract the derivative value from the parameter, it causes theta to become smaller, moving closer to the desired minimum.", "Same goes for when theta is too small. When a parameter needs to be larger, it lies to the left of the minimum, which makes for a negative derivative. Subtracting a negative induces addition, which is exactly what is needed in this scenario.", "The first value of theta is denoted by the purple dot on the cost function. Theta begins as being overly large. As you can see, this value causes the loss to be very high. The slope at that point is rather steep and has a large positive value. This derivative is multiplied by some small integer alpha and is subtracted from theta. In subtracting, both the magnitude and direction of the partial derivative play a crucial role. The slope being positive, \u03b8(1) becomes smaller. The slope being steep, or large, \u03b8(1) is changed dramatically, which is appropriate seeing as theta is very far from its target value.", "As this process is repeated and \u03b8(1) approaches the loss minimum, the slope of the function decreases. The steps then become smaller, as the size of the increments are based on the slope. Eventually, the changes become infinitesimal and simply play the role of fine-tuning. This handy detail is what allows gradient descent to avoid overshooting the minimum and provide very accurate and suiting values for the parameters.", "Let\u2019s have a look on how this process would affect a value of theta which is too small.", "Above, we can see that the value of the derivative is less than zero at \u03b8(1) = -1, as the slope of the tangent line is negative at that point. In this scenario we would repeatedly subtract this negative value from \u03b8(1), slowly increasing its value until it achieves minimization of the function.", "Another interesting characteristic to observe about this algorithm is that at the minimum of any function, the slope is 0. This is important because it shows us that upon convergence, theta values will no longer be changed when steps are taken. (\u03b8(1) - 0 = \u03b8(1)).", "On multivariate cost functions, we update all of our theta values at once, and thus our incremental steps towards a minimum resemble gradients, like this:", "Above, we see the cost function J(\u03b8). The black line shows the movements taken by gradient descent as the values of theta are tweaked. Remember that this process is identical to that of two-dimensional functions. It is just carried out for both \u03b8(0) and \u03b8(1) simultaneously, which is what causes the descent to appear in the form of a gradient.", "Now let\u2019s review how this process is denoted mathematically. Here is, once again, the algorithm for reference:", "We already know what \u03b8(j) represents.", "The := symbol says that we are setting a variable equal to a value specified on the other side of the operator. x := 3 means that we set x equal to 3.", "Now how about this alpha you\u2019ve been hearing about? \u221d is alpha. \u221d is what is called a hyper parameter. This means that it is not one of the values that is used by the model to make predictions, but it is a variable which has an impact on our final model.", "Alpha modifies the learning rate, or size of increments in gradient descent. If alpha takes on a large value, the gradient descent algorithm can converge (arrive at a minimum) faster. If it is small, gradient descent needs more iterations to produce an accurate model.", "Well, no.  If \u221d is too large, gradient descent may overshoot the desired minimum. If this is the case, the algorithm will diverge, meaning the cost gets progressively higher. On the other hand, if alpha is too small, thousands or even millions of increments may be needed to achieve an accurate hypothesis.", "There is very much a sweet spot for \u221d. Finding it can be very involved, however, and model analysis requires an article unto itself. For now, you should imagine that the best way to find alpha is through calculated trials. Common values lie between 0.001 and 10.", "The final part of the gradient descent equation is the derivative term of the cost function with respect to \u03b8(j). This is the part of the formula to the right of the summation term.", "Here\u2019s how the derivative with respect to some parameter \u03b8(j) is calculated:", "For a parameter \u03b8(j): Looping through every training example using (i) as an iterator, the expected output y(i) is subtracted from the hypothesis output x(i) for that example. This difference is multiplied by the feature x(i)j (the feature associated with \u03b8(j)). This product is added to a running sum.", "Note: The bias unit does not have any related feature, meaning that we do not multiply the error (x(i) \u2014 y(i)) by a value of x(i)j when calculating the partial derivative derivative.", "Upon completion of the summation, we divide the resulting value by the total number of training examples. This provides us with the partial derivative with respect to \u03b8(j).", "We repeat the derivative calculation for every parameter \u03b8 before updating all parameters simultaneously.", "Now, read this explanation of the gradient descent once more:", "The algorithm says: for every value j between 0 and n (the number of parameters), set \u03b8(j) equal to \u03b8(j) minus the partial derivative of the cost function with respect to \u03b8(j) at the current coordinate, multiplied by the value alpha. Repeat until the cost is minimized.", "I\u2019m sure that\u2019s more understandable at this point.", "Congratulations! That\u2019s gradient descent \u2014 the most common machine learning algorithm. And you now have a thorough understanding of it.", "We have a data set X, containing training examples x(i), and labels y(i). Each example has features x(i)1 through x(i)j, j being the total number of features. These features each provide unique information.", "The hypothesis, h\u03b8(x), makes predictions on the data. It contains a theta value associated with each feature, as well as a bias unit, \u03b80, for a total of j + 1 parameters. This model creates a predictive output by summing the product of each feature of a training example with its respective parameter and adding the bias unit to this total.", "The cost function measures how accurate the hypothesis is. By lowering the cost, we obtain a more precise model.", "Gradient descent can achieve this minimization by changing theta values based on their partial derivatives relative to the cost function. By taking the negative slope at the current coordinate on the loss function, each parameter is moved incrementally closer to a minimum on the graph. This is repeated until convergence is achieved.", "The model is then able to take new data as input and produce accurate output.", "That\u2019s linear regression. And you understand it! Amazing!", "Seeing as we are surrounded by artificial intelligence on the daily, it is astounding how few people understand what it is doing. This technology is evolving extremely rapidly, and ignorance is not bliss this time around. Making sense of the world around us is human nature. It\u2019s important to see what kind of advancements are soon to become mainstream.", "You now have some insight on what the professional field of machine learning looks like. Let\u2019s take a final look at what we perceive A.I. to be:", "Artificial Intelligence (A.I.): Predictive models. Squared error cost functions. The gradient descent algorithm.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "A curious, young individual, eager to share interesting information and ideas."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F6338d9559748&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-involved-introduction-to-supervised-learning-for-the-common-human-6338d9559748&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-involved-introduction-to-supervised-learning-for-the-common-human-6338d9559748&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-involved-introduction-to-supervised-learning-for-the-common-human-6338d9559748&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-involved-introduction-to-supervised-learning-for-the-common-human-6338d9559748&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----6338d9559748--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----6338d9559748--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@farisnolan?source=post_page-----6338d9559748--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@farisnolan?source=post_page-----6338d9559748--------------------------------", "anchor_text": "Faris Nolan"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd7fe3a41bf69&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-involved-introduction-to-supervised-learning-for-the-common-human-6338d9559748&user=Faris+Nolan&userId=d7fe3a41bf69&source=post_page-d7fe3a41bf69----6338d9559748---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6338d9559748&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-involved-introduction-to-supervised-learning-for-the-common-human-6338d9559748&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6338d9559748&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-involved-introduction-to-supervised-learning-for-the-common-human-6338d9559748&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----6338d9559748---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----6338d9559748---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/linear-regression?source=post_page-----6338d9559748---------------linear_regression-----------------", "anchor_text": "Linear Regression"}, {"url": "https://medium.com/tag/supervised-learning?source=post_page-----6338d9559748---------------supervised_learning-----------------", "anchor_text": "Supervised Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6338d9559748&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-involved-introduction-to-supervised-learning-for-the-common-human-6338d9559748&user=Faris+Nolan&userId=d7fe3a41bf69&source=-----6338d9559748---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6338d9559748&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-involved-introduction-to-supervised-learning-for-the-common-human-6338d9559748&user=Faris+Nolan&userId=d7fe3a41bf69&source=-----6338d9559748---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6338d9559748&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-involved-introduction-to-supervised-learning-for-the-common-human-6338d9559748&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----6338d9559748--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F6338d9559748&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-involved-introduction-to-supervised-learning-for-the-common-human-6338d9559748&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----6338d9559748---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----6338d9559748--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----6338d9559748--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----6338d9559748--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----6338d9559748--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----6338d9559748--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----6338d9559748--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----6338d9559748--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----6338d9559748--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@farisnolan?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@farisnolan?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Faris Nolan"}, {"url": "https://medium.com/@farisnolan/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "42 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd7fe3a41bf69&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-involved-introduction-to-supervised-learning-for-the-common-human-6338d9559748&user=Faris+Nolan&userId=d7fe3a41bf69&source=post_page-d7fe3a41bf69--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Fd7fe3a41bf69%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-involved-introduction-to-supervised-learning-for-the-common-human-6338d9559748&user=Faris+Nolan&userId=d7fe3a41bf69&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}