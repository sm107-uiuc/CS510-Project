{"url": "https://towardsdatascience.com/understanding-logistic-regression-81779525d5c6", "time": 1683015404.512286, "path": "towardsdatascience.com/understanding-logistic-regression-81779525d5c6/", "webpage": {"metadata": {"title": "Understanding Logistic Regression | by Dorian Lazar | Towards Data Science", "h1": "Understanding Logistic Regression", "description": "What is logistic regression? Logistic regression is just adapting linear regression to a special case where you can have only 2 outputs: 0 or 1. And this thing is most commonly applied to\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.nablasquared.com/understanding-logistic-regression/", "anchor_text": "here", "paragraph_index": 36}, {"url": "https://www.nablasquared.com/", "anchor_text": "https://www.nablasquared.com/", "paragraph_index": 38}], "all_paragraphs": ["What is logistic regression? Logistic regression is just adapting linear regression to a special case where you can have only 2 outputs: 0 or 1. And this thing is most commonly applied to classification problems where 0 and 1 represent two different classes and we want to distinguish between them.", "Linear regression outputs a real number that ranges from -\u221e to +\u221e. And we can use just this even in the 0/1 classification problem: if we get a value >= 0.5 report it as class label 1, if the output is < 0.5 report it as a 0.", "Where x is a vector of features (plus a component with constant 1 for bias) of one observation, and w is the weights vector.", "But, we can get slightly better results both in terms of accuracy and interpretability if we squash the regression line into an \u201cS\u201d-shaped curve between 0 and 1. We squash the regression line by applying the sigmoid function to the output value of a linear regression model.", "More exactly, we compute the output as follows: take the weighted sum of the inputs, and then pass this resulting number into the sigmoid function and report the sigmoid\u2019s output as the output of our logistic regression model.", "This procedure helps us both in getting slightly better accuracy and in the interpretability of the output. If our model outputs any real number like -5 or 7, what these numbers actually mean? What can we tell about our two classes: 0 and 1?", "But, when we have outputs between 0 and 1, we can interpret them as probabilities. The output of a logistic regression model is the probability of our input belonging to the class labeled with 1. And the complement of our model\u2019s output is the probability of our input belonging to the class labeled with 0.", "Where y is the true class label of the input x.", "OK. So, by now we have seen how a logistic regression model obtains its outputs, given the input. But what about its weights? What weights should it have to make good predictions?", "Our model needs to learn those weights, and the way it learns is by giving to our model an objective function, then it finds the weights that minimize or maximize this objective.", "There are many ways we can come up with an objective function, especially if we consider adding regularization terms to our objective.", "In this article, we\u2019ll explore only 2 such objective functions.", "First, let\u2019s write our logistic regression model as follows:", "Where X is a matrix that contains all our observations as rows, and columns represent the features. y hat is the output of our model, it is a vector that contains the predictions for each observation.", "Let\u2019s rewrite our logistic regression equation in the following way:", "The operations on the right-hand side of the last line are element-wise.", "What do you observe in the last line above? If we apply the function on the right-hand side of the last equation on the labels for logistic regression and consider the output of this function application as the new labels, then we obtain a linear regression. So, we can use the sum of squared errors as a loss function and find the weights that minimize it. We can find the weights by using either a closed-form formula or SGD (stochastic gradient descent) as you can read more about in the following article on linear regression:", "Below are the closed-form solution and the gradient of the loss (that we can use in the SGD algorithm) for linear regression:", "For logistic regression we just need to replace the y in these 2 equations above with the right-hand side of the previous equation:", "When we apply these formulas, we provide the true labels for y hat.", "So, we can treat logistic regression as a form of linear regression and use the tools of linear regression to solve logistic regression. OK. What can we do besides that?", "We can take advantage of the properties of logistic regression to come up with a slightly better method. What type of output does logistic regression have? A probability.", "A convenient method to apply when probabilities are involved is the Maximum Likelihood Estimation. We will find the weights of our model that maximizes the likelihood of the labels given the inputs.", "We start by writing the likelihood function. The likelihood is just the joint probability of labels given the inputs, which, if we assume observations to be independent, can be written as the product of the probabilities for each observation.", "Where m is the number of observations.", "The likelihood is a function of everything: inputs x, true labels y, and weights w. But for our purposes here (maximizing it with respect to w) we will consider it further as a function of just w. x and y we consider as given constants that we cannot change.", "Each one of the individual probabilities has one of the following values, depending on yi being 0 or 1:", "A more compact way of writing this is:", "Now, we replace this quantity in the likelihood function, simplify the argmax of it, and transition to matrix notation:", "As you can see above, maximizing the likelihood w.r.t. the weights is the same thing as minimizing the quantity on the last line. Finding a closed-form solution this time is more difficult (if even possible), so the best thing we can do is to compute the gradient of this quantity:", "Where: the operations involved in that fraction above are element-wise. The dot before X means \u201cmultiply element-wise the column vector on the left with each column of the matrix X\u201d. The 1s above are column vectors with the same shape as y filled with values of 1.", "Now, the above gradient can be used with a gradient-based optimization algorithm (like SGD) to find the optimal weights.", "Before we\u2019re done, let\u2019s recap a few things that we saw through this article:", "And that\u2019s it for this article. I hope you found it useful.", "In the next couple of articles, I will show how to implement logistic regression in NumPy, TensorFlow, and PyTorch.", "I hope you found this information useful and thanks for reading!", "This article is also posted on my own website here. Feel free to have a look!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Passionate about Data Science, AI, Programming & Math | Owner of \u2207\u00b2 https://www.nablasquared.com/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F81779525d5c6&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-logistic-regression-81779525d5c6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-logistic-regression-81779525d5c6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-logistic-regression-81779525d5c6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-logistic-regression-81779525d5c6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----81779525d5c6--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----81779525d5c6--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://dorianlazar.medium.com/?source=post_page-----81779525d5c6--------------------------------", "anchor_text": ""}, {"url": "https://dorianlazar.medium.com/?source=post_page-----81779525d5c6--------------------------------", "anchor_text": "Dorian Lazar"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F79574042e17b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-logistic-regression-81779525d5c6&user=Dorian+Lazar&userId=79574042e17b&source=post_page-79574042e17b----81779525d5c6---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F81779525d5c6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-logistic-regression-81779525d5c6&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F81779525d5c6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-logistic-regression-81779525d5c6&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/understanding-linear-regression-eaaaed2d983e", "anchor_text": "Understanding Linear RegressionThe math behind Linear Regression explained in detailtowardsdatascience.com"}, {"url": "https://towardsdatascience.com/how-to-code-logistic-regression-from-scratch-with-numpy-d33c46d08b7f", "anchor_text": "How to code Logistic Regression from scratch with NumPySharpen your NumPy skills while learning Logistic Regressiontowardsdatascience.com"}, {"url": "https://medium.com/nabla-squared/how-to-implement-logistic-regression-with-tensorflow-f5bf18416da1", "anchor_text": "How to Implement Logistic Regression with TensorFlow\u2026something not as hard as you may thinkmedium.com"}, {"url": "https://medium.com/nabla-squared/how-to-implement-logistic-regression-with-pytorch-fe60ea3d7ad", "anchor_text": "How to Implement Logistic Regression with PyTorchUnderstand Logistic Regression and sharpen your PyTorch skillsmedium.com"}, {"url": "https://www.nablasquared.com/understanding-logistic-regression/", "anchor_text": "here"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----81779525d5c6---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----81779525d5c6---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/data-science?source=post_page-----81779525d5c6---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/mathematics?source=post_page-----81779525d5c6---------------mathematics-----------------", "anchor_text": "Mathematics"}, {"url": "https://medium.com/tag/logistic-regression?source=post_page-----81779525d5c6---------------logistic_regression-----------------", "anchor_text": "Logistic Regression"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F81779525d5c6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-logistic-regression-81779525d5c6&user=Dorian+Lazar&userId=79574042e17b&source=-----81779525d5c6---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F81779525d5c6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-logistic-regression-81779525d5c6&user=Dorian+Lazar&userId=79574042e17b&source=-----81779525d5c6---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F81779525d5c6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-logistic-regression-81779525d5c6&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----81779525d5c6--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F81779525d5c6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-logistic-regression-81779525d5c6&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----81779525d5c6---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----81779525d5c6--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----81779525d5c6--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----81779525d5c6--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----81779525d5c6--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----81779525d5c6--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----81779525d5c6--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----81779525d5c6--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----81779525d5c6--------------------------------", "anchor_text": ""}, {"url": "https://dorianlazar.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://dorianlazar.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Dorian Lazar"}, {"url": "https://dorianlazar.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "593 Followers"}, {"url": "https://www.nablasquared.com/", "anchor_text": "https://www.nablasquared.com/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F79574042e17b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-logistic-regression-81779525d5c6&user=Dorian+Lazar&userId=79574042e17b&source=post_page-79574042e17b--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F9fdaa579c06&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-logistic-regression-81779525d5c6&newsletterV3=79574042e17b&newsletterV3Id=9fdaa579c06&user=Dorian+Lazar&userId=79574042e17b&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}