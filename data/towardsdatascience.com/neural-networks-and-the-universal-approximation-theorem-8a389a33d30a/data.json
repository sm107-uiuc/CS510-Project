{"url": "https://towardsdatascience.com/neural-networks-and-the-universal-approximation-theorem-8a389a33d30a", "time": 1683008695.778045, "path": "towardsdatascience.com/neural-networks-and-the-universal-approximation-theorem-8a389a33d30a/", "webpage": {"metadata": {"title": "Neural Networks and the Universal Approximation Theorem | by Milind Sahay | Towards Data Science", "h1": "Neural Networks and the Universal Approximation Theorem", "description": "The concept of Neural Networks has been around us for a few decades. Why did it take so much time to pace up? What is this sudden boom that Neural Networks and Deep Learning created? What makes\u2026"}, "outgoing_paragraph_urls": [{"url": "http://neuralnetworksanddeeplearning.com/chap2.html", "anchor_text": "backpropagation algorithm", "paragraph_index": 2}, {"url": "https://www.statisticssolutions.com/what-is-linear-regression/", "anchor_text": "linear regression", "paragraph_index": 5}, {"url": "https://missinglink.ai/guides/neural-network-concepts/7-types-neural-network-activation-functions-right/", "anchor_text": "activation function", "paragraph_index": 6}, {"url": "https://openreview.net/pdf?id=ryxB0Rtxx", "anchor_text": "identity function", "paragraph_index": 6}, {"url": "https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/", "anchor_text": "ReLu", "paragraph_index": 6}, {"url": "https://en.wikipedia.org/wiki/Sigmoid_function", "anchor_text": "sigmoid", "paragraph_index": 6}, {"url": "https://en.wikipedia.org/wiki/Softmax_function", "anchor_text": "softmax", "paragraph_index": 6}, {"url": "https://arxiv.org/pdf/2004.08867.pdf", "anchor_text": "here", "paragraph_index": 6}], "all_paragraphs": ["The concept of Neural Networks has been around us for a few decades. Why did it take so much time to pace up? What is this sudden boom that Neural Networks and Deep Learning created? What makes Neural Networks this hype-worthy? Let\u2019s explore.", "To get a brief overview of what Neural Networks are, a neural network is simply a collection of Neurons(also known as activations), that are connected through various layers. It attempts to learn the mapping of input data to output data, on being provided a training set.", "The training of the neural network later facilitates the predictions made by it on a testing data of the same distribution. This mapping is attained by a set of trainable parameters called weights, distributed over different layers. The weights are learned by the backpropagation algorithm whose aim is to minimize a loss function. A loss function measures how distant the predictions made by the network are from the actual values. Every layer in a neural network is followed by an activation layer that performs some additional operations on the neurons.", "Mathematically speaking, any neural network architecture aims at finding any mathematical function y= f(x) that can map attributes(x) to output(y). The accuracy of this function i.e. mapping differs depending on the distribution of the dataset and the architecture of the network employed. The function f(x) can be arbitrarily complex. The Universal Approximation Theorem tells us that Neural Networks has a kind of universality i.e. no matter what f(x) is, there is a network that can approximately approach the result and do the job! This result holds for any number of inputs and outputs.", "If we observe the neural network above, considering the input attributes provided as height and width, our job is to predict the gender of the person. If we exclude all the activation layers from the above network, we realize that h\u2081 is a linear function of both weight and height with parameters w\u2081, w\u2082, and the bias term b\u2081. Therefore mathematically,", "Going along these lines we realize that o1 is also a linear function of h\u2081 and h2, and therefore depends linearly on input attributes weight and height as well. This essentially boils down to a linear regression model. Does a linear function suffice at approaching the Universal Approximation Theorem? The answer is NO. This is where activation layers come into play.", "An activation layer is applied right after a linear layer in the Neural Network to provide non-linearities. Non-linearities help Neural Networks perform more complex tasks. An activation layer operates on activations (h\u2081, h2 in this case) and modifies them according to the activation function provided for that particular activation layer. Activation functions are generally non-linear except for the identity function. Some commonly used activation functions are ReLu, sigmoid, softmax, etc. With the introduction of non-linearities along with linear terms, it becomes possible for a neural network to model any given function approximately on having appropriate parameters(w\u2081, w\u2082, b\u2081, etc in this case). The parameters converge to appropriateness on training suitably. You can get better acquainted mathematically with the Universal Approximation theorem from here.", "Neural networks have the capability to map complex functions and have been a theory on paper since forever. What made it a prodigy in Machine learning suddenly?", "A recent explosion for interest in deep learning models is credited to the high computational resources and the enriching data that the world has to offer nowadays. Deep neural networks are data-hungry models. This boom is also majorly credited to the inexpensive high-speed computing that has arrived in the hands of the common folks. This unprecedented increase in data along with computational power has created wonders in almost all domains of life.", "Deep learning models are firmly believed to extract features from raw data automatically, a concept also known as feature learning. No matter what you feed into a sufficiently large and deep Neural Network, it can learn hidden features and relations between attributes and later leverages those same relations to predict results. This comes real handy and requires minimal preprocessing of data. Along with this, the tools and frameworks(PyTorch, Tensorflow, Theano) used to design and build these data-driven models are increasing by the day, are pretty high level, and are easily available. They require an inconsiderable low-level understanding of programming languages. On top of it, research by top companies has proved that this domain is indeed worth spending valuable time and money on.", "It is highly acclaimed that deep learning models are indeed scalable with data. This indicates that the results almost always get better with the increase in data and by employing larger models. These larger models require more computation to train. With the advent of conducive computational environments that are easily accessible nowadays, it has, therefore, become easier to experiment and improve the algorithms and architectures in real-time, thus giving rise to better and better practices in short spans. That being said, Deep Neural Networks have wide found applications in many domains like Computer Vision, Natural Language Processing, Recommender Systems, and much more. Various cross-domain applications have also picked up pace recently.", "In October 2019, Google published the results of its Quantum Supremacy experiment on \u201cSycamore,\u201d its 54-qubit processor. The quantum computer conducted the target calculation in 200 seconds, which would take about 10,000 years for the world\u2019s fastest supercomputers. With this increase in computational power by each passing day, there\u2019s no knowing when Machine Learning will transcend superhuman boundaries. One can only speculate.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Undergraduate student at Delhi Technological University | TensorFlow | Deep learning | AI"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F8a389a33d30a&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-and-the-universal-approximation-theorem-8a389a33d30a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-and-the-universal-approximation-theorem-8a389a33d30a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-and-the-universal-approximation-theorem-8a389a33d30a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-and-the-universal-approximation-theorem-8a389a33d30a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----8a389a33d30a--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8a389a33d30a--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@milind_sahay?source=post_page-----8a389a33d30a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@milind_sahay?source=post_page-----8a389a33d30a--------------------------------", "anchor_text": "Milind Sahay"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff83b59f1b544&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-and-the-universal-approximation-theorem-8a389a33d30a&user=Milind+Sahay&userId=f83b59f1b544&source=post_page-f83b59f1b544----8a389a33d30a---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8a389a33d30a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-and-the-universal-approximation-theorem-8a389a33d30a&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8a389a33d30a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-and-the-universal-approximation-theorem-8a389a33d30a&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@frantic?utm_source=medium&utm_medium=referral", "anchor_text": "Alex Kotliarskyi"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://www.kdnuggets.com/2017/10/neural-network-foundations-explained-gradient-descent.html", "anchor_text": "The architecture of a Neural Network"}, {"url": "http://neuralnetworksanddeeplearning.com/chap2.html", "anchor_text": "backpropagation algorithm"}, {"url": "https://victorzhou.com/blog/intro-to-neural-networks/", "anchor_text": "image source"}, {"url": "https://www.statisticssolutions.com/what-is-linear-regression/", "anchor_text": "linear regression"}, {"url": "https://missinglink.ai/guides/neural-network-concepts/7-types-neural-network-activation-functions-right/", "anchor_text": "activation function"}, {"url": "https://openreview.net/pdf?id=ryxB0Rtxx", "anchor_text": "identity function"}, {"url": "https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/", "anchor_text": "ReLu"}, {"url": "https://en.wikipedia.org/wiki/Sigmoid_function", "anchor_text": "sigmoid"}, {"url": "https://en.wikipedia.org/wiki/Softmax_function", "anchor_text": "softmax"}, {"url": "https://arxiv.org/pdf/2004.08867.pdf", "anchor_text": "here"}, {"url": "https://machinelearningmastery.com/what-is-deep-learning/", "anchor_text": "models scaled with data"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----8a389a33d30a---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/tech?source=post_page-----8a389a33d30a---------------tech-----------------", "anchor_text": "Tech"}, {"url": "https://medium.com/tag/ai?source=post_page-----8a389a33d30a---------------ai-----------------", "anchor_text": "AI"}, {"url": "https://medium.com/tag/data-science?source=post_page-----8a389a33d30a---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----8a389a33d30a---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8a389a33d30a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-and-the-universal-approximation-theorem-8a389a33d30a&user=Milind+Sahay&userId=f83b59f1b544&source=-----8a389a33d30a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8a389a33d30a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-and-the-universal-approximation-theorem-8a389a33d30a&user=Milind+Sahay&userId=f83b59f1b544&source=-----8a389a33d30a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8a389a33d30a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-and-the-universal-approximation-theorem-8a389a33d30a&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8a389a33d30a--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F8a389a33d30a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-and-the-universal-approximation-theorem-8a389a33d30a&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----8a389a33d30a---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----8a389a33d30a--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----8a389a33d30a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----8a389a33d30a--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----8a389a33d30a--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----8a389a33d30a--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----8a389a33d30a--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----8a389a33d30a--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----8a389a33d30a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@milind_sahay?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@milind_sahay?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Milind Sahay"}, {"url": "https://medium.com/@milind_sahay/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "26 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff83b59f1b544&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-and-the-universal-approximation-theorem-8a389a33d30a&user=Milind+Sahay&userId=f83b59f1b544&source=post_page-f83b59f1b544--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F5eb8188ccfe3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-and-the-universal-approximation-theorem-8a389a33d30a&newsletterV3=f83b59f1b544&newsletterV3Id=5eb8188ccfe3&user=Milind+Sahay&userId=f83b59f1b544&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}