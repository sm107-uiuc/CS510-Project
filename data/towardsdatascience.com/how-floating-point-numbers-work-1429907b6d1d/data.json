{"url": "https://towardsdatascience.com/how-floating-point-numbers-work-1429907b6d1d", "time": 1683010311.939466, "path": "towardsdatascience.com/how-floating-point-numbers-work-1429907b6d1d/", "webpage": {"metadata": {"title": "How Floating Point Numbers Work. With Applications to Deep Learning and\u2026 | by Ravi Charan | Towards Data Science", "h1": "How Floating Point Numbers Work", "description": "We'll dig into the nuts and bolts of floating point numbers, infinities, NaNs, subnormals, and applications to Google's TPU bfloat16 format and HDR imaging."}, "outgoing_paragraph_urls": [{"url": "https://www.joelonsoftware.com/2002/11/11/the-law-of-leaky-abstractions/", "anchor_text": "leaky abstraction", "paragraph_index": 1}, {"url": "https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus", "anchor_text": "modified", "paragraph_index": 1}, {"url": "https://en.wikipedia.org/wiki/Binary_number", "anchor_text": "binary", "paragraph_index": 2}, {"url": "https://en.wikipedia.org/wiki/Two%27s_complement", "anchor_text": "Two\u2019s Complement", "paragraph_index": 5}, {"url": "https://en.wikipedia.org/wiki/Ones%27_complement", "anchor_text": "One\u2019s Complement", "paragraph_index": 5}, {"url": "https://en.wikipedia.org/wiki/Double-precision_floating-point_format#IEEE_754_double-precision_binary_floating-point_format:_binary64", "anchor_text": "IEEE 754", "paragraph_index": 13}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "transformer architecture", "paragraph_index": 29}, {"url": "https://en.wikipedia.org/wiki/Softmax_function", "anchor_text": "softmax", "paragraph_index": 29}, {"url": "https://en.wikipedia.org/wiki/Atan2", "anchor_text": "atan2", "paragraph_index": 33}, {"url": "https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus", "anchor_text": "8 times smaller", "paragraph_index": 41}, {"url": "https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus", "anchor_text": "blog post", "paragraph_index": 43}, {"url": "https://en.wikipedia.org/wiki/Gamma_correction", "anchor_text": "gamma encoding", "paragraph_index": 45}, {"url": "https://en.wikipedia.org/wiki/RGBE_image_format", "anchor_text": "Radiance HDR", "paragraph_index": 47}, {"url": "https://en.wikipedia.org/wiki/LogSumExp", "anchor_text": "log-sum-exp", "paragraph_index": 48}], "all_paragraphs": ["It is a pesky fact that computers work in binary approximations while humans tend to think in terms of exact values. This is why, in your high school physics class, you may have experienced \u201crounding error\u201d when computing intermediate numerical values in your solutions and why, if you open a python terminal and compute 0.1 * 3, you will get a weird result.\u00b9", "this makes floating point numbers an example of a leaky abstraction. Normally, python and numerical computing libraries like numpy or PyTorch handle this behind the scenes. But understanding the details can help you avoid otherwise unexpected errors and speed up many machine learning computations. For example, Google\u2019s Tensor Processing Units (TPUs) use a modified floating point format to substantially improve computational efficiency while trying to maintain good results.", "In this article we\u2019ll dig into the nuts and bolts of floating point numbers, cover the edge cases (numerical underflow and overflow), and close with applications: TPU\u2019s bfloat16 format and HDR imaging. The main background assumed is that you understand how to count in binary, as well as how binary fractions work.", "Note also that we would typically abbreviate this with a hexadecimal (base 16) representation: 0x00 to 0xFF. The 0x prefix means \u201cthis is a hex number\u201d. The hexadecimal digits are 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, A, B, C, D, E, F; so F is essentially short for the four bits \u201c1111\u201d (both 0xF and 1111 are 15 in base-10). Also 8 bits are a byte, so our number is a measly 1 byte. But we won\u2019t focus too much on hexadecimal in this article.", "Now, you will notice that with an unsigned int, we can\u2019t represent simple numbers like -2. One way you could try to solve this is to make the first bit represent the sign. Say \u201c0\u201d means negative and \u201c1\u201d means positive. Thinking about 4-bit numbers, 0111 would be -7, while 1111 would be +7. However, this has some weird features. For example, 0000 is \u201c-0\u201d while 1000 is \u201c+0\u201d. This is not great: comparing two numbers for equality would get tricky; plus we are wasting space.", "The standard solution to this is to use Two\u2019s Complement, which is what most implementations use for signed integers. (There is also a little-used One\u2019s Complement). However, this isn\u2019t what we are going to need for floating point numbers, so we won\u2019t delve into it.", "Since most recently produced personal computers use a 64 bit processor, it\u2019s pretty common for the default floating-point implementation to be 64 bit. This is called \u201cdouble precision\u201d because it is double of the previous-standard 32-bit precision (common computers switched to 64 bit processors sometime in the last decade).", "For context, the basic idea of a floating point number is to use the binary-equivalent of scientific notation. Your high-school science teachers hopefully drilled into you exactly how to do this (along with a whole bunch about those dreaded signficant figures \u2013 sigfigs). For example, the scientific representation of 8191.31 is:", "You should notice three key elements. First, a sign (is the number + or -?). Second, we always write the number with a single digit (between 1 and 9 inclusive), followed by a decimal point, followed by a number of digits. Compare that to the below, which are not in scientific notation even though they are true mathematical facts.", "With that in mind, let\u2019s think about what will change when we go to binary. First of all, instead of using 10 as the base of the exponent (also called the radix), we\u2019ll want to use 2. Secondly, instead of decimal fractions, we\u2019ll want to use binary fractions.", "Please note that I have chosen to write the radix (2 or 10) and their exponents (1 or 0 respectively) in their decimal forms while the numbers on the left hand side and the significands are in binary or decimal respectively.", "The binary number 1101 is 13 in base 10. And 13/16 is 0.8125. This is a binary fraction. If you haven\u2019t played with these yet, you should convince yourself of the following:", "Great. We are now ready to dig into the details of floating point numbers.", "Here is the diagram for the \u201cIEEE 754\u201d standard commonly implemented. The first bit is the sign. 0 is positive and 1 is negative (the opposite of what we na\u00efvely suggested above). There are 11 bits for the exponent and 52 or 53 (depending how you count) bits for the fraction, also called the \u201cmantissa\u201d or \u201csignificand\u201d. The sign just works like the flag we saw above, so we\u2019ll go into each of the last two in some depth.", "The 52-bit significand represents a binary fraction. If you review the scientific notation section above, you\u2019ll see that whenever we write a binary number in \u201cbinary scientific notation,\u201d the leading digit is always 1. (In base 10 it could be between 1 and 9, but 2\u20139 aren\u2019t binary digits). Since we know the leading digit will always be 1 (with some caveats to be discussed), we don\u2019t need to actually store it on the computer (this would be wasteful). This is why I said the significand is 53 bits \u201cdepending on how you count.\u201d", "In other words, the 52 bits stored on the computer represent the 52 bits that come after the decimal point (or maybe we should call it a \u201cbinary point\u201d). A leading 1 is always assumed.", "I keep mentioning some caveats, and I intend to put them off for as long as possible. A \u201cnormal number\u201d is a non-zero number that doesn\u2019t use any of these caveats, and we are in a position to give some examples.", "How would we represent the decimal number 1?", "Well, the sign is positive, so the sign bit is 0. (Think of 1 as a flag for \u201cnegative\u201d). The exponent is 0. Remembering that the biased representation means we add 1023, we get the binary representation 01111111111. Finally, all the fraction bits are 0. Easy:", "I\u2019ve written the binary floating-point representation with a space separating the three parts. As usual, the radix and exponent in the \u201cbinary scientific\u201d representation are actually in base 10.", "What about a harder example, like 3? 3 is 1.5 times 2 (in decimal), so turning that into a binary fraction, we have 1.1. The exponent 2\u00b9 is represented as 10000000000 accounting for bias.", "What\u2019s the largest (normal) number we can get? We should make the exponent 11111111110 (we can\u2019t make it all ones, that\u2019s reserved), which in decimal is 1023.", "but we can also take advantage of the fact that Python has native arbitrary-precision integer arithmetic to gratuitously write out all 309 digits in base 10:", "The smallest possible float is just the negative of this. But what is the smallest positive (normal) float? We already said the smallest positive exponent is \u20131022. Make the significand all 0s, and that means the smallest positive normal floating point number is:", "Again, arbitrary precision integer arithmetic means we can exploit the middle fraction to easily get an exact decimal value in all its glory.", "You know, just in case you were curious. By the way, you can check all of this on your python + hardware setup with:", "and essentially every other programming language has a similar feature.", "Okay, here\u2019s where things get weird. If all of the exponent bits are 1, then the number represented is either infinite or not a number (NaN):", "The thing I initially found surprising about this is that this is a hardware implementation on commonly used chips. This means, for example, you can use it on a GPU. Why would you want to do that? Well, consider the convenient fact that e to the power of \u2013\u221e is 0.", "In the paper that introduced the transformer architecture for NLP tasks (the one used by BERT, GPT-2, and their more recent cousins), the training was autoregressive which meant that in the attention module\u2019s softmax layers, certain outputs were required to be 0. But if you look at the formula for the softmax and recall that your high school math teacher told you that \u201cthere is no number such that exponentiating to it is 0,\u201d you will see it\u2019s tricky to make a softmax return 0. Unless of course, you make (minus) infinity a number!", "And, crucially, this is a hardware implementation. If it was a gimmicky Python (or PyTorch, or Numpy) workaround that represented numbers as an object which might sometimes contain a floating point number, this would substantially slow down numerical computations.", "Also, the unending complexity of computer hardware is always impressive.", "But wait, there\u2019s more! We haven\u2019t even described how to represent 0 yet. Using our exponents and our fraction bits, we were only able to make a very small positive number, not actually 0. The solution of course is that if the exponent bits are all 0 and so is the fraction, then the number is 0. In other words, if the exponent bits are 00000000000 and the fraction bits are also all zeros. Note this means that 0 is \u201csigned\u201d \u2013 there is both +0 and \u20130. In Python, they are stored differently, but they are equal to each other.", "There are a few edge cases where things get weird though. When trying to compute an angle with atan2, you will see that they are in fact represented differently:", "The final case to cover is when all the exponent bits are 0, but the fraction bits are not 0. If we have a representation that doesn\u2019t use some possible bit sequences, we are wasting space. So why not use it to represent even smaller numbers? These numbers are called subnormal (or denormal) numbers.", "Basically, the rule is that the exponent is still considered to have its minimal value (\u20131022) and instead of our \u201cbinary scientific\u201d notation always starting with a 1 (as in 1.001), we assume instead that it starts with a 0. So we can have 0.001 times 2 to the power of \u20131022. This lets us represent numbers up with an exponent 52 less (as small as \u20131074). Thus:", "The benefits of subnormal numbers are that, when you subtract two different normal floats, you are guaranteed to get a non-zero result. The cost is lost precision (there is no precision stored in the leading 0s \u2013 remember how sigfigs work?). This is called gradual underflow. As floats get smaller and smaller, they gradually lose precision.", "Without subnormal numbers you would have to flush to zero, losing all your precision at once and significantly increasing the chance that you\u2019ll accidentally end up dividing by 0. However, subnormal numbers significantly slow down calculations.", "Okay, we spent all this time talking about floating point numbers. Besides some weird edge case about 0.1 * 3 that never really comes up, who cares?", "Besides the 64-bit float we explored at length, there are also 32-bit floats (single precision) and 16-bit floats (half-precision) commonly available. PyTorch and other numerical computing libraries tend to stick to 32-bit floats by default. Half the size means the computations can be done faster (half as many bits to crunch).", "But lower precision comes with a cost. With a standard half-precision float (5 exponent bits, 10 significand bits), the smallest number bigger than 1 is about 1.001. You can\u2019t represent the integer 2049 (you have to pick either 2050 or 2048; and no decimals in between either). 65535 is the largest possible number (or close, depending on precise implementation details).", "Google\u2019s Tensor Processing Units instead use a modified 16-bit format for multiplication as part of their many optimizations for deep-learning tasks. The 8-bit exponent with 7-bit significand has just as many exponent bits as a 32-bit floating point number. And it turns out that in deep learning applications, this matters more than the significand bits. Also, when multiplying, the exponents can be added (easy) while the significand bits have to be multiplied (harder). Making the significand smaller makes the silicon that multiplies floats about 8 times smaller.", "Plus, the TPU float format flushes to zero instead of using subnormal numbers to boost speed.", "If you read the Google blog post about their custom 16-bit float format, you\u2019ll see they talk about \u201cdynamic range.\u201d In fact, this something similar is going on with HDR images (like the ones you can capture on your phone).", "A standard image uses an 8-bit RGB encoding. Those 8 bits represent an unsigned integer between 0 and 255. The problem with this is that the relative precision (% jump between consecutive values) is much worse when it\u2019s darker. For example, between a (decimal) pixel value of 10 and 11, there is a 10% jump! But for bright values, the relative difference between 250 and 251 is just 0.4%.", "Now the human eye is more sensitive to changes in brightness with dark tones than with bright ones. Meaning the fixed-precision representation is the opposite of what we\u2019d want. Thus, a standard digital or phone camera shooting a JPEG or similar adjusts its sensitivity by recording relatively more precision in the darker tones using a gamma encoding.", "The downside to this is that, even if you add bits (say with a 16-bit RBG image), you don\u2019t necessarily gain as much precision in the parts of your image that are bright.", "So, an HDR image uses floating point numbers to represent the pixels! This allows a high \u201cdynamic\u201d range (the exponent can be high or low) while still maintaining relative precision across all brightness scales. Perfect for keeping the data from scenes with high contrast. For example in the Radiance HDR format, the exponent is shared across the three colors (channels) in each pixel.", "This might be more than you ever wanted to know about floating point numbers. With any luck, you won\u2019t encounter too much numerical under-flow or over-flow that can\u2019t be solved with a simple log-sum-exp or arbitrary-precision integers. But if you do, you\u2019ll be well-prepared! Hopefully, you are also well-positioned to think about just how much precision you need in your machine-learning models as well.", "[1] Note: this article assumes a relatively standard setup. It is possible (though uncommon) your results could differ depending on your hardware and software implementation.", "[2] I mean, you Python interpreter is allowed to throw an error, crash, and then stop doing things. Your CPU can\u2019t do that exactly: it has to stay alive.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Scientist, Mathematician. Formerly @MIT, @McKinsey, currently teaching computers to read"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F1429907b6d1d&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-floating-point-numbers-work-1429907b6d1d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-floating-point-numbers-work-1429907b6d1d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-floating-point-numbers-work-1429907b6d1d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-floating-point-numbers-work-1429907b6d1d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----1429907b6d1d--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1429907b6d1d--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@rmcharan?source=post_page-----1429907b6d1d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@rmcharan?source=post_page-----1429907b6d1d--------------------------------", "anchor_text": "Ravi Charan"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F393ce2bbf82c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-floating-point-numbers-work-1429907b6d1d&user=Ravi+Charan&userId=393ce2bbf82c&source=post_page-393ce2bbf82c----1429907b6d1d---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1429907b6d1d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-floating-point-numbers-work-1429907b6d1d&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1429907b6d1d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-floating-point-numbers-work-1429907b6d1d&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@nhillier?utm_source=medium&utm_medium=referral", "anchor_text": "Nick Hillier"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://www.joelonsoftware.com/2002/11/11/the-law-of-leaky-abstractions/", "anchor_text": "leaky abstraction"}, {"url": "https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus", "anchor_text": "modified"}, {"url": "https://en.wikipedia.org/wiki/Binary_number", "anchor_text": "binary"}, {"url": "https://en.wikipedia.org/wiki/Two%27s_complement", "anchor_text": "Two\u2019s Complement"}, {"url": "https://en.wikipedia.org/wiki/Ones%27_complement", "anchor_text": "One\u2019s Complement"}, {"url": "https://en.wikipedia.org/wiki/Double-precision_floating-point_format#IEEE_754_double-precision_binary_floating-point_format:_binary64", "anchor_text": "IEEE 754"}, {"url": "https://en.wikipedia.org/wiki/File:IEEE_754_Double_Floating_Point_Format.svg", "anchor_text": "Wikipedia"}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "transformer architecture"}, {"url": "https://en.wikipedia.org/wiki/Softmax_function", "anchor_text": "softmax"}, {"url": "https://en.wikipedia.org/wiki/Atan2", "anchor_text": "atan2"}, {"url": "https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus", "anchor_text": "8 times smaller"}, {"url": "https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus", "anchor_text": "blog post"}, {"url": "https://en.wikipedia.org/wiki/Gamma_correction", "anchor_text": "gamma encoding"}, {"url": "https://en.wikipedia.org/wiki/RGBE_image_format", "anchor_text": "Radiance HDR"}, {"url": "https://en.wikipedia.org/wiki/LogSumExp", "anchor_text": "log-sum-exp"}, {"url": "https://medium.com/tag/computer-science?source=post_page-----1429907b6d1d---------------computer_science-----------------", "anchor_text": "Computer Science"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----1429907b6d1d---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----1429907b6d1d---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/ai?source=post_page-----1429907b6d1d---------------ai-----------------", "anchor_text": "AI"}, {"url": "https://medium.com/tag/editors-pick?source=post_page-----1429907b6d1d---------------editors_pick-----------------", "anchor_text": "Editors Pick"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1429907b6d1d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-floating-point-numbers-work-1429907b6d1d&user=Ravi+Charan&userId=393ce2bbf82c&source=-----1429907b6d1d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1429907b6d1d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-floating-point-numbers-work-1429907b6d1d&user=Ravi+Charan&userId=393ce2bbf82c&source=-----1429907b6d1d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1429907b6d1d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-floating-point-numbers-work-1429907b6d1d&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1429907b6d1d--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F1429907b6d1d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-floating-point-numbers-work-1429907b6d1d&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----1429907b6d1d---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----1429907b6d1d--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----1429907b6d1d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----1429907b6d1d--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----1429907b6d1d--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----1429907b6d1d--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----1429907b6d1d--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----1429907b6d1d--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----1429907b6d1d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@rmcharan?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@rmcharan?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ravi Charan"}, {"url": "https://medium.com/@rmcharan/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "599 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F393ce2bbf82c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-floating-point-numbers-work-1429907b6d1d&user=Ravi+Charan&userId=393ce2bbf82c&source=post_page-393ce2bbf82c--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F6bca6dd641ca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-floating-point-numbers-work-1429907b6d1d&newsletterV3=393ce2bbf82c&newsletterV3Id=6bca6dd641ca&user=Ravi+Charan&userId=393ce2bbf82c&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}