{"url": "https://towardsdatascience.com/how-deepminds-unreal-agent-performed-9-times-better-than-experts-on-atari-9c6ee538404e", "time": 1683007618.941647, "path": "towardsdatascience.com/how-deepminds-unreal-agent-performed-9-times-better-than-experts-on-atari-9c6ee538404e/", "webpage": {"metadata": {"title": "DeepMind's UNREAL Algorithm Explained | Towards Data Science", "h1": "DeepMind\u2019s UNREAL Algorithm Explained", "description": "An outline of DeepMind's UNREAL reinforcement learning algorithm. It uses deep neural networks to learn Atari and performed 9 times better than experts."}, "outgoing_paragraph_urls": [{"url": "https://deepmind.com/", "anchor_text": "DeepMind\u2019s", "paragraph_index": 1}, {"url": "https://medium.com/u/18dfe63fa7f0?source=post_page-----9c6ee538404e--------------------------------", "anchor_text": "Arthur Juliani", "paragraph_index": 3}, {"url": "https://www.atari.com/", "anchor_text": "Atari", "paragraph_index": 36}, {"url": "https://arxiv.org/pdf/1611.05397.pdf", "anchor_text": "Reinforcement Learning with Unsupervised Auxiliary Tasks", "paragraph_index": 40}], "all_paragraphs": ["The marriage between deep learning and reinforcement learning has given birth to a plethora of opportunities for artificial intelligence. However, sparse reward environments have been a long-studied challenge to reinforcement learning researchers. Updating parameters on uninformative reward signals often makes learning converge to suboptimal policies.", "DeepMind\u2019s Unsupervised Reinforcement and Auxiliary Learning (UNREAL) agent takes a unique approach to work its way around this problem. It uses auxiliary tasks and pseudo-rewards to autonomously select features in the environment most relevant to accomplishing its goal.", "Here, we outline DeepMind\u2019s novel UNREAL agent and its performance compared to other state-of-the-art algorithms.", "UNREAL\u2019s core agent uses Asynchronous Advantage Actor-Critic (A3C) to optimize its policy and value function. For a quick recap on A3C, Arthur Juliani has a fantastic introduction to it found here:", "We can define the A3C loss as follows:", "We\u2019ll be stepping through the different parts of UNREAL\u2019s architecture:", "Inputs are images passed into a convolutional layer and inputted into an LSTM. One important thing to notice is that each of the four components uses the same convolutional network and LSTM layers. For example, the auxiliary control task named \u201cPixel Control\u201d shares both of these layers with the \u201cBase A3C Agent.\u201d", "By having these tasks share parameters with the main A3C policy, the agent is able to extract meaningful information from the environment without directly optimizing towards its goal.", "This intuition will be useful later on!", "Since we assume prior knowledge of A3C, let\u2019s dive into the latter three parts of the architecture. Just keep in mind that, while the other parts of the architecture are concurrently being trained, A3C is still using on-policy updates [across several instances of the environment] in the background to optimize its policy and value functions.", "We can think of auxiliary tasks as \u201cside quests.\u201d Although they don\u2019t directly help achieve the overall goal, they help the agent learn about environment dynamics and extract relevant information. In turn, that helps the agent learn how to achieve the desired overall end state. We can also view them as additional pseudo-reward functions for the agent to interact with.", "Overall, the goal is to maximize the sum of two terms:", "where the superscript c denotes an auxiliary control task reward. Here are the two control tasks used by UNREAL:", "For more details on how these tasks are defined and learned, feel free to skim this paper [1]. For now, just know that the agent tries to find accurate Q value functions to best achieve these auxiliary tasks, using auxiliary rewards defined by the user.", "Okay, perfect! Now we just add the extrinsic and auxiliary rewards then run A3C using the sum as a newly defined reward! Right?", "In actuality, UNREAL does something different. Instead of training a single policy to optimize this reward, it trains a policy for each of the tasks on top of the base A3C policy. While all auxiliary task policies share some network components with the base A3C agent, they each also add individual components to define separate policies.", "For example, the \u201cPixel Control\u201d task has a deconvolutional network after the shared convolutional network and LSTM. The output defines the Q-values for the pixel control policy. (Skim [1] for details on the implementation)", "Each of the policies optimizes an n-step Q-learning loss:", "Even more amazingly, we never explicitly use these auxiliary control task policies. Even though we discover which actions optimize each of the auxiliary tasks, we only use the base A3C agent\u2019s actions in the environment. Then, you may think, all this auxiliary training was for nothing!", "Not quite. The key is that there are shared parts of the architecture between the A3C agent and auxiliary control tasks! As we optimize policies over the auxiliary tasks, we are changing parameters that are also used by the base agent. This has, what I like to call, a \u201cnudging effect.\u201d", "Updating shared components not only helps learn auxiliary tasks but also better equips the agent to solve the overall problem by extracting relevant information from the environment.", "In other words, we get more information from the environment than if we did not use auxiliary tasks.", "Just like any RL problem, an agent must recognize states that lead to higher rewards. Of course, this is what our value function is for right?", "This is where it becomes hairy. We have two issues:", "As a result, UNREAL uses an auxiliary reward task called reward prediction. The agent tries to predict the immediate reward given a sequence of states the agent observes previously.", "However, similar to the auxiliary control policies, we don\u2019t explicitly use these predicted rewards anywhere! Notice how the reward prediction module uses the same convolutional network as the base A3C agent. Then, this input is concatenated and passed through a fully-connected layer. In practice, UNREAL uses cross-entropy loss by separating predictions into three classes: negative, zero, and positive.", "The agent only learns reward prediction to shape the base A3C parameters for feature selection.", "As the agent learns to correctly predict these rewards, it\u2019ll naturally pick out features that are important to anticipating future rewards. This helps the base A3C agent learn.", "Also, when training reward prediction, we can skew sampling from experience replay towards transitions with high rewards without oversaturating the value network. In practice, UNREAL samples such that the probability of non-zero reward is 50%.", "As a result, we\u2019ve effectively solved two of the roadblocks above:", "Lastly, we notice how the LSTM is not used for reward prediction. Instead, we just use the raw convolutional network output. The idea? We only want immediate information to estimate immediate rewards. We don\u2019t look too far in the past nor too far in the future.", "We had used experience replay for auxiliary reward task updating, but can we apply experience replay sampling anywhere else? The LSTM makes this difficult. When we have a hidden state, storing that hidden state becomes irrelevant after updating network parameters.", "Interestingly, UNREAL does off-policy learning on top of A3C on-policy learning for its value network. For this off-policy learning, we neglect using stored hidden states and pretend that the first state in a contiguous sample begins an episode. This boosts robustness because this varies the kind of experience our agent encounters.", "But still, why do we do this?", "We can think of an RNN as a sliding window over time. It has a certain section of time and observations that it takes into account. However, if we were to plop the agent into an arbitrary starting state with no prior observations, it can fully exploit the new features discovered by the reward prediction task instead of exploiting past observations in the hidden state.", "The overall optimized loss looks like this:", "UNREAL beat out many strong contenders including A3C and Prioritized Dueling DQN. It performed 880% better than human normalized performance after 57 Atari games.", "UNREAL was also pitted against these agents in Labyrinth. It was shown that it was significantly more data-efficient than other methods. In fact, in one of the environments, the UNREAL agent used approximately 10% of the data to reach A3C performance. This is a significant reduction.", "Of course, this paper [1] was published in 2016. As a result, it is almost guaranteed that more state-of-the-art algorithms have performed better than UNREAL. However, this agent, in particular, opened a fantastic avenue for deep reinforcement learning agents in environments with sparse rewards.", "That concludes the overview of DeepMind\u2019s UNREAL agent and why it is an incredibly robust approach to RL problems. For a more detailed outline of the algorithm, feel free to check out the paper below or take a look at DeepMind\u2019s official website:", "[1] M. Jaderberg, V. Mnih, W. Czarnecki, T. Schaul, J. Leibo, D. Silver, K. Kavukkcuoglu, Reinforcement Learning with Unsupervised Auxiliary Tasks (2016).", "From the classic to state-of-the-art, here are related articles discussing both multi-agent and single-agent reinforcement learning:", "Part-time writer \u00b7 Full-time learner \u00b7 PhD Student @ University of Michigan"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F9c6ee538404e&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-deepminds-unreal-agent-performed-9-times-better-than-experts-on-atari-9c6ee538404e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-deepminds-unreal-agent-performed-9-times-better-than-experts-on-atari-9c6ee538404e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-deepminds-unreal-agent-performed-9-times-better-than-experts-on-atari-9c6ee538404e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-deepminds-unreal-agent-performed-9-times-better-than-experts-on-atari-9c6ee538404e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/@austinnguyen517?source=post_page-----9c6ee538404e--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----9c6ee538404e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@austinnguyen517?source=post_page-----9c6ee538404e--------------------------------", "anchor_text": "Austin Nguyen"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6a1b6c5da9ab&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-deepminds-unreal-agent-performed-9-times-better-than-experts-on-atari-9c6ee538404e&user=Austin+Nguyen&userId=6a1b6c5da9ab&source=post_page-6a1b6c5da9ab----9c6ee538404e---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----9c6ee538404e--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9c6ee538404e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-deepminds-unreal-agent-performed-9-times-better-than-experts-on-atari-9c6ee538404e&user=Austin+Nguyen&userId=6a1b6c5da9ab&source=-----9c6ee538404e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9c6ee538404e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-deepminds-unreal-agent-performed-9-times-better-than-experts-on-atari-9c6ee538404e&source=-----9c6ee538404e---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://unsplash.com/@ikukevk?utm_source=medium&utm_medium=referral", "anchor_text": "Kevin Ku"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://deepmind.com/", "anchor_text": "DeepMind\u2019s"}, {"url": "https://medium.com/u/18dfe63fa7f0?source=post_page-----9c6ee538404e--------------------------------", "anchor_text": "Arthur Juliani"}, {"url": "https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2", "anchor_text": "Simple Reinforcement Learning with Tensorflow Part 8: Asynchronous Actor-Critic Agents (A3C)In this article, I want to provide a tutorial on implementing the Asynchronous Advantage Actor-Critic (A3C) algorithm in\u2026medium.com"}, {"url": "https://unsplash.com/@scienceinhd?utm_source=medium&utm_medium=referral", "anchor_text": "Science in HD"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://unsplash.com/@markusspiske?utm_source=medium&utm_medium=referral", "anchor_text": "Markus Spiske"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://unsplash.com/@leecampbell?utm_source=medium&utm_medium=referral", "anchor_text": "Lee Campbell"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://www.atari.com/", "anchor_text": "Atari"}, {"url": "https://deepmind.com/blog/article/reinforcement-learning-unsupervised-auxiliary-tasks", "anchor_text": "Reinforcement learning with unsupervised auxiliary tasksOur primary mission at DeepMind is to push the boundaries of AI, developing programs that can learn to solve any\u2026deepmind.com"}, {"url": "https://arxiv.org/pdf/1611.05397.pdf", "anchor_text": "Reinforcement Learning with Unsupervised Auxiliary Tasks"}, {"url": "https://towardsdatascience.com/openais-multi-agent-deep-deterministic-policy-gradients-maddpg-9d2dad34c82", "anchor_text": "OpenAI\u2019s MADDPG AlgorithmAn actor-critic approach to multi-agent RL problemstowardsdatascience.com"}, {"url": "https://towardsdatascience.com/hierarchical-reinforcement-learning-feudal-networks-44e2657526d7", "anchor_text": "Hierarchical Reinforcement Learning: FeUdal NetworksLetting computers see the bigger picturetowardsdatascience.co"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----9c6ee538404e---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----9c6ee538404e---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----9c6ee538404e---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----9c6ee538404e---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----9c6ee538404e---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9c6ee538404e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-deepminds-unreal-agent-performed-9-times-better-than-experts-on-atari-9c6ee538404e&user=Austin+Nguyen&userId=6a1b6c5da9ab&source=-----9c6ee538404e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9c6ee538404e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-deepminds-unreal-agent-performed-9-times-better-than-experts-on-atari-9c6ee538404e&user=Austin+Nguyen&userId=6a1b6c5da9ab&source=-----9c6ee538404e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9c6ee538404e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-deepminds-unreal-agent-performed-9-times-better-than-experts-on-atari-9c6ee538404e&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/@austinnguyen517?source=post_page-----9c6ee538404e--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----9c6ee538404e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6a1b6c5da9ab&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-deepminds-unreal-agent-performed-9-times-better-than-experts-on-atari-9c6ee538404e&user=Austin+Nguyen&userId=6a1b6c5da9ab&source=post_page-6a1b6c5da9ab----9c6ee538404e---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F2cb997cef03c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-deepminds-unreal-agent-performed-9-times-better-than-experts-on-atari-9c6ee538404e&newsletterV3=6a1b6c5da9ab&newsletterV3Id=2cb997cef03c&user=Austin+Nguyen&userId=6a1b6c5da9ab&source=-----9c6ee538404e---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/@austinnguyen517?source=post_page-----9c6ee538404e--------------------------------", "anchor_text": "Written by Austin Nguyen"}, {"url": "https://medium.com/@austinnguyen517/followers?source=post_page-----9c6ee538404e--------------------------------", "anchor_text": "208 Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----9c6ee538404e--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6a1b6c5da9ab&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-deepminds-unreal-agent-performed-9-times-better-than-experts-on-atari-9c6ee538404e&user=Austin+Nguyen&userId=6a1b6c5da9ab&source=post_page-6a1b6c5da9ab----9c6ee538404e---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F2cb997cef03c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-deepminds-unreal-agent-performed-9-times-better-than-experts-on-atari-9c6ee538404e&newsletterV3=6a1b6c5da9ab&newsletterV3Id=2cb997cef03c&user=Austin+Nguyen&userId=6a1b6c5da9ab&source=-----9c6ee538404e---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/openais-multi-agent-deep-deterministic-policy-gradients-maddpg-9d2dad34c82?source=author_recirc-----9c6ee538404e----0---------------------4e823fed_a18a_41bb_85b5_c954df0d5afb-------", "anchor_text": ""}, {"url": "https://medium.com/@austinnguyen517?source=author_recirc-----9c6ee538404e----0---------------------4e823fed_a18a_41bb_85b5_c954df0d5afb-------", "anchor_text": ""}, {"url": "https://medium.com/@austinnguyen517?source=author_recirc-----9c6ee538404e----0---------------------4e823fed_a18a_41bb_85b5_c954df0d5afb-------", "anchor_text": "Austin Nguyen"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----9c6ee538404e----0---------------------4e823fed_a18a_41bb_85b5_c954df0d5afb-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/openais-multi-agent-deep-deterministic-policy-gradients-maddpg-9d2dad34c82?source=author_recirc-----9c6ee538404e----0---------------------4e823fed_a18a_41bb_85b5_c954df0d5afb-------", "anchor_text": "OpenAI\u2019s MADDPG AlgorithmAn actor-critic approach to multi-agent RL problems"}, {"url": "https://towardsdatascience.com/openais-multi-agent-deep-deterministic-policy-gradients-maddpg-9d2dad34c82?source=author_recirc-----9c6ee538404e----0---------------------4e823fed_a18a_41bb_85b5_c954df0d5afb-------", "anchor_text": "\u00b78 min read\u00b7May 26, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9d2dad34c82&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fopenais-multi-agent-deep-deterministic-policy-gradients-maddpg-9d2dad34c82&user=Austin+Nguyen&userId=6a1b6c5da9ab&source=-----9d2dad34c82----0-----------------clap_footer----4e823fed_a18a_41bb_85b5_c954df0d5afb-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/openais-multi-agent-deep-deterministic-policy-gradients-maddpg-9d2dad34c82?source=author_recirc-----9c6ee538404e----0---------------------4e823fed_a18a_41bb_85b5_c954df0d5afb-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9d2dad34c82&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fopenais-multi-agent-deep-deterministic-policy-gradients-maddpg-9d2dad34c82&source=-----9c6ee538404e----0-----------------bookmark_preview----4e823fed_a18a_41bb_85b5_c954df0d5afb-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----9c6ee538404e----1---------------------4e823fed_a18a_41bb_85b5_c954df0d5afb-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----9c6ee538404e----1---------------------4e823fed_a18a_41bb_85b5_c954df0d5afb-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----9c6ee538404e----1---------------------4e823fed_a18a_41bb_85b5_c954df0d5afb-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----9c6ee538404e----1---------------------4e823fed_a18a_41bb_85b5_c954df0d5afb-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----9c6ee538404e----1---------------------4e823fed_a18a_41bb_85b5_c954df0d5afb-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----9c6ee538404e----1---------------------4e823fed_a18a_41bb_85b5_c954df0d5afb-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----4e823fed_a18a_41bb_85b5_c954df0d5afb-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----9c6ee538404e----1---------------------4e823fed_a18a_41bb_85b5_c954df0d5afb-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----9c6ee538404e----1-----------------bookmark_preview----4e823fed_a18a_41bb_85b5_c954df0d5afb-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----9c6ee538404e----2---------------------4e823fed_a18a_41bb_85b5_c954df0d5afb-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----9c6ee538404e----2---------------------4e823fed_a18a_41bb_85b5_c954df0d5afb-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----9c6ee538404e----2---------------------4e823fed_a18a_41bb_85b5_c954df0d5afb-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----9c6ee538404e----2---------------------4e823fed_a18a_41bb_85b5_c954df0d5afb-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----9c6ee538404e----2---------------------4e823fed_a18a_41bb_85b5_c954df0d5afb-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----9c6ee538404e----2---------------------4e823fed_a18a_41bb_85b5_c954df0d5afb-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----2-----------------clap_footer----4e823fed_a18a_41bb_85b5_c954df0d5afb-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----9c6ee538404e----2---------------------4e823fed_a18a_41bb_85b5_c954df0d5afb-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----9c6ee538404e----2-----------------bookmark_preview----4e823fed_a18a_41bb_85b5_c954df0d5afb-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-used-machine-learning-to-automatically-hand-draw-any-picture-7d024d0de997?source=author_recirc-----9c6ee538404e----3---------------------4e823fed_a18a_41bb_85b5_c954df0d5afb-------", "anchor_text": ""}, {"url": "https://medium.com/@austinnguyen517?source=author_recirc-----9c6ee538404e----3---------------------4e823fed_a18a_41bb_85b5_c954df0d5afb-------", "anchor_text": ""}, {"url": "https://medium.com/@austinnguyen517?source=author_recirc-----9c6ee538404e----3---------------------4e823fed_a18a_41bb_85b5_c954df0d5afb-------", "anchor_text": "Austin Nguyen"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----9c6ee538404e----3---------------------4e823fed_a18a_41bb_85b5_c954df0d5afb-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/how-i-used-machine-learning-to-automatically-hand-draw-any-picture-7d024d0de997?source=author_recirc-----9c6ee538404e----3---------------------4e823fed_a18a_41bb_85b5_c954df0d5afb-------", "anchor_text": "How I Used Machine Learning to Automatically Hand-Draw Any PictureSupervised and unsupervised learning made easy!"}, {"url": "https://towardsdatascience.com/how-i-used-machine-learning-to-automatically-hand-draw-any-picture-7d024d0de997?source=author_recirc-----9c6ee538404e----3---------------------4e823fed_a18a_41bb_85b5_c954df0d5afb-------", "anchor_text": "\u00b78 min read\u00b7Jun 1, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7d024d0de997&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-used-machine-learning-to-automatically-hand-draw-any-picture-7d024d0de997&user=Austin+Nguyen&userId=6a1b6c5da9ab&source=-----7d024d0de997----3-----------------clap_footer----4e823fed_a18a_41bb_85b5_c954df0d5afb-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-used-machine-learning-to-automatically-hand-draw-any-picture-7d024d0de997?source=author_recirc-----9c6ee538404e----3---------------------4e823fed_a18a_41bb_85b5_c954df0d5afb-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "4"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7d024d0de997&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-used-machine-learning-to-automatically-hand-draw-any-picture-7d024d0de997&source=-----9c6ee538404e----3-----------------bookmark_preview----4e823fed_a18a_41bb_85b5_c954df0d5afb-------", "anchor_text": ""}, {"url": "https://medium.com/@austinnguyen517?source=post_page-----9c6ee538404e--------------------------------", "anchor_text": "See all from Austin Nguyen"}, {"url": "https://towardsdatascience.com/?source=post_page-----9c6ee538404e--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----9c6ee538404e----0---------------------abb53b61_4e7a_4fb8_a212_78abb00bf64d-------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=read_next_recirc-----9c6ee538404e----0---------------------abb53b61_4e7a_4fb8_a212_78abb00bf64d-------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=read_next_recirc-----9c6ee538404e----0---------------------abb53b61_4e7a_4fb8_a212_78abb00bf64d-------", "anchor_text": "Wouter van Heeswijk, PhD"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----9c6ee538404e----0---------------------abb53b61_4e7a_4fb8_a212_78abb00bf64d-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----9c6ee538404e----0---------------------abb53b61_4e7a_4fb8_a212_78abb00bf64d-------", "anchor_text": "Proximal Policy Optimization (PPO) ExplainedThe journey from REINFORCE to the go-to algorithm in continuous control"}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----9c6ee538404e----0---------------------abb53b61_4e7a_4fb8_a212_78abb00bf64d-------", "anchor_text": "\u00b713 min read\u00b7Nov 29, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fabed1952457b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fproximal-policy-optimization-ppo-explained-abed1952457b&user=Wouter+van+Heeswijk%2C+PhD&userId=33f45c9ab481&source=-----abed1952457b----0-----------------clap_footer----abb53b61_4e7a_4fb8_a212_78abb00bf64d-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----9c6ee538404e----0---------------------abb53b61_4e7a_4fb8_a212_78abb00bf64d-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fabed1952457b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fproximal-policy-optimization-ppo-explained-abed1952457b&source=-----9c6ee538404e----0-----------------bookmark_preview----abb53b61_4e7a_4fb8_a212_78abb00bf64d-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----9c6ee538404e----1---------------------abb53b61_4e7a_4fb8_a212_78abb00bf64d-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData?source=read_next_recirc-----9c6ee538404e----1---------------------abb53b61_4e7a_4fb8_a212_78abb00bf64d-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData?source=read_next_recirc-----9c6ee538404e----1---------------------abb53b61_4e7a_4fb8_a212_78abb00bf64d-------", "anchor_text": "Andrew Austin"}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----9c6ee538404e----1---------------------abb53b61_4e7a_4fb8_a212_78abb00bf64d-------", "anchor_text": "AI Anyone Can Understand Part 1: Reinforcement LearningReinforcement learning is a way for machines to learn by trying different things and seeing what works best. For example, a robot could\u2026"}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----9c6ee538404e----1---------------------abb53b61_4e7a_4fb8_a212_78abb00bf64d-------", "anchor_text": "\u00b74 min read\u00b7Dec 11, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F6c3b3d623a2d&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40MoneyAndData%2Fai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d&user=Andrew+Austin&userId=42d388912d13&source=-----6c3b3d623a2d----1-----------------clap_footer----abb53b61_4e7a_4fb8_a212_78abb00bf64d-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----9c6ee538404e----1---------------------abb53b61_4e7a_4fb8_a212_78abb00bf64d-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6c3b3d623a2d&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40MoneyAndData%2Fai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d&source=-----9c6ee538404e----1-----------------bookmark_preview----abb53b61_4e7a_4fb8_a212_78abb00bf64d-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/rainbow-dqn-the-best-reinforcement-learning-has-to-offer-166cb8ed2f86?source=read_next_recirc-----9c6ee538404e----0---------------------abb53b61_4e7a_4fb8_a212_78abb00bf64d-------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=read_next_recirc-----9c6ee538404e----0---------------------abb53b61_4e7a_4fb8_a212_78abb00bf64d-------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=read_next_recirc-----9c6ee538404e----0---------------------abb53b61_4e7a_4fb8_a212_78abb00bf64d-------", "anchor_text": "Wouter van Heeswijk, PhD"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----9c6ee538404e----0---------------------abb53b61_4e7a_4fb8_a212_78abb00bf64d-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/rainbow-dqn-the-best-reinforcement-learning-has-to-offer-166cb8ed2f86?source=read_next_recirc-----9c6ee538404e----0---------------------abb53b61_4e7a_4fb8_a212_78abb00bf64d-------", "anchor_text": "Rainbow DQN \u2014 The Best Reinforcement Learning Has to Offer?What happens if the most successful techniques in Deep Q-Learning are combined into a single algorithm?"}, {"url": "https://towardsdatascience.com/rainbow-dqn-the-best-reinforcement-learning-has-to-offer-166cb8ed2f86?source=read_next_recirc-----9c6ee538404e----0---------------------abb53b61_4e7a_4fb8_a212_78abb00bf64d-------", "anchor_text": "\u00b711 min read\u00b7Dec 8, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F166cb8ed2f86&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frainbow-dqn-the-best-reinforcement-learning-has-to-offer-166cb8ed2f86&user=Wouter+van+Heeswijk%2C+PhD&userId=33f45c9ab481&source=-----166cb8ed2f86----0-----------------clap_footer----abb53b61_4e7a_4fb8_a212_78abb00bf64d-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/rainbow-dqn-the-best-reinforcement-learning-has-to-offer-166cb8ed2f86?source=read_next_recirc-----9c6ee538404e----0---------------------abb53b61_4e7a_4fb8_a212_78abb00bf64d-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F166cb8ed2f86&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frainbow-dqn-the-best-reinforcement-learning-has-to-offer-166cb8ed2f86&source=-----9c6ee538404e----0-----------------bookmark_preview----abb53b61_4e7a_4fb8_a212_78abb00bf64d-------", "anchor_text": ""}, {"url": "https://medium.datadriveninvestor.com/feature-importance-with-deep-neural-network-for-cryptocurrencies-f06191e2d562?source=read_next_recirc-----9c6ee538404e----1---------------------abb53b61_4e7a_4fb8_a212_78abb00bf64d-------", "anchor_text": ""}, {"url": "https://byfintech.medium.com/?source=read_next_recirc-----9c6ee538404e----1---------------------abb53b61_4e7a_4fb8_a212_78abb00bf64d-------", "anchor_text": ""}, {"url": "https://byfintech.medium.com/?source=read_next_recirc-----9c6ee538404e----1---------------------abb53b61_4e7a_4fb8_a212_78abb00bf64d-------", "anchor_text": "Bruce Yang ByFinTech"}, {"url": "https://medium.datadriveninvestor.com/?source=read_next_recirc-----9c6ee538404e----1---------------------abb53b61_4e7a_4fb8_a212_78abb00bf64d-------", "anchor_text": "DataDrivenInvestor"}, {"url": "https://medium.datadriveninvestor.com/feature-importance-with-deep-neural-network-for-cryptocurrencies-f06191e2d562?source=read_next_recirc-----9c6ee538404e----1---------------------abb53b61_4e7a_4fb8_a212_78abb00bf64d-------", "anchor_text": "Feature Importance with Deep Neural Network for CryptocurrenciesA FinRL-Meta Tutorial for NeurIPS 2022 Datasets and Benchmarks"}, {"url": "https://medium.datadriveninvestor.com/feature-importance-with-deep-neural-network-for-cryptocurrencies-f06191e2d562?source=read_next_recirc-----9c6ee538404e----1---------------------abb53b61_4e7a_4fb8_a212_78abb00bf64d-------", "anchor_text": "\u00b710 min read\u00b7Jan 2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fdatadriveninvestor%2Ff06191e2d562&operation=register&redirect=https%3A%2F%2Fmedium.datadriveninvestor.com%2Ffeature-importance-with-deep-neural-network-for-cryptocurrencies-f06191e2d562&user=Bruce+Yang+ByFinTech&userId=a878fc45fb3f&source=-----f06191e2d562----1-----------------clap_footer----abb53b61_4e7a_4fb8_a212_78abb00bf64d-------", "anchor_text": ""}, {"url": "https://medium.datadriveninvestor.com/feature-importance-with-deep-neural-network-for-cryptocurrencies-f06191e2d562?source=read_next_recirc-----9c6ee538404e----1---------------------abb53b61_4e7a_4fb8_a212_78abb00bf64d-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff06191e2d562&operation=register&redirect=https%3A%2F%2Fmedium.datadriveninvestor.com%2Ffeature-importance-with-deep-neural-network-for-cryptocurrencies-f06191e2d562&source=-----9c6ee538404e----1-----------------bookmark_preview----abb53b61_4e7a_4fb8_a212_78abb00bf64d-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----9c6ee538404e----2---------------------abb53b61_4e7a_4fb8_a212_78abb00bf64d-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----9c6ee538404e----2---------------------abb53b61_4e7a_4fb8_a212_78abb00bf64d-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----9c6ee538404e----2---------------------abb53b61_4e7a_4fb8_a212_78abb00bf64d-------", "anchor_text": "The PyCoach"}, {"url": "https://artificialcorner.com/?source=read_next_recirc-----9c6ee538404e----2---------------------abb53b61_4e7a_4fb8_a212_78abb00bf64d-------", "anchor_text": "Artificial Corner"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----9c6ee538404e----2---------------------abb53b61_4e7a_4fb8_a212_78abb00bf64d-------", "anchor_text": "You\u2019re Using ChatGPT Wrong! Here\u2019s How to Be Ahead of 99% of ChatGPT UsersMaster ChatGPT by learning prompt engineering."}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----9c6ee538404e----2---------------------abb53b61_4e7a_4fb8_a212_78abb00bf64d-------", "anchor_text": "\u00b77 min read\u00b7Mar 17"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fartificial-corner%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&user=The+PyCoach&userId=fb44e21903f3&source=-----886a50dabc54----2-----------------clap_footer----abb53b61_4e7a_4fb8_a212_78abb00bf64d-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----9c6ee538404e----2---------------------abb53b61_4e7a_4fb8_a212_78abb00bf64d-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "276"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&source=-----9c6ee538404e----2-----------------bookmark_preview----abb53b61_4e7a_4fb8_a212_78abb00bf64d-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/trust-region-policy-optimization-trpo-explained-4b56bd206fc2?source=read_next_recirc-----9c6ee538404e----3---------------------abb53b61_4e7a_4fb8_a212_78abb00bf64d-------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=read_next_recirc-----9c6ee538404e----3---------------------abb53b61_4e7a_4fb8_a212_78abb00bf64d-------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=read_next_recirc-----9c6ee538404e----3---------------------abb53b61_4e7a_4fb8_a212_78abb00bf64d-------", "anchor_text": "Wouter van Heeswijk, PhD"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----9c6ee538404e----3---------------------abb53b61_4e7a_4fb8_a212_78abb00bf64d-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/trust-region-policy-optimization-trpo-explained-4b56bd206fc2?source=read_next_recirc-----9c6ee538404e----3---------------------abb53b61_4e7a_4fb8_a212_78abb00bf64d-------", "anchor_text": "Trust Region Policy Optimization (TRPO) ExplainedThe Reinforcement Learning algorithm TRPO builds upon natural policy gradient algorithms, ensuring updates remain within \u2018trustworthy\u2019\u2026"}, {"url": "https://towardsdatascience.com/trust-region-policy-optimization-trpo-explained-4b56bd206fc2?source=read_next_recirc-----9c6ee538404e----3---------------------abb53b61_4e7a_4fb8_a212_78abb00bf64d-------", "anchor_text": "\u00b712 min read\u00b7Oct 12, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4b56bd206fc2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrust-region-policy-optimization-trpo-explained-4b56bd206fc2&user=Wouter+van+Heeswijk%2C+PhD&userId=33f45c9ab481&source=-----4b56bd206fc2----3-----------------clap_footer----abb53b61_4e7a_4fb8_a212_78abb00bf64d-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/trust-region-policy-optimization-trpo-explained-4b56bd206fc2?source=read_next_recirc-----9c6ee538404e----3---------------------abb53b61_4e7a_4fb8_a212_78abb00bf64d-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4b56bd206fc2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrust-region-policy-optimization-trpo-explained-4b56bd206fc2&source=-----9c6ee538404e----3-----------------bookmark_preview----abb53b61_4e7a_4fb8_a212_78abb00bf64d-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----9c6ee538404e--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----9c6ee538404e--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----9c6ee538404e--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----9c6ee538404e--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----9c6ee538404e--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----9c6ee538404e--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----9c6ee538404e--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----9c6ee538404e--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----9c6ee538404e--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----9c6ee538404e--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}