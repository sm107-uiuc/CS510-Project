{"url": "https://towardsdatascience.com/uncertainty-quantification-of-predictions-with-bayesian-inference-6192e31a9fa9", "time": 1683005684.605095, "path": "towardsdatascience.com/uncertainty-quantification-of-predictions-with-bayesian-inference-6192e31a9fa9/", "webpage": {"metadata": {"title": "Uncertainty Assessment of Predictions with Bayesian Inference | by Georgi Ivanov | Towards Data Science", "h1": "Uncertainty Assessment of Predictions with Bayesian Inference", "description": "Frequentist statistics suffers from several limitations such as lacking uncertainty information in predictions (we usually only model the expectation), no built-in regularization, or no inclusion of\u2026"}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/Frequentist_probability", "anchor_text": "Frequentist statistics", "paragraph_index": 0}, {"url": "https://en.wikipedia.org/wiki/Uncertainty", "anchor_text": "uncertainty information", "paragraph_index": 0}, {"url": "https://en.wikipedia.org/wiki/Regularization_%28mathematics%29", "anchor_text": "regularization", "paragraph_index": 0}, {"url": "https://en.wikipedia.org/wiki/Prior_probability", "anchor_text": "prior", "paragraph_index": 0}, {"url": "https://en.wikipedia.org/wiki/Subject-matter_expert", "anchor_text": "subject-matter expert", "paragraph_index": 0}, {"url": "https://en.wikipedia.org/wiki/Bayesian_probability", "anchor_text": "Bayesian statistics", "paragraph_index": 1}, {"url": "https://en.wikipedia.org/wiki/Bayes%27_theorem", "anchor_text": "Bayes\u2019 theorem", "paragraph_index": 2}, {"url": "https://en.wikipedia.org/wiki/Thomas_Bayes", "anchor_text": "Thomas Bayes", "paragraph_index": 2}, {"url": "https://en.wikipedia.org/wiki/Probability_axioms#Further_consequences", "anchor_text": "sum and product rules", "paragraph_index": 2}, {"url": "https://en.wikipedia.org/wiki/Posterior_probability", "anchor_text": "posterior probability distribution", "paragraph_index": 2}, {"url": "https://en.wikipedia.org/wiki/Marginal_distribution", "anchor_text": "marginal distribution", "paragraph_index": 3}, {"url": "https://en.wikipedia.org/wiki/Probability_distribution", "anchor_text": "probability distributions", "paragraph_index": 3}, {"url": "https://en.wikipedia.org/wiki/Proportionality_(mathematics)", "anchor_text": "proportionality", "paragraph_index": 3}, {"url": "https://en.wikipedia.org/wiki/Pseudo-random_number_sampling", "anchor_text": "pseudo-random number generation", "paragraph_index": 6}, {"url": "https://en.wikipedia.org/wiki/Rejection_sampling", "anchor_text": "rejection sampling", "paragraph_index": 6}, {"url": "https://en.wikipedia.org/wiki/Linear_congruential_generator", "anchor_text": "linear congruential generator", "paragraph_index": 6}, {"url": "https://en.wikipedia.org/wiki/Monte_Carlo_integration", "anchor_text": "stochastic numerical integration", "paragraph_index": 7}, {"url": "https://en.wikipedia.org/wiki/Monte_Carlo_method", "anchor_text": "Monte Carlo methods", "paragraph_index": 7}, {"url": "https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo", "anchor_text": "Markov chain Monte Carlo", "paragraph_index": 8}, {"url": "https://en.wikipedia.org/wiki/Markov_chain", "anchor_text": "Markov chain", "paragraph_index": 8}, {"url": "https://en.wikipedia.org/wiki/Stochastic_process", "anchor_text": "stochastic process", "paragraph_index": 8}, {"url": "https://en.wikipedia.org/wiki/Markov_property", "anchor_text": "Markov property", "paragraph_index": 8}, {"url": "https://en.wikipedia.org/wiki/Gibbs_sampling", "anchor_text": "Gibbs sampling", "paragraph_index": 8}, {"url": "https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm", "anchor_text": "Metropolis-Hastings", "paragraph_index": 8}, {"url": "https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm", "anchor_text": "Metropolis algorithm", "paragraph_index": 11}, {"url": "https://en.wikipedia.org/wiki/Convergence_(logic)", "anchor_text": "convergence", "paragraph_index": 11}, {"url": "https://en.wikipedia.org/wiki/Mixing_(mathematics)", "anchor_text": "mixing", "paragraph_index": 11}, {"url": "http://archive.ics.uci.edu/ml/datasets/wine", "anchor_text": "wine data set", "paragraph_index": 13}, {"url": "https://en.wikipedia.org/wiki/Logistic_regression", "anchor_text": "Logistic regression", "paragraph_index": 13}, {"url": "https://en.wikipedia.org/wiki/Supervised_learning", "anchor_text": "supervised", "paragraph_index": 13}, {"url": "https://en.wikipedia.org/wiki/High-performance_liquid_chromatography", "anchor_text": "high-performance liquid chromatography", "paragraph_index": 14}, {"url": "https://en.wikipedia.org/wiki/Flavonoid", "anchor_text": "flavonoids", "paragraph_index": 14}, {"url": "https://en.wikipedia.org/wiki/Proline", "anchor_text": "proline", "paragraph_index": 14}, {"url": "https://docs.pymc.io/", "anchor_text": "PyMC", "paragraph_index": 15}, {"url": "https://en.wikipedia.org/wiki/Prior_probability#Uninformative_priors", "anchor_text": "non-informative priors", "paragraph_index": 15}, {"url": "https://en.wikipedia.org/wiki/Bootstrap_aggregating", "anchor_text": "ensemble", "paragraph_index": 15}, {"url": "https://en.wikipedia.org/wiki/Autocorrelation", "anchor_text": "autocorrelation", "paragraph_index": 19}, {"url": "https://en.wikipedia.org/wiki/Variational_Bayesian_methods", "anchor_text": "variational Bayes", "paragraph_index": 24}, {"url": "https://en.wikipedia.org/wiki/Variational_Bayesian_methods#Mean_field_approximation", "anchor_text": "mean field approximation", "paragraph_index": 24}, {"url": "https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence", "anchor_text": "Kullback-Leibler divergence", "paragraph_index": 24}], "all_paragraphs": ["Frequentist statistics suffers from several limitations such as lacking uncertainty information in predictions (we usually only model the expectation), no built-in regularization, or no inclusion of prior knowledge. A prior can be available from past information, such as previous experiments, but can also arise from the purely subjective assessment of a trained subject-matter expert.", "Bayesian statistics is an alternative theory in the field of statistics based on the Bayesian interpretation of probability in which probability expresses a degree of belief or information (knowledge) about an event. This differs from the frequentist interpretation that views probability as the limit of the relative frequency of an event after many trials. Many Bayesian methods require a lot of computation to complete, which is the reason why most methods that were widely used during the last century were based on the frequentist interpretation. However, with the availablility of powerful computers and new algorithms, Bayesian methods have seen increasing use within statistics in the 21st century.", "Bayesian statistics revolves around the usage of the Bayes\u2019 theorem, named after Thomas Bayes, as well as sum and product rules of probability to update priors p(y) after more evidence p(x|y) is obtained, resulting in a posterior probability distribution p(y|x).", "The sum over all possible configuartions of variable Y of the distribution p(x,y) = p(x|y)p(y) in Equation 1 results in the marginal distribution of X, i.e., p(x); this calculation is called marginalization. In the case of continuous variables, the sum in the denominator of Equation 1 becomes an integral that is usually intractable. Moreover, p(x) is a normalization constant that scales the posterior distribution so that its integral over the whole space becomes 1 again \u2014 one of the attributes of probability distributions. (However, if normalization is omitted, Equation 1 becomes a proportionality.)", "In principle, any parametric model (linear regression, logistic regression, or even a neural network) can be formulated as corresponding Bayesian version by stating priors over the parameters \u03b2. Using training data (X, y) and Bayes\u2019 theorem, the posterior probability distribution of the parameters is calculated according to", "In general, it may be impossible/impractical to derive the posterior distribution analytically. However, it is possible to obtain numeric results by using dedicated algorithms, which will be explained in the following sections.", "First, a few key concepts need to be introduced, one being pseudo-random number generation. Random numbers are generated via sampling algorithms, e.g., rejection sampling. The aim of these algorithms is to generate numbers that, as a set, follow some specified distribution, e.g., Gaussian. However, rejection sampling relies on a simpler building block that generates uniform distributed random numbers in the interval between zero and one (U(0,1)), e.g., a linear congruential generator. Rejection sampling works well for very simple distributions with few dimensions; in high dimensions, the probability mass is concentrated within a small volume, leading to a lot of samples being rejected (which is inefficient). For such scenarios, more advanced algorithms have been developed; however, they still make use of the simpler sampling algorithms such as the ones mentioned above.", "Another important concept is stochastic numerical integration using Monte Carlo methods (or Monte Carlo experiments). Monte Carlo methods are a broad class of computational algorithms that rely on the generation of random numbers to obtain numerical results. For instance, by generating random samples from a desired distribution, the integral in the denominator of Equation 2 becomes a regular expectation (the maximum likelihood estimate for the mean) whose accuracy is proportional to the number of samples drawn.", "A class of sampling algorithms, known as Markov chain Monte Carlo, has been developed to draw samples from complex probability distributions. A Markov chain is a sequence of possible events (outcomes) in which the probability of each event depends only on the state attained in the previous event. It is the result of a stochastic process that satisfies the Markov property (a Markov process), i.e., the conditional probability distribution of future states of the process depends only upon the most present state \u2014 not on the sequence of events that preceded it. Within the class of Markov chain Monte Carlo algorithms, there are several variants, e.g., Gibbs sampling or Metropolis-Hastings; one of the early variants, the Metropolis algorithm, will be described briefly in the next section, since it is very comprehensible.", "The goal in Bayesian computation is to obtain a set of independent draws from the posterior distribution to estimate quantities of interest with reasonable accuracy. More precisely, such random draws are used to summarize the posterior distribution of the model parameters, e.g., by reporting the 2.5%, 25%, 50%, 75%, and 97.5% points of the sampled distribution. However, the draws can also be visualized with scatter plots or histograms.", "Finally, once the posterior distribution of the model parameters has been simulated, predictions for new samples can be performed by marginalizing the parameters from the conditional posterior distribution. Equation 3 is essentially another integral that can be calculated by random sampling using numerical methods.", "As mentioned above, drawing samples from a multi-dimensional (possibly not normalized) probability distribution p(z) is challenging. The idea of sampling algorithms is to draw samples iteratively from a simpler proposal distribution, e.g., a product of one-dimensional Gaussians. One such Monte Carlo method is known as Metropolis algorithm, which generates a Markov chain and eventually converges to the desired distribution p(z). More presicely, random numbers are generated using the proposal distribution, and new samples are either accepted or discarded depending on whether they are more or less likely than the previous sample \u2014 and only the previous sample (hence the Markov property). In practice, due to increased computational power, several chains are simulated in parallel to check for convergence and mixing.", "A proposal distribution with low variance leads to small step size and highly correlated samples; on the opposite side, high variance leads to larger step size but lower acceptance rate, so many samples will be rejected slowing down the algorithm. For this reason, more advanced algorithms have been developed that account for this. For more theory on this topic, the book \u201cBayesian Data Analysis\u201d by Andrew Gelman, which is available as PDF-file for free, can be recommended.", "Let\u2019s look at a specific example using the wine data set. Logistic regression is an algorithm that models the effects of variables on a binary class label (y=0, y=1), i.e., solving supervised classification problems. The posterior probability distribution gives the interval estimates for each weight or output of the model. It is essential in data analysis to not only provide a good model but also an uncertainty estimate of the conclusions.", "We have a collection of wine bottles; we want to develop a classification model for one specific type of wine which is often imitated due to its high price. Furthermore, we want to act only when we\u2019re (not) confident enough that it\u2019s (not) an imitation. To characterize a wine, we use its chemical composition (determined by high-performance liquid chromatography) with respect to flavonoids and proline.", "A linear model for the posterior probability distribution p(y=1|x, \u03b2) is developed and inferred by sampling using the Python library PyMC. From the figure above, it can be already guessed that a linear decision boundary might be sufficient. Furthermore, since there isn\u2019t any prior knowledge yet, we have to set non-informative priors, which is comparable to an ensemble of logistic regressions. Note that the variables are not standardized before analysis, hence the magnitude of the parameters won\u2019t have any meaning.", "The resulting chains is given in the following plots, showing convergence and mixing, but the relevant statistics should be evaluated. Note that the total amount of samples per chain (usually four in total) is 3'000, but the first third of each chain is discarded.", "To evaluate the success of the sampling algorithm, several statistics have been developed. The table below is a summary of the inference procedure.", "The most important statistic is the R-hat, which is a diagnostic that tests for lack of convergence by comparing the variance between multiple chains to the variance within each chain. If convergence has been achieved, the between-chain and within-chain variances should be identical. To be most effective in detecting evidence for non-convergence, each chain should have been initialized to starting values that are dispersed relative to the target distribution. In this case, R-hat is 1, hence convergence and mixing are achieved.", "Another statistic is the effective sample size, which is basically the total amount of samples (samples times number of chains) corrected by the amount of autocorrelation within the sequences, i.e., dividing the total amount by a correction factor. If this metric is (too) low, more samples might be needed.", "Let\u2019s have a look at our estimated parameters. The 94% Bayesian credible interval does not include zero, so we seem to be confident that both variables are relevant for the model.", "Let\u2019s look at the partial dependence of the variables. Both of them increase the probability with increasing value. However, there is more uncertainty in the flavanoids content, as the dispersion of the lines is larger.", "Now, we have two new bottles of wine. How sure can we be that it\u2019s (or it\u2019s not) the wine we\u2019re looking for? Let\u2019s do a prediction and evalue posterior distributions.", "The first bottle has a 94% credible interval of [0.90, 1.00], and the second one has [0.50, 1.00]. For the second one, we\u2019re less confident, and in fact, the model has never seen such a bottle like this during inference.", "It is worth mentioning that there is another approach to the whole problem called variational Bayes in which the posterior is approximated with some simpler parametric distribution, a so-called variational distribution, from which integrals can be computed directly. This distribution is usually formulated as a product of the single univariate probability distributions (mean field approximation), and its parameters are found by minimizing the Kullback-Leibler divergence between the true posterior and the variational distribution. Variational methods are less accurate, but much faster, and they can serve as starting point for simulation-based methods.", "Bayesian statistics is a powerful framework that addresses issues which frequentist approaches can\u2019t solve such as incertainty in predictions. Such uncertainty arises to due insufficient training data or anomalous samples in test data. Furthermore, it is a form of quality control, and with libraries such as PyMC3 and TensorFlow Probability, these methods have become accessible to everyone.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F6192e31a9fa9&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funcertainty-quantification-of-predictions-with-bayesian-inference-6192e31a9fa9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funcertainty-quantification-of-predictions-with-bayesian-inference-6192e31a9fa9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funcertainty-quantification-of-predictions-with-bayesian-inference-6192e31a9fa9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funcertainty-quantification-of-predictions-with-bayesian-inference-6192e31a9fa9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----6192e31a9fa9--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----6192e31a9fa9--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://g-ivanov.medium.com/?source=post_page-----6192e31a9fa9--------------------------------", "anchor_text": ""}, {"url": "https://g-ivanov.medium.com/?source=post_page-----6192e31a9fa9--------------------------------", "anchor_text": "Georgi Ivanov"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F54224776d918&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funcertainty-quantification-of-predictions-with-bayesian-inference-6192e31a9fa9&user=Georgi+Ivanov&userId=54224776d918&source=post_page-54224776d918----6192e31a9fa9---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6192e31a9fa9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funcertainty-quantification-of-predictions-with-bayesian-inference-6192e31a9fa9&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6192e31a9fa9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funcertainty-quantification-of-predictions-with-bayesian-inference-6192e31a9fa9&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@qwitka?utm_source=medium&utm_medium=referral", "anchor_text": "Maksym Kaharlytskyi"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://en.wikipedia.org/wiki/Frequentist_probability", "anchor_text": "Frequentist statistics"}, {"url": "https://en.wikipedia.org/wiki/Uncertainty", "anchor_text": "uncertainty information"}, {"url": "https://en.wikipedia.org/wiki/Regularization_%28mathematics%29", "anchor_text": "regularization"}, {"url": "https://en.wikipedia.org/wiki/Prior_probability", "anchor_text": "prior"}, {"url": "https://en.wikipedia.org/wiki/Subject-matter_expert", "anchor_text": "subject-matter expert"}, {"url": "https://en.wikipedia.org/wiki/Bayesian_probability", "anchor_text": "Bayesian statistics"}, {"url": "https://en.wikipedia.org/wiki/Bayes%27_theorem", "anchor_text": "Bayes\u2019 theorem"}, {"url": "https://en.wikipedia.org/wiki/Thomas_Bayes", "anchor_text": "Thomas Bayes"}, {"url": "https://en.wikipedia.org/wiki/Probability_axioms#Further_consequences", "anchor_text": "sum and product rules"}, {"url": "https://en.wikipedia.org/wiki/Posterior_probability", "anchor_text": "posterior probability distribution"}, {"url": "https://en.wikipedia.org/wiki/Marginal_distribution", "anchor_text": "marginal distribution"}, {"url": "https://en.wikipedia.org/wiki/Probability_distribution", "anchor_text": "probability distributions"}, {"url": "https://en.wikipedia.org/wiki/Proportionality_(mathematics)", "anchor_text": "proportionality"}, {"url": "https://en.wikipedia.org/wiki/Pseudo-random_number_sampling", "anchor_text": "pseudo-random number generation"}, {"url": "https://en.wikipedia.org/wiki/Rejection_sampling", "anchor_text": "rejection sampling"}, {"url": "https://en.wikipedia.org/wiki/Linear_congruential_generator", "anchor_text": "linear congruential generator"}, {"url": "https://en.wikipedia.org/wiki/Monte_Carlo_integration", "anchor_text": "stochastic numerical integration"}, {"url": "https://en.wikipedia.org/wiki/Monte_Carlo_method", "anchor_text": "Monte Carlo methods"}, {"url": "https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo", "anchor_text": "Markov chain Monte Carlo"}, {"url": "https://en.wikipedia.org/wiki/Markov_chain", "anchor_text": "Markov chain"}, {"url": "https://en.wikipedia.org/wiki/Stochastic_process", "anchor_text": "stochastic process"}, {"url": "https://en.wikipedia.org/wiki/Markov_property", "anchor_text": "Markov property"}, {"url": "https://en.wikipedia.org/wiki/Gibbs_sampling", "anchor_text": "Gibbs sampling"}, {"url": "https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm", "anchor_text": "Metropolis-Hastings"}, {"url": "https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm", "anchor_text": "Metropolis algorithm"}, {"url": "https://en.wikipedia.org/wiki/Convergence_(logic)", "anchor_text": "convergence"}, {"url": "https://en.wikipedia.org/wiki/Mixing_(mathematics)", "anchor_text": "mixing"}, {"url": "http://www.stat.columbia.edu/~gelman/book/", "anchor_text": "Home page for the book, \"Bayesian Data Analysis\"Here is the book in pdf form, available for download for non-commercial purposes.www.stat.columbia.edu"}, {"url": "http://archive.ics.uci.edu/ml/datasets/wine", "anchor_text": "wine data set"}, {"url": "https://en.wikipedia.org/wiki/Logistic_regression", "anchor_text": "Logistic regression"}, {"url": "https://en.wikipedia.org/wiki/Supervised_learning", "anchor_text": "supervised"}, {"url": "https://en.wikipedia.org/wiki/High-performance_liquid_chromatography", "anchor_text": "high-performance liquid chromatography"}, {"url": "https://en.wikipedia.org/wiki/Flavonoid", "anchor_text": "flavonoids"}, {"url": "https://en.wikipedia.org/wiki/Proline", "anchor_text": "proline"}, {"url": "https://docs.pymc.io/", "anchor_text": "PyMC"}, {"url": "https://en.wikipedia.org/wiki/Prior_probability#Uninformative_priors", "anchor_text": "non-informative priors"}, {"url": "https://en.wikipedia.org/wiki/Bootstrap_aggregating", "anchor_text": "ensemble"}, {"url": "https://en.wikipedia.org/wiki/Autocorrelation", "anchor_text": "autocorrelation"}, {"url": "https://en.wikipedia.org/wiki/Variational_Bayesian_methods", "anchor_text": "variational Bayes"}, {"url": "https://en.wikipedia.org/wiki/Variational_Bayesian_methods#Mean_field_approximation", "anchor_text": "mean field approximation"}, {"url": "https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence", "anchor_text": "Kullback-Leibler divergence"}, {"url": "https://towardsdatascience.com/bayesian-neural-networks-with-tensorflow-probability-fbce27d6ef6", "anchor_text": "Bayesian Neural Networks with TensorFlow ProbabilityA step by step guide to uncertainty prediction with probabilistic modeling.towardsdatascience.com"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----6192e31a9fa9---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----6192e31a9fa9---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/bayesian-statistics?source=post_page-----6192e31a9fa9---------------bayesian_statistics-----------------", "anchor_text": "Bayesian Statistics"}, {"url": "https://medium.com/tag/statistics?source=post_page-----6192e31a9fa9---------------statistics-----------------", "anchor_text": "Statistics"}, {"url": "https://medium.com/tag/data-visualization?source=post_page-----6192e31a9fa9---------------data_visualization-----------------", "anchor_text": "Data Visualization"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6192e31a9fa9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funcertainty-quantification-of-predictions-with-bayesian-inference-6192e31a9fa9&user=Georgi+Ivanov&userId=54224776d918&source=-----6192e31a9fa9---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6192e31a9fa9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funcertainty-quantification-of-predictions-with-bayesian-inference-6192e31a9fa9&user=Georgi+Ivanov&userId=54224776d918&source=-----6192e31a9fa9---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6192e31a9fa9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funcertainty-quantification-of-predictions-with-bayesian-inference-6192e31a9fa9&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----6192e31a9fa9--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F6192e31a9fa9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funcertainty-quantification-of-predictions-with-bayesian-inference-6192e31a9fa9&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----6192e31a9fa9---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----6192e31a9fa9--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----6192e31a9fa9--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----6192e31a9fa9--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----6192e31a9fa9--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----6192e31a9fa9--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----6192e31a9fa9--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----6192e31a9fa9--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----6192e31a9fa9--------------------------------", "anchor_text": ""}, {"url": "https://g-ivanov.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://g-ivanov.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Georgi Ivanov"}, {"url": "https://g-ivanov.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "325 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F54224776d918&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funcertainty-quantification-of-predictions-with-bayesian-inference-6192e31a9fa9&user=Georgi+Ivanov&userId=54224776d918&source=post_page-54224776d918--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F4e7dcd411990&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funcertainty-quantification-of-predictions-with-bayesian-inference-6192e31a9fa9&newsletterV3=54224776d918&newsletterV3Id=4e7dcd411990&user=Georgi+Ivanov&userId=54224776d918&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}