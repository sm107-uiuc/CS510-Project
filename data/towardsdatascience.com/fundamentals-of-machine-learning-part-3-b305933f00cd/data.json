{"url": "https://towardsdatascience.com/fundamentals-of-machine-learning-part-3-b305933f00cd", "time": 1682995268.213967, "path": "towardsdatascience.com/fundamentals-of-machine-learning-part-3-b305933f00cd/", "webpage": {"metadata": {"title": "Fundamentals of Machine Learning (Part 3) | by William Fleshman | Towards Data Science", "h1": "Fundamentals of Machine Learning (Part 3)", "description": "The purpose of this post is to cover important concepts from Information Theory and describe how they are used in machine learning."}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/probability-fundamentals-of-machine-learning-part-1-a156b4703e69", "anchor_text": "Part 1", "paragraph_index": 6}, {"url": "https://towardsdatascience.com/probability-fundamentals-of-machine-learning-part-1-a156b4703e69", "anchor_text": "Part 1", "paragraph_index": 11}, {"url": "https://towardsdatascience.com/maximum-likelihood-estimation-984af2dcfcac", "anchor_text": "Part 2", "paragraph_index": 22}, {"url": "https://www.microsoft.com/en-us/research/people/cmbishop/#!prml-book", "anchor_text": "Pattern Recognition and Machine Learning", "paragraph_index": 32}, {"url": "https://www.inference.org.uk/itprnn/book.pdf", "anchor_text": "Information Theory, Inference, and Learning Algorithms", "paragraph_index": 33}], "all_paragraphs": ["This is part 3 in a series on topics I consider fundamental to machine learning. These topics act as building blocks for gaining a deeper understanding of the field. So far we have covered:", "The purpose of this post is to cover important concepts from Information Theory and describe how they are used in machine learning. Many of the topics here will build on concepts we discussed in the post on Probability Theory, such as independence and expectation.", "I\u2019ll be motivating and constructing these concepts using ideas from machine learning and statistics. If you have a background in Physics, you may have seen these concepts motivated differently (i.e. through thermodynamics).", "Let\u2019s motivate our discussion by assuming our goal is to detect the author of a given text by the words that are used. Which words are useful for detecting authorship? Intuitively, words like \u201cthe\u201d, \u201cor\u201d, and \u201cit\u201d aren\u2019t going to be very useful, because those words have high probability of showing up in any text regardless of author. It seems that common words contain less information than rare words, and that information content is somehow directly related to probability.", "Let\u2019s make this more formal by letting x be a random variable representing all possible words, and p(x) be the probability of observing a specific word x. We\u2019d like to create some quantifiable notion for the information gained by observing a specific word.", "We\u2019ll use h(x) to represent the information gained from observing word x. Similarly, for two words x and y we could write the information gained by observing them together as h(x, y). Our goal is to find a suitable h.", "First, let\u2019s consider the independence of two words x and y. Words like \u201cfootball\u201d and \u201cscore\u201d are not independent, as seeing one makes the other more likely. Therefore, the information gained from seeing \u201cscore\u201d after \u201cfootball\u201d should be small. Recall from Part 1, that independence formally means that p(x, y) = p(x)p(y). If two words x and y are unrelated, we want h(x, y) = h(x) + h(y). In other words, the information gained from seeing two unrelated words together should be the sum of the information gained by seeing them both alone.", "We want our formula for h to be related to p, but we have this issue where the joint probability of independent observations is a product, yet the joint information is a sum. To solve this issue, we use the product rule of logarithms to turn the product into a sum:", "Relating the information to the log of the probability seems to be the trick we needed. We write the information associated with x as:", "This formula meets our requirements that the more probable observations yield less information, and for two independent observations x and y we have h(x, y) = -log p(x, y) = -log [p(x)p(y)] = [-log p(x)] + [-log p(y)] = h(x) + h(y).", "The base of the logarithm is an arbitrary choice. Information theorists generally use base 2 and call the units of information \u201cbits\u201d, whereas in machine learning it is common to use base e and call the units of information \u201cnats\u201d. An alternative view of this information quantity is that h(x) gives us the number of bits (if base 2) required to send a message x given some encoding.", "We\u2019ve just written down information as a function of a random variable. In Part 1, we learned that the expectation of a function g on a random variable x, gives us the expected value (or average) of g:", "The expectation of our information function h is one of the most fundamental concepts in information theory. This expectation is called the entropy of the random variable x and is often represented with a capital H:", "The entropy is the expected value of the information function, or in other words, it\u2019s the average amount of information gained by observing a random draw from p(x). This is a useful value to know!", "An information theorist might use the entropy to calculate the expected number of bits needed to transmit a message of a given length. Better yet, to reduce transmission cost, it would be beneficial to find an encoding of messages such that the entropy is minimized. This would allow a transmitter to compress messages and send fewer bits per message.", "Finding an efficient encoding is similar to finding an efficient representation of features for a machine learning problem. We\u2019d like to represent our data with as few features as possible (to avoid overfitting and the curse of dimensionality) while still maintaining enough information to make accurate decisions.", "We\u2019ll see that this notion of entropy will be useful for thinking about several other important ideas.", "Let\u2019s go back to our author classification problem. Let\u2019s simplify the problem by considering only two classes: {text is from author A, text is not from author A}. We\u2019ll treat each document we\u2019re trying to classify as a \u201cbag of words\u201d, meaning we don\u2019t care about the order of the words, just which ones are present. We represent each document as a vector x where entry j is a 1 if word j is present in the document, and 0 otherwise.", "We assume there is some ground truth distribution p(x) which gives us the probability that document x was written by author A. If we can learn this distribution, then for any document where p(x) > 0.5, we can guess that the document was written by A.", "But we don\u2019t know p, so instead we write down a model q and try to fit the parameters of q so that it approximates p. Then we\u2019ll use q to make decisions about whether or not A is the author of any given text.", "Since we\u2019re using q, the amount of information needed for classifying a document x is h(x) = -log q(x). But if x is actually distributed by p, then the expectation of the information needed with respect to x is:", "This is the cross entropy of q with respect to a true distribution p. If we use this as a cost function and find the parameters of q which minimize the cross entropy, then the best solution will occur when q is identical to p.", "Minimizing cross entropy is often the objective for logistic regression and neural networks used for classification. It can be shown that minimizing the cross entropy is equivalent to maximizing the likelihood of our model q. Therefore, this approach to solving classification finds the Maximum Likelihood Estimator as discussed in Part 2.", "The cross entropy gives us the average amount of information needed to classify our documents given that we\u2019re using q instead of p. Obviously, if we\u2019re using p, then on average we\u2019d only need H(x) amount of bits (the entropy of the true distribution). But since we\u2019re using q it takes H(p, q) bits on average (the cross entropy).", "How much worse is using q? Using q instead of p requires H(p, q)-H(x) extra bits. We call this value the relative entropy or Kullback-Leibler (KL) divergence. If we plug in the equations for cross entropy and entropy and then collect terms, we get that KL-Divergence is:", "Again, this gives us the extra number of bits (or nats) needed given that we\u2019re using q in place of p. The KL-Divergence is always greater than 0 unless q equals p. Therefore, it\u2019s also a common objective function to minimize. Minimizing the KL-Divergence also minimizes the cross entropy, which we\u2019ve already said is the Maximum Likelihood Estimator.", "We discussed earlier that some words aren\u2019t independent of each other, and that the dependence causes us to get redundant information when we observe them both (\u201cfootball\u201d and \u201cscore\u201d for example). We\u2019d like some measure of how much information two variables share.", "Recall, that if x and y are independent variables, then their joint distribution is p(x, y) = p(x)p(y). If they aren\u2019t independent, then we can\u2019t factor the joint distribution in this way. We don\u2019t have any redundancy when the variables are independent, and as the variables get more dependent, the amount of redundant information increases. To quantify this, we use the mutual information of x and y:", "The mutual information is the amount of extra information needed if we\u2019re representing the true joint distribution with the independent factorization. If the variables are independent, then the KL-Divergence will be 0, otherwise it will grow as the variables increase in redundancy.", "Mutual information has been frequently used to perform feature selection in machine learning. For a given feature, we can measure the feature\u2019s mutual information with the class labels. If the mutual information is high, then the feature is a strong indicator of the class. For example, if author A always includes their name in their documents, then the mutual information between their name and the class will be extremely high.", "Similarly, if we have too many features to consider, we can use mutual information between features to remove those that are redundant. If author A always includes their name and their home town, then we can safely remove their home town from our vocabulary and still perform well on the task.", "In this post, we\u2019ve covered the main concepts from Information Theory which are directly applicable to machine learning. This hasn\u2019t been an exhaustive treatment in any sense, but these are concepts that I\u2019ve personally seen come up time and time again in practice.", "For more reading, I suggest checking out Christopher Bishop\u2019s book Pattern Recognition and Machine Learning. The entire book is a gold mine of knowledge, but the concepts I\u2019ve discussed can all be found in Chapter 1 under the information theory section.", "David MacKay\u2019s book Information Theory, Inference, and Learning Algorithms is also very popular.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fb305933f00cd&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffundamentals-of-machine-learning-part-3-b305933f00cd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffundamentals-of-machine-learning-part-3-b305933f00cd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffundamentals-of-machine-learning-part-3-b305933f00cd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffundamentals-of-machine-learning-part-3-b305933f00cd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----b305933f00cd--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b305933f00cd--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@william.fleshman?source=post_page-----b305933f00cd--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@william.fleshman?source=post_page-----b305933f00cd--------------------------------", "anchor_text": "William Fleshman"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F797753643492&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffundamentals-of-machine-learning-part-3-b305933f00cd&user=William+Fleshman&userId=797753643492&source=post_page-797753643492----b305933f00cd---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb305933f00cd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffundamentals-of-machine-learning-part-3-b305933f00cd&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb305933f00cd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffundamentals-of-machine-learning-part-3-b305933f00cd&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/probability-fundamentals-of-machine-learning-part-1-a156b4703e69", "anchor_text": "Probability Theory"}, {"url": "https://towardsdatascience.com/maximum-likelihood-estimation-984af2dcfcac", "anchor_text": "Maximum Likelihood Estimation"}, {"url": "https://towardsdatascience.com/probability-fundamentals-of-machine-learning-part-1-a156b4703e69", "anchor_text": "Part 1"}, {"url": "https://towardsdatascience.com/probability-fundamentals-of-machine-learning-part-1-a156b4703e69", "anchor_text": "Part 1"}, {"url": "https://towardsdatascience.com/maximum-likelihood-estimation-984af2dcfcac", "anchor_text": "Part 2"}, {"url": "https://www.microsoft.com/en-us/research/people/cmbishop/#!prml-book", "anchor_text": "Pattern Recognition and Machine Learning"}, {"url": "https://www.inference.org.uk/itprnn/book.pdf", "anchor_text": "Information Theory, Inference, and Learning Algorithms"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----b305933f00cd---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/information-theory?source=post_page-----b305933f00cd---------------information_theory-----------------", "anchor_text": "Information Theory"}, {"url": "https://medium.com/tag/data-science?source=post_page-----b305933f00cd---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/william-fleshman?source=post_page-----b305933f00cd---------------william_fleshman-----------------", "anchor_text": "William Fleshman"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb305933f00cd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffundamentals-of-machine-learning-part-3-b305933f00cd&user=William+Fleshman&userId=797753643492&source=-----b305933f00cd---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb305933f00cd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffundamentals-of-machine-learning-part-3-b305933f00cd&user=William+Fleshman&userId=797753643492&source=-----b305933f00cd---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb305933f00cd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffundamentals-of-machine-learning-part-3-b305933f00cd&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b305933f00cd--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fb305933f00cd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffundamentals-of-machine-learning-part-3-b305933f00cd&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----b305933f00cd---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----b305933f00cd--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----b305933f00cd--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----b305933f00cd--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----b305933f00cd--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----b305933f00cd--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----b305933f00cd--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----b305933f00cd--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----b305933f00cd--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@william.fleshman?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@william.fleshman?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "William Fleshman"}, {"url": "https://medium.com/@william.fleshman/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "865 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F797753643492&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffundamentals-of-machine-learning-part-3-b305933f00cd&user=William+Fleshman&userId=797753643492&source=post_page-797753643492--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fbf0fad40b9b5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffundamentals-of-machine-learning-part-3-b305933f00cd&newsletterV3=797753643492&newsletterV3Id=bf0fad40b9b5&user=William+Fleshman&userId=797753643492&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}