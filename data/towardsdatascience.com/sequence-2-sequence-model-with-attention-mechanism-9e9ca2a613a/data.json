{"url": "https://towardsdatascience.com/sequence-2-sequence-model-with-attention-mechanism-9e9ca2a613a", "time": 1683003081.9476962, "path": "towardsdatascience.com/sequence-2-sequence-model-with-attention-mechanism-9e9ca2a613a/", "webpage": {"metadata": {"title": "Attention: Sequence 2 Sequence model with Attention Mechanism | by Renu Khandelwal | Towards Data Science", "h1": "Attention: Sequence 2 Sequence model with Attention Mechanism", "description": "Let\u2019s consider two scenarios, scenario one, where you are reading an article related to the current news. The second scenario where you are preparing for a test. Is the level of attention the same or\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/datadriveninvestor/recurrent-neural-network-rnn-52dd4f01b7e8", "anchor_text": "Recurrent neural networks(RNN) like LSTM and GRU", "paragraph_index": 1}, {"url": "https://arxiv.org/pdf/1409.0473.pdf", "anchor_text": "Neural machine translation by jointly learning to align and translate Dzmitry Bahdanau", "paragraph_index": 38}, {"url": "https://arxiv.org/pdf/1508.04025.pdf", "anchor_text": "Effective Approaches to Attention-based Neural Machine Translation: Minh-Thang Luong Hieu Pham Christopher D. Manning", "paragraph_index": 39}], "all_paragraphs": ["In this article, you will learn:", "Recurrent neural networks(RNN) like LSTM and GRU", "Let\u2019s consider two scenarios, scenario one, where you are reading an article related to the current news. The second scenario where you are preparing for a test. Is the level of attention the same or different in both situations?", "You will be reading with considerable attention when preparing for the test compared to the news article. While preparing for the test, you will learn with a greater focus on keywords to help you remember a simple or a complex concept. The same implies to any deep learning task where we want to focus on a particular area of interest.", "Sequence to Sequence(Seq2Seq) models uses encoder-decoder architecture.", "A few use cases for seq2seq", "Seq2Seq model maps a source sequence to the target sequence. The source sequence in the case of neural machine translation could be English, and the target sequence can be Hindi.", "We pass a source sentence in English to an encoder; the encoder encodes the complete information of the source sequence into a single real-valued vector, also known as the context vector. This context vector is then passed on the decoder to produce an output sequence in a target language like Hindi. The context vector has the responsibility to summarize the entire input sequence into a single vector.", "What if the input sentence is long, can a single vector from the encoder hold all the relevant information to provide to the decoder?", "Is it possible to focus on a few relevant words in a sentence when predicting the target word rather than a single vector holding the information about the entire sentence?", "Attention mechanisms help solve the problem.", "The basic idea of Attention mechanism is to avoid attempting to learn a single vector representation for each sentence, instead, it pays attention to specific input vectors of the input sequence based on the attention weights.", "At every decoding step, the decoder will be informed how much \u201cattention\u201d needs to be paid to each input word using a set of attention weights. These attention weights provide contextual information to the decoder for translation", "Bahdanau et al. proposed an attention mechanism that learns to align and translate jointly. It is also known as Additive attention as it performs a linear combination of encoder states and the decoder states.", "let\u2019s understand the Attention mechanism suggested by Bahdanau", "Seq2Seq model with an attention mechanism consists of an encoder, decoder, and attention layer.", "The alignment score maps how well the inputs around position \u201cj\u201d and the output at position \u201ci\u201d match. The score is based on the previous decoder\u2019s hidden state, s\u208d\u1d62\u208b\u2081\u208e just before predicting the target word and the hidden state, h\u2c7c of the input sentence", "The decoder decides which part of the source sentence it needs to pay attention to, instead of having encoder encode all the information of the source sentence into a fixed-length vector.", "The alignment vector that has the same length with the source sequence and is computed at every time step of the decoder", "In our example, to predict the second target word, \u0924\u0947\u091c\u093c\u0940, we will generate a high score for the input word quickly", "We apply a softmax activation function to the alignment scores to obtain the attention weights.", "Softmax activation function will get the probabilities whose sum will be equal to 1, This will help to represent the weight of influence for each of the input sequence. Higher the attention weight of the input sequence, the higher will be its influence on predicting the target word.", "In our example, we see a higher attention weight value for the input word quickly to predict the target word, \u0924\u0947\u091c\u093c\u0940", "The context vector is used to compute the final output of the decoder. The context vector \ud835\udcb8\u1d62 is the weighted sum of attention weights and the encoder hidden states (h\u2081, h\u2082, \u2026,h\u209c\u2093), which maps to the input sentence.", "To predict the target word, the decoder uses", "Luong\u2019s attention is also referred to as Multiplicative attention. It reduces encoder states and decoder state into attention scores by simple matrix multiplications. Simple matrix multiplication makes it is faster and more space-efficient.", "Luong suggested two types of attention mechanism based on where the attention is placed in the source sequence", "The commonality between Global and Local attention", "Global and local attention models differ in how the context vector \ud835\udcb8\u209c is derived", "Before we discuss the global and local attention, let\u2019s understand the conventions used by Luong\u2019s attention mechanism for any given time t", "What happens when the source sequence is a large paragraph or a big document?", "As Global attention model considers all the words of the source sequence to predict the target wors, it becomes computationally expensive and can be challenging to translate longer sentences", "We can solve this drawback of global attention model by using Local attention", "Bahdanau et al. uses the concatenation of the forward and backward hidden states in the bi-directional encoder and previous target\u2019s hidden states in their non-stacking unidirectional decoder", "Loung et al. attention uses hidden states at the top LSTM layers in both the encoder and decoder", "Luong attention mechanism uses the current decoder\u2019s hidden state to compute the alignment vector, whereas Bahdanau uses the output of the previous time step", "Bahdanau uses only concat score alignment model whereas Luong uses dot, general and concat alignment score models", "With the knowledge of attention mechanism, you can now build powerful deep NLP algorithms.", "Neural machine translation by jointly learning to align and translate Dzmitry Bahdanau", "Effective Approaches to Attention-based Neural Machine Translation: Minh-Thang Luong Hieu Pham Christopher D. Manning", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "A Technology Enthusiast who constantly seeks out new challenges by exploring cutting-edge technologies to make the world a better place!"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F9e9ca2a613a&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsequence-2-sequence-model-with-attention-mechanism-9e9ca2a613a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsequence-2-sequence-model-with-attention-mechanism-9e9ca2a613a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsequence-2-sequence-model-with-attention-mechanism-9e9ca2a613a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsequence-2-sequence-model-with-attention-mechanism-9e9ca2a613a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----9e9ca2a613a--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----9e9ca2a613a--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://arshren.medium.com/?source=post_page-----9e9ca2a613a--------------------------------", "anchor_text": ""}, {"url": "https://arshren.medium.com/?source=post_page-----9e9ca2a613a--------------------------------", "anchor_text": "Renu Khandelwal"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F31b07253bc35&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsequence-2-sequence-model-with-attention-mechanism-9e9ca2a613a&user=Renu+Khandelwal&userId=31b07253bc35&source=post_page-31b07253bc35----9e9ca2a613a---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9e9ca2a613a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsequence-2-sequence-model-with-attention-mechanism-9e9ca2a613a&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9e9ca2a613a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsequence-2-sequence-model-with-attention-mechanism-9e9ca2a613a&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/datadriveninvestor/recurrent-neural-network-rnn-52dd4f01b7e8", "anchor_text": "Recurrent neural networks(RNN) like LSTM and GRU"}, {"url": "https://towardsdatascience.com/intuitive-explanation-of-neural-machine-translation-129789e3c59f", "anchor_text": "Seq2Seq- Neural machine translation"}, {"url": "https://arxiv.org/pdf/1508.04025.pdf", "anchor_text": "Effective Approaches to Attention-based Neural Machine Translation"}, {"url": "https://arxiv.org/pdf/1508.04025.pdf", "anchor_text": "Effective Approaches to Attention-based Neural Machine Translation"}, {"url": "https://arxiv.org/pdf/1409.0473.pdf", "anchor_text": "Neural machine translation by jointly learning to align and translate Dzmitry Bahdanau"}, {"url": "https://arxiv.org/pdf/1508.04025.pdf", "anchor_text": "Effective Approaches to Attention-based Neural Machine Translation: Minh-Thang Luong Hieu Pham Christopher D. Manning"}, {"url": "https://devopedia.org/attention-mechanism-in-neural-networks", "anchor_text": "Attention Mechanism in Neural NetworksIn machine translation, the encoder-decoder architecture is common. The encoder reads a sequence of words and\u2026devopedia.org"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----9e9ca2a613a---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/nlp?source=post_page-----9e9ca2a613a---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/seq2seq?source=post_page-----9e9ca2a613a---------------seq2seq-----------------", "anchor_text": "Seq2seq"}, {"url": "https://medium.com/tag/attention-mechanism?source=post_page-----9e9ca2a613a---------------attention_mechanism-----------------", "anchor_text": "Attention Mechanism"}, {"url": "https://medium.com/tag/machine-translation?source=post_page-----9e9ca2a613a---------------machine_translation-----------------", "anchor_text": "Machine Translation"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9e9ca2a613a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsequence-2-sequence-model-with-attention-mechanism-9e9ca2a613a&user=Renu+Khandelwal&userId=31b07253bc35&source=-----9e9ca2a613a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9e9ca2a613a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsequence-2-sequence-model-with-attention-mechanism-9e9ca2a613a&user=Renu+Khandelwal&userId=31b07253bc35&source=-----9e9ca2a613a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9e9ca2a613a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsequence-2-sequence-model-with-attention-mechanism-9e9ca2a613a&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----9e9ca2a613a--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F9e9ca2a613a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsequence-2-sequence-model-with-attention-mechanism-9e9ca2a613a&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----9e9ca2a613a---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----9e9ca2a613a--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----9e9ca2a613a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----9e9ca2a613a--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----9e9ca2a613a--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----9e9ca2a613a--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----9e9ca2a613a--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----9e9ca2a613a--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----9e9ca2a613a--------------------------------", "anchor_text": ""}, {"url": "https://arshren.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://arshren.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Renu Khandelwal"}, {"url": "https://arshren.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "5.9K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F31b07253bc35&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsequence-2-sequence-model-with-attention-mechanism-9e9ca2a613a&user=Renu+Khandelwal&userId=31b07253bc35&source=post_page-31b07253bc35--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fb1cb44d62203&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsequence-2-sequence-model-with-attention-mechanism-9e9ca2a613a&newsletterV3=31b07253bc35&newsletterV3Id=b1cb44d62203&user=Renu+Khandelwal&userId=31b07253bc35&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}