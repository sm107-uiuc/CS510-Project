{"url": "https://towardsdatascience.com/training-weakly-labelled-multilabel-data-with-google-bert-classifying-documents-from-westeros-a2dda03e506e", "time": 1682996400.8314571, "path": "towardsdatascience.com/training-weakly-labelled-multilabel-data-with-google-bert-classifying-documents-from-westeros-a2dda03e506e/", "webpage": {"metadata": {"title": "Training weakly-labelled, multilabel data with Google BERT: Classifying documents from Westeros and Essos | by Alvin Leong | Towards Data Science", "h1": "Training weakly-labelled, multilabel data with Google BERT: Classifying documents from Westeros and Essos", "description": "Papers on text classification in the context of NLP mostly deal with neatly labelled data which fits only into one class (single-label multiclass classification problems; e.g. an image either\u2026"}, "outgoing_paragraph_urls": [{"url": "https://awoiaf.westeros.org/index.php/Viserys_II_Targaryen", "anchor_text": "Viserys II Targaryen", "paragraph_index": 6}, {"url": "https://pytorch.org/docs/stable/nn.html#torch.nn.BCEWithLogitsLoss", "anchor_text": "sigmoid with binary cross entropy loss", "paragraph_index": 21}, {"url": "https://arxiv.org/abs/1903.05987", "anchor_text": "This paper", "paragraph_index": 36}, {"url": "https://arxiv.org/abs/1905.05583", "anchor_text": "paper", "paragraph_index": 39}, {"url": "https://github.com/HazyResearch/snorkel", "anchor_text": "Snorkel", "paragraph_index": 40}], "all_paragraphs": ["Papers on text classification in the context of NLP mostly deal with neatly labelled data which fits only into one class (single-label multiclass classification problems; e.g. an image either contains a cat or not).", "In the real world though, we often have to work with data with not only 1) weak labels due to the lack of manpower for tagging data, but also 2) our problems are usually multilabel \u2014 a single document has multiple labels. (e.g. a image can have cats AND dogs)", "Furthermore, analysts at different companies often have highly specific internally relevant categories that can\u2019t be neatly slotted under commercial NLP APIs like Google Cloud ML.", "I\u2019m part of a team working on an NLP project to do multilabel text classification for a financial institution.", "We\u2019re under an NDA, so I can\u2019t reveal what company we\u2019re working for, or the specific task, so I\u2019m using names from George R R Martin\u2019s book series to both anonymize my data and hopefully give you a fun learning example.", "I\u2019m going to give a problem description in terms of the \u201cA Song of Fire and Ice\u201d fictional universe, then go on to discuss a bit more about how I used Google BERT to solve this problem.", "You are a wizard from the land of Asshai armed with a magical black mirror called a \u201ccomputer\u201d. The Hand of King Viserys II Targaryen has contacted you to help him. Everyday, he and his staff receive documents covering political entities all over Westeros and Essos; most of the documents are from official court sources and share a similar format, but they also comprise of other sources such as letters and spy reports.", "Ultimately, what the King\u2019s Hand wants is to make decisions to preserve peace and prosperity in the realm, but he and his staff spend a lot of time having to read through all these documents and compile them into reports.", "The Hand and his staff have classified and validated a sample of the documents for you, while with your computer, you can magically obtain a sample of documents albeit without labels from spirits by with names like \u201cGuugle\u201d, \u201cB\u2019eeng\u201d, and \u201cYaa Huu\u201d. You also have access to a golem by the name of BERT, who has been trained in over 6 million forms of communication.", "The King\u2019s Hand gives you a set amount of time to create a prototype model. Subsequently, he will compare your results on \u201clive\u201d data, against how his own staff performs on that same data.", "We will conduct a test against the King\u2019s Hand and his staff on \u201clive\u201d data.", "Hopefully, the above gave you a fun metaphor for our problem which wasn\u2019t too confusing.", "Here\u2019s how I approached the problem:", "Following Andrew Ng\u2019s guidelines, we ideally should have train, validation, and test datasets.", "In the best possible situation, the datasets should all be similar to each other, but if this is not possible, we should at least have the validation set be similar to the test set. We then train and tweak hyperparameters on the train set to get good results when evaluating on the validation set before finally applying it to the test set.", "In this case, I scraped a large number of documents from the web, using the presence of keywords in the headlines as weak labels.", "So for example, when I was scraping documents for House Stark, I\u2019d search on bing for \u2018domain: \u201cvarys.com\u201d intitle: \u201cbrandon stark\u201d\u2019 and treat that as a weak label for \u201cHouse Stark\u201d.", "I then extracted the strings from all the HTML and PDF files I scraped and put it all into a large dataframe. After that, I fine-tuned BERT on the scraped text to have a BERT model fine-tuned on \u201cpolitical\u201d language.", "My initial approach at this point was simply to input and tokenize entire documents into BERT as a whole, letting it truncate documents which end up with more than 512 WordPiece tokens and tag that document with my single weak label.", "Remember that due to the way I scraped it, there is only one weak label per document, e.g. \u201cHouse Stark\u201d, but the document itself could also mention \u201cHouse Lannister\u201d and so on.", "Basically, I had few positive samples per label as compared to the \u201cnegative\u201d samples, many of which in reality were false negatives. Out of 10,000 documents, maybe 100 were labelled as \u201cHouse Stark\u201d from my scraped data, but in reality, many of the remaining 9,900 could also contain information about \u201cHouse Stark\u201d.", "To deal with this, in my loss function (sigmoid with binary cross entropy loss) I increased the weight for positive samples. (pos_weight in PyTorch) Intuitively, I\u2019m treating each positive sample with * pos_weight importance. With a pos_weight of 10, I\u2019d be counting the positive samples as worth 10x more than the \u201cnegative\u201d samples.", "The metric I optimized for was the combined F1 score, with a slight preference for recall.", "By this approach, I got to about a 0.35 F1 score on the validation set \u2014 the F1 score plateaued by around 5\u20136 epochs of training the BERT classifier.", "This wasn\u2019t very good, and was worse than the classical machine learning algorithms using tf-idf that my partner used.", "At this point, I thought that the truncated texts and lack of labels were a huge limitation for using BERT.", "I created a spaCy rules matcher script with matching keywords for each label. e.g. For House Stark, I would include \u201cstark, winterfell, bran stark, robb stark\u201d and so on. As long as the matcher picked up one of the keywords, I would flag that article with a \u201c1\u201d for that category (e.g. \u201cHouse Stark\u201d).", "I then split the texts into individual paragraphs based on the raw string \u201c\\n\u201d and ran the rules matcher script on the split texts.", "Finally I fine-tuned my BERT model for this specific classification task.", "In essence, I was training the BERT model on the keywords, and hoping that it would pick up more contextual information beyond what the keywords themselves offered.", "With this approach, my F1 score hit about 0.50 on the validation set, which was higher than a pure \u201crules matcher-only\u201d model. So the model did pick up additional information!", "There was definitely room for improvement \u2014 I only matched my friend\u2019s performance with a tf-idf based approach at this point, and it\u2019s frankly a waste of time to train BERT just to get values that match an older and computationally cheaper model.", "At this point, I thought that to pick up on additional human features that couldn\u2019t be captured with the rules matcher, I\u2019d just have to fine-tune the model on the validation set itself, since that was hand-labelled by human experts.", "I did so, and when it finally came for the proof-of-concept test, the BERT matcher itself did produce F1 scores that surpassed that of the tf-idf model (around 0.68).", "In the end however, we actually found that an ensemble of both the BERT model and the tf-idf model produced an even better F1 score.", "Learning points and avenues for improvement", "Looking at blogs and discussion boards, and talking to some of my peers, I find that people are using BERT in different ways. This paper describes the two main divisions: 1) taking the \u201cfrozen\u201d weights as a feature to be used in a downstream task, 2) unfreezing the weights and fine-tuning them for that specific task. In the paper, they are represented as ice (\u2744\ufe0f) \ufe0fand fire (\ud83d\udd25) respectively.", "I chose to go with fire \ud83d\udd25, and I think that paper demonstrates that BERT does seem to perform better with this approach than with the ice \u2744\ufe0f approach. I suspect that this might be due to BERT\u2019s ability to pick up on context being lost with the \u2744\ufe0f approach.", "In the task above, when fine-tuning the model on the validation set, I also simply used the default BERT method of inputting text \u2014 namely, it involved truncating text that went beyond the max sequence length.", "A recent Chinese paper mentioned that taking a combination of the beginning of the document and the end of the document helped improved their scores for long documents. I am planning to implement this in future iterations of the model.", "I\u2019ve also been looking into Snorkel, which helps you create a probabilistic model for weak labels as long as you have some \u201cgold-standard\u201d, human-validated data to compare it to.", "In the real world, labelled data is a luxury, and you have to think of ways to work with weak labels for machine learning tasks. Training a BERT model on domain-specific language, then fine-tuning it on \u201cweak labels\u201d based on keywords, then finally fine-tuning it on a human-validated dataset seems to produce results that surpass traditional NLP models.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Former machine learning and data science practitioner, currently work in venture capital in crypto/web3."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fa2dda03e506e&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-weakly-labelled-multilabel-data-with-google-bert-classifying-documents-from-westeros-a2dda03e506e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-weakly-labelled-multilabel-data-with-google-bert-classifying-documents-from-westeros-a2dda03e506e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-weakly-labelled-multilabel-data-with-google-bert-classifying-documents-from-westeros-a2dda03e506e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-weakly-labelled-multilabel-data-with-google-bert-classifying-documents-from-westeros-a2dda03e506e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----a2dda03e506e--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a2dda03e506e--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@atlantean?source=post_page-----a2dda03e506e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@atlantean?source=post_page-----a2dda03e506e--------------------------------", "anchor_text": "Alvin Leong"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe75dc526a378&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-weakly-labelled-multilabel-data-with-google-bert-classifying-documents-from-westeros-a2dda03e506e&user=Alvin+Leong&userId=e75dc526a378&source=post_page-e75dc526a378----a2dda03e506e---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa2dda03e506e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-weakly-labelled-multilabel-data-with-google-bert-classifying-documents-from-westeros-a2dda03e506e&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa2dda03e506e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-weakly-labelled-multilabel-data-with-google-bert-classifying-documents-from-westeros-a2dda03e506e&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://awoiaf.westeros.org/index.php/Viserys_II_Targaryen", "anchor_text": "Viserys II Targaryen"}, {"url": "https://pytorch.org/docs/stable/nn.html#torch.nn.BCEWithLogitsLoss", "anchor_text": "sigmoid with binary cross entropy loss"}, {"url": "https://arxiv.org/abs/1903.05987", "anchor_text": "This paper"}, {"url": "https://arxiv.org/abs/1905.05583", "anchor_text": "paper"}, {"url": "https://github.com/HazyResearch/snorkel", "anchor_text": "Snorkel"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----a2dda03e506e---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/nlp?source=post_page-----a2dda03e506e---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----a2dda03e506e---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/text-classification?source=post_page-----a2dda03e506e---------------text_classification-----------------", "anchor_text": "Text Classification"}, {"url": "https://medium.com/tag/game-of-thrones?source=post_page-----a2dda03e506e---------------game_of_thrones-----------------", "anchor_text": "Game Of Thrones"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa2dda03e506e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-weakly-labelled-multilabel-data-with-google-bert-classifying-documents-from-westeros-a2dda03e506e&user=Alvin+Leong&userId=e75dc526a378&source=-----a2dda03e506e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa2dda03e506e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-weakly-labelled-multilabel-data-with-google-bert-classifying-documents-from-westeros-a2dda03e506e&user=Alvin+Leong&userId=e75dc526a378&source=-----a2dda03e506e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa2dda03e506e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-weakly-labelled-multilabel-data-with-google-bert-classifying-documents-from-westeros-a2dda03e506e&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a2dda03e506e--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fa2dda03e506e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-weakly-labelled-multilabel-data-with-google-bert-classifying-documents-from-westeros-a2dda03e506e&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----a2dda03e506e---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----a2dda03e506e--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----a2dda03e506e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----a2dda03e506e--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----a2dda03e506e--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----a2dda03e506e--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----a2dda03e506e--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----a2dda03e506e--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----a2dda03e506e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@atlantean?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@atlantean?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Alvin Leong"}, {"url": "https://medium.com/@atlantean/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "43 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe75dc526a378&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-weakly-labelled-multilabel-data-with-google-bert-classifying-documents-from-westeros-a2dda03e506e&user=Alvin+Leong&userId=e75dc526a378&source=post_page-e75dc526a378--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fd97c5766575a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-weakly-labelled-multilabel-data-with-google-bert-classifying-documents-from-westeros-a2dda03e506e&newsletterV3=e75dc526a378&newsletterV3Id=d97c5766575a&user=Alvin+Leong&userId=e75dc526a378&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}