{"url": "https://towardsdatascience.com/analyzing-twitter-spheres-through-nlp-techniques-748b0df10b6c", "time": 1683001788.4622028, "path": "towardsdatascience.com/analyzing-twitter-spheres-through-nlp-techniques-748b0df10b6c/", "webpage": {"metadata": {"title": "Analyzing Twitter spheres through NLP techniques | by Sejal Dua | Towards Data Science", "h1": "Analyzing Twitter spheres through NLP techniques", "description": "Preliminary exploratory analysis of distinct spheres of the Twitter community, offering categorical- and account-level comparisons of tweet content. Future research directions are also discussed."}, "outgoing_paragraph_urls": [{"url": "https://github.com/marquisvictor/Optimized-Modified-GetOldTweets3-OMGOT", "anchor_text": "Optimized and Modified GetOldTweets3 library (OMGOT)", "paragraph_index": 3}, {"url": "https://github.com/fnielsen/afinn", "anchor_text": "affin", "paragraph_index": 8}, {"url": "https://github.com/sejaldua/viral-network", "anchor_text": "GitHub repo", "paragraph_index": 11}, {"url": "https://github.com/sejaldua/twitter-spheres", "anchor_text": "code", "paragraph_index": 55}, {"url": "http://sejaldua.com", "anchor_text": "sejaldua.com", "paragraph_index": 58}], "all_paragraphs": ["By Sejal Dua and Camille Bowman", "Twitter is a platform used for all types of communication: shower thoughts, funny encounters, serious news, and more. We were curious about how different types of accounts used Twitter. We looked at 10 big accounts in 8 categories: fast food, airlines, sports leagues, colleges, tech companies, streaming services, news outlets, and celebrities. Using their tweets, we examined the patterns in the words the accounts used through sentiment analysis, word clouds, networks, and recurrent neural networks.", "Initially, our plan was to use the Twitter API and the tweepy library to obtain a dataset primarily composed of popular hashtags and geolocation data corresponding to the usage of those hashtags. However, we soon discovered that, with our free developer account, the Twitter REST API would only permit us to mine tweets from the past 7 days, and even if we were to pay, we could only feasibly get our hands on 100 tweets per day. Upon doing some more research, we learned that there were some workarounds. For example, there was a page scroller that could surpass the rate limit of our API queries. But once we started gathering trending tweet data, we faced yet another issue. Twitter is a platform on which users can fully engage with endless content and a vast network of users without ever having to give any location data. People can voluntary check into a location, but this location was often something incredibly vague and quirky (e.g. \u201cEarth\u201d). We assessed all of these factors and determined that the obstacles ahead would be too difficult to overcome given the timeline of our project and the whole overarching purpose of it: to collect big data and use powerful techniques to gain insights from it. From there, we knew we had to find an alternative method of data collection, and we also had to redefine our research question to one that did not involve visualizing geolocation data on a map.", "We browsed the internet and GitHub, searching for those absolute legends on StackOverflow who always save the day right when you are about to give up and switch gears. Fortunately, we found just what we needed to continue with this Twitter project. The Optimized and Modified GetOldTweets3 library (OMGOT) can be used to mine old and backdated tweets in high volumes. We used this Python library to pull up to 10,000 tweets posted in the past five years from specific users. If you\u2019re looking to scrape public accounts on Twitter, this library is far simpler to use than the Twitter API (no rate limits!), and doesn\u2019t require an API key.", "Given the capabilities of our new scraper, we figured that the best route to take our project would be into the domain of categorical analysis. We picked each of the 8 categories below because we believe they collectively represent distinct usages of Twitter as a platform for interacting with other people, entire communities, and the greater world that is now right at our fingertips. We selected the 10 accounts per category by finding users who tweeted often and were popular names outside of the \u201cTwitterverse\u201d. The accounts we selected are represented in the logo chart below:", "We had to eliminate some accounts that we had initially chosen because they had too few tweets. Originally, we had a singers and actors category, but we decided to combine these two categories into a more diverse celebrities category. This enabled us to include people like Amy Schumer and Chrissy Teigen (both very active on Twitter) and eliminate many of the actors we had chosen with a more sporadic Twitter presence.", "Once we collected the data, we had to pre-process it in order to construct a corpus for our sentiment analysis and word cloud creation. We performed the following cleaning measures:", "These cleaning techniques were intended to remove characters and words that would lead to extraneous or obfuscated information in our analyses. We wanted to make sure that all the words that were the same looked the same\u2013\u2013hence, lowercasing and removing punctuation\u2013\u2013and we also wanted all the words in the corpus to be meaningful, thus justifying why we removed special characters. Our cleaning, or removal of the chaff, if you will, was intended to make our later analyses more informative and helpful.", "For our first phase of exploratory data analysis, we thought it would be interesting to look at how negative or positive tweets were by category, and how negative or positive different accounts were, on average. We used the affin library to calculate sentiment scores and normalized by length of the cleaned tweet. Then we plotted each account\u2019s average sentiment score, broken down by category.", "News was the only category for which all accounts averaged a negative sentiment score. This makes sense, as most of the companies are responding to complaints (and trying to keep everything positive), and celebrities are trying to maintain their brand. However, news sources are constantly reporting on tragedies, wars, and various other negative events worldwide. We did expect to see a difference between more conservative news sources (Fox, the Blaze, and WSJ) and more liberal news sources (Washington Post, NYTimes, and Huffpost). However, there was no observable difference between news source positivity and their placement on the political spectrum. Huffpost and WSJ were the two least negative Twitters, whereas NBC and the Blaze were two of the most negative.", "We also analyzed the sentiment of all tweets in a given category as a histogram, paying no respect to sentiment differences between different accounts.", "If you want to interact with our Bokeh histogram above (highly recommend), please visit our GitHub repo. Unfortunately, at this moment you cannot embed Bokeh plots into Medium (a real shame, Bokeh and Codepen should collab and give the people what they want). Just as we observed with the by-account bar charts in the earlier figure, news is significantly more negative than the others. It is the only category whose median is less than 0 while other categories skew towards the positive side of the sentiment spectrum. That being said, all graphs are centered around 0 (and have a massive spike at 0), thus implying that many tweets sent by the accounts are composed of an equal distribution of positive and negative words.", "We found it interesting that all of the tweets by airlines and fast food companies apologizing for bad service or customer issues didn\u2019t skew the results more negatively, as these tweets are chock full of words like \u201cno\u201d and \u201csorry\u201d. We hypothesize that this may be due to the fact that these tweets often contain many other neutral and positive words like \u201cplease\u201d, \u201chelp\u201d, and \u201cbetter\u201d, for instance, which counteract the more negative words.", "Though our findings were interesting, the categories and many accounts were so neutral on the aggregate. We wanted to do a deeper exploration of the words used by category and by account, so we could get a better picture of what they were really tweeting about.", "Sentiment analysis can be extremely valuable in terms of quantifying the emotional charge of any given sample of text, but visualizations of sentiment scores can often be underwhelming, and we learned that there are only so many insights one can draw from a histogram that is heavily concentrated around 0. Moreover, when trying to characterize a dataset that has a category label like \u201cfast food\u201d, sentiment analysis completely overlooks the high prevalence of nouns that may be interesting to look at. For this reason, we decided to pivot and dive into a different type of analysis: word frequency. We hypothesized that by visualizing the corpus in the form of a word cloud, we would be able to identify enlarged, high-frequency words that make a lot of sense with respect to the category from which they came.", "Our hypothesis was correct in many ways. As you can see in the GIF above, the word clouds seem to reflect the content of words that you might observe when looking at tweets from each of the 8 categories.", "Here are some observations from visually inspecting the word clouds:", "fast food: us, please, thank, sorry, dm, contact, info, hear, phoneairlines: please, thank, dm, us, hi, sorry, hear, flight, numberleagues: game, match, pt, run, goal, win, watch, lead, team, player, sportcolleges: student, new, year, us, via, duke, research, study, worldtech giants: thank, hi, us, help, learn, please, love, new, intel, ibmstreaming: click, new, thank, use, love, now, listen, confirm, promo, contentnews: trump, new, president, people, year, said, us, impeachmentcelebrities: love, thank, tonight, happy, people, friend, time, fun, year", "It is fascinating how just jotting down a list of which words pop out to the human eye, we are able to implicitly synthesize the data to some extent. Some words that are common across multiple categories include \u201cplease\u201d, \u201cthank\u201d. \u201cus\u201d, \u201cnew\u201d, \u201cyear\u201d, etc. It is interesting how both fast food chains and airlines seem to use Twitter from the perspective of customer service and have a high volume of reply tweets to specific users who are often unhappy about a product. This intuition might explain why those two categories share the words \u201cplease\u201d, \u201cthank\u201d, \u201cdm\u201d, \u201cus\u201d, \u201csorry\u201d, and \u201chear\u201d, among others. Whereas a fast food company can request a customer\u2019s contact information and \u201cmake it right\u201d with a free meal, an airline cannot really offer a nice gesture like this. Just food for thought!", "News and celebrities were interesting categories because the tweet selection was so diverse in nature. Each individual news provider focuses on stories that are topical, thus there aren\u2019t many longitudinal trends we can observe that illuminate some centralized theme. In a similar manner, celebrities tweet content that is relevant to their lives and their lives alone. Ryan Reynolds often name drops \u201cDeadpool\u201d, but it is no surprise that none of the other celebrities in the dataset made any reference to the movie. The takeaway here is that because the corpus was saturated with so many rare nouns, the number of high-frequency words seemed to be diminished. We see highly generic words like \u201cnew\u201d, \u201cyear\u201d, \u201csaid\u201d, and \u201cus\u201d, and then in the largest font size of all, we see \u201ctrump\u201d, \u201cpresident\u201d, and \u201cimpeachment\u201d. Enough said\u2026", "The word clouds served as a great stimulus with which to gather an initial impression of the corpus for each category, but we crave numbers and data! Not only that, but the word clouds made us curious about how we could gain insights about the most common words by account, within each category rather than just a general categorical comparison. So we looked further into the word frequency data and extracted the 10 most common words per account, displayed them by category. The results can be seen in the GIF below.", "The fast food, airline, and tech Twitter accounts say \u201cplease\u201d, \u201csorry\u201d, and \u201ccontact\u201d a lot (IHOP\u2019s customer service line is their third most used word). As we mentioned earlier, most of the tweets from these accounts are to dissatisfied customers, helping them address their issues. This meant that their top words all had incredibly high counts, and that there was a point at which there was some drop-off, as many of their tweets have a similar premise (handling customer complaints).", "Starbucks, Tesla, and Snapchat were the only ones of these accounts that seemed to mostly use it for advertising, and their counts per word were far lower, probably due to the minimal repetitive replies to customers. Starbucks\u2019 top two words were \u201csip\u201d and \u201chappy\u201d, Tesla\u2019s were \u201csupercharger\u201d and \u201cmodel\u201d, and Snapchat\u2019s were \u201cintroducing\u201d and \u201cmeet\u201d. All of these high-frequency words are non-generic, and they are also not explicit references to a company name or product. One could take a look at these word frequency insights and make the claim that these three tech companies are using the Twitter platform effectively or optimally because they are striking the perfect balance between too many direct replies to users and too much promotional content. However, Snapchat is not a particularly active Twitter user relative to its competition: its seventh through tenth top words were all only used 5 times. Maybe they should allocate less time to developing new voice changing and facial morphing filters and more time to blowing up people\u2019s Twitter timelines! Maybe their Twitter presence does not play any significant role in them achieving their goals as a young corporation. Who knows?", "The sports league Twitters used a lot of lingo appropriate to their sport, and many words pointed to tweets used to advertise games (words like \u201ctonight\u201d, \u201cwatch\u201d, and \u201cgame\u201d were common).", "The college Twitters, it seemed, like to talk about themselves. All the accounts but Princeton and UMiami had their college name (in some form) as their most used word. Other common words were \u201cstudent\u201d, \u201cresearch\u201d, and \u201ccampus\u201d, indicating that their Twitters are often used to talk about what\u2019s happening at their respective universities. Most tech companies and some streaming companies, as well, had their names in their common word lists. This was fairly uncommon in other categories.", "Streaming sites appeared to mostly use their Twitters to promote new releases, as \u201cpremiers\u201d, \u201cpromo\u201d, \u201cnew\u201d, and \u201ctonight\u201d were all commonly used across accounts. Unlike some of the other customer-facing categories, few of the words implied the same level of addressing client issues as with fast food, tech, and airlines.", "As 9/10 of our news Twitters were American sites, many of the words related heavily to American politics and current events, such as \u201cpresident\u201d, \u201ctrump\u201d, and \u201cimpeachment\u201d. Interestingly, BBC (the only international news source), also had \u201ctrump\u201d and \u201cpresident\u201d in its list. The only word obviously about a non-U.S. current event was \u201chong\u201d, the Wall Street Journal\u2019s tenth most used word. We bet that their 11th most used word would have been \u201ckong\u201d. Any takers?", "The celebrity accounts appeared to be mostly about thanking their fans. The words \u201clove\u201d, \u201cthank\u201d, and \u201cfun\u201d were all popular words. There also appeared to be significant overlap between the different celebrity Twitter accounts. Though the word counts within and between categories were fairly variable, the celebrities\u2019 top word counts tended to be much smaller. Unlike many corporations who tweet multiple times a day, addressing complaints and mentioning various items or sales, celebrities have a much smaller volume of tweets. This results in fewer uses of the top word, and much more variation in tweet content.", "We also wanted to look at how the overlap between the top 10 words by category. Because we thought a 10-way venn diagram would be a bit hectic, we chose to use a network. Larger orange nodes represent categories, smaller blue nodes represent words, and a connection means that word is in that category\u2019s top 10.", "We found it interesting that sports is the only category with absolutely no overlap with any other category. Out of the 55 distinct words, 13 of them were shared by at least two categories: \u201cus\u201d, \u201cplease\u201d, \u201cdm\u201d, \u201csorry\u201d, \u201cyou\u201d, \u201cthanks\u201d, \u201clike\u201d, \u201chi\u201d, \u201cthank\u201d, \u201cwe\u2019re\u201d, \u201cnew\u201d, \u201clove\u201d, and \u201cone\u201d. The word nodes had an average degree of 1.45, meaning that the average top word was usually a common word for more than just one category. When looking only at common words shared across multiple categories, the average degree was 2.92 (big increase). Lots of these shared words reflect the prevalence of consumers to seek help for their issues or negative experiences over Twitter, and the number of tweets companies send out fixing these problems. Many of the words only used for one category are far more specific to the category, like all of the sports lingo, \u201cflight\u201d, \u201cprofessor\u201d, and \u201ctrump\u201d.", "All in all, the NLP techniques performed in this section helped us get well acquainted with the content of tweets from varying accounts and Twitter spheres. It is pretty amazing that we pretty randomly selected some categories and accounts, thinking that we were constructing a diverse enough dataset to discover something interesting, and we actually did gather data-driven insights. Now that we know our research scope is juicy enough to further investigate, it is like we have validated our proof of concept design, and we can refine our research question to be more narrow and more valuable.", "If you have made it this far, consider this part like drinking the chocolate milk after having a bowl of Cocoa Krispies. It doesn\u2019t have all that much to do with the original reason we embarked on this project, but it certainly augments the experience.", "You may be wondering why, for the past 10 minutes of your time, we have been going on and on about NLP techniques like sentiment analysis, word cloud visualizations, and word frequency breakdowns, and we now are bringing up RNNs. Or perhaps you are wondering what an RNN even is? That is completely valid.", "We decided to include this section because, let\u2019s face it, learning that Jimmy Fallon uses the word \u201ctonight\u201d 1303 times in his last 5 years worth of tweets is cool and very precise, but what are you and I supposed to do with that information? How do we detect patterns in a way that is maximally informative. Well, guess what. The answer is within us. Human beings are exceptionally good at synthesizing information. Our neurons are kind of wild like that. But even if we are good at synthesizing information, our ability to articulate the patterns we have observed is quite abysmal.", "So why don\u2019t we train a neural network model to learn things from the data, similar to the way in which a human would? RNNs are an incredibly useful variant of neural networks for this type of application. Unlike a vanilla neural network which relies on the assumption that two successive inputs are independent of each other, trying to infer patterns pertaining to language-based data requires a \u201cmemory\u201d of sorts. The network must capture, retain, and accumulate information. It is imperative that dependence on previous observations is considered.", "We repurposed code from our \u201cArtificial Neural Networks and Deep Learning\u201d course taken at Danish Institute for Study Abroad (DIS) in order to train four different neural networks, each one fitted to data from a unique category. We trained for 50 epochs and obtained accuracy scores across 60% across the board. We also generated tweet samples with varying diversities (0.2, 0.5, and 1.0). In the four sections below, we will embed some of the sample tweets generated with 0.2 diversity, and we will compare how much the network had learned at early epochs versus later ones.", "to make it right? Very mysterious! ortanting ontant. Can you DM us thin to oktintion. Can you DM us thinging totion. Can you DM us thin to oktintion. Can you DM us to thinging totion. Can you DM us thin to outtintiontant into thinging totion. Can you antonttintoontiontinttontain to oktant. Can you DM us thin to o", "On the first epoch of training, our RNN model started generating a tweet (280 characters) with a starting seed (40 characters) that reads \u201cto make it right? Very mysterious!\u201d. It is clearly very confused as it starts to diverge from the input sequence. Due to the high prevalence of the phrase \u201cCan you DM us\u201d in the dataset, it repetitively tries to formulate sentences starting with that expression but can\u2019t quite produce real English words beyond that expression.", "Please send us a DM if you\u2019re interested in Spicy Nuggets, Angela! Please DM us the restaurant location and your phone #so we can make this right. Thanks! That\u2019s not okay! Please DM us the restaurant location and your phone #so we can make this right. Thanks! That\u2019s not okay!", "After nearly doubling its accuracy, the neural network performs much better. We see coherent English sentences and can even spot \u201cspicy nuggets\u201d referenced in this tweet from the fast food sphere, which is exactly the type of content we were hoping to see. We additionally see the network attempt to throw in some hashtags. Granted, there is no need to say #so in a tweet, but the fact that it knows the \u201c#\u201d character occurs in high frequency in the dataset and it actively tries to be hip with the times is enough of a reason to be proud of it. Our neural network always puts the customer first, repeatedly committing to \u201cmake this right\u201d and repeatedly saying \u201cThat\u2019s not okay!\u201d. This shows that it has understood and can regurgitate the language patterns it has observed from the training data.", "ty @Jamal_Carter6! Watch him scrape by the loss and the first from the first the first from the first the first the first the final the first the first the first from the first the first the first the first the first that the first players and the first the first @ConnecticutSun with the first the first @ConnecticutSun\u2019", "Although, just like with fast food, the network seems to get stuck in an infinite loop, trying to use the phrase \u201cthe first\u201d over and over again, it picked up on some important aspects of sports tweets. It attempted to mention a player and a team and used the word \u201closs\u201d.", "professionals for their impact on the game of the @Nationals are in the @PGAVillage in the @LAClippers #PGAChamp | @BKoepka (22 PTS, 6 REB, 11 AST) and @SheffieldUnited the @Lakers and @ConnecticutSun in the @LAClippers win the @Nationals : #NFL100 Greatest Teams on @NFLNetwork \u21161: 1997 @NCAADII women\u2019s golf and the", "This maximum accuracy tweet is not as good as its fast food counterpart (perhaps due to less homogeneity across accounts), but it mentions real teams and uses real hashtags. \u201c@BKoepka\u201d is Brooks Koepka\u2019s real Twitter handle and \u201c(22 PT, 6 REB, 11 AST)\u201d is a legitimate stat report for a basketball player, but the two portions do not make sense when placed adjacent to one another in a tweet. Although the Nationals (a baseball team) would never be in PGA Village (for golfers), we can see some important aspects of sports tweets: announcements about a win, player statistics, and mentions. This implies that many sports tweets contain information about games that are about to happen and the outcome of past games.", "dispatcher is being lauded for recognizing a more than the president to the the world of the impeachment inquiry on the the world of a prices are the extended that the students that the contests that the states and the president at the states and the prosecutors are the mayor and the threatens to be a contests and the", "Again, though this tweet doesn\u2019t make a lot of sense. It picked up on some of the buzzwords we noted in the frequency analysis, such as \u201cpresident\u201d and \u201cimpeachment\u201d. Despite its nonsensical structure, it was able to regurgitate a lot of frequent words used by news outlets.", "Sen. Warren on Bloomberg entering the race of the call to the court for the state of the court of the president in the case of the president. The state of the makes a sent a media was a received to see to the state of the carame of the case of the impeachment hearings and they have want to stand of the campaign in the", "This tweet is a lot closer to a real news tweet than in the first epoch. It is able to extract information common in the tweets (like \u201cimpeachment hearings\u201d and \u201ccampaign\u201d), and we can infer that many of the tweets by news organizations currently are discussing the two largest aspects of American politics: Trump\u2019s impeachment hearings and the upcoming presidential election.", "/Bv13d8lFHjc/?utm_source=ig_twitter_share&amp;igshid=1ric6ctn7zzux #Irresponsible #TheMich #TheGood #HustleHart #TheMiller #FallonTonight Thank you for the show to the past of the show to the work and the the the look to see the show the show to the best was the see you for the best to the come and the fath on the best", "This tweet has far more hashtags than any other category, and are pulled from a variety of different accounts. We can see, again, some of the top 10 words from the frequency analysis being pulled in, as the last section uses a lot of \u201cthank you\u201ds, \u201cshow\u201ds, and \u201cthe best\u201d \u2014 the language that we believe signifies celebrities thanking their fans.", "This tweet contains a litany of mentions and hashtags. From this, we can assume that, even more than sports accounts, celebrities are keen to tag others and include hashtags. This is most likely to boost the popularity of their tweets and promote various projects they are working on with the hope that some of the hashtags they use explode and become Twitter trends. This particular RNN-generated tweet contains \u201ctonight\u201d, \u201cthanks\u201d, and \u201c#FallonTonight\u201d, which suggests to us that this particular RNN may have been overfit due to the enormous volume of post-show \u201cthank you\u201ds doled out by Jimmy Fallon (he needs to chill and stop skewing our dataset).", "We believe that the four RNNs we trained help us observe language patterns in a really valuable way. There is not necessarily a demand for tweet generation because most Twitter users do not have a hard time coming up with a mere 280 characters worth of content to share with the world, but the fact that these recurrent neural networks were able to synthesize a huge set of tweets and spit something back out that reveals characteristics of the original data\u2013\u2013 is very cool!", "This project can be extended in many potential directions in the future. Since GetOldTweets3 makes it so easy to collect Twitter data, we would love to examine more categories and more accounts per category, to see if more patterns emerge. As well, we would have loved to have the time to train a recurrent neural network on all of the categories in order to extract more patterns from the data. This would have helped us further analyze the general topics discussed by each category of Twitter and better understand some of the trends among different groups of accounts.", "Additionally, it would have been really interesting to introduce a few different types of graphs. The way our data looks, since we took max 10,000 tweets and GetOldTweets3 scrapes in reverse chronological order, our dataset is not exactly set up to look at tweet volume over time. If we had time, we would have eliminated our tweet volume limit and re-scraped every account to get all tweets in the last 5 years, and then we would plot a time series analysis of the tweets by category. We are curious to see if there have been any trends over the past five years, specifically in terms of tweet content with respect to time. We can envision this time series analysis launching another big data project about predicting news trends using NLP and ML techniques.", "One last thing we would have loved to do if we had time is create a machine learning model to predict how viral a given tweet would become given the hashtags attached to it. We think this would be feasible because the Twitter API and UI search field make it very easy to query tweets with trending hashtags and all the accounts we investigated throughout this project are popular and verified. We are curious to examine the relationship between hashtag(s) used in a given tweet and how viral the tweet was, which can be quantified in terms of like and retweet counts. This model could potentially be used to help companies decide which trends they should capitalize on and which are not worth the energy. Being able to somewhat accurately predict the success of a given tweet would be a useful and interesting tool, especially in this age of internet marketing.", "If this project interests you in any way, shape, or form, we urge you to play with our code. Take a technique that we used in this article and run with it to build something cool. Then write a story so that everyone can learn from and with you.", "Thank you to our professor Lucian Leahu for all his help with this project and for teaching us so much about the field of big data. And thank you to Ulf Aslak Jensen for creating this class and the exercises that helped us learn some fundamentals skills with which we can now tackle awesome projects. Thank you to GetOldTweets3, which was like a big hug compared to Twitter\u2019s API. As always, a big shout out to the open source community and StackOverflow for helping us with all of our fun problems along the way. And to readers like you for inspiring to keep creating and to keep telling stories with data!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Scientist @ Nike | Previously @ IBM Watson Health | Writing about Python, ML, NLP, Data Visualization, Music, and Sports | sejaldua.com"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F748b0df10b6c&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanalyzing-twitter-spheres-through-nlp-techniques-748b0df10b6c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanalyzing-twitter-spheres-through-nlp-techniques-748b0df10b6c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanalyzing-twitter-spheres-through-nlp-techniques-748b0df10b6c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanalyzing-twitter-spheres-through-nlp-techniques-748b0df10b6c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----748b0df10b6c--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----748b0df10b6c--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://sejaldua.medium.com/?source=post_page-----748b0df10b6c--------------------------------", "anchor_text": ""}, {"url": "https://sejaldua.medium.com/?source=post_page-----748b0df10b6c--------------------------------", "anchor_text": "Sejal Dua"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe353ddb0c125&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanalyzing-twitter-spheres-through-nlp-techniques-748b0df10b6c&user=Sejal+Dua&userId=e353ddb0c125&source=post_page-e353ddb0c125----748b0df10b6c---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F748b0df10b6c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanalyzing-twitter-spheres-through-nlp-techniques-748b0df10b6c&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F748b0df10b6c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanalyzing-twitter-spheres-through-nlp-techniques-748b0df10b6c&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://github.com/marquisvictor/Optimized-Modified-GetOldTweets3-OMGOT", "anchor_text": "Optimized and Modified GetOldTweets3 library (OMGOT)"}, {"url": "https://github.com/fnielsen/afinn", "anchor_text": "affin"}, {"url": "https://github.com/sejaldua/viral-network", "anchor_text": "GitHub repo"}, {"url": "https://github.com/sejaldua/twitter-spheres", "anchor_text": "code"}, {"url": "https://github.com/sejaldua/twitter-spheres", "anchor_text": "sejaldua/twitter-spheresPerforming a categorical analysis of Twitter content using an 8 x 10 x 10000 tweet dataset and big data techniques\u2026github.com"}, {"url": "https://medium.com/@IrekponorVictor/twitter-data-mining-mining-twitter-data-without-api-keys-a2a2bd3f11c", "anchor_text": "Twitter Data Mining: Mining Twitter Data without API keys.Getting Old Twitter Big Data for Analysis with a Single Line of Command.medium.com"}, {"url": "https://towardsdatascience.com/creating-word-clouds-with-python-f2077c8de5cc", "anchor_text": "Creating word clouds with pythonDuring a recent NLP project, I discovered word clouds could be created with a mask. In this article I walk through how\u2026towardsdatascience.com"}, {"url": "https://pypi.org/project/netwulf/", "anchor_text": "netwulfSimple and interactive network visualization in Python. Network visualization is an indispensable tool for exploring\u2026pypi.org"}, {"url": "https://towardsdatascience.com/understanding-neural-networks-from-neuron-to-rnn-cnn-and-deep-learning-cd88e90e0a90", "anchor_text": "Understanding Neural Networks. From neuron to RNN, CNN, and Deep LearningNeural Networks is one of the most popular machine learning algorithms at present. It has been decisively proven over\u2026towardsdatascience.com"}, {"url": "https://medium.com/tag/nlp?source=post_page-----748b0df10b6c---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/recurrent-neural-network?source=post_page-----748b0df10b6c---------------recurrent_neural_network-----------------", "anchor_text": "Recurrent Neural Network"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----748b0df10b6c---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/tag/twitter-data?source=post_page-----748b0df10b6c---------------twitter_data-----------------", "anchor_text": "Twitter Data"}, {"url": "https://medium.com/tag/word-cloud?source=post_page-----748b0df10b6c---------------word_cloud-----------------", "anchor_text": "Word Cloud"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F748b0df10b6c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanalyzing-twitter-spheres-through-nlp-techniques-748b0df10b6c&user=Sejal+Dua&userId=e353ddb0c125&source=-----748b0df10b6c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F748b0df10b6c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanalyzing-twitter-spheres-through-nlp-techniques-748b0df10b6c&user=Sejal+Dua&userId=e353ddb0c125&source=-----748b0df10b6c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F748b0df10b6c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanalyzing-twitter-spheres-through-nlp-techniques-748b0df10b6c&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----748b0df10b6c--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F748b0df10b6c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanalyzing-twitter-spheres-through-nlp-techniques-748b0df10b6c&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----748b0df10b6c---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----748b0df10b6c--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----748b0df10b6c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----748b0df10b6c--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----748b0df10b6c--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----748b0df10b6c--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----748b0df10b6c--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----748b0df10b6c--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----748b0df10b6c--------------------------------", "anchor_text": ""}, {"url": "https://sejaldua.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://sejaldua.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Sejal Dua"}, {"url": "https://sejaldua.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "476 Followers"}, {"url": "http://sejaldua.com", "anchor_text": "sejaldua.com"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe353ddb0c125&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanalyzing-twitter-spheres-through-nlp-techniques-748b0df10b6c&user=Sejal+Dua&userId=e353ddb0c125&source=post_page-e353ddb0c125--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F307d04c98997&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanalyzing-twitter-spheres-through-nlp-techniques-748b0df10b6c&newsletterV3=e353ddb0c125&newsletterV3Id=307d04c98997&user=Sejal+Dua&userId=e353ddb0c125&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}