{"url": "https://towardsdatascience.com/scooping-into-model-pruning-in-deep-learning-da92217b84ac", "time": 1683009696.8553002, "path": "towardsdatascience.com/scooping-into-model-pruning-in-deep-learning-da92217b84ac/", "webpage": {"metadata": {"title": "Model Pruning in Deep Learning. This article discusses pruning\u2026 | by Sayak Paul | Towards Data Science", "h1": "Model Pruning in Deep Learning", "description": "This report discusses pruning techniques in the context of deep learning."}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/a-tale-of-model-quantization-in-tf-lite-aebe09f255ca", "anchor_text": "Quantization", "paragraph_index": 0}, {"url": "https://www.tensorflow.org/model_optimization", "anchor_text": "TensorFlow Model Optimization Toolkit", "paragraph_index": 2}, {"url": "https://app.wandb.ai/authors/pruning/reports/Scooping-into-Model-Pruning-in-Deep-Learning--VmlldzoxMzcyMDg", "anchor_text": "here", "paragraph_index": 3}, {"url": "https://colab.research.google.com/github/matthew-mcateer/Keras_pruning/blob/master/Model_pruning_exploration.ipynb", "anchor_text": "here", "paragraph_index": 14}, {"url": "https://www.tensorflow.org/model_optimization/guide/pruning/comprehensive_guide", "anchor_text": "this guide", "paragraph_index": 21}, {"url": "https://www.tensorflow.org/model_optimization/api_docs/python/tfmot/sparsity/keras/strip_pruning", "anchor_text": "tfmot.sparsity.keras.strip_pruning", "paragraph_index": 34}, {"url": "https://app.wandb.ai/sayakpaul/tale-of-quantization/reports/A-Tale-of-Model-Quantization-in-TF-Lite--Vmlldzo5MzQwMA", "anchor_text": "This report", "paragraph_index": 38}, {"url": "https://arxiv.org/abs/1803.03635", "anchor_text": "Lottery Ticket Hypothesis", "paragraph_index": 43}, {"url": "https://www.youtube.com/watch?v=ZVVnvZdUMUk", "anchor_text": "this beautiful explanation", "paragraph_index": 44}, {"url": "https://www.youtube.com/watch?v=s7DqRZVvRiQ", "anchor_text": "presented at ICLR 2019", "paragraph_index": 46}, {"url": "https://arxiv.org/abs/1905.01067", "anchor_text": "Deconstructing Lottery Tickets", "paragraph_index": 46}, {"url": "https://arxiv.org/abs/1912.05671", "anchor_text": "Linear Mode Connectivity", "paragraph_index": 47}, {"url": "https://openreview.net/forum?id=S1gSj0NKvB", "anchor_text": "Learning Rate Rewinding", "paragraph_index": 48}, {"url": "https://arxiv.org/abs/2005.07683", "anchor_text": "Movement Pruning", "paragraph_index": 50}, {"url": "https://www.youtube.com/watch?v=nxEr4VNgYOE", "anchor_text": "excellent explanation", "paragraph_index": 52}, {"url": "https://arxiv.org/abs/2006.05467", "anchor_text": "SynFlow", "paragraph_index": 55}, {"url": "http://twitter.com/RisingSayak", "anchor_text": "@RisingSayak", "paragraph_index": 56}, {"url": "https://sayak.dev/", "anchor_text": "https://sayak.dev/", "paragraph_index": 58}], "all_paragraphs": ["This article is the successor to my previous article on Quantization. In this article, we\u2019re going to go over the mechanics of model pruning in the context of deep learning. Model pruning is the art of discarding those weights that do not signify a model\u2019s performance. Carefully pruned networks lead to their better-compressed versions and they often become suitable for on-device deployment scenarios.", "The content of the article is structured into the following sections:", "(The code snippets that we\u2019ll be discussing will be based on TensorFlow (2) and the TensorFlow Model Optimization Toolkit)", "Note: The article is mirrored here.", "Neural networks are function approximators. We train them to learn functions that capture underlying representations formulating the input data points. The weights and the biases of a neural network are referred to as its (learnable) parameters. Often, the weights are referred to as coefficients of the function being learned.", "In the above function, we have two terms on the RHS: x and x\u00b2. The coefficients are 1 and 5 respectively. In the following figure, we can see that the behavior of the function does not change much when the first coefficient is nudged.", "Here are the coefficients in the different variants of the original function can be referred to as non-significant. Discarding those coefficients won\u2019t really change the behavior of the function.", "The above concept can be applied to neural networks as well. This needs a bit more details to be unfolded. Consider the weights of a trained network. How could we make sense of the weights that are non-significant? What\u2019s the premise here?", "For this to be answered, consider the optimization process with gradient descent. Not all the weights are updated using the same gradient magnitudes. The gradients of a given loss function are taken with respect to the weights (and biases). During the optimization process, some of the weights are updated with larger gradient magnitudes (both positive and negative) than the others. These weights are considered to be significant by the optimizer to minimize the training objective. The weights that receive relatively smaller gradients can be considered as non-significant.", "After the training is complete, we can inspect the weight magnitudes of a network layer by layer and figure out the weights that are significant. This decision be made using several heuristics -", "i. The threshold can be the weight magnitude that is the lowest inside the entire network.", "ii. The threshold can be the weight magnitude local to the layers inside a network. In this case, the significant weights are filtered out on a layer by layer basis.", "If all of these are becoming hard to comprehend, don\u2019t worry. In the next section, things will become clearer.", "Now that we have a fair bit of understanding of what could be called significant weights, we can discuss magnitude-based pruning. In magnitude-based pruning, we consider weight magnitude to be the criteria for pruning. By pruning what we really mean is zeroing out the non-significant weights. Following code, snippet might be helpful to understand this -", "(This code snippet comes from here)", "Here\u2019s a pictorial representation of the transformation that would be happening to the weights after they have been learned -", "It can be applied to the biases also. It\u2019s important to note that here we consider an entire layer receiving an input of shape (1,2) and containing 3 neurons. It's often advisable to retrain the network after it is pruned to compensate for any drop in its performance. When doing such retraining it's important to note that, the weights that were pruned, won't be updated during the retraining.", "Enough jibber-jabber! Let\u2019s see these things in action. To keep things simple we\u2019ll be testing these concepts on the MNIST dataset but you should be able to extend them to more complex datasets as well. We\u2019ll be using a shallow fully-connected network having the following topology -", "The network has got a total of 20,410 trainable parameters. Training this network for 10 epochs gets us a good baseline -", "Let\u2019s now prune it! We\u2019ll be using tensorflow_model_optimization (aliased as tfmot). tfmot gives us two recipes for pruning:", "We are going to experiment with both of them. Both of these recipes should include a pruning schedule which we are going to discuss in a moment. The rationale behind pruning a network in the form of training is to better guide the training objective so that the gradient updates can happen accordingly to adjust the unpruned weights in an effective manner.", "Note that it is also possible to prune specific layers within your model and tfmot does allow you to do that. Check this guide in order to know more about it.", "You are encouraged to follow along with this Colab Notebook mentioned at the top from this point on.", "We are going to take the network we trained earlier and prune it from there. We will apply a pruning schedule that will keep the sparsity level constant (to be specified by the developer) throughout the training. The code to express this is as follows:", "A pruned model needs a re-compile before we can begin training it. We compile it in the same way and we print its summary -", "We see that the number of parameters has got changed now. This is because tfmot adds non-trainable masks for each of the weights in the network to denote if a given weight should be pruned. The masks are either 0 or 1.", "We can see that pruning the model does not hurt the performance. The red lines correspond to the pruning experiments.", "We can also verify if tfmot reached to the target sparsity by writing tests like the following:", "Running it on the pruned model should produce True for all the layers that were pruned.", "Everything remains the same in this case except that we are not starting with an already-trained network instead we will be starting with a randomly initialized network.", "The green lines correspond to pruning from scratch experiment. We can observe some drop in performance with respect to the other two models but this is expected since we are not starting with an already trained model.", "When we pruning the network by training it from scratch, it usually takes the highest amount of time. This is also expected since the network is figuring out how to best update the parameters in order to reach the target sparsity level.", "All of this is fine but to be able to really appreciate the power of pruning, we need to dig a bit deeper:", "Let\u2019s deal with it in the next section.", "We will be using the standard zipfile library to compress the models to .zip format. We need to use tfmot.sparsity.keras.strip_pruning when serializing the pruned models, it will remove the pruning wrappers that were added to the models by tfmot. Otherwise, we won't be able to see any compression benefits in the pruned models.", "Compressing the regular Keras models remains the same, however.", "file should be a path to an already serialized Keras model (both pruned and regular).", "In the below figure, we can see the compressed models weigh lesser than the regular Keras model and they still yield pretty good performance.", "We can quantize our models using TensorFlow Lite to further reduce their sizes without hurting performance. Note that while passing the pruned models to TensorFlow Lite\u2019s converter you should pass them after stripping the pruning wrappers. This report discusses quantization in more detail.", "Apart from the accuracy measurement, compression ratio is another widely used technique to measure the efficacy of a particular pruning algorithm. The compression ratio is the inverse of the fraction of the parameter remaining in a pruned network.", "This flavor of quantization is also known as post-training quantization. So, here\u2019s a simple recipe for you to follow for optimizing your models for deployment:", "In the next section, we will be going through a few modern recipes for pruning. If you want to pursue the field of model optimization more, these ideas will be worth exploring further.", "Let\u2019s start this section with the following motivating questions:", "The first question has been explored tremendously by Frankle et al. in their seminal paper on Lottery Ticket Hypothesis. So, after pruning an already-trained network, the subnetworks that have the initialization just described above are referred to as the winning tickets.", "As a rationale behind this methodology, you could reason that during the initial training of a network a particular initialization of the parameters guided the optimization process. Now, the weights that responded well in the optimization landscape (meaning that they traveled further than the other weights) actually end up in the winning lotteries. So in order for it to (re)train well, if we initialize the weights to their utmost initial magnitudes the optimization process allures itself very nicely with them. Thanks to Yannic Kilcher for this beautiful explanation.", "The paper presents a plethora of different experiments to support this hypothesis and it\u2019s an absolute recommended read.", "In the original Lottery Ticket Hypothesis paper, Frankle et al. only explored how did a pruned network perform if the surviving weights were reinitialized to their utmost initial magnitudes before retraining. Just after the Lottery Ticket Hypothesis was presented at ICLR 2019, Zhou et al. published a paper on Deconstructing Lottery Tickets studying different ways to handle both the weights that did survive and did not survive during pruning. The also proposed supermasks which are basically learnable masks.", "To be able to scale the Lottery Ticket Hypothesis to datasets like ImageNet, Frankle et al. published a paper on Linear Mode Connectivity that is sort of a generalization of the Lottery Ticket Hypothesis. It proposes weight rewinding as a potential way to initialize the surviving weights of a pruned network. Earlier, we were initializing them with their utmost initial magnitudes. What weight rewinding does is it rewinds the surviving weights to somewhere later in the training of the original network. In other words, the surviving weights get initialized to magnitudes from epoch 5, say, of the training of the original network.", "Extending this idea, Renda et al. published a paper on Learning Rate Rewinding that applies to rewind to learning rate schedules while retraining a pruned network. The authors also propose this as an alternative to fine-tuning.", "So, these were some exciting ideas evolving primarily around magnitude-based pruning. In the final section, we will see a pruning method that performs better than magnitude-based pruning, especially for transfer learning regimes.", "In their paper on Movement Pruning, Sanh et al. propose an alternative to magnitude-based pruning that is specifically geared towards handling the pruning of pre-trained models for transfer learning tasks.", "Magnitude-based pruning is very positively correlated with the notion of significance that we already discussed earlier. In this case, the significance here simply denotes the absolute magnitudes of the weights. The lower these magnitudes the lesser the significance. Now, this significance can actually change when we try to do transfer learning with a model pre-trained on a different dataset. The weights that were significant while optimizing the source dataset might not be significant for the target dataset.", "So, during transfer learning the pre-trained weights that move toward zero can be actually considered as non-significant with respect to the target task, and the weights that move further away can be considered as significant. This is where the method derives its name from \u2014 movement pruning. Thanks to Yannic again for his excellent explanation.", "If you stick to the end, great! I hope this report gave you a fair idea of what pruning is in the context of deep learning. I would like to acknowledge Raziel and Yunlu (of Google) who provided me with important information about tfmotand some additional thoughts about pruning itself.", "Some further ideas I would like to explore in this area are:", "At the time of writing this report (June 2020), one of the most recent approaches to pruning was SynFlow. SynFlow does not require any data for pruning a network and it uses Synaptic Saliency Scores to determine the significance of parameters in a network.", "I am open to hearing your feedback via Twitter (@RisingSayak).", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "ML at \ud83e\udd17 | Netflix Nerd | Personal site: https://sayak.dev/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fda92217b84ac&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscooping-into-model-pruning-in-deep-learning-da92217b84ac&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscooping-into-model-pruning-in-deep-learning-da92217b84ac&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscooping-into-model-pruning-in-deep-learning-da92217b84ac&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscooping-into-model-pruning-in-deep-learning-da92217b84ac&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----da92217b84ac--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----da92217b84ac--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://sayakpaul.medium.com/?source=post_page-----da92217b84ac--------------------------------", "anchor_text": ""}, {"url": "https://sayakpaul.medium.com/?source=post_page-----da92217b84ac--------------------------------", "anchor_text": "Sayak Paul"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc7edfd638e74&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscooping-into-model-pruning-in-deep-learning-da92217b84ac&user=Sayak+Paul&userId=c7edfd638e74&source=post_page-c7edfd638e74----da92217b84ac---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fda92217b84ac&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscooping-into-model-pruning-in-deep-learning-da92217b84ac&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fda92217b84ac&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscooping-into-model-pruning-in-deep-learning-da92217b84ac&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/tagged/production-ml", "anchor_text": "Machine Learning in Production"}, {"url": "https://towardsdatascience.com/a-tale-of-model-quantization-in-tf-lite-aebe09f255ca", "anchor_text": "Quantization"}, {"url": "https://www.tensorflow.org/model_optimization", "anchor_text": "TensorFlow Model Optimization Toolkit"}, {"url": "https://app.wandb.ai/authors/pruning/reports/Scooping-into-Model-Pruning-in-Deep-Learning--VmlldzoxMzcyMDg", "anchor_text": "here"}, {"url": "https://colab.research.google.com/github/matthew-mcateer/Keras_pruning/blob/master/Model_pruning_exploration.ipynb", "anchor_text": "here"}, {"url": "https://app.wandb.ai/authors/pruning", "anchor_text": "here"}, {"url": "https://www.tensorflow.org/model_optimization/guide/pruning/comprehensive_guide", "anchor_text": "this guide"}, {"url": "https://app.wandb.ai/authors/pruning", "anchor_text": "here"}, {"url": "https://www.tensorflow.org/model_optimization/api_docs/python/tfmot/sparsity/keras/UpdatePruningStep", "anchor_text": "UpdatePruningStep"}, {"url": "https://www.tensorflow.org/model_optimization/api_docs/python/tfmot/sparsity/keras/PruningSummaries", "anchor_text": "PruningSummaries"}, {"url": "https://tensorboard.dev/experiment/sRQnrycaTMWQOaswXzClYA/#scalars&_smoothingWeight=0", "anchor_text": "here"}, {"url": "https://www.tensorflow.org/model_optimization/api_docs/python/tfmot/sparsity/keras/PolynomialDecay", "anchor_text": "PolynomialDecay"}, {"url": "https://app.wandb.ai/authors/pruning", "anchor_text": "here"}, {"url": "https://app.wandb.ai/authors/pruning", "anchor_text": "here"}, {"url": "https://www.tensorflow.org/model_optimization/api_docs/python/tfmot/sparsity/keras/strip_pruning", "anchor_text": "tfmot.sparsity.keras.strip_pruning"}, {"url": "https://app.wandb.ai/sayakpaul/tale-of-quantization/reports/A-Tale-of-Model-Quantization-in-TF-Lite--Vmlldzo5MzQwMA", "anchor_text": "This report"}, {"url": "https://arxiv.org/abs/1803.03635", "anchor_text": "Lottery Ticket Hypothesis"}, {"url": "https://www.youtube.com/watch?v=ZVVnvZdUMUk", "anchor_text": "this beautiful explanation"}, {"url": "https://www.youtube.com/watch?v=s7DqRZVvRiQ", "anchor_text": "presented at ICLR 2019"}, {"url": "https://arxiv.org/abs/1905.01067", "anchor_text": "Deconstructing Lottery Tickets"}, {"url": "https://arxiv.org/abs/1912.05671", "anchor_text": "Linear Mode Connectivity"}, {"url": "https://openreview.net/forum?id=S1gSj0NKvB", "anchor_text": "Learning Rate Rewinding"}, {"url": "https://arxiv.org/abs/2005.07683", "anchor_text": "Movement Pruning"}, {"url": "https://www.youtube.com/watch?v=nxEr4VNgYOE", "anchor_text": "excellent explanation"}, {"url": "https://arxiv.org/abs/2006.05467", "anchor_text": "SynFlow"}, {"url": "http://twitter.com/RisingSayak", "anchor_text": "@RisingSayak"}, {"url": "https://colab.research.google.com/github/matthew-mcateer/Keras_pruning/blob/master/Model_pruning_exploration.ipynb", "anchor_text": "Model Pruning Exploration in Keras"}, {"url": "https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras", "anchor_text": "Official"}, {"url": "https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras", "anchor_text": "tfmot"}, {"url": "https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras", "anchor_text": "guide on Pruning in Keras"}, {"url": "http://www.jfrankle.com/dissecting-pruned-neural-nets.pdf", "anchor_text": "Dissecting Pruned Neural Networks"}, {"url": "https://www.youtube.com/watch?v=ZVVnvZdUMUk", "anchor_text": "Lottery Ticket Hypothesis explanation video"}, {"url": "https://www.youtube.com/watch?v=nxEr4VNgYOE", "anchor_text": "Movement Pruning explanation video"}, {"url": "https://medium.com/tag/production-ml?source=post_page-----da92217b84ac---------------production_ml-----------------", "anchor_text": "Production Ml"}, {"url": "https://medium.com/tag/tensorflow?source=post_page-----da92217b84ac---------------tensorflow-----------------", "anchor_text": "TensorFlow"}, {"url": "https://medium.com/tag/keras?source=post_page-----da92217b84ac---------------keras-----------------", "anchor_text": "Keras"}, {"url": "https://medium.com/tag/model-optimization?source=post_page-----da92217b84ac---------------model_optimization-----------------", "anchor_text": "Model Optimization"}, {"url": "https://medium.com/tag/wandb?source=post_page-----da92217b84ac---------------wandb-----------------", "anchor_text": "Wandb"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fda92217b84ac&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscooping-into-model-pruning-in-deep-learning-da92217b84ac&user=Sayak+Paul&userId=c7edfd638e74&source=-----da92217b84ac---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fda92217b84ac&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscooping-into-model-pruning-in-deep-learning-da92217b84ac&user=Sayak+Paul&userId=c7edfd638e74&source=-----da92217b84ac---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fda92217b84ac&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscooping-into-model-pruning-in-deep-learning-da92217b84ac&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----da92217b84ac--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fda92217b84ac&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscooping-into-model-pruning-in-deep-learning-da92217b84ac&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----da92217b84ac---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----da92217b84ac--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----da92217b84ac--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----da92217b84ac--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----da92217b84ac--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----da92217b84ac--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----da92217b84ac--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----da92217b84ac--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----da92217b84ac--------------------------------", "anchor_text": ""}, {"url": "https://sayakpaul.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://sayakpaul.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Sayak Paul"}, {"url": "https://sayakpaul.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "743 Followers"}, {"url": "https://sayak.dev/", "anchor_text": "https://sayak.dev/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc7edfd638e74&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscooping-into-model-pruning-in-deep-learning-da92217b84ac&user=Sayak+Paul&userId=c7edfd638e74&source=post_page-c7edfd638e74--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F253c32626149&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscooping-into-model-pruning-in-deep-learning-da92217b84ac&newsletterV3=c7edfd638e74&newsletterV3Id=253c32626149&user=Sayak+Paul&userId=c7edfd638e74&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}