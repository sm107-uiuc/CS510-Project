{"url": "https://towardsdatascience.com/10-invaluable-tips-tricks-for-building-successful-neural-networks-566aca17a0f1", "time": 1683013310.4878578, "path": "towardsdatascience.com/10-invaluable-tips-tricks-for-building-successful-neural-networks-566aca17a0f1/", "webpage": {"metadata": {"title": "10 Invaluable Tips & Tricks for Building Successful Neural Networks | by Andre Ye | Towards Data Science", "h1": "10 Invaluable Tips & Tricks for Building Successful Neural Networks", "description": "Building neural networks is difficult because there is so much variability involved. With these 10 tips and tricks, you\u2019ll not only have concrete pointers on changes to try but a strategy and\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/manifold-learning-t-sne-lle-isomap-made-easy-42cfd61f5183", "anchor_text": "manifold learning techniques like t-SNE", "paragraph_index": 9}, {"url": "https://keras.io/api/applications/", "anchor_text": "using pre-trained image recognition models in Keras", "paragraph_index": 11}, {"url": "https://keras.io/examples/nlp/pretrained_word_embeddings/", "anchor_text": "using pre-trained embeddings in Keras", "paragraph_index": 11}, {"url": "https://towardsdatascience.com/a-comprehensive-guide-to-working-with-recurrent-neural-networks-in-keras-f3b2d5e2fa7f?source=---------4------------------", "anchor_text": "guide to using embeddings in Keras", "paragraph_index": 11}, {"url": "https://towardsdatascience.com/5-perspectives-to-why-dropout-works-so-well-1c10617b8028", "anchor_text": "here", "paragraph_index": 15}, {"url": "https://medium.com/analytics-vidhya/you-dont-understand-neural-networks-until-you-understand-the-universal-approximation-theorem-85b3e7677126?source=your_stories_page---------------------------", "anchor_text": "Universal Approximation Theorem", "paragraph_index": 20}, {"url": "https://towardsdatascience.com/a-short-practical-guide-to-callbacks-in-neural-network-training-3a4d69568aef?source=your_stories_page---------------------------", "anchor_text": "Reduce Learning Rate on Plateau", "paragraph_index": 22}, {"url": "https://towardsdatascience.com/if-youre-hyped-about-gpt-3-writing-code-you-haven-t-heard-of-nas-19c8c30fcc8a?source=your_stories_page---------------------------", "anchor_text": "very new and promising", "paragraph_index": 23}, {"url": "https://keras.io/guides/preprocessing_layers/", "anchor_text": "here", "paragraph_index": 25}, {"url": "https://medium.com/swlh/machine-learning-algorithms-are-much-more-fragile-than-you-think-25fdb3939fee?source=your_stories_page---------------------------", "anchor_text": "adversarial/malicious inputs", "paragraph_index": 28}, {"url": "https://towardsdatascience.com/you-shouldnt-ignore-all-these-parameters-in-keras-s-imagedatagenerator-eaa0530c76db?source=your_stories_page---------------------------", "anchor_text": "here", "paragraph_index": 29}, {"url": "https://medium.com/analytics-vidhya/the-strategy-that-increases-model-accuracy-every-time-guaranteed-6ee5e476262d", "anchor_text": "this article", "paragraph_index": 31}, {"url": "https://medium.com/analytics-vidhya/breaking-down-the-innovative-deep-learning-behind-google-translate-355889e104f1?source=your_stories_page---------------------------", "anchor_text": "Google Neural Machine Translation", "paragraph_index": 32}, {"url": "https://towardsdatascience.com/the-clever-trick-behind-googles-inception-the-1-1-convolution-58815b20113?source=your_stories_page---------------------------", "anchor_text": "1x1 convolutions", "paragraph_index": 33}, {"url": "https://towardsdatascience.com/a-comprehensive-guide-to-working-with-recurrent-neural-networks-in-keras-f3b2d5e2fa7f?source=your_stories_page---------------------------", "anchor_text": "RNN designs", "paragraph_index": 33}, {"url": "https://andre-ye.medium.com/membership", "anchor_text": "https://andre-ye.medium.com/membership", "paragraph_index": 36}], "all_paragraphs": ["Building neural networks is difficult because there is so much variability involved. With these 10 tips and tricks, you\u2019ll not only have concrete pointers on changes to try but a strategy and mentality towards approaching the ambiguous task of building a successful neural network.", "The last layer of the neural network is important to get right because it\u2019s the last step in the prediction process. Furthermore, the activation function of the last layer is the last function before the information flow of the network is outputted as a prediction. Getting the function right is important.", "The sigmoid function is used for binary outputs. For instance, if a network were to predict if an image was a cat or a dog, sigmoid would be the way to go. The sigmoid function \u2014 defined mathematically as 1/(1+e^(-x)) \u2014 notably, is the inverse of the logit function, which is why it is a natural choice for modelling probability.", "On the other hand, if you have multiple outputs, for instance classifying images into 10 possible digits, we would use the SoftMax function, which is an expansion of the sigmoid function for multiple classes with the restriction that all probabilities must add to 1. For instance, if predictions for four classes are [0.48, 0.23, 0.56,0.03], it is an illegitimate result because probabilities center around the fact that they always sum to 100%.", "Since sigmoid and SoftMax are both bounded functions, meaning that their y-values cannot go above or below a certain value (0 and 1), they cannot handle regression problems, which are continuous values, not probabilities.", "Instead, use a linear activation function (y=x), which is not bounded. Additionally, if all the y-values are larger than 0, using ReLU is also acceptable, since the slope of the line (a in max(0, ax)) is a tunable parameter. Alternatively, an activation unbounded on both sides like Leaky ReLU or ELU (Exponential Linear Unit) can be used.", "It\u2019s difficult to learn everything from scratch \u2014 even as humans \u2014 which is why it\u2019s beneficial to have some universal knowledge as a base for further learning. In the image recognition world, several pretrained models like VGG16 and Inception, designed and trained to recognize objects, can be accessed and further fine-tuned.", "For instance, consider the task of recognizing a dog or a cat in an image; while one could construct a neural network from scratch, why not use an architecture and weights that have proven to work on a similar task? Libraries that offer pretrained models also allow users to stack on additional layers before or after the model for some customizability.", "In Natural Language Processing, embeddings are used to map words or other tokens from their high-dimensional vectorized form into a lower-dimensional space. Words that have similar meanings or are in similar categories with respect to the context of the dataset (e.g. water & liquid, king & man, queen & woman) are placed physically closer.", "Embeddings are valuable because they are a less-costly method to extract deeper-level meanings of words before expensive neural network operations. They reduce the dimensionality of data effectively and can be visualized & interpreted using manifold learning techniques like t-SNE to make sure the network is on the right learning track.", "Pre-trained embeddings like GloVE, or Global Vectors for Word Representation, are created on created on the world\u2019s largest repositories of text, like Wikipedia. Incorporating pretrained elements in your model increases its power and reduces guesswork.", "Check out these resources: using pre-trained image recognition models in Keras, guide to using pre-trained embeddings in Keras, guide to using embeddings in Keras.", "Especially with deep networks, it\u2019s always important to prioritize a healthy gradient flow. Backpropagation can only work effectively if the architecture is built in a way such that the signal can pass through the entire network. If it gradually diminishes, the front layers are not changed (vanishing gradient problem), and if it is amplified too much, over time weight shifts become so extreme to the point where they become NaN.", "Training a neural network is a difficult task \u2014 it is highly dependent on a variety of parameters and initialization. Regulation can act as a guard rail and prevent the neural network from straying too far from its purpose, which is fairly abstract: to not underfit to the data, and not to overfit to it.", "Of course, the actual goal is to perform well on the test set, but the neural network isn\u2019t supposed to be exposed to it, or it will not be a test set in the first place. Naturally, a neural network \u2014 designed to approximate functions \u2014 will fit to the data. Regulation can prevent it from overfitting, or taking the easy way out by memorizing data points instead of actually learning generalizations.", "Adding dropout layers is perhaps the easiest method to add regulation into a neural network. Dropout randomly blocks a fraction of neurons from connecting to the next layer, so intuitively it prevents the network from passing on too much specific information. There\u2019s many other perspectives of understanding why Dropout works so well \u2014 for example, it can be thought of as an ensemble or as progressive updating. Read more here.", "Additionally, L2 regularization is another method of keeping weights in check, although it is arguably a less \u2018natural\u2019 method of doing so. Regardless of the means used to achieve it, regularization and gradient flow should be the at the forefront of architecture design.", "Building neural networks can be daunting because so much of the process is variable \u2014 the number of layers, the number of neurons in each layer, the types of layers, etc. Chances are, however, that changing the amount of neurons isn\u2019t really going to change the predictive power.", "In tutorials, you may often see the number of neurons in each layer or batch sizes written as powers of two. While there are some studies that claim using powers of two is more efficient (something to do with efficient GPU usage and the bit\u2026?), they\u2019re likely not universal. It hasn\u2019t been shown decisively that using powers of two is optimal in any way for the selection of hyperparameters.", "However, it is a great method to follow for few reasons:", "Of course, it\u2019s good to think through the information flow of the network, but the number of layers or neurons is relatively arbitrary within a certain range. For a more intuitive explanation, explore the Universal Approximation Theorem, which demonstrates how neural networks approach tasks and the role a single neuron plays in prediction.", "In general, the philosophy of model-building is to follow the data and reduce as much dependency on hard-coding constants as possible. This is especially true yet difficult with neural networks. While certain constants like the number of neurons in a layer or filter sizes must be hard-coded*, try to stray away from setting constants yourself if you can, because these are potential sources of error.", "For instance, don\u2019t hard-code the learning rate. You can set a high initial learning rate and use a tool like Reduce Learning Rate on Plateau to automatically adjust the rate when performance stagnates. This is true for many other complex parameters of the neural network \u2014 as a very dynamic algorithm, some parameters simply cannot remain fixed.", "*However, the development of automatic selection of neural network architectures with very little constant hard-coding and machine learning algorithms in general is very new and promising.", "Keras and several other deep learning libraries offer preprocessing libraries that can be added as the first few layers of a neural network, for instance in vectorizing text, standardizing images, or normalizing data. In order to make your model portable and easy to work with, it\u2019s always a good idea to use preprocessing layers for deployment.", "Read about preprocessing layers in Keras here.", "If you\u2019re not using data augmentation in image recognition tasks, you\u2019re wasting your dataset.", "Image data is hard to come by, and it\u2019s a shame if a network can only extract a limited amount of learning from such an expensive piece of information. Data augmentation artificially boosts the size of the dataset by passing images through random generated filters, which can apply zooms, rotations, flips, dimming, brightening, whitening, color changes, etc.", "When data augmentation is applied correctly, it improves the network\u2019s ability to generalize to images and better addresses real-world issues in object recognition like adversarial/malicious inputs, which can for instance trick a sign-recognizing self-driving car into accelerating at deadly speeds.", "Understand all sorts of parameters in data generators and why you need to be careful with which ones to use here.", "More data is, of course, the best solution. The fanciest algorithms can\u2019t even compare to the benefit a good batch of data can bring: data is a valuable, and hence expensive, commodity. Simply incorporating additional data can widen the horizons of the model, more than spending hours fine-tuning a model\u2019s technical parameters ever could.", "In this article, I reduce the mean absolute error over tenfold for coronavirus forecasting using a simple random forest regression model simply by adding Wikipedia data for statistics by country.", "To build a great neural network, look towards the masters! Existing models like BERT, Google Neural Machine Translation (GNMT), and Inception are built in certain ways for certain reasons.", "Inception is built in a modular format, with multiple stacked Inception cells. It uses 1x1 convolutions before others and orders pooling and convolutions for certain reasons. GNMT stacks eight LSTMs each into an encoder and a decoder for Google Translate, and while you may not care about how translation works, it\u2019s worthwhile exploring how the architecture deals with such a deep recurrent architecture and apply it to your RNN designs.", "The best tips and tricks come from the innovation at top companies and research departments developing the newest methods and cultivating the newest concepts. Take time to explore and understand ideas.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "ML enthusiast. Join Medium through my referral link: https://andre-ye.medium.com/membership."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F566aca17a0f1&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F10-invaluable-tips-tricks-for-building-successful-neural-networks-566aca17a0f1&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2F10-invaluable-tips-tricks-for-building-successful-neural-networks-566aca17a0f1&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F10-invaluable-tips-tricks-for-building-successful-neural-networks-566aca17a0f1&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2F10-invaluable-tips-tricks-for-building-successful-neural-networks-566aca17a0f1&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----566aca17a0f1--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----566aca17a0f1--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://andre-ye.medium.com/?source=post_page-----566aca17a0f1--------------------------------", "anchor_text": ""}, {"url": "https://andre-ye.medium.com/?source=post_page-----566aca17a0f1--------------------------------", "anchor_text": "Andre Ye"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fbe743a65b006&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F10-invaluable-tips-tricks-for-building-successful-neural-networks-566aca17a0f1&user=Andre+Ye&userId=be743a65b006&source=post_page-be743a65b006----566aca17a0f1---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F566aca17a0f1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F10-invaluable-tips-tricks-for-building-successful-neural-networks-566aca17a0f1&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F566aca17a0f1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F10-invaluable-tips-tricks-for-building-successful-neural-networks-566aca17a0f1&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/photos/11KDtiUWRq4", "anchor_text": "Unsplash"}, {"url": "https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/1200px-Logistic-curve.svg.png", "anchor_text": "Wikimedia"}, {"url": "https://mlfromscratch.com/activation-functions-explained/#/", "anchor_text": "ML From Scratch"}, {"url": "https://towardsdatascience.com/manifold-learning-t-sne-lle-isomap-made-easy-42cfd61f5183", "anchor_text": "manifold learning techniques like t-SNE"}, {"url": "https://www.tensorflow.org/images/linear-relationships.png", "anchor_text": "Tensorflow"}, {"url": "https://keras.io/api/applications/", "anchor_text": "using pre-trained image recognition models in Keras"}, {"url": "https://keras.io/examples/nlp/pretrained_word_embeddings/", "anchor_text": "using pre-trained embeddings in Keras"}, {"url": "https://towardsdatascience.com/a-comprehensive-guide-to-working-with-recurrent-neural-networks-in-keras-f3b2d5e2fa7f?source=---------4------------------", "anchor_text": "guide to using embeddings in Keras"}, {"url": "https://medium.com/analytics-vidhya/if-rectified-linear-units-are-linear-how-do-they-add-nonlinearity-40247d3e4792", "anchor_text": "this"}, {"url": "https://towardsdatascience.com/5-perspectives-to-why-dropout-works-so-well-1c10617b8028", "anchor_text": "here"}, {"url": "https://www.researchgate.net/figure/9-An-illustration-of-the-dropout-mechanism-within-the-proposed-CNN-a-Shows-a_fig23_317277576", "anchor_text": "Source"}, {"url": "https://medium.com/analytics-vidhya/you-dont-understand-neural-networks-until-you-understand-the-universal-approximation-theorem-85b3e7677126?source=your_stories_page---------------------------", "anchor_text": "Universal Approximation Theorem"}, {"url": "https://towardsdatascience.com/a-short-practical-guide-to-callbacks-in-neural-network-training-3a4d69568aef?source=your_stories_page---------------------------", "anchor_text": "Reduce Learning Rate on Plateau"}, {"url": "https://towardsdatascience.com/if-youre-hyped-about-gpt-3-writing-code-you-haven-t-heard-of-nas-19c8c30fcc8a?source=your_stories_page---------------------------", "anchor_text": "very new and promising"}, {"url": "https://keras.io/guides/preprocessing_layers/", "anchor_text": "here"}, {"url": "https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html", "anchor_text": "Keras"}, {"url": "https://medium.com/swlh/machine-learning-algorithms-are-much-more-fragile-than-you-think-25fdb3939fee?source=your_stories_page---------------------------", "anchor_text": "adversarial/malicious inputs"}, {"url": "https://towardsdatascience.com/you-shouldnt-ignore-all-these-parameters-in-keras-s-imagedatagenerator-eaa0530c76db?source=your_stories_page---------------------------", "anchor_text": "here"}, {"url": "https://medium.com/analytics-vidhya/the-strategy-that-increases-model-accuracy-every-time-guaranteed-6ee5e476262d", "anchor_text": "this article"}, {"url": "https://medium.com/analytics-vidhya/breaking-down-the-innovative-deep-learning-behind-google-translate-355889e104f1?source=your_stories_page---------------------------", "anchor_text": "Google Neural Machine Translation"}, {"url": "https://towardsdatascience.com/the-clever-trick-behind-googles-inception-the-1-1-convolution-58815b20113?source=your_stories_page---------------------------", "anchor_text": "1x1 convolutions"}, {"url": "https://towardsdatascience.com/a-comprehensive-guide-to-working-with-recurrent-neural-networks-in-keras-f3b2d5e2fa7f?source=your_stories_page---------------------------", "anchor_text": "RNN designs"}, {"url": "https://www.jeremyjordan.me/convnet-architectures/", "anchor_text": "Jeremy Jordan"}, {"url": "https://medium.com/analytics-vidhya/11-essential-neural-network-architectures-visualized-explained-7fc7da3486d8", "anchor_text": "11 Essential Neural Network Architectures, Visualized & ExplainedStandard, Recurrent, Convolutional, & Autoencoder Networksmedium.com"}, {"url": "https://medium.com/tag/data-science?source=post_page-----566aca17a0f1---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----566aca17a0f1---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----566aca17a0f1---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/ai?source=post_page-----566aca17a0f1---------------ai-----------------", "anchor_text": "AI"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----566aca17a0f1---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F566aca17a0f1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F10-invaluable-tips-tricks-for-building-successful-neural-networks-566aca17a0f1&user=Andre+Ye&userId=be743a65b006&source=-----566aca17a0f1---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F566aca17a0f1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F10-invaluable-tips-tricks-for-building-successful-neural-networks-566aca17a0f1&user=Andre+Ye&userId=be743a65b006&source=-----566aca17a0f1---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F566aca17a0f1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F10-invaluable-tips-tricks-for-building-successful-neural-networks-566aca17a0f1&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----566aca17a0f1--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F566aca17a0f1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F10-invaluable-tips-tricks-for-building-successful-neural-networks-566aca17a0f1&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----566aca17a0f1---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----566aca17a0f1--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----566aca17a0f1--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----566aca17a0f1--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----566aca17a0f1--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----566aca17a0f1--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----566aca17a0f1--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----566aca17a0f1--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----566aca17a0f1--------------------------------", "anchor_text": ""}, {"url": "https://andre-ye.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://andre-ye.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Andre Ye"}, {"url": "https://andre-ye.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "9.8K Followers"}, {"url": "https://andre-ye.medium.com/membership", "anchor_text": "https://andre-ye.medium.com/membership"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fbe743a65b006&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F10-invaluable-tips-tricks-for-building-successful-neural-networks-566aca17a0f1&user=Andre+Ye&userId=be743a65b006&source=post_page-be743a65b006--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Ff44a966e4ff1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F10-invaluable-tips-tricks-for-building-successful-neural-networks-566aca17a0f1&newsletterV3=be743a65b006&newsletterV3Id=f44a966e4ff1&user=Andre+Ye&userId=be743a65b006&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}