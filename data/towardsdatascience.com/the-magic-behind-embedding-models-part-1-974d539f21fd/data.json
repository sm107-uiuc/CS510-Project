{"url": "https://towardsdatascience.com/the-magic-behind-embedding-models-part-1-974d539f21fd", "time": 1683001679.516667, "path": "towardsdatascience.com/the-magic-behind-embedding-models-part-1-974d539f21fd/", "webpage": {"metadata": {"title": "About Text Vectorization. The magic of converting text to numbers | by Mohamed Gharibi | Towards Data Science", "h1": "About Text Vectorization", "description": "This post will walk you through the basics of text vectorization which is converting text to vectors (list of numbers). In this post, we present Bag of Words (BOW) and its flavors: Frequency Vectors\u2026"}, "outgoing_paragraph_urls": [], "all_paragraphs": ["This post will walk you through the basics of text vectorization which is converting text to vectors (list of numbers). In this post, we present Bag of Words (BOW) and its flavors: Frequency Vectors, One Hot Encoding (OHE), and Term Frequency/Inverse Document Frequency (TF/IDF).", "Representing text with numbers has many advantages, mainly:", "BOW is a technique to parse the features of a document. The meanings of features are the characteristics and properties that you can use to make a decision (to buy a house you look for few features such as how many rooms and its location). The features of the text are how many unique words in the corpus and the occurrence for each word, etc. BOW is a feature extraction technique in which the output is a vector space that represents each document in the corpus. The length of this vector (dimensions) corresponds to the number of unique words in the corpus (no repetition, each word occurs only once). BOW model has different flavors where each extends or modifies the base BOW. Next will discuss three different vectors: Frequency vectors (count vectors), One Hot Encoding, and Term Frequency/Inverse Document Frequency.", "This is the simplest encoding technique, yet it is still effective in some use cases. Simply we fill the document vector with the count of how many times each word appeared in the document. As an example, let us say our corpus has two documents. While the first one contains \u201cAlice loves pasta\u201d, the second document contains \u201cAlice loves fish. Alice and Bob are friends\u201d. To represent the count we can either use a table or JavaScript Object Notation (JSON) as bellow:", "As you can see we have 8 unique words in our corpus. Therefore, our vector will have a size of 8. To represent document 1, we simply take the first row in our table [1, 1, 1, 0, 0, 0, 0, 0]. This vector helps in comparing documents. While this technique is helpful in some use cases it has some limitations such as: does not keep the document structure (does not keep the order of the words, rather it just counts) and it also has the sparsity problem (most of the values in the vector are zeros, which increase the time complexity and add bias for the model, and the stopping words (such as \u2018and\u2019, \u2018or\u2019, \u2018is\u2019, \u2018the\u2019, etc.) appear many times more than the other words. Therefore, we use some techniques such as Stemming and Lemmatization. We also remove the stopping words and the rare words that appeared only a few times in the entire corpus.", "As discussed in frequency vectors, tokens that appear frequently have more magnitude than others that appeared less. Therefore, the OHE vector provides a boolean vector as a solution for this problem where we fill the vector with only 1\u2019s and 0\u2019s. We place 1 if the word appears in the document (1 instead of the count) and 0 otherwise. Document 2 can be presented as [1, 1, 0, 1, 1, 1, 1, 1].", "One Hot Encoding can also be used to represent the words. 1 for the word that we want to represent and 0 for the rest. The word \u201cAlice\u201d can be represented as [1, 0, 0, 0, 0, 0, 0, 0] or we can add the count as well, so \u201cAlice\u201d can be represented as [3, 0, 0, 0, 0, 0, 0, 0] (will discuss this in details in part 2 of this blog).", "So far we have been treating each document as a standalone entity without looking at the context of the corpus. TF/IDF is one of the common techniques to normalize the frequency of tokens in a document with respect to the corpus context. TF/ID represents two things:", "1. Term frequency tf(t, d): how frequently a term (t) occurs in a document (d). If we denote the raw count by f(t, d), then the simplest tf scheme is tf(t, d) = f(t, d) (Other techniques discussed below) and let us denote the total number of words appear in document d by len(d). For example, to rank documents that are most related to the query \u201cthe blue sky\u201d, we count the number of times each word occurs in each document. However, since each document is different in size, it is not fair to compare how many times a word occurs in a document with 10 words and a document with 1M words. Therefore, we scale tf to prevent the bias of long documents as follows:", "Other methods of tf that adjust and reduce the count of most repeated words in a document:", "2) Inverse Document Frequency: It measures how important a term is. IDF reduces the value of common words that appear in different documents. Given our previous example \u201cthe blue sky\u201d the word \u201cthe\u201d is a common word and therefore the term frequency tends to incorrectly emphasize documents with repeated words with fewer values such as \u201cthe\u201d. As a solution, we calculate the log() for the total number of documents (D) divided by n which is the number of documents with t appeared in:", "and finally, TF/IDF can be calculated as:", "Finally, we just add TF-IDF scores in vectors instead of frequency count or OHE.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Ph.D. candidate interested in Machine Learning"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F974d539f21fd&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-magic-behind-embedding-models-part-1-974d539f21fd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-magic-behind-embedding-models-part-1-974d539f21fd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-magic-behind-embedding-models-part-1-974d539f21fd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-magic-behind-embedding-models-part-1-974d539f21fd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----974d539f21fd--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----974d539f21fd--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@gharibimo?source=post_page-----974d539f21fd--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@gharibimo?source=post_page-----974d539f21fd--------------------------------", "anchor_text": "Mohamed Gharibi"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F532ac9d36827&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-magic-behind-embedding-models-part-1-974d539f21fd&user=Mohamed+Gharibi&userId=532ac9d36827&source=post_page-532ac9d36827----974d539f21fd---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F974d539f21fd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-magic-behind-embedding-models-part-1-974d539f21fd&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F974d539f21fd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-magic-behind-embedding-models-part-1-974d539f21fd&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@art_maltsev", "anchor_text": "Artem Maltsev"}, {"url": "https://www.oreilly.com/library/view/applied-text-analysis/9781491963036/ch04.html", "anchor_text": "Applied Text Analysis with Python by Tony Ojeda, Rebecca Bilbro, Benjamin Bengfort"}, {"url": "https://en.wikipedia.org/wiki/Tf%E2%80%93idf#:~:targetText=In%20information%20retrieval%2C%20tf%E2%80%93idf,in%20a%20collection%20or%20corpus.", "anchor_text": "Wikipedia: TF-IDF"}, {"url": "https://en.wikipedia.org/wiki/Bag-of-words_model", "anchor_text": "Wikipedia: Bag-of-words"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----974d539f21fd---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/embedding?source=post_page-----974d539f21fd---------------embedding-----------------", "anchor_text": "Embedding"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----974d539f21fd---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/text-mining?source=post_page-----974d539f21fd---------------text_mining-----------------", "anchor_text": "Text Mining"}, {"url": "https://medium.com/tag/bag-of-words?source=post_page-----974d539f21fd---------------bag_of_words-----------------", "anchor_text": "Bag Of Words"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F974d539f21fd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-magic-behind-embedding-models-part-1-974d539f21fd&user=Mohamed+Gharibi&userId=532ac9d36827&source=-----974d539f21fd---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F974d539f21fd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-magic-behind-embedding-models-part-1-974d539f21fd&user=Mohamed+Gharibi&userId=532ac9d36827&source=-----974d539f21fd---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F974d539f21fd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-magic-behind-embedding-models-part-1-974d539f21fd&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----974d539f21fd--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F974d539f21fd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-magic-behind-embedding-models-part-1-974d539f21fd&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----974d539f21fd---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----974d539f21fd--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----974d539f21fd--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----974d539f21fd--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----974d539f21fd--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----974d539f21fd--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----974d539f21fd--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----974d539f21fd--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----974d539f21fd--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@gharibimo?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@gharibimo?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Mohamed Gharibi"}, {"url": "https://medium.com/@gharibimo/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "98 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F532ac9d36827&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-magic-behind-embedding-models-part-1-974d539f21fd&user=Mohamed+Gharibi&userId=532ac9d36827&source=post_page-532ac9d36827--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F532ac9d36827%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-magic-behind-embedding-models-part-1-974d539f21fd&user=Mohamed+Gharibi&userId=532ac9d36827&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}