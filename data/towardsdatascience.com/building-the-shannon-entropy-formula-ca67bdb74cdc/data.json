{"url": "https://towardsdatascience.com/building-the-shannon-entropy-formula-ca67bdb74cdc", "time": 1683007114.6536992, "path": "towardsdatascience.com/building-the-shannon-entropy-formula-ca67bdb74cdc/", "webpage": {"metadata": {"title": "Building the Shannon entropy formula | by Alexandru Frujina | Towards Data Science", "h1": "Building the Shannon entropy formula", "description": "The writing of this article is a result of trying to understand the decision tree algorithm in which the Shannon entropy formula can be used. The article aims to present an intuitive reasoning behind\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/u/ff11baf3f5fe?source=post_page-----ca67bdb74cdc--------------------------------", "anchor_text": "Luis Serrano", "paragraph_index": 31}, {"url": "https://medium.com/udacity/shannon-entropy-information-gain-and-picking-balls-from-buckets-5810d35d54b4", "anchor_text": "\u201cShannon Entropy, Information Gain, and Picking Balls from Buckets\u201d", "paragraph_index": 31}], "all_paragraphs": ["The writing of this article is a result of trying to understand the decision tree algorithm in which the Shannon entropy formula can be used. The article aims to present an intuitive reasoning behind the formula by first illustrating entropy with an example and then building up to the formula step-by-step.", "Entropy is a measure of uncertainty and was introduced in the field of information theory by Claude E. Shannon. Two related quantities can be distinguished in this context: entropy, which deals with a set of events, and self-entropy, which is associated with a single event.", "A fundamental idea to remember is that information and entropy are directly related.", "Self-entropy measures the uncertainty or information contained in an event.", "When considering self-entropy a highly uncertain event has both high entropy and provides a high amount of information while a certain event has both low entropy and provides a low amount of information.", "Taking the extreme cases, an event that will always happen has zero self-entropy while an event that will never happen has infinite self-entropy.", "Entropy, on the other hand, measures the average amount of self-entropy that all the events contribute to a system.", "To illustrate both entropy types, consider you have a container in which you can only place four balls that can be either green or red as in Figure 1. This leads to a number of five configurations where the ordering does not matter.", "Considering container configuration 1 the entropy of the entire system is zero as there is no uncertainty associated with the event of extracting a ball as it will always be red. The self-entropy of extracting a red ball is zero and infinite for the green ball.", "In the container configuration 2 the entropy of the entire system is low as one is likely to extract a red ball from the container. In this case the self-entropy of extracting a green ball is high while the self-entropy of extracting a red ball is low.", "The last important case is container configuration 3 as it provides maximum entropy because both events are equally likely. The self-entropy of these two events is medium and equal.", "Container configurations 4 and 5 are identical to container configuration 2 and 1 respectively by reversing the green and red colors.", "Self-entropy for above events can be summarized by the graph in Figure 2.", "In the same way, the entropy of a system of two events can be summarized by the graph in Figure 3.", "As entropy and information are directly related measuring one also leads to a way of measuring the other.", "One, more intuitive, way to represent information is to use binary digits, or bits. Integer values, for example, can be represented as bit patterns of zeros and ones.", "Events like extracting a ball from a container or messages like \u201cthe away team won the game\u201d can also be represented by certain bit patterns as in Table 1.", "A bit pattern can be associated with a probability which depends on the number of bits. For example, any single bit pattern has a probability of 1/2 while any bit pattern of two bits has a probability of 1/4 and so on like in Figure 4.", "In general, the probability of a certain bit pattern is:", "and, conversely, by applying log base 2 on both sides:", "The number of information bits can be obtained for the entire probability range of [0, 1] as in Figure 4 which is close to the intuition in Figure 2.", "When measuring the entropy of a system the average self-entropy contribution of each event is taken into account.", "To give an example consider an event (or message) that contains 10 bits of information. If that message occurs with a probability of 0.2 then its average contribution to the entire system is 10 bits * 0.2 = 2 bits so, taking into account the formula for self-entropy the average contribution of each event is:", "The entropy contained in a system of n events with their associated probabilities is the sum of all these average contributions:", "Coming back to the first illustrated example and considering a system of two events the formula becomes:", "As the sum of all probabilities equals 1 the above can be replaced by:", "Graphing the formula for the entropy for a system of two events in Figure 5 we can again confirm the intuition in Figure 3.", "Table 2 presents the of values obtained when applying the self-entropy and entropy formulas for a system of two events. Event 1 can be associated with \u201cextracting a red ball\u201d and event 2 with \u201cextracting a green ball\u201d as in the illustrated example.", "I hope this provides an insight into how the entropy formula can be built-up.", "Looking back at the purpose of the article, decision trees use the Shannon entropy formula to pick a feature that splits the data set recursively into sub-sets with a high degree of uniformity (low entropy value). Splitting the data set more uniformly results in the decision tree with less depth which makes it easier to use.", "If you have comments or suggestions I will be happy to hear from you!", "The example represented in Figure 1 is mostly a boiled-down version of Luis Serrano\u2019s in the article \u201cShannon Entropy, Information Gain, and Picking Balls from Buckets\u201d", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Embedded systems engineer with an interest in machine learning"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fca67bdb74cdc&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-the-shannon-entropy-formula-ca67bdb74cdc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-the-shannon-entropy-formula-ca67bdb74cdc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-the-shannon-entropy-formula-ca67bdb74cdc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-the-shannon-entropy-formula-ca67bdb74cdc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----ca67bdb74cdc--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ca67bdb74cdc--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@alexandrufrujina?source=post_page-----ca67bdb74cdc--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@alexandrufrujina?source=post_page-----ca67bdb74cdc--------------------------------", "anchor_text": "Alexandru Frujina"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6ca1bf99e08&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-the-shannon-entropy-formula-ca67bdb74cdc&user=Alexandru+Frujina&userId=6ca1bf99e08&source=post_page-6ca1bf99e08----ca67bdb74cdc---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fca67bdb74cdc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-the-shannon-entropy-formula-ca67bdb74cdc&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fca67bdb74cdc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-the-shannon-entropy-formula-ca67bdb74cdc&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@kellysikkema?utm_source=medium&utm_medium=referral", "anchor_text": "Kelly Sikkema"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://medium.com/u/ff11baf3f5fe?source=post_page-----ca67bdb74cdc--------------------------------", "anchor_text": "Luis Serrano"}, {"url": "https://medium.com/udacity/shannon-entropy-information-gain-and-picking-balls-from-buckets-5810d35d54b4", "anchor_text": "\u201cShannon Entropy, Information Gain, and Picking Balls from Buckets\u201d"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----ca67bdb74cdc---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/entropy?source=post_page-----ca67bdb74cdc---------------entropy-----------------", "anchor_text": "Entropy"}, {"url": "https://medium.com/tag/data-science?source=post_page-----ca67bdb74cdc---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/information-theory?source=post_page-----ca67bdb74cdc---------------information_theory-----------------", "anchor_text": "Information Theory"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fca67bdb74cdc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-the-shannon-entropy-formula-ca67bdb74cdc&user=Alexandru+Frujina&userId=6ca1bf99e08&source=-----ca67bdb74cdc---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fca67bdb74cdc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-the-shannon-entropy-formula-ca67bdb74cdc&user=Alexandru+Frujina&userId=6ca1bf99e08&source=-----ca67bdb74cdc---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fca67bdb74cdc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-the-shannon-entropy-formula-ca67bdb74cdc&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ca67bdb74cdc--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fca67bdb74cdc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-the-shannon-entropy-formula-ca67bdb74cdc&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----ca67bdb74cdc---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----ca67bdb74cdc--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----ca67bdb74cdc--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----ca67bdb74cdc--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----ca67bdb74cdc--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----ca67bdb74cdc--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----ca67bdb74cdc--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----ca67bdb74cdc--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----ca67bdb74cdc--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@alexandrufrujina?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@alexandrufrujina?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Alexandru Frujina"}, {"url": "https://medium.com/@alexandrufrujina/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "5 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6ca1bf99e08&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-the-shannon-entropy-formula-ca67bdb74cdc&user=Alexandru+Frujina&userId=6ca1bf99e08&source=post_page-6ca1bf99e08--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F6ca1bf99e08%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-the-shannon-entropy-formula-ca67bdb74cdc&user=Alexandru+Frujina&userId=6ca1bf99e08&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}