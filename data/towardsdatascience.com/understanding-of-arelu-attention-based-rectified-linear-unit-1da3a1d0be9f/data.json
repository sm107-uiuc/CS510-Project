{"url": "https://towardsdatascience.com/understanding-of-arelu-attention-based-rectified-linear-unit-1da3a1d0be9f", "time": 1683012079.814261, "path": "towardsdatascience.com/understanding-of-arelu-attention-based-rectified-linear-unit-1da3a1d0be9f/", "webpage": {"metadata": {"title": "Understanding of ARELU (Attention-based Rectified Linear Unit) | by dhwani mehta | Towards Data Science", "h1": "Understanding of ARELU (Attention-based Rectified Linear Unit)", "description": "Activation function is one of the building blocks of neural networks which has crucial impact upon the training procedure. The Rectified Linear Activation Function (i.e. RELU) has rapidly become the\u2026"}, "outgoing_paragraph_urls": [{"url": "https://github.com/densechen/AReLU/blob/master/activations/arelu.py", "anchor_text": "code", "paragraph_index": 11}, {"url": "https://arxiv.org/abs/2006.13858", "anchor_text": "paper", "paragraph_index": 11}], "all_paragraphs": ["Activation function is one of the building blocks of neural networks which has crucial impact upon the training procedure. The Rectified Linear Activation Function (i.e. RELU) has rapidly become the default activation function when developing most types of neural networks by reason of rectifier function being trivial to implement, representational sparsity as well as almost completely avoid the problem of vanishing gradients by having gradient slope of 1.", "This article is intended to demonstrate a freshly born new perspective of learnable activation function through formulating them with element-wise attention mechanism. The resulting activation function coined ARELU facilitates fast network training under small learning rates which is highly suitable for transfer learning as well as makes the network training more resistant to gradient vanishing.", "A neural network is considered to be an effort to mimic human brain actions in a simplified manner. Attention Mechanism is also an attempt to implement the same action of selectively concentrating on a few relevant things, while ignoring others in deep neural networks.", "Attention is arguably one of the most powerful concepts in the deep learning field nowadays as it directs the network to learn what part of input contributes more to the output. Some of the attention mechanisms with attention map at different granularities are as follows :", "Out of all the attention mechanisms, Element-wise attention is most fine grained as each element in the feature volume receives different amount of attention. Hence, ARELU is devised such that for each layer of network, an element-wise attention module learning sign based attention map which scales elements base on its sign is obtained leading to amplification of positive elements and suppression of negative elements.", "The entire mechanism can be build up as follows :", "The modulator function in equation (ii) performs the element wise multiplication of the input vector with the attention map. In order to meet the dimensionality requirements, attention map is extended to the full dimensions of input vector as in equation (i).", "For amplification of positive elements and clampdown the negative ones, the attention map scales based upon its sign in ELSA. The attention is determined by two parameters alpha and beta leading to sign wise attention mechanism with two learnable parameters.", "From equation (v) its evidently justifies that ARELU magnifies the positive elements and on the flip side diminishes the negative elements.", "Optimization involves minimization of ErrorFunction. Applying chain rule to minimize the error function, the gradients obtained for corresponding parameters in equation (v) are as follows :", "ARELU activation being able to augment gradients over activated inputs makes it suitable to avoid gradient vanishing leading towards accelerate training convergence of the model. The comparative study of various activation functions shows that ARELU performs best with only two learnable parameters and is also versatile for different network architectures.", "Official Pytorch Implementation of ARELU: [code] [paper]", "Obtaining ARELU through adding the attention and RELU module, has evidently shown to significantly boost the performance of most mainstream network architectures with introduction of not more than two extra learnable parameters per layer. ARELU not only makes the network training immune to gradient vanishing but also enables fast learning with smaller learning rates making it especially suited for transfer learning.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Machine Learning | Data Scientist | Founder @clique_org"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F1da3a1d0be9f&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-of-arelu-attention-based-rectified-linear-unit-1da3a1d0be9f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-of-arelu-attention-based-rectified-linear-unit-1da3a1d0be9f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-of-arelu-attention-based-rectified-linear-unit-1da3a1d0be9f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-of-arelu-attention-based-rectified-linear-unit-1da3a1d0be9f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----1da3a1d0be9f--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1da3a1d0be9f--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@dhwanidm1996?source=post_page-----1da3a1d0be9f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@dhwanidm1996?source=post_page-----1da3a1d0be9f--------------------------------", "anchor_text": "dhwani mehta"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F269bc58b71d5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-of-arelu-attention-based-rectified-linear-unit-1da3a1d0be9f&user=dhwani+mehta&userId=269bc58b71d5&source=post_page-269bc58b71d5----1da3a1d0be9f---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1da3a1d0be9f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-of-arelu-attention-based-rectified-linear-unit-1da3a1d0be9f&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1da3a1d0be9f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-of-arelu-attention-based-rectified-linear-unit-1da3a1d0be9f&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@sloppyperfectionist?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Hans-Peter Gauster"}, {"url": "https://unsplash.com/s/photos/learning?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://arxiv.org/abs/2006.13858", "anchor_text": "image source]"}, {"url": "https://arxiv.org/abs/2006.13858", "anchor_text": "image source"}, {"url": "https://github.com/densechen/AReLU/blob/master/activations/arelu.py", "anchor_text": "code"}, {"url": "https://arxiv.org/abs/2006.13858", "anchor_text": "paper"}, {"url": "https://medium.com/tag/computer-vision?source=post_page-----1da3a1d0be9f---------------computer_vision-----------------", "anchor_text": "Computer Vision"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----1da3a1d0be9f---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/convolution-neural-net?source=post_page-----1da3a1d0be9f---------------convolution_neural_net-----------------", "anchor_text": "Convolution Neural Net"}, {"url": "https://medium.com/tag/research?source=post_page-----1da3a1d0be9f---------------research-----------------", "anchor_text": "Research"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1da3a1d0be9f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-of-arelu-attention-based-rectified-linear-unit-1da3a1d0be9f&user=dhwani+mehta&userId=269bc58b71d5&source=-----1da3a1d0be9f---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1da3a1d0be9f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-of-arelu-attention-based-rectified-linear-unit-1da3a1d0be9f&user=dhwani+mehta&userId=269bc58b71d5&source=-----1da3a1d0be9f---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1da3a1d0be9f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-of-arelu-attention-based-rectified-linear-unit-1da3a1d0be9f&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1da3a1d0be9f--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F1da3a1d0be9f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-of-arelu-attention-based-rectified-linear-unit-1da3a1d0be9f&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----1da3a1d0be9f---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----1da3a1d0be9f--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----1da3a1d0be9f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----1da3a1d0be9f--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----1da3a1d0be9f--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----1da3a1d0be9f--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----1da3a1d0be9f--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----1da3a1d0be9f--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----1da3a1d0be9f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@dhwanidm1996?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@dhwanidm1996?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "dhwani mehta"}, {"url": "https://medium.com/@dhwanidm1996/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "34 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F269bc58b71d5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-of-arelu-attention-based-rectified-linear-unit-1da3a1d0be9f&user=dhwani+mehta&userId=269bc58b71d5&source=post_page-269bc58b71d5--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fb07dbb45808c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-of-arelu-attention-based-rectified-linear-unit-1da3a1d0be9f&newsletterV3=269bc58b71d5&newsletterV3Id=b07dbb45808c&user=dhwani+mehta&userId=269bc58b71d5&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}