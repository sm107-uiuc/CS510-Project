{"url": "https://towardsdatascience.com/breaking-neural-networks-with-adversarial-attacks-f4290a9a45aa", "time": 1682994935.2107341, "path": "towardsdatascience.com/breaking-neural-networks-with-adversarial-attacks-f4290a9a45aa/", "webpage": {"metadata": {"title": "Breaking neural networks with adversarial attacks | by Anant Jain | Towards Data Science", "h1": "Breaking neural networks with adversarial attacks", "description": "As many of you may know, Deep Neural Networks are highly expressive machine learning networks that have been around for many decades. In 2012, with gains in computing power and improved tooling, a\u2026"}, "outgoing_paragraph_urls": [{"url": "http://www.cleverhans.io", "anchor_text": "blog", "paragraph_index": 19}, {"url": "https://anantja.in", "anchor_text": "https://anantja.in", "paragraph_index": 21}], "all_paragraphs": ["As many of you may know, Deep Neural Networks are highly expressive machine learning networks that have been around for many decades. In 2012, with gains in computing power and improved tooling, a family of these machine learning models called ConvNets started achieving state of the art performance on visual recognition tasks. Up to this point, machine learning algorithms simply didn\u2019t work well enough for anyone to be surprised when it failed to do the right thing.", "In 2014, a group of researchers at Google and NYU found that it was far too easy to fool ConvNets with an imperceivable, but carefully constructed nudge in the input. Let\u2019s look at an example. We start with an image of a panda, which our neural network correctly recognizes as a \u201cpanda\u201d with 57.7% confidence. Add a little bit of carefully constructed noise and the same neural network now thinks this is an image of a gibbon with 99.3% confidence! This is, clearly, an optical illusion \u2014 but for the neural network. You and I can clearly tell that both the images look like pandas \u2014 in fact, we can\u2019t even tell that some noise has been added to the original image to construct the adversarial example on the right!", "In 2017, another group demonstrated that it\u2019s possible for these adversarial examples to generalize to the real world by showing that when printed out, an adversarially constructed image will continue to fool neural networks under different lighting and orientations:", "Another interesting work, titled \u201cAccessorize to a Crime: Real and Stealthy Attacks on State-of-the-Art Face Recognition\u201d showed that one can fool facial recognition software by constructing adversarial glasses by dodging face detection altogether. These glasses could let you impersonate someone else as well:", "Shortly after, another research group demonstrated various methods for constructing stop signs that can fool models by placing various stickers on a stop sign. The perturbations were designed to mimic graffiti, and thus \u201chide in the human psyche.\u201d", "\u201cAdversarial Patch\u201d, a paper published at NIPS 2017 demonstrated how to generate a patch that can be placed anywhere within the field of view of the classifier and cause the classifier to output a targeted class. In the video below, a banana is correctly classified as a banana. Placing a sticker with a toaster printed on it is not enough to fool the network and it still continues to classify it as a banana. However, with a carefully constructed \u201cadversarial patch\u201d, it\u2019s easy to trick the network into thinking that it\u2019s a toaster:", "To quote the authors, \u201cthis attack was significant because the attacker does not need to know what image they are attacking when constructing the attack. After generating an adversarial patch, the patch could be widely distributed across the Internet for other attackers to print out and use.\u201d", "What these examples show us is that our neural networks are still quite fragile when explicitly attacked by an adversary in this way. Let\u2019s dive deeper!", "First, as we saw above, it\u2019s easy to attain high confidence in the incorrect classification of an adversarial example \u2014 recall that in the first \u201cpanda\u201d example we looked at, the network is less sure of an actual image looking like a panda (57.7%) than our adversarial example on the right looking like a gibbon (99.3%). Another intriguing point is how imperceptibly little noise we needed to add to fool the system \u2014 after all, clearly, the added noise is not enough to fool us, the humans.", "Second, the adversarial examples don\u2019t depend much on the specific deep neural network used for the task \u2014 an adversarial example trained for one network seems to confuse another one as well. In other words, multiple classifiers assign the same (wrong) class to an adversarial example. This \u201ctransferability\u201d enables attackers to fool systems in what are known as \u201cblack-box attacks\u201d where they don\u2019t have access to the model\u2019s architecture, parameters or even the training data used to train the network.", "Not really. Let\u2019s quickly look at two categories of defenses that have been proposed so far:", "One of the easiest and most brute-force way to defend against these attacks is to pretend to be the attacker, generate a number of adversarial examples against your own network, and then explicitly train the model to not be fooled by them. This improves the generalization of the model but hasn\u2019t been able to provide a meaningful level of robustness \u2014 in fact, it just ends up being a game of whack-a-mole where attackers and defenders are just trying to one-up each other.", "In defensive distillation, we \u201ctrain a secondary model whose surface is smoothed in the directions an attacker will typically try to exploit, making it difficult for them to discover adversarial input tweaks that lead to incorrect categorization\u201d [6]. The reason it works is that unlike the first model, the second model is trained on the primary model\u2019s \u201csoft\u201d probability outputs, rather than the \u201chard\u201d (0/1) true labels from the real training data. This technique was shown to have some success defending initial variants of adversarial attacks but has been beaten by more recent ones, like the Carlini-Wagner attack, which is the current benchmark for evaluating the robustness of a neural network against adversarial attacks.", "Let\u2019s try to develop an intuition behind what\u2019s going on here. Most of the time, machine learning models work very well but only work on a very small amount of all the many possible inputs they might encounter. In a high-dimensional space, a very small perturbation in each individual input pixel can be enough to cause a dramatic change in the dot products down the neural network. So, it\u2019s very easy to nudge the input image to a point in high-dimensional space that our networks have never seen before. This is a key point to keep in mind: the high dimensional spaces are so sparse that most of our training data is concentrated in a very small region known as the manifold. Although our neural networks are nonlinear by definition, the most common activation function we use to train them, the Rectifier Linear Unit, or ReLu, is linear for inputs greater than 0.", "ReLu became the preferred activations function due to its ease of trainability. Compared to sigmoid or tanh activation functions that simply saturate to a capped value at high activations and thus have gradients getting \u201cstuck\u201d very close to 0, the ReLu has a non-zero gradient everywhere to the right of 0, making it much more stable and faster to train. But, that also makes it possible to push the ReLu activation function to arbitrarily high values.", "Looking at this trade-off between trainability and robustness to adversarial attacks, we can conclude that the neural network models we have been using are intrinsically flawed. Ease of optimization has come at the cost of models that are easily misled.", "The real problem here is that our machine learning models exhibit unpredictable and overly confident behavior outside of the training distribution. Adversarial examples are just a subset of this broader problem. We would like our models to be able to exhibit appropriately low confidence when they\u2019re operating in regions they have not seen before. We want them to \u201cfail gracefully\u201d when used in production.", "According to Ian Goodfellow, one of the pioneers of this field, \u201cmany of the most important problems still remain open, both in terms of theory and in terms of applications. We do not yet know whether defending against adversarial examples is a theoretically hopeless endeavor or if an optimal strategy would give the defender an upper ground. On the applied side, no one has yet designed a truly powerful defense algorithm that can resist a wide variety of adversarial example attack algorithms.\u201d", "If nothing else, the topic of adversarial examples gives us an insight into what most researchers have been saying for a while \u2014 despite the breakthroughs, we are still in the infancy of machine learning and still have a long way to go here. Machine Learning is just another tool, susceptible to adversarial attacks which can have huge implications in a world where we trust them with human lives via self-driving cars and other automation.", "Here are the links to the papers and posts referenced above. I also highly recommend checking out Ian Goodfellow\u2019s blog on the topic.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Now: engineering @brexhq. Past: Co-founder @commonlounge, @eagerpanda. @iitdelhi \u201912. More at https://anantja.in"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Ff4290a9a45aa&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbreaking-neural-networks-with-adversarial-attacks-f4290a9a45aa&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbreaking-neural-networks-with-adversarial-attacks-f4290a9a45aa&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbreaking-neural-networks-with-adversarial-attacks-f4290a9a45aa&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbreaking-neural-networks-with-adversarial-attacks-f4290a9a45aa&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----f4290a9a45aa--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f4290a9a45aa--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@anantja_in?source=post_page-----f4290a9a45aa--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@anantja_in?source=post_page-----f4290a9a45aa--------------------------------", "anchor_text": "Anant Jain"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7d93bb16f55b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbreaking-neural-networks-with-adversarial-attacks-f4290a9a45aa&user=Anant+Jain&userId=7d93bb16f55b&source=post_page-7d93bb16f55b----f4290a9a45aa---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff4290a9a45aa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbreaking-neural-networks-with-adversarial-attacks-f4290a9a45aa&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff4290a9a45aa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbreaking-neural-networks-with-adversarial-attacks-f4290a9a45aa&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://arxiv.org/abs/1412.6572", "anchor_text": "Explaining and Harnessing Adversarial Examples"}, {"url": "https://arxiv.org/pdf/1607.02533.pdf", "anchor_text": "Adversarial Examples in the Physical World"}, {"url": "https://www.cs.cmu.edu/~sbhagava/papers/face-rec-ccs16.pdf", "anchor_text": "Accessorize to a Crime: Real and Stealthy Attacks on State-of-the-Art Face Recognition"}, {"url": "https://arxiv.org/pdf/1707.08945.pdf", "anchor_text": "Robust Physical-World Attacks on Deep Learning Visual Classification"}, {"url": "https://arxiv.org/pdf/1712.09665.pdf", "anchor_text": "https://arxiv.org/pdf/1712.09665.pdf"}, {"url": "https://arxiv.org/pdf/1712.09665.pdf", "anchor_text": "https://arxiv.org/pdf/1712.09665.pdf"}, {"url": "https://arxiv.org/abs/1412.6572", "anchor_text": "Explaining and Harnessing Adversarial Examples"}, {"url": "http://www.cleverhans.io", "anchor_text": "blog"}, {"url": "https://arxiv.org/abs/1412.6572", "anchor_text": "Explaining and Harnessing Adversarial Examples"}, {"url": "https://arxiv.org/pdf/1607.02533.pdf", "anchor_text": "Adversarial Examples in the Physical World"}, {"url": "https://www.cs.cmu.edu/~sbhagava/papers/face-rec-ccs16.pdf", "anchor_text": "Accessorize to a Crime: Real and Stealthy Attacks on State-of-the-Art Face Recognition"}, {"url": "https://arxiv.org/pdf/1707.08945.pdf", "anchor_text": "Robust Physical-World Attacks on Deep Learning Visual Classification"}, {"url": "https://arxiv.org/pdf/1712.09665.pdf", "anchor_text": "Adversarial Patch"}, {"url": "https://openai.com/blog/adversarial-example-research/", "anchor_text": "Attacking Machine Learning with Adversarial Examples"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----f4290a9a45aa---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----f4290a9a45aa---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/adversarial-example?source=post_page-----f4290a9a45aa---------------adversarial_example-----------------", "anchor_text": "Adversarial Example"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff4290a9a45aa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbreaking-neural-networks-with-adversarial-attacks-f4290a9a45aa&user=Anant+Jain&userId=7d93bb16f55b&source=-----f4290a9a45aa---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff4290a9a45aa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbreaking-neural-networks-with-adversarial-attacks-f4290a9a45aa&user=Anant+Jain&userId=7d93bb16f55b&source=-----f4290a9a45aa---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff4290a9a45aa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbreaking-neural-networks-with-adversarial-attacks-f4290a9a45aa&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f4290a9a45aa--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Ff4290a9a45aa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbreaking-neural-networks-with-adversarial-attacks-f4290a9a45aa&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----f4290a9a45aa---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----f4290a9a45aa--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----f4290a9a45aa--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----f4290a9a45aa--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----f4290a9a45aa--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----f4290a9a45aa--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----f4290a9a45aa--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----f4290a9a45aa--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----f4290a9a45aa--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@anantja_in?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@anantja_in?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Anant Jain"}, {"url": "https://medium.com/@anantja_in/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.1K Followers"}, {"url": "https://anantja.in", "anchor_text": "https://anantja.in"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7d93bb16f55b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbreaking-neural-networks-with-adversarial-attacks-f4290a9a45aa&user=Anant+Jain&userId=7d93bb16f55b&source=post_page-7d93bb16f55b--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fb5170650104b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbreaking-neural-networks-with-adversarial-attacks-f4290a9a45aa&newsletterV3=7d93bb16f55b&newsletterV3Id=b5170650104b&user=Anant+Jain&userId=7d93bb16f55b&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}