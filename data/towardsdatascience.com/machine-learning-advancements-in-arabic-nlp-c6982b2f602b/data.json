{"url": "https://towardsdatascience.com/machine-learning-advancements-in-arabic-nlp-c6982b2f602b", "time": 1683015680.79313, "path": "towardsdatascience.com/machine-learning-advancements-in-arabic-nlp-c6982b2f602b/", "webpage": {"metadata": {"title": "Machine learning advancements in Arabic NLP | by Haaya Naushan | Towards Data Science", "h1": "Machine learning advancements in Arabic NLP", "description": "Natural language processing (NLP) is not a new discipline; its roots date back to the 1600s when philosophers such as Descartes and Leibniz proposed theoretical codes for language. In the past\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.sciencedirect.com/science/article/pii/S1319157818310553#b0155", "anchor_text": "Arabic is the 4th most used language on the Internet", "paragraph_index": 1}, {"url": "https://www.statista.com/topics/5550/internet-usage-in-mena/", "anchor_text": "Statistics from 2018", "paragraph_index": 1}, {"url": "https://dl.acm.org/doi/10.1145/3314941", "anchor_text": "In a paper from May, 2019", "paragraph_index": 3}, {"url": "https://github.com/bakrianoo/aravec", "anchor_text": "AraVec", "paragraph_index": 5}, {"url": "https://github.com/aub-mind/arabert", "anchor_text": "AraBERT", "paragraph_index": 5}, {"url": "http://jalammar.github.io/illustrated-word2vec/", "anchor_text": "this illustrated guide to word embeddings by Jay Alammar", "paragraph_index": 6}, {"url": "https://www.sciencedirect.com/science/article/pii/S1877050917321749", "anchor_text": "Soliman et al. 2017.", "paragraph_index": 7}, {"url": "https://radimrehurek.com/gensim/index.html", "anchor_text": "Gensim developed by Radim Rehurek", "paragraph_index": 9}, {"url": "https://medium.com/@raghavaggarwal0089/bi-lstm-bc3d68da8bd0", "anchor_text": "(bi-LSTM", "paragraph_index": 14}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "Vaswani et al.", "paragraph_index": 16}, {"url": "https://nlp.seas.harvard.edu/2018/04/03/attention.html", "anchor_text": "The Annotated Transformer by the Harvard NLP", "paragraph_index": 16}, {"url": "https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21", "anchor_text": "LSTM or GRU", "paragraph_index": 17}, {"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "BERT in 2019 by researchers from the Google AI Language team", "paragraph_index": 21}, {"url": "https://en.wikipedia.org/wiki/Cloze_test", "anchor_text": "Cloze test", "paragraph_index": 21}, {"url": "https://arxiv.org/abs/2003.00104v3", "anchor_text": "Antoun at al. 2020.", "paragraph_index": 22}, {"url": "https://arxiv.org/abs/1611.04033", "anchor_text": "1.5 billion words Arabic Corpus", "paragraph_index": 22}, {"url": "https://www.aclweb.org/anthology/W19-4619/", "anchor_text": "OSIAN: the Open Source International Arabic News Corpus", "paragraph_index": 22}, {"url": "https://github.com/komari6/Arabic-twitter-corpus-AJGT", "anchor_text": "Arabic Jordanian General Tweets (AJGT) Corpus", "paragraph_index": 23}, {"url": "https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments", "anchor_text": "TrainingArguments", "paragraph_index": 28}, {"url": "https://link.springer.com/chapter/10.1007/978-3-540-70939-8_13", "anchor_text": "Arabic NER corpus (ANERcorp)", "paragraph_index": 35}, {"url": "https://link.springer.com/article/10.1023/A:1007379606734", "anchor_text": "inductive knowledge transfer", "paragraph_index": 36}, {"url": "https://arxiv.org/abs/1706.05098", "anchor_text": "MTL has rapidly gained popularity", "paragraph_index": 36}], "all_paragraphs": ["Natural language processing (NLP) is not a new discipline; its roots date back to the 1600s when philosophers such as Descartes and Leibniz proposed theoretical codes for language. In the past decade, the results of this long history have led to the integration of NLP into our own homes, in the form of digital assistants like Siri and Alexa. Although machine learning has remarkably accelerated the improvement of English NLP techniques, the study of NLP for other languages has always lagged behind.", "As the official language of 22 countries spread across the Middle-East North Africa (MENA) region, Arabic is the 4th most used language on the Internet. Statistics from 2018 show 164 million internet users in the Middle East and 121 million internet users in North Africa.", "As a language, Arabic has complex morphology and various dialects. The complexity increases significantly when considering the informal nature of social-media text and the distinction between Modern Standard Arabic (MSA) and Dialectical Arabic (DA). MSA is used for formal writing and DA is used for informal daily communication; however, both forms are present on social media with the latter being the most common form.", "Further complicating matters, there are numerous dialects, for example the Egyptian dialect is different from the Levantine dialect which is used in Palestine, Jordan, Syria, Lebanon and Israel. Both of these dialects are also distinct from the Gulf dialect used in Kuwait, Bahrain, Qatar, and the United Arab Emirates. In a paper from May, 2019 researchers commented that the inflectional and derivational nature of Arabic language makes monophonic analysis on Arabic more difficult. Simply put, due to the differences between English and Arabic, the advancements to English NLP are not easily transferred to the development of Arabic NLP resources. Additionally, crude Arabic-to-English translations cannot be relied upon as sufficient preprocessing before the application of sophisticated English NLP methods; much is lost in translation.", "Despite all these challenges, the past four years have been fruitful, and Arabic NLP research in areas such as sentiment analysis and machine translation have produced extremely useful resources. My focus is on Arabic social media, since these platforms have been instrumental in the democratic development of the MENA region. Furthermore, in my opinion, social media text is the most accessible form of data available to study this often inscrutable region.", "This will be the first part of a three-part series about Arabic NLP. In this post, I will focus on the two most effective and accessible tools I have used in my research, AraVec and AraBERT. Those of you familiar with current research trends in English NLP, will notice the similarities between these names and the popular English NLP tools, Word2Vec and BERT. For a beginner-friendly overview of English NLP resources, check out my earlier post on Sentiment Analysis with Python where I discuss both Word2Vec and BERT in detail.", "When the influential Word2Vec was released in 2013 by a Google team led by Tomas Mikolov, the use of word embeddings became popular for a wide variety of NLP tasks, achieving state-of-the-art accuracy scores that revolutionized the field. With word embeddings, words are represented as vectors in a continuous space. The embeddings capture the syntactic and semantic relationships between words. I recommend this illustrated guide to word embeddings by Jay Alammar for an excellent introduction to the theory behind this method.", "In 2017, word embeddings were adapted for Arabic as AraVec, a pre-trained distributed word representation (word embedding) open-source project introduced in a paper by Soliman et al. 2017. Currently, the third version of AraVec provides 16 different word embedding models built on top of two different Arabic content domains; Twitter and Wikipedia Arabic articles. In the current version there are two different types of models, uni-gram and n-gram models. Embeddings can be trained in a shallow neural net, or be used as an embedding layer within deep learning model architecture. My preference is usually for the latter despite the longer training time, because it often results in higher accuracy.", "In the original paper, it is stated that the total number of tokens used to build the first iteration models amount to more than 3,300,000,000 tokens. For the original Twitter embedding the researchers collected more than 77,600,000 Arabic tweets posted between 2008 and 2016, obtained from different random geographical locations (in order to account for a variety of dialects). Preprocessing of Arabic characters included removing diacritics, which are marks put above, below, through or on a letter, for the purpose of for normalization. For example, the letter\u201c \u0629\u201d was replaced with \u201c \u0647\u201d, and the letters \u201c\u0623\u201c ,\u201d\u0625\u201d and \u201c\u0622\u201d are replaced with \u201c\u0627\u201d.", "The AraVec models were built using the Python library, Gensim developed by Radim Rehurek, which also contains the implementation for the original Word2Vec model. In the original Word2Vec paper, Mikolov and his team proposed two different model architectures for representing words in a multidimensional vector space: the continuous bag-of-words (CBOW) model and the skip-gram model. The CBOW model learns embeddings by predicting the central word in a sequence, given the other words within that sequence, without regard to their order within the sentence. The skip-gram model is the opposite of the CBOW model since it aims to predict the surrounding contextual words given the central word. The diagram below shows the difference between these two model architectures.", "Compared to the skip-gram model, the advantage of the CBOW model is that it is much faster and has a higher accuracy for frequent words. However, the skip-gram model architecture provides greater accuracy when the training dataset is small, in addition to being better at handling rare words. I experimented with two AraVec n-gram models, the Twitter-CBOW and the Twitter-SkipGram, both with Gensim in combination with the Python NLP libraries, spaCy and NLTK.", "First I needed to clean my Arabic text with the preprocessing steps previously mentioned for normalization. The code snippet below shows the function I used, as adapted from the AraVec project\u2019s repository.", "Next, I loaded AraVec, exported it to a Word2Vec format, and then initialized a spaCy model using the AraVec vectors. The code snippet below outlines the process.", "Lastly, I used the spaCy model to create a tokenizer wrapped around the preprocessing steps to transform raw Arabic social media text to word embeddings.", "I tested the AraVec embeddings with a variety of deep learning models, and in accordance with current research trends, my most successful model was a Bi-directional long short term memory network (bi-LSTM).", "The primary disadvantage of AraVec and other word embeddings is that they provide a single representation for a word that is the same regardless of context. Therefore, words such as \u201cbank\u201d which, depending on context can have different meanings (eg. \u201criver bank\u201d or \u201cinvestment bank\u201d), have a resultant vector that represents an average of the different contexts, thereby not accurately representing either meaning of the word \u201cbank\u201d. This issue is also of concern in Arabic, especially when informal Arabic slang can have different meanings depending on context or region. This deficiency led to my discovery of the AraBERT model, an implementation of the transformer architecture released in February 2020.", "I would argue that the most influential NLP paper of the past few years is the 2017 paper \u201cAttention is all you need\u201d by Vaswani et al. which introduces the transformer architecture. I recommend The Annotated Transformer by the Harvard NLP research group for a thorough explanation of this essential paper that includes code annotations presented in a notebook format.", "Up until the creation of the Transformer model, the most advanced NLP methods involved using word embeddings with convolutional or recurrent neural networks used with attention mechanisms, such as a LSTM or GRU. Attention mechanisms are important as a way of alleviating the vanishing gradient problem by focusing on the most relevant parts of an input sequence for each output sequence. This is achieved by preserving auto-regressive properties so that the ability to predict the future is aided by knowledge of past positional context.", "To capture positional information, recurrent models generate a sequence of hidden states, which is computed as a function of the previous hidden state in combination with the input at a particular position, forming a cycle. This cyclical sequential nature makes parallelization difficult, a problem that could be solved by convolutional networks which would compute all the hidden states in parallel for all input and output positions. Convolutions, however, make it more difficult to learn long range dependencies, with the difficulty being determined by maximum path length. Importantly, the transformer introduced a way to use attention mechanisms without recurrence or convolutions. This allows for parallelization, speeding up training times significantly.", "As seen in the diagram below, the transformer architecture involves an encoder and decoder stack, each with multiple layers, that each have sub-layers (2 for each encoder layer and 3 for each decoder layer), one of which is a feed-forward network (not cyclical). Since the transformer model does not contain recurrence or convolution to account for position, positional encodings are added to input embeddings. The encodings represent the relative or absolute position of tokens in a sequence. This is achieved by each dimension of the positional encoding corresponding to a sinusoidal function that allows the model to easily attend to relative positions. Furthermore, in order to be summed, the dimensionality of the positional encodings is the same as the input and output embeddings.", "To address the issue of path length, the transformer also utilizes multi-head attention over the output of the encoder stack to better learn long range dependencies. Specifically, multi-head attention allows the model to perform attention in parallel by taking the linear projections of the query, key and value matrices, that is then concatenated and projected linearly for a final attention value. The importance of multi-head attention is in allowing the model to jointly attend to information from different subspaces at different positions. As a transduction model relying entirely on self-attention for computing the representations (embeddings) of the input and output sequences, the transformer proves that, as the title states, attention is all you need.", "As mentioned, the main disadvantage of word embeddings is the lack of contextual information being captured by the representations. The transformer architecture was further advanced with the development of BERT in 2019 by researchers from the Google AI Language team. BERT stands for Bi-directional Encoder Representations from Transformers. The main concept of BERT is pre-training bi-directional representations from unlabeled text by jointly conditioning on both left and right context in all layers. This means that a pre-trained BERT model can be fine-tuned with labeled data on a downstream task for state-of-the-art accuracy scores. Pre-training consists of two unsupervised learning tasks. The first is a masked language model (MLM) objective that is inspired by the Cloze test, and second is a next sentence prediction (NSP) task that allows the model to learn sentence relationships. It is important to note the size of these models, the base BERT model is 110 million parameters and the large model is 340 million parameters. That means that using the transformer architecture for Arabic NLP would require constructing a similarly large language model; which would be expensive, both computationally and monetarily.", "AraBERT was released earlier this year, introduced in a paper by Antoun at al. 2020. This pre-trained Arabic language model was developed using BERT and has 110 million parameters mirroring the base BERT model. The pre-training setup for AraBERT includes the two unsupervised tasks of MLM and NSP, following the procedure outlined in the BERT paper. The pre-training data amounted to 24GB (roughly 700 million sentences), and tokenization resulted in a vocabulary size of 64K words, derived from sources such as the 1.5 billion words Arabic Corpus and OSIAN: the Open Source International Arabic News Corpus that consists of 3.5 million articles.", "The Github repository for AraBERT includes examples of how to use this model; there are Colab notebooks for the two most popular Python machine learning frameworks of Tensorflow and Pytorch, as well as a notebook for the Python library Hugging Face transformers. I prefer using the Python library, Hugging Face transformers to test out new pre-trained models due to its simplicity and ease of implementation. The examples showcase fine-tuning on the Arabic Jordanian General Tweets (AJGT) Corpus for sentiment classification. The AJGT dataset consists of 1,800 tweets in MSA or the Jordanian dialect, annotated as either positive or negative.", "In order to better fit the Arabic language, the authors introduced additional preprocessing steps prior to the model\u2019s pre-training. In the paper, they note that Arabic is known for its lexical sparsity, which means that words can have different forms and share the same meaning, which is mainly due to the complex concatenative system of the language. To handle this issue they utilize Farasa, \u201ca fast and furious segmenter for Arabic\u201d, to segment the words into stems, prefixes and suffixes. Using the version of AraBERT that includes the Fararsa segmenter, fine-tuning on the AJGT achieves a binary classification accuracy of 93.8%.", "In the code snippet below that lists the required imports, I import the Farasa segmenter to preprocess the Arabic text.", "Preprocessing and preparing a dataset for training takes only a few lines to create a segmenter, apply the preprocessing and split the dataset for training.", "The next step is to set up the components needed for training a model with Hugging Face transformers Trainer. Firstly, the config, tokenizer and model need to be initialized. Then the Hugging Face data processor for single sentence classification is applied to the train and test datasets. Lastly, features are created using the tokenizer with the maximum sequence length set to 128.", "In order to access all the customization options during a training, Hugging Face recommends creating TrainingArguments before instantiating the Trainer. I used the training arguments parameters as defined by the creators of AraBERT.", "The last piece before instantiating is to create a custom function to compute metrics using the Python library, SciKit-Learn, which was imported earlier with the necessary sub-modules.", "All that is left is to instantiate the trainer and start training, and this is accomplished simply with the following two lines of code.", "The following table taken from the AraBERT paper shows the exceptional state-of-the-art performance of their model on various Arabic NLP benchmark datasets, compared to the performance of a multi-lingual BERT (mBERT) and the previous state-of-the-art records.", "The two Arabic NLP tools discussed, AraVec and AraBERT, are excellent starting points for research on Arabic social media. In particular, there are many possible uses of AraBERT across a wide range of NLP tasks, limited only by the availability of a labeled dataset for fine-tuning. Applications could include creative use of text classification or named-entity recognition for, sentiment analysis, topic labeling or detection tools. Another option is to use multitask learning (MTL) to simultaneously learn multiple NLP tasks.", "For example, it would be interesting to fine-tune a model on the task of identifying medical misinformation on social media, especially since the COVID-19 pandemic continues to have devastating effects across the MENA region. By the same measure, a general rumour detection classifier could be utilized to investigate the development of rumours, providing opportunity to combat harmful fake news. Tracking tools are another possibility; it is possible to filter social media text to track trends relating to political unrest, protests, or grievances. Another potential use would be to train a classifier to detect human rights violations by tracking Twitter in real time and flagging possible abuse for investigation.", "Named-entity recognition (NER) is a NLP subtask that is used to locate and classify named entities mentioned in unstructured text by extracting information that fits into predefined categories such as names, organizations, locations, monetary values, or other entities. Currently, there are no detailed studies applying NER to Arabic text for research purposes. However, it would be a worthwhile endeavour to apply NER to Arabic media such as blog posts or news articles to surface content and gain insight into trends by identifying the subjects and themes in the available text.", "AraBERT has proven to be adept at NER, it scores higher on the Arabic NER corpus (ANERcorp) than the previous state-of-the-art record as measured by macro-F1 scores. The current updated version of AraBERT as of September, 2020 has significantly improved AraBERT\u2019s macro-F1 score on the ANERcorp dataset by an additional 4.5% compared to the initial score published. This was achieved by further preprocessing the ANERcorp training dataset, a technique used to improve accuracy that can be applied to other tasks.", "As a learning paradigm, MTL relies on inductive knowledge transfer under the assumption that commonalities exist between learned tasks. Inspired by the human ability to transfer important learned information between related tasks, models developed with MTL have the advantage of requiring less labeled data. More importantly, they are able to leverage shared information from related tasks. This means that it is possible to improve the classification accuracy of two separate but related tasks such as sentiment analysis and rumour detection by coupling them. In the past three years, due to gains made in the area of deep learning, MTL has rapidly gained popularity; however, there are very few studies applying this method to specific research questions.", "Currently, my main interest with regards to Arabic NLP, is in applied data science; how to take these remarkable tools and apply them to actual research concerns. To that end, I will continue to develop the tools I am currently using by staying up to date on the current literature while searching for creative applications of this technology. In the next part of this three-part series, I will detail my experiments integrating big data tools such as Apache Spark into my Arabic NLP workflow. In the final tutorial, I will present a detailed walkthrough of my own method for implementing MTL in Arabic NLP.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Research Consultant and Data Scientist. Enthusiastic about machine learning, social justice, video games and philosophy."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fc6982b2f602b&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-advancements-in-arabic-nlp-c6982b2f602b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-advancements-in-arabic-nlp-c6982b2f602b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-advancements-in-arabic-nlp-c6982b2f602b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-advancements-in-arabic-nlp-c6982b2f602b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----c6982b2f602b--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c6982b2f602b--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://haaya-naushan.medium.com/?source=post_page-----c6982b2f602b--------------------------------", "anchor_text": ""}, {"url": "https://haaya-naushan.medium.com/?source=post_page-----c6982b2f602b--------------------------------", "anchor_text": "Haaya Naushan"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F68f801f1b50b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-advancements-in-arabic-nlp-c6982b2f602b&user=Haaya+Naushan&userId=68f801f1b50b&source=post_page-68f801f1b50b----c6982b2f602b---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc6982b2f602b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-advancements-in-arabic-nlp-c6982b2f602b&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc6982b2f602b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-advancements-in-arabic-nlp-c6982b2f602b&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.sciencedirect.com/science/article/pii/S1319157818310553#b0155", "anchor_text": "Arabic is the 4th most used language on the Internet"}, {"url": "https://www.statista.com/topics/5550/internet-usage-in-mena/", "anchor_text": "Statistics from 2018"}, {"url": "https://www.researchgate.net/figure/MENA-Region-Countries_fig1_316156532/actions#reference", "anchor_text": "Image sourced from researchgate.net"}, {"url": "https://dl.acm.org/doi/10.1145/3314941", "anchor_text": "In a paper from May, 2019"}, {"url": "https://github.com/bakrianoo/aravec", "anchor_text": "AraVec"}, {"url": "https://github.com/aub-mind/arabert", "anchor_text": "AraBERT"}, {"url": "https://towardsdatascience.com/sentiment-analysis-of-social-media-with-python-45268dc8f23f", "anchor_text": "Sentiment Analysis of Social Media with PythonBeginner-friendly overview of Python tools available for classifying sentiment in social media text. I discuss my\u2026towardsdatascience.com"}, {"url": "http://jalammar.github.io/illustrated-word2vec/", "anchor_text": "this illustrated guide to word embeddings by Jay Alammar"}, {"url": "https://www.sciencedirect.com/science/article/pii/S1877050917321749", "anchor_text": "Soliman et al. 2017."}, {"url": "https://radimrehurek.com/gensim/index.html", "anchor_text": "Gensim developed by Radim Rehurek"}, {"url": "https://www.sciencedirect.com/science/article/pii/S1877050917321749", "anchor_text": "AraVec: A set of Arabic Word Embedding Models for use in Arabic NLP"}, {"url": "https://medium.com/@raghavaggarwal0089/bi-lstm-bc3d68da8bd0", "anchor_text": "(bi-LSTM"}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "Vaswani et al."}, {"url": "https://nlp.seas.harvard.edu/2018/04/03/attention.html", "anchor_text": "The Annotated Transformer by the Harvard NLP"}, {"url": "https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21", "anchor_text": "LSTM or GRU"}, {"url": "https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html", "anchor_text": "Lilian Weng"}, {"url": "http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf", "anchor_text": "Vaswani, et al. 2017"}, {"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "BERT in 2019 by researchers from the Google AI Language team"}, {"url": "https://en.wikipedia.org/wiki/Cloze_test", "anchor_text": "Cloze test"}, {"url": "https://arxiv.org/abs/2003.00104v3", "anchor_text": "Antoun at al. 2020."}, {"url": "https://arxiv.org/abs/1611.04033", "anchor_text": "1.5 billion words Arabic Corpus"}, {"url": "https://www.aclweb.org/anthology/W19-4619/", "anchor_text": "OSIAN: the Open Source International Arabic News Corpus"}, {"url": "https://github.com/komari6/Arabic-twitter-corpus-AJGT", "anchor_text": "Arabic Jordanian General Tweets (AJGT) Corpus"}, {"url": "https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments", "anchor_text": "TrainingArguments"}, {"url": "https://arxiv.org/abs/2003.00104v3", "anchor_text": "AraBERT: Transformer-based Model for Arabic Language Understanding"}, {"url": "https://link.springer.com/chapter/10.1007/978-3-540-70939-8_13", "anchor_text": "Arabic NER corpus (ANERcorp)"}, {"url": "https://link.springer.com/article/10.1023/A:1007379606734", "anchor_text": "inductive knowledge transfer"}, {"url": "https://arxiv.org/abs/1706.05098", "anchor_text": "MTL has rapidly gained popularity"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----c6982b2f602b---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/nlp?source=post_page-----c6982b2f602b---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/arabic?source=post_page-----c6982b2f602b---------------arabic-----------------", "anchor_text": "Arabic"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc6982b2f602b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-advancements-in-arabic-nlp-c6982b2f602b&user=Haaya+Naushan&userId=68f801f1b50b&source=-----c6982b2f602b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc6982b2f602b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-advancements-in-arabic-nlp-c6982b2f602b&user=Haaya+Naushan&userId=68f801f1b50b&source=-----c6982b2f602b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc6982b2f602b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-advancements-in-arabic-nlp-c6982b2f602b&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c6982b2f602b--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fc6982b2f602b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-advancements-in-arabic-nlp-c6982b2f602b&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----c6982b2f602b---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----c6982b2f602b--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----c6982b2f602b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----c6982b2f602b--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----c6982b2f602b--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----c6982b2f602b--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----c6982b2f602b--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----c6982b2f602b--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----c6982b2f602b--------------------------------", "anchor_text": ""}, {"url": "https://haaya-naushan.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://haaya-naushan.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Haaya Naushan"}, {"url": "https://haaya-naushan.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "913 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F68f801f1b50b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-advancements-in-arabic-nlp-c6982b2f602b&user=Haaya+Naushan&userId=68f801f1b50b&source=post_page-68f801f1b50b--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F4e411f1cc489&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-advancements-in-arabic-nlp-c6982b2f602b&newsletterV3=68f801f1b50b&newsletterV3Id=4e411f1cc489&user=Haaya+Naushan&userId=68f801f1b50b&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}