{"url": "https://towardsdatascience.com/capturing-context-in-emotion-ai-innovations-in-multimodal-video-sentiment-analysis-65e128ad8a1a", "time": 1682996549.564043, "path": "towardsdatascience.com/capturing-context-in-emotion-ai-innovations-in-multimodal-video-sentiment-analysis-65e128ad8a1a/", "webpage": {"metadata": {"title": "Capturing Context in Emotion AI: Innovations in Multimodal Video Sentiment Analysis | by Alexander Shropshire | Towards Data Science", "h1": "Capturing Context in Emotion AI: Innovations in Multimodal Video Sentiment Analysis", "description": "Blog based on: \u201cContext-Dependent Sentiment Analysis in User-Generated Videos\u201d. Poria; Cambria; Hazarika; Majumder; Zadeh; Morency (2017). Link to Full Technical Research Paper Imagine you\u2019re\u2026"}, "outgoing_paragraph_urls": [{"url": "https://aclweb.org/anthology/P17-1081", "anchor_text": "Link to Full Technical Research Paper", "paragraph_index": 0}], "all_paragraphs": ["Blog based on: \u201cContext-Dependent Sentiment Analysis in User-Generated Videos\u201d. Poria; Cambria; Hazarika; Majumder; Zadeh; Morency (2017). Link to Full Technical Research Paper", "Imagine you\u2019re watching a reaction video on YouTube about a new TV show premiere. For whatever reason, the opinion holder expresses, in different moments over the course of 5 minutes, excitement, then sadness\u2026they yell loudly, then they\u2019re monotone\u2026they become surprised, then disgusted. What allows us to easily interpret their emotional expressions? Is it just about the words they\u2019re choosing? Their facial features? Their vocal intensity? It\u2019s likely more than simply the audio, simply the image on the screen, or simply the meaning of the words they chose. It\u2019s about the combination of all three, and how each interacts in concert over time.", "Previous research in multimodal video sentiment analysis at the \u201cutterance\u201d-level ignores the natural interdependencies of sequential human utterances (instances of speech bound by pauses) in videos and instead treats each expression as completely independent of those just before and after it. Sequential utterances of a video are actually quite contextually correlated and, therefore, the sequence itself should affect the emotional classification of close neighbors in time. For example, in the utterance \u201cWhat would have been a better name for the movie\u201d, a speaker attempts to comment on the quality of the movie by giving an appropriate name. However, the sentiment expressed requires context about the speaker\u2019s demeanor and his/her opinion about the film. The previous state-of-the-art fails to classify utterances like this correctly.", "Researchers from the US, Singapore, India, and Mexico came together in 2017 to build a Long Short-Term Memory (LSTM) classification model that incorporates the surrounding contextual information captured in each video\u2019s auditory, visual, and textual data on a moment-to-moment basis, pushing the model\u2019s performance forward in accuracy by 5\u201310% versus the previous state-of-the-art model.", "The vocal inflections and facial expressions in the visual data, along with the language appearing in a textual transcript, provide important cues to better identify affective states of opinion holders, creating a more robust emotion recognition model. Though the primary focus of the experiment I will summarize below was to classify sentiment in videos (positive, neutral, or negative), their findings can be incorporated into the more specific task of emotion classification across a wider spectrum of basic emotional categories (anger, happiness, sadness, neutral, excitement, frustration, fear, surprise, and other). Example business applications include the development of virtual assistants, analysis of YouTube and other digital or social videos (ex. product reviews, advertising campaigns), analysis of news videos, individual emotion monitoring for mental health professionals, and surely many others.", "Three datasets with different attributes and caveats were used to test the yet-to-be-described model. MOSI contains 93 people reviewing topics in English and is rich with sentimental expressions. Each segment is labeled from -3 to +3 (negative to positive sentiment) by multiple judges. For the experiment, the average is taken, yielding a binary either above 0 (positive sentiment) or below 0 (negative). MOUD contains product review videos from 55 people in Spanish (translated with Google Translate), with utterances labeled as positive, negative, or neutral. After dropping neutral videos, 59 videos were used.", "The most exciting dataset used, in my opinion, is the IEMOCAP data. This set contains 10 English speakers in two-way conversations, with utterances labeled with an emotion: anger, happiness, sadness, neutral, excitement, frustration, fear, surprise, or other (eventually researchers took only the first four to compare versus the previous state-of-the-art research).", "Initially, the researchers do not consider the contextual dependency among the utterances in their first feature extraction.", "Text: Creating model inputs from the textual transcript of the video\u2019s spoken words using a CNN", "Using a convolutional neural network (CNN), the researchers first represent each utterance as the concatenation of vectors of the words used, where the vectors are the publicly available 300-dimensional word2vec vectors trained on 100 billion words from Google News. In simple terms, to extract the sentiment of each word, each utterance, and eventually, each video, the CNN converts a textual utterance to a logical form: a machine-understandable representation of its meaning.", "Audio: sound-descriptive feature extraction using openSMILE", "Audio features are extracted at 30 Hz frame-rate and a sliding window of 100 ms using openSMILE, open-source software that automatically extracts sound descriptors such as pitch and vocal intensity. Voice intensity is thresholded to identify samples with and without meaningful speech.", "Visual: image-based feature extraction using a 3D-CNN", "Under the hypothesis that 3D-CNN will not only be able to learn relevant features from each frame but will also learn the changes among a given number of consecutive frames, the team chose this to extract video features for each utterance. Using a 3D convolutional filter, max pooling, a dense layer, and softmax, the relevant features are extracted.", "Since utterances in a video maintain a sequence and given a hypothesis of inter-utterance dependency with respect to recognizing sentimental hints, the researchers used an LSTM-based recurrent neural network (RNN) scheme to classify utterances. How is this different compared to traditional feedforward neural networks? Well, a trained feedforward network can be exposed to any random collection of photographs, and the first photograph it is exposed to will not necessarily alter how it classifies the second. There is no notion of time as these networks only remember their training, but not their recent past. RNNs, however, take as their input, not just the current input they\u2019re fed, but also what they have perceived previously in time. Recent past matters and RNNs act closer to how we actually perceive the emotional context of what we see and hear.", "To oversimplify: In the animation below, each x (red) is an input example, w (blue) prepares inputs by determining how much importance to accord to both the present input and the past hidden state, a (green) is the activation of the hidden layer (a combination of the filtered input and a \u201cmemory\u201d-like input), and b (orange) is the output at each point in time, whose \u201cmemory\u201d-like information is passed forward to the RNN at time+1.", "As humans, we don\u2019t and shouldn\u2019t remember everything. Our recent past is more vivid, and in general, our distant past tends to affect our current decision-making slightly less. In order to be our best, we make conscious or unconscious decisions about what to store in memory, what to forget, what to learn, and what to call upon in the moment to make a decision. This metaphor may not be perfect, but LSTMs are similar in that they make such decisions via gates that open and close. These gated cells learn when to allow data to enter, leave, or be deleted through the iterative process of guessing, backpropagating error, and adjusting weights via gradient descent. LSTM enables a sort of selective memory. It allows recurrent nets to continue to learn over many more time steps, with the flexibility to forget when it makes sense to (ex. scanning text information on a brand new document, when the previous document should be considered completely unrelated). These features make it the model of choice to classify sentiment and emotions over many sequential words, many sequential utterances, and many unique videos, over time.", "With the hope of avoiding getting too technical, the input to the contextual-LSTM model is a matrix built from a vector for every utterance of every video for all 3 unimodal features. Each utterance vector is passed through an LSTM cell. The output of the LSTM cell is then introduced to dropout, a regularization method to reduce overfitting and improve generalization of the model by temporarily dropping randomly selected network nodes. The output is then passed into a dense layer to prepare the contextually related features for softmax activation. Finally, the output is passed into a softmax activation layer, which squashes a vector between 0 and 1 where all the resulting elements add up to 1, yielding class probabilities which will dictate classification.", "The training of the LSTM network is performed using categorical cross-entropy loss on each utterance\u2019s softmax output per video. Since the videos have different utterance counts, padding is used to serve as neutral utterances. To avoid too much noise within the network, bit masking is done on these padded utterances to eliminate their effect in the network. Hyperparameter tuning is done on the training set after an 80%/20% train/test split. RMSprop has been used as the optimizer which is known to resolve Adagrad\u2019s radically diminishing learning rates by using a moving average instead of letting the estimate of squared gradients continually accumulate.", "Unlike the unidirectional LSTM cells visualized in image above, the best performing classifier in the report ended up proving to be the bi-directional architecture. Bi-directional LSTMs are two unidirectional LSTMs stacked together having opposite directions. This means that an utterance can get information from utterances occurring before and after itself in the video, which seems closer to how we actually perceive the expressional context.", "The researchers proposed a hierarchical deep network consisting of two levels to approach fusing the three modalities \u2014 textual, visual, and auditory. In Level-1, context-independent features are fed to the proposed LSTM network outlined above to get context-sensitive unimodal feature representations for each utterance. Individual LSTM networks are used for each of the three modalities. In Level-2, the outputs from each LSTM network in Level-1 are concatenated and fed into this new LSTM network, which is independently trained and computed.", "The bi-directional LSTM network variant, and the many other variants tested by the researchers but not mentioned here, significantly outperform a baseline uni-SVM on all the datasets by the margin of 2% to 5%, proving the researchers\u2019 initial hypothesis that modeling the contextual dependencies among utterances (which uni-SVM cannot do) improves the classification. The need to consider context-dependency is important in utterance-level sentiment classification.", "As expected, bimodal and trimodal models outperform unimodal models in this case and in many other types of experiments. Overall, audio performed better than visual on all the datasets. On MOSI and IEMOCAP datasets, the textual classifier achieves the best performance. All classifiers had trouble classifying neutral utterances in the IEMOCAP dataset. The textual modality, combined with non-textual modes, boosts the performance in IEMOCAP by a large margin that is much larger in that case than in other datasets.", "On MOUD data, text performed worse than audio, likely due to the Spanish to English translation. Keeping it in the original language boosts performance by 10% (Spanish) but sacrifices comparability to the state-of-the-art (English). When assessing generalizability, the researchers found that in cross-lingual scenarios facial expressions carry more robust information than audio and textual modalities (by testing the MODI-fed model on the MOUD data). There weren\u2019t any other utterance-level multi-emotion datasets to test the generalizability of the IEMOCAP model on.", "Meaningful contextual relationships were found throughout the datasets. To illustrate the interdependency of modalities, the utterance \u201cwho doesn\u2019t have any presence or greatness at all\u201d was classified as positive by the audio classifier because \u201cpresence and greatness at all\u201d was spoken with enthusiasm. However, the textual modality caught the negation induced by \u201cdoesn\u2019t\u201d and pushed the classifier in the right direction. In a different example, the textual classifier classified the utterance \u201cthat like to see comic book characters treated responsibly\u201d as positive just from seeing \u201clike to see\u201d and \u201cresponsibly\u201d. The clear anger in the person\u2019s voice and a vivid frown helped to identify this as negative. There are many examples of the strength of one modality overriding the model\u2019s hunch gathered from a situationally weaker modality, and there are also examples of surrounding context incorrectly overriding a weak or neutral momentary sentiment.", "Overall, the model is not perfect, but it\u2019s definitely a step forward and adds to the compounding evidence that we\u2019re living in an extremely exciting time for emotion-focused artificial intelligence!", "Let\u2019s connect! I encourage you to like, comment, share, or message me directly with your thoughts on the ideas presented here, or suggestions on interesting topics I should look into going forward.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Finance x Data @ Brex | Past: Legendary Entertainment, VEVO, AOL | Let\u2019s Connect!"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F65e128ad8a1a&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcapturing-context-in-emotion-ai-innovations-in-multimodal-video-sentiment-analysis-65e128ad8a1a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcapturing-context-in-emotion-ai-innovations-in-multimodal-video-sentiment-analysis-65e128ad8a1a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcapturing-context-in-emotion-ai-innovations-in-multimodal-video-sentiment-analysis-65e128ad8a1a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcapturing-context-in-emotion-ai-innovations-in-multimodal-video-sentiment-analysis-65e128ad8a1a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----65e128ad8a1a--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----65e128ad8a1a--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@as6140?source=post_page-----65e128ad8a1a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@as6140?source=post_page-----65e128ad8a1a--------------------------------", "anchor_text": "Alexander Shropshire"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F993feda9902d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcapturing-context-in-emotion-ai-innovations-in-multimodal-video-sentiment-analysis-65e128ad8a1a&user=Alexander+Shropshire&userId=993feda9902d&source=post_page-993feda9902d----65e128ad8a1a---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F65e128ad8a1a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcapturing-context-in-emotion-ai-innovations-in-multimodal-video-sentiment-analysis-65e128ad8a1a&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F65e128ad8a1a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcapturing-context-in-emotion-ai-innovations-in-multimodal-video-sentiment-analysis-65e128ad8a1a&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://aclweb.org/anthology/P17-1081", "anchor_text": "Link to Full Technical Research Paper"}, {"url": "https://imgur.com/kpZBDfV", "anchor_text": "https://imgur.com/kpZBDfV"}, {"url": "https://skymind.ai/wiki/lstm#long", "anchor_text": "https://skymind.ai/wiki/lstm#long"}, {"url": "https://aclweb.org/anthology/P17-1081", "anchor_text": "https://aclweb.org/anthology/P17-1081"}, {"url": "https://aclweb.org/anthology/P17-1081", "anchor_text": "https://aclweb.org/anthology/P17-1081"}, {"url": "https://aclweb.org/anthology/P17-1081", "anchor_text": "https://aclweb.org/anthology/P17-1081"}, {"url": "https://aclweb.org/anthology/P17-1081", "anchor_text": "Full Technical Paper"}, {"url": "http://medium.com/@as6140", "anchor_text": "medium.com/@as6140"}, {"url": "http://github.com/as6140", "anchor_text": "github.com/as6140"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----65e128ad8a1a---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----65e128ad8a1a---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----65e128ad8a1a---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----65e128ad8a1a---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/emotional-intelligence?source=post_page-----65e128ad8a1a---------------emotional_intelligence-----------------", "anchor_text": "Emotional Intelligence"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F65e128ad8a1a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcapturing-context-in-emotion-ai-innovations-in-multimodal-video-sentiment-analysis-65e128ad8a1a&user=Alexander+Shropshire&userId=993feda9902d&source=-----65e128ad8a1a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F65e128ad8a1a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcapturing-context-in-emotion-ai-innovations-in-multimodal-video-sentiment-analysis-65e128ad8a1a&user=Alexander+Shropshire&userId=993feda9902d&source=-----65e128ad8a1a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F65e128ad8a1a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcapturing-context-in-emotion-ai-innovations-in-multimodal-video-sentiment-analysis-65e128ad8a1a&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----65e128ad8a1a--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F65e128ad8a1a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcapturing-context-in-emotion-ai-innovations-in-multimodal-video-sentiment-analysis-65e128ad8a1a&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----65e128ad8a1a---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----65e128ad8a1a--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----65e128ad8a1a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----65e128ad8a1a--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----65e128ad8a1a--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----65e128ad8a1a--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----65e128ad8a1a--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----65e128ad8a1a--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----65e128ad8a1a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@as6140?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@as6140?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Alexander Shropshire"}, {"url": "https://medium.com/@as6140/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "188 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F993feda9902d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcapturing-context-in-emotion-ai-innovations-in-multimodal-video-sentiment-analysis-65e128ad8a1a&user=Alexander+Shropshire&userId=993feda9902d&source=post_page-993feda9902d--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F6afb749b7ed1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcapturing-context-in-emotion-ai-innovations-in-multimodal-video-sentiment-analysis-65e128ad8a1a&newsletterV3=993feda9902d&newsletterV3Id=6afb749b7ed1&user=Alexander+Shropshire&userId=993feda9902d&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}