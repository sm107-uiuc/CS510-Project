{"url": "https://towardsdatascience.com/kaggle-tensorflow-speech-recognition-challenge-b46a3bca2501", "time": 1682993276.71012, "path": "towardsdatascience.com/kaggle-tensorflow-speech-recognition-challenge-b46a3bca2501/", "webpage": {"metadata": {"title": "Kaggle Tensorflow Speech Recognition Challenge | by Chris Dinant | Towards Data Science", "h1": "Kaggle Tensorflow Speech Recognition Challenge", "description": "From November 2017 to January 2018 the Google Brain team hosted a speech recognition challenge on Kaggle. The goal of this challenge was to write a program that can correctly identify one of 10 words\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.kaggle.com/c/tensorflow-speech-recognition-challenge#", "anchor_text": "Kaggle", "paragraph_index": 0}, {"url": "https://www.kaggle.com/c/tensorflow-speech-recognition-challenge/discussion/44283#256644", "anchor_text": "Kaggle discussion post", "paragraph_index": 8}, {"url": "https://towardsdatascience.com/an-overview-of-resnet-and-its-variants-5281e2f56035", "anchor_text": "ResNet", "paragraph_index": 11}, {"url": "https://www.coursera.org/learn/nlp-sequence-models/home/welcome", "anchor_text": "deep learning with python course", "paragraph_index": 17}, {"url": "https://github.com/mlrobsmt/KerasDeepSpeech", "anchor_text": "repo on github", "paragraph_index": 18}, {"url": "https://arxiv.org/pdf/1511.06233.pdf", "anchor_text": "open set recognition", "paragraph_index": 21}, {"url": "https://github.com/chrisdinant/speech", "anchor_text": "https://github.com/chrisdinant/speech", "paragraph_index": 23}, {"url": "http://chrisdinant.github.io", "anchor_text": "chrisdinant.github.io", "paragraph_index": 25}], "all_paragraphs": ["From November 2017 to January 2018 the Google Brain team hosted a speech recognition challenge on Kaggle. The goal of this challenge was to write a program that can correctly identify one of 10 words being spoken in a one-second long audio file. Having just made up my mind to start seriously studying data science with the goal of turning a new corner in my career, I decided to tackle this as my first serious kaggle challenge.", "In this post I will talk about ResNets, RNNs, 1D and 2D convolution, Connectionist Temporal Classification and more. Let\u2019s go!", "The training data supplied by Google Brain consists of ca. 60,000 1-second-long .wav files in 32 directories that are named by the word spoken in the files. Only 10 of these are classes you need to identify, the others should go in the \u2018unknown\u2019 or \u2018silence\u2019 classes. There are a couple of things you can do to get a grip on the data you\u2019re working with. This data set is not completely cleaned up for you. For example, some files are not exactly 1 second long. And there are no \u2018silence\u2019 files as such. What you get is a few longer recordings with \u2018background\u2019 noises that you can split up into 1 second fragments yourself. You can also mix background noises with word files to get some different \u2018environments\u2019 for your sounds to live in during training.", "One important thing you need to do to clean up the data was mentioned in a Discussion comment on Kaggle: There are quite a few files with extremely low sound volumes. Some of these are corrupt and only contain noise and some are basically background noise without any spoken word. To remove or correctly label these files it helps to sort all files on dynamic range of the output volume and then check if there\u2019s a threshold minimum sound level below which all files are basically silence. It turns out that output volume by itself is not sufficient to separate corrupt/silence from good files, so I ended up doing some manual cleaning by listening to suspect files and looking at a lot of spectrograms (see below).", "When you are classifying audio you can either use the raw wav data itself or you can transform the audio into spectrograms. A spectrogram is a visual representation of sound with a time and a frequency axis and pixel intensities representing the amplitude or energy of the sound at that moment and at that frequency. There\u2019s bunch of parameters to play around with when making spectrograms that will affect how much information can be extracted from the frequency or the time domains. I haven\u2019t done an exhaustive analysis of all these parameters with regards to suitability for deep learning because it would take forever. Instead I plotted a bunch of spectrograms with different dimensions and intensity ranges etc for different words and picked what looked easiest to classify as a different word visually.", "Some different visualizations of the audio source of someone saying \u2018yes\u2019. I chose the spectrogram in the right bottom as input for most of my networks. The advantage of using spectrograms over raw wav data is that you can approach it as an image classification problem, which most of us are already very familiar with. I attempted both approaches.", "To speed up eventual training of the network I decided to do most of the preprocessing separately and save individual train and validation sets as numpy arrays in .npy files. The amount of data is small enough that this can be done on my home pc. Google Brain suggests that you split the data in \u2018train\u2019 \u2018validation\u2019 and \u2018test\u2019 sets based on file names in a \u2018validation.txt\u2019 and \u2018test.txt\u2019 file that they supply. Since they also supply a test set on Kaggle that is used for the leaderboard scoring I decided to combine the \u2018validation\u2019 and \u2018test\u2019 text files into one validation set. Preprocessing included creating spectrograms, normalizing around zero, creating \u2018label\u2019 or \u2018Y\u2019 arrays with integers 0\u201311 for the ten main classes plus \u2018silence\u2019 and \u2018unknown\u2019. For the CTC model I did not use the \u2018unknown\u2019 label. All 32 classes were treated as equal.", "Because I approached this challenge as a deep learning training exercise I implemented a bunch of different network designs. Getting practical training like this was fantastic. I learned more from this one project than I did in the ten MOOCs I started before. (Ok, I exaggerate slightly). I will discuss a few of my design attempts and the things to pay attention to when implementing them.", "With spectrograms you use a specific algorithm to extract features from wav files, but you have to fine tune a bunch of parameters. A well designed neural network should be able to learn features from the raw wav files by itself and potentially get more informative features than one could get from a spectrogram. This first model is an attempt to do that. The initial design was heavily inspired by a Kaggle discussion post by user ttagu99. This is the result of an early attempt with a 6-layer 1D convolutional network.", "The network scores on the validation set. Top: sklearn classifcation report. Bottom: sklearn confusion matrix.", "It is not very effective yet. Some of the mistakes it makes are easy to understand. It often confuses \u2018no\u2019 with \u2018go\u2019 and \u2018off\u2019 with \u2018up\u2019. Theses words have very similar vowel sounds, so confusion can be expected. Some mistakes are less obvious: does it really think \u2018right\u2019 is \u2018left\u2019 16 times? Maybe it\u2019s the \u2018t\u2019 at the end? This image also illustrates how unbalanced the classes are if you combine all the \u2018extra\u2019 classes into one \u2018unknown\u2019 class (~500 vs ~8300 samples). I dealt with this by using the \u2018class_weight\u2019 argument in the Keras fit function. This parameter accepts a dictionary mapping classes to a weight float and \u2018rebalances\u2019 the training set by punishing misclassification of an underrepresented class more heavily. Another way of balancing the training set better is by creating batches with equal amounts of samples from each class. I assume one would use the leftover \u2018unknown\u2019 samples in subsequent epochs, but I haven\u2019t tried this myself. I don\u2019t know which balancing trick is best. After some tweaking of the Conv1D model I got to a score on the Kaggle leaderboard of around 83%. Not very good yet, but instead of tweaking more on this model now I wanted to try some other network architectures.", "A residual neural network, or ResNet, is basically a deep convolutional NN with shortcuts. Every couple of layers there is an identity connection back to a layer a few levels up. These shortcut connections are thought to help the network generalize better. And this definitely works: I don\u2019t need any dropout layers anymore. What happens is that the network depth defaults to using as many shortcut connections as possible. This keeps the network small and therefore helps generalization. The residual layers in between the shortcuts are updated only when needed.", "Small subset of ResNet model with two shortcut connections", "Blocks of 2 Conv2D layers with BatchNormalization and ReLu activation are separated by a connection layer adding the previous connection layer to the output of the blocks. Every three of such blocks is then further separated by a Conv2D layer with stride 2 in order to learn larger scale features. In Keras, this is how I implemented this:", "This model performed really well. It reached almost 98% accuracy on the validation set and 85% on the kaggle leaderboard. This discrepancy between validation and test sets is concerning, and something I will get to in more depth below, but it has to do with the test set containing a lot of words that are not given to us in the training set. So called unknown unknowns. The next architecture I looked into to improve the model was Recurrent Neural Networks.", "One axis of a spectrogram represents the frequency and the other axis the time dimension of the sound file. There is something to be said for not treating these two dimensions as equal in a convolutional neural network. Recurrent Neural Networks or RNNs are used for modeling sequences, for example to predict what should come next. In our case they can be used to keep track of what happened first, and what came after, something that a ResNet wouldn\u2019t do. What I mean is that a normal convolutional network might think that \u2018yse\u2019 and \u2018yes\u2019 are the same word, because it has no notion of the sequence of the sound. What I tried to do was to add an RNN layer to the end of my pretty well-performing ResNet and play around with that. As you can see in the code above there is one MaxPooling layer and two Conv2D layers with stride 2 which reduce the size of the input from (61,75,1) to (8,10,128) at the end of the network. The first dimension here (8) represents the time dimension, but RNNs take 3D inputs and (including the batch) our ResNet takes 4D. So to get from the final residual+shortcut layer into an RNN layer we have to use a Reshape layer in Keras. This is how you can do this:", "We combine the 2nd and 3rd dimensions into one large vector resulting in 8 timepoints with 1280 numbers describing the convolved frequency distributions. A TimeDistributed wrapper layer is required to get the RNN layer (LSTM in this case) to use the 8 time rows as sequential inputs. After a couple of tweaks and iterations a combined ResNet RNN model gave an 87% accuracy on the Kaggle leaderboard. My best try and good for circa 200th place out of 1300 or so. I thought this was pretty ok for my first Kaggle project. But I still felt like trying some stuff and learning more deep learning tricks.", "Around the time of the submission deadline for the Kaggle challenge the final module of Andrew Ng\u2019s Coursera deep learning with python course about sequence models was opened to the public. I had applied some RNN layers in the combined model above, but I did not really know how it worked, so I took the course to learn all about RNNs. This post is not a review for this course, but suffice it to say I would recommend it to everyone interested in the math internals of RNNs. Besides the basics and not-so-basics of RNNs, I was particularly intrigued with a speech-to-text method that Andrew mentions but doesn\u2019t really go deeper into called Connectionist Temporal Classification, or CTC. I was really excited by this because I have had an idea for a project in the back of my mind involving speech-to-text for a while, but it was still very abstract, I didn\u2019t know at all where to start. But now someone was actually giving me a very concrete place to start. A speech-to-text model takes a spectrogram (or raw wav data) as input and outputs letters or words. What such a network needs to do is identify so-called phonemes in each RNN input, translate them into letters and combine letters into correct words.", "So for spoken English words, you want to network to output one-hot vectors with length 28 (the alphabet plus \u2018space\u2019 and \u2018blank\u2019) for each timepoint and then somehow determine how \u2018wrong\u2019 the prediction is so we can do backprop. This is what CTC does. It takes the output , \u201d_ _ _ Y _ EEEE _ SSS _ _\u201d, for example and reduces it to \u2018yes\u2019 by collapsing connecting multiples of letters and ignoring \u2018blanks\u2019 between different letters. If the output gives a \u2018blank\u2019 between two identical letters (\u201cE _ E\u201d) it considers it as two separate letters. A \u2018blank\u2019 should not be confused with a \u2018space\u2019, but I didn\u2019t have to worry about this because all my inputs were single words. With Keras you can use the backend function ctc_batch_cost() for the implementation, but it requires four parameters (y_true, y_pred, input_length and label_length) instead of only y_true and y_pred, which means you can\u2019t use it as a conventional loss function and plug it into your compile statement. I found a repo on github with a working Keras implementation of Baidu\u2019s DeepSpeech model, which uses CTC, and took the parts I needed for mine.", "Here y_pred is the TimeDistributed output of the RNN with length 28 (output_dim) one-hot vectors. Together with the other three parameters it is fed into the ctc_batch_cost function which is wrapped in a Lambda layer and then I create one model for training (\u201cmodel\u201d) and one for testing/validating (\u201ctest_model\u201d) that doesn\u2019t include the Lambda layer. When you run model.Compile() it needs a loss function, so you give it a dummy function that takes y_true and y_pred as parameters and simply returns y_pred. Then, training the model you give the .fit() function a list of four inputs:", "The outputs parameter of the .fit() function takes an array of zeros with length len(Y_train), our dummy loss function does nothing with it anyway. I think it\u2019s very exciting that I got this to work. Using the raw output of this model (the upstream layers are a Conv1D and two Bidirectional LSTMs) reaches a score of 83% on the Kaggle LB, but this is without trying to run some kind of spellcheck or language model on the predicted words. I saw a lot of \u201cye\u201d\u2019s and \u201criget\u201ds etc. I will play around with it some more and see what more I can get out of it.", "As I shortly mentioned above, the test set contains a lot of words, and also voices, that are not in the training set. This means that to get a really good score your model has to learn features of unknown unknowns. From what I have read on the Discussion forum at Kaggle after the deadline most of the top scorers used a lot of feature engineering to create more \u201cunknown\u201d samples. You can for example cut and paste pieces of words together to create new words, or you can use pitch shifting or reversing the samples. I tried a bit of this but was not very successful. Another thing I saw many top scorers do is unsupervised training on the test set to learn what kind of features are present there that are not in the train set. This seemed to give a large improvement in scores, but it feels a bit like cheating to me. I myself took quite a lot of time trying different forms of outlier or anomaly detection. This means trying to get the network to reject sounds that it has not heard before and put them in the \u2018unknown\u2019 class. I tried training autoencoders and variational autoencoders on the 10 main classes, but these models always remained equally good at decoding unseen spectrograms as they were at decoding the inputs I trained with. I tried open set recognition but I gave up after trying for a few weeks to understand exactly how they were doing this. In the end it would have been nice if some of these techniques improved my models, but if not a super score on the leaderboard studying this did give me a lot of new knowledge and experience so I\u2019m glad I did.", "It\u2019s been 15 years since I left University. It was great fun to learn so much in so little time again. Besides getting a better idea about what it takes to do speech recognition, I also learned a bit more about doing Kaggle challenges and what it takes to score high. When I started I thought that if I find some trick that the other participants didn\u2019t then I would score better than them, but now I realize much better what it really takes to score high. Those guys definitely know a lot more tricks than I do!", "Implementation of the ResNet and CTC models at https://github.com/chrisdinant/speech", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Scientist | Microscopist | Deep Learning fan | chrisdinant.github.io"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fb46a3bca2501&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fkaggle-tensorflow-speech-recognition-challenge-b46a3bca2501&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fkaggle-tensorflow-speech-recognition-challenge-b46a3bca2501&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fkaggle-tensorflow-speech-recognition-challenge-b46a3bca2501&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fkaggle-tensorflow-speech-recognition-challenge-b46a3bca2501&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----b46a3bca2501--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b46a3bca2501--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@chris.dinant?source=post_page-----b46a3bca2501--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@chris.dinant?source=post_page-----b46a3bca2501--------------------------------", "anchor_text": "Chris Dinant"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1155630a63a8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fkaggle-tensorflow-speech-recognition-challenge-b46a3bca2501&user=Chris+Dinant&userId=1155630a63a8&source=post_page-1155630a63a8----b46a3bca2501---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb46a3bca2501&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fkaggle-tensorflow-speech-recognition-challenge-b46a3bca2501&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb46a3bca2501&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fkaggle-tensorflow-speech-recognition-challenge-b46a3bca2501&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.kaggle.com/c/tensorflow-speech-recognition-challenge#", "anchor_text": "Kaggle"}, {"url": "https://www.kaggle.com/c/tensorflow-speech-recognition-challenge/discussion/44283#256644", "anchor_text": "Kaggle discussion post"}, {"url": "https://towardsdatascience.com/an-overview-of-resnet-and-its-variants-5281e2f56035", "anchor_text": "ResNet"}, {"url": "https://www.coursera.org/learn/nlp-sequence-models/home/welcome", "anchor_text": "deep learning with python course"}, {"url": "https://github.com/mlrobsmt/KerasDeepSpeech", "anchor_text": "repo on github"}, {"url": "https://arxiv.org/pdf/1511.06233.pdf", "anchor_text": "open set recognition"}, {"url": "https://dinantdatascientist.blogspot.dk/2018/02/kaggle-tensorflow-speech-recognition.html", "anchor_text": "dinantdatascientist.blogspot.com"}, {"url": "https://github.com/chrisdinant/speech", "anchor_text": "https://github.com/chrisdinant/speech"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----b46a3bca2501---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----b46a3bca2501---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/speech-recognition?source=post_page-----b46a3bca2501---------------speech_recognition-----------------", "anchor_text": "Speech Recognition"}, {"url": "https://medium.com/tag/kaggle?source=post_page-----b46a3bca2501---------------kaggle-----------------", "anchor_text": "Kaggle"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----b46a3bca2501---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb46a3bca2501&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fkaggle-tensorflow-speech-recognition-challenge-b46a3bca2501&user=Chris+Dinant&userId=1155630a63a8&source=-----b46a3bca2501---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb46a3bca2501&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fkaggle-tensorflow-speech-recognition-challenge-b46a3bca2501&user=Chris+Dinant&userId=1155630a63a8&source=-----b46a3bca2501---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb46a3bca2501&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fkaggle-tensorflow-speech-recognition-challenge-b46a3bca2501&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b46a3bca2501--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fb46a3bca2501&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fkaggle-tensorflow-speech-recognition-challenge-b46a3bca2501&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----b46a3bca2501---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----b46a3bca2501--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----b46a3bca2501--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----b46a3bca2501--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----b46a3bca2501--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----b46a3bca2501--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----b46a3bca2501--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----b46a3bca2501--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----b46a3bca2501--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@chris.dinant?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@chris.dinant?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Chris Dinant"}, {"url": "https://medium.com/@chris.dinant/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "593 Followers"}, {"url": "http://chrisdinant.github.io", "anchor_text": "chrisdinant.github.io"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1155630a63a8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fkaggle-tensorflow-speech-recognition-challenge-b46a3bca2501&user=Chris+Dinant&userId=1155630a63a8&source=post_page-1155630a63a8--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Feb9cf58136ee&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fkaggle-tensorflow-speech-recognition-challenge-b46a3bca2501&newsletterV3=1155630a63a8&newsletterV3Id=eb9cf58136ee&user=Chris+Dinant&userId=1155630a63a8&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}