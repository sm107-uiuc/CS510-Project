{"url": "https://towardsdatascience.com/bert-nlp-how-to-build-a-question-answering-bot-98b1d1594d7b", "time": 1683009247.193397, "path": "towardsdatascience.com/bert-nlp-how-to-build-a-question-answering-bot-98b1d1594d7b/", "webpage": {"metadata": {"title": "BERT NLP \u2014 How To Build a Question Answering Bot | by Michel Kana, Ph.D | Towards Data Science", "h1": "BERT NLP \u2014 How To Build a Question Answering Bot", "description": "This article will present key ideas about creating and coding a question answering system based on a neural network. The implementation uses Google\u2019s language model known as pre-trained BERT\u2026"}, "outgoing_paragraph_urls": [{"url": "https://rajpurkar.github.io/SQuAD-explorer/", "anchor_text": "Stanford Question Answering Dataset (SQuAD)", "paragraph_index": 5}, {"url": "https://towardsdatascience.com/lost-in-translation-found-by-transformer-46a16bf6418f", "anchor_text": "Transformer", "paragraph_index": 11}, {"url": "https://arxiv.org/pdf/1506.06724.pdf", "anchor_text": "Book Corpus", "paragraph_index": 11}, {"url": "https://rajpurkar.github.io/SQuAD-explorer/", "anchor_text": "SQuAD 1.0 leaderboard", "paragraph_index": 15}, {"url": "https://drive.google.com/file/d/1Zp2_Uka8oGDYsSe5ELk-xz6wIX8OIkB7/view?usp=sharing", "anchor_text": "PyTorch code for NLU with BERT", "paragraph_index": 18}, {"url": "https://drive.google.com/file/d/1Zp2_Uka8oGDYsSe5ELk-xz6wIX8OIkB7/view?usp=sharing", "anchor_text": "Google Colab Notebook", "paragraph_index": 19}, {"url": "https://www.linkedin.com/in/prachur-bhargava-83478a4/", "anchor_text": "Prachur Bhargava", "paragraph_index": 19}, {"url": "https://www.linkedin.com/in/himanshu-mohan-869561137/", "anchor_text": "Himanshu Mohan", "paragraph_index": 19}], "all_paragraphs": ["This article will present key ideas about creating and coding a question answering system based on a neural network. The implementation uses Google\u2019s language model known as pre-trained BERT. Hands-on proven PyTorch code for question answering with BERT fine-tuned and SQuAD is provided at the end of the article.", "In Question Answering tasks, the model receives a question regarding text content and is required to mark the beginning and end of the answer in the text.", "In the example above, the answer to the question \u201cWhere else besides the SCN cells are independent circadian rhythms also found?\u201d is found at the position highlighted with red color. The question \u201cWhat is the term for the independent clocks?\u201d is answered at the blue position.", "If we have a very large set of such texts together with sample questions and the position of the answers in the text, we can train a neural network to learn relationships between context, questions, and answers. The resulting network would be able to answer unseen questions given new contexts which are similar to the training texts.", "Machine reading comprehension has captured the minds of computer scientists for decades. The recent production of large-scale labeled datasets has allowed researchers to build supervised neural systems that automatically answer questions posed in a natural language.", "The Stanford Question Answering Dataset (SQuAD) is a prime example of large-scale labeled datasets for reading comprehension. Rajpurkar et al. developed SQuAD 2.0, which combines 100,000 answerable questions with 50,000 unanswerable questions about the same paragraph from a set of Wikipedia articles. The unanswerable questions were written adversarially by crowd workers to look similar to answerable ones.", "doc_tokens describes the context, i.e. the text which we want our model to understand.", "question_text describes the question which should be answered from the context.", "orig_answer_text represents the correct answer to the question.", "The answer is always a portion from the context starting at start_position and ending at end_position. If the question does not have any answer in the context, is_impossible has the value true.", "The SQuAD dataset offers 150,000 questions, which is not that much in the deep learning world. The idea behind transfer learning is to take a model that was trained on a very large dataset, then fine-tune that model using the SQuAD dataset.", "BERT is a trained Transformer Encoder stack, with twelve in the Base version, and twenty-four in the Large version. BERT was trained on Wikipedia and Book Corpus, a dataset containing +10,000 books of different genres. I cover the Transformer architecture in detail in my article below.", "We can use BERT to extract high-quality language features from the SQuAD text just by adding a single linear layer on top. The linear layer has two outputs, the first for predicting the probability that the current subtoken is the start of the answer and the second output for the end position of the answer.", "Below you can find a summary of the model. I only display the first and last encoder layers for simplicity\u2019s sake. The same layer would normally be repeated 12 times. We can see the BertEmbeddings layer at the beginning, followed by a Transformer architecture for each encoder layer: BertAttention, BertIntermediate, BertOutput. In the end, we have the BertPooler and Linear layer for two outputs.", "Training the model is relatively straightforward. The bottom layers have already great English word representation, and we only really need to train the top layer, with a bit of tweaking going on in the lower levels to accommodate our task for question answering. You can customize the code I provided in my article below for this purpose.", "You might expect a F1-score of around 74%. If we check the current SQuAD 1.0 leaderboard, we\u2019ll see that this evaluation of the test dataset puts us in Top 100, which is acceptable given the limited resources available on free GPUs.", "The fine-tuned model can be used to run inference on text and questions of our choice.", "In this article, I explained how to fine-tune a pre-trained BERT model on the SQUaD dataset for solving question answering task on any text.", "You can adapt my PyTorch code for NLU with BERT to solve your question-answering task.", "Some readers might find the full code in this Google Colab Notebook more straight-forward. Credits go to Prachur Bhargava, Lead Data Scientist at Microsoft, and Himanshu Mohan, Software Engineer at American Express, who elegantly solved memory issues around training the model on Google Colab. Thank you, Prachur and Himanshu!", "You can learn more about language models in my articles below.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Husband & Dad. Mental health advocate. Top Medium Writer. 20 years in IT. AI Expert @Harvard. Empowering human-centered organizations with high-tech."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F98b1d1594d7b&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbert-nlp-how-to-build-a-question-answering-bot-98b1d1594d7b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbert-nlp-how-to-build-a-question-answering-bot-98b1d1594d7b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbert-nlp-how-to-build-a-question-answering-bot-98b1d1594d7b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbert-nlp-how-to-build-a-question-answering-bot-98b1d1594d7b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----98b1d1594d7b--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----98b1d1594d7b--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://michel-kana.medium.com/?source=post_page-----98b1d1594d7b--------------------------------", "anchor_text": ""}, {"url": "https://michel-kana.medium.com/?source=post_page-----98b1d1594d7b--------------------------------", "anchor_text": "Michel Kana, Ph.D"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcb0b01fe20d2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbert-nlp-how-to-build-a-question-answering-bot-98b1d1594d7b&user=Michel+Kana%2C+Ph.D&userId=cb0b01fe20d2&source=post_page-cb0b01fe20d2----98b1d1594d7b---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F98b1d1594d7b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbert-nlp-how-to-build-a-question-answering-bot-98b1d1594d7b&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F98b1d1594d7b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbert-nlp-how-to-build-a-question-answering-bot-98b1d1594d7b&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/photos/6ZvMJlNF4YU", "anchor_text": "Unsplash"}, {"url": "https://rajpurkar.github.io/SQuAD-explorer/", "anchor_text": "Stanford Question Answering Dataset (SQuAD)"}, {"url": "https://arxiv.org/pdf/1810.04805.pdf", "anchor_text": "Jacob Devlin et.al. @ Google AI Language"}, {"url": "https://towardsdatascience.com/lost-in-translation-found-by-transformer-46a16bf6418f", "anchor_text": "Transformer"}, {"url": "https://arxiv.org/pdf/1506.06724.pdf", "anchor_text": "Book Corpus"}, {"url": "https://towardsdatascience.com/lost-in-translation-found-by-transformer-46a16bf6418f", "anchor_text": "BERT explained. Lost in Translation. Found by Transformer.Building the next chatbot? BERT, GPT-2: tackle the mystery of Transformer model.towardsdatascience.com"}, {"url": "https://towardsdatascience.com/bert-for-dummies-step-by-step-tutorial-fb90890ffe03", "anchor_text": "BERT for dummies \u2014 Step by Step TutorialDIY Practical guide on Transformer. Hands-on proven PyTorch code for Intent Classification with BERT fine-tuned.towardsdatascience.com"}, {"url": "https://rajpurkar.github.io/SQuAD-explorer/", "anchor_text": "SQuAD 1.0 leaderboard"}, {"url": "https://drive.google.com/file/d/1Zp2_Uka8oGDYsSe5ELk-xz6wIX8OIkB7/view?usp=sharing", "anchor_text": "PyTorch code for NLU with BERT"}, {"url": "https://drive.google.com/file/d/1Zp2_Uka8oGDYsSe5ELk-xz6wIX8OIkB7/view?usp=sharing", "anchor_text": "Google Colab Notebook"}, {"url": "https://www.linkedin.com/in/prachur-bhargava-83478a4/", "anchor_text": "Prachur Bhargava"}, {"url": "https://www.linkedin.com/in/himanshu-mohan-869561137/", "anchor_text": "Himanshu Mohan"}, {"url": "https://towardsdatascience.com/representing-text-in-natural-language-processing-1eead30e57d8", "anchor_text": "Representing text in natural language processingUnderstanding the written words: gentle review of Word2vec, GloVe, TF-IDF, Bag-of-words, N-grams, 1-hot encoding\u2026towardsdatascience.com"}, {"url": "https://towardsdatascience.com/bert-for-dummies-step-by-step-tutorial-fb90890ffe03", "anchor_text": "BERT for dummies \u2014 Step by Step TutorialDIY Practical guide on Transformer. Hands-on proven PyTorch code for Intent Classification with BERT fine-tuned.towardsdatascience.com"}, {"url": "https://towardsdatascience.com/topic-modeling-with-latent-dirichlet-allocation-by-example-3b22cd10c835", "anchor_text": "Topic Modeling Tutorial with Latent Dirichlet Allocation (LDA)A practical guide with proven hands-on Python code. Finding what people are tweeting about.towardsdatascience.com"}, {"url": "https://towardsdatascience.com/sentiment-analysis-a-benchmark-903279cab44a", "anchor_text": "Sentiment Analysis: a benchmarkRecurrent neural networks explained. Classifying Customer Reviews using FCNNs, CNNs, RNNs and Embeddings.towardsdatascience.com"}, {"url": "https://towardsdatascience.com/natural-language-understanding-with-sequence-to-sequence-models-e87d41ad258b", "anchor_text": "Natural Language Understanding with Sequence to Sequence ModelsHow to predict the intent behind a customer query. Seq2Seq models explained. Slot filling demonstrated on ATIS dataset\u2026towardsdatascience.com"}, {"url": "https://towardsdatascience.com/heres-how-to-build-a-language-translator-in-few-lines-of-code-using-keras-30f7e0b3aa1d", "anchor_text": "Neural Machine Translation for Dummies \u2014 A 5 min guideCan AI keep endangered languages from disappearing?towardsdatascience.com"}, {"url": "https://towardsdatascience.com/truecasing-in-natural-language-processing-12c4df086c21", "anchor_text": "Truecasing in natural language processingRestoring capitalization in tweets and short messages improves the readability. Proper truecasing is essential for\u2026towardsdatascience.com"}, {"url": "https://medium.com/tag/nlp?source=post_page-----98b1d1594d7b---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/data-science?source=post_page-----98b1d1594d7b---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----98b1d1594d7b---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----98b1d1594d7b---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----98b1d1594d7b---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F98b1d1594d7b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbert-nlp-how-to-build-a-question-answering-bot-98b1d1594d7b&user=Michel+Kana%2C+Ph.D&userId=cb0b01fe20d2&source=-----98b1d1594d7b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F98b1d1594d7b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbert-nlp-how-to-build-a-question-answering-bot-98b1d1594d7b&user=Michel+Kana%2C+Ph.D&userId=cb0b01fe20d2&source=-----98b1d1594d7b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F98b1d1594d7b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbert-nlp-how-to-build-a-question-answering-bot-98b1d1594d7b&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----98b1d1594d7b--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F98b1d1594d7b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbert-nlp-how-to-build-a-question-answering-bot-98b1d1594d7b&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----98b1d1594d7b---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----98b1d1594d7b--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----98b1d1594d7b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----98b1d1594d7b--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----98b1d1594d7b--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----98b1d1594d7b--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----98b1d1594d7b--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----98b1d1594d7b--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----98b1d1594d7b--------------------------------", "anchor_text": ""}, {"url": "https://michel-kana.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://michel-kana.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Michel Kana, Ph.D"}, {"url": "https://michel-kana.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "5.4K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcb0b01fe20d2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbert-nlp-how-to-build-a-question-answering-bot-98b1d1594d7b&user=Michel+Kana%2C+Ph.D&userId=cb0b01fe20d2&source=post_page-cb0b01fe20d2--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F69e95067d2a1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbert-nlp-how-to-build-a-question-answering-bot-98b1d1594d7b&newsletterV3=cb0b01fe20d2&newsletterV3Id=69e95067d2a1&user=Michel+Kana%2C+Ph.D&userId=cb0b01fe20d2&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}