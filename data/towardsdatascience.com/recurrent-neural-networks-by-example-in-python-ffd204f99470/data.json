{"url": "https://towardsdatascience.com/recurrent-neural-networks-by-example-in-python-ffd204f99470", "time": 1682993893.894964, "path": "towardsdatascience.com/recurrent-neural-networks-by-example-in-python-ffd204f99470/", "webpage": {"metadata": {"title": "Recurrent Neural Networks by Example in Python | by Will Koehrsen | Towards Data Science", "h1": "Recurrent Neural Networks by Example in Python", "description": "In this article, we\u2019ll walk through building a recurrent neural network to write patent abstracts. We\u2019ll focus on the application in Python and getting up and running with natural language processing."}, "outgoing_paragraph_urls": [{"url": "https://www.manning.com/books/deep-learning-with-python", "anchor_text": "Deep Learning with Python", "paragraph_index": 0}, {"url": "https://course.fast.ai/about.html", "anchor_text": "top-down approach", "paragraph_index": 4}, {"url": "https://github.com/WillKoehrsen/recurrent-neural-networks", "anchor_text": "Jupyter Notebooks on GitHub", "paragraph_index": 6}, {"url": "https://github.com/WillKoehrsen/recurrent-neural-networks/tree/master/models", "anchor_text": "pre-trained models", "paragraph_index": 6}, {"url": "https://github.com/WillKoehrsen/recurrent-neural-networks/blob/master/notebooks/Quick%20Start%20to%20Recurrent%20Neural%20Networks.ipynb", "anchor_text": "Quick Start to Recurrent Neural Networks", "paragraph_index": 6}, {"url": "https://github.com/WillKoehrsen/recurrent-neural-networks/blob/master/notebooks/Deep%20Dive%20into%20Recurrent%20Neural%20Networks.ipynb", "anchor_text": "Deep Dive into Recurrent Neural Networks", "paragraph_index": 6}, {"url": "http://karpathy.github.io/2015/05/21/rnn-effectiveness/", "anchor_text": "recurrent neural network", "paragraph_index": 7}, {"url": "https://en.wikipedia.org/wiki/Bag-of-words_model", "anchor_text": "bag of words model", "paragraph_index": 11}, {"url": "https://en.wikipedia.org/wiki/Long_short-term_memory", "anchor_text": "Long Short-Term Memory", "paragraph_index": 12}, {"url": "https://stats.stackexchange.com/questions/185639/how-does-lstm-prevent-the-vanishing-gradient-problem", "anchor_text": "form of a gradient", "paragraph_index": 12}, {"url": "https://machinelearningmastery.com/sequence-prediction-problems-learning-lstm-recurrent-neural-networks/", "anchor_text": "sequence learning", "paragraph_index": 14}, {"url": "http://www.patentsview.org/querydev/", "anchor_text": "USPTO PatentsView", "paragraph_index": 19}, {"url": "https://keras.io/preprocessing/text/#tokenizer", "anchor_text": "Keras", "paragraph_index": 21}, {"url": "https://keras.io/preprocessing/text/#tokenizer", "anchor_text": "Tokenizer", "paragraph_index": 21}, {"url": "https://research.google.com/pubs/archive/35179.pdf", "anchor_text": "performance of the network is proportional to the amount of data", "paragraph_index": 29}, {"url": "http://keras.io", "anchor_text": "Keras", "paragraph_index": 35}, {"url": "https://deepsense.ai/keras-or-pytorch/", "anchor_text": "other neural network libraries may be faster or allow more flexibility", "paragraph_index": 35}, {"url": "http://neuralnetworksanddeeplearning.com/chap2.html", "anchor_text": "back-propagation", "paragraph_index": 38}, {"url": "https://machinelearningmastery.com/5-step-life-cycle-neural-network-models-keras/", "anchor_text": "don\u2019t have to worry about how this happens", "paragraph_index": 38}, {"url": "https://machinelearningmastery.com/reshape-input-data-long-short-term-memory-networks-keras/", "anchor_text": "The input to the", "paragraph_index": 39}, {"url": "https://machinelearningmastery.com/reshape-input-data-long-short-term-memory-networks-keras/", "anchor_text": "LSTM", "paragraph_index": 39}, {"url": "https://machinelearningmastery.com/reshape-input-data-long-short-term-memory-networks-keras/", "anchor_text": "layer is", "paragraph_index": 39}, {"url": "https://machinelearningmastery.com/reshape-input-data-long-short-term-memory-networks-keras/", "anchor_text": "(None, 50, 100)", "paragraph_index": 39}, {"url": "https://machinelearningmastery.com/reshape-input-data-long-short-term-memory-networks-keras/", "anchor_text": "which means", "paragraph_index": 39}, {"url": "https://github.com/WillKoehrsen/recurrent-neural-networks/blob/master/notebooks/Deep%20Dive%20into%20Recurrent%20Neural%20Networks.ipynb", "anchor_text": "the notebook", "paragraph_index": 40}, {"url": "http://ahogrammer.com/2017/01/20/the-list-of-pretrained-word-embeddings/", "anchor_text": "numerous embeddings you can find online", "paragraph_index": 41}, {"url": "https://nlp.stanford.edu/data/", "anchor_text": "available from Stanford", "paragraph_index": 41}, {"url": "https://nlp.stanford.edu/projects/glove/", "anchor_text": "GloVe (Global Vectors for Word Representation)", "paragraph_index": 41}, {"url": "https://towardsdatascience.com/neural-network-embeddings-explained-4d028e6f0526", "anchor_text": "Embeddings are learned", "paragraph_index": 46}, {"url": "https://keras.io/callbacks/", "anchor_text": "ModelCheckpoint and EarlyStopping in the form of Keras callbacks", "paragraph_index": 48}, {"url": "https://stats.stackexchange.com/questions/231061/how-to-use-early-stopping-properly-for-training-deep-neural-network", "anchor_text": "Early Stopping", "paragraph_index": 49}, {"url": "https://aws.amazon.com/ec2/instance-types/p2/", "anchor_text": "Amazon p2.xlarge instance", "paragraph_index": 51}, {"url": "https://github.com/WillKoehrsen/recurrent-neural-networks/blob/master/notebooks/Exploring%20Model%20Results.ipynb", "anchor_text": "notebook here", "paragraph_index": 54}, {"url": "https://github.com/WillKoehrsen/recurrent-neural-networks/tree/master/models", "anchor_text": "pre-trained models", "paragraph_index": 54}, {"url": "https://medium.com/machine-learning-at-petiteprogrammer/sampling-strategies-for-recurrent-neural-networks-9aea02a6616f", "anchor_text": "the predictions", "paragraph_index": 56}, {"url": "http://www.wipo.int/standards/en/pdf/03-12-a.pdf", "anchor_text": "nature of patent abstracts", "paragraph_index": 57}, {"url": "https://projector.tensorflow.org", "anchor_text": "Projector tool", "paragraph_index": 63}, {"url": "https://bigthink.com/endless-innovation/humans-are-the-worlds-best-pattern-recognition-machines-but-for-how-long", "anchor_text": "you could argue that humans are simply extreme pattern recognition machines", "paragraph_index": 64}, {"url": "https://machinelearningmastery.com/encoder-decoder-recurrent-neural-network-models-neural-machine-translation/", "anchor_text": "machine translation", "paragraph_index": 65}, {"url": "https://cs.stanford.edu/people/karpathy/sfmltalk.pdf", "anchor_text": "image captioning", "paragraph_index": 65}, {"url": "https://arxiv.org/ftp/arxiv/papers/1506/1506.04891.pdf", "anchor_text": "authorship identification", "paragraph_index": 65}, {"url": "https://github.com/DOsinga/deep_learning_cookbook", "anchor_text": "dive in and build applications", "paragraph_index": 66}, {"url": "http://twitter.com/@koehrsen_will", "anchor_text": "@koehrsen_will", "paragraph_index": 67}, {"url": "https://willk.online", "anchor_text": "willk.online", "paragraph_index": 67}], "all_paragraphs": ["The first time I attempted to study recurrent neural networks, I made the mistake of trying to learn the theory behind things like LSTMs and GRUs first. After several frustrating days looking at linear algebra equations, I happened on the following passage in Deep Learning with Python:", "In summary, you don\u2019t need to understand everything about the specific architecture of an LSTM cell; as a human, it shouldn\u2019t be your job to understand it. Just keep in mind what the LSTM cell is meant to do: allow past information to be reinjected at a later time.", "This was the author of the library Keras (Francois Chollet), an expert in deep learning, telling me I didn\u2019t need to understand everything at the foundational level! I realized that my mistake had been starting at the bottom, with the theory, instead of just trying to build a recurrent neural network.", "Shortly thereafter, I switched tactics and decided to try the most effective way of learning a data science technique: find a problem and solve it!", "This top-down approach means learning how to implement a method before going back and covering the theory. This way, I\u2019m able to figure out what I need to know along the way, and when I return to study the concepts, I have a framework into which I can fit each idea. In this mindset, I decided to stop worrying about the details and complete a recurrent neural network project.", "This article walks through how to build and use a recurrent neural network in Keras to write patent abstracts. The article is light on the theory, but as you work through the project, you\u2019ll find you pick up what you need to know along the way. The end result is you can build a useful application and figure out how a deep learning method for natural language processing works.", "The full code is available as a series of Jupyter Notebooks on GitHub. I\u2019ve also provided all the pre-trained models so you don\u2019t have to train them for several hours yourself! To get started as quickly as possible and investigate the models, see the Quick Start to Recurrent Neural Networks, and for in-depth explanations, refer to Deep Dive into Recurrent Neural Networks.", "It\u2019s helpful to understand at least some of the basics before getting to the implementation. At a high level, a recurrent neural network (RNN) processes sequences \u2014 whether daily stock prices, sentences, or sensor measurements \u2014 one element at a time while retaining a memory (called a state) of what has come previously in the sequence.", "Recurrent means the output at the current time step becomes the input to the next time step. At each element of the sequence, the model considers not just the current input, but what it remembers about the preceding elements.", "This memory allows the network to learn long-term dependencies in a sequence which means it can take the entire context into account when making a prediction, whether that be the next word in a sentence, a sentiment classification, or the next temperature measurement. A RNN is designed to mimic the human way of processing sequences: we consider the entire sentence when forming a response instead of words by themselves. For example, consider the following sentence:", "\u201cThe concert was boring for the first 15 minutes while the band warmed up but then was terribly exciting.\u201d", "A machine learning model that considers the words in isolation \u2014 such as a bag of words model \u2014 would probably conclude this sentence is negative. An RNN by contrast should be able to see the words \u201cbut\u201d and \u201cterribly exciting\u201d and realize that the sentence turns from negative to positive because it has looked at the entire sequence. Reading a whole sequence gives us a context for processing its meaning, a concept encoded in recurrent neural networks.", "At the heart of an RNN is a layer made of memory cells. The most popular cell at the moment is the Long Short-Term Memory (LSTM) which maintains a cell state as well as a carry for ensuring that the signal (information in the form of a gradient) is not lost as the sequence is processed. At each time step the LSTM considers the current word, the carry, and the cell state.", "The LSTM has 3 different gates and weight vectors: there is a \u201cforget\u201d gate for discarding irrelevant information; an \u201cinput\u201d gate for handling the current input, and an \u201coutput\u201d gate for producing predictions at each time step. However, as Chollet points out, it is fruitless trying to assign specific meanings to each of the elements in the cell.", "The function of each cell element is ultimately decided by the parameters (weights) which are learned during training. Feel free to label each cell part, but it\u2019s not necessary for effective use! Recall, the benefit of a Recurrent Neural Network for sequence learning is it maintains a memory of the entire sequence preventing prior information from being lost.", "There are several ways we can formulate the task of training an RNN to write text, in this case patent abstracts. However, we will choose to train it as a many-to-one sequence mapper. That is, we input a sequence of words and train the model to predict the very next word. The words will be mapped to integers and then to vectors using an embedding matrix (either pre-trained or trainable) before being passed into an LSTM layer.", "When we go to write a new patent, we pass in a starting sequence of words, make a prediction for the next word, update the input sequence, make another prediction, add the word to the sequence and continue for however many words we want to generate.", "The steps of the approach are outlined below:", "Keep in mind this is only one formulation of the problem: we could also use a character level model or make predictions for each word in the sequence. As with many concepts in machine learning, there is no one correct answer, but this approach works well in practice.", "Even with a neural network\u2019s powerful representation ability, getting a quality, clean dataset is paramount. The raw data for this project comes from USPTO PatentsView, where you can search for information on any patent applied for in the United States. I searched for the term \u201cneural network\u201d and downloaded the resulting patent abstracts \u2014 3500 in all. I found it best to train on a narrow subject, but feel free to try with a different set of patents.", "We\u2019ll start out with the patent abstracts as a list of strings. The main data preparation steps for our model are:", "These two steps can both be done using the Keras Tokenizer class. By default, this removes all punctuation, lowercases words, and then converts words to sequences of integers. A Tokenizer is first fit on a list of strings and then converts this list into a list of lists of integers. This is demonstrated below:", "The output of the first cell shows the original abstract and the output of the second the tokenized sequence. Each abstract is now represented as integers.", "We can use the idx_word attribute of the trained tokenizer to figure out what each of these integers means:", "If you look closely, you\u2019ll notice that the Tokenizer has removed all punctuation and lowercased all the words. If we use these settings, then the neural network will not learn proper English! We can adjust this by changing the filters to the Tokenizer to not remove punctuation.", "See the notebooks for different implementations, but, when we use pre-trained embeddings, we\u2019ll have to remove the uppercase because there are no lowercase letters in the embeddings. When training our own embeddings, we don\u2019t have to worry about this because the model will learn different representations for lower and upper case.", "The previous step converts all the abstracts to sequences of integers. The next step is to create a supervised machine learning problem with which to train the network. There are numerous ways you can set up a recurrent neural network task for text generation, but we\u2019ll use the following:", "Give the network a sequence of words and train it to predict the next word.", "The number of words is left as a parameter; we\u2019ll use 50 for the examples shown here which means we give our network 50 words and train it to predict the 51st. Other ways of training the network would be to have it predict the next word at each point in the sequence \u2014 make a prediction for each input word rather than once for the entire sequence \u2014 or train the model using individual characters. The implementation used here is not necessarily optimal \u2014 there is no accepted best solution \u2014 but it works well!", "Creating the features and labels is relatively simple and for each abstract (represented as integers) we create multiple sets of features and labels. We use the first 50 words as features with the 51st as the label, then use words 2\u201351 as features and predict the 52nd and so on. This gives us significantly more training data which is beneficial because the performance of the network is proportional to the amount of data that it sees during training.", "The implementation of creating features and labels is below:", "The features end up with shape (296866, 50) which means we have almost 300,000 sequences each with 50 tokens. In the language of recurrent neural networks, each sequence has 50 timesteps each with 1 feature.", "We could leave the labels as integers, but a neural network is able to train most effectively when the labels are one-hot encoded. We can one-hot encode the labels with numpy very quickly using the following:", "To find the word corresponding to a row in label_array , we use:", "After getting all of our features and labels properly formatted, we want to split them into a training and validation set (see notebook for details). One important point here is to shuffle the features and labels simultaneously so the same abstracts do not all end up in one set.", "Keras is an incredible library: it allows us to build state-of-the-art models in a few lines of understandable Python code. Although other neural network libraries may be faster or allow more flexibility, nothing can beat Keras for development time and ease-of-use.", "The code for a simple LSTM is below with an explanation following:", "We are using the Keras Sequential API which means we build the network up one layer at a time. The layers are as follows:", "The model is compiled with the Adam optimizer (a variant on Stochastic Gradient Descent) and trained using the categorical_crossentropy loss. During training, the network will try to minimize the log loss by adjusting the trainable parameters (weights). As always, the gradients of the parameters are calculated using back-propagation and updated with the optimizer. Since we are using Keras, we don\u2019t have to worry about how this happens behind the scenes, only about setting up the network correctly.", "Without updating the embeddings, there are many fewer parameters to train in the network. The input to the LSTM layer is (None, 50, 100) which means that for each batch (the first dimension), each sequence has 50 timesteps (words), each of which has 100 features after embedding. Input to an LSTM layer always has the (batch_size, timesteps, features) shape.", "There are many ways to structure this network and there are several others covered in the notebook. For example, we can use two LSTM layers stacked on each other, a Bidirectional LSTM layer that processes sequences from both directions, or more Dense layers. I found the set-up above to work well.", "Once the network is built, we still have to supply it with the pre-trained word embeddings. There are numerous embeddings you can find online trained on different corpuses (large bodies of text). The ones we\u2019ll use are available from Stanford and come in 100, 200, or 300 dimensions (we\u2019ll stick to 100). These embeddings are from the GloVe (Global Vectors for Word Representation) algorithm and were trained on Wikipedia.", "Even though the pre-trained embeddings contain 400,000 words, there are some words in our vocab that are included. When we represent these words with embeddings, they will have 100-d vectors of all zeros. This problem can be overcome by training our own embeddings or by setting the Embedding layer's trainable parameter to True (and removing the Masking layer).", "We can quickly load in the pre-trained embeddings from disk and make an embedding matrix with the following code:", "What this does is assign a 100-dimensional vector to each word in the vocab. If the word has no pre-trained embedding then this vector will be all zeros.", "To explore the embeddings, we can use the cosine similarity to find the words closest to a given query word in the embedding space:", "Embeddings are learned which means the representations apply specifically to one task. When using pre-trained embeddings, we hope the task the embeddings were learned on is close enough to our task so the embeddings are meaningful. If these embeddings were trained on tweets, we might not expect them to work well, but since they were trained on Wikipedia data, they should be generally applicable to a range of language processing tasks.", "If you have a lot of data and the computer time, it\u2019s usually better to learn your own embeddings for a specific task. In the notebook I take both approaches and the learned embeddings perform slightly better.", "With the training and validation data prepared, the network built, and the embeddings loaded, we are almost ready for our model to learn how to write patent abstracts. However, good steps to take when training neural networks are to use ModelCheckpoint and EarlyStopping in the form of Keras callbacks:", "Using Early Stopping means we won\u2019t overfit to the training data and waste time training for extra epochs that don\u2019t improve performance. The Model Checkpoint means we can access the best model and, if our training is disrupted 1000 epochs in, we won\u2019t have lost all the progress!", "The model can then be trained with the following code:", "On an Amazon p2.xlarge instance ($0.90 / hour reserved), this took just over 1 hour to finish. Once the training is done, we can load back in the best saved model and evaluate a final time on the validation data.", "Overall, the model using pre-trained word embeddings achieved a validation accuracy of 23.9%. This is pretty good considering as a human I find it extremely difficult to predict the next word in these abstracts! A naive guess of the most common word (\u201cthe\u201d) yields an accuracy around 8%. The metrics for all the models in the notebook are shown below:", "The best model used pre-trained embeddings and the same architecture as shown above. I\u2019d encourage anyone to try training with a different model!", "Of course, while high metrics are nice, what matters is if the network can produce reasonable patent abstracts. Using the best model we can explore the model generation ability. If you want to run this on your own hardware, you can find the notebook here and the pre-trained models are on GitHub.", "To produce output, we seed the network with a random sequence chosen from the patent abstracts, have it make a prediction of the next word, add the prediction to the sequence, and continue making predictions for however many words we want. Some results are shown below:", "One important parameter for the output is the diversity of the predictions. Instead of using the predicted word with the highest probability, we inject diversity into the predictions and then choose the next word with a probability proportional to the more diverse predictions. Too high a diversity and the generated output starts to seem random, but too low and the network can get into recursive loops of output.", "The output isn\u2019t too bad! Some of the time it\u2019s tough to determine which is computer generated and which is from a machine. Part of this is due to the nature of patent abstracts which, most of the time, don\u2019t sound like they were written by a human.", "Another use of the network is to seed it with our own starting sequence. We can use any text we want and see where the network takes it:", "Again, the results are not entirely believable but they do resemble English.", "As a final test of the recurrent neural network, I created a game to guess whether the model or a human generated the output. Here\u2019s the first example where two of the options are from a computer and one is from a human:", "What\u2019s your guess? The answer is that the second is the actual abstract written by a person (well, it\u2019s what was actually in the abstract. I\u2019m not sure these abstracts are written by people). Here\u2019s another one:", "This time the third had a flesh and blood writer.", "There are additional steps we can use to interpret the model such as finding which neurons light up with different input sequences. We can also look at the learned embeddings (or visualize them with the Projector tool). We\u2019ll leave those topics for another time, and conclude that we know now how to implement a recurrent neural network to effectively mimic human text.", "It\u2019s important to recognize that the recurrent neural network has no concept of language understanding. It is effectively a very sophisticated pattern recognition machine. Nonetheless, unlike methods such as Markov chains or frequency analysis, the rnn makes predictions based on the ordering of elements in the sequence. Getting a little philosophical here, you could argue that humans are simply extreme pattern recognition machines and therefore the recurrent neural network is only acting like a human machine.", "The uses of recurrent neural networks go far beyond text generation to machine translation, image captioning, and authorship identification. Although this application we covered here will not displace any humans, it\u2019s conceivable that with more training data and a larger model, a neural network would be able to synthesize new, reasonable patent abstracts.", "It can be easy to get stuck in the details or the theory behind a complex technique, but a more effective method for learning data science tools is to dive in and build applications. You can always go back later and catch up on the theory once you know what a technique is capable of and how it works in practice. Most of us won\u2019t be designing neural networks, but it\u2019s worth learning how to use them effectively. This means putting away the books, breaking out the keyboard, and coding up your very own network.", "As always, I welcome feedback and constructive criticism. I can be reached on Twitter @koehrsen_will or through my website at willk.online.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Scientist at Cortex Intel, Data Science Communicator"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fffd204f99470&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrent-neural-networks-by-example-in-python-ffd204f99470&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrent-neural-networks-by-example-in-python-ffd204f99470&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrent-neural-networks-by-example-in-python-ffd204f99470&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrent-neural-networks-by-example-in-python-ffd204f99470&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----ffd204f99470--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ffd204f99470--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://williamkoehrsen.medium.com/?source=post_page-----ffd204f99470--------------------------------", "anchor_text": ""}, {"url": "https://williamkoehrsen.medium.com/?source=post_page-----ffd204f99470--------------------------------", "anchor_text": "Will Koehrsen"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe2f299e30cb9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrent-neural-networks-by-example-in-python-ffd204f99470&user=Will+Koehrsen&userId=e2f299e30cb9&source=post_page-e2f299e30cb9----ffd204f99470---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fffd204f99470&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrent-neural-networks-by-example-in-python-ffd204f99470&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fffd204f99470&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrent-neural-networks-by-example-in-python-ffd204f99470&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.pexels.com/photo/agriculture-alternative-energy-clouds-countryside-414837/", "anchor_text": "Source"}, {"url": "https://www.manning.com/books/deep-learning-with-python", "anchor_text": "Deep Learning with Python"}, {"url": "https://course.fast.ai/about.html", "anchor_text": "top-down approach"}, {"url": "https://github.com/WillKoehrsen/recurrent-neural-networks", "anchor_text": "Jupyter Notebooks on GitHub"}, {"url": "https://github.com/WillKoehrsen/recurrent-neural-networks/tree/master/models", "anchor_text": "pre-trained models"}, {"url": "https://github.com/WillKoehrsen/recurrent-neural-networks/blob/master/notebooks/Quick%20Start%20to%20Recurrent%20Neural%20Networks.ipynb", "anchor_text": "Quick Start to Recurrent Neural Networks"}, {"url": "https://github.com/WillKoehrsen/recurrent-neural-networks/blob/master/notebooks/Deep%20Dive%20into%20Recurrent%20Neural%20Networks.ipynb", "anchor_text": "Deep Dive into Recurrent Neural Networks"}, {"url": "http://karpathy.github.io/2015/05/21/rnn-effectiveness/", "anchor_text": "recurrent neural network"}, {"url": "https://www.manning.com/books/deep-learning-with-python", "anchor_text": "Source"}, {"url": "https://en.wikipedia.org/wiki/Bag-of-words_model", "anchor_text": "bag of words model"}, {"url": "https://en.wikipedia.org/wiki/Long_short-term_memory", "anchor_text": "Long Short-Term Memory"}, {"url": "https://stats.stackexchange.com/questions/185639/how-does-lstm-prevent-the-vanishing-gradient-problem", "anchor_text": "form of a gradient"}, {"url": "https://www.manning.com/books/deep-learning-with-python", "anchor_text": "Source"}, {"url": "https://machinelearningmastery.com/sequence-prediction-problems-learning-lstm-recurrent-neural-networks/", "anchor_text": "sequence learning"}, {"url": "http://www.patentsview.org/querydev/", "anchor_text": "USPTO PatentsView"}, {"url": "https://keras.io/preprocessing/text/#tokenizer", "anchor_text": "Keras"}, {"url": "https://keras.io/preprocessing/text/#tokenizer", "anchor_text": "Tokenizer"}, {"url": "https://research.google.com/pubs/archive/35179.pdf", "anchor_text": "performance of the network is proportional to the amount of data"}, {"url": "http://keras.io", "anchor_text": "Keras"}, {"url": "https://deepsense.ai/keras-or-pytorch/", "anchor_text": "other neural network libraries may be faster or allow more flexibility"}, {"url": "https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/", "anchor_text": "dropout to prevent overfitting"}, {"url": "http://neuralnetworksanddeeplearning.com/chap2.html", "anchor_text": "back-propagation"}, {"url": "https://machinelearningmastery.com/5-step-life-cycle-neural-network-models-keras/", "anchor_text": "don\u2019t have to worry about how this happens"}, {"url": "https://machinelearningmastery.com/reshape-input-data-long-short-term-memory-networks-keras/", "anchor_text": "The input to the"}, {"url": "https://machinelearningmastery.com/reshape-input-data-long-short-term-memory-networks-keras/", "anchor_text": "LSTM"}, {"url": "https://machinelearningmastery.com/reshape-input-data-long-short-term-memory-networks-keras/", "anchor_text": "layer is"}, {"url": "https://machinelearningmastery.com/reshape-input-data-long-short-term-memory-networks-keras/", "anchor_text": "(None, 50, 100)"}, {"url": "https://machinelearningmastery.com/reshape-input-data-long-short-term-memory-networks-keras/", "anchor_text": "which means"}, {"url": "https://github.com/WillKoehrsen/recurrent-neural-networks/blob/master/notebooks/Deep%20Dive%20into%20Recurrent%20Neural%20Networks.ipynb", "anchor_text": "the notebook"}, {"url": "http://ahogrammer.com/2017/01/20/the-list-of-pretrained-word-embeddings/", "anchor_text": "numerous embeddings you can find online"}, {"url": "https://nlp.stanford.edu/data/", "anchor_text": "available from Stanford"}, {"url": "https://nlp.stanford.edu/projects/glove/", "anchor_text": "GloVe (Global Vectors for Word Representation)"}, {"url": "https://towardsdatascience.com/neural-network-embeddings-explained-4d028e6f0526", "anchor_text": "Embeddings are learned"}, {"url": "https://keras.io/callbacks/", "anchor_text": "ModelCheckpoint and EarlyStopping in the form of Keras callbacks"}, {"url": "https://stats.stackexchange.com/questions/231061/how-to-use-early-stopping-properly-for-training-deep-neural-network", "anchor_text": "Early Stopping"}, {"url": "https://aws.amazon.com/ec2/instance-types/p2/", "anchor_text": "Amazon p2.xlarge instance"}, {"url": "https://github.com/WillKoehrsen/recurrent-neural-networks/blob/master/notebooks/Exploring%20Model%20Results.ipynb", "anchor_text": "notebook here"}, {"url": "https://github.com/WillKoehrsen/recurrent-neural-networks/tree/master/models", "anchor_text": "pre-trained models"}, {"url": "https://medium.com/machine-learning-at-petiteprogrammer/sampling-strategies-for-recurrent-neural-networks-9aea02a6616f", "anchor_text": "the predictions"}, {"url": "http://www.wipo.int/standards/en/pdf/03-12-a.pdf", "anchor_text": "nature of patent abstracts"}, {"url": "https://projector.tensorflow.org", "anchor_text": "Projector tool"}, {"url": "https://bigthink.com/endless-innovation/humans-are-the-worlds-best-pattern-recognition-machines-but-for-how-long", "anchor_text": "you could argue that humans are simply extreme pattern recognition machines"}, {"url": "https://machinelearningmastery.com/encoder-decoder-recurrent-neural-network-models-neural-machine-translation/", "anchor_text": "machine translation"}, {"url": "https://cs.stanford.edu/people/karpathy/sfmltalk.pdf", "anchor_text": "image captioning"}, {"url": "https://arxiv.org/ftp/arxiv/papers/1506/1506.04891.pdf", "anchor_text": "authorship identification"}, {"url": "https://developer.nvidia.com/discover/lstm", "anchor_text": "Source"}, {"url": "https://github.com/DOsinga/deep_learning_cookbook", "anchor_text": "dive in and build applications"}, {"url": "http://twitter.com/@koehrsen_will", "anchor_text": "@koehrsen_will"}, {"url": "https://willk.online", "anchor_text": "willk.online"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----ffd204f99470---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----ffd204f99470---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/education?source=post_page-----ffd204f99470---------------education-----------------", "anchor_text": "Education"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----ffd204f99470---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/tag/python?source=post_page-----ffd204f99470---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fffd204f99470&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrent-neural-networks-by-example-in-python-ffd204f99470&user=Will+Koehrsen&userId=e2f299e30cb9&source=-----ffd204f99470---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fffd204f99470&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrent-neural-networks-by-example-in-python-ffd204f99470&user=Will+Koehrsen&userId=e2f299e30cb9&source=-----ffd204f99470---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fffd204f99470&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrent-neural-networks-by-example-in-python-ffd204f99470&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ffd204f99470--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fffd204f99470&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrent-neural-networks-by-example-in-python-ffd204f99470&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----ffd204f99470---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----ffd204f99470--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----ffd204f99470--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----ffd204f99470--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----ffd204f99470--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----ffd204f99470--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----ffd204f99470--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----ffd204f99470--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----ffd204f99470--------------------------------", "anchor_text": ""}, {"url": "https://williamkoehrsen.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://williamkoehrsen.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Will Koehrsen"}, {"url": "https://williamkoehrsen.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "38K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe2f299e30cb9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrent-neural-networks-by-example-in-python-ffd204f99470&user=Will+Koehrsen&userId=e2f299e30cb9&source=post_page-e2f299e30cb9--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fe7d4a87a913e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrent-neural-networks-by-example-in-python-ffd204f99470&newsletterV3=e2f299e30cb9&newsletterV3Id=e7d4a87a913e&user=Will+Koehrsen&userId=e2f299e30cb9&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}