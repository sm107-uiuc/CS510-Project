{"url": "https://towardsdatascience.com/qrash-course-ii-from-q-learning-to-gradient-policy-actor-critic-in-12-minutes-8e8b47129c8c", "time": 1683001551.2210891, "path": "towardsdatascience.com/qrash-course-ii-from-q-learning-to-gradient-policy-actor-critic-in-12-minutes-8e8b47129c8c/", "webpage": {"metadata": {"title": "Qrash Course II: From Q-Learning to Gradient Policy & Actor-Critic in 12 Minutes | by Shaked Zychlinski | Towards Data Science", "h1": "Qrash Course II: From Q-Learning to Gradient Policy & Actor-Critic in 12 Minutes", "description": "Policy Gradients and Actor-Critics are some of the fundamental building blocks of Reinforcement Learning. We\u2019ll cover these two in this blogpost, as well as some very common optimizations."}, "outgoing_paragraph_urls": [{"url": "https://github.com/shakedzy/notebooks/tree/master/gradient_policy_and_actor_critic", "anchor_text": "my GitHub page", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/qrash-course-ii-from-q-learning-to-gradient-policy-actor-critic-in-12-minutes-8e8b47129c8c?sk=77e4a62c7dd982d32d1034c151552271", "anchor_text": "Friends Link", "paragraph_index": 1}, {"url": "https://medium.com/@shakedzy/qrash-course-deep-q-networks-from-the-ground-up-1bbda41d3677", "anchor_text": "Qrash Course post", "paragraph_index": 2}, {"url": "http://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf", "anchor_text": "free online version", "paragraph_index": 15}, {"url": "https://github.com/shakedzy/notebooks/blob/master/gradient_policy_and_actor_critic/Cart-Pole%20with%20Policy%20Gradient.ipynb", "anchor_text": "my solution", "paragraph_index": 21}, {"url": "https://github.com/shakedzy/notebooks/blob/master/gradient_policy_and_actor_critic/Acrobot%20with%20Actor-Critic.ipynb", "anchor_text": "my implementation", "paragraph_index": 28}, {"url": "https://gym.openai.com/envs/Acrobot-v1/", "anchor_text": "Acrobot", "paragraph_index": 28}, {"url": "https://github.com/shakedzy/notebooks/blob/master/gradient_policy_and_actor_critic/Acrobot%20with%20A2C.ipynb", "anchor_text": "my implementation of A2C on GitHub", "paragraph_index": 34}, {"url": "https://github.com/shakedzy/notebooks/blob/master/gradient_policy_and_actor_critic/Continuous%20Mountain%20Car%20with%20DDPG.ipynb", "anchor_text": "my implementation of DDPG", "paragraph_index": 37}, {"url": "http://shakedzy.xyz", "anchor_text": "shakedzy.xyz", "paragraph_index": 40}], "all_paragraphs": ["Implementations of all algorithms discussed in this blogpost can be found on my GitHub page.", "Read this blogpost even without being a Medium member using this Friends Link", "The previous \u2014 and first \u2014 Qrash Course post took us from knowing pretty much nothing about Reinforcement Learning all the way to fully understand one of the most fundamental algorithms of RL: Q Learning, as well as its Deep Learning version, Deep Q-Network. Let\u2019s continue our journey and introduce two more algorithms: Gradient Policy and Actor-Critic. These two, along with DQN, are probably the most fundamental building-blocks of modern Deep Reinforcement Learning.", "The first question we should probably ask ourselves is why should we advance from Q-Learning? Where does it fail or underperforms? Well, this algorithm does have a few pitfalls, and it\u2019s important to understand them:", "How should we handle these situation? Let\u2019s get to know our saviors!", "A Q-Learning algorithms learns by trying to find each state\u2019s action-value function \u2014 the Q-Value function. Its entire learning procedure is based on the idea of figuring out the quality of each possible action, and select according to this knowledge. So Q-Learning tries to have complete and unbiased knowledge of all possible moves \u2014 which is also its most major drawback, as this requires sufficient number of attempts of each possible state and action. Policy Gradient algorithm learns in a more robust way, by not trying to evaluate the value of each action \u2014 but by simply evaluate which action should it prefer.", "Let me illustrate it better: each model eventually has some learned-parameters which it tries to optimize (these are usually the neural-network\u2019s weights). Let\u2019s denote these parameters as \u03b8. In DQN, the learning cycle is:", "This means that the training procedure optimizes the learned parameters of the network, which is then used to compute Q-Values. We then use those Q-Values to decide on our policy \u03c0 (where a policy is simply each action\u2019s probability to be selected) \u2014 DQN\u2019s \u03b5-greedy policy depends on the predicted Q-Values, as it gives the highest probability to the action with the highest Q-Value:", "The learning cycle of a Policy Gradient algorithm is shorter, skipping the Q-Value part:", "This mean that the network will directly output the probability of selecting each action, skipping the extra calculation. This will allow this algorithm to be more robust.", "We\u2019re now left with one very important question \u2014 how do we optimize the network\u2019s weights? In DQN it was straightforward: we know how to calculate Q-Values, so we can optimize the algorithm according to them. But what do we do now? Well, our main goal it to make sure to model gets better after each learning phase. Let\u2019s try to write this as a very general update rule of the weights at each time step t as:", "where we defined J as some performance measure, which we\u2019ll get to in a second, and \u03b1 is a learning rate. I\u2019d like to take a minute to clarify the difference between a time-step t, which indicates steps taken in an episode, and s, which marks states of an episode. States are defined by the environment, for example \u2014 all possible boards of chess. Steps are defined by the agent, and mark the sequence of states it has been through. That means that step t=3 and t=7 might be the same state s, as some environments allow an agent to return to the exact same state multiple times. On the other hand, each step t happens once and only once in each episode. Steps are a chronological time sequence.", "Let\u2019s go back to our update rule, which we defined as dependent on the gradient of our new J parameter. While it might look familiar to the back-propagation phase of Neural Networks training, there\u2019s a super-important difference here: in Gradient Policy we wish to increase performance, and therefore we wish to maximize the derivative of J, and not minimize it. This is known as gradient ascent \u2014 contrary to gradient descent performed in back-propagation. It\u2019s actually quite the same logic, just the other way around.", "At this point it\u2019s kind of obvious that how we define J will make all the difference, but it\u2019s also entirely up to us to decide what is the right definition. Let\u2019s try to find something simple \u2014 something that depends on the Q-Value (even though the algorithm never calculates it). Recall that a Q-Value Q(s,a) is a measure of all the rewards the agent shall receive till the end of the episode, starting from state s and after performing action a. The sum of all rewards seems to be quite a good performance measurement \u2014 after all, the agent\u2019s sole purpose is to increase its overall collected rewards. So we can use as our performance measure the total accumulated rewards over an entire episode:", "This is the sum of all Q-Values of the initial state s\u2070, multiplied by probability to select each action. This exactly our expected overall reward of an entire episode. Now, this summation over all Q-Values of a certain state multiplied by their probabilities has a name: it\u2019s called the state\u2019s Value Function, and is denoted as V(s). A state\u2019s Value Function is a measure of the expected reward from state s and till the end of the episode, without knowing which action will be chosen. So we basically defined the initial state\u2019s Value Function as our performance measure: J = V(s\u2070).", "Now that we have J, we need to calculate its gradient. This involves some complex math, as J depends on the probability of action selection p(a), which is derived from the policy \u03c0 \u2014 but \u03c0 depends on J, since this is how we defined it. You can find the entire mathematical derivation in \u201cReinforcement Learning, An Introduction\u201d by Sutton & Barto (2nd ed.), pages 325\u2013327 (here\u2019s a free online version). I\u2019ll skip it here and write the down the solution:", "Where G_t is the total accumulated reward from step t till the end of the episode. This means the update rule for our \u03b8 is:", "It might seems intimidating or bizarre to perform a logarithm of a policy \u2014 but it\u2019s just another mathematical function we need to apply, nothing more.", "I\u2019d like to emphasize again an important feature of the Policy Gradient algorithm \u2014 it learns the policy directly. This means the network\u2019s input is the current state s and the output is the probability to select each action:", "So how do we actually select an action? We simply perform a weighted-sampling over the action. This also solves our need of exploration \u2014 every action has a chance of being selected, but not equally, as the best action will be the most likely to be chosen.", "Unlike Q-Learning, the Policy Gradient algorithm is an on-policy algorithm \u2014 which means it learns only using state-action transitions made by the current active policy. Technically, this means there is not Experience Replay memory like in DQN. Once the model is trained, its \u03b8 parameters change, and therefore also it policy. This means that all experience collected before this training must be discarded, and cannot be used for training anymore. So every piece of data we collect for training is used once, and only once.", "While the general idea is hopefully understood at this point, an example is always better. A very popular task to solve using Reinforcement Learning is the Cart-Pole problem: a cart, which can move either left or right, needs to make sure the pole which stands on it doesn\u2019t fall. Here\u2019s my solution to this challenge. Try to implement it yourself too.", "Let\u2019s take a minute to realize what we have developed so far: we now have an Agent that learns a policy without the need to learn the actual value of each action. The only thing it really needs is some performance metric, which it will try to maximize \u2014 and in our case we chose the total expected reward of an entire episode.", "The benefits of this method is straightforward \u2014 there\u2019s no need to visit and try each possible state-action pair, as the Agent develops some kind of a \u201cgut-feeling\u201d about what it should do. But this comes with a drawback due to the Agent\u2019s absolute dependency in its performance metric. Let\u2019s consider our case, where we chose the overall reward of an episode as the performance: consider an episode with a hundred steps taken. Each step yields a reward of +10, but step 47 yields a reward of -100. The performance measure we chose cannot distinguish this pitfall, as all it only knows the overall reward. This means our Agent might never try another action when it reaches step 47, as it doesn\u2019t learn any state- and action-specific knowledge.", "How can we tackle this? We would\u2019ve liked to have an Agent that on one hand, learns a policy in a similar way to the Gradient Policy method, though on the other hand we understand the importance of state- and action-specific knowledge, like in the Q-Learning method. The solution? Combine the two together.", "The combined method is known as Actor-Critic, and is made of two sub-Agents learning together: one learns the policy which should be acted by (and is therefore known as the Actor), and the other learns the Q-Value of each state and action (and is therefore known as the Critic). We then change our update rule of our \u03b8 to be:", "Notice we simply replaced the total reward G with the Q-Value, but the Q-Value is now also a learned parameter \u2014 learned by the second sub-Agent. This yields a bit more complex architecture:", "Note that just like Policy Gradients, Actor-Critics are also on-policy models \u2014 which again means that after each training, all previous training data is discarded.", "And that\u2019s it. You got it! You can try to implement an Actor-Critic yourself now, or check out my implementation solving the Acrobot challenge.", "We\u2019re pretty much done here, but let\u2019s take one more step forward and get to know some very useful optimizations to our Actor-Critic Agent.", "Recall that we now use the Q-Value in our update rule in order to allow the Agent to have some state-action-specific knowledge. Now let me ask you a question: is Q(s,a)=100 a good Q-Value or not? The answer is \u2014 we don\u2019t know. We don\u2019t know because we don\u2019t have information about the Q-Values of the other possible actions. If all other actions yield a Q-Value of 1, then 100 is really really good. But if all others yield 10,000 \u2014 then 100 is actually quite bad. This isn\u2019t confusing only to you \u2014 but also to the model. It would have been helpful to know how good is a certain action compared to rest \u2014 or in other words, what is the advantage of taking a specific action. So we can replace the Q-Value in our Actor-Critic update rule with the Advantage of an action, which is defined as:", "where V(s) is the state\u2019s Value Function, which we\u2019ve discussed before. Learning the advantage of an action is much easier\u2014 a positive A(s,a) is good, and a negative one is bad. This variation of the Actor-Critic model is known as the Advantage Actor-Critic, which is abbreviated to AAC, or more commonly: A2C (which is just a fancy way of writing that there are two As and a C).", "You might notice that the A2C version seems to make learning a little more complex, as now the Agent needs to learn both Q(s,a) and V(s). But that\u2019s actually not true. If a Q-Value is the reward received from state s and action a and then continuing till the end of the episode, we can write it like this:", "meaning \u2014 a Q-Value is actually the immediate reward r and the value of the next state, s\u2019. This means we can write the Advantage as:", "So we actually only need to learn the Value Function, and use it twice \u2014 for the state s and the next state s\u2019. This little tweak actually makes the A2C easier to implement than the original Actor-Critic. You can check out my implementation of A2C on GitHub.", "Why having only one Agent when we can have many? This is the core idea behind the A3C model, which stands for Asynchronous Advantage Actor-Critic. The idea is simple: have many different Agents, each playing in its own copy of the environment, but they all share the same policy and parameters. Each Agent update the shared policy on its own time (asynchronously of the other Agents), which makes the learning process faster and more robust.", "The DDPG (Deep Deterministic Policy Gradient) algorithm was designed with the problem of continuous action-space in mind. The training phase of an Actor-Critic model is very noisy, as it learns based on its own predictions. To tackle this, DDPG borrows some components from our beloved DQN: First, it uses an Experience Replay memory, which makes it an off-policy model. Second, it uses the same noise-reduction method of the Double DQN model \u2014 it uses two copies of both the Actor and the Critic, one copy is trained and the second is updated slowly, in the following way:", "Here, the online network is the one trained, and \u03c4 << 1. See my implementation of DDPG for a more detailed explanation.", "In this tutorial, we\u2019ve expanded our knowledge of the fundamental building-blocks of Reinforcement Learning, adding Gradient Policy and Actor-Critic to our arsenal of algorithms. Modern Reinforcement Learning is much more vast, but eventually it all relies on some similar ideas \u2014 which you now understand. Way to go!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Head of Recommendations at Lightricks. Lives in Tel-Aviv, Israel. See me on shakedzy.xyz"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F8e8b47129c8c&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fqrash-course-ii-from-q-learning-to-gradient-policy-actor-critic-in-12-minutes-8e8b47129c8c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fqrash-course-ii-from-q-learning-to-gradient-policy-actor-critic-in-12-minutes-8e8b47129c8c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fqrash-course-ii-from-q-learning-to-gradient-policy-actor-critic-in-12-minutes-8e8b47129c8c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fqrash-course-ii-from-q-learning-to-gradient-policy-actor-critic-in-12-minutes-8e8b47129c8c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----8e8b47129c8c--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8e8b47129c8c--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://shakedzy.medium.com/?source=post_page-----8e8b47129c8c--------------------------------", "anchor_text": ""}, {"url": "https://shakedzy.medium.com/?source=post_page-----8e8b47129c8c--------------------------------", "anchor_text": "Shaked Zychlinski"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F43218078e688&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fqrash-course-ii-from-q-learning-to-gradient-policy-actor-critic-in-12-minutes-8e8b47129c8c&user=Shaked+Zychlinski&userId=43218078e688&source=post_page-43218078e688----8e8b47129c8c---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8e8b47129c8c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fqrash-course-ii-from-q-learning-to-gradient-policy-actor-critic-in-12-minutes-8e8b47129c8c&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8e8b47129c8c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fqrash-course-ii-from-q-learning-to-gradient-policy-actor-critic-in-12-minutes-8e8b47129c8c&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://github.com/shakedzy/notebooks/tree/master/gradient_policy_and_actor_critic", "anchor_text": "my GitHub page"}, {"url": "https://towardsdatascience.com/qrash-course-ii-from-q-learning-to-gradient-policy-actor-critic-in-12-minutes-8e8b47129c8c?sk=77e4a62c7dd982d32d1034c151552271", "anchor_text": "Friends Link"}, {"url": "https://medium.com/@shakedzy/qrash-course-deep-q-networks-from-the-ground-up-1bbda41d3677", "anchor_text": "Introduction to Reinforcement Learning and Q-Learning"}, {"url": "https://medium.com/@shakedzy/qrash-course-deep-q-networks-from-the-ground-up-1bbda41d3677", "anchor_text": "Qrash Course post"}, {"url": "http://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf", "anchor_text": "free online version"}, {"url": "https://github.com/shakedzy/notebooks/blob/master/gradient_policy_and_actor_critic/Cart-Pole%20with%20Policy%20Gradient.ipynb", "anchor_text": "my solution"}, {"url": "https://github.com/shakedzy/notebooks/blob/master/gradient_policy_and_actor_critic/Acrobot%20with%20Actor-Critic.ipynb", "anchor_text": "my implementation"}, {"url": "https://gym.openai.com/envs/Acrobot-v1/", "anchor_text": "Acrobot"}, {"url": "https://github.com/shakedzy/notebooks/blob/master/gradient_policy_and_actor_critic/Acrobot%20with%20A2C.ipynb", "anchor_text": "my implementation of A2C on GitHub"}, {"url": "https://github.com/shakedzy/notebooks/blob/master/gradient_policy_and_actor_critic/Continuous%20Mountain%20Car%20with%20DDPG.ipynb", "anchor_text": "my implementation of DDPG"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----8e8b47129c8c---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----8e8b47129c8c---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----8e8b47129c8c---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----8e8b47129c8c---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----8e8b47129c8c---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8e8b47129c8c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fqrash-course-ii-from-q-learning-to-gradient-policy-actor-critic-in-12-minutes-8e8b47129c8c&user=Shaked+Zychlinski&userId=43218078e688&source=-----8e8b47129c8c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8e8b47129c8c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fqrash-course-ii-from-q-learning-to-gradient-policy-actor-critic-in-12-minutes-8e8b47129c8c&user=Shaked+Zychlinski&userId=43218078e688&source=-----8e8b47129c8c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8e8b47129c8c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fqrash-course-ii-from-q-learning-to-gradient-policy-actor-critic-in-12-minutes-8e8b47129c8c&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8e8b47129c8c--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F8e8b47129c8c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fqrash-course-ii-from-q-learning-to-gradient-policy-actor-critic-in-12-minutes-8e8b47129c8c&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----8e8b47129c8c---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----8e8b47129c8c--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----8e8b47129c8c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----8e8b47129c8c--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----8e8b47129c8c--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----8e8b47129c8c--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----8e8b47129c8c--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----8e8b47129c8c--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----8e8b47129c8c--------------------------------", "anchor_text": ""}, {"url": "https://shakedzy.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://shakedzy.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Shaked Zychlinski"}, {"url": "https://shakedzy.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.3K Followers"}, {"url": "http://shakedzy.xyz", "anchor_text": "shakedzy.xyz"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F43218078e688&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fqrash-course-ii-from-q-learning-to-gradient-policy-actor-critic-in-12-minutes-8e8b47129c8c&user=Shaked+Zychlinski&userId=43218078e688&source=post_page-43218078e688--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F4123ceb9438d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fqrash-course-ii-from-q-learning-to-gradient-policy-actor-critic-in-12-minutes-8e8b47129c8c&newsletterV3=43218078e688&newsletterV3Id=4123ceb9438d&user=Shaked+Zychlinski&userId=43218078e688&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}