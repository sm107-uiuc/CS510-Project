{"url": "https://towardsdatascience.com/deep-reinforcement-learning-for-automated-stock-trading-f1dad0126a02", "time": 1683013007.460565, "path": "towardsdatascience.com/deep-reinforcement-learning-for-automated-stock-trading-f1dad0126a02/", "webpage": {"metadata": {"title": "Deep Reinforcement Learning for Automated Stock Trading | by Bruce Yang ByFinTech | Towards Data Science", "h1": "Deep Reinforcement Learning for Automated Stock Trading", "description": "Note from Towards Data Science\u2019s editors: While we allow independent authors to publish articles in accordance with our rules and guidelines, we do not endorse each author\u2019s contribution. You should\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/questions-96667b06af5", "anchor_text": "rules and guidelines", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/readers-terms-b5d780a700a4", "anchor_text": "Reader Terms", "paragraph_index": 0}, {"url": "https://ai-finance.org/conference-program/", "anchor_text": "ICAIF 2020", "paragraph_index": 1}, {"url": "https://github.com/AI4Finance-Foundation/FinRL-Trading/tree/master/old_repo_ensemble_strategy", "anchor_text": "Github", "paragraph_index": 2}, {"url": "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3690996", "anchor_text": "SSRN", "paragraph_index": 3}, {"url": "https://towardsdatascience.com/finrl-for-quantitative-finance-tutorial-for-single-stock-trading-37d6d7c30aac", "anchor_text": "FinRL for Quantitative Finance: Tutorial for Single Stock Trading", "paragraph_index": 7}, {"url": "https://towardsdatascience.com/finrl-for-quantitative-finance-tutorial-for-multiple-stock-trading-7b00763b7530", "anchor_text": "FinRL for Quantitative Finance: Tutorial for Multiple Stock Trading", "paragraph_index": 8}, {"url": "https://towardsdatascience.com/finrl-for-quantitative-finance-tutorial-for-portfolio-allocation-9b417660c7cd", "anchor_text": "FinRL for Quantitative Finance: Tutorial for Portfolio Allocation", "paragraph_index": 9}, {"url": "https://wrds-web.wharton.upenn.edu/wrds/ds/compd/secd/index.cfm?navId=83", "anchor_text": "Wharton Research Data Services (WRDS)", "paragraph_index": 26}, {"url": "http://web.stanford.edu/~wfsharpe/art/sr/sr.htm", "anchor_text": "Sharpe ratio", "paragraph_index": 43}, {"url": "https://github.com/quantopian/pyfolio", "anchor_text": "pyfolio", "paragraph_index": 47}, {"url": "https://arxiv.org/abs/1602.01783", "anchor_text": "https://arxiv.org/abs/1602.01783", "paragraph_index": 48}, {"url": "https://arxiv.org/abs/1509.02971", "anchor_text": "https://arxiv.org/abs/1509.02971", "paragraph_index": 49}, {"url": "https://arxiv.org/abs/1502.05477", "anchor_text": "https://arxiv.org/abs/1502.05477", "paragraph_index": 50}, {"url": "https://github.com/BruceYanghy", "anchor_text": "https://github.com/BruceYanghy", "paragraph_index": 52}], "all_paragraphs": ["Note from Towards Data Science\u2019s editors: While we allow independent authors to publish articles in accordance with our rules and guidelines, we do not endorse each author\u2019s contribution. You should not rely on an author\u2019s works without seeking professional advice. See our Reader Terms for details.", "This blog is based on our paper: Deep Reinforcement Learning for Automated Stock Trading: An Ensemble Strategy, presented at ICAIF 2020: ACM International Conference on AI in Finance.", "Our codes are available on Github.", "Our paper is available on SSRN.", "If you want to cite our paper, the reference format is as follows:", "Hongyang Yang, Xiao-Yang Liu, Shan Zhong, and Anwar Walid. 2020. Deep Reinforcement Learning for Automated Stock Trading: An Ensemble Strategy. In ICAIF \u201920: ACM International Conference on AI in Finance, Oct. 15\u201316, 2020, Manhattan, NY. ACM, New York, NY, USA.", "A most recent DRL library for Automated Trading-FinRL can be found here:", "FinRL for Quantitative Finance: Tutorial for Single Stock Trading", "FinRL for Quantitative Finance: Tutorial for Multiple Stock Trading", "FinRL for Quantitative Finance: Tutorial for Portfolio Allocation", "ElegantRL supports state-of-the-art DRL algorithms and provides user-friendly tutorials in Jupyter notebooks. The core codes <1,000 lines, using PyTorch, OpenAI Gym, and NumPy.", "One can hardly overestimate the crucial role stock trading strategies play in investment.", "Profitable automated stock trading strategy is vital to investment companies and hedge funds. It is applied to optimize capital allocation and maximize investment performance, such as expected return. Return maximization can be based on the estimates of potential return and risk. However, it is challenging to design a profitable strategy in a complex and dynamic stock market.", "Every player wants a winning strategy. Needless to say, a profitable strategy in such a complex and dynamic stock market is not easy to design.", "Yet, we are to reveal a deep reinforcement learning scheme that automatically learns a stock trading strategy by maximizing investment return.", "Our Solution: Ensemble Deep Reinforcement Learning Trading StrategyThis strategy includes three actor-critic based algorithms: Proximal Policy Optimization (PPO), Advantage Actor-Critic (A2C), and Deep Deterministic Policy Gradient (DDPG).It combines the best features of the three algorithms, thereby robustly adjusting to different market conditions.", "The performance of the trading agent with different reinforcement learning algorithms is evaluated using the Sharpe ratio and compared with both the Dow Jones Industrial Average index and the traditional min-variance portfolio allocation strategy.", "Existing works are not satisfactory. Deep Reinforcement Learning approach has many advantages.", "Reinforcement Learning is one of three approaches of machine learning techniques, and it trains an agent to interact with the environment by sequentially receiving states and rewards from the environment and taking actions to reach better rewards.", "Deep Reinforcement Learning approximates the Q value with a neural network. Using a neural network as a function approximator would allow reinforcement learning to be applied to large data.", "Bellman Equation is the guiding principle to designing reinforcement learning algorithms.", "Markov Decision Process (MDP) is used to model the environment.", "Recent applications of deep reinforcement learning in financial markets consider discrete or continuous state and action spaces and employ one of these learning approaches: a critic-only approach, an actor-only approach, or and actor-critic approach.", "1. Critic-only approach: the critic-only learning approach, which is the most common, solves a discrete action space problem using, for example, Q-learning, Deep Q-learning (DQN), and its improvements, and trains an agent on a single stock or asset. The idea of the critic-only approach is to use a Q-value function to learn the optimal action-selection policy that maximizes the expected future reward given the current state. Instead of calculating a state-action value table, DQN minimizes the mean squared error between the target Q-values and uses a neural network to perform function approximation. The major limitation of the critic-only approach is that it only works with discrete and finite state and action spaces, which is not practical for a large portfolio of stocks since the prices are of course continuous.", "2. Actor-only approach: The idea here is that the agent directly learns the optimal policy itself. Instead of having a neural network to learn the Q-value, the neural network learns the policy. The policy is a probability distribution that is essentially a strategy for a given state, namely the likelihood to take an allowed action. The actor-only approach can handle continuous action space environments.", "3. Actor-Critic approach: The actor-critic approach has been recently applied in finance. The idea is to simultaneously update the actor-network that represents the policy and the critic network that represents the value function. The critic estimates the value function, while the actor updates the policy probability distribution guided by the critic with policy gradients. Over time, the actor learns to take better actions and the critic gets better at evaluating those actions. The actor-critic approach has proven to be able to learn and adapt to large and complex environments and has been used to play popular video games, such as Doom. Thus, the actor-critic approach fits well in trading with a large stock portfolio.", "We track and select the Dow Jones 30 stocks (at 2016/01/01) and use historical daily data from 01/01/2009 to 05/08/2020 to train the agent and test the performance. The dataset is downloaded from Compustat database accessed through Wharton Research Data Services (WRDS).", "The whole dataset is split in the following figure. Data from 01/01/2009 to 12/31/2014 is used for training, and the data from 10/01/2015 to 12/31/2015 is used for validation and tuning of parameters. Finally, we test our agent\u2019s performance on trading data, which is the unseen out-of-sample data from 01/01/2016 to 05/08/2020. To better exploit the trading data, we continue training our agent while in the trading stage, since this will help the agent to better adapt to the market dynamics.", "\u2022 State \ud835\udc94 = [\ud835\udc91, \ud835\udc89, \ud835\udc4f]: a vector that includes stock prices \ud835\udc91 \u2208 R+^D, the stock shares \ud835\udc89 \u2208 Z+^D, and the remaining balance \ud835\udc4f \u2208 R+, where \ud835\udc37 denotes the number of stocks and Z+ denotes non-negative integers.", "\u2022 Action \ud835\udc82: a vector of actions over \ud835\udc37 stocks. The allowed actions on each stock include selling, buying, or holding, which result in decreasing, increasing, and no change of the stock shares \ud835\udc89, respectively.", "\u2022 Reward \ud835\udc5f(\ud835\udc60,\ud835\udc4e,\ud835\udc60\u2032):the direct reward of taking action \ud835\udc4e at state \ud835\udc60 and arriving at the new state \ud835\udc60\u2032.", "\u2022 Policy \ud835\udf0b (\ud835\udc60): the trading strategy at state \ud835\udc60, which is the probability distribution of actions at state \ud835\udc60.", "\u2022 Q-value \ud835\udc44\ud835\udf0b (\ud835\udc60, \ud835\udc4e): the expected reward of taking action \ud835\udc4e at state \ud835\udc60 following policy \ud835\udf0b .", "The state transition of our stock trading process is shown in the following figure. At each state, one of three possible actions is taken on stock \ud835\udc51 (\ud835\udc51 = 1, \u2026, \ud835\udc37) in the portfolio.", "At time \ud835\udc61 an action is taken and the stock prices update at \ud835\udc61+1, accordingly the portfolio values may change from \u201cportfolio value 0\u201d to \u201cportfolio value 1\u201d, \u201cportfolio value 2\u201d, or \u201cportfolio value 3\u201d, respectively, as illustrated in Figure 2. Note that the portfolio value is \ud835\udc91\ud835\udc7b \ud835\udc89 + \ud835\udc4f.", "We define our reward function as the change of the portfolio value when action \ud835\udc4e is taken at state \ud835\udc60 and arriving at new state \ud835\udc60 + 1.", "The goal is to design a trading strategy that maximizes the change of the portfolio value \ud835\udc5f(\ud835\udc60\ud835\udc61,\ud835\udc4e\ud835\udc61,\ud835\udc60\ud835\udc61+1) in the dynamic environment, and we employ the deep reinforcement learning method to solve this problem.", "State Space: We use a 181-dimensional vector (30 stocks * 6 + 1) consists of seven parts of information to represent the state space of multiple stocks trading environment", "A2C is a typical actor-critic algorithm which we use as a component in the ensemble method. A2C is introduced to improve the policy gradient updates. A2C utilizes an advantage function to reduce the variance of the policy gradient. Instead of only estimates the value function, the critic network estimates the advantage function. Thus, the evaluation of an action not only depends on how good the action is, but also considers how much better it can be. So that it reduces the high variance of the policy networks and makes the model more robust.", "A2C uses copies of the same agent working in parallel to update gradients with different data samples. Each agent works independently to interact with the same environment. After all of the parallel agents finish calculating their gradients, A2C uses a coordinator to pass the average gradients over all the agents to a global network. So that the global network can update the actor and the critic network. The presence of a global network increases the diversity of training data. The synchronized gradient update is more cost-effective, faster and works better with large batch sizes. A2C is a great model for stock trading because of its stability.", "DDPG is an actor-critic based algorithm which we use as a component in the ensemble strategy to maximize the investment return. DDPG combines the frameworks of both Q-learning and policy gradient, and uses neural networks as function approximators. In contrast with DQN that learns indirectly through Q-values tables and suffers the curse of dimensionality problem, DDPG learns directly from the observations through policy gradient. It is proposed to deterministically map states to actions to better fit the continuous action space environment.", "We explore and use PPO as a component in the ensemble method. PPO is introduced to control the policy gradient update and ensure that the new policy will not be too different from the older one. PPO tries to simplify the objective of Trust Region Policy Optimization (TRPO) by introducing a clipping term to the objective function.", "The objective function of PPO takes the minimum of the clipped and normal objective. PPO discourages large policy change move outside of the clipped interval. Therefore, PPO improves the stability of the policy networks training by restricting the policy update at each training step. We select PPO for stock trading because it is stable, fast, and simpler to implement and tune.", "Our purpose is to create a highly robust trading strategy. So we use an ensemble method to automatically select the best performing agent among PPO, A2C, and DDPG to trade based on the Sharpe ratio. The ensemble process is described as follows:", "Step 1. We use a growing window of \ud835\udc5b months to retrain our three agents concurrently. In this paper, we retrain our three agents at every three months.", "Step 2. We validate all three agents by using a 3-month validation rolling window followed by training to pick the best performing agent which has the highest Sharpe ratio. We also adjust risk-aversion by using turbulence index in our validation stage.", "Step 3. After validation, we only use the best model with the highest Sharpe ratio to predict and trade for the next quarter.", "We use Quantopian\u2019s pyfolio to do the backtesting. The charts look pretty good, and it takes literally one line of code to implement it. You just need to convert everything into daily returns.", "A2C:Volodymyr Mnih, Adria\u0300 Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. 2016. Asynchronous methods for deep reinforcement learning. The 33rd International Conference on Machine Learning (02 2016). https://arxiv.org/abs/1602.01783", "DDPG:Timothy Lillicrap, Jonathan Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. 2015. Continuous control with deep reinforcement learning. International Conference on Learning Representations (ICLR) 2016 (09 2015). https://arxiv.org/abs/1509.02971", "PPO:John Schulman, Sergey Levine, Philipp Moritz, Michael Jordan, and Pieter Abbeel. 2015. Trust region policy optimization. In The 31st International Conference on Machine Learning. https://arxiv.org/abs/1502.05477", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Chief Data Scientist, Educator | Quant, Fraud Detection | Guest Lecturer at Columbia University, Author of AI4Finance-Foundation https://github.com/BruceYanghy"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Ff1dad0126a02&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-reinforcement-learning-for-automated-stock-trading-f1dad0126a02&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-reinforcement-learning-for-automated-stock-trading-f1dad0126a02&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-reinforcement-learning-for-automated-stock-trading-f1dad0126a02&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-reinforcement-learning-for-automated-stock-trading-f1dad0126a02&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----f1dad0126a02--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f1dad0126a02--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://byfintech.medium.com/?source=post_page-----f1dad0126a02--------------------------------", "anchor_text": ""}, {"url": "https://byfintech.medium.com/?source=post_page-----f1dad0126a02--------------------------------", "anchor_text": "Bruce Yang ByFinTech"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa878fc45fb3f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-reinforcement-learning-for-automated-stock-trading-f1dad0126a02&user=Bruce+Yang+ByFinTech&userId=a878fc45fb3f&source=post_page-a878fc45fb3f----f1dad0126a02---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff1dad0126a02&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-reinforcement-learning-for-automated-stock-trading-f1dad0126a02&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff1dad0126a02&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-reinforcement-learning-for-automated-stock-trading-f1dad0126a02&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@chrisliverani", "anchor_text": "Chris"}, {"url": "https://unsplash.com/photos/dBI_My696Rk", "anchor_text": "Unsplash"}, {"url": "https://towardsdatascience.com/questions-96667b06af5", "anchor_text": "rules and guidelines"}, {"url": "https://towardsdatascience.com/readers-terms-b5d780a700a4", "anchor_text": "Reader Terms"}, {"url": "https://ai-finance.org/conference-program/", "anchor_text": "ICAIF 2020"}, {"url": "https://github.com/AI4Finance-Foundation/FinRL-Trading/tree/master/old_repo_ensemble_strategy", "anchor_text": "Github"}, {"url": "https://github.com/AI4Finance-Foundation/FinRL-Trading/tree/master/old_repo_ensemble_strategy", "anchor_text": "FinRL-Trading/old_repo_ensemble_strategy at master \u00b7 AI4Finance-Foundation/FinRL-TradingThis repository provides codes for ICAIF 2020 paper This ensemble strategy is reimplemented in a Jupiter Notebook at\u2026github.com"}, {"url": "https://github.com/AI4Finance-Foundation/FinRL/blob/master/examples/FinRL_Ensemble_StockTrading_ICAIF_2020.ipynb", "anchor_text": "FinRL/FinRL_Ensemble_StockTrading_ICAIF_2020.ipynb at master \u00b7 AI4Finance-Foundation/FinRLYou can't perform that action at this time. You signed in with another tab or window. You signed out in another tab or\u2026github.com"}, {"url": "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3690996", "anchor_text": "SSRN"}, {"url": "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3690996", "anchor_text": "Deep Reinforcement Learning for Automated Stock Trading: An Ensemble StrategyDate Written: September 11, 2020 Stock trading strategies play a critical role in investment. However, it is\u2026papers.ssrn.com"}, {"url": "https://github.com/AI4Finance-Foundation/FinRL", "anchor_text": "GitHub - AI4Finance-Foundation/FinRL: A Deep Reinforcement Learning Framework for Automated Trading\u2026News: we plan to share our codes for both paper trading and live trading. Please actively share your interests with our\u2026github.com"}, {"url": "https://towardsdatascience.com/finrl-for-quantitative-finance-tutorial-for-single-stock-trading-37d6d7c30aac", "anchor_text": "FinRL for Quantitative Finance: Tutorial for Single Stock Trading"}, {"url": "https://towardsdatascience.com/finrl-for-quantitative-finance-tutorial-for-multiple-stock-trading-7b00763b7530", "anchor_text": "FinRL for Quantitative Finance: Tutorial for Multiple Stock Trading"}, {"url": "https://towardsdatascience.com/finrl-for-quantitative-finance-tutorial-for-portfolio-allocation-9b417660c7cd", "anchor_text": "FinRL for Quantitative Finance: Tutorial for Portfolio Allocation"}, {"url": "https://towardsdatascience.com/elegantrl-a-lightweight-and-stable-deep-reinforcement-learning-library-95cef5f3460b", "anchor_text": "ElegantRL: A Lightweight and Stable Deep Reinforcement Learning LibraryMastering deep reinforcement learning in one day.towardsdatascience.com"}, {"url": "https://unsplash.com/@by_syeoni", "anchor_text": "Suhyeon"}, {"url": "https://unsplash.com/photos/H5bGh1SEzrI", "anchor_text": "Unsplash"}, {"url": "https://deepmind.com/blog/article/alphago-zero-starting-scratch", "anchor_text": "DeepMind\u2019s AlphaGo"}, {"url": "https://unsplash.com/@ikukevk", "anchor_text": "Kevin"}, {"url": "https://unsplash.com/photos/w7ZyuGYNpRQ", "anchor_text": "Unsplash"}, {"url": "https://econpapers.repec.org/paper/zbwiwqwdp/122018.htm", "anchor_text": "Related works"}, {"url": "https://unsplash.com/@markusspiske", "anchor_text": "Markus"}, {"url": "https://unsplash.com/photos/5gGcn2PRrtc", "anchor_text": "Unsplash"}, {"url": "https://wrds-web.wharton.upenn.edu/wrds/ds/compd/secd/index.cfm?navId=83", "anchor_text": "Wharton Research Data Services (WRDS)"}, {"url": "https://www.top1000funds.com/wp-content/uploads/2010/11/FAJskulls.pdf", "anchor_text": "turbulence index"}, {"url": "https://unsplash.com/@isaacmsmith", "anchor_text": "Isaac"}, {"url": "https://unsplash.com/photos/6EnTPvPPL6I", "anchor_text": "Unsplash"}, {"url": "https://arxiv.org/abs/1602.01783", "anchor_text": "A2C"}, {"url": "https://arxiv.org/abs/1509.02971", "anchor_text": "DDPG"}, {"url": "https://arxiv.org/abs/1707.06347", "anchor_text": "PPO"}, {"url": "http://web.stanford.edu/~wfsharpe/art/sr/sr.htm", "anchor_text": "Sharpe ratio"}, {"url": "https://github.com/quantopian/pyfolio", "anchor_text": "pyfolio"}, {"url": "https://arxiv.org/abs/1602.01783", "anchor_text": "https://arxiv.org/abs/1602.01783"}, {"url": "https://arxiv.org/abs/1509.02971", "anchor_text": "https://arxiv.org/abs/1509.02971"}, {"url": "https://arxiv.org/abs/1502.05477", "anchor_text": "https://arxiv.org/abs/1502.05477"}, {"url": "https://arxiv.org/abs/1707.06347", "anchor_text": "https://arxiv.org/abs/1707.06347"}, {"url": "https://medium.com/tag/deep-reinforcement?source=post_page-----f1dad0126a02---------------deep_reinforcement-----------------", "anchor_text": "Deep Reinforcement"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----f1dad0126a02---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----f1dad0126a02---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/stock-trading?source=post_page-----f1dad0126a02---------------stock_trading-----------------", "anchor_text": "Stock Trading"}, {"url": "https://medium.com/tag/markov-decision-process?source=post_page-----f1dad0126a02---------------markov_decision_process-----------------", "anchor_text": "Markov Decision Process"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff1dad0126a02&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-reinforcement-learning-for-automated-stock-trading-f1dad0126a02&user=Bruce+Yang+ByFinTech&userId=a878fc45fb3f&source=-----f1dad0126a02---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff1dad0126a02&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-reinforcement-learning-for-automated-stock-trading-f1dad0126a02&user=Bruce+Yang+ByFinTech&userId=a878fc45fb3f&source=-----f1dad0126a02---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff1dad0126a02&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-reinforcement-learning-for-automated-stock-trading-f1dad0126a02&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f1dad0126a02--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Ff1dad0126a02&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-reinforcement-learning-for-automated-stock-trading-f1dad0126a02&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----f1dad0126a02---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----f1dad0126a02--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----f1dad0126a02--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----f1dad0126a02--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----f1dad0126a02--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----f1dad0126a02--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----f1dad0126a02--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----f1dad0126a02--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----f1dad0126a02--------------------------------", "anchor_text": ""}, {"url": "https://byfintech.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://byfintech.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Bruce Yang ByFinTech"}, {"url": "https://byfintech.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.3K Followers"}, {"url": "https://github.com/BruceYanghy", "anchor_text": "https://github.com/BruceYanghy"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa878fc45fb3f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-reinforcement-learning-for-automated-stock-trading-f1dad0126a02&user=Bruce+Yang+ByFinTech&userId=a878fc45fb3f&source=post_page-a878fc45fb3f--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F6ca58a53196a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-reinforcement-learning-for-automated-stock-trading-f1dad0126a02&newsletterV3=a878fc45fb3f&newsletterV3Id=6ca58a53196a&user=Bruce+Yang+ByFinTech&userId=a878fc45fb3f&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}