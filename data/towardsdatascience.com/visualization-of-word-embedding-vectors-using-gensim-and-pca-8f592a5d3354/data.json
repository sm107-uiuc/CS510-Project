{"url": "https://towardsdatascience.com/visualization-of-word-embedding-vectors-using-gensim-and-pca-8f592a5d3354", "time": 1683007269.366585, "path": "towardsdatascience.com/visualization-of-word-embedding-vectors-using-gensim-and-pca-8f592a5d3354/", "webpage": {"metadata": {"title": "Visualization of Word Embedding Vectors using Gensim and PCA | by Saket Thavanani | Towards Data Science", "h1": "Visualization of Word Embedding Vectors using Gensim and PCA", "description": "There are certain ways to extract features out of any text data for feeding it into the Machine Learning model. The most basic techniques are\u2014 Count Vectorizer, Tf-Idf. The major drawback with these\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/@saketthavananilindan", "anchor_text": "Medium", "paragraph_index": 27}], "all_paragraphs": ["There are certain ways to extract features out of any text data for feeding it into the Machine Learning model. The most basic techniques are\u2014 Count Vectorizer, Tf-Idf. The major drawback with these techniques is that they do not capture the semantic meaning of the text data and each word is assigned to a new dimension just like one-hot encoded vectors. Hence, if the corpus size is pretty huge then the dimensions can go up to millions of words which is infeasible and can result in a poor model. We can verify this using a simple cosine similarity calculation. Suppose we have two sentences each comprising of 1 word \u201cgood\u201d and \u201cnice\u201d. Here we know that these two words share somewhat similarity with each other but when we compute the cosine similarity using count vectorizer vectors it comes out to be zero.", "where U and V are vector representation of two sentences-[1,0] and [0,1]", "This is the technique that maps words to real number vectors, at the same time capturing something about the meaning of the text. It says that if two words have similar meaning they will lie close to each other in the dense space. Like the two words, we used earlier \u201c good\u201d and \u201cnice\u201d will lie close to each other in the embedded space. In this article, we will be visualizing these kinds of words in the dense space using the Word2Vec algorithm from gensim library. The Word2Vec contains two models for training Skip-Gram model and continuous bag of words(CBOW).", "For people who want to get familiar with the basic concepts of word embedding, they should first review the articles given below. In this article, we will focus mostly on python implementation and visualization.", "I have taken a short paragraph of text from Wikipedia\u2019s definition of word embedding. We will be representing the words of this text in the dense space.", "We will install gensim library and import Word2Vec module from it. Further we will import NLTK and use it for sentence tokenization.", "We will be tokenizing the sentences with the help of NLTK tokenizer.", "This model taken in sentences in the tokenized format as we obtained in the previous part will be directly fed into it. Apart from that this class has various parameters- size, window, min_count, sg", "Size=This decided the number of dimensions we want for the representation of words. (Default=100)", "Window=This is the maximum distance between a central word and words around the central word.", "min_count=This is the minimum count of words to consider when training the model; words with an occurrence less than this number will be ignored.", "sg= This specifies the training algorithm CBOW (0), Skip-Gram (1)", "For our setting, since the text is less we will use min_count=1 to consider all the words. We will use window=50 with the skip-gram model so sg=1.", "We can see the vector representation of the word \u201cthe\u201d is obtained by using model [\u201cthe\u201d]. We can see the vector representation of the word \u201cthe\u201d is in 50 dimensions.", "We can print the learned vocabulary as follows:", "Further, we will store all the word vectors in the data frame with 50 dimensions and use this data frame for PCA.", "We will be implementing PCA using the numpy library. Steps involved in PCA are as follows-", "1-Standardize the dataset and compute the correlation matrix.", "2-Calculate the eigenvalues and eigenvectors using eigen decomposition.", "3-Sort the eigenvalues and their coresponding eigen vectors.", "4-Pick top two eigenvalues an create a matrix of eigen vectors.", "5-Transform the original data using dot product with these new eigen vectors.", "In this case, we can ignore the standardization step, since the data is in same unit.", "The new dataset obtained looks like this:", "We will be using matplotlib to visualize the words in the dense space.", "In this article, we learned how to transform text data into feature vectors using Word2Vec technique. Further, we learned how to represent these word vectors in a two-dimensional space using matplotlib and PCA. In my next article, We will discuss the mathematics behind word embedding techniques.", "If you like my work and want to support me.", "1-The BEST way to support me is by following me on Medium.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Masters Student | University of Toronto | IIT Kharagpur | Data Science, Machine Learning and Deep Learning Enthusiast"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F8f592a5d3354&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvisualization-of-word-embedding-vectors-using-gensim-and-pca-8f592a5d3354&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvisualization-of-word-embedding-vectors-using-gensim-and-pca-8f592a5d3354&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvisualization-of-word-embedding-vectors-using-gensim-and-pca-8f592a5d3354&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvisualization-of-word-embedding-vectors-using-gensim-and-pca-8f592a5d3354&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----8f592a5d3354--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8f592a5d3354--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@saketuoft?source=post_page-----8f592a5d3354--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@saketuoft?source=post_page-----8f592a5d3354--------------------------------", "anchor_text": "Saket Thavanani"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F102f526f83de&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvisualization-of-word-embedding-vectors-using-gensim-and-pca-8f592a5d3354&user=Saket+Thavanani&userId=102f526f83de&source=post_page-102f526f83de----8f592a5d3354---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8f592a5d3354&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvisualization-of-word-embedding-vectors-using-gensim-and-pca-8f592a5d3354&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8f592a5d3354&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvisualization-of-word-embedding-vectors-using-gensim-and-pca-8f592a5d3354&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/introduction-to-word-embeddings-4cf857b12edc", "anchor_text": "Introduction to Word EmbeddingsWhat is a word embedding?towardsdatascience.com"}, {"url": "https://medium.com/analytics-vidhya/maths-behind-word2vec-explained-38d74f32726b", "anchor_text": "A math-first explanation of Word2VecIntroductionmedium.com"}, {"url": "https://d2l.ai/chapter_natural-language-processing-pretraining/word2vec.html", "anchor_text": "https://d2l.ai/chapter_natural-language-processing-pretraining/word2vec.html"}, {"url": "https://medium.com/@saketthavananilindan", "anchor_text": "Medium"}, {"url": "https://www.linkedin.com/in/saket-thavanani-b1a149a0/", "anchor_text": "LinkedIn"}, {"url": "https://medium.com/tag/data-science?source=post_page-----8f592a5d3354---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----8f592a5d3354---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/visualization?source=post_page-----8f592a5d3354---------------visualization-----------------", "anchor_text": "Visualization"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----8f592a5d3354---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/word-embeddings?source=post_page-----8f592a5d3354---------------word_embeddings-----------------", "anchor_text": "Word Embeddings"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8f592a5d3354&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvisualization-of-word-embedding-vectors-using-gensim-and-pca-8f592a5d3354&user=Saket+Thavanani&userId=102f526f83de&source=-----8f592a5d3354---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8f592a5d3354&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvisualization-of-word-embedding-vectors-using-gensim-and-pca-8f592a5d3354&user=Saket+Thavanani&userId=102f526f83de&source=-----8f592a5d3354---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8f592a5d3354&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvisualization-of-word-embedding-vectors-using-gensim-and-pca-8f592a5d3354&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8f592a5d3354--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F8f592a5d3354&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvisualization-of-word-embedding-vectors-using-gensim-and-pca-8f592a5d3354&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----8f592a5d3354---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----8f592a5d3354--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----8f592a5d3354--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----8f592a5d3354--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----8f592a5d3354--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----8f592a5d3354--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----8f592a5d3354--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----8f592a5d3354--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----8f592a5d3354--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@saketuoft?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@saketuoft?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Saket Thavanani"}, {"url": "https://medium.com/@saketuoft/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "396 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F102f526f83de&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvisualization-of-word-embedding-vectors-using-gensim-and-pca-8f592a5d3354&user=Saket+Thavanani&userId=102f526f83de&source=post_page-102f526f83de--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F5dae2614ed2d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvisualization-of-word-embedding-vectors-using-gensim-and-pca-8f592a5d3354&newsletterV3=102f526f83de&newsletterV3Id=5dae2614ed2d&user=Saket+Thavanani&userId=102f526f83de&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}