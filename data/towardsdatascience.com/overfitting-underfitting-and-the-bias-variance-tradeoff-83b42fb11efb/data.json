{"url": "https://towardsdatascience.com/overfitting-underfitting-and-the-bias-variance-tradeoff-83b42fb11efb", "time": 1682996299.914064, "path": "towardsdatascience.com/overfitting-underfitting-and-the-bias-variance-tradeoff-83b42fb11efb/", "webpage": {"metadata": {"title": "Overfitting, underfitting, and the bias-variance tradeoff | by Steve Klosterman | Towards Data Science", "h1": "Overfitting, underfitting, and the bias-variance tradeoff", "description": "Overfitting, underfitting, and the bias-variance tradeoff are foundational concepts in machine learning. A model is overfit if performance on the training data, used to fit the model, is\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.amazon.com/Data-Science-Projects-Python-scikit-learn/dp/1838551026", "anchor_text": "book", "paragraph_index": 27}, {"url": "https://www.amazon.com/Data-Science-Projects-Python-valuable/dp/1838551026/", "anchor_text": "Amazon", "paragraph_index": 41}, {"url": "http://www.steveklosterman.com/book/", "anchor_text": "here", "paragraph_index": 41}, {"url": "https://ocw.mit.edu/courses/sloan-school-of-management/15-097-prediction-machine-learning-and-statistics-spring-2012/lecture-notes/MIT15_097S12_lec04.pdf", "anchor_text": "this document", "paragraph_index": 43}, {"url": "https://web.stanford.edu/~hastie/ElemStatLearn/", "anchor_text": "The Elements of Statistical Learning", "paragraph_index": 43}, {"url": "https://www.coursera.org/learn/machine-learning/lecture/XcNcz/data-for-machine-learning", "anchor_text": "Machine Learning", "paragraph_index": 44}, {"url": "https://www.steveklosterman.com/", "anchor_text": "https://www.steveklosterman.com/", "paragraph_index": 47}], "all_paragraphs": ["Overfitting, underfitting, and the bias-variance tradeoff are foundational concepts in machine learning. A model is overfit if performance on the training data, used to fit the model, is substantially better than performance on a test set, held out from the model training process. For example, the prediction error of the training data may be noticeably smaller than that of the testing data. Comparing model performance metrics between these two data sets is one of the main reasons that data are split for training and testing. This way, the model\u2019s capability for predictions with new, unseen data can be assessed.", "When a model overfits the training data, it is said to have high variance. One way to think about this is that whatever variability exists in the training data, the model has \u201clearned\u201d this very well. In fact, too well. A model with high variance is likely to have learned the noise in the training set. Noise consists of the random fluctuations, or offsets from true values, in the features (independent variables) and response (dependent variable) of the data. Noise can obscure the true relationship between features and the response variable. Virtually all real-world data are noisy.", "If there is random noise in the training set, then there is probably also random noise in the testing set. However, the specific values of the random fluctuations will be different than those of the training set, because after all, the noise is random. The model cannot anticipate the fluctuations in the new, unseen data of the testing set. This why testing performance of an overfit model is lower than training performance.", "Overfitting is more likely in the following circumstances:", "At the opposite end of the spectrum, if a model is not fitting the training data very well, this is known as underfitting, and the model is said to have high bias. In this case, the model may not be complex enough, in terms of the features or the type of model being used.", "Let\u2019s examine concrete examples of underfitting, overfitting, and the ideal that sits in between, by fitting polynomial models to synthetic data in Python. The code below can be run in a Jupyter notebook to produce the results and plots shown here.", "First, we create the synthetic data. We:", "Then we make a scatter plot of the data.", "This looks like the shape of a parabola, as expected for a quadratic transformation of x. However we can see the noise in the fact that not all the points appear that they would lie perfectly along a parabola.", "In our synthetic example, we know the data generating process: the response variable y is a quadratic transformation of the feature x. In general, when building machine learning models, the data generating process is not known. Instead, several candidate features are proposed, a model is proposed, and an exploration is made of how well these features and this model can explain the data.", "In this case, we would likely plot the data, observe the apparent quadratic relationship, and use a quadratic model. However, for the sake of illustration of an underfit model, what does a linear model for these data look like?", "We can fit a polynomial model of degree 1, in other words a linear model, with numpy\u2019s polyfit:", "This has produced the slope and intercept of the line of best fit for these data. Let\u2019s plot it and see what it looks like. We can calculate the linear transformation of the feature using the slope and intercept we got from fitting the linear model, again using numpy, this time the polyval function. We'll label this the \"Underfit model\".", "Doesn\u2019t look like a very good fit, does it!", "Now let\u2019s imagine we have many features available, that are polynomial transformations of x, specifically x\u00b2, x\u00b3,\u2026 x\u00b9\u2075. This would be a large amount of features relative to the amount of samples (20) that we have. Again, in \u201creal life\u201d, we probably wouldn\u2019t consider a model with all these features, since by observation we can see that a quadratic model, or 2nd degree polynomial, would likely be sufficient. However, identifying the ideal features isn\u2019t always so straightforward. Our illustrative example serves to show what happens when we are very clearly using too many features.", "Let\u2019s make a 15 degree polynomial fit:", "These are the coefficients of all the powers of from 1 through 15 in this model, and the intercept. Notice the widely different scales of the coefficients: some are close to zero, while others are fairly large in terms of magnitude (absolute value). Let\u2019s plot this model as well. First we generate a large number of evenly spaced points over our range of x values, so we can see how this model looks not only for the particular values used for model training, but also for values in between.", "This is a classic case of overfitting. The overfit model passes nearly perfectly through all the training data. However it\u2019s easy to see that for values in between, the overfit model does not look like a realistic representation of the data generating process. Rather, the overfit model has become tuned to the noise of the training data. This matches the definition of high variance given above.", "In the last graph, you can see another definition of high variance: a small change in the input x can result in a large change in the output y. This relationship between changes in inputs and outputs is an additional way that people talk about variance of machine learning models.", "You can also imagine that if we generated a new, larger synthetic data set, using the same quadratic function y=(\u2212x+2)(x\u22129), but added new randomly generated noise according to the same distribution we used above, then randomly sampled 20 points and fit the high-degree polynomial, the resulting model would look much different. It would pass almost perfectly through these new noisy points and the coefficients of the 15 degree polynomial would be very different to allow this to happen. Repeating this process with different samples of 20 points, would continue to result in highly variable coefficient estimates. In other words, the coefficients would have high variance between samples of the data used for model training. This is yet another definition of a model with high variance.", "With our synthetic data, since in this case we know the data generating process, we can see how a 2nd degree polynomial fit looks in comparison with the underfit and overfit models.", "That\u2019s more like it. But what do we do in the real world, when we\u2019re not using made-up data, and we don\u2019t know the data generating process? There are a number of machine learning techniques to deal with overfitting. One of the most popular is regularization.", "In order to show how regularization works to reduce overfitting, we\u2019ll use the scikit-learn package. First, we need to create polynomial features manually. While above we simply had to tell numpy to fit a 15 degree polynomial to the data, here we need to manually create the features x\u00b2, x\u00b3,\u2026 x\u00b9\u2075 and then fit a linear model to find their coefficients. Scikit-learn makes creating polynomial features easy with PolynomialFeatures. We just say we want 15 degrees worth of polynomial features, without a bias feature (intercept), then pass our array reshaped as a column.", "We get back 15 columns, where the first column is x, the second x\u00b2, etc. Now we need to determine coefficients for these polynomial features. Above, we did this by using numpy to find the coefficients that provided the best fit to the training data. However, we saw this led to an overfit model. Here, we will pass these data to a routine for ridge regression, which is a kind of regularized regression. Without going too far in to details here, regularized regression works by finding the coefficients resulting in the best fit to the data while also limiting the magnitude of the coefficients.", "The effect of this is to provide a slightly worse fit to the data, in other words a model with higher bias. However, the goal is to avoid fitting the random noise, thus eliminating the high variance issue. Therefore, we are hoping to trade some variance for some bias, to obtain a model of the signal and not the noise.", "We will use the Ridge class from scikit-learn to do a ridge regression.", "There are many options to set when instantiating the Ridge class. An important one is alpha. This controls how much regularization is applied; in other words, how strongly the coefficient magnitudes are penalized, and kept close to zero. We will use a value of alpha I already found, just to illustrate the effects of regularization here. In general, the procedure for selecting alpha is to systematically evaluate a range of values, by either examining model performance on a validation set or by using a cross-validation procedure, to determine which one would be expected to provide the best performance on the unseen test set. alpha is a model hyperparameter and this would be the process of hyperparameter tuning.", "The other options we specified for Ridge indicate that we'd like to fit an intercept (because we didn\u2019t include one when generating PolynomialFeatures), normalize the features to the same scale before model fitting, which is necessary since the coefficients will all be penalized in the same way, and a few others. I'm glossing over these details here, although you can consult the scikit-learn documentation, as while as my book, for further information on regularization as well as hyperparameter tuning.", "Now we proceed to fit the ridge regression using the polynomial features and the response variable.", "What do the values of the fitted coefficients look like, in comparison to those found above by fitting the polynomial with numpy?", "We can see that the coefficient values from the regularized regression are relatively small in magnitude, compared to those from the polynomial fit. This is how regularization works: by \u201cshrinking\u201d coefficient values toward zero. For this reason regularization may also be referred to as shrinkage.", "Let\u2019s obtain the predicted values y_pred over the large number of evenly spaced points curve_x we used above, for plotting. First we need to generate the polynomial features for all these points.", "We\u2019ll remove the underfit model from our plot, and add the regularized model.", "The regularized model looks similar to the ideal model. This shows that even if we don\u2019t have knowledge of the data generating process, as we typically don\u2019t in real-world predictive modeling work, we can still use regularization to reduce the effect of overfitting when a large number of candidate features are available.", "Note, however, that the regularized model should not be used for extrapolation. We can see that the regularized model starts to increase toward the right side of the plot. This increase should be viewed with suspicion, as there is nothing in the training data that makes it clear that this would be expected. This is an example of the general view that extrapolation of model predictions outside the range of the training data is not recommended.", "The goal of trading variance for bias is to improve model performance on unseen testing data. Let\u2019s generate some testing data, in the same way we generated the training data, to see if we achieved this goal. We repeat the process used to generate x and y=(\u2212x+2)(x\u22129)+\u03f5 above, but with a different random seed. This results in different points x over the same interval and different random noise \u03f5 from the same distribution, creating new values for the response variable y, but from the same data generating process.", "We\u2019ll also define a lambda function to measure prediction error as the root mean squared error (RMSE).", "What is the RMSE of our first model, the polynomial fit to the training data?", "How about the RMSE on the newly generated testing data?", "The testing error is vastly larger than the training error from this model, a clear sign of overfitting. How does the regularized model compare?", "While the regularized model has a bit higher training error (higher bias) than the polynomial fit, the testing error is greatly improved. This shows how the bias-variance tradeoff can be leveraged to improve model predictive capability.", "This post illustrates the concepts of overfitting, underfitting, and the bias-variance tradeoff through an illustrative example in Python and scikit-learn. It expands on a section from my book Data Science Projects with Python: A case study approach to successful data science projects using Python, pandas, and scikit-learn. For a more in-depth explanation of how regularization works, how to use cross-validation for hyperparameter selection, and hands-on practice with these and other machine learning techniques, check out the book, which you can find on Amazon, with Q&A and errata here.", "Here are a few final thoughts on bias and variance.", "Statistical definitions of bias and variance: This post has focused on the intuitive machine learning definitions of bias and variance. There are also more formal statistical definitions. See this document for a derivation of the mathematical expressions for the bias-variance decomposition of squared error, as well as The Elements of Statistical Learning by Hastie et al. for more discussion of the bias-variance decomposition and tradeoff.", "Countering high variance with more data: In his Coursera course Machine Learning, Andrew Ng states that, according to the large data rationale, training on a very large data set can be an effective way to combat overfitting. The idea is that, with enough training data, the difference between the training and testing errors should be small, which means reduced variance. There is an underlying assumption to this rationale, that the features contain sufficient information to make accurate predictions of the response variable. If not, the model will suffer from high bias (high training error), so the low variance would be a moot point.", "A little overfitting may not be a problem: Model performance on the testing set is often a bit lower than on the training set. We saw this with our regularized model above. Technically, there is at least a little bit of overfitting going on here. However, it may not matter, since the best model is usually considered to be that with the highest testing score.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data scientist, PhD, and author of Data Science Projects with Python. https://www.steveklosterman.com/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F83b42fb11efb&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foverfitting-underfitting-and-the-bias-variance-tradeoff-83b42fb11efb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foverfitting-underfitting-and-the-bias-variance-tradeoff-83b42fb11efb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foverfitting-underfitting-and-the-bias-variance-tradeoff-83b42fb11efb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foverfitting-underfitting-and-the-bias-variance-tradeoff-83b42fb11efb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----83b42fb11efb--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----83b42fb11efb--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://steve-klosterman.medium.com/?source=post_page-----83b42fb11efb--------------------------------", "anchor_text": ""}, {"url": "https://steve-klosterman.medium.com/?source=post_page-----83b42fb11efb--------------------------------", "anchor_text": "Steve Klosterman"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F20266e1c64db&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foverfitting-underfitting-and-the-bias-variance-tradeoff-83b42fb11efb&user=Steve+Klosterman&userId=20266e1c64db&source=post_page-20266e1c64db----83b42fb11efb---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F83b42fb11efb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foverfitting-underfitting-and-the-bias-variance-tradeoff-83b42fb11efb&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F83b42fb11efb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foverfitting-underfitting-and-the-bias-variance-tradeoff-83b42fb11efb&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "http://thestrategyguysite.com/strategic-business-planning/does-your-strategic-plan-fit-you/", "anchor_text": "The Strategy Guy"}, {"url": "https://www.amazon.com/Data-Science-Projects-Python-scikit-learn/dp/1838551026", "anchor_text": "book"}, {"url": "https://www.amazon.com/Data-Science-Projects-Python-valuable/dp/1838551026/", "anchor_text": "Amazon"}, {"url": "http://www.steveklosterman.com/book/", "anchor_text": "here"}, {"url": "https://ocw.mit.edu/courses/sloan-school-of-management/15-097-prediction-machine-learning-and-statistics-spring-2012/lecture-notes/MIT15_097S12_lec04.pdf", "anchor_text": "this document"}, {"url": "https://web.stanford.edu/~hastie/ElemStatLearn/", "anchor_text": "The Elements of Statistical Learning"}, {"url": "https://www.coursera.org/learn/machine-learning/lecture/XcNcz/data-for-machine-learning", "anchor_text": "Machine Learning"}, {"url": "https://www.steveklosterman.com/over-under/", "anchor_text": "https://www.steveklosterman.com"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----83b42fb11efb---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----83b42fb11efb---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/python?source=post_page-----83b42fb11efb---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/numpy?source=post_page-----83b42fb11efb---------------numpy-----------------", "anchor_text": "Numpy"}, {"url": "https://medium.com/tag/scikit-learn?source=post_page-----83b42fb11efb---------------scikit_learn-----------------", "anchor_text": "Scikit Learn"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F83b42fb11efb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foverfitting-underfitting-and-the-bias-variance-tradeoff-83b42fb11efb&user=Steve+Klosterman&userId=20266e1c64db&source=-----83b42fb11efb---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F83b42fb11efb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foverfitting-underfitting-and-the-bias-variance-tradeoff-83b42fb11efb&user=Steve+Klosterman&userId=20266e1c64db&source=-----83b42fb11efb---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F83b42fb11efb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foverfitting-underfitting-and-the-bias-variance-tradeoff-83b42fb11efb&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----83b42fb11efb--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F83b42fb11efb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foverfitting-underfitting-and-the-bias-variance-tradeoff-83b42fb11efb&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----83b42fb11efb---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----83b42fb11efb--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----83b42fb11efb--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----83b42fb11efb--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----83b42fb11efb--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----83b42fb11efb--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----83b42fb11efb--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----83b42fb11efb--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----83b42fb11efb--------------------------------", "anchor_text": ""}, {"url": "https://steve-klosterman.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://steve-klosterman.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Steve Klosterman"}, {"url": "https://steve-klosterman.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "69 Followers"}, {"url": "https://www.steveklosterman.com/", "anchor_text": "https://www.steveklosterman.com/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F20266e1c64db&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foverfitting-underfitting-and-the-bias-variance-tradeoff-83b42fb11efb&user=Steve+Klosterman&userId=20266e1c64db&source=post_page-20266e1c64db--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F20266e1c64db%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foverfitting-underfitting-and-the-bias-variance-tradeoff-83b42fb11efb&user=Steve+Klosterman&userId=20266e1c64db&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}