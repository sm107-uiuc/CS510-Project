{"url": "https://towardsdatascience.com/beyond-a-b-testing-multi-armed-bandit-experiments-1493f709f804", "time": 1682995634.136478, "path": "towardsdatascience.com/beyond-a-b-testing-multi-armed-bandit-experiments-1493f709f804/", "webpage": {"metadata": {"title": "Beyond A/B Testing: Multi-armed Bandit Experiments | by Shaw Lu | Towards Data Science", "h1": "Beyond A/B Testing: Multi-armed Bandit Experiments", "description": "A/B testing relies on classic statistical test for statistical significance. When we come up with a new product feature, we probably want to test whether it is useful before launching it to the\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/visualizing-beta-distribution-7391c18031f1", "anchor_text": "here", "paragraph_index": 4}, {"url": "https://support.google.com/analytics/answer/2846882", "anchor_text": "here", "paragraph_index": 10}], "all_paragraphs": ["A/B testing relies on classic statistical test for statistical significance. When we come up with a new product feature, we probably want to test whether it is useful before launching it to the entire user base. The test involves two groups: a treatment group (who have access to the new feature) and a control group. Then we measure a key metric for the two groups: average time on site (social network), average checkout time (e-commerce), or click through rate (online ads). The difference between the groups is tested for statistical significance.", "Classical statistical test (z-test, t-test) guarantees that false positive rate is no more than \u03b1, which is often set to 5%. This means that when there is no difference between the treatment group and control group, the test will find a statistical difference by chance 5% of the time.", "A balanced AB test would allocate equal amount of traffic to each group, until reaching sufficient sample size. However, we cannot adjust traffic allocation during the test based on what is observed. So the disadvantage of A/B testing is obvious: if the treatment group is clearly superior, we still have to spend lots of traffic on the control group, in order to obtain statistical significance.", "Whereas A/B testing is a frequentist approach, we can also conduct the test from the Bayesian way. It is understandable that once we see one treatment is clearly better, we want to add more users to that treatment right away. Multi-armed bandit experiment makes this possible in a controlled way.", "The foundation of the multi-armed bandit experiment is Bayesian updating. Each treatment (called \u201carm\u201d, see class definition below) has a probability of success, which is modeled as a Bernoulli process. The probability of success is unknown, and is modeled by a Beta distribution. As the experiment continues, each arm receives user traffic, and the Beta distribution is updated accordingly. To visualize the updating process, see my earlier post here.", "In this post, I\u2019m using the Google Analytics example of online advertisements matching. Suppose there are K arms. Each arm is an ad with click-through rate (ctr) that follows a Beta distribution. The goal of the experiment is to find the ad with the highest click through rate.", "In a nutshell, Thompson sampling is a greedy method that always chooses the arm that maximizes expected reward. In each iteration of the bandit experiment, Thompson sampling simply draws a sample ctr from each arm\u2019s Beta distribution, and assign the user to the arm with the highest ctr.", "The elegant part of bandit experiment is that Thompson sampling and Bayesian update work hand-in-hand. If one of the arms is performing well, its Beta distribution parameters are updated to remember this, and Thompson sampling will more likely draw a high ctr from this arm. Throughout the experiment, high-performing arms are rewarded with more traffic, whereas under-performing arms are punished with less traffic.", "Whereas the Beta distribution estimates the ctr, we need to know how confident we are about each estimate of ctr. If we are confident enough about the arm that currently has the highest ctr, we can end the experiment.", "The way Monte Carlo simulation works is to randomly draw samples from each of the K arms multiple times, and empirically compute how often each of the arms wins (with highest ctr). If the winning arm is beating the second arm by a large enough margin, the experiment terminates.", "Google Analytics introduces the concept of \u201cvalue remaining in experiment\u201d (more details here). In each Monte Carlo simulation, the value remaining is computed. If we choose \u03b1 = 5%, then the experiment terminates when 95% of the samples in a Monte Carlo simulation have remaining value less than 1% of the winning arm\u2019s value.", "Having defined the utility functions above, it is straightforward to put them together. For each iteration, a new user arrives. We apply Thompson sampling to select an arm and see whether user clicks. Then we update the Beta parameters of the arm, check whether we are confident enough about the winning arm to end the experiment.", "Note that I introduced a burn-in parameters. This is the minimum number of iterations that must be run before declaring a winner. The beginning of the experiment is the nosiest period, and any loser arm could get ahead by chance. The burn-in period helps prevent prematurely ending the experiment before the noise settles down.", "In reality, this also helps controls novelty effect, cold-start and other user-psychology related confounding variables. Google Analytics forces all bandit experiments to run for a minimum of 2 weeks.", "The main advantage of bandit experiment is that it terminates earlier than A/B test because it requires much smaller sample. In a two-armed experiment with click-through rate 4% and 5%, traditional A/B testing requires 11,165 in each treatment group at 95% significance level. With 100 users a day, the experiment will take 223 days. In the bandit experiment, however, simulation ended after 31 days, at the above termination criterion.", "A second advantage of bandit experiment is that the experiment is making fewer mistakes than A/B testing. A balanced A/B test would always send 50% of traffic to each group. The plot above shows that as the experiment progresses, fewer and fewer traffic was sent to the losing arm.", "Here is how a 5-armed bandit experiment ran in simulation. We see that the red arm (with ctr 4.4%) was mistaken as the winning arm during the first 150 iterations, and we were diverting as much as 80% traffic to the losing arm. But the true blue arm (ctr 4.8%) caught up and emerged as the true winner.", "There is no free lunch, and the convenience of smaller sample size comes at a cost of a larger false positive rate. Although I had used \u03b1 empirically as the false positive rate to terminate experiment, the false positive rate is higher than \u03b1, after repeating the simulation many times.", "Empirically, an \u03b1 of 5% finds the winning arm about 91% of the time, instead of 95%. The smaller the \u03b1 we set, the larger the sample size we need (shown in red), which is consistent with how A/B testing behaves.", "As it turns out, there is no definite winner, and it is important for product manager, data scientist and practitioners to understand the strength and weakness of the two methods before making a choice. Multi-armed bandit test is preferred during the following situations:", "A clear limitation of multi-armed bandit test is that each arm must be modeled by a Beta distribution, meaning that every time you try an arm, it results in success or failure. This is good for modeling click through rate and conversion rate, but if you are testing which checkout process is faster, you have to do t-test on difference of means.", "On the other hand, A/B testing is a better option when the company has large enough user base, when it\u2019s important to control for type I error (false positives), and when there are few enough variants that we can test each one of them against the control group one at a time.", "The following blogs covers topics relevant to AB testing, and more in-depth review of key concepts mentioned in this article.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F1493f709f804&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-a-b-testing-multi-armed-bandit-experiments-1493f709f804&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-a-b-testing-multi-armed-bandit-experiments-1493f709f804&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-a-b-testing-multi-armed-bandit-experiments-1493f709f804&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-a-b-testing-multi-armed-bandit-experiments-1493f709f804&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----1493f709f804--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1493f709f804--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@shawlu95?source=post_page-----1493f709f804--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@shawlu95?source=post_page-----1493f709f804--------------------------------", "anchor_text": "Shaw Lu"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd418b99bedad&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-a-b-testing-multi-armed-bandit-experiments-1493f709f804&user=Shaw+Lu&userId=d418b99bedad&source=post_page-d418b99bedad----1493f709f804---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1493f709f804&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-a-b-testing-multi-armed-bandit-experiments-1493f709f804&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1493f709f804&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-a-b-testing-multi-armed-bandit-experiments-1493f709f804&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/visualizing-beta-distribution-7391c18031f1", "anchor_text": "here"}, {"url": "https://support.google.com/analytics/answer/2846882", "anchor_text": "here"}, {"url": "https://towardsdatascience.com/visualizing-beta-distribution-7391c18031f1", "anchor_text": "link"}, {"url": "https://towardsdatascience.com/understanding-confidence-interval-d7b5aa68e3b", "anchor_text": "link"}, {"url": "https://towardsdatascience.com/the-power-of-a-b-testing-3387c04a14e3", "anchor_text": "link"}, {"url": "https://towardsdatascience.com/do-you-know-credible-interval-e5b833adf399#bce7", "anchor_text": "link"}, {"url": "https://www.kaggle.com/shawlu/k-armed-bandit", "anchor_text": "notebook"}, {"url": "https://medium.com/tag/data-science?source=post_page-----1493f709f804---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/statistics?source=post_page-----1493f709f804---------------statistics-----------------", "anchor_text": "Statistics"}, {"url": "https://medium.com/tag/data-visualization?source=post_page-----1493f709f804---------------data_visualization-----------------", "anchor_text": "Data Visualization"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----1493f709f804---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----1493f709f804---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1493f709f804&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-a-b-testing-multi-armed-bandit-experiments-1493f709f804&user=Shaw+Lu&userId=d418b99bedad&source=-----1493f709f804---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1493f709f804&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-a-b-testing-multi-armed-bandit-experiments-1493f709f804&user=Shaw+Lu&userId=d418b99bedad&source=-----1493f709f804---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1493f709f804&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-a-b-testing-multi-armed-bandit-experiments-1493f709f804&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1493f709f804--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F1493f709f804&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-a-b-testing-multi-armed-bandit-experiments-1493f709f804&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----1493f709f804---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----1493f709f804--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----1493f709f804--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----1493f709f804--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----1493f709f804--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----1493f709f804--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----1493f709f804--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----1493f709f804--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----1493f709f804--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@shawlu95?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@shawlu95?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Shaw Lu"}, {"url": "https://medium.com/@shawlu95/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "498 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd418b99bedad&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-a-b-testing-multi-armed-bandit-experiments-1493f709f804&user=Shaw+Lu&userId=d418b99bedad&source=post_page-d418b99bedad--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fadedc11d5c1a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-a-b-testing-multi-armed-bandit-experiments-1493f709f804&newsletterV3=d418b99bedad&newsletterV3Id=adedc11d5c1a&user=Shaw+Lu&userId=d418b99bedad&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}