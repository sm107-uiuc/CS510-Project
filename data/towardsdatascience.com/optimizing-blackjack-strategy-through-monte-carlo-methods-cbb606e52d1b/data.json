{"url": "https://towardsdatascience.com/optimizing-blackjack-strategy-through-monte-carlo-methods-cbb606e52d1b", "time": 1683001459.6725628, "path": "towardsdatascience.com/optimizing-blackjack-strategy-through-monte-carlo-methods-cbb606e52d1b/", "webpage": {"metadata": {"title": "Optimizing Blackjack Strategy through Monte Carlo Methods | by Adrian Yijie Xu | Towards Data Science", "h1": "Optimizing Blackjack Strategy through Monte Carlo Methods", "description": "Reinforcement Learning has taken the AI world by storm. From AlphaGo to AlphaStar, increasing numbers of traditional human-dominated activities have now been conquered by AI agents powered by\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.bbc.co.uk/news/technology-35785875", "anchor_text": "AlphaGo", "paragraph_index": 0}, {"url": "https://www.space.com/alphastar-artificial-intelligence-starcraft-2-grandmaster.html", "anchor_text": "AlphaStar", "paragraph_index": 0}, {"url": "https://medium.com/gradientcrescent", "anchor_text": "GradientCrescent", "paragraph_index": 0}, {"url": "https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-the-k-bandit-problem-illustrat-940eea430296", "anchor_text": "basic bandit systems", "paragraph_index": 0}, {"url": "https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-automating-pong-in-using-a-policy-model-an-implementation-b71f64c158ff", "anchor_text": "olicy-based approaches", "paragraph_index": 0}, {"url": "https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-markov-decision-processes-policies-value-functions-94f7389e1e82", "anchor_text": "Markovian environments", "paragraph_index": 0}, {"url": "https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-navigating-gridworld-with-dynamic-programming-9b98a6f20310", "anchor_text": "dynamic programming", "paragraph_index": 0}, {"url": "http://aigradients.com/2019/07/03/blackjack-with-monte-carlo-prediction/", "anchor_text": "Sudharsan et. al.", "paragraph_index": 22}, {"url": "https://github.com/EXJUSTICE/GradientCrescent", "anchor_text": "Github", "paragraph_index": 22}, {"url": "https://medium.com/gradientcrescent", "anchor_text": "GradientCrescent, covering applied AI", "paragraph_index": 35}], "all_paragraphs": ["Reinforcement Learning has taken the AI world by storm. From AlphaGo to AlphaStar, increasing numbers of traditional human-dominated activities have now been conquered by AI agents powered by reinforcement learning. Briefly, these achievements rely on the optimization of an agent\u2019s actions within an environment to achieve maximal reward. Over the past few articles on GradientCrescent, we\u2019ve covered various fundamental aspects of reinforcement learning, from basic bandit systems and policy-based approaches, to optimizing reward-based behavior within Markovian environments. All of these approaches have demanded that we have complete knowledge of our environment \u2014 dynamic programming for example, requires that we possess the complete probability distributions of all possible state transitions. However, in reality we find that most systems are impossible to know completely, and that probability distributions cannot be obtained in explicit formed due to complexity, innate uncertainty, or computational limitations. As an analogy, consider the task of a meteorologist \u2014 the number of factors involved behind predicting weather may be so numerous that it\u2019s simply improbable to know the exact probabilities involved.", "For these situations, sample based learning methods such as Monte Carlo are a solution. The term Monte Carlo is usually used to describe any estimation approach relying on random sampling. In other words, we do not assume of knowledge of our environment, but instead only learn from experience, through sample sequences of states, actions, and rewards obtained from interactions with the environment.These methods work by directly observing the rewards returned by the model during normal operation to judge the average value of its states. Interestingly, it\u2019s been shown that even without any knowledge of the environment\u2019s dynamics (which can be thought of as the probability distribution of state transitions), we can still obtain optimal behavior to maximize reward.", "As an example, consider the return from throwing 12 dice rolls. By considering these rolls as a single state, we can average these returns to approach the true expected return. As the number of samples increases, the more accurately we approach the actual expected return.", "This kind of sampling-based valuation may feel familiar to our loyal readers, as sampling is also done for k-bandit systems. Instead of comparing different bandits, Monte Carlo methods are used to compare different policies in Markovian environments, by determining the value of a state while following a particular policy until termination.", "Within the context of reinforcement learning, Monte Carlo methods are a way of estimating the values of states in a model by averaging sample returns. Due to the need of a terminal state, Monte Carlo methods are inherently applicable to episodic environments. Due to this restriction, Monte Carlo approaches are commonly considered as being \u201coffline\u201d, in which all updates are done after the terminal state is reached. A simple analogy would be randomly navigating a maze- an offline approach would have the agent reach the end, before using the experience to try and decrease the maze time. In contrast, an online approach would have the agent constantly modifying its behavior already within the maze \u2014 perhaps it notices that green corridors lead to dead-ends, and decides to avoid them while already in the maze. We will discuss online approaches in the next article.", "The Monte Carlo procedure can be summarized as follows:", "To better understand how Monte Carlo works, consider the state transition diagram below. The reward for each state-transition is shown in black, and a discount factor of 0.5 applied. Let\u2019s put aside the actual state values for now, and focus on calculating one round of returns.", "Given that the terminal state has a return of 0, let\u2019s calculate the return of every state, starting from the terminal state (G5). Note that we have set the discount factor to 0.5, resulting in a weighting towards more recent states.", "To avoid keeping all of the returns in a list, we can execute the Monte-Carlo state-value update procedure incrementally, with an equation that shares some similarities with traditional gradient descent:", "Within reinforcement learning, Monte Carlo methods can be further classified as \u201cFirst-visit\u201d or \u201cEvery visit\u201d. Briefly, the difference between the two lies in the number of times a state can be visited within a episode before an MC update is made. The first-visit MC method estimates the value of all states as the average of the returns following first visits to each state before termination, whereas the every-visit MC method averages the returns following an n-number of visits to a state before termination. We\u2019ll be using the first-visit Monte Carlo throughout this article due to its relative simplicity.", "If a model is not available to provide policy, MC can also be used to estimate state-action values. This is more useful than state values alone, as an idea of of the value of each action (q) within a given state allows the agent to automatically form a policy from observations in an unknown environment.", "More formally, we can use Monte Carlo to estimate q(s, a,pi), the expected return when starting in state s, taking action a, and thereafter following policy pi. The Monte Carlo methods remain the same, except that we now have the added dimensionality of actions taken for a certain state. A state\u2013 action pair (s, a) is said to be visited in an episode if ever the state s is visited and action a is taken in it. Similarly, state-action value estimation can be done via first-visit or every-visit approaches.", "As in Dynamic Programming, we can use generalized policy iteration to to form a policy from observations of state-action values.", "By alternating through policy evaluation and policy improvement steps and incorporating exploring starts to ensure that all possible actions are visited, we can achieve optimal policies for every state. For Monte Carlo GPI, this alternation is generally done after the termination of each episode.", "To better understand how Monte Carlo works in practice in valuing different state values and state-action values, let\u2019s perform a step-by-step demonstration with the game of Blackjack. To begin with, let\u2019s define the rules and conditions of our game:", "Let\u2019s demonstrate some Monte Carlo with a few hands of blackjack.", "You draw a total of 19. But pushing your luck you hit, draw a 3, and go bust. As you went bust, the dealer only had a single visible card, with a sum of 10. This can be visualized as follows:", "As we went bust, our reward for this round is -1. Let\u2019s assign this accordingly as the return for the penultimate state, using the format of [Agent sum, dealer sum, ace?]:", "Well that was unfortunate. Let\u2019s go for another round.", "You draw a total of 19. This time, you decided to stay. The dealer obtained 13, hits and goes bust. The penultimate states can be described as follows.", "Let\u2019s describe the states and rewards that have occurred in this round:", "With episode termination, we can now update the values of all of our states in this round using the calculated returns. Assuming a discount factor of 1, we simply propagate our new reward across our previous hands as done with the state transitions previously. As the state V(19, 10, no) has had a previous return of -1, we calculate the expected return and assign them to our state:", "Let\u2019s implement a game of blackjack using first-visit Monte Carlo to learn about all of the possible state-values (or different hand combinations) within the game, by using a Python approach based on that by Sudharsan et. al. As usual, our code can be found on the GradientCrescent Github.", "We\u2019ll use OpenAI\u2019s gym environment to make this facile. Think of the environment as an interface for running games of blackjack with minimal code, allowing us to focus on implementing reinforcement learning. Conveniently, all of the collected information about states, actions, and rewards are kept within \u201cobservation\u201d variables, which are accumulated through running sessions of the game.", "Let\u2019s start by importing all of the libraries we\u2019ll need to obtain and plot our results.", "Next let\u2019s initialize our gym environment and define the policy that\u2019ll guide our agent\u2019s actions. Essentially, we will keep hitting until our hand sum reaches 19 or more, after which we\u2019ll stand.", "Next, let\u2019s define a method to generate data for an episode using our policy. We\u2019ll store information on the state, the action taken, and the reward immediately following that action.", "Finally, let\u2019s define the first-visit Monte Carlo prediction function. Firstly, we initialize an empty dictionary to store the current state-values along with another dictionary storing the number of entries for each state across episodes.", "For each episode, we call upon our previous generate_episode method to generate information about the values of states and rewards earned following that state. We also initialize a variable to store our incremental returns. Next, we obtain the reward and current state-value for every state visited during the episode, and increment our returns variable with our reward for that step.", "Recall that as we are performing first-visit Monte Carlo, we only visit a single state within an episode once. Hence we perform a conditional check on the state-dictionary to see if the state has already been visited. If this condition is met, we can then calculate the new value using the Monte-Carlo state-value update procedure defined previously, and increase the number of observations for that state by 1. We then repeat the process for the following episode, in order to eventually obtain an average return.", "Let\u2019s run and take a look at our results!", "Sample output showing the state values of various hands of blackjack.", "We can continue to observe Monte Carlo for 5000 episodes, and plot a state-value distribution describing the values of any combination of player and dealer hands.", "So let\u2019s summarize what we\u2019ve learned.", "That wraps up this introduction to Monte Carlo method. In our next article, we\u2019ll move on to online methods of sample-based learning, in the form of Temporal Difference learning.", "We hope you enjoyed this article on Towards Data Science, and hope you check out the many other articles on our mother publication, GradientCrescent, covering applied AI.", "White et. al, Fundamentals of Reinforcement Learning, University of Alberta", "Silva et. al, Reinforcement Learning, UCL", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fcbb606e52d1b&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimizing-blackjack-strategy-through-monte-carlo-methods-cbb606e52d1b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimizing-blackjack-strategy-through-monte-carlo-methods-cbb606e52d1b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimizing-blackjack-strategy-through-monte-carlo-methods-cbb606e52d1b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimizing-blackjack-strategy-through-monte-carlo-methods-cbb606e52d1b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----cbb606e52d1b--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----cbb606e52d1b--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@adrianitsaxu?source=post_page-----cbb606e52d1b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@adrianitsaxu?source=post_page-----cbb606e52d1b--------------------------------", "anchor_text": "Adrian Yijie Xu"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc834a59b6354&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimizing-blackjack-strategy-through-monte-carlo-methods-cbb606e52d1b&user=Adrian+Yijie+Xu&userId=c834a59b6354&source=post_page-c834a59b6354----cbb606e52d1b---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcbb606e52d1b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimizing-blackjack-strategy-through-monte-carlo-methods-cbb606e52d1b&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcbb606e52d1b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimizing-blackjack-strategy-through-monte-carlo-methods-cbb606e52d1b&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.bbc.co.uk/news/technology-35785875", "anchor_text": "AlphaGo"}, {"url": "https://www.space.com/alphastar-artificial-intelligence-starcraft-2-grandmaster.html", "anchor_text": "AlphaStar"}, {"url": "https://medium.com/gradientcrescent", "anchor_text": "GradientCrescent"}, {"url": "https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-the-k-bandit-problem-illustrat-940eea430296", "anchor_text": "basic bandit systems"}, {"url": "https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-automating-pong-in-using-a-policy-model-an-implementation-b71f64c158ff", "anchor_text": "olicy-based approaches"}, {"url": "https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-markov-decision-processes-policies-value-functions-94f7389e1e82", "anchor_text": "Markovian environments"}, {"url": "https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-navigating-gridworld-with-dynamic-programming-9b98a6f20310", "anchor_text": "dynamic programming"}, {"url": "http://aigradients.com/2019/07/03/blackjack-with-monte-carlo-prediction/", "anchor_text": "Sudharsan et. al."}, {"url": "https://github.com/EXJUSTICE/GradientCrescent", "anchor_text": "Github"}, {"url": "https://medium.com/gradientcrescent", "anchor_text": "GradientCrescent, covering applied AI"}, {"url": "http://www.ccs.neu.edu/home/rplatt/cs7180_fall2018/slides/monte_carlo.pdf", "anchor_text": "Platt et. Al, Northeaster University"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----cbb606e52d1b---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----cbb606e52d1b---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/ai?source=post_page-----cbb606e52d1b---------------ai-----------------", "anchor_text": "AI"}, {"url": "https://medium.com/tag/blackjack?source=post_page-----cbb606e52d1b---------------blackjack-----------------", "anchor_text": "Blackjack"}, {"url": "https://medium.com/tag/monte-carlo?source=post_page-----cbb606e52d1b---------------monte_carlo-----------------", "anchor_text": "Monte Carlo"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcbb606e52d1b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimizing-blackjack-strategy-through-monte-carlo-methods-cbb606e52d1b&user=Adrian+Yijie+Xu&userId=c834a59b6354&source=-----cbb606e52d1b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcbb606e52d1b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimizing-blackjack-strategy-through-monte-carlo-methods-cbb606e52d1b&user=Adrian+Yijie+Xu&userId=c834a59b6354&source=-----cbb606e52d1b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcbb606e52d1b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimizing-blackjack-strategy-through-monte-carlo-methods-cbb606e52d1b&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----cbb606e52d1b--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fcbb606e52d1b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimizing-blackjack-strategy-through-monte-carlo-methods-cbb606e52d1b&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----cbb606e52d1b---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----cbb606e52d1b--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----cbb606e52d1b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----cbb606e52d1b--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----cbb606e52d1b--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----cbb606e52d1b--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----cbb606e52d1b--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----cbb606e52d1b--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----cbb606e52d1b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@adrianitsaxu?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@adrianitsaxu?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Adrian Yijie Xu"}, {"url": "https://medium.com/@adrianitsaxu/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "604 Followers"}, {"url": "https://github.com/EXJUSTICE/", "anchor_text": "https://github.com/EXJUSTICE/"}, {"url": "https://www.linkedin.com/in/yijie-xu-0174a325/", "anchor_text": "https://www.linkedin.com/in/yijie-xu-0174a325/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc834a59b6354&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimizing-blackjack-strategy-through-monte-carlo-methods-cbb606e52d1b&user=Adrian+Yijie+Xu&userId=c834a59b6354&source=post_page-c834a59b6354--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F362de3a1de04&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimizing-blackjack-strategy-through-monte-carlo-methods-cbb606e52d1b&newsletterV3=c834a59b6354&newsletterV3Id=362de3a1de04&user=Adrian+Yijie+Xu&userId=c834a59b6354&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}