{"url": "https://towardsdatascience.com/debugging-a-machine-learning-model-written-in-tensorflow-and-keras-f514008ce736", "time": 1682993851.7136042, "path": "towardsdatascience.com/debugging-a-machine-learning-model-written-in-tensorflow-and-keras-f514008ce736/", "webpage": {"metadata": {"title": "Debugging a Machine Learning model written in TensorFlow and Keras | by Lak Lakshmanan | Towards Data Science", "h1": "Debugging a Machine Learning model written in TensorFlow and Keras", "description": "In this article, you get to look over my shoulder as I go about debugging a TensorFlow model. I did a lot of dumb things, so please don\u2019t judge. You can see the final (working) model on GitHub. I\u2019m\u2026"}, "outgoing_paragraph_urls": [{"url": "https://github.com/GoogleCloudPlatform/training-data-analyst/tree/master/blogs/lightning/ltgpred", "anchor_text": "final (working) model", "paragraph_index": 1}, {"url": "https://ams.confex.com/ams/2019Annual/meetingapp.cgi/Paper/354046", "anchor_text": "present it at the American Meteorological Society", "paragraph_index": 1}, {"url": "https://github.com/tensorflow/tpu/tree/master/models/official/resnet", "anchor_text": "ResNet model written for the TPU", "paragraph_index": 3}, {"url": "https://keras.io/layers/convolutional/#conv2d", "anchor_text": "Keras\u2019 Conv2D", "paragraph_index": 13}, {"url": "https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer", "anchor_text": "documentation for the AdamOptimizer", "paragraph_index": 52}, {"url": "https://stackoverflow.com/questions/51830947/how-to-compute-standard-deviation-in-apache-beam/", "anchor_text": "unanswered question", "paragraph_index": 74}, {"url": "https://stackoverflow.com/a/52958771/3645038", "anchor_text": "the answer", "paragraph_index": 76}, {"url": "https://currents.soest.hawaii.edu/ocn760_4/_static/masked_arrays.html", "anchor_text": "Masked arrays", "paragraph_index": 80}], "all_paragraphs": ["In this article, you get to look over my shoulder as I go about debugging a TensorFlow model. I did a lot of dumb things, so please don\u2019t judge.", "You can see the final (working) model on GitHub. I\u2019m building a model to predict lightning 30 minutes into the future and plan to present it at the American Meteorological Society. The basic idea is to create 64x64 image patches around each pixel of infrared and Global Lightning Mapper (GLM) GOES-16 data and label the pixel as \u201chas_ltg=1\u201d if the lighting image actually occurs 30 minutes later within a 16x16 image patch around the pixel.", "A model trained in this way can be used to predict lightning 30 minutes ahead in real-time given the current infrared and GLM data.", "I wrote up a convnet model borrowing liberally from the training loop of the ResNet model written for the TPU and adapted the input function (to read my data, not JPEG) and the model (a simple convolutional network, not ResNet).", "The key bit of the code to get the input into a tensor:", "Essentially, each TensorFlow record (created by an Apache Beam pipeline) consists of ref, ltg and has_ltg fields. The ref and ltg are variable length arrays that are reshaped into dense 64x64 matrices (using tf.sparse_tensor_to_dense). The label is simply 0 or 1.", "Then, I take the two parsed images and stack them together:", "At this point, I have a tensor that is [?, 64, 64, 2], i.e. a batch of 2-channel images. The rest of the input pipeline in make_input_fn (listing files, reading the files via parallel interleave, preprocessing the data, making the shape static, and prefetching) was essentially just copy-pasted from the ResNet code.", "I developed the code by running the trainer locally for a few steps with a small batch size:", "Then, I ran it on a larger dataset on Cloud ML Engine using a large machine with a heftier GPU:", "This was useful. Much of my debugging and development was done by running locally. This way I could work disconnected and didn\u2019t need a GPU mounted on my machine.", "Once I had the code written and I ran it, I discovered that the model started quickly producing suspiciously identical losses:", "and reached an accuracy metric that was stubbornly stuck at this number starting at epoch 1000:", "When a machine learning model won\u2019t learn, there are a few usual suspects. I tried changing the initialization. By default, TensorFlow uses zeros_initializer [edit: Turns out I didn\u2019t need to do this \u2014 tf.layers.conv2d inherits from Keras\u2019 Conv2D which uses glorot_uniform which is the same as Xavier]. How about using Xavier (which uses small initial values), and setting the random seed for repeatability?", "How about changing the gradient from the sophisticated AdamOptimizer to the trusty standby GradientDescentOptimizer?", "How about reducing the learning rate from 0.01 to 1e-6?", "None of these helped, but that\u2019s what I first tried.", "Was my input function perhaps returning the same data each time? That would explain why the model gets stuck. How do I know what the input function is reading in?", "An easy way to verify that the input function is correct is to simply print out the values read. However, you can\u2019t just print a tensor:", "That will simply print out the metadata of the tensor, not its values. Instead, you need to evaluate the value of the tensor while the program is being executed:", "and even that won\u2019t work because the Estimator API doesn\u2019t give you a handle to the session. The solution is to use tf.Print:", "What this does is to insert the print node between the original img node and the new img node, so that the value gets printed before the image node gets used again:", "Once I did this, though, I got only the first 3 or 4 values (By default, tf.Print doesn\u2019t print the whole tensor ) and they were mostly zero. Are they zero because satellite images tend to have a lot of zeros or is it zero because my input pipeline has a bug? Simply printing the image is not a great idea. So, I decided to print out statistics of the inputs:", "The reduce_* functions in TensorFlow allow you to sum along an axis. Hence the first function reduce_sum(labels) computes the sum of the labels over the batch. Since the label is 0 or 1, this sum tells me the number of lighting examples in the batch.", "I also want to print the mean of the reflectivity and lightning input image patches. For that, I want to do a sum over the height and width but keep each example and channel separate \u2014 that\u2019s why you see [1,2] in the reduce_mean calls. The first tf.slice gets the first channel, and the second slice gets me the second channel (-1 in tf.slice tells TensorFlow to get all the elements in that dimension).", "Notice also that I have inserted the Print node at the location of ylogits and get to print a whole list of tensors. This is important \u2014 you have to stick in the Print() into the graph at a node that actually gets used. If I had done:", "the whole branch would have been optimized away since my model doesn\u2019t actually use numltg anywhere!", "Once I ran the code, I found that each batch had a good (and differing) mix of lightning points, that the means looked sort of similar within each batch, but were different from batch to batch.", "Although this is not the problem we are trying to solve, this similarity of means by batch is quite odd. Looking over the code with this curiosity in mind, I found that I had hardcoded a shuffle size:", "The issue is that because the data is created from sliding windows, successive examples tend to be highly correlated, so I need to shuffle over a large enough buffer that we get examples from a different part of the satellite image (or better still, different images). In the case of ResNet, each training example is a completely different image, so this is not an issue they were concerned about. Copy-paste strikes again!", "But back to the original problem. Why are the accuracy and RMSE stuck? I kinda lucked out here. I had to insert the tf.Print() somewhere and so I stuck into a node that I knew I needed \u2014 on the output node of my model function (ylogits). I also happened to print out ylogits as well and lo and behold \u2026 while the inputs were all different values each time, ylogits started out random but quickly became zero.", "Why is the ylogits zero? Looking carefully at the ylogits calculation, I noticed that I had written:", "Oops! By setting the ReLu activation function on the output dense layer, I had made sure ylogits would never be negative. Meanwhile, lightning is rarer than non-lightning, so the optimizer was pushing ylogits to the smallest possible value it could take. Which is zero. Because ReLu saturates below zero, it is possible for things to get stuck there.", "The last-but-one-layer of a classification network is like the last layer of a regression network. It has to be:", "Silly, silly, bug. Found and fixed. Whew!", "Now, when I ran it though, I didn\u2019t get a stuck accuracy metric. Worse. I got \u2026 \u201cNaN loss during training.\u201d If there is anything that will strike terror into the hearts of a ML practitioner, it\u2019s NaN losses. Oh, well, if I can figure this thing out, I will get a blog post out of it.", "As with a model that doesn\u2019t train, there are a few usual suspects for NaN losses. Trying to compute cross entropy loss yourself is one of them. But I wasn\u2019t doing that. I was computing the loss as:", "This should be numerically stable. The other problem is a learning rate that is too high. I switched back to AdamOptimizer and tried setting a low learning rate (1e-6). No go.", "Another problem is that the input data might itself contain NaNs. This should not be possible \u2014 one of the benefits of using TFRecords is that the TFRecordWriter will not accept NaN values. Just to make sure, I went back to my input pipeline and added np.nan_to_num to the piece of code that inserted arrays into the TFRecord:", "Maybe there are way too many weights? I made the number of layers in my model configurable and tried different numbers and smaller kernel sizes:", "Still no go. The NaN loss remained.", "What if I completely remove the deep learning model? Remember my debugging statistics on the input image? What if we try to train a model to predict lighting based on just those engineered features:", "I decided to create two sets of statistics, one in a 64x64 box and the other in a 16x16 one and create a logistic regression model with just these 6 input features.", "No go. Still NaN. This is very, very strange. A linear model should have never NaN-out. Mathematically, this is crazy.", "But just before it NaN-ed out, the model reached a 75% accuracy. That\u2019s awfully promising. But this NaN thing is getting to be super annoying. The funny thing is that just before it \u201cdiverges\u201d with loss = NaN, the model hasn\u2019t been diverging at all, the loss has been going down:", "Poking around to find out whether there is an alternate way to compute the loss, I discover that there is now a convenience function:", "Of course, that probably won\u2019t fix anything but it\u2019s better to use this function rather than call reduce_mean myself. But while here, let\u2019s add asserts to narrow down the problem:", "Essentially, I assert that ylogits is numeric, and that each label is between 0 and 1. Only if these conditions are met do I compute the loss. Otherwise, the program is supposed to throw an error.", "The assertions don\u2019t trigger, but the program still ends with a NaN loss. At this point, it seems clear the problem does not lie in the input data (because I have done a nan_to_num there) or in our model calculation (because assertions don\u2019t trigger) itself. Could it instead be in the back-propagation, possibly in the gradient computation?", "For example, perhaps an unusual (but correct) example leads to unexpectedly high gradient magnitudes. Let\u2019s just limit the effect of such unusual examples by clipping the gradient:", "If there are lot of very similar inputs, it is possible for the weights themselves to blow up. Over time, you might get a very high positive magnitude associated with one input node and a very high negative magnitude associated with the next input node. One way to get the network to avoid such a situation is to penalize high magnitude weights by adding an extra term to the loss function:", "This gets all the trainable variables (weights and biases) and adds a penalty to the loss based on the value of those trainable variables. Ideally, we penalize just the weights (not the biases), but this is fine just for trying out.", "Poking around some more, I realize that the documentation for the AdamOptimizer explains the default epsilon value of 1e-8 may be problematic. Essentially, small values of epsilon cause instability even though the purpose of the epsilon is to prevent a divide-by-zero. This begs the question of why this is the default, but let\u2019s try the larger value that is recommended.", "That pushes the NaN further out, but it still fails.", "Another reason for this could be CUDA bugs and the like. Let\u2019s try training on a different hardware (with P100 instead of Tesla K80) to see if the problem persists.", "Sometimes, when left with an intractable bug, it is better to just try rewriting the model in a completely different way.", "So, I decided to rewrite the model in Keras. This will also give me the opportunity to learn Keras,\u00a0something\u00a0I\u2019ve\u00a0been\u00a0meaning\u00a0to\u00a0do\u00a0for\u00a0a\u00a0while. Lemonade out of lemons and all that.", "Creating a CNN with batchnorm (which will help keep gradients in range) is quite easy in Keras. (Thank you, Francois). So easy that I put batchnorm everywhere.", "Notice also that the final layer directly adds a sigmoid activation. There is no need to muck around with the logits because the optimization takes care of the numeric problems for us:", "Unfortunately (you know it by now), I still got NaNs!", "Gaah. Okay, back to the drawing board. Let\u2019s do all the things we did in TensorFlow in Keras.", "First step is to forget about all this deep learning stuff and build a linear model. How do you do that? I have an image. I need to do feature engineering and send it to a Dense layer. That means, in Keras, that I have to write my own layer to do the feature engineering:", "The engineered_features is exactly the same TensorFlow function as before! The key idea is that to wrap a TensorFlow function into a Keras layer, you can use a Lambda layer and invoke the TensorFlow function.", "But I want to print out the layer to make sure that the numbers flowing through are correct. How do I do that? tf.Print() won\u2019t work because, well, I don\u2019t have tensors. I have Keras layers.", "Well, tf.Print() is a TensorFlow function and so, use the same Lambda layer trick:", "which can then be invoked as:", "Clipping the gradient in Keras? Easy! Every optimizer supports clipnorm.", "Hey, I\u2019m liking this Keras thing \u2014 it gives me a nice, simple API. Plus, it interoperates nicely with TensorFlow to give me low-level control whenever I need it.", "Oh, right. I still have the NaN problem", "One final thing, something I kinda discounted. The NaN problem could also arise from unscaled data. But my reflectivity and lightning data are both in the range [0,1]. So, I don\u2019t really need to scale things at all.", "Still, I\u2019m at a loose end. Why don\u2019t I normalize the image data (subtract the mean, divide by the variance) and see if it helps.", "To compute the variance, I need to walk through the entire dataset, and so this is a job for Beam. I could do this in TensorFlow Transform, but for now, let me hack it in Beam.", "Since I am rewriting my pipeline, I might as well fix the shuffling issue (see section 2d) once and for all. Increasing the shuffle buffersize was a hack. I really don\u2019t want to write the data in the order that I created the image patches. Let\u2019s randomize the order of the image patches in Apache Beam:", "Essentially, I assign a random key (between 1 and 1000) to each record, group by that random key, remove the key and write out the records. Now, successive image patches will not follow one after the other.", "How do I compute the variance in Apache Beam? I did what I always do, search on StackOverflow for something I can copy-paste. Unfortunately, all I found was an unanswered question. Oh, well, buckle down and write a custom Combiner:", "Note the workflow here \u2014 I can quickly test it on a list of numbers to make sure it works.", "Then, I went and added the answer to StackOverflow. Maybe all I need is some good karma \u2026", "I can then go to my pipeline code and add:", "Essentially, I pull out example[\u2018ref\u2019] and example[\u2018ltg\u2019] and create tuples that I group by key. Then, I can compute the mean and standard deviation of every pixel in both these images over the entire dataset.", "Once I run the pipeline, I can print the resulting statistics:", "Masked? What the @#$@#$@# does masked mean? Turns out that Masked arrays are a special numpy thing. Masked values are not NaN and so, if you process them with Numpy, nan_to_num() won\u2019t do anything to it. On the other hand, it looks numeric, and so all my TensorFlow assertions don\u2019t raise. Numeric operations with a masked value results in a masked value.", "Masked values are just like NaN \u2014 they will slurp your milkshake from across a room but the regular numpy and TensorFlow library methods know nothing about masking.", "Since the masking happens only in the reflectivity grids (I create the lightning grids myself by accumulating lightning flashes), I have to convert the masked values to a nice number after reading", "and now, when I rerun the pipeline, I get reasonable values:", "These are small numbers. There should be no reason to scale anything. So, let\u2019s just train with the masking problem fixed.", "This time, no NaN. Instead the program crashes and logs show:", "Replica zero is the input pipeline (reading data happens on the CPU). Why does it want to read 1280000 records all at once into the shuffle buffer?", "Well, now that the input data are so well shuffled by my Beam/Dataflow, I don\u2019t even need that large a shuffle buffer (this used to be 5000, see Section 2d):", "And \u2026 12 minutes later, the training finishes with an accuracy of 83%(!!!). No NaNs anywhere.", "Remember that we did only a linear model consisting of engineered features. Let\u2019s add back the convnet in parallel. The idea is to concatenate the two dense layers, so that my model architecture can contain both the CNN layers and the feature engineering:", "This is how to do this in Keras, with some batchnorm and dropouts thrown in, just because they are so easy to add:", "Now, I have an accuracy of 85%. This is still a small dataset (only 60 days of satellite data) and that\u2019s probably why the deep learning path doesn\u2019t add that much value. So, I\u2019ll go back and generate more data, train longer, train on the TPU etc.", "But those things can wait. Right now, I\u2019m off to celebrate.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "articles are personal observations and not investment advice."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Ff514008ce736&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdebugging-a-machine-learning-model-written-in-tensorflow-and-keras-f514008ce736&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdebugging-a-machine-learning-model-written-in-tensorflow-and-keras-f514008ce736&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdebugging-a-machine-learning-model-written-in-tensorflow-and-keras-f514008ce736&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdebugging-a-machine-learning-model-written-in-tensorflow-and-keras-f514008ce736&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----f514008ce736--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f514008ce736--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://lakshmanok.medium.com/?source=post_page-----f514008ce736--------------------------------", "anchor_text": ""}, {"url": "https://lakshmanok.medium.com/?source=post_page-----f514008ce736--------------------------------", "anchor_text": "Lak Lakshmanan"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F247b0630b5d6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdebugging-a-machine-learning-model-written-in-tensorflow-and-keras-f514008ce736&user=Lak+Lakshmanan&userId=247b0630b5d6&source=post_page-247b0630b5d6----f514008ce736---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff514008ce736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdebugging-a-machine-learning-model-written-in-tensorflow-and-keras-f514008ce736&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff514008ce736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdebugging-a-machine-learning-model-written-in-tensorflow-and-keras-f514008ce736&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://github.com/GoogleCloudPlatform/training-data-analyst/tree/master/blogs/lightning/ltgpred", "anchor_text": "final (working) model"}, {"url": "https://ams.confex.com/ams/2019Annual/meetingapp.cgi/Paper/354046", "anchor_text": "present it at the American Meteorological Society"}, {"url": "https://github.com/tensorflow/tpu/tree/master/models/official/resnet", "anchor_text": "ResNet model written for the TPU"}, {"url": "https://keras.io/layers/convolutional/#conv2d", "anchor_text": "Keras\u2019 Conv2D"}, {"url": "https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer", "anchor_text": "documentation for the AdamOptimizer"}, {"url": "https://stackoverflow.com/questions/51830947/how-to-compute-standard-deviation-in-apache-beam/", "anchor_text": "unanswered question"}, {"url": "https://stackoverflow.com/a/52958771/3645038", "anchor_text": "the answer"}, {"url": "https://currents.soest.hawaii.edu/ocn760_4/_static/masked_arrays.html", "anchor_text": "Masked arrays"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----f514008ce736---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/tensorflow?source=post_page-----f514008ce736---------------tensorflow-----------------", "anchor_text": "TensorFlow"}, {"url": "https://medium.com/tag/keras?source=post_page-----f514008ce736---------------keras-----------------", "anchor_text": "Keras"}, {"url": "https://medium.com/tag/debugging?source=post_page-----f514008ce736---------------debugging-----------------", "anchor_text": "Debugging"}, {"url": "https://medium.com/tag/numpy?source=post_page-----f514008ce736---------------numpy-----------------", "anchor_text": "Numpy"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff514008ce736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdebugging-a-machine-learning-model-written-in-tensorflow-and-keras-f514008ce736&user=Lak+Lakshmanan&userId=247b0630b5d6&source=-----f514008ce736---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff514008ce736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdebugging-a-machine-learning-model-written-in-tensorflow-and-keras-f514008ce736&user=Lak+Lakshmanan&userId=247b0630b5d6&source=-----f514008ce736---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff514008ce736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdebugging-a-machine-learning-model-written-in-tensorflow-and-keras-f514008ce736&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f514008ce736--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Ff514008ce736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdebugging-a-machine-learning-model-written-in-tensorflow-and-keras-f514008ce736&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----f514008ce736---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----f514008ce736--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----f514008ce736--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----f514008ce736--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----f514008ce736--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----f514008ce736--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----f514008ce736--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----f514008ce736--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----f514008ce736--------------------------------", "anchor_text": ""}, {"url": "https://lakshmanok.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://lakshmanok.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Lak Lakshmanan"}, {"url": "https://lakshmanok.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "9.3K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F247b0630b5d6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdebugging-a-machine-learning-model-written-in-tensorflow-and-keras-f514008ce736&user=Lak+Lakshmanan&userId=247b0630b5d6&source=post_page-247b0630b5d6--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F44e3f26acd1a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdebugging-a-machine-learning-model-written-in-tensorflow-and-keras-f514008ce736&newsletterV3=247b0630b5d6&newsletterV3Id=44e3f26acd1a&user=Lak+Lakshmanan&userId=247b0630b5d6&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}