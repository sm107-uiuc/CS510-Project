{"url": "https://towardsdatascience.com/optimization-descent-algorithms-bf595f069788", "time": 1683005287.766856, "path": "towardsdatascience.com/optimization-descent-algorithms-bf595f069788/", "webpage": {"metadata": {"title": "Optimization \u2014 Descent Algorithms | by Omar Aflak | Towards Data Science", "h1": "Optimization \u2014 Descent Algorithms", "description": "Many algorithms used in Machine Learning are based on basic mathematical optimization methods. Discovering these algorithms directly in the context of Machine Learning might be confusing because of\u2026"}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/Test_functions_for_optimization", "anchor_text": "here for instance", "paragraph_index": 10}, {"url": "https://en.wikipedia.org/wiki/Gradient", "anchor_text": "gradient", "paragraph_index": 11}, {"url": "https://en.wikipedia.org/wiki/Hessian_matrix", "anchor_text": "hessian", "paragraph_index": 11}, {"url": "https://en.wikipedia.org/wiki/Taylor_series", "anchor_text": "Taylor expansion", "paragraph_index": 15}, {"url": "https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization", "anchor_text": "Newton\u2019s Method", "paragraph_index": 24}, {"url": "https://en.wikipedia.org/wiki/Golden-section_search", "anchor_text": "Golden Section Search", "paragraph_index": 37}, {"url": "https://colab.research.google.com/drive/1jOK_C_pSDV6J98ggV1-uEaWUrPuVm5tu", "anchor_text": "Google Colab", "paragraph_index": 43}], "all_paragraphs": ["Many algorithms used in Machine Learning are based on basic mathematical optimization methods. Discovering these algorithms directly in the context of Machine Learning might be confusing because of all the prerequisites. Thus, I think it might be a good idea to see these algorithms free of any context in order to get a better understanding of these techniques.", "Descent algorithms are meant to minimise a given function, that\u2019s it. Really. These algorithms proceed iteratively, it means that they successively improve their current solution. You might think:", "What if I want to find the maximum of a function ?", "Simply, add a minus sign in front of your function, and it becomes a \u201cmin\u201d problem!", "Let\u2019s dive in. This is our problem definition:", "One prerequisite you must know is that if a point is a minimum, maximum, or a saddle point (meaning both at the same time), then the gradient of the function is zero at that point.", "Descent algorithms consist of building a sequence {x} that will converge towards x* (arg min f(x)). The sequence is built the following way:", "Where k is the iteration, and d is a vector, same size as x, called the descent vector. Then, this is what the algorithm looks like:", "That\u2019s it! We keep doing the update until the norm of the gradient is small enough (as it should reach a zero value at some extremum).", "We will see 3 different descent/direction vectors: Newton\u2019s direction, Gradient\u2019s direction, and Gradient + Optimal Step Size direction. First, we need to define a function that we will try to minimise during our experiments.", "I chose the Rosenbrock function, but you may find many others, here for instance. Another good one would be Himmelblau\u2019s function.", "We will also need, two other pieces of information, the gradient of that function, as well as the hessian matrix.", "Let\u2019s open up a file and start a Python script. I will do this in a Google Colab, and all the code used in this post will be available here:", "Here is our first piece of code.", "From now on, I will refer to the function input vector as x, akin to the problem definition earlier. Now that we are ready, let\u2019s see the first descent vector!", "You can find this updated formula by doing the 2nd order Taylor expansion of f(x + d), since the update we are performing is x_new = x + d.", "We want to find d such that f(x + d) is as low as possible. Supposing f\u2019\u2019(x) is positive, this equation is a parabola that has a minimum. That minimum is reached when the derivative of f(x + d) is zero.", "In n-dimensions, f\u2019\u2019(x) becomes the hessian matrix, and 1/f\u2019\u2019(x) shows up as the inverse hessian matrix. Finally, f\u2019(x) will be the gradient.", "We need to compute the inverse of the hessian matrix. For big matrices, this is a very computationally intensive task. Therefore, in practice, we solve this a bit differently, but in a totally equivalent manner.", "Instead of computing the inverse of the hessian matrix, we solve this equation for g and make the update rule the following:", "You will notice a small difference with the algorithm I presented at the beginning. I added a max_iteration parameter, so that the algorithm doesn\u2019t run indefinitely if it doesn\u2019t converge. Let\u2019s try it.", "The algorithm converged in only 2 iterations! That\u2019s really fast. You might think:", "Hey, the initial x is very close to the target x*, that makes the task easy!", "You\u2019re right. Try with some other values, for instance x_init = [50, -30], the algorithm terminates in 5 iterations.", "This algorithm is called the Newton\u2019s Method and all descent algorithms are modifications of this method! It\u2019s kind of the mother formula. The reason why it\u2019s really fast is that it uses second order information (the hessian matrix).", "Using the hessian matrix, even though it\u2019s dope, comes at a cost: efficiency. Computing an inverse matrix is a computationally intensive task, so mathematicians came up with solutions to overcome this problem. Mainly: Quasi-Newton methods, and Gradient methods. Quasi-Newton methods try to approximate the inverse of the hessian matrix with various techniques, whereas Gradient methods simply stick to first order information.", "If you did some Machine Learning, you\u2019ve probably seen this already. The gradient direction:", "Where \u03b1 is called the step size (or learning rate in ML), and is a real number in the range [0, 1].", "If you have been doing some Machine Learning, now you know this formula is actually part of a bigger one: Newton\u2019s direction, except we replaced the inverse hessian with a constant! Anyways, the update rule is now:", "You can tweak the values of alpha, epsilon, and max_iterations. In order to get a result similar to the Newton\u2019s method I came up with those. This is the result:", "Wow! Gradient descent took 5000 iterations where the Newton\u2019s method took only 2! Moreover, the algorithm didn\u2019t completely reach the minimum point (1, 1).", "The main reason for which this algorithm converged so slowly compared to Newton, is that not only we no longer have the information given by the second derivative of f, but we used a constant to replace the inverse hessian.", "Think about it. The derivative of a function is the rate of change of that function. So the hessian gives information about the rate of change of the gradient. Since finding the minimum implies necessarily a zero gradient, the hessian becomes super useful as it tells you when the gradient goes up or down.", "Many papers in ML are just about finding a better approach for this specific step. Momentum, Adagrad, or Adadelta are some examples.", "One improvement to the classical gradient descent is to use a variable step size at each iteration, not a constant. Not only it\u2019s going to be a variable step size, but it\u2019s also the best possible step size.", "How do we find \u03b1? Since we want this update to be as efficient as possible, i.e. to minimise f as much as possible, we are looking for \u03b1 such that:", "Notice that at this step, x and grad(x) are constants. Therefore, we can define a new function q:", "Where q is actually a function of one variable. And we want to find the \u03b1 that minimises this function. Umm\u2026 Gradient descent? We could, but while we\u2019re at it, let\u2019s learn a new method: Golden Section Search.", "Golden Section Search aims at finding the extremum (minimum or maximum) of a function inside a specified interval. Since we use \u03b1 in the range [0, 1], this is the perfect opportunity to use this algorithm.", "As this post is starting to be pretty long I\u2019m not going to go into the details. Hopefully, with the help of that magnificent GIF I took ages to make, and the code below, you\u2019ll be able to understand what\u2019s happening here.", "Now that we are able to find the best \u03b1, let\u2019s code gradient descent with optimal step size!", "Then, we can run this code:", "Even though in this case the results are not significantly better than pure gradient descent, generally the optimal step size performs better. For instance, I tried the same comparison with Himmelblau\u2019s function, and gradient descent with optimal step size was more than twice as fast as pure gradient descent.", "This is the end of this post. I hope you learned some new things that triggered your curiosity for mathematical optimization! There are tons of other interesting methods. Go find them! Don\u2019t forget to check out the Google Colab file, you will find all the code used and the same tests we did here with Himmelblau\u2019s function. Don\u2019t hesitate to leave a comment, and until next time, peace! :)", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fbf595f069788&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimization-descent-algorithms-bf595f069788&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimization-descent-algorithms-bf595f069788&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimization-descent-algorithms-bf595f069788&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimization-descent-algorithms-bf595f069788&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----bf595f069788--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----bf595f069788--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://omaraflak.medium.com/?source=post_page-----bf595f069788--------------------------------", "anchor_text": ""}, {"url": "https://omaraflak.medium.com/?source=post_page-----bf595f069788--------------------------------", "anchor_text": "Omar Aflak"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc215fdc67eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimization-descent-algorithms-bf595f069788&user=Omar+Aflak&userId=c215fdc67eb&source=post_page-c215fdc67eb----bf595f069788---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbf595f069788&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimization-descent-algorithms-bf595f069788&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbf595f069788&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimization-descent-algorithms-bf595f069788&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://en.wikipedia.org/wiki/Test_functions_for_optimization", "anchor_text": "here for instance"}, {"url": "https://en.wikipedia.org/wiki/Gradient", "anchor_text": "gradient"}, {"url": "https://en.wikipedia.org/wiki/Hessian_matrix", "anchor_text": "hessian"}, {"url": "https://colab.research.google.com/drive/1jOK_C_pSDV6J98ggV1-uEaWUrPuVm5tu", "anchor_text": "Descent AlgorithmsOmar Aflakcolab.research.google.com"}, {"url": "https://en.wikipedia.org/wiki/Taylor_series", "anchor_text": "Taylor expansion"}, {"url": "https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization", "anchor_text": "Newton\u2019s Method"}, {"url": "https://en.wikipedia.org/wiki/Golden-section_search", "anchor_text": "Golden Section Search"}, {"url": "https://colab.research.google.com/drive/1jOK_C_pSDV6J98ggV1-uEaWUrPuVm5tu", "anchor_text": "Google Colab"}, {"url": "https://medium.com/tag/mathematics?source=post_page-----bf595f069788---------------mathematics-----------------", "anchor_text": "Mathematics"}, {"url": "https://medium.com/tag/optimization?source=post_page-----bf595f069788---------------optimization-----------------", "anchor_text": "Optimization"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----bf595f069788---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/algorithms?source=post_page-----bf595f069788---------------algorithms-----------------", "anchor_text": "Algorithms"}, {"url": "https://medium.com/tag/python?source=post_page-----bf595f069788---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbf595f069788&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimization-descent-algorithms-bf595f069788&user=Omar+Aflak&userId=c215fdc67eb&source=-----bf595f069788---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbf595f069788&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimization-descent-algorithms-bf595f069788&user=Omar+Aflak&userId=c215fdc67eb&source=-----bf595f069788---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbf595f069788&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimization-descent-algorithms-bf595f069788&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----bf595f069788--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fbf595f069788&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimization-descent-algorithms-bf595f069788&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----bf595f069788---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----bf595f069788--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----bf595f069788--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----bf595f069788--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----bf595f069788--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----bf595f069788--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----bf595f069788--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----bf595f069788--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----bf595f069788--------------------------------", "anchor_text": ""}, {"url": "https://omaraflak.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://omaraflak.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Omar Aflak"}, {"url": "https://omaraflak.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "497 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc215fdc67eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimization-descent-algorithms-bf595f069788&user=Omar+Aflak&userId=c215fdc67eb&source=post_page-c215fdc67eb--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fb98d8ad5cb06&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimization-descent-algorithms-bf595f069788&newsletterV3=c215fdc67eb&newsletterV3Id=b98d8ad5cb06&user=Omar+Aflak&userId=c215fdc67eb&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}