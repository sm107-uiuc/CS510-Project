{"url": "https://towardsdatascience.com/manifold-learning-2-99a25eeb677d", "time": 1683013661.782672, "path": "towardsdatascience.com/manifold-learning-2-99a25eeb677d/", "webpage": {"metadata": {"title": "Latent graph neural networks: Manifold learning 2.0? | by Michael Bronstein | Towards Data Science", "h1": "Latent graph neural networks: Manifold learning 2.0?", "description": "Graph neural networks exploit relational inductive biases for data that come in the form of a graph. However, in many cases the graph is not available. Can we still use them?"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/graph-deep-learning/home", "anchor_text": "previous posts", "paragraph_index": 0}, {"url": "https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction", "anchor_text": "non-linear dimensionality reduction", "paragraph_index": 6}, {"url": "https://en.wikipedia.org/wiki/Principal_component_analysis", "anchor_text": "PCA", "paragraph_index": 7}, {"url": "https://towardsdatascience.com/transformers-are-graph-neural-networks-bca9f75412aa", "anchor_text": "generalisations of the transformer architecture", "paragraph_index": 12}, {"url": "https://arxiv.org/pdf/2007.12002.pdf", "anchor_text": "Grale: Designing networks for graph learning", "paragraph_index": 14}, {"url": "https://towardsdatascience.com/do-we-need-deep-graph-neural-networks-be62d3ec5c59", "anchor_text": "exponential growth", "paragraph_index": 14}, {"url": "https://arxiv.org/pdf/2006.05205.pdf", "anchor_text": "On the bottleneck of graph neural networks and its practical implications", "paragraph_index": 14}, {"url": "https://arxiv.org/pdf/1811.08790.pdf", "anchor_text": "Learning quadratic games on networks", "paragraph_index": 15}, {"url": "https://arxiv.org/pdf/1612.00593", "anchor_text": "PointNet: Deep learning on point sets for 3D classification and segmentation", "paragraph_index": 16}, {"url": "https://arxiv.org/pdf/1801.07829.pdf", "anchor_text": "Dynamic graph CNN for learning on point clouds", "paragraph_index": 17}, {"url": "https://arxiv.org/pdf/2002.04999.pdf", "anchor_text": "Differentiable Graph Module (DGM) for graph convolutional networks", "paragraph_index": 18}, {"url": "https://arxiv.org/pdf/2003.13620.pdf", "anchor_text": "Latent patient network learning for automatic diagnosis", "paragraph_index": 18}, {"url": "https://arxiv.org/pdf/1806.01738.pdf", "anchor_text": "Disease prediction using graph convolutional networks: application to autism spectrum disorder and Alzheimer\u2019s disease", "paragraph_index": 19}, {"url": "http://web.mit.edu/cocosci/Papers/sci_reprint.pdf", "anchor_text": "A global geometric framework for nonlinear dimensionality reduction", "paragraph_index": 22}, {"url": "https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm", "anchor_text": "Dijkstra algorithm", "paragraph_index": 22}, {"url": "https://en.wikipedia.org/wiki/Multidimensional_scaling", "anchor_text": "multidimensional scaling", "paragraph_index": 22}, {"url": "https://papers.nips.cc/paper/1961-laplacian-eigenmaps-and-spectral-techniques-for-embedding-and-clustering.pdf", "anchor_text": "Laplacian eigenmaps and spectral techniques for embedding and clustering", "paragraph_index": 23}, {"url": "https://arxiv.org/pdf/1806.00088.pdf", "anchor_text": "PeerNets: Exploiting peer wisdom against adversarial attacks", "paragraph_index": 24}, {"url": "http://stanford.edu/class/ee367/reading/A%20non-local%20algorithm%20for%20image%20denoising.pdf", "anchor_text": "A non-local algorithm for image denoising", "paragraph_index": 25}, {"url": "http://www.cs.technion.ac.il/~ron/PAPERS/scale_space97_belt.pdf", "anchor_text": "From high energy physics to low level vision", "paragraph_index": 25}, {"url": "http://www.cse.ucsc.edu/~manduchi/Papers/ICCV98.pdf", "anchor_text": "Bilateral filtering for gray and color images", "paragraph_index": 25}, {"url": "https://arxiv.org/pdf/1312.6199", "anchor_text": "Intriguing properties of neural networks", "paragraph_index": 26}, {"url": "https://arxiv.org/pdf/1710.08864", "anchor_text": "One pixel attack for fooling deep neural networks", "paragraph_index": 26}, {"url": "https://arxiv.org/abs/1610.08401", "anchor_text": "Universal adversarial perturbations", "paragraph_index": 26}, {"url": "https://arxiv.org/pdf/1802.04687.pdf", "anchor_text": "Neural relational inference for interaction systems", "paragraph_index": 27}, {"url": "https://en.wikipedia.org/wiki/IceCube_Neutrino_Observatory", "anchor_text": "IceCube", "paragraph_index": 28}, {"url": "https://arxiv.org/abs/1809.06166", "anchor_text": "Graph neural networks for IceCube signal classification", "paragraph_index": 28}, {"url": "https://link.springer.com/article/10.1140/epjc/s10052-019-7113-9", "anchor_text": "Learning representations of irregular particle-detector geometry with distance-weighted graph networks", "paragraph_index": 28}, {"url": "https://arxiv.org/pdf/1803.03324.pdf", "anchor_text": "Learning deep generative models of graphs", "paragraph_index": 29}, {"url": "http://proceedings.mlr.press/v97/franceschi19a/franceschi19a.pdf", "anchor_text": "Learning discrete structures for graph neural networks", "paragraph_index": 30}, {"url": "https://towardsdatascience.com/graph-deep-learning/home", "anchor_text": "blog", "paragraph_index": 31}, {"url": "https://michael-bronstein.medium.com/subscribe", "anchor_text": "subscribe", "paragraph_index": 31}, {"url": "https://michael-bronstein.medium.com/membership", "anchor_text": "Medium membership", "paragraph_index": 31}, {"url": "https://twitter.com/mmbronstein", "anchor_text": "Twitter", "paragraph_index": 31}], "all_paragraphs": ["The past few years have witnessed a surge of interest in developing ML methods for graph-structured data. Such data naturally arises in many applications such as social sciences (e.g. the Follow graph of users on Twitter or Facebook), chemistry (where molecules can be modelled as graphs of atoms connected by bonds), or biology (where interactions between different biomolecules are often modelled as a graph referred to as the interactome). Graph neural networks (GNNs), which I have covered extensively in my previous posts, are a particularly popular method of learning on such graphs by means of local operations with shared parameters exchanging information between adjacent nodes.", "In some settings, however, we do not have the luxury of being given a graph to work with as input. This is a typical situation for many biological problems, where graphs such as protein-to-protein interaction are only partially known in the best case, as the experiments by which interactions are discovered are expensive and noisy. We are therefore interested in inferring the graph from the data and applying a GNN on it [1] \u2014 I call this setting \u201clatent graph learning\u201d [2]. The latent graph might be application-specific and optimised for the downstream task. Furthermore, sometimes such a graph might be even more important than the task itself, as it may convey important insights about the data and offer a way to interpret the results.", "A way of thinking of latent graph learning is that of a graph with an empty edge set. In this setting, the input is provided as a point cloud in some high-dimensional feature space. Unlike methods for deep learning on sets such as PointNet [3], which apply some shared learnable point-wise function to each point, we also seek to pass information across points. This is done by message passing on a graph constructed from the point features themselves.", "The first architecture of this kind, Dynamic Graph CNN (DGCNN) [4], was developed by Yue Wang from MIT, with whom I had the pleasure to collaborate during my sabbatical at that institution. Motivated by problems in computer graphics dealing with the analysis of 3D point clouds, the idea was to use the graph as a coarse representation of the local smooth manifold structure underlying a point cloud. A key observation of Yue was that the graph does not need to stay the same throughout the neural network, and in fact it can and should be updated dynamically \u2014 hence the name of the method. The following figure from our paper illustrates why this might be useful in computer graphics problems:", "One of the limitations of DGCNNs was that the same space is used to construct the graph and the features on that graph. In a recent work with Anees Kazi from TUM and my postdoc Luca Cosmo, we proposed a new architecture called Differentiable Graph Module (DGM) [5] extending DGCNN by decoupling the graph and feature construction, as shown in the following figure:", "DGM showed impressive results when applied to problems from the medical domain, such as disease prediction from brain imaging data. In these tasks, we are provided with the electronic health records of multiple patients, including demographic features (such as age, sex, etc) and brain imaging features, and attempt to predict whether a patient suffers from a neurological disease. Previous works showed the application of GNNs to such tasks using diffusion on a \u201cpatient graph\u201d constructed by hand from the demographic features [6]. DGM offers the advantage of learning the graph, which possibly conveys insight into how certain features depend on each other for the specific diagnosis task. As a bonus, DGM also beat DGCNN in its game of point cloud classification, albeit only slightly.", "DGCNN and DGM bear conceptual similarity to a family of algorithms called manifold learning or non-linear dimensionality reduction, which were extremely popular in machine learning when I was a student in the 2000s, and are still used for data visualisation. The assumption underlying manifold learning methods is that of the data having an intrinsic low-dimensional structure. Though the data can be represented in a space of hundreds or even thousands of dimensions, it only has a few degrees of freedom, as shown in the following example:", "The purpose of manifold learning is to capture these degrees of freedom (by reconstructing the underlying \u201cmanifold\u201d, hence the name [7]) and reduce the dimensionality of the data to its intrinsic dimension. The important difference from linear dimensionality reduction such as PCA is that, due to the non-Euclidean structure of the data, there might be no possibility to recover the manifold by means of a linear projection [8]:", "Manifold learning algorithms vary in the way they approach the recovery of the \u201cmanifold\u201d, but share a common blueprint. First, they create a representation of the data, which is typically done by constructing a k-nearest neighbour graph capturing its local structure. Second, they compute a low-dimensional representation (embedding) of the data trying to preserve the structure of the original data. This is where most manifold learning methods differ. For example, Isomap [9] tries to preserve the graph geodesic distance, Locally Linear Embedding [10] finds a local representation of adjacent points, and Laplacian eigenmaps [11] use the eigenfunctions of the graph Laplacian operator as the low-dimensional embedding. This new representation \u201cflattens\u201d the original non-Euclidean structure into a Euclidean space that is easier to deal with. Third, once the representation is computed, a machine learning algorithm (typically clustering) is applied to it.", "One of the challenges is that the construction of the graph is decoupled from the ML algorithm, and sometimes delicate parameter tuning (e.g. the number of neighbours or the neighbourhood radius) is needed in order to figure out how to build the graph to make the downstream task work well. Perhaps a far more serious drawback of manifold learning algorithms is that data rarely presents itself as low-dimensional in its native form. When dealing with images, for example, various handcrafted feature extraction techniques had to be used as pre-processing steps.", "Graph deep learning offers a modern take on this process, by replacing this three-stage process outlined above with a single graph neural network. In dynamic graph CNNs or DGM, for instance, the construction of the graph and the learning are part of the same architecture:", "The appeal of this approach is the possibility to combine the treatment of individual data points and the space in which they reside in the same pipeline. In the example of images, one could use traditional CNNs to extract the visual features from each image and use a GNN to model the relations between them. This approach was used in the work of my PhD student Jan Svoboda: he proposed a graph-based regularisation layer (called PeerNet) for CNNs that allows to exchange information between multiple images [12]. PeerNets bear similarity to non-local means filters [13] in the way they aggregate information from multiple locations, with the main difference that the aggregation happens across multiple images rather than a single one. We showed that such a regularisation dramatically reduces the effect of adversarial perturbations to which standard CNNs are highly susceptible [14].", "There are many other interesting applications of latent graph learning. One is few-shot learning, where graph-based techniques can help generalise from a few examples. Few-shot learning is becoming increasingly important in computer vision where the cost of data labelling is significant [5]. Another field is biology, where one often observes experimentally expression levels of biomolecules such as proteins and tries to reconstruct their interaction and signalling networks [15]. Third problem is the analysis of physical systems where a graph can describe interactions between multiple objects [16]. In particular, high-energy physicists dealing with complex particle interactions have recently been showing keen interest in graph-based approaches [17]. Last but not least are problems in NLP, where graph neural networks can be seen as generalisations of the transformer architecture. Many of the mentioned problems also raise questions on incorporating priors on the graph structure, which is still largely open: for example, one may wish to force the graph to obey certain construction rules or be compatible with some statistical model [18].", "I believe that latent graph learning, while not entirely new, offers a new perspective on old problems. It is for sure an interesting setting of graph ML problems, providing a new playground for GNN researchers.", "[1] A slightly different but related class of methods seeks to decouple the graph provided as input from the computational graph used for message passing in graph neural networks, see e.g. J. Halcrow et al. Grale: Designing networks for graph learning (2020). arXiv:2007.12002. There are multiple reasons why one may wish to do it, one of which is breaking the bottlenecks related to the exponential growth of the neighbourhood size in some graphs, as shown by U. Alon and E. Yahav, On the bottleneck of graph neural networks and its practical implications (2020). arXiv:2006.05205.", "[2] Problems of reconstructing graphs underlying some data were considered in the signal processing context in the PhD thesis of Xiaowen Dong, in whose defence committee I took part in May 2014, just a few days before the birth of my son. X. Dong et al. Learning graphs from data: A signal representation perspective (2019), IEEE Signal Processing Magazine 36(3):44\u201363 presents a good summary of this line of work. A more recent incarnation of these approaches from the perspective of network games is the work of Y. Leng et al. Learning quadratic games on networks (2020). Proc. ICML, on whose PhD committee at MIT I was earlier this year.", "[3] C. Qi et al. PointNet: Deep learning on point sets for 3D classification and segmentation (2017), Proc. CVPR. PointNet is an architecture for deep learning on sets, where a shared function is applied to the representation of each point, and can be considered as a trivial case of a GNN applied to a graph with empty edge set.", "[4] Y. Wang et al. Dynamic graph CNN for learning on point clouds (2019). ACM Trans. Graphics 38(5):146. This paper has become quite popular in the computer graphics community and is often used as a baseline for point cloud methods. Ironically, it was rejected from SIGGRAPH in 2018 and was presented at the same conference only two years later after having gathered over 600 citations.", "[5] A. Kazi et al., Differentiable Graph Module (DGM) for graph convolutional networks (2020) arXiv:2002.04999. We show multiple applications, including medical imaging, 3D point cloud analysis, and few shot learning. See also our paper L. Cosmo et al. Latent patient network learning for automatic diagnosis (2020). Proc. MICCAI, focusing on a medical application of this method. Anees was a visiting PhD student in my group at Imperial College in 2019.", "[6] To the best of my knowledge, the first use of GNNs for brain disease prediction is by S. Parisot et al. Disease prediction using graph convolutional networks: application to autism spectrum disorder and Alzheimer\u2019s disease (2017). Proc. MICCAI. The key drawback of this approach was a handcrafted construction of the graph from demographic features.", "[7] Formally speaking, it is not a \u201cmanifold\u201d in the differential geometric sense of the term, since for example the local dimension can vary at different points. However, it is a convenient metaphor.", "[8] The more correct term is \u201cnon-Euclidean\u201d rather than \u201cnon-linear\u201d.", "[9] J. B. Tenenbaum et al., A global geometric framework for nonlinear dimensionality reduction (2000), Science 290:2319\u20132323. Introduced the Isomap algorithm that embeds the data manifold by trying to preserve the geodesic distances on it, approximated using a k-NN graph. Geodesic distances on the graph are the lengths of the shortest paths connecting any pair of points, computed by means of the Dijkstra algorithm. Endowed with such a distance metric, the dataset is considered as a (non-Euclidean) metric space. A configuration of points in a low-dimensional space whose pairwise Euclidean distances are equal to the graph geodesic distances is known as isometric embedding in metric geometry. Usually, isometric embeddings do not exist and one has to resort to an approximation that preserves the distances the most in some sense. One way of computing such an approximation is by means of multidimensional scaling (MDS) algorithms.", "[11] M, Belkin and P. Niyogi, Laplacian eigenmaps and spectral techniques for embedding and clustering (2001). Proc. NIPS.", "[12] J. Svoboda et al. PeerNets: Exploiting peer wisdom against adversarial attacks (2019), Proc. ICLR uses GNN module that aggregates information from multiple images to reduce the sensitivity of CNNs to adversarial perturbations of the input.", "[13] Non-local means is a non-linear image filtering technique introduced by A. Buades et al., A non-local algorithm for image denoising (2005), Proc. CVPR. It can be seen as a precursor to modern attention mechanisms used in deep learning. Non-local means itself is a variant of edge-preserving diffusion methods such as the Beltrami flow proposed by my PhD advisor Ron Kimmel in the paper R. Kimmel et al., From high energy physics to low level vision (1997), Proc. Scale-Space Theories in Computer Vision, or the bilateral filter from C. Tomasi and R. Manduchi, Bilateral filtering for gray and color images (1998). Proc. ICCV.", "[14] Adversarial perturbation is a carefully constructed input noise that significantly reduces the performance of CNNs e.g. in image classification. This phenomenon was described in C. Szegedy et al. Intriguing properties of neural networks (2014), Proc. ICLR, and resulted in multiple follow-up works that showed bizarre adversarial attacks as extreme as changing a single pixel (J. Su et al. One pixel attack for fooling deep neural networks (2017), arXiv:1710.08864) or data-independent \u201cuniversal\u201d perturbations (S. M. Moosavi-Dezfooli et al., Universal adversarial perturbations (2017), Proc. CVPR).", "[16] T. Kipf et al., Neural relational inference for interaction systems (2019). Proc. ICML. Recovers a graph \u201cexplaining\u201d the physics of a system by using a variational autoencoder, in which the latent vectors represent the underlying interaction graph and the decoder is a graph neural network.", "[17] The use of GNNs in high-energy physics is a fascinating topic worth a separate post. Together with my PhD student Federico Monti we have worked with the IceCube collaboration developing probably the first GNN-based approach for particle physics. Our paper N. Choma, F. Monti et al., Graph neural networks for IceCube signal classification (2018), Proc. ICMLA, where we used the MoNet architecture for astrophysical neutrino classification, got the best paper award. In a more recent work, S. R. Qasim et al., Learning representations of irregular particle-detector geometry with distance-weighted graph networks (2019), European Physical Journal C 79, used a variant of DGCNN similar to DGM called GravNet for particle reconstruction.", "[18] A somewhat related class of approaches are generative graph models, see e.g. Y. Li et al, Learning deep generative models of graphs (2018). arXiv:1803.03324. One of the applications is generating molecular graphs of chemical compounds that adhere to strict construction rules.", "[19] There are many more works on latent graph learning papers that have appeared in the past couple of years \u2014 if I omit some, this is because my goal is not to be exhaustive but rather to show a principle. I will refer to one additional work of L. Franceschi et al. Learning discrete structures for graph neural networks (2019). Proc. ICML, which also mentions the relation to Isomap and manifold learning techniques.", "I am grateful to Ben Chamberlain, Xiaowen Dong, Fabrizio Frasca, Anees Kazi, and Yue Wang for proof-reading this post, and to Gal Mishne for pointing to the origins of the Swiss roll. Interested in graph ML and Geometric Deep Learning? See my blog on Towards Data Science, subscribe to my posts, get Medium membership, or follow me on Twitter.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "DeepMind Professor of AI @Oxford. Serial startupper. ML for biochemistry, drug design, animal communication."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F99a25eeb677d&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmanifold-learning-2-99a25eeb677d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmanifold-learning-2-99a25eeb677d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmanifold-learning-2-99a25eeb677d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmanifold-learning-2-99a25eeb677d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----99a25eeb677d--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----99a25eeb677d--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://michael-bronstein.medium.com/?source=post_page-----99a25eeb677d--------------------------------", "anchor_text": ""}, {"url": "https://michael-bronstein.medium.com/?source=post_page-----99a25eeb677d--------------------------------", "anchor_text": "Michael Bronstein"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7b1129ddd572&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmanifold-learning-2-99a25eeb677d&user=Michael+Bronstein&userId=7b1129ddd572&source=post_page-7b1129ddd572----99a25eeb677d---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F99a25eeb677d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmanifold-learning-2-99a25eeb677d&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F99a25eeb677d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmanifold-learning-2-99a25eeb677d&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/graph-deep-learning/home", "anchor_text": "previous posts"}, {"url": "https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction", "anchor_text": "non-linear dimensionality reduction"}, {"url": "http://web.mit.edu/cocosci/isomap/datasets.html", "anchor_text": "supplementary materials"}, {"url": "https://en.wikipedia.org/wiki/Principal_component_analysis", "anchor_text": "PCA"}, {"url": "http://papers.nips.cc/paper/1332-mapping-a-manifold-of-perceptual-observations.pdf", "anchor_text": "coined by Joshua Tenenbaum"}, {"url": "https://towardsdatascience.com/transformers-are-graph-neural-networks-bca9f75412aa", "anchor_text": "generalisations of the transformer architecture"}, {"url": "https://arxiv.org/pdf/2007.12002.pdf", "anchor_text": "Grale: Designing networks for graph learning"}, {"url": "https://towardsdatascience.com/do-we-need-deep-graph-neural-networks-be62d3ec5c59", "anchor_text": "exponential growth"}, {"url": "https://arxiv.org/pdf/2006.05205.pdf", "anchor_text": "On the bottleneck of graph neural networks and its practical implications"}, {"url": "https://arxiv.org/pdf/1811.08790.pdf", "anchor_text": "Learning quadratic games on networks"}, {"url": "https://arxiv.org/pdf/1612.00593", "anchor_text": "PointNet: Deep learning on point sets for 3D classification and segmentation"}, {"url": "https://arxiv.org/pdf/1801.07829.pdf", "anchor_text": "Dynamic graph CNN for learning on point clouds"}, {"url": "https://arxiv.org/pdf/2002.04999.pdf", "anchor_text": "Differentiable Graph Module (DGM) for graph convolutional networks"}, {"url": "https://arxiv.org/pdf/2003.13620.pdf", "anchor_text": "Latent patient network learning for automatic diagnosis"}, {"url": "https://arxiv.org/pdf/1806.01738.pdf", "anchor_text": "Disease prediction using graph convolutional networks: application to autism spectrum disorder and Alzheimer\u2019s disease"}, {"url": "http://web.mit.edu/cocosci/Papers/sci_reprint.pdf", "anchor_text": "A global geometric framework for nonlinear dimensionality reduction"}, {"url": "https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm", "anchor_text": "Dijkstra algorithm"}, {"url": "https://en.wikipedia.org/wiki/Multidimensional_scaling", "anchor_text": "multidimensional scaling"}, {"url": "https://cs.nyu.edu/~roweis/lle/papers/lleintro.pdf", "anchor_text": "Nonlinear dimensionality reduction by locally linear embedding"}, {"url": "https://papers.nips.cc/paper/1961-laplacian-eigenmaps-and-spectral-techniques-for-embedding-and-clustering.pdf", "anchor_text": "Laplacian eigenmaps and spectral techniques for embedding and clustering"}, {"url": "https://arxiv.org/pdf/1806.00088.pdf", "anchor_text": "PeerNets: Exploiting peer wisdom against adversarial attacks"}, {"url": "http://stanford.edu/class/ee367/reading/A%20non-local%20algorithm%20for%20image%20denoising.pdf", "anchor_text": "A non-local algorithm for image denoising"}, {"url": "http://www.cs.technion.ac.il/~ron/PAPERS/scale_space97_belt.pdf", "anchor_text": "From high energy physics to low level vision"}, {"url": "http://www.cse.ucsc.edu/~manduchi/Papers/ICCV98.pdf", "anchor_text": "Bilateral filtering for gray and color images"}, {"url": "https://arxiv.org/pdf/1312.6199", "anchor_text": "Intriguing properties of neural networks"}, {"url": "https://arxiv.org/pdf/1710.08864", "anchor_text": "One pixel attack for fooling deep neural networks"}, {"url": "https://arxiv.org/abs/1610.08401", "anchor_text": "Universal adversarial perturbations"}, {"url": "https://arxiv.org/pdf/1904.10098.pdf", "anchor_text": "DAG-GNN: DAG structure learning with graph neural networks"}, {"url": "https://arxiv.org/pdf/1802.04687.pdf", "anchor_text": "Neural relational inference for interaction systems"}, {"url": "https://en.wikipedia.org/wiki/IceCube_Neutrino_Observatory", "anchor_text": "IceCube"}, {"url": "https://arxiv.org/abs/1809.06166", "anchor_text": "Graph neural networks for IceCube signal classification"}, {"url": "https://link.springer.com/article/10.1140/epjc/s10052-019-7113-9", "anchor_text": "Learning representations of irregular particle-detector geometry with distance-weighted graph networks"}, {"url": "https://arxiv.org/pdf/1803.03324.pdf", "anchor_text": "Learning deep generative models of graphs"}, {"url": "http://proceedings.mlr.press/v97/franceschi19a/franceschi19a.pdf", "anchor_text": "Learning discrete structures for graph neural networks"}, {"url": "https://towardsdatascience.com/graph-deep-learning/home", "anchor_text": "blog"}, {"url": "https://michael-bronstein.medium.com/subscribe", "anchor_text": "subscribe"}, {"url": "https://michael-bronstein.medium.com/membership", "anchor_text": "Medium membership"}, {"url": "https://twitter.com/mmbronstein", "anchor_text": "Twitter"}, {"url": "https://medium.com/tag/graph-neural-networks?source=post_page-----99a25eeb677d---------------graph_neural_networks-----------------", "anchor_text": "Graph Neural Networks"}, {"url": "https://medium.com/tag/manifold-learning?source=post_page-----99a25eeb677d---------------manifold_learning-----------------", "anchor_text": "Manifold Learning"}, {"url": "https://medium.com/tag/dimensionality-reduction?source=post_page-----99a25eeb677d---------------dimensionality_reduction-----------------", "anchor_text": "Dimensionality Reduction"}, {"url": "https://medium.com/tag/computer-vision?source=post_page-----99a25eeb677d---------------computer_vision-----------------", "anchor_text": "Computer Vision"}, {"url": "https://medium.com/tag/editors-pick?source=post_page-----99a25eeb677d---------------editors_pick-----------------", "anchor_text": "Editors Pick"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F99a25eeb677d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmanifold-learning-2-99a25eeb677d&user=Michael+Bronstein&userId=7b1129ddd572&source=-----99a25eeb677d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F99a25eeb677d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmanifold-learning-2-99a25eeb677d&user=Michael+Bronstein&userId=7b1129ddd572&source=-----99a25eeb677d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F99a25eeb677d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmanifold-learning-2-99a25eeb677d&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----99a25eeb677d--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F99a25eeb677d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmanifold-learning-2-99a25eeb677d&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----99a25eeb677d---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----99a25eeb677d--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----99a25eeb677d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----99a25eeb677d--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----99a25eeb677d--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----99a25eeb677d--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----99a25eeb677d--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----99a25eeb677d--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----99a25eeb677d--------------------------------", "anchor_text": ""}, {"url": "https://michael-bronstein.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://michael-bronstein.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Michael Bronstein"}, {"url": "https://michael-bronstein.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "9.7K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7b1129ddd572&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmanifold-learning-2-99a25eeb677d&user=Michael+Bronstein&userId=7b1129ddd572&source=post_page-7b1129ddd572--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F936a58357100&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmanifold-learning-2-99a25eeb677d&newsletterV3=7b1129ddd572&newsletterV3Id=936a58357100&user=Michael+Bronstein&userId=7b1129ddd572&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}