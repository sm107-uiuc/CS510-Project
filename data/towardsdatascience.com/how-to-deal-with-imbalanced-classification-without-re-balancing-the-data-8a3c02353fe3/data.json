{"url": "https://towardsdatascience.com/how-to-deal-with-imbalanced-classification-without-re-balancing-the-data-8a3c02353fe3", "time": 1683011834.8176682, "path": "towardsdatascience.com/how-to-deal-with-imbalanced-classification-without-re-balancing-the-data-8a3c02353fe3/", "webpage": {"metadata": {"title": "How To Deal With Imbalanced Classification, Without Re-balancing the Data | by David B Rosen (PhD) | Towards Data Science", "h1": "How To Deal With Imbalanced Classification, Without Re-balancing the Data", "description": "How To Deal With Imbalanced Classification, Without Re-balancing the Data: Before considering oversampling your skewed data, try adjusting your classification decision threshold (simple Python code shown in its entirety)"}, "outgoing_paragraph_urls": [{"url": "https://www.kaggle.com/mlg-ulb/creditcardfraud", "anchor_text": "credit card fraud identification data set", "paragraph_index": 2}, {"url": "https://www3.nd.edu/~dial/publications/dalpozzolo2015calibrating.pdf", "anchor_text": "here", "paragraph_index": 15}, {"url": "https://stats.stackexchange.com/questions/476911/how-are-artificially-balanced-datasets-corrected-for", "anchor_text": "here", "paragraph_index": 15}, {"url": "https://documentation.sas.com/doc/en/emxndg/15.1/p1vqpbjwoo4bv7n1sw77e0z64xxs.htm", "anchor_text": "here", "paragraph_index": 15}, {"url": "https://medium.com/lumiata/cross-validation-for-imbalanced-datasets-9d203ba47e8", "anchor_text": "Cross-Validation for Imbalanced Datasets", "paragraph_index": 16}, {"url": "https://imbalanced-learn.org/dev/auto_examples/applications/plot_over_sampling_benchmark_lfw.html", "anchor_text": "this example", "paragraph_index": 16}, {"url": "https://dabruro.medium.com/to-panos-v-s-question-how-to-implement-this-with-a-gridsearchcv-though-2ab2437784c3", "anchor_text": "my reply", "paragraph_index": 16}, {"url": "http://linkedin.com/in/rosen1/", "anchor_text": "linkedin.com/in/rosen1/", "paragraph_index": 24}], "all_paragraphs": ["In machine learning, when building a classification model with data having far more instances of one class than another, the initial default classifier is often unsatisfactory because it classifies almost every case as the majority class. Many articles show you how you could use oversampling (e.g. SMOTE) or sometimes undersampling or simply class-based sample weighting to retrain the model on \u201crebalanced\u201d data, but this isn\u2019t always necessary. Here we aim instead to show how much you can do without balancing the data or retraining the model.", "We do this by simply adjusting the the threshold for which we say \u201cClass 1\u201d when the model\u2019s predicted probability of Class 1 is above it in two-class classification, rather than na\u00efvely using the default classification rule which chooses which ever class is predicted to be most probable (probability threshold of 0.5). We will see how this gives you the flexibility to make any desired trade-off between false positive and false negative classifications while avoiding problems created by rebalancing the data.", "We will use the credit card fraud identification data set from Kaggle to illustrate. Each row of the data set represents a credit card transaction, with the target variable Class==0 indicating a legitimate transaction and Class==1 indicating that the transaction turned out to be a fraud. There are 284,807 transactions, of which only 492 (0.173%) are frauds \u2014 very imbalanced indeed.", "We will use a gradient boosting classifier because these often give good results. Specifically Scikit-Learn\u2019s new HistGradientBoostingClassifier because it is much faster than their original GradientBoostingClassifier when the data set is relatively large like this one.", "First let\u2019s import some libraries and read in the data set.", "V1 through V28 (from a principal components analysis) and the transaction Amount are the features, which are all numeric and there is no missing data. Because we are only using a tree-based classifier, we don\u2019t need to standardize or normalize the features.", "We will now train the model after splitting the data into train and test sets. This took about half a minute on my laptop. We use the n_iter_no_change to stop the training early if the performance on a validation subset starts to deteriorate due to overfitting. I separately did a little bit of hyperparameter tuning to choose the learning_rate and max_leaf_nodes, but this is not the focus of the present article.", "Now we apply this model to the test data as the default hard-classifier, predicting 0 or 1 for each transaction. We are implicitly applying decision threshold 0.5 to the model\u2019s continuous probability prediction as a soft-classifier. When the probability prediction is over 0.5 we say \u201c1\u201d and when it is under 0.5 we say \u201c0\u201d.", "We also print the confusion matrix for the result, considering Class 1 (fraud) to be the \u201cpositive\u201d class, by convention because it is the rarer class. The confusion matrix shows the number of True Negatives, False Positives, False Negatives, and True Positives. The normalized confusion matrix rates (e.g. FNR = False Negative Rate) are included as percentages in parentheses.", "We see that the Recall for Class 1 (a.k.a. Sensitivity or True Positive Rate shown as TPR above) is only 71.62%, meaning that 71.62% of the true frauds are correctly identified as frauds and thus denied. So 28.38% of the true frauds are unfortunately approved as if legitimate.", "Especially with imbalanced data (or generally any time false positives and false negatives may have different consequences), it\u2019s important not to limit ourselves to using the default implicit classification decision threshold of 0.5, as we did above by using \u201c.predict( )\u201d. We want to improve the Recall of class 1 (the TPR) to reduce our fraud losses (reduce false negatives). To do this, we can reduce the threshold for which we say \u201cClass 1\u201d when we predict a probability above the threshold. This way we say \u201cClass 1\u201d for a wider range of predicted probabilities. Such strategies are known as threshold-moving.", "Ultimately it is a business decision to what extent we want to reduce these False Negatives since as a trade-off the number of False Positives (true legitimate transactions rejected as frauds) will inevitably increase as we adjust the threshold that we apply to the model\u2019s probability prediction (obtained from \u201c.predict_proba( )\u201d instead of \u201c.predict( )\u201d).", "To elucidate this trade-off and help us choose a threshold, we plot the False Positive Rate and False Negative Rate against the Threshold. This is a variant of the Receiver Operating Characteristic (ROC) curve, but emphasizing the threshold.", "Although there exist some rules of thumb or proposed metrics for choosing the optimal threshold, ultimately it depends solely on the cost to the business of false negatives vs. false positives. For example, looking at the plot above, we might choose to apply a threshold of 0.00035 (vertical green line has been added) as follows.", "We have reduced our False Negative Rate from 28.38% down to 9.46% (i.e. identified and denied 90.54% of our true frauds as our new Recall or Sensitivity or True Positive Rate or TPR), while our False Positive Rate (FPR) has increased from 0.01% to 5.75% (i.e. still approved 94.25% of our legitimate transactions). It might well be worth the trade-off to us of denying about 6% of the legitimate transactions as the price we pay in order to approve only less than 10% of the fraudulent transactions, down from a very costly 28% of the frauds when we were using the default hard-classifier with an implicit classification decision threshold of 0.5.", "One reason to avoid \u201cbalancing\u201d your imbalanced training data is that such methods bias/distort the resulting trained model\u2019s probability predictions so that these become miscalibrated, by systematically increasing the model\u2019s predicted probabilities of the original minority class, and are thus reduced to being merely relative ordinal discriminant scores or decision functions or confidence scores rather than being potentially accurate predicted class probabilities in the original (\u201cimbalanced\u201d) train and test set and future data that the classifier may make predictions on. In the event that such rebalancing for training is truly needed, but numerically-accurate probability predictions are still desired, one would then have to recalibrate the predicted probabilities to a data set having the original/imbalanced class proportions, or apply an appropriate correction to the predicted probabilities from the balanced model \u2014 see here, here, or here.", "Another problem with balancing your data by oversampling (as opposed to class-dependent instance weighting which doesn\u2019t have this problem) is that it biases na\u00efve cross-validation, potentially leading to excessive overfitting that is not detected in the cross-validation. If you perform oversampling on the full training set, then in cross-validation, each time the data gets split into a \u201cfold\u201d subset, there may be instances in one fold that are duplicates of, or were generated from, instances in another fold. Thus the folds are not truly independent as cross-validation assumes \u2014 there is data \u201cbleed\u201d or \u201cleakage\u201d. For example see Cross-Validation for Imbalanced Datasets which describes how you could re-implement cross-validation correctly for this situation by performing the oversampling only on each set of training folds, inside the cross-validation loop, as in this example. Alternatively you could perform the oversampling in a pipeline created by imblearn.pipeline.make_pipeline(), which takes care of this by applying the oversampling inside the cross-validation loop only to the training folds, never to the testing fold. In scikit-learn, at least for the case of oversampling by simple instance duplication/resampling (not necessarily SMOTE or ADASYN), the issue can alternatively be worked around by using model_selection.GroupKFold for cross-validation, which groups the instances according to a selected group identifier that has the same value for all duplicates of a given instance \u2014 see my reply to a response to the aforelinked article.", "Instead of na\u00efvely or implicitly applying a default threshold of 0.5, or immediately re-training using re-balanced training data, we can try using the original model (trained on the original \u201cimbalanced\u201d data set) and simply plot the trade-off between false positives and false negatives to choose a threshold that may produce a desirable business result.", "Please leave a response if you have questions or comments, or read others\u2019 responses and my replies: tap the Response speech bubble \ud83d\udde8\ufe0f (actually a round one) down below next to the Clap (applause) icon.", "The average probability prediction produced by your model will approximate the proportion of training instances that are class 1, because this is the average actual value of the target class variable (which has values of 0 and 1). The same is true in regression: the average predicted value of the target variable is expected to approximate the average actual value of the target variable. When the data is highly imbalanced and class 1 is the minority class, this average probability prediction will be much less than 0.5 and the vast majority of predictions of the probability of class 1 will be less than 0.5 and therefore classified as class 0 (majority class). If you rebalance the training data, the average predicted probability increases to 0.5 and so then many instances will be above a default threshold of 0.5 as well as many below \u2014 the predicted classes will be more balanced. So instead of reducing the threshold so that more often the probability predictions are above it and give class 1 (minority class), rebalancing increases the the predicted probabilities so that more often the probability predictions will be above the default threshold of 0.5 and give class 1.", "If you want to get similar (not identical) results to those of rebalancing, without actually rebalancing or reweighting the data, you could try simply setting the threshold equal to the average or median value of the model\u2019s predicted probability of class 1. But of course this won\u2019t necessarily be the threshold that provides the optimal balance between false positives and false negatives for your particular business problem, nor will rebalancing your data and using a threshold of 0.5.", "It is possible that there are situations and models where rebalancing the data will materially improve the model beyond merely moving the average predicted probability to equal the default threshold of 0.5. But the mere fact that the model chooses the majority class the vast majority of the time when using the default threshold of 0.5 does not in itself support a claim that rebalancing the data will accomplish anything beyond making the average probability prediction equal to the threshold. If you want the average probability prediction to be equal to the threshold, you could simply set the threshold equal to the average probability prediction, without modifying or reweighting your training data in order to distort the probability predictions.", "If your classifier doesn\u2019t have a predict_proba method, e.g. support vector classifiers, you can just as well use its decision_function method in its place, producing an ordinal discriminant score or confidence score model output which can be thresholded in the same way even if it is not interpretable as a probability prediction between 0 and 1. Depending on how a particular classifier model calculates its confidence scores for the two classes, it might sometimes be necessary, instead of applying the threshold directly to the confidence score of class 1 (which we did above as the predicted probability of class 1 because the predicted probability of class 0 is simply one minus that), to alternatively apply the threshold to the difference between the confidence score for class 0 and that for class 1, with the default threshold being 0 for the case where the cost of a false positive is assumed to be the same as that of a false negative. This article assumes a two-class classification problem.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Lead Data Scientist for Automated Credit Approval at (not speaking for) IBM Financing. linkedin.com/in/rosen1/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F8a3c02353fe3&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-deal-with-imbalanced-classification-without-re-balancing-the-data-8a3c02353fe3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-deal-with-imbalanced-classification-without-re-balancing-the-data-8a3c02353fe3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-deal-with-imbalanced-classification-without-re-balancing-the-data-8a3c02353fe3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-deal-with-imbalanced-classification-without-re-balancing-the-data-8a3c02353fe3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----8a3c02353fe3--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8a3c02353fe3--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://dabruro.medium.com/?source=post_page-----8a3c02353fe3--------------------------------", "anchor_text": ""}, {"url": "https://dabruro.medium.com/?source=post_page-----8a3c02353fe3--------------------------------", "anchor_text": "David B Rosen (PhD)"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3100241e7f2c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-deal-with-imbalanced-classification-without-re-balancing-the-data-8a3c02353fe3&user=David+B+Rosen+%28PhD%29&userId=3100241e7f2c&source=post_page-3100241e7f2c----8a3c02353fe3---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8a3c02353fe3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-deal-with-imbalanced-classification-without-re-balancing-the-data-8a3c02353fe3&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8a3c02353fe3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-deal-with-imbalanced-classification-without-re-balancing-the-data-8a3c02353fe3&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@miracleday?utm_source=medium&utm_medium=referral", "anchor_text": "Elena Mozhvilo"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://www.kaggle.com/mlg-ulb/creditcardfraud", "anchor_text": "credit card fraud identification data set"}, {"url": "https://www3.nd.edu/~dial/publications/dalpozzolo2015calibrating.pdf", "anchor_text": "here"}, {"url": "https://stats.stackexchange.com/questions/476911/how-are-artificially-balanced-datasets-corrected-for", "anchor_text": "here"}, {"url": "https://documentation.sas.com/doc/en/emxndg/15.1/p1vqpbjwoo4bv7n1sw77e0z64xxs.htm", "anchor_text": "here"}, {"url": "https://medium.com/lumiata/cross-validation-for-imbalanced-datasets-9d203ba47e8", "anchor_text": "Cross-Validation for Imbalanced Datasets"}, {"url": "https://imbalanced-learn.org/dev/auto_examples/applications/plot_over_sampling_benchmark_lfw.html", "anchor_text": "this example"}, {"url": "https://dabruro.medium.com/to-panos-v-s-question-how-to-implement-this-with-a-gridsearchcv-though-2ab2437784c3", "anchor_text": "my reply"}, {"url": "https://medium.com/tag/imbalanced-data?source=post_page-----8a3c02353fe3---------------imbalanced_data-----------------", "anchor_text": "Imbalanced Data"}, {"url": "https://medium.com/tag/threshold-moving?source=post_page-----8a3c02353fe3---------------threshold_moving-----------------", "anchor_text": "Threshold Moving"}, {"url": "https://medium.com/tag/confidence-score?source=post_page-----8a3c02353fe3---------------confidence_score-----------------", "anchor_text": "Confidence Score"}, {"url": "https://medium.com/tag/oversampling?source=post_page-----8a3c02353fe3---------------oversampling-----------------", "anchor_text": "Oversampling"}, {"url": "https://medium.com/tag/class-probability?source=post_page-----8a3c02353fe3---------------class_probability-----------------", "anchor_text": "Class Probability"}, {"url": "https://creativecommons.org/licenses/by-nd/4.0/", "anchor_text": "Some rights reserved"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8a3c02353fe3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-deal-with-imbalanced-classification-without-re-balancing-the-data-8a3c02353fe3&user=David+B+Rosen+%28PhD%29&userId=3100241e7f2c&source=-----8a3c02353fe3---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8a3c02353fe3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-deal-with-imbalanced-classification-without-re-balancing-the-data-8a3c02353fe3&user=David+B+Rosen+%28PhD%29&userId=3100241e7f2c&source=-----8a3c02353fe3---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8a3c02353fe3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-deal-with-imbalanced-classification-without-re-balancing-the-data-8a3c02353fe3&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8a3c02353fe3--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F8a3c02353fe3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-deal-with-imbalanced-classification-without-re-balancing-the-data-8a3c02353fe3&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----8a3c02353fe3---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----8a3c02353fe3--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----8a3c02353fe3--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----8a3c02353fe3--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----8a3c02353fe3--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----8a3c02353fe3--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----8a3c02353fe3--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----8a3c02353fe3--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----8a3c02353fe3--------------------------------", "anchor_text": ""}, {"url": "https://dabruro.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://dabruro.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "David B Rosen (PhD)"}, {"url": "https://dabruro.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "218 Followers"}, {"url": "http://linkedin.com/in/rosen1/", "anchor_text": "linkedin.com/in/rosen1/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3100241e7f2c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-deal-with-imbalanced-classification-without-re-balancing-the-data-8a3c02353fe3&user=David+B+Rosen+%28PhD%29&userId=3100241e7f2c&source=post_page-3100241e7f2c--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fdf05d045f708&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-deal-with-imbalanced-classification-without-re-balancing-the-data-8a3c02353fe3&newsletterV3=3100241e7f2c&newsletterV3Id=df05d045f708&user=David+B+Rosen+%28PhD%29&userId=3100241e7f2c&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}