{"url": "https://towardsdatascience.com/building-a-turing-machine-with-reinforcement-learning-9d06a4f0ce6", "time": 1682995864.667166, "path": "towardsdatascience.com/building-a-turing-machine-with-reinforcement-learning-9d06a4f0ce6/", "webpage": {"metadata": {"title": "Building a Turing Machine with Reinforcement Learning | by Moritz Kirschte | Towards Data Science", "h1": "Building a Turing Machine with Reinforcement Learning", "description": "a human-readable solution of an arbitrary optimization problem through Reinforcement Learning"}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/P_versus_NP_problem", "anchor_text": "P=NP-Problem", "paragraph_index": 3}, {"url": "https://towardsdatascience.com/what-to-expect-from-reinforcement-learning-a22e8c16f40c", "anchor_text": "Reinforcement Learning", "paragraph_index": 7}, {"url": "https://gist.github.com/moritzmoritz98/b9a4d7159cda1514192f632400ad4032", "anchor_text": "my gist", "paragraph_index": 11}, {"url": "https://arxiv.org/abs/1410.5401", "anchor_text": "Neural Turing Machines", "paragraph_index": 13}, {"url": "https://en.wikipedia.org/wiki/Halting_problem", "anchor_text": "Hilbert\u2019s halting problem", "paragraph_index": 17}, {"url": "https://github.com/moritzmoritz98/rlopt/blob/master/rlopt/rl_sorting.py#L187", "anchor_text": "reconstructed ready-to-use lambda function", "paragraph_index": 48}, {"url": "http://linkedin.com/in/kirschte", "anchor_text": "http://linkedin.com/in/kirschte", "paragraph_index": 52}], "all_paragraphs": ["I\u2019ll be honest with you, there is no such thing in computer science which is more boring than yet another sorting algorithm. However, this time sorting is just a representation of a universal Turing machine self-learned with an advanced Reinforcement Learning framework.", "As a matter of fact, coming up with an efficient optimization algorithm takes a lot of time, resources and efforts. Moreover, you never know if there is still a more efficient version. Yet until the 1960s before the invention of Quicksort, sorting was known as an O(n\u00b2)-problem and took a lot of time. Not even to mention the hilarious race of optimizing matrix multiplications as presented in the graph below\u2026", "This is where Reinforcement Learning comes into place: It is not just able to act as a universal heuristic toolkit moreover it could serve as a way of figuring out a mathematically-proven new algorithm in the optimization domain.", "In the following, I\u2019ll show you exactly the latter with the example of sorting. However, please feel free to apply this framework for your own needs, and perhaps you\u2019re able to proof by contradiction the P=NP-Problem through solving an NP-complete problem in polynomial time: It\u2019d be a pleasure for me to get a part of that $1,000,000 price. ;)", "Optimization takes place in any industry. Essentially there is no place where someone couldn\u2019t optimize something.", "However, the crucial part is that given the needed data most of the time we not only don\u2019t know a suitable algorithm to optimize a certain problem moreover we are relying on an intense thinking process combined with good ideas and a long-breathed try-and-error procedure.", "Wouldn\u2019t it be nice to leave the optimization to a universal framework, so that defining an objective (e.g. an optimal strategy) is enough? Requirements to such a system would yield to:", "By making use of Reinforcement Learning \u2014 one latest advance in Machine Learning \u2014 we should be able to meet those requirements. See, if we\u2019ll get to that later in this article. ;)", "For illustration purpose, I chose to learn a sorting algorithm because it's easy to verify. However, the following framework available on my GitHub could be used for other optimization techniques as well:", "In the learning phase, we aim to learn an algorithm solving the sorting objective. For that, I chose the tabular Q-learning algorithm as one specificity of Reinforcement Learning due to its easy notation and interpretability.", "For those of you how are not familiar with tabular Q-learning, essentially we\u2019re spanning actions on the columns and states in the rows where each cell determines the expected future reward in that state by taking its action. The learning process (also called Bellman backup) is described in the picture below.", "In case of further interest, take a look at my gist about how tabular Q-learning is functioning.", "In Reinforcement Learning and so in tabular Q-Learning actions, states and rewards are to be defined. Since we aim for a generalized algorithm we have to take special care on how to choose them.", "In essence, we\u2019re designing a multi-head (two heads: I and J) Turing machine M=\u27e8 Q, \u0393, s, b, F, \u03b4 \u27e9 processing on its tape a to-be-sorted list L. In contrast to existing techniques like Alex Graves\u2019 Neural Turing Machines in this work, I used a much simpler approach with the price of less customizability but instead with higher interpretability.", "The states have to be independent of the tape, therefore, I refer to them as the RL-dependent variables.", "In detail, the first state is binary, whereas the next ones are ternary and the last one 6-nary hence they could be represented by one boolean, three trileans, and one hexalean resulting in 2\u00b9\u00d73\u00b3\u00d76\u00b9=324 different states. Quite low-dimensional actually but still enough to successfully learn a sorting algorithm.", "In the code I used the following alphabet:TERMINATE,INCI,INCJ,RESETI,RESETJ,SWAP (values at I and J).", "There are basically two heads on our Turing machine positioned at I and J. By increasing one of them or resetting them to zero the machine has the ability to move within the tape. Swapping is the only tape-modifying action and terminating simulates a halting signal by the machine. Funnily, Hilbert\u2019s halting problem occurred during one of my experiments, too.", "Note that, one could separate the tape movement with the actual program however, I personally decided against this formalism due to practical considerations. Moreover, the \u2018TERMINATE\u2019 action is a workaround too in order to ensure halting.", "Essentially, just \u2018SWAP\u2019 and \u2018NOOP\u2019 should be part of \u0393.", "No blank symbol included due to simplicity however, one could think of adding a \u2018NOOP\u2019 operation.", "Those are learned by the Reinforcement Learning algorithm.", "Learned with the algorithm: Every non-final state from Q paired with an action from \u0393 has a deterministic result consisting of a next state landing in by taking an action from \u0393 together with a tape movement (i.e. one of INCI, INCJ, RESETI, RESETJ).", "Later, I\u2019ll visualize this transition table when we\u2019ll discuss the results.", "Although not part of a Turing machine, the reward is needed to learn a sorting behavior with Reinforcement Learning. For that, I decided to make the reward function as sparse and simple as possible to leave enough free space for the algorithm to unfold itself:", "Even though I didn\u2019t specify that out-of-bounds access is prohibited the program learned that on its own (a useless action decreases the seen reward).", "Also note, that in this scenario a sparse reward has its opportunity with respect to the final performance however, it significantly increases the time to learn especially when dealing with random exploration techniques. Instead by applying a histogram exploration, each state is enforced with a bonus term depending on its novelty to the algorithm. This behavior removes the sparsity of the reward function in the beginning phase as well as increases the chance of reaching a global maximum.", "After making use of this advanced histogram exploration technique, the algorithm is able to solve the list in a few 1k steps nonetheless, it still needs 5M steps to be certain and optimal in its outcome.", "One small notice on the input data: I generated all possible permutations of list length between three and six (870 distinctive\u2019s) to mitigate any thinkable overfitting issues.", "Congratulations! You made it through the tough technical part. Now you wanna see results, right?", "Okay, you got me. The table doesn\u2019t look brilliant yet, right?", "Nonetheless, it's worth mentioning what happened here. Basically, I learned the (324,6)-Q-table over 5,000,000 steps and \u201ctested\u201d it by predicting all 870 distinctive permutations of a list (length: 3 to 6) plus 5,040 permutations of list length 7 and collected during execution what states are actually used and what kind of actions are taken in that states.", "Surprisingly, there aren\u2019t many unique states (essentially 20) and moreover, the algorithm does generalize well to the 5,040 7th-list-permutations without any error.", "I\u2019d really like to emphasize that point again: With just 5M steps and training data of a small subset of all infinite possible lists combined with an easy hand-crafted reward function, we were able to teach an algorithm or a universe Turing machine respectively how to sort.", "Could you now imagine how easily one could advance this to other optimization dilemmas?", "However, we\u2019re still not at the destination of our journey yet unless you\u2019re able to read and actively understand our learned algorithm from those 20 lines or eventually from below\u2019s GraphViz.", "In the algorithmic description, I left off the final states set and the transition function as it\u2019s learned by our Q-learning. Now we are able to define our Turing machine altogether.", "First, the final states set is simple, it\u2019s state 16 as seen in the transition graph.", "Second, our transition function gets a little bit trickier as a more readable version than the formally correct from above\u2019s graph and table is intended. Despite being not a valid transition function anymore, I\u2019m really happy to actually shortened it to 4 distinct situation in which a question mark represents irrelevancy.", "Finally, from the simplified transition table, it is quite intuitive to actually code it statically: Our main goal of this post!", "To be honest, until now I have no clue if Reinforcement Learning was in this case actually able to find a new sorting algorithm or just a very unpopular one. About the only thing I am certain is that it is actually sorting any arbitrary list a 100%-correctly!", "That said, I\u2019d really appreciate it, if you could leave a note about what kind of algorithm that looks like to you!", "As a result of the limited action space, learning something better than O(n\u00b2) is per definition implausible. Nonetheless, the following algorithmic complexities could be obtained.", "Non-simplified / raw version w.r.t. the number of needed steps:", "On the simplified version however it\u2019s faster and (surprisingly) constant:", "Note, that by splitting the tape movements (i.e. I0, J0, I++, J++) from the actual algorithmic-related action alphabet (i.e. SWAP, TERMINATE, NOOP) one could improve the algorithmic performance since with an action most of the time also a movement on the tape is required and vice versa. However, I\u2019ll leave that for future work.", "Unfortunately, my RLSort algorithm is not as fast as bubble sort probably due to overhead and the matter of a fact that for-loops are better optimized on a CPU than a while-loop with an abort criterion. Additionally, the lower performance bound is with O(n\u00b2) not competitive with a quick O(n)-validation check: He is doing a lot of nonsense with an already sorted list!", "I really like to encourage you to try this out and apply this framework for your own ideas. In case you\u2019re optimizing on something similar to a list, it\u2019s just three simple steps and a few minutes of your CPU:", "You won\u2019t just get the result of an optimized list of your training data, you\u2019ll also receive a reconstructed ready-to-use lambda function!", "Of course, please let me know about your use case! I\u2019m really interested in hearing your unique story with my framework. :)", "Below you\u2019ll find an example call of RLSort sorting [6 4 3 7 0] clueing about how the learned sorting algorithm works in detail: Have fun with the riddle!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "AI Researcher @novomind | Host & Public Speaker @meetup.ai | AGI / RL Enthusiast | Ambassador of freedom, awardee @FNF | http://linkedin.com/in/kirschte"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F9d06a4f0ce6&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-turing-machine-with-reinforcement-learning-9d06a4f0ce6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-turing-machine-with-reinforcement-learning-9d06a4f0ce6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-turing-machine-with-reinforcement-learning-9d06a4f0ce6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-turing-machine-with-reinforcement-learning-9d06a4f0ce6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----9d06a4f0ce6--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----9d06a4f0ce6--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@kirschte?source=post_page-----9d06a4f0ce6--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@kirschte?source=post_page-----9d06a4f0ce6--------------------------------", "anchor_text": "Moritz Kirschte"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe9624bd27418&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-turing-machine-with-reinforcement-learning-9d06a4f0ce6&user=Moritz+Kirschte&userId=e9624bd27418&source=post_page-e9624bd27418----9d06a4f0ce6---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9d06a4f0ce6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-turing-machine-with-reinforcement-learning-9d06a4f0ce6&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9d06a4f0ce6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-turing-machine-with-reinforcement-learning-9d06a4f0ce6&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.acm.org/media-center/2019/march/turing-award-2018", "anchor_text": "Fathers of Deep Learning"}, {"url": "https://en.wikipedia.org/wiki/P_versus_NP_problem", "anchor_text": "P=NP-Problem"}, {"url": "https://towardsdatascience.com/what-to-expect-from-reinforcement-learning-a22e8c16f40c", "anchor_text": "Reinforcement Learning"}, {"url": "https://www.npr.org/2018/11/21/660168325/optimized-prime-how-ai-and-anticipation-power-amazons-1-hour-deliveries?t=1554890603422", "anchor_text": "anticipatory shipping"}, {"url": "https://medium.com/@RemiStudios/artificial-intelligence-for-inventory-management-c8a9c0c2a694", "anchor_text": "inventory management"}, {"url": "https://www.youtube.com/watch?v=gdwWrf9roDU", "anchor_text": "efficient trucking"}, {"url": "https://www.youtube.com/watch?v=Vv-tYg03xlU", "anchor_text": "a fully-utilized uber/cab/bus drive"}, {"url": "https://inbuildingtech.com/ai-ml/ai-ml-smart-building/", "anchor_text": "smart building"}, {"url": "https://arxiv.org/pdf/1901.04693.pdf", "anchor_text": "energy management"}, {"url": "https://papers.nips.cc/paper/1073-improving-elevator-performance-using-reinforcement-learning.pdf", "anchor_text": "elevator scheduling"}, {"url": "https://blog.acolyer.org/2017/09/27/an-efficient-bandit-algorithm-for-real-time-multivariate-optimization/", "anchor_text": "UX/I optimization"}, {"url": "https://www.youtube.com/watch?v=kY-BCNHd_dM", "anchor_text": "movie recommendations"}, {"url": "https://arxiv.org/pdf/1803.09967.pdf", "anchor_text": "dynamic pricing"}, {"url": "http://axon.cs.byu.edu/papers/Spencer.CEC10.pdf", "anchor_text": "sorting"}, {"url": "https://arxiv.org/abs/1611.09940", "anchor_text": "neural combinatorial optimization (e.g. traveling salesman)"}, {"url": "https://arxiv.org/abs/1807.08518", "anchor_text": "neural Turing machines"}, {"url": "https://deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-40/", "anchor_text": "data centre cooling"}, {"url": "https://github.com/moritzmoritz98/rlopt", "anchor_text": "moritzmoritz98/rlopthuman-readable solution of an arbitrary optimization problem through Reinforcement Learning \u2014 moritzmoritz98/rloptgithub.com"}, {"url": "https://gist.github.com/moritzmoritz98/b9a4d7159cda1514192f632400ad4032", "anchor_text": "my gist"}, {"url": "https://arxiv.org/abs/1410.5401", "anchor_text": "Neural Turing Machines"}, {"url": "https://commons.wikimedia.org/wiki/File:MonusTuringMachine_svg.svg", "anchor_text": "Turing machine to compute the truncated subtraction (\u201cmonus\u201d), after John E. Hopcroft and Jeffrey D. Ullman (1979)."}, {"url": "https://en.wikipedia.org/wiki/Halting_problem", "anchor_text": "Hilbert\u2019s halting problem"}, {"url": "https://github.com/moritzmoritz98/rlopt", "anchor_text": "repo"}, {"url": "https://github.com/moritzmoritz98/rlopt/blob/master/rlopt/env/env.py#L112", "anchor_text": "reward function"}, {"url": "https://github.com/moritzmoritz98/rlopt/blob/master/rlopt/rl_sorting.py#L187", "anchor_text": "reconstructed ready-to-use lambda function"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----9d06a4f0ce6---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----9d06a4f0ce6---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/tech?source=post_page-----9d06a4f0ce6---------------tech-----------------", "anchor_text": "Tech"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----9d06a4f0ce6---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/tag/optimization?source=post_page-----9d06a4f0ce6---------------optimization-----------------", "anchor_text": "Optimization"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9d06a4f0ce6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-turing-machine-with-reinforcement-learning-9d06a4f0ce6&user=Moritz+Kirschte&userId=e9624bd27418&source=-----9d06a4f0ce6---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9d06a4f0ce6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-turing-machine-with-reinforcement-learning-9d06a4f0ce6&user=Moritz+Kirschte&userId=e9624bd27418&source=-----9d06a4f0ce6---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9d06a4f0ce6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-turing-machine-with-reinforcement-learning-9d06a4f0ce6&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----9d06a4f0ce6--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F9d06a4f0ce6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-turing-machine-with-reinforcement-learning-9d06a4f0ce6&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----9d06a4f0ce6---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----9d06a4f0ce6--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----9d06a4f0ce6--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----9d06a4f0ce6--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----9d06a4f0ce6--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----9d06a4f0ce6--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----9d06a4f0ce6--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----9d06a4f0ce6--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----9d06a4f0ce6--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@kirschte?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@kirschte?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Moritz Kirschte"}, {"url": "https://medium.com/@kirschte/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "69 Followers"}, {"url": "http://linkedin.com/in/kirschte", "anchor_text": "http://linkedin.com/in/kirschte"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe9624bd27418&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-turing-machine-with-reinforcement-learning-9d06a4f0ce6&user=Moritz+Kirschte&userId=e9624bd27418&source=post_page-e9624bd27418--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Fe9624bd27418%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-turing-machine-with-reinforcement-learning-9d06a4f0ce6&user=Moritz+Kirschte&userId=e9624bd27418&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}