{"url": "https://towardsdatascience.com/machine-learning-the-great-stagnation-3a0f044e17e0", "time": 1683017309.802659, "path": "towardsdatascience.com/machine-learning-the-great-stagnation-3a0f044e17e0/", "webpage": {"metadata": {"title": "Machine Learning: The Great Stagnation | by Mark Saroufim | Towards Data Science", "h1": "Machine Learning: The Great Stagnation", "description": "This blog post generated a lot of discussion on Hacker News \u2014 many people have reached out to me giving more examples of the stagnation and more examples of projects avoiding it. Maybe I\u2019ll add to\u2026"}, "outgoing_paragraph_urls": [{"url": "https://news.ycombinator.com/item?id=25775740", "anchor_text": "Hacker News", "paragraph_index": 0}, {"url": "http://robotoverlordmanual.com/", "anchor_text": "robotoverlordmanual.com", "paragraph_index": 0}, {"url": "https://keras.io/", "anchor_text": "Keras", "paragraph_index": 59}, {"url": "http://book.realworldhaskell.org/", "anchor_text": "Fast.ai", "paragraph_index": 68}, {"url": "https://github.com/fastai/nbdev", "anchor_text": "nbdev", "paragraph_index": 70}, {"url": "https://julialang.org/", "anchor_text": "Julia", "paragraph_index": 74}, {"url": "https://julialang.org/blog/2019/01/fluxdiffeq/", "anchor_text": "neural ODE solver for free", "paragraph_index": 74}, {"url": "https://medium.com/swlh/neural-ode-for-reinforcement-learning-and-nonlinear-optimal-control-cartpole-problem-revisited-5408018b8d71", "anchor_text": "Neural ODE approach to Robotic simulations to be extremely exciting", "paragraph_index": 76}, {"url": "https://openai.com/", "anchor_text": "Open AI", "paragraph_index": 78}, {"url": "https://openai.com/blog/openai-api/", "anchor_text": "pay to use GPT-3", "paragraph_index": 80}, {"url": "https://huggingface.co/", "anchor_text": "HuggingFace", "paragraph_index": 81}, {"url": "http://book.realworldhaskell.org/", "anchor_text": "Real world Haskell", "paragraph_index": 84}, {"url": "https://github.com/hasktorch/hasktorch", "anchor_text": "Hasktorch", "paragraph_index": 86}, {"url": "https://blog.jle.im/entry/purely-functional-typed-models-1.html", "anchor_text": "Purely Functional Typed Models", "paragraph_index": 86}, {"url": "https://gym.openai.com/", "anchor_text": "Open AI gym", "paragraph_index": 88}, {"url": "https://github.com/Unity-Technologies/ml-agents", "anchor_text": "Unity ML agents", "paragraph_index": 90}, {"url": "https://www.youtube.com/watch?v=ZiuNC7Q1pZw", "anchor_text": "I explain how it works on YouTube", "paragraph_index": 93}, {"url": "http://www.twitch.tv/marksaroufim", "anchor_text": "twitch.tv/marksaroufim", "paragraph_index": 97}], "all_paragraphs": ["This blog post generated a lot of discussion on Hacker News \u2014 many people have reached out to me giving more examples of the stagnation and more examples of projects avoiding it. Maybe I\u2019ll add to this article or maybe I\u2019ll write a new one, let\u2019s see what happens. In the meantime if you can\u2019t wait for me to stop staring at the ceiling and write something new, I\u2019m pretty sure you\u2019ll enjoy my e-book robotoverlordmanual.com", "Academics think of themselves as trailblazers, explorers \u2014 seekers of the truth.", "Any fundamental discovery involves a significant degree of risk. If an idea is guaranteed to work then it moves from the realm of research to engineering. Unfortunately, this also means that most research careers will invariably be failures at least if failures are measured via \u201cobjective\u201d metrics like citations.", "The construction of Academia was predicated on providing a downside hedge or safety net for researchers. Where they can pursue ambitious ideas where the likelihood of success if secondary to the boldness of the vision.", "Academics sacrifice material opportunity costs in exchange for intellectual freedom. Society admires risk takers, for it is only via their heroic self sacrifice that society moves forward.", "Unfortunately most of the admiration and prestige we have towards academics are from a bygone time. Economists were the first to figure out how to maintain the prestige of academia while taking on no monetary or intellectual risk. They\u2019d show up on CNBC finance and talk about \u201ccorrections\u201d or \u201cirrational fear/exuberance\u201d. Regardless of how correct their predictions were, their media personalities grew with the feedback loops from the YouTube recommendation algorithm.", "It\u2019s hard to point the blame towards any individual researcher, after all while risk is good for the collective it\u2019s almost necessarily bad for the individual. However, this risk free approach is growing in popularity and has specifically permeated my field \u201cMachine Learning\u201d. FAANG salary with an academic appointment is the best job available in the world today.", "With State Of The Art (SOTA) Chasing we\u2019ve rewarded and lauded incremental researchers as innovators, increased their budgets so they can do even more incremental research parallelized over as many employees or graduate students that report to them.", "Machine Learning Researchers can now engage in risk-free, high-income, high-prestige work", "They are today\u2019s Medieval Catholic priests", "Machine Learning PhD students are the new Investment Banking analysts, both seek optionality in their career choices but differ in superficial ways like preferring Meditation over Parties and Marijuana & Adderall over Alcohol and Cocaine.", "A Machine Learning PhD is now just an extended interview for FAANG", "The entire data science interview process at larger labs has become a mix of trivia and prestige. Checking out a portfolio takes way to too long but checking that you graduated from Stanford or coauthored a paper with Google Brain, now that\u2019s a good filter!", "We\u2019ve gamified and standardized the process so much that it\u2019s starting to resemble case studies at consulting interviews.", "\u201cRecruiter: Which activation functions do you know?\u201d", "\u201cRecruiter: Ok.. Tell me about your biggest career failure\u201d", "\u201cMe: Having to answer your questions\u201d", "I often get asked by young students new to Machine Learning, what math do I need to know for Deep Learning and my answer is Matrix Multiplication and Derivatives of square functions. All these neuron analogies do more harm than good in explaining how Machine Learning actually works.", "LSTMs a bunch of matrix multiplications, Transformers a whole bunch of matrix multiplications, CNNs use convolutions which are a generalization of matrix multiplication.", "Deep Neural Networks are a composition of matrix multiplications with the occasional non-linearity in between", "Matrix multiplication was invented way back in 1812 by Jacques Philippe Marie Benet but you\u2019d be forgiven for thinking forward propagation was invented much later than that.", "With Automatic Differentiation, the backward pass is essentially free and is as engaging to compute as 50 digit number long division. Deriving long complicated gradients is fake rigor that was useful before we had computers.", "When I was a graduate student at UC San Diego I remember shying away from Deep Learning because it was not considered serious Machine Learning because there were no good proofs for why these models should work.", "I\u2019ve learnt \u201cthe hard way\u201d that Deep Learning is an empirical field, so why or how something works is often anecdotal as opposed to theoretical.", "The best people in empirical fields are typically those who have accumulated the biggest set of experiences and there\u2019s essentially two ways to do this.", "Age is a proxy for experience but an efficient experimentation methodology allows you to compress the amount of time it would take to gain more experiences.", "If you have a data center at your disposal this further multiplies your ability to learn. If you and all your peers have access to data centers this is yet another multiplicative feedback loop since you can all learn from each other.", "This helps explain why the most impactful research in Machine Learning gets published in only a few labs such as Google Brain, DeepMind and Open AI. There are feedback loops everywhere.", "The past 3 years in particular have been an unrelenting deluge of incremental work with paper titles that read like tabloid headlines:", "\u201cUseful\u201d Machine Learning research on all datasets has essentially reduced to making Transformers faster, smaller and scale to longer sequence lengths.", "This is a problem reminiscent of the discovery of NP-completeness \u2014 journals were flooded with the proofs that yet another problem was NP-complete.", "BERT engineer is now a full time job. Qualifications include:", "It\u2019s kind of like Dev-ops but you get paid more.", "Neural Network weights are learnt via Gradient Descent and Neural Network architectures are learnt via Graduate Student Descent.", "The below flow chart describes how Graduate Student Descent works.", "Graduate Student Descent is one of the most reliable ways of getting state of the art performance in Machine Learning today and it\u2019s also a fully parallelizable over as many graduate students or employees your lab has. Armed with Graduate Student descent you are more likely to get published or promoted than if you took on uncertain projects.", "The popularity of Graduate Student Descent stems from Cargo Culting configs where certain loss functions, depths, architecture are generally regarded as good.", "It\u2019s quite difficult to actually reason from first principles because Machine Learning algorithms are complex systems with a huge number of variability in parameters where the interactions are nonlinear and unpredictable. Ablations help but even then aren\u2019t entirely conclusive over such a wide range of parameters.", "Technical papers that try to give intuition behind how or why a specific technique works often looks closer to astrology than science with sentences like \u201cencourage the network to be confident in its predictions\u201d.", "I sometimes get the impression that academics think that transitioning to large models goes something like:", "It\u2019s trivial to think of running a large model but it\u2019s certainly not trivial to actually do it. This misconception is best illustrated with this immensely popular meme.", "If something simple like stacking more layers works better than statistical learning, then you have to wonder who the real clown is", "To \u201cSTACK MORE LAYERS\u201d you need to worry about model and data parallelism, pipelining, tuning your hyperparameters, hardware accelerators, network vs compute vs storage vs IO bottlenecks, early stopping, scalable architectures, distillation, pruning etc..", "You can do all the convergence bounds you like with Chernoff Bounds, Markov Inequality over some parameter which is assumed to be Gaussian but if your proposed algorithm is worse than \u201cSTACK MORE LAYERS\u201d then your proposed algorithm isn\u2019t very good.", "Every paper is SOTA, has strong theoretical guarantees, an intuitive explanation, is interpretable and fair but almost none are mutually consistent with each other", "If Constantinople fell so can the institution of Theoretical Machine Learning. It doesn\u2019t really matter how many angels can dance on the head of a pin.", "I\u2019m cautiously optimistic about Causal Reasoning, what I\u2019d like to see is it graduating from a tool for meditation to libraries that people actually use daily.", "This does not mean I\u2019m opposed to Mathematical formalism, if anything I love math and I want to see more of it in Deep Learning \u2014 I only caution against fake rigor.", "Assuming certain \u201cnice properties\u201d about data so that theorems work out, gradient derivations that take up multiple pages of an appendix instead of just using Automatic Differentiation.", "But how do you prove that new theoretical results are useful?", "Well the worst way to do this is to simply combine complicated mathematical ideas into a neural network because you can, you can draw on the readers aesthetic sensibilities and discuss why Fourier is the heart of computation. The optimization community is often guilty of guilty of this where they propose activation functions like swishand then spend pages and pages talking about the nice properties of the loss landscape.", "The most reliable to get new ideas widespread is to create a benchmark where existing SOTA methods fail and then show how your technique is better. This is hard by design, it should be hard to displace proven ideas with unproven ones. This technique has the advantage of not requiring any Twitter arguments.", "It\u2019s important to avoid becoming Gary Marcus and criticize existing technique that work without proposing something else that works even better.", "If you can\u2019t do that then maybe you haven\u2019t stumbled on something useful, maybe it\u2019s just beautiful or elegant. Searching for warmth and fuzziness is still a worthwhile goal.", "While my introductory lament may have led you to believe that there is no innovation going on in Machine Learning, that we\u2019ve settled on a cargo-culting monoculture \u2014 nothing could be further from the truth.", "There is still substantial innovation happening in Machine Learning just not from Data Scientists or Machine Learning Researchers.", "Here are the projects that I believe represent a glimmer of hope against the Stagnation of Machine Learning.", "Machine Learning is a Language, Compiler and Design problem", "A programming language is tool for thinking that needs to be designed with the same sensibilities as any other consumer product.", "When I first got interested in Keras, some of my peers mentioned to me that it wasn\u2019t as serious doing \u201creal ML work\u201d in Tensorflow and it made me silently wonder why they weren\u2019t programming in FORTRAN.", "Keras introduced me to the idea of viewing Machine Learning as a language design problem. Instead of thinking of matrices or neurons I was now thinking in terms of layers just like I do when I read a paper.", "A matrix is a linear map but linear maps are far more intuitive to think about than matrices", "You can then build networks with multiple inputs and outputs to build far more interesting networks.", "Keras is a user centric library whereas Tensorflow especially Tensorflow 1.0 is a machine centric library. ML researchers think in terms of terms of layers, automatic differentiation engines think in terms of computational graphs.", "As far as I\u2019m concerned my time is more valuable than the cycles of a machine so I\u2019d rather use something like Keras.", "This doesn\u2019t mean I\u2019m happy with slow code which is why it\u2019s crucial to build good compilers and Intermediate representations like XLA that would run my user friendly code fast.", "Performance vs Abstraction is a false dichotomy \u2014 the history of computing is proof of this", "Putting the user first is the approach that Fast.ai took when building their Deep Learning library. I think of Jeremy Howard as the Don Norman of Machine Learning.", "Instead of just focusing on the model building part, Fast.ai builds tools around all of the below.", "By tools I don\u2019t mean a black box service, I mean software design patterns specific to Machine Learning. Instead of Abstract Factories, abstractions like Pipelines to chain preprocessing steps, callbacks for early stopping all the way to generic yet simple implementations of Transformers. Design patterns are more useful than black boxes because you can understand how they work, modify them and improve for both yourself and others.", "Honorable mention to nbdev which aims at removing some of the common annoyances of working with notebooks by eliminating all obstacles that would get in your way of shipping your code as a library including a human readable git representation, continuous integration and automated PyPi package submission.", "The promise of Deep Learning is Differentiable Computing, where instead of writing programs to accomplish certain tasks you feed a model input/output pairs and ask it to generate a program for you.", "Once you start thinking of neural networks as programs, you can also think of programs as neural networks and differentiate programs.", "Unfortunately, Python is not differentiable which is Google and Facebook have each built their own languages in C++ with Python bindings that is automatically differentiable.", "Julia on the other hand is a language made for scientific computing where everything is automatically differentiable by default. So if you build an ODE solver you get a neural ODE solver for free.", "Ordinary and Partial Differential Equations are the way we formalize most relationships in Science from astronomy to pharmacology so being able to speed up these simulations by orders of magnitude means we\u2019re in the middle of a golden age of Scientific Computing.", "I also particularly find the Neural ODE approach to Robotic simulations to be extremely exciting. The main problem in Reinforcement Learning is that models are generally large and difficult to train but you can make them much smaller and easier to train if instead of treating a simulation as a black box you can treat it a like differentiable white box ODE solver.", "What is the most important Machine Learning company today?", "If you asked this question in 2018 the answer may have been Open AI. They dazzled the world with beautiful demos of agents besting expert video game players. They reminded me why I got into Machine Learning in the first place. Their reputation shadowed everything but overtime it became their core product.", "Open AI is a media and service company", "Look at the pretty blog posts with beautiful typography, pay to use GPT-3. Open AI is not a platform company.", "I can\u2019t think of a single large company where the NLP team hasn\u2019t experimented with HuggingFace. They add new Transformer models within days of the papers being published, they maintain tokenizers, datasets, data loaders, NLP apps. HuggingFace has created multiple layers of platforms that each could be a compelling company in its own right.", "Billions of Dollars of value will be created from HuggingFace on problems which are a lot less speculative than AGI. HuggingFace avoids the typical ML startup trap of turning into either a consulting firm or citation farm.", "Haskellers often resemble a cult, everything is a function or a Monad or a Lens. It\u2019s often hard to follow what Bi-Category is or why you should care.", "Real world Haskell functions try to model IO as a Monad or a web server as a function with state but there\u2019s an application that I believe Haskellers don\u2019t hammer on quite enough.", "Haskell is the best functional programming language in the world and Neural Networks are functions", "This is the main motivation behind Hasktorch which lets you discover new kinds of Neural Network architectures by combining functional operators. Justin Le does this idea a lot of justice in his series Purely Functional Typed Models", "Admittedly the RNN code sample is a tad complicated but consider it\u2019s orders of magnitude less code than what you\u2019d see in a typical neural network library and not something you can get by combing logistic regression with a couple of extra operators. I\u2019m hoping I can convince Justin to one day create a lecture for us mere mortals to invent our own neural network architectures.", "Open AI gym was a great attempt at creating a platform where various games were used in benchmarks for Reinforcement Learning research. The goal was to eventually advance Reinforcement Learning to deal with more complex problems and really push the boundary.", "Unfortunately, SOTA chasing meant that the benchmarks became the goal and an entire community of researchers has overfit techniques like \u201cworld models\u201d to this dataset. So generally there\u2019s a trend towards ever more complex algorithms on simple benchmarks whereas the most complex benchmarks like Dota seem to be using the simplest algorithms scaled with an impressive infrastructure.", "Unity is the most user friendly Game Engine out today, I love it and use it for all my side projects. Unity ML agents is a way for you to turn a video game into a Reinforcement Learning environment.", "Reinforcement Learning environments are essentially custom datasets and I\u2019m confident it will be the de-facto simulator of complex robotic applications. Create a complex multi agent negotiation with bluffing game where agents need to have a grasp of physics and an understanding of optical illusions. Go crazy!", "Any intelligent behavior is best benchmarked with Unity ML agents", "I wrote this article before AlphaFold2 was released \u2014 I explain how it works on YouTube. Biotech is a huge deal and is most definitely not stagnating, I\u2019ll probably address this in a future article. But as a sneak peek, most interesting scientific problems can be modeled as graphs which motivates why I\u2019m so interested in Graph Neural Networks.", "I am sad to see that most exciting work in Machine Learning is coming from outside of Machine Learning \u2014 I\u2019ve spent 10 years in this field and I learn more from the crackpot outsiders on Twitter today then I do from peer reviewed papers.", "I want to hear more proposals that I know for a fact will probably work, I want more ideas, I want Machine Learning to be fun again.", "Keep an open mind and most importantly don\u2019t be this guy. And if you enjoyed this please let me know by subscribing!", "Thank you sudomaze, thecedarprince, krishnanpc and 19_Rafael for helpful feedback while I was livestreaming myself writing this article on twitch.tv/marksaroufim", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F3a0f044e17e0&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-the-great-stagnation-3a0f044e17e0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-the-great-stagnation-3a0f044e17e0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-the-great-stagnation-3a0f044e17e0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-the-great-stagnation-3a0f044e17e0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----3a0f044e17e0--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3a0f044e17e0--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://marksaroufim.medium.com/?source=post_page-----3a0f044e17e0--------------------------------", "anchor_text": ""}, {"url": "https://marksaroufim.medium.com/?source=post_page-----3a0f044e17e0--------------------------------", "anchor_text": "Mark Saroufim"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F426d54390627&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-the-great-stagnation-3a0f044e17e0&user=Mark+Saroufim&userId=426d54390627&source=post_page-426d54390627----3a0f044e17e0---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3a0f044e17e0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-the-great-stagnation-3a0f044e17e0&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3a0f044e17e0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-the-great-stagnation-3a0f044e17e0&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://news.ycombinator.com/item?id=25775740", "anchor_text": "Hacker News"}, {"url": "http://robotoverlordmanual.com/", "anchor_text": "robotoverlordmanual.com"}, {"url": "https://en.wikipedia.org/wiki/Disputation_of_the_Holy_Sacrament#/media/File:Disputa_del_Sacramento_(Rafael).jpg", "anchor_text": "https://en.wikipedia.org/wiki/Disputation_of_the_Holy_Sacrament#/media/File:Disputa_del_Sacramento_(Rafael).jpg"}, {"url": "https://www.youtube.com/channel/UCZHmQk67mSJgfCCTn7xBfew", "anchor_text": "Yannic Kilcher\u2019s new Transformer paper"}, {"url": "https://www.reddit.com/r/ProgrammerHumor/comments/8c1i45/stack_more_layers/", "anchor_text": "https://www.reddit.com/r/ProgrammerHumor/comments/8c1i45/stack_more_layers/"}, {"url": "https://keras.io/", "anchor_text": "Keras"}, {"url": "http://book.realworldhaskell.org/", "anchor_text": "Fast.ai"}, {"url": "https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/", "anchor_text": "https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/"}, {"url": "https://github.com/fastai/nbdev", "anchor_text": "nbdev"}, {"url": "https://marksaroufim.medium.com/can-deep-learning-solve-my-problem-a-type-theoretic-heuristic-e57f4d1658f", "anchor_text": "https://marksaroufim.medium.com/can-deep-learning-solve-my-problem-a-type-theoretic-heuristic-e57f4d1658f"}, {"url": "https://julialang.org/", "anchor_text": "Julia"}, {"url": "https://julialang.org/blog/2019/01/fluxdiffeq/", "anchor_text": "neural ODE solver for free"}, {"url": "https://medium.com/swlh/neural-ode-for-reinforcement-learning-and-nonlinear-optimal-control-cartpole-problem-revisited-5408018b8d71", "anchor_text": "Neural ODE approach to Robotic simulations to be extremely exciting"}, {"url": "https://openai.com/", "anchor_text": "Open AI"}, {"url": "https://openai.com/blog/openai-api/", "anchor_text": "pay to use GPT-3"}, {"url": "https://huggingface.co/", "anchor_text": "https://huggingface.co/"}, {"url": "https://huggingface.co/", "anchor_text": "HuggingFace"}, {"url": "http://book.realworldhaskell.org/", "anchor_text": "Real world Haskell"}, {"url": "https://github.com/hasktorch/hasktorch", "anchor_text": "Hasktorch"}, {"url": "https://blog.jle.im/entry/purely-functional-typed-models-1.html", "anchor_text": "Purely Functional Typed Models"}, {"url": "https://gym.openai.com/", "anchor_text": "Open AI gym"}, {"url": "https://github.com/Unity-Technologies/ml-agents", "anchor_text": "Unity ML agents"}, {"url": "https://www.youtube.com/watch?v=ZiuNC7Q1pZw", "anchor_text": "I explain how it works on YouTube"}, {"url": "https://www.wikiwand.com/en/Pedant", "anchor_text": "https://www.wikiwand.com/en/Pedant"}, {"url": "http://www.twitch.tv/marksaroufim", "anchor_text": "twitch.tv/marksaroufim"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----3a0f044e17e0---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/editors-pick?source=post_page-----3a0f044e17e0---------------editors_pick-----------------", "anchor_text": "Editors Pick"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----3a0f044e17e0---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/transformers?source=post_page-----3a0f044e17e0---------------transformers-----------------", "anchor_text": "Transformers"}, {"url": "https://medium.com/tag/bert?source=post_page-----3a0f044e17e0---------------bert-----------------", "anchor_text": "Bert"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3a0f044e17e0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-the-great-stagnation-3a0f044e17e0&user=Mark+Saroufim&userId=426d54390627&source=-----3a0f044e17e0---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3a0f044e17e0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-the-great-stagnation-3a0f044e17e0&user=Mark+Saroufim&userId=426d54390627&source=-----3a0f044e17e0---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3a0f044e17e0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-the-great-stagnation-3a0f044e17e0&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3a0f044e17e0--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F3a0f044e17e0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-the-great-stagnation-3a0f044e17e0&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----3a0f044e17e0---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----3a0f044e17e0--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----3a0f044e17e0--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----3a0f044e17e0--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----3a0f044e17e0--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----3a0f044e17e0--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----3a0f044e17e0--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----3a0f044e17e0--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----3a0f044e17e0--------------------------------", "anchor_text": ""}, {"url": "https://marksaroufim.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://marksaroufim.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Mark Saroufim"}, {"url": "https://marksaroufim.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "675 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F426d54390627&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-the-great-stagnation-3a0f044e17e0&user=Mark+Saroufim&userId=426d54390627&source=post_page-426d54390627--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F6f02e82219a8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-the-great-stagnation-3a0f044e17e0&newsletterV3=426d54390627&newsletterV3Id=6f02e82219a8&user=Mark+Saroufim&userId=426d54390627&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}