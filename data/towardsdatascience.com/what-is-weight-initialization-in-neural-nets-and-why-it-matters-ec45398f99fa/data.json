{"url": "https://towardsdatascience.com/what-is-weight-initialization-in-neural-nets-and-why-it-matters-ec45398f99fa", "time": 1682995847.043659, "path": "towardsdatascience.com/what-is-weight-initialization-in-neural-nets-and-why-it-matters-ec45398f99fa/", "webpage": {"metadata": {"title": "Why cautiously initializing deep neural networks matters? | by Poonam Ligade | Towards Data Science", "h1": "Why cautiously initializing deep neural networks matters?", "description": "ReLU activation function is important to achieve SOTA in deep CNNs. We\u2019ll discuss a robust weight initialization method which speeds up convergence of deeper models by Kaiming He et al."}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/abs/1502.01852", "anchor_text": "Delving Deep into Rectifiers", "paragraph_index": 0}, {"url": "https://www.fast.ai/", "anchor_text": "Fast.ai", "paragraph_index": 1}, {"url": "https://twitter.com/jeremyphoward", "anchor_text": "Jeremy Howard", "paragraph_index": 1}, {"url": "http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf", "anchor_text": "Understanding the difficulty of training deep feedforward neural networks", "paragraph_index": 8}, {"url": "https://medium.com/impactai/cnns-from-different-viewpoints-fab7f52d159c", "anchor_text": "this", "paragraph_index": 20}, {"url": "http://JarvisLabs.ai", "anchor_text": "JarvisLabs.ai", "paragraph_index": 67}, {"url": "http://Fast.ai", "anchor_text": "Fast.ai", "paragraph_index": 67}, {"url": "https://www.kaggle.com/poonaml", "anchor_text": "https://www.kaggle.com/poonaml", "paragraph_index": 67}], "all_paragraphs": ["A significant part of recent success in deep learning goes to ReLU activation function. It has achieved the state of art results in deep CNNs for Image classification problems. In this blog, we\u2019ll discuss a robust weight initialization method, which helps in faster convergence of deeper neural models. Kaiming He et al. proposes this method in the Delving Deep into Rectifiers paper(2015).", "This blog takes inspiration from Fast.ai\u2019s course Deep Learning for coders, Part 2, taught by Jeremy Howard at USF.", "Parameters of neural networks include weights and biases. These numbers are randomly initialized first. Then our model learns them, which means we use gradients in the backward pass to update them gradually.", "The most widespread way to initialize parameters is by using Gaussian Distribution. This distribution has 0 mean and a standard deviation of 1.", "If m is the input size and nh is number of hidden units, then weights can be initialized as,", "Deep neural networks are hard to train. Initializing parameters randomly, too small or too large can be problematic while backpropagating the gradients all the way till initial layers.", "What happens when we initialize weights too small(<1)? Their gradient tends to get smaller as we move backward through the hidden layers, which means that neurons in the earlier layers learn much more slowly than neurons in later layers. This causes minor weight updates. This phenomenon is called vanishing gradient problem, where weights vanish to 0.", "What if we initialize weights too large(>1)? The gradient gets much larger in the earlier layers, which causes extremely high weight updates overshooting the minimal value. This is the exploding gradient problem, where weights explode to infinity(NaN). Both of these cases makes neural network difficult to converge.", "Below are the images from the experiment conducted by Gloriot et al. in the paper Understanding the difficulty of training deep feedforward neural networks. Authors considered neural networks with 5 layers deep with standard initialization. They initialized random weights from a normal distribution(0 mean and 1 variance).", "The above graph depicts, as training progresses(forward pass from layer 1 to 5) the mean of all activation values is becoming smaller(vanishing to 0) in the last layers. In layer 5 they are almost 0.", "We calculate gradients backward, from layer 5 to layer 1. For layer 1 all the gradients have nearly vanished. A bad initialization can really hinder the learning of the highly non-linear system. Due to random initialization, the first layer throws away most information about the input image. So even if we train later layers extensively they don\u2019t have enough information to learn from the input image.", "Careful weight initialization prevents both of these from happening and results in faster convergence of deep neural networks.", "A neural network works best when input data is centered (have a mean of 0 and std of 1). So when input values get multiplied by weight values, their activation remains on scale 1. What does it do? It helps in graceful optimization of neural networks. Since the hidden activation functions don\u2019t saturate that fast. And thus does not give near zero gradients early on in learning.", "ReLU is a non-linear activation function. Definition of ReLU is,", "2. This makes the neural net learn faster. And also speeds up the convergence of the training process.", "3. ReLU gives better solutions sooner than conventional sigmoid units.", "Kaiming et al.(1) derived a sound initialization method by cautiously modeling non-linearity of ReLUs, which makes extremely deep models (>30 layers) to converge.", "The forward pass consists of activations coming out of sequential matrix multiplications (between layers\u2019 inputs and weight) coupled with ReLU non-linearities. The output of this is passed on to consecutive layers carrying out similar operations.", "For each convolutional layer l, the response is below equation (1),", "Why convolutional layer performing linear operations?", "On a side note, if you are wondering how can a convolutional layer perform a linear equation like linear layer. Conv layer does convolutions. But if you follow this blog from Matthew Kleinsmith. You will understand convolutions are just matrix multiplications as shown below.", "Now back to our equation, if we take the variance of linear equation (1), we get equation (2)", "where y_l, w_l, x_l are random variables of each of element in y_l, W_l, x_l resp. We assume w_l zero mean. Substituting that in Eqn. (2), we have the variance of the product of independent variables as Eqn. (3)", "But how did we arrive at this? For random variables x_l and W_l, which are independent of each other. We can use properties of expectation to prove that", "Let\u2019s focus on the term E[x\u00b2_l]. Here E() stands for the expectation of a given variable, which is its mean value. But below Eqn. doesn\u2019t hold", "unless x_l has zero mean. x_l cannot have 0 mean, because it is ReLU activation function of the previous layer (l-1).", "If we further assume w_(l\u22121) have a symmetric distribution around zero and b_(l\u22121) = 0. Then y_(l\u22121) has zero mean and has a symmetric distribution around zero. So now we have below Eqn. (4) when f is ReLU.", "Why does ReLU add scalar 1/2 to the output?", "For the family of ReLUs, we have generic activation function as,", "where, 1) y_i is the input of the nonlinear activation f on the ith channel, 2) a_i is the coefficient controlling the slope of the negative part.", "ReLu is obtained when a_i=0. The resultant activation function is of the form f(y_i)=max(0,y_i).", "ReLU activation is a threshold at zero which enables the network to have sparse representations. For eg, after uniform initialization of the weights, around 50% of hidden units continuous output values are real zeros.", "Relu loses a lot of information(get replaced by zero values) this effects into aggressive data compression. To retain data you can use PRelu or LRelu which adds slope of a_i to the negative side of the axis represented by,", "ReLu activation function retains only the positive half-axis values, so we have", "Substituting this in the Eqn (3), we obtain Eqn. (5) as", "Now we have an Eqn. with activations of layer l and those of layer (l -1). With L layers put together and starting from last layer L, we have below product as Eqn. (5)", "Note: x_l is the input of the network, that\u2019s why the above Eqn. starts at l = 2. This product is the key to the initialization design. A proper initialization method should avoid reducing or magnifying the magnitudes of input signals exponentially. So we expect our product to take a proper scalar(eg. 1).A sufficient condition (1) for the forward pass is", "This leads to a zero-mean Gaussian distribution whose standard deviation is", "We also initialize bias, b = 0.For the first layer (l = 1), we should have only n_1Var[w_1] = 1 since there is no ReLU applied on the input, we don\u2019t need to halve the input. The small factor 1/2 does not matter if it just exists on one layer.", "The backward pass is gradients calculated backward from the last layer to the first one. For backpropagation, the gradient of convolutional layer l is given as Eqn. (6)", "During backprop, we are moving through the network in the reverse (backward) direction, so below equation doesn\u2019t hold true.", "In backward propagation we further have,", "where f\u2032 is the derivative of activation function f.For the ReLU case, f\u2032(y_l) is zero or one, and their probabilities are equal. Pr(0)=1/2 and Pr(1)=1/2", "we again assume that f\u2032(y_l) and \u2206x_(l+1) are independent of each other.As we have seen in Eqn. (4) ReLU adds scalar 1/2 to its output. Thus, we have", "Also, because \u0394y_l has a zero mean, and that f\u2032(y_l)\u00b2=f\u2032(y_l). By taking the expectation of the square of the equation", "we get below Eqn. Here, the scalar 1/2 is the result of ReLU.", "Then we compute the variance of the gradient in Eqn. (6):", "With L layers put together, we have variance(\u2206x_L) as Eq. (7)", "We have to define a sufficient condition (2) that makes sure the gradient is not exponentially large/small.", "Note, the only difference between the condition in backpropagation and the forward pass is we have n\u0302_l instead of n_l.k is image size and d are input channels.", "and channels of current layer c_l are same as filters of previous layer d_(l-1).", "Also, the condition satisfies zero-mean Gaussian Distribution whose standard deviation is,", "For the first layer (l = 1), we need not compute \u2206x1 because it represents the image. The factor of a single layer does not make the overall product exponentially large/small.", "It is safe to use either condition (1) or (2) to make a neural model converge without having exploding/vanishing gradients.", "Suppose we substitute condition (2) in Eqn. (7), we get", "Instead of scalar 1, the constant to look for is c2/dL. i.e The number of channels at the beginning of the network and at the end of it. This is not a diminishing number to make neural networks face exploding or vanishing gradients problems. According to the authors, this equation properly scales forward and backward passes making neural net converge efficiently.", "Xavier and Bengio had earlier on proposed the \u201cXavier\u201d initialization, a method whose derivation was based on the assumption that the activations are linear. This assumption is invalid for relu. The main difference between kaiming and Xavier initialization is that Kaiming addresses ReLU non-linearity. Xavier method uses below a condition which initializes weights from the standard distribution of", "Here, i is nothing but our layer index l. The normalization factor which keeps activations and backpropagated gradients stable while moving up and down in the network according to Xavier\u2019s normalized initialization is", "We can use Xavier formula in PyTorch as", "Here are the graphs that compare the convergence of Xavier and Kaiming initialization methods with ReLU nonlinearity. On 22 and 30(27 conv, 3 fc) layers deep neural networks.", "As we see, both the init methods helps 22 layer deep neural net converge, while using ReLU. But Kaiming\u2019s starts decreasing error rate much earlier than Xavier\u2019s.", "As we can see, only Kaiming initialization method enables the deep model to converge. The Xavier\u2019s one completely halts the learning, and the gradients are diminishing leading to no convergence at all.", "Here is how to initialize weights using Kaiming init strategy", "torch.nn uses below formula to demonstrate Kaiming initialization", "There are 2 modes to initialize with -1. fan_in (default) \u2014 preserves the magnitude of weights in the forward pass. 2. fan_out \u2014 preserves the magnitudes of the weights in the backward pass.", "we used fan_out since we need backpropagated gradients to be stable. Kaiming init is must for deeper rectified neural networks.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Building JarvisLabs.ai - a GPU cloud. Deep Learning Practitioner. Fast.ai fellow. Kaggle Master https://www.kaggle.com/poonaml"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fec45398f99fa&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-weight-initialization-in-neural-nets-and-why-it-matters-ec45398f99fa&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-weight-initialization-in-neural-nets-and-why-it-matters-ec45398f99fa&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-weight-initialization-in-neural-nets-and-why-it-matters-ec45398f99fa&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-weight-initialization-in-neural-nets-and-why-it-matters-ec45398f99fa&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----ec45398f99fa--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ec45398f99fa--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@poonam.v.ligade?source=post_page-----ec45398f99fa--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@poonam.v.ligade?source=post_page-----ec45398f99fa--------------------------------", "anchor_text": "Poonam Ligade"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2c70181460b4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-weight-initialization-in-neural-nets-and-why-it-matters-ec45398f99fa&user=Poonam+Ligade&userId=2c70181460b4&source=post_page-2c70181460b4----ec45398f99fa---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fec45398f99fa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-weight-initialization-in-neural-nets-and-why-it-matters-ec45398f99fa&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fec45398f99fa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-weight-initialization-in-neural-nets-and-why-it-matters-ec45398f99fa&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/photos/LNDgBERq8Q0", "anchor_text": "@kimzy"}, {"url": "https://arxiv.org/abs/1502.01852", "anchor_text": "Delving Deep into Rectifiers"}, {"url": "https://www.fast.ai/", "anchor_text": "Fast.ai"}, {"url": "https://twitter.com/jeremyphoward", "anchor_text": "Jeremy Howard"}, {"url": "https://www.graphpad.com/guides/prism/7/curve-fitting/reg_how_to_gaussian.html", "anchor_text": "Bell Curve"}, {"url": "http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf", "anchor_text": "Understanding the difficulty of training deep feedforward neural networks"}, {"url": "https://ailephant.com/glossary/relu-function/", "anchor_text": "ReLu"}, {"url": "https://pouannes.github.io/blog/initialization/", "anchor_text": "source"}, {"url": "https://pouannes.github.io/blog/initialization/#bringing-forward-propagation-and-backward-propagation-together", "anchor_text": "source"}, {"url": "https://medium.com/impactai/cnns-from-different-viewpoints-fab7f52d159c", "anchor_text": "this"}, {"url": "https://www.jefkine.com/deep/2016/08/08/initialization-of-deep-networks-case-of-rectifiers/", "anchor_text": "source"}, {"url": "https://arxiv.org/pdf/1502.01852.pdf", "anchor_text": "source"}, {"url": "https://arxiv.org/pdf/1502.01852.pdf", "anchor_text": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification, Kaiming et al., Microsoft, 2015"}, {"url": "http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf", "anchor_text": "Understanding the difficulty of training deep feedforward neural networks, Glorot et al., Universite de Montreal, 2010"}, {"url": "https://pouannes.github.io/blog/initialization/", "anchor_text": "Pierre Ouannes, How to initialize deep neural networks? Xavier and Kaiming initialization, Mar 22, 2019"}, {"url": "https://towardsdatascience.com/@jamesdell", "anchor_text": "James Dellinger"}, {"url": "https://towardsdatascience.com/weight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79", "anchor_text": "Weight Initialization in Neural Networks: A Journey From the Basics to Kaiming"}, {"url": "https://www.jefkine.com/deep/2016/08/08/initialization-of-deep-networks-case-of-rectifiers/", "anchor_text": "Initialization"}, {"url": "https://www.jefkine.com/deep/2016/08/08/initialization-of-deep-networks-case-of-rectifiers/", "anchor_text": "of deep networks case of rectifiers"}, {"url": "https://medium.com/tag/paper-review?source=post_page-----ec45398f99fa---------------paper_review-----------------", "anchor_text": "Paper Review"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----ec45398f99fa---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/weight-initialization?source=post_page-----ec45398f99fa---------------weight_initialization-----------------", "anchor_text": "Weight Initialization"}, {"url": "https://medium.com/tag/deep-neural-networks?source=post_page-----ec45398f99fa---------------deep_neural_networks-----------------", "anchor_text": "Deep Neural Networks"}, {"url": "https://medium.com/tag/relu?source=post_page-----ec45398f99fa---------------relu-----------------", "anchor_text": "Relu"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fec45398f99fa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-weight-initialization-in-neural-nets-and-why-it-matters-ec45398f99fa&user=Poonam+Ligade&userId=2c70181460b4&source=-----ec45398f99fa---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fec45398f99fa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-weight-initialization-in-neural-nets-and-why-it-matters-ec45398f99fa&user=Poonam+Ligade&userId=2c70181460b4&source=-----ec45398f99fa---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fec45398f99fa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-weight-initialization-in-neural-nets-and-why-it-matters-ec45398f99fa&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ec45398f99fa--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fec45398f99fa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-weight-initialization-in-neural-nets-and-why-it-matters-ec45398f99fa&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----ec45398f99fa---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----ec45398f99fa--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----ec45398f99fa--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----ec45398f99fa--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----ec45398f99fa--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----ec45398f99fa--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----ec45398f99fa--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----ec45398f99fa--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----ec45398f99fa--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@poonam.v.ligade?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@poonam.v.ligade?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Poonam Ligade"}, {"url": "https://medium.com/@poonam.v.ligade/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "27 Followers"}, {"url": "http://JarvisLabs.ai", "anchor_text": "JarvisLabs.ai"}, {"url": "http://Fast.ai", "anchor_text": "Fast.ai"}, {"url": "https://www.kaggle.com/poonaml", "anchor_text": "https://www.kaggle.com/poonaml"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2c70181460b4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-weight-initialization-in-neural-nets-and-why-it-matters-ec45398f99fa&user=Poonam+Ligade&userId=2c70181460b4&source=post_page-2c70181460b4--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F2c70181460b4%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-weight-initialization-in-neural-nets-and-why-it-matters-ec45398f99fa&user=Poonam+Ligade&userId=2c70181460b4&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}