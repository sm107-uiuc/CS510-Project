{"url": "https://towardsdatascience.com/the-basics-logistic-regression-and-regularization-828b0d2d206c", "time": 1683001177.303643, "path": "towardsdatascience.com/the-basics-logistic-regression-and-regularization-828b0d2d206c/", "webpage": {"metadata": {"title": "The Basics: Logistic Regression and Regularization | by Max Miller | Towards Data Science", "h1": "The Basics: Logistic Regression and Regularization", "description": "Ordinary Least Squares linear regression is powerful and versatile right out of the box, but there are certain circumstances where it fails. For one thing, it is, expressly, a \u2018regression\u2019 framework\u2026"}, "outgoing_paragraph_urls": [], "all_paragraphs": ["Ordinary Least Squares linear regression is powerful and versatile right out of the box, but there are certain circumstances where it fails. For one thing, it is, expressly, a \u2018regression\u2019 framework, which makes it hard to apply as a classifier. For another, unlike, say, a decision tree, linear regression models don\u2019t perform their own implicit feature selection, meaning they are prone to overfit if too many features are included. Luckily, there are some extensions to the linear model that allow us to overcome these issues. Logistic regression turns the linear regression framework into a classifier and various types of \u2018regularization\u2019, of which the Ridge and Lasso methods are most common, help avoid overfit in feature rich instances.", "Logistic regression essentially adapts the linear regression formula to allow it to act as a classifier. Why can\u2019t a regular OLS linear regression act as a classifier on its own? Let\u2019s consider an example. Below, here\u2019s a simple classification example in one dimension. The x-variable is continuous, but the y-variable is categorical, and is either zero or one:", "There is some overlap, but we can see visually that our categorical tab becomes more prominent as we move to the right. On the right side, more of the points are one than are zero and above a certain x-value, all the points we are see have a y-value of one. This looks like a reasonably easy relationship to model, but if we try to simply fit a linear regression to this data, the results are a little screwy:", "On the one hand this line is in a way successfully capturing the positive association between the two variables, but the output of this line doesn\u2019t really make a whole lot of sense. If I consult this line at at the x-value of, say, .25, we find that the line predicts a value of .71. Since we\u2019re trying to predict a variable that only ever takes the values 0 or 1, a prediction of .71 is a little weird; our binary variable can\u2019t actually take on that value, so what would this prediction even mean?", "One way to interpret fractional values in this sort of situation would be to reframe our question a little bit. Instead of asking our model to predict the value of our independent variable, we can ask it to give us the probability that our variable will have the value of one. As the model\u2019s output goes up, we can say that the chances that the variable is one goes up. Fractional values in this framework make a little bit more sense. What would a value like .71 mean? Well, that the outcome is more likely than if the model had given us a lower number and less likely than if it had given us a higher one!", "There\u2019s still a little bit of work to turn our simple linear regression into this sort of model, however. For one thing, it intuitively seems like the probability our model should output is non-linear. As it currently stands, our linear regression will sometimes give us values above one or negative values, which, again, make no sense. Our solution in this case is to pass our linear model through a sigmoid function. \u2018Sigmoid\u2019 here simply means \u2019S-shaped\u2019 and there are a few functions that we might use, but the most common one is the logistic function. A logistic function has a general form like this:", "That may look a little intimidating, but it\u2019s worth thinking through it a little bit. The exponent on e on the bottom of the fraction looks like our previous linear regression equation, except that the whole thing has been made negative. As the output of this \u2018regression equation\u2019 gets very large, the exponent gets correspondingly negative, and the value of e raised to that power goes to zero; the value of the whole expression therefore gets closer 1/(1+0) which is one. If the output of this regression equation is very negative, then e gets raised to a positive value, and the bottom of the fraction becomes very large; the value of the whole expression gets closer to 0.", "In our previous example, the fitted logistic curve looks like this:", "Our curve never goes below 0 or above 1, so we can sensibly interpret it as the probability of our binary variable being 1. The process for fitting this curve is essentially the same as when we fit the normal linear regression line. There, we were finding an intercept and a coefficient beta 1 that minimized the vertical distance between the line and the points, and here we are similarly finding an intercept and a coefficient, but now we\u2019re minimizing the distance to this curve.", "Once you have derived the most appropriate logistic curve, it\u2019s relatively simple to turn the predicted probabilities into predicted outcomes. Since the outcomes are binary, your predictions are as well. Most simply, and what most statistical packages are likely to do if you ask them for the predicted outcomes, you can simply predict the class anytime your logistic regression gives you a probability above 50%. Let\u2019s include these predictions in our visualization:", "Of course, since the model is producing more granular estimations for the probability at any point, you can use a logistic model to produce inputs for further models that themselves take in probabilities. Someone forecasting election results, for instance, might have a set of models predicting the outcomes of the election in each state and then use those probabilities in a model that predicts the range of outcomes across all states for the country as a whole.", "Logistic regression generalizes to multiple variables in much same the way that simple linear regression does, adding more features and corresponding coefficients to the regression formula:", "The coefficients in the logistic version are a little harder to interpret than in the ordinary linear regression. You can, in theory, directly interpret them by relating them to changes in the log-odds of the outcome being modeled, but what that means is a little opaque since practically speaking the effect on the probability that moving one of the input features will have depends where you start from. You can, however, interpret the direction of the coefficients simply. Features with positive coefficients increase the probability of the modeled outcome as they increase, while features with negative coefficients decrease the probability as they increase.", "Ridge and Lasso regularizations are also known as \u2018shrinkage\u2019 methods, because they reduce or shrink the coefficients in the resulting regression. This reduces the variance in the model: as input variables are changed, the model\u2019s prediction changes less than it would have without the regularization. Why would you want to reduce the variance of a model? To avoid overfit.", "Over fit results in our model failing to generalize. Sometimes we might find that we\u2019ve trained a model on some set of data, and it appears to work well on that data, but when we test it on some new set of data the performance suffers. There are a few reasons why this might be the case. Perhaps the sample from which we derived our model was biased in some way for instance. Another common problem is overfit, where the model too closely conforms to the training set and therefore misses the more generalizable trends.", "Consider creating a model to estimate the price that a new house on the market in a certain city will sell for. You have data on previous house sales and set about creating a linear regression. Obviously bigger houses tend to be more expensive than smaller ones, so you might naturally include features like square feet or number of bedrooms. You might also notice that different neighborhoods might seem to be priced differently, that, say, a typical two bedroom in one neighborhood may be more expensive than one in another neighborhood. So, you decide to include neighborhood in your model, but why stop there? Maybe there\u2019s variation within a neighborhood as well, so you consider including sub-neighborhood sized units, maybe individual streets or even individual blocks. You might be able to imagine why such finely grained markers of location would actually have an impact on the final price of the house \u2014 maybe one end of the block is too close a noisy, busy intersection and is less desirable or another part of the block gets better sun \u2014 but when it comes to actually train your model you\u2019ll run into a couple of challenges.", "For one thing, the more variables you include in a regression, the more likely you are to run into excessive covariance between features (something especially possible when adding interaction or power terms). For another, the amount of data supporting your estimation of any given coefficient may be small. When grouping house sales by neighborhood, you likely have many data points for each neighborhood, but if you split the data all the way down to the block level, any given block may only have a handful of examples you can use to train your model. If you were to include features for every single block, the coefficient for any block could be easily skewed by an outlier \u2014 if, say, one of the houses in your training data happened to sell for an uncharacteristically high or low price. Your model would represent you training data well, but wouldn\u2019t necessarily perform well on future predictions.", "How many features is too many? And which features are most important to include? Regularization can help. Ridge and Lasso regularization both work by adding a new term to the cost function used to derive your regression formula. Recall that an OLS regression finds the coefficients and intercept by minimizing the squared prediction errors across your training data, represented by a formula like this:", "Lasso regularization adds another term to this cost function, representing the sum of the magnitudes of all the coefficients in the model:", "In the above formula, the first term is the same sum of squared residuals we know and love, and the second term is a penalty whose size depends on the total magnitude of all the coefficients. The term in front of that sum, represented by the Greek letter lambda, is a tuning parameter that adjusts how large a penalty there will be. If it is set to 0, you end up with an ordinary OLS regression. Ridge regression follows the same pattern, but the penalty term is the sum of the coefficients squared:", "Including the extra penalty term essentially disincentives including extra features. A new feature may help your regression minimize the first term in the cost function by reducing the residual errors, but it will increase the penalty term. There is ultimately a balancing act here, where the value of increasing a coefficient is weighed against the corresponding increase to the overall variance of the model.", "Lowering the variance of the model can improve the model\u2019s accuracy on unseen data. In our housing price predictor example you can imagine including those block-by-block variables, but seeing the coefficients on those variables coming out pretty low. The one outlier house no longer throws all the predictions in that area off so much.", "Implicitly, Ridge and Lasso also act as their own sort of feature selection; features which don\u2019t drive the predictive power of the regression see their coefficients pushed down, while the more predictive features see higher coefficients despite the added the penalty. Ridge regression, because it squares the coefficients in the penalty term, tends to send coefficients on less useful features down close to zero, but not quite all the way to zero. Lasso, on the other hand, will helpfully send some coefficients all the way down to zero.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data scientist with a particular passion for limericks, policy and renewable energy."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F828b0d2d206c&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-basics-logistic-regression-and-regularization-828b0d2d206c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-basics-logistic-regression-and-regularization-828b0d2d206c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-basics-logistic-regression-and-regularization-828b0d2d206c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-basics-logistic-regression-and-regularization-828b0d2d206c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----828b0d2d206c--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----828b0d2d206c--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@max.samuel.miller?source=post_page-----828b0d2d206c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@max.samuel.miller?source=post_page-----828b0d2d206c--------------------------------", "anchor_text": "Max Miller"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdfd5ba1a8332&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-basics-logistic-regression-and-regularization-828b0d2d206c&user=Max+Miller&userId=dfd5ba1a8332&source=post_page-dfd5ba1a8332----828b0d2d206c---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F828b0d2d206c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-basics-logistic-regression-and-regularization-828b0d2d206c&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F828b0d2d206c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-basics-logistic-regression-and-regularization-828b0d2d206c&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----828b0d2d206c---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/logistic-regression?source=post_page-----828b0d2d206c---------------logistic_regression-----------------", "anchor_text": "Logistic Regression"}, {"url": "https://medium.com/tag/data-science-ground-up?source=post_page-----828b0d2d206c---------------data_science_ground_up-----------------", "anchor_text": "Data Science Ground Up"}, {"url": "https://medium.com/tag/data-science?source=post_page-----828b0d2d206c---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F828b0d2d206c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-basics-logistic-regression-and-regularization-828b0d2d206c&user=Max+Miller&userId=dfd5ba1a8332&source=-----828b0d2d206c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F828b0d2d206c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-basics-logistic-regression-and-regularization-828b0d2d206c&user=Max+Miller&userId=dfd5ba1a8332&source=-----828b0d2d206c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F828b0d2d206c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-basics-logistic-regression-and-regularization-828b0d2d206c&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----828b0d2d206c--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F828b0d2d206c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-basics-logistic-regression-and-regularization-828b0d2d206c&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----828b0d2d206c---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----828b0d2d206c--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----828b0d2d206c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----828b0d2d206c--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----828b0d2d206c--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----828b0d2d206c--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----828b0d2d206c--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----828b0d2d206c--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----828b0d2d206c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@max.samuel.miller?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@max.samuel.miller?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Max Miller"}, {"url": "https://medium.com/@max.samuel.miller/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "409 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdfd5ba1a8332&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-basics-logistic-regression-and-regularization-828b0d2d206c&user=Max+Miller&userId=dfd5ba1a8332&source=post_page-dfd5ba1a8332--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F930bd413e257&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-basics-logistic-regression-and-regularization-828b0d2d206c&newsletterV3=dfd5ba1a8332&newsletterV3Id=930bd413e257&user=Max+Miller&userId=dfd5ba1a8332&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}