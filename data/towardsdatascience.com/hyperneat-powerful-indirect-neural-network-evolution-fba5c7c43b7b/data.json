{"url": "https://towardsdatascience.com/hyperneat-powerful-indirect-neural-network-evolution-fba5c7c43b7b", "time": 1682994599.060142, "path": "towardsdatascience.com/hyperneat-powerful-indirect-neural-network-evolution-fba5c7c43b7b/", "webpage": {"metadata": {"title": "HyperNEAT: Powerful, Indirect Neural Network Evolution | by Hunter Heidenreich | Towards Data Science", "h1": "HyperNEAT: Powerful, Indirect Neural Network Evolution", "description": "Last week, I wrote an article about NEAT (NeuroEvolution of Augmenting Topologies) and we discussed a lot of the cool things that surrounded the algorithm. We also briefly touched upon how this older\u2026"}, "outgoing_paragraph_urls": [{"url": "http://hunterheidenreich.com/blog/neuroevolution-of-augmenting-topologies/", "anchor_text": "NEAT (NeuroEvolution of Augmenting Topologies)", "paragraph_index": 0}, {"url": "http://axon.cs.byu.edu/~dan/778/papers/NeuroEvolution/stanley3**.pdf", "anchor_text": "HyperNEAT", "paragraph_index": 1}, {"url": "http://axon.cs.byu.edu/~dan/778/papers/NeuroEvolution/stanley3**.pdf", "anchor_text": "\u201cA Hypercube-Based Indirect Encoding for Evolving Large-Scale Neural Networks\u201d", "paragraph_index": 4}, {"url": "https://twitter.com/hunter_heiden", "anchor_text": "Twitter", "paragraph_index": 32}], "all_paragraphs": ["Last week, I wrote an article about NEAT (NeuroEvolution of Augmenting Topologies) and we discussed a lot of the cool things that surrounded the algorithm. We also briefly touched upon how this older algorithm might even impact how we approach network building today, alluding to the fact that neural networks need not be built entirely by hand.", "Today, we are going to dive into a different approach to neuroevolution, an extension of NEAT called HyperNEAT. NEAT, as you might remember, had a direct encoding for its network structure. This was so that networks could be more intuitively evolved, node by node and connection by connection. HyperNEAT drops this idea because in order to evolve a network like the brain (with billions of neurons), one would need a much faster way of evolving that structure.", "HyperNEAT is a much more conceptually complex algorithm (in my opinion, at least) and even I am working on understanding the nuts and bolts of how it all works. Today, we will take a look under the hood and explore some of the components of this algorithm so that we might better understand what makes it so powerful and reason about future extensions in this age of deep learning.", "Before diving into the paper and algorithm, I think it\u2019s worth exploring a bit more the motivation behind HyperNEAT.", "The full name of the paper is \u201cA Hypercube-Based Indirect Encoding for Evolving Large-Scale Neural Networks\u201d, which is quite the mouthful! But already, we can see two of the major points. It\u2019s a hypercube-based indirect encoding. We\u2019ll get into the hypercube part later, but already we know that it\u2019s a move from direct encodings to indirect encodings (see my last blog on NEAT for a more detailed description of some of the differences between the two). Furthermore, we get the major reasoning behind it as well: For evolving big neural nets!", "More than that, the creators of this algorithm highlight that if one were to look at the brain, they see a \u201cnetwork\u201d with billions of nodes and trillions of connections. They see a network that uses repetition of structure, reusing a mapping of the same gene to generate the same physical structure multiple times. They also highlight that the human brain is constructed in a way so as to exploit physical properties of the world: symmetry (have mirrors of structures, two eyes for input for example) and locality (where nodes are in the structure influences their connections and functions).", "Contrast this what we know about neural networks, either built through an evolution procedure or constructed by hand and trained. Do any of these properties hold? Sure, if we force the network to have symmetry and locality, maybe\u2026 However, even then, take a dense, feed-forward network where all nodes in one layer are connected to all nodes in the next! And when looking at the networks constructed by the vanilla NEAT algorithm? They tend to be disorganized, sporadic, and not exhibit any of these nice regularities.", "Enter in HyperNEAT! Utilizing an indirect encoding through something called connective Compositional Pattern Producing Networks (CPPNs), HyperNEAT attempts to exploit geometric properties to produce very large neural networks with these nice features that we might like to see in our evolved networks.", "In the previous post, we discussed encodings and today we\u2019ll dive deeper into the indirect encoding used for HyperNEAT. Now, indirect encodings are a lot more common than you might think. In fact, you have one inside yourself!", "DNA is an indirect encoding because the phenotypic results (what we actually see) are orders of magnitude larger than the genotypic content (the genes in the DNA). If you look at a human genome, we\u2019ll say it has about 30,000 genes coding for approximately 3 billion amino acids. Well, the brain has 3 trillion connections. Obviously, there is something indirect going on here!", "Something borrowed from the ideas of biology is an encoding scheme called developmental encoding. This is the idea that all genes should be able to be reused at any point in time during the developmental process and at any location within the individual. Compositional Pattern Producing Networks (CPPNs) are an abstraction of this concept that have been show to be able to create patterns for repeating structures in Cartesian space. See some structures that were produced with CPPNs here:", "A phenotype can be described as a function of n dimensions, where n is the number of phenotypic traits. What we see is the result of some transformation from genetic encoding to the exhibited traits. By composing simple functions, complex patterns can actually be easily represented. Things like symmetry, repetition, asymmetry, and variation all easily fall out of an encoding structure like this depending on the types of networks that are produced.", "We\u2019ll go a little bit deeper into the specifics of how CPPNs are specifically used in this context, but hopefully this gives you the general feel for why and how they are important in the context of indirect encodings.", "In HyperNEAT, a bunch of familiar properties reappear for the original NEAT paper. Things like complexification over time are important (we\u2019ll start with simple and evolve complexity if and when it\u2019s needed). Historical markings will be used so that we can properly line up encodings for any sort of crossover. Uniform starting populations will also be used so that there\u2019s no wildcard, incompatible networks from the start.", "The major difference in how NEAT is used in this paper and the previous? Instead of using the NEAT algorithm to evolve neural networks directly, HyperNEAT uses NEAT to evolve CPPNs. This means that more \u201cactivation\u201d functions are used for CPPNs since things like Gaussians give rise to symmetry and trigonometric functions help with repetition of structure.", "So now that we\u2019ve talked about what a CPPN is and that we use the NEAT algorithm to evolve and adjust them, it begs the question of how are these actually used in the overall HyperNEAT context?", "First, we need to introduce the concept of a substrate. In the scope of HyperNEAT, a substrate is simply a geometric ordering of nodes. The simplest example could be a plane or a grid, where each discrete (x, y) point is a node. A connective CPPN will actually take two of these points and compute weight between these two nodes. We could think of that as the following equation:", "Where CPPN is an evolved CPPN, like that of what we\u2019ve discussed in previous sections. We can see that in doing this, every single node will actually have some sort of weight connection between them (even allowing for recurrent connections). Connections can be positive or negative, and a minimum weight magnitude can also be defined so that any outputs below that threshold will result in no connection.", "The geometric layout of nodes must be specified prior to the evolution of any CPPN. As a result, as the CPPN is evolved, the actually connection weights and network topology will result in a pattern that is geometric (all inputs are based on the positions of nodes).", "In the case where the nodes are arranged on some sort of 2 dimensional plane or grid, the CPPN is a function of four dimensions and thus we can say it is being evolved on a four dimensional hypercube. This is where we get the name of the paper!", "All regularities that we\u2019ve mentioned before can easily fall out of an encoding like this. Symmetry can occur by using symmetric functions over something like x1 and x2. This can be a function like a Gaussian function. Imperfect symmetry can occur when symmetry is used over things like both x and y, but only with respect to one axis.", "Repetition also falls out, like we\u2019ve mentioned before, with periodic functions such as sine, cosine, etc. And like with symmetry, variation against repetition can be introduced by inducing a periodic function over a non-repeating aspect of the substrate. Thus all four major regularities that were aimed for are able to develop from this encoding.", "You may have guessed from the above that the configuration of the substrate is critical. And that makes a lot of sense. In biology, the structure of something is tied to its functionality. Therefore, in our own evolution schema, the structure of our nodes are tightly linked to the functionality and performance that may be seen on a particular task.", "Here, we can see a couple of substrate configurations specifically outlined in the original paper:", "I think it\u2019s very important to look at the configuration that is a three dimensional cube and note how it simply adjusts our CPPN equation from four dimensional to six dimensional:", "Also, the grid can be extended to the sandwich configuration by only allowing for nodes on one half to connect to the other half. This can be seen easily as an input/output configuration! The authors of the paper actually use this configuration to take in visual activation on the input half and use it to activate certain nodes on the output half.", "The circular layout is also interesting, as geometry need not be a grid for a configuration. A radial geometry can be used instead, allowing for interesting behavioral properties to spawn out of the unique geometry that a circle can represent.", "Inputs and outputs are laid out prior to the evolution of CPPNs. However, unlike a traditional neural network, our HyperNEAT algorithm is made aware of the geometry of the inputs and outputs and can learn to exploit and embrace the regularities of it. Locality and repetition of inputs and outputs can be easily exploited through this extra information that HyperNEAT receives.", "Another powerful and unique property of HyperNEAT is the ability to scale the resolution of a substrate up and down. What does that mean? Well, let\u2019s say you evolve a HyperNEAT network based on images of a certain size. The underlying geometry that was exploited to perform well at that size results in the same pattern when scaled to a new size. Except, no extra training is needed. It simply scales to another size!", "I think with all that information about how this algorithm works, it\u2019s worth summarizing the steps of it.", "And there we have it! That\u2019s the HyperNEAT algorithm. I encourage you to take a look at the paper if you wish to explore more of the details or wish to look at the performance on some of the experiments they did with the algorithm (I particularly enjoy their food gathering robot experiment).", "What are the implications for the future? That\u2019s something I\u2019ve been thinking about recently as well. Is there a tie-in from HyperNEAT to training traditional deep networks today? Is this a better way to train deep networks? There\u2019s another paper of Evolvable Substrate HyperNEAT where the actual substrates are evolved as well, a paper I wish to explore in the future! But is there something hidden in that paper that bridges the gap between HyperNEAT and deep neural networks? Only time will tell and only we can answer that question!", "Hopefully, this article was helpful! If I missed anything or if you have questions, let me know. I\u2019m still learning and exploring a lot of this myself so I\u2019d be more than happy to have a conversation about it on here or on Twitter.", "And if you want to read more of what I\u2019ve written, maybe check out:", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "CS Undergrad at Drexel University | AI | ML | NLP | DS"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Ffba5c7c43b7b&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperneat-powerful-indirect-neural-network-evolution-fba5c7c43b7b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperneat-powerful-indirect-neural-network-evolution-fba5c7c43b7b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperneat-powerful-indirect-neural-network-evolution-fba5c7c43b7b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperneat-powerful-indirect-neural-network-evolution-fba5c7c43b7b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----fba5c7c43b7b--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----fba5c7c43b7b--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@hunterheidenreich?source=post_page-----fba5c7c43b7b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@hunterheidenreich?source=post_page-----fba5c7c43b7b--------------------------------", "anchor_text": "Hunter Heidenreich"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F66c914ddeac8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperneat-powerful-indirect-neural-network-evolution-fba5c7c43b7b&user=Hunter+Heidenreich&userId=66c914ddeac8&source=post_page-66c914ddeac8----fba5c7c43b7b---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffba5c7c43b7b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperneat-powerful-indirect-neural-network-evolution-fba5c7c43b7b&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffba5c7c43b7b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperneat-powerful-indirect-neural-network-evolution-fba5c7c43b7b&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "http://hunterheidenreich.com/blog/neuroevolution-of-augmenting-topologies/", "anchor_text": "NEAT (NeuroEvolution of Augmenting Topologies)"}, {"url": "http://axon.cs.byu.edu/~dan/778/papers/NeuroEvolution/stanley3**.pdf", "anchor_text": "HyperNEAT"}, {"url": "http://axon.cs.byu.edu/~dan/778/papers/NeuroEvolution/stanley3**.pdf", "anchor_text": "\u201cA Hypercube-Based Indirect Encoding for Evolving Large-Scale Neural Networks\u201d"}, {"url": "https://twitter.com/hunter_heiden", "anchor_text": "Twitter"}, {"url": "http://hunterheidenreich.com/blog/what-is-a-gan/", "anchor_text": "What is a Generative Adversarial Network"}, {"url": "http://hunterheidenreich.com/blog/intro-to-word-embeddings/", "anchor_text": "Introduction to Word Embeddings"}, {"url": "http://hunterheidenreich.com/blog/knowledge-based-agents/", "anchor_text": "An Introduction to Knowledge-Based Agents"}, {"url": "http://hunterheidenreich.com/blog/next-gen-neuroevolution-hyperneat/", "anchor_text": "hunterheidenreich.com"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----fba5c7c43b7b---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----fba5c7c43b7b---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----fba5c7c43b7b---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/ai?source=post_page-----fba5c7c43b7b---------------ai-----------------", "anchor_text": "AI"}, {"url": "https://medium.com/tag/evolution?source=post_page-----fba5c7c43b7b---------------evolution-----------------", "anchor_text": "Evolution"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffba5c7c43b7b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperneat-powerful-indirect-neural-network-evolution-fba5c7c43b7b&user=Hunter+Heidenreich&userId=66c914ddeac8&source=-----fba5c7c43b7b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffba5c7c43b7b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperneat-powerful-indirect-neural-network-evolution-fba5c7c43b7b&user=Hunter+Heidenreich&userId=66c914ddeac8&source=-----fba5c7c43b7b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffba5c7c43b7b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperneat-powerful-indirect-neural-network-evolution-fba5c7c43b7b&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----fba5c7c43b7b--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Ffba5c7c43b7b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperneat-powerful-indirect-neural-network-evolution-fba5c7c43b7b&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----fba5c7c43b7b---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----fba5c7c43b7b--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----fba5c7c43b7b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----fba5c7c43b7b--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----fba5c7c43b7b--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----fba5c7c43b7b--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----fba5c7c43b7b--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----fba5c7c43b7b--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----fba5c7c43b7b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@hunterheidenreich?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@hunterheidenreich?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Hunter Heidenreich"}, {"url": "https://medium.com/@hunterheidenreich/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "915 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F66c914ddeac8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperneat-powerful-indirect-neural-network-evolution-fba5c7c43b7b&user=Hunter+Heidenreich&userId=66c914ddeac8&source=post_page-66c914ddeac8--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F8a81cf6d293c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperneat-powerful-indirect-neural-network-evolution-fba5c7c43b7b&newsletterV3=66c914ddeac8&newsletterV3Id=8a81cf6d293c&user=Hunter+Heidenreich&userId=66c914ddeac8&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}