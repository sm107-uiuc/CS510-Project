{"url": "https://towardsdatascience.com/dimensionality-reduction-with-latent-dirichlet-allocation-8d73c586738c", "time": 1682996481.694071, "path": "towardsdatascience.com/dimensionality-reduction-with-latent-dirichlet-allocation-8d73c586738c/", "webpage": {"metadata": {"title": "Dimensionality-Reduction with Latent Dirichlet Allocation | by Xiao Ma | Towards Data Science", "h1": "Dimensionality-Reduction with Latent Dirichlet Allocation", "description": "Dimensionality-reduction is an unsupervised machine learning technique that is often used in conjunction with supervised models. While reducing the dimensionality often makes a feature-based model\u2026"}, "outgoing_paragraph_urls": [{"url": "http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf", "anchor_text": "research paper", "paragraph_index": 1}, {"url": "https://en.wikipedia.org/wiki/Dirichlet_distribution", "anchor_text": "Dirichlet distribution", "paragraph_index": 2}, {"url": "https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge", "anchor_text": "Toxic Comments Classification Kaggle challenge", "paragraph_index": 5}], "all_paragraphs": ["Dimensionality-reduction is an unsupervised machine learning technique that is often used in conjunction with supervised models. While reducing the dimensionality often makes a feature-based model less interpretable, it\u2019s always very effective in preventing over-fitting and shortening the training time by reducing the number of features. The most popular dimensionality-reduction techniques are no doubt the Principle Component Analysis (PCA) and its variants. However, when it comes to text data, I find Latent Dirichlet Allocation (LDA) excitingly effective.", "The idea of Latent Dirichlet Allocation was first introduced in the research paper authored by David Blei, Andrew Ng, and Michael Jordan in 2003. It is described by the authors as a \u201cgenerative probabilistic model of a corpus.\u201d LDA is widely used in performing Topic Modeling \u2014 a statistical technique that can extract underlying themes/topics from a corpus.", "In a traditional Bag-of-words approach for text feature extraction, we map each document directly to all the word tokens through a Document-Term matrix. This approach often results in a huge, sparse matrix with the majority of entries equal 0 \u2014 we have lots of parameters to estimate when we use such a matrix as model inputs, but many of them provide limited and sometimes \u201cnoisy\u201d information. In the LDA approach, instead of modeling the relationships between each text document and each word token directly, the \u201cLatent variables\u201d are introduced as \u201cbridges.\u201d Each document within the corpus is characterized by a Dirichlet distribution over the latent variables (topics) and each topic is characterized by another Dirichlet Distribution over all the word tokens.", "Assuming the number of documents to be N, and we choose the number of topics to be K. The idea of reducing dimension with LDA is to focus on the first Dirichlet Distribution mentioned above. For each document, we can obtain a vector of length K which represents the probability distribution of the document over K topics. Appending such vectors for all the documents, we can get an N-by-K feature matrix that can be input into our supervised model with certain labels.", "Recently, inspired by the original research paper cited above, I did a small experiment in exploring the performance of LDA in dimensionality reduction. In the experiment, I was able to reduce the dimension of training data by over 99% with less than 3% drop in raw accuracy. Next, let me illustrate this experiment in greater details.", "The data I used comes from the training data of the Toxic Comments Classification Kaggle challenge. Since my goal was to explore LDA in dimensionality reduction, I subset and filtered the data in the following ways to simplify the process:", "As the first step, I built an XG-boost classification model using the standard uni-gram approach with a TF-IDF vectorizer:", "2. Fit the TF-IDF vectorizer for n_gram models. Notice that I filter the tokens so that tokens appear in less than 15 documents more than 90% of the documents are removed. tokenizer_xm is a customized tokenizer with tokenization, lemmatization and stemming included", "3. Get the document-term matrix for n-gram model and fit with training data.", "Next, I start building the model with LDA. To use LDA, we need to obtain the Document-Term-Matrix first. Since LDA is based on raw counts, it is better to use a Count-Vectorizer instead of TF-IDF.", "Create the training document-term matrix for LDA weights", "As in the code above, LDA requires a \u201cn_components\u201d parameter, which is essentially the number of topics. However, we usually don\u2019t know how many \u201ctopics\u201d are there in the corpus. Therefore, I repeated the process above 9 times with topic number equals to 1, 2, 3, 4, 5, 6, 12, 24, 50 and score the model against the test data, attempting to find a relatively optimal value. Below are the results.", "It seems that the best topic number is 5 with an accuracy of around 84%. Below are the top 10 \u201csalient\u201d tokens for each of the five topics based on the Dirichlet distribution.", "We are not digging deep into the performance of topic modeling with LDA in this post. But it is interesting to note the differences among topics. Topic 1 appears to be the most neutral one based while topic 5 is extremely toxic. The Dirichlet Distribution actually helps separate the non-toxic comments out from the corpus.", "Next, I ran the entire process above, with 1%, 25%, 50%, 75% and 100% of the training data, recorded the performance of the N-gram based (xgbc_ngram) as well as the LDA-based (xgbc_lda) Extreme Gradient Boosting Classifier (notice that the two models are both initialized with 200 estimators and have all other hyperparameters left as defaults) and compared their precision, recall, and accuracy scores.", "Below is the raw table for the plots above:", "As in the figures shown above, although the accuracy for the N-gram-based model is always slightly higher than the LDA-based one, it eventually reaches 3367 distinct features with 100% of the training data while the feature number for LDA-based model still remains 5 \u2014 a significant difference! Interestingly, the LDA-based model actually out-performs the n-gram model in terms of Recall while it is not doing so well in terms of Precision. That is, the LDA-based model is better at finding all toxic examples, but not very good at preventing non-toxic comments from getting labeled as toxic. In general, the LDA-based model seems to have a better balance between precision and recall.", "Using LDA for dimensionality reduction, we cannot bypass the n-gram feature extraction process. Therefore we didn\u2019t get too much of a boost in shortening the end-to-end model building time. However, while LDA seems to be at least as efficient as PCA at dimensionality reduction, it is very interpretable. It\u2019s a promising technique in feature-based text classification/regression models as we can compress all the information in text into a low-dimension and dense matrix so that we have the ability to add many other features to help improve the model performance without worrying about over-fitting.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F8d73c586738c&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdimensionality-reduction-with-latent-dirichlet-allocation-8d73c586738c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdimensionality-reduction-with-latent-dirichlet-allocation-8d73c586738c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdimensionality-reduction-with-latent-dirichlet-allocation-8d73c586738c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdimensionality-reduction-with-latent-dirichlet-allocation-8d73c586738c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----8d73c586738c--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8d73c586738c--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@marshalma0923?source=post_page-----8d73c586738c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@marshalma0923?source=post_page-----8d73c586738c--------------------------------", "anchor_text": "Xiao Ma"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4ed3d24bf2c1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdimensionality-reduction-with-latent-dirichlet-allocation-8d73c586738c&user=Xiao+Ma&userId=4ed3d24bf2c1&source=post_page-4ed3d24bf2c1----8d73c586738c---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8d73c586738c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdimensionality-reduction-with-latent-dirichlet-allocation-8d73c586738c&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8d73c586738c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdimensionality-reduction-with-latent-dirichlet-allocation-8d73c586738c&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf", "anchor_text": "research paper"}, {"url": "https://en.wikipedia.org/wiki/Dirichlet_distribution", "anchor_text": "Dirichlet distribution"}, {"url": "https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge", "anchor_text": "Toxic Comments Classification Kaggle challenge"}, {"url": "https://medium.com/tag/data-science?source=post_page-----8d73c586738c---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----8d73c586738c---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/dimensionality-reduction?source=post_page-----8d73c586738c---------------dimensionality_reduction-----------------", "anchor_text": "Dimensionality Reduction"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----8d73c586738c---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8d73c586738c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdimensionality-reduction-with-latent-dirichlet-allocation-8d73c586738c&user=Xiao+Ma&userId=4ed3d24bf2c1&source=-----8d73c586738c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8d73c586738c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdimensionality-reduction-with-latent-dirichlet-allocation-8d73c586738c&user=Xiao+Ma&userId=4ed3d24bf2c1&source=-----8d73c586738c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8d73c586738c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdimensionality-reduction-with-latent-dirichlet-allocation-8d73c586738c&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8d73c586738c--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F8d73c586738c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdimensionality-reduction-with-latent-dirichlet-allocation-8d73c586738c&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----8d73c586738c---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----8d73c586738c--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----8d73c586738c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----8d73c586738c--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----8d73c586738c--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----8d73c586738c--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----8d73c586738c--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----8d73c586738c--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----8d73c586738c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@marshalma0923?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@marshalma0923?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Xiao Ma"}, {"url": "https://medium.com/@marshalma0923/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "45 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4ed3d24bf2c1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdimensionality-reduction-with-latent-dirichlet-allocation-8d73c586738c&user=Xiao+Ma&userId=4ed3d24bf2c1&source=post_page-4ed3d24bf2c1--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F4ed3d24bf2c1%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdimensionality-reduction-with-latent-dirichlet-allocation-8d73c586738c&user=Xiao+Ma&userId=4ed3d24bf2c1&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}