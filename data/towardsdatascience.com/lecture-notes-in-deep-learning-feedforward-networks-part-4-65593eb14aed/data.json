{"url": "https://towardsdatascience.com/lecture-notes-in-deep-learning-feedforward-networks-part-4-65593eb14aed", "time": 1683009945.495683, "path": "towardsdatascience.com/lecture-notes-in-deep-learning-feedforward-networks-part-4-65593eb14aed/", "webpage": {"metadata": {"title": "Feedforward Networks \u2014 Part 4. Layer Abstraction | by Andreas Maier | Towards Data Science", "h1": "Feedforward Networks \u2014 Part 4", "description": "These are the lecture notes for FAU\u2019s YouTube Lecture \u201cDeep Learning\u201d. This is a full transcript of the lecture video & matching slides. We hope, you enjoy this as much as the videos. Of course, this\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/lecture-notes-in-deep-learning-feedforward-networks-part-3-d2a0441b8bca", "anchor_text": "Previous Lecture", "paragraph_index": 1}, {"url": "https://youtu.be/BTbHaKsH4y0", "anchor_text": "Watch this Video", "paragraph_index": 1}, {"url": "https://towardsdatascience.com/all-you-want-to-know-about-deep-learning-8d68dcffc258", "anchor_text": "Top Level", "paragraph_index": 1}, {"url": "https://towardsdatascience.com/lecture-notes-in-deep-learning-loss-and-optimization-part-1-f702695cbd99", "anchor_text": "Next Lecture", "paragraph_index": 1}, {"url": "https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf", "anchor_text": "matrix cookbook", "paragraph_index": 6}, {"url": "https://medium.com/@akmaier", "anchor_text": "more essays here", "paragraph_index": 15}, {"url": "https://lme.tf.fau.de/teaching/free-deep-learning-resources/", "anchor_text": "here", "paragraph_index": 15}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj", "anchor_text": "Deep", "paragraph_index": 15}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Learning", "paragraph_index": 15}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj", "anchor_text": "Lecture", "paragraph_index": 15}, {"url": "https://www.youtube.com/c/AndreasMaierTV", "anchor_text": "YouTube", "paragraph_index": 15}, {"url": "https://twitter.com/maier_ak", "anchor_text": "Twitter", "paragraph_index": 15}, {"url": "https://www.facebook.com/andreas.maier.31337", "anchor_text": "Facebook", "paragraph_index": 15}, {"url": "https://www.linkedin.com/in/andreas-maier-a6870b1a6/", "anchor_text": "LinkedIn", "paragraph_index": 15}, {"url": "https://creativecommons.org/licenses/by/4.0/deed.de", "anchor_text": "Creative Commons 4.0 Attribution License", "paragraph_index": 15}], "all_paragraphs": ["These are the lecture notes for FAU\u2019s YouTube Lecture \u201cDeep Learning\u201d. This is a full transcript of the lecture video & matching slides. We hope, you enjoy this as much as the videos. Of course, this transcript was created with deep learning techniques largely automatically and only minor manual modifications were performed. If you spot mistakes, please let us know!", "Previous Lecture / Watch this Video / Top Level / Next Lecture", "Welcome, everybody to our next video on deep learning! So, today we want to talk about again feed-forward networks. In the fourth part, the main focus will be on the layer abstraction. Of course, we talked about those neurons and individual nodes but this grows really complex for larger networks. So we want to introduce this layering concept also in our computation of the gradients. This is really useful because we can then talk directly about gradients on entire layers and don\u2019t need to go towards all of the different nodes.", "So, how do we express this? Let\u2019s recall what our single neuron is doing. The single neuron is computing essentially an inner product of its weights. By the way, we are skipping over this bias notation. So, we are expanding this vector by one additional element. This allows us to describe the bias also and the inner product as shown on the slide here. This is really nice because then you can see that the output y hat is just an inner product.", "Now think about the case that we have M neurons which means that we get some y hat index m. All of them are inner products. So, if you bring this into a vector notation, you can see that the vector y hat is nothing else than a matrix multiplication of x with this matrix W. You see that a fully connected layer is nothing else than matrix multiplication. So, we can essentially represent arbitrary connections and topologies using this fully connected layer. Then, we also apply a pointwise non-linearity such that we get the nonlinear effect. The nice thing about matrix notation is of course that we can describe now the entire layer derivatives using matrix calculus.", "So, our fully connected layer would then get the following configuration: Three elements for the input and then weights for every neuron. Let\u2019s say you have two neurons, then we get these weight vectors. We multiply the two with x. In the forward pass, we have determined this y hat for the entire module using a matrix. If you want to compute the gradients, then we need exactly two partial derivatives. These are the same as we already mentioned: We need the derivative with respect to the weights. This is going to be the partial derivative with respect to W and the partial derivatives with respect to x for the backpropagation to pass it on to the next module.", "So how do we compute this? Well, we have the layer that is y hat equals to W x. So there\u2019s a matrix multiplication in the forward pass. Then, we need the derivative with respect to the weights. Now you can see that what we essentially need to do is we need a matrix derivative here. The derivative of y hat with respect to W is going to be simply x\u1d40. So, if we have the loss that comes into our module, the update to our weights is gonna be this loss vector multiplied with x\u1d40. So, we have some loss vector and x\u1d40 which essentially means that you have an outer product. One is a column vector and the other one is a row vector because of the transpose. So, if you multiply the two, you will end up with a matrix. The above partial derivative with respect to W will always result in a matrix. Then if you look at the bottom row, you need the partial derivative of y hat with respect to x. Also something you can find in the matrix cookbook, by the way. It is very very useful. You find all kinds of matrix derivatives in this one. So if you do that, you can see for the above equation, the partial with respect to x is going to be W\u1d40. Now, you have W\u1d40 multiplied again by some loss vector. This loss vector times a matrix is going to be a vector again. This is the vector that you will pass on in the backpropagation process towards the next higher layer.", "Okay so let\u2019s look into some example. We have a simple example first and then a multi-layer example next. So, the simple example is going to be the same network as we had it already. So this was network without any non-linearity W x. Now, we need some loss function. Here, we don\u2019t take cross-entropy, but we take the L2 loss which is a common vector norm. What it does is simply take the output of the network subtract and the desired output and compute the L2 norm. This means that we element-wise square the different vector values and sum all of them up. In the end, we would have to take a square root, but we want to omit this. So, we take it to the power of two. When we now compute the derivatives of this L2-norm to the power of 2, of course, we have a factor of two showing up. This will be canceled out by this factor 1 over 2 in the beginning. By the way, this is a regression loss and also has statistical relations. We will talk about this when we talk about loss functions in more detail. The nice thing with L2 loss is that that you also find its matrix derivatives the matrix cookbook. We now compute the partial derivative of L with respect to y hat. This will give us then Wx \u2014 y and we can continue and compute the update for our weights. So the update for the weights is what we compute using the loss function\u2019s derivative. The derivative of the loss function with respect to the input was Wx \u2014 y times x\u1d40. This will give us an update for the matrix weight. The other derivative that we want to compute is the partial derivative of the loss with respect to x. So, this is going to be \u2014 as we\u2019ve seen on the previous slide \u2014 W\u1d40 times the vector that comes from the loss function: Wx \u2014 y, as we determined in the third row of the slide.", "Ok so let\u2019s add some layers and change our estimator into three nested functions. Here, we have some linear matrices. So, this is an academic example: you could see that by multiplying W\u2081, W\u2082, and W\u2083 with each other, they would simply collapse into a single matrix. Still, I find this example useful because it shows you what actually happens in the computation of the backpropagation process and why those specific steps are really useful. So, again we take the L2 loss function. Here, we have our three matrices inside.", "Next, we have to go ahead and compute derivatives. Now for the derivatives, we start with Layer 3, the most outer layer. So, you see that we now compute the partial derivative of the loss function with respect to W\u2083. First, the chain rule. Then, we have to compute the partial derivative of the loss function with respect to f\u2083(x) hat with respect to W\u2083. The partial derivative of the loss function again is simply the inner part of the L2 norm. So is this W\u2083 W\u2082 W\u2081 x \u2014 y. The partial derivative of the net is gonna be (W\u2082 W\u2081 x)\u1d40, as we\u2019ve seen on the previous slide. Note that I\u2019m indicating the affinity of the matrix operator using a dot. For matrices, it makes a difference whether you multiply them from the left or from the right. Both multiplication directions are different. Hence, I\u2019m indicating that you have to compute this product from the right-hand side. Now let\u2019s do that and we end up with the final update for W\u2083 that is simply computed from those two expressions.", "Now, the partial derivative with respect to W\u2082 is a bit more complicated because we have to apply the chain rule twice. So, again we have to compute the partial derivative of the loss function with respect to f\u2083(x) hat. Then, we need the partial derivative of f\u2083(x) hat with respect to W\u2082 which means we have to apply the chain rule again. So we have to expand the partial derivative of f\u2083(x) hat with respect to f\u2082(x) hat and then the partial derivative of f\u2082(x) hat with respect to W\u2082. This doesn\u2019t change much. The Loss term is the same as we used before. Now, if we compute the partial derivative of f\u2083(x) hat with respect to f\u2082(x) hat \u2014 remember f\u2082(x) = W\u2082 W\u2081 x \u2014 it\u2019s gonna be W\u2083\u1d40 and we have to multiply it from the left-hand side. Then, we go ahead and compute the partial derivative of f\u2082(x) hat with respect to W\u2082. You remain with (W\u2081 x)\u1d40. So, the final matrix derivative is going to be the product of the three terms. We can repeat this for the last layer, but now we have to apply the chain rule again. We see already two parts that we pre-computed, but we have to apply it again. So here we then get the partial derivative of f\u2082(x) hat with respect to f\u2081(x) hat and a partial derivative of f\u2081(x) hat with respect to W\u2081 which then yields two terms that we used before. The partial derivative of f\u2082(x) hat with respect to f\u2081(x) hat that is W\u2081 x, is going to be W\u2082\u1d40. Then, we still have to compute the partial derivative of f\u2081(x) with respect to W\u2081. This is going to be x\u1d40. So, we end up with the product of four terms for this partial derivative.", "Now, you can see if we do the backpropagation algorithm, we end up in a very similar way of processing. So first, we compute the forward path through our entire network and evaluate the loss function. Then, we can look at the different partial derivatives, and depending on where I want to go, I have to compute the respective partials. For the update of the last layer, I have to compute the partial derivative of the loss function and multiply it with the partial derivative of the last layer with respect to the weights. Now, if I go the second last layer, I have to compute the partial derivative with respect to the loss function, the partial derivative of the last layer from respect to the inputs, and the partial derivative of the second last layer with respect to the weights to get the update. If I want to go to the first layer, I have to compute all the respective backpropagation steps for the entire layers until I end up with the respective update on the very first layer. You can see that we can pre-compute a lot of those values and reuse them which allows us to implement backpropagation very efficiently.", "Let\u2019s summarize what we\u2019ve seen so far. We\u2019ve seen that we can combine the softmax activation function with the cross-entropy loss. Then, we can very naturally work with multi-class problems. We used gradient descent as the default choice for training network and we can achieve local minima using the strategy. We can, of course, compute gradients only numerically by finite differences and this is very useful for checking your implementations. This is something you will definitely need in the exercises! Then, we used the backpropagation algorithm to compute the gradients very efficiently. In order to be able to update the weights of the fully connected layers, we\u2019ve seen that they can be abstracted as a complete layer. Hence, we can also compute layer-wise derivatives. So, it\u2019s not required to compute everything on a node level, but you can really go into layer abstraction. You also saw that matrix calculus turns out to be very useful.", "What happens next time in deep learning? Well, we will see that right now, we have only a limited number of loss functions. So, we will see problem adapted loss functions for regression and classification. The very simple optimization that we talked about right now with a single \u03b7 is probably not the right way to go. So, there are much better optimization programs. They can be adapted to the needs of every single parameter. Then we\u2019ll also see an argument why neural networks shouldn\u2019t perform that well and some recent insights why they actually do perform quite well.", "I also have a couple of comprehensive questions. So you should definitely be able to name different loss functions for multi-class classification. One-hot encoding is something everybody needs to know if you want to take the oral exam with me. You will have to be able to describe this. Then, of course, something I probably won\u2019t ask in the exam but something that will be very useful for your daily routine is that you work with finite differences and use them for implementation checks. You have to be able to describe the backpropagation algorithm and to be honest, I think this \u2014 although it\u2019s academic \u2014 but this multi-layer way abstraction way of describing backpropagation algorithm is really useful. It\u2019s also very nice if you want to explain the backpropagation in an exam situation. What else do you have to be able to describe? The problem with exploding and vanishing gradients: What happens if you choose your \u03b7 too high or too low? What\u2019s a lost curve and how does it change over the iterations? Take a look at those graphs. They are really relevant and they also help you understand what\u2019s going wrong in your training process. So you need to be aware of those and also it should be clear to you by now why the sign function is a bad choice for an activation function. We have plenty of references below this post. So, I hope you still had fun with those videos. Please continue watching and see you in the next video!", "If you liked this post, you can find more essays here, more educational material on Machine Learning here, or have a look at our Deep Learning Lecture. I would also appreciate a clap or a follow on YouTube, Twitter, Facebook, or LinkedIn in case you want to be informed about more essays, videos, and research in the future. This article is released under the Creative Commons 4.0 Attribution License and can be reprinted and modified if referenced.", "[1] R. O. Duda, P. E. Hart, and D. G. Stork. Pattern Classification. John Wiley and Sons, inc., 2000.[2] Christopher M. Bishop. Pattern Recognition and Machine Learning (Information Science and Statistics). Secaucus, NJ, USA: Springer-Verlag New York, Inc., 2006.[3] F. Rosenblatt. \u201cThe perceptron: A probabilistic model for information storage and organization in the brain.\u201d In: Psychological Review 65.6 (1958), pp. 386\u2013408.[4] WS. McCulloch and W. Pitts. \u201cA logical calculus of the ideas immanent in nervous activity.\u201d In: Bulletin of mathematical biophysics 5 (1943), pp. 99\u2013115.[5] D. E. Rumelhart, G. E. Hinton, and R. J. Williams. \u201cLearning representations by back-propagating errors.\u201d In: Nature 323 (1986), pp. 533\u2013536.[6] Xavier Glorot, Antoine Bordes, and Yoshua Bengio. \u201cDeep Sparse Rectifier Neural Networks\u201d. In: Proceedings of the Fourteenth International Conference on Artificial Intelligence Vol. 15. 2011, pp. 315\u2013323.[7] William H. Press, Saul A. Teukolsky, William T. Vetterling, et al. Numerical Recipes 3rd Edition: The Art of Scientific Computing. 3rd ed. New York, NY, USA: Cambridge University Press, 2007.", "I do research in Machine Learning. My positions include being Prof @FAU_Germany, President @DataDonors, and Board Member for Science & Technology @TimeMachineEU"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F65593eb14aed&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flecture-notes-in-deep-learning-feedforward-networks-part-4-65593eb14aed&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flecture-notes-in-deep-learning-feedforward-networks-part-4-65593eb14aed&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flecture-notes-in-deep-learning-feedforward-networks-part-4-65593eb14aed&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flecture-notes-in-deep-learning-feedforward-networks-part-4-65593eb14aed&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/tagged/fau-lecture-notes", "anchor_text": "FAU Lecture Notes"}, {"url": "https://akmaier.medium.com/?source=post_page-----65593eb14aed--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----65593eb14aed--------------------------------", "anchor_text": ""}, {"url": "https://akmaier.medium.com/?source=post_page-----65593eb14aed--------------------------------", "anchor_text": "Andreas Maier"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb1444918afee&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flecture-notes-in-deep-learning-feedforward-networks-part-4-65593eb14aed&user=Andreas+Maier&userId=b1444918afee&source=post_page-b1444918afee----65593eb14aed---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----65593eb14aed--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F65593eb14aed&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flecture-notes-in-deep-learning-feedforward-networks-part-4-65593eb14aed&user=Andreas+Maier&userId=b1444918afee&source=-----65593eb14aed---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F65593eb14aed&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flecture-notes-in-deep-learning-feedforward-networks-part-4-65593eb14aed&source=-----65593eb14aed---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning"}, {"url": "https://towardsdatascience.com/lecture-notes-in-deep-learning-feedforward-networks-part-3-d2a0441b8bca", "anchor_text": "Previous Lecture"}, {"url": "https://youtu.be/BTbHaKsH4y0", "anchor_text": "Watch this Video"}, {"url": "https://towardsdatascience.com/all-you-want-to-know-about-deep-learning-8d68dcffc258", "anchor_text": "Top Level"}, {"url": "https://towardsdatascience.com/lecture-notes-in-deep-learning-loss-and-optimization-part-1-f702695cbd99", "anchor_text": "Next Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf", "anchor_text": "matrix cookbook"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://medium.com/@akmaier", "anchor_text": "more essays here"}, {"url": "https://lme.tf.fau.de/teaching/free-deep-learning-resources/", "anchor_text": "here"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj", "anchor_text": "Deep"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Learning"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj", "anchor_text": "Lecture"}, {"url": "https://www.youtube.com/c/AndreasMaierTV", "anchor_text": "YouTube"}, {"url": "https://twitter.com/maier_ak", "anchor_text": "Twitter"}, {"url": "https://www.facebook.com/andreas.maier.31337", "anchor_text": "Facebook"}, {"url": "https://www.linkedin.com/in/andreas-maier-a6870b1a6/", "anchor_text": "LinkedIn"}, {"url": "https://creativecommons.org/licenses/by/4.0/deed.de", "anchor_text": "Creative Commons 4.0 Attribution License"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----65593eb14aed---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----65593eb14aed---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----65593eb14aed---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----65593eb14aed---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/fau-lecture-notes?source=post_page-----65593eb14aed---------------fau_lecture_notes-----------------", "anchor_text": "Fau Lecture Notes"}, {"url": "http://creativecommons.org/licenses/by/4.0/", "anchor_text": "Some rights reserved"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F65593eb14aed&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flecture-notes-in-deep-learning-feedforward-networks-part-4-65593eb14aed&user=Andreas+Maier&userId=b1444918afee&source=-----65593eb14aed---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F65593eb14aed&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flecture-notes-in-deep-learning-feedforward-networks-part-4-65593eb14aed&user=Andreas+Maier&userId=b1444918afee&source=-----65593eb14aed---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F65593eb14aed&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flecture-notes-in-deep-learning-feedforward-networks-part-4-65593eb14aed&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://akmaier.medium.com/?source=post_page-----65593eb14aed--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----65593eb14aed--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb1444918afee&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flecture-notes-in-deep-learning-feedforward-networks-part-4-65593eb14aed&user=Andreas+Maier&userId=b1444918afee&source=post_page-b1444918afee----65593eb14aed---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fa5f0dee142a2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flecture-notes-in-deep-learning-feedforward-networks-part-4-65593eb14aed&newsletterV3=b1444918afee&newsletterV3Id=a5f0dee142a2&user=Andreas+Maier&userId=b1444918afee&source=-----65593eb14aed---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://akmaier.medium.com/?source=post_page-----65593eb14aed--------------------------------", "anchor_text": "Written by Andreas Maier"}, {"url": "https://akmaier.medium.com/followers?source=post_page-----65593eb14aed--------------------------------", "anchor_text": "2.2K Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----65593eb14aed--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb1444918afee&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flecture-notes-in-deep-learning-feedforward-networks-part-4-65593eb14aed&user=Andreas+Maier&userId=b1444918afee&source=post_page-b1444918afee----65593eb14aed---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fa5f0dee142a2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flecture-notes-in-deep-learning-feedforward-networks-part-4-65593eb14aed&newsletterV3=b1444918afee&newsletterV3Id=a5f0dee142a2&user=Andreas+Maier&userId=b1444918afee&source=-----65593eb14aed---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/codex/10-ideas-to-make-money-from-large-language-models-86f2cb31bb25?source=author_recirc-----65593eb14aed----0---------------------568d20c8_3560_43fa_823d_c6aa0d07378a-------", "anchor_text": ""}, {"url": "https://akmaier.medium.com/?source=author_recirc-----65593eb14aed----0---------------------568d20c8_3560_43fa_823d_c6aa0d07378a-------", "anchor_text": ""}, {"url": "https://akmaier.medium.com/?source=author_recirc-----65593eb14aed----0---------------------568d20c8_3560_43fa_823d_c6aa0d07378a-------", "anchor_text": "Andreas Maier"}, {"url": "https://medium.com/codex?source=author_recirc-----65593eb14aed----0---------------------568d20c8_3560_43fa_823d_c6aa0d07378a-------", "anchor_text": "CodeX"}, {"url": "https://medium.com/codex/10-ideas-to-make-money-from-large-language-models-86f2cb31bb25?source=author_recirc-----65593eb14aed----0---------------------568d20c8_3560_43fa_823d_c6aa0d07378a-------", "anchor_text": "10 Ideas to Make Money from Large Language ModelsLarge Language Models work, but what can we do with them?"}, {"url": "https://medium.com/codex/10-ideas-to-make-money-from-large-language-models-86f2cb31bb25?source=author_recirc-----65593eb14aed----0---------------------568d20c8_3560_43fa_823d_c6aa0d07378a-------", "anchor_text": "\u00b73 min read\u00b7Dec 11, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fcodex%2F86f2cb31bb25&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fcodex%2F10-ideas-to-make-money-from-large-language-models-86f2cb31bb25&user=Andreas+Maier&userId=b1444918afee&source=-----86f2cb31bb25----0-----------------clap_footer----568d20c8_3560_43fa_823d_c6aa0d07378a-------", "anchor_text": ""}, {"url": "https://medium.com/codex/10-ideas-to-make-money-from-large-language-models-86f2cb31bb25?source=author_recirc-----65593eb14aed----0---------------------568d20c8_3560_43fa_823d_c6aa0d07378a-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F86f2cb31bb25&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fcodex%2F10-ideas-to-make-money-from-large-language-models-86f2cb31bb25&source=-----65593eb14aed----0-----------------bookmark_preview----568d20c8_3560_43fa_823d_c6aa0d07378a-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----65593eb14aed----1---------------------568d20c8_3560_43fa_823d_c6aa0d07378a-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----65593eb14aed----1---------------------568d20c8_3560_43fa_823d_c6aa0d07378a-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----65593eb14aed----1---------------------568d20c8_3560_43fa_823d_c6aa0d07378a-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----65593eb14aed----1---------------------568d20c8_3560_43fa_823d_c6aa0d07378a-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----65593eb14aed----1---------------------568d20c8_3560_43fa_823d_c6aa0d07378a-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----65593eb14aed----1---------------------568d20c8_3560_43fa_823d_c6aa0d07378a-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----568d20c8_3560_43fa_823d_c6aa0d07378a-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----65593eb14aed----1---------------------568d20c8_3560_43fa_823d_c6aa0d07378a-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----65593eb14aed----1-----------------bookmark_preview----568d20c8_3560_43fa_823d_c6aa0d07378a-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----65593eb14aed----2---------------------568d20c8_3560_43fa_823d_c6aa0d07378a-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----65593eb14aed----2---------------------568d20c8_3560_43fa_823d_c6aa0d07378a-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----65593eb14aed----2---------------------568d20c8_3560_43fa_823d_c6aa0d07378a-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----65593eb14aed----2---------------------568d20c8_3560_43fa_823d_c6aa0d07378a-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----65593eb14aed----2---------------------568d20c8_3560_43fa_823d_c6aa0d07378a-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----65593eb14aed----2---------------------568d20c8_3560_43fa_823d_c6aa0d07378a-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----2-----------------clap_footer----568d20c8_3560_43fa_823d_c6aa0d07378a-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----65593eb14aed----2---------------------568d20c8_3560_43fa_823d_c6aa0d07378a-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----65593eb14aed----2-----------------bookmark_preview----568d20c8_3560_43fa_823d_c6aa0d07378a-------", "anchor_text": ""}, {"url": "https://medium.com/codex/gradient-descent-and-back-tracking-line-search-d8bd120bd625?source=author_recirc-----65593eb14aed----3---------------------568d20c8_3560_43fa_823d_c6aa0d07378a-------", "anchor_text": ""}, {"url": "https://akmaier.medium.com/?source=author_recirc-----65593eb14aed----3---------------------568d20c8_3560_43fa_823d_c6aa0d07378a-------", "anchor_text": ""}, {"url": "https://akmaier.medium.com/?source=author_recirc-----65593eb14aed----3---------------------568d20c8_3560_43fa_823d_c6aa0d07378a-------", "anchor_text": "Andreas Maier"}, {"url": "https://medium.com/codex?source=author_recirc-----65593eb14aed----3---------------------568d20c8_3560_43fa_823d_c6aa0d07378a-------", "anchor_text": "CodeX"}, {"url": "https://medium.com/codex/gradient-descent-and-back-tracking-line-search-d8bd120bd625?source=author_recirc-----65593eb14aed----3---------------------568d20c8_3560_43fa_823d_c6aa0d07378a-------", "anchor_text": "Gradient Descent and Back-tracking Line SearchAn Introduction to Optimization using Gradient Descent"}, {"url": "https://medium.com/codex/gradient-descent-and-back-tracking-line-search-d8bd120bd625?source=author_recirc-----65593eb14aed----3---------------------568d20c8_3560_43fa_823d_c6aa0d07378a-------", "anchor_text": "\u00b713 min read\u00b7Apr 10, 2021"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fcodex%2Fd8bd120bd625&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fcodex%2Fgradient-descent-and-back-tracking-line-search-d8bd120bd625&user=Andreas+Maier&userId=b1444918afee&source=-----d8bd120bd625----3-----------------clap_footer----568d20c8_3560_43fa_823d_c6aa0d07378a-------", "anchor_text": ""}, {"url": "https://medium.com/codex/gradient-descent-and-back-tracking-line-search-d8bd120bd625?source=author_recirc-----65593eb14aed----3---------------------568d20c8_3560_43fa_823d_c6aa0d07378a-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd8bd120bd625&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fcodex%2Fgradient-descent-and-back-tracking-line-search-d8bd120bd625&source=-----65593eb14aed----3-----------------bookmark_preview----568d20c8_3560_43fa_823d_c6aa0d07378a-------", "anchor_text": ""}, {"url": "https://akmaier.medium.com/?source=post_page-----65593eb14aed--------------------------------", "anchor_text": "See all from Andreas Maier"}, {"url": "https://towardsdatascience.com/?source=post_page-----65593eb14aed--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----65593eb14aed----0---------------------d1d543ac_0e2e_4233_b920_000fb46c480f-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----65593eb14aed----0---------------------d1d543ac_0e2e_4233_b920_000fb46c480f-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----65593eb14aed----0---------------------d1d543ac_0e2e_4233_b920_000fb46c480f-------", "anchor_text": "The PyCoach"}, {"url": "https://artificialcorner.com/?source=read_next_recirc-----65593eb14aed----0---------------------d1d543ac_0e2e_4233_b920_000fb46c480f-------", "anchor_text": "Artificial Corner"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----65593eb14aed----0---------------------d1d543ac_0e2e_4233_b920_000fb46c480f-------", "anchor_text": "You\u2019re Using ChatGPT Wrong! Here\u2019s How to Be Ahead of 99% of ChatGPT UsersMaster ChatGPT by learning prompt engineering."}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----65593eb14aed----0---------------------d1d543ac_0e2e_4233_b920_000fb46c480f-------", "anchor_text": "\u00b77 min read\u00b7Mar 17"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fartificial-corner%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&user=The+PyCoach&userId=fb44e21903f3&source=-----886a50dabc54----0-----------------clap_footer----d1d543ac_0e2e_4233_b920_000fb46c480f-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----65593eb14aed----0---------------------d1d543ac_0e2e_4233_b920_000fb46c480f-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "276"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&source=-----65593eb14aed----0-----------------bookmark_preview----d1d543ac_0e2e_4233_b920_000fb46c480f-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/using-transformers-for-computer-vision-6f764c5a078b?source=read_next_recirc-----65593eb14aed----1---------------------d1d543ac_0e2e_4233_b920_000fb46c480f-------", "anchor_text": ""}, {"url": "https://wolfecameron.medium.com/?source=read_next_recirc-----65593eb14aed----1---------------------d1d543ac_0e2e_4233_b920_000fb46c480f-------", "anchor_text": ""}, {"url": "https://wolfecameron.medium.com/?source=read_next_recirc-----65593eb14aed----1---------------------d1d543ac_0e2e_4233_b920_000fb46c480f-------", "anchor_text": "Cameron R. Wolfe"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----65593eb14aed----1---------------------d1d543ac_0e2e_4233_b920_000fb46c480f-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/using-transformers-for-computer-vision-6f764c5a078b?source=read_next_recirc-----65593eb14aed----1---------------------d1d543ac_0e2e_4233_b920_000fb46c480f-------", "anchor_text": "Using Transformers for Computer VisionAre Vision Transformers actually useful?"}, {"url": "https://towardsdatascience.com/using-transformers-for-computer-vision-6f764c5a078b?source=read_next_recirc-----65593eb14aed----1---------------------d1d543ac_0e2e_4233_b920_000fb46c480f-------", "anchor_text": "\u00b713 min read\u00b7Oct 5, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6f764c5a078b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-transformers-for-computer-vision-6f764c5a078b&user=Cameron+R.+Wolfe&userId=28aa6026c553&source=-----6f764c5a078b----1-----------------clap_footer----d1d543ac_0e2e_4233_b920_000fb46c480f-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/using-transformers-for-computer-vision-6f764c5a078b?source=read_next_recirc-----65593eb14aed----1---------------------d1d543ac_0e2e_4233_b920_000fb46c480f-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "4"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6f764c5a078b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-transformers-for-computer-vision-6f764c5a078b&source=-----65593eb14aed----1-----------------bookmark_preview----d1d543ac_0e2e_4233_b920_000fb46c480f-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----65593eb14aed----0---------------------d1d543ac_0e2e_4233_b920_000fb46c480f-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----65593eb14aed----0---------------------d1d543ac_0e2e_4233_b920_000fb46c480f-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----65593eb14aed----0---------------------d1d543ac_0e2e_4233_b920_000fb46c480f-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----65593eb14aed----0---------------------d1d543ac_0e2e_4233_b920_000fb46c480f-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----65593eb14aed----0---------------------d1d543ac_0e2e_4233_b920_000fb46c480f-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----65593eb14aed----0---------------------d1d543ac_0e2e_4233_b920_000fb46c480f-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----0-----------------clap_footer----d1d543ac_0e2e_4233_b920_000fb46c480f-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----65593eb14aed----0---------------------d1d543ac_0e2e_4233_b920_000fb46c480f-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----65593eb14aed----0-----------------bookmark_preview----d1d543ac_0e2e_4233_b920_000fb46c480f-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----65593eb14aed----1---------------------d1d543ac_0e2e_4233_b920_000fb46c480f-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----65593eb14aed----1---------------------d1d543ac_0e2e_4233_b920_000fb46c480f-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----65593eb14aed----1---------------------d1d543ac_0e2e_4233_b920_000fb46c480f-------", "anchor_text": "Alexander Nguyen"}, {"url": "https://levelup.gitconnected.com/?source=read_next_recirc-----65593eb14aed----1---------------------d1d543ac_0e2e_4233_b920_000fb46c480f-------", "anchor_text": "Level Up Coding"}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----65593eb14aed----1---------------------d1d543ac_0e2e_4233_b920_000fb46c480f-------", "anchor_text": "Why I Keep Failing Candidates During Google Interviews\u2026They don\u2019t meet the bar."}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----65593eb14aed----1---------------------d1d543ac_0e2e_4233_b920_000fb46c480f-------", "anchor_text": "\u00b74 min read\u00b7Apr 13"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fgitconnected%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&user=Alexander+Nguyen&userId=a148fd75c2e9&source=-----dc8f865b2c19----1-----------------clap_footer----d1d543ac_0e2e_4233_b920_000fb46c480f-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----65593eb14aed----1---------------------d1d543ac_0e2e_4233_b920_000fb46c480f-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "91"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&source=-----65593eb14aed----1-----------------bookmark_preview----d1d543ac_0e2e_4233_b920_000fb46c480f-------", "anchor_text": ""}, {"url": "https://jehillparikh.medium.com/u-nets-with-attention-c8d7e9bf2416?source=read_next_recirc-----65593eb14aed----2---------------------d1d543ac_0e2e_4233_b920_000fb46c480f-------", "anchor_text": ""}, {"url": "https://jehillparikh.medium.com/?source=read_next_recirc-----65593eb14aed----2---------------------d1d543ac_0e2e_4233_b920_000fb46c480f-------", "anchor_text": ""}, {"url": "https://jehillparikh.medium.com/?source=read_next_recirc-----65593eb14aed----2---------------------d1d543ac_0e2e_4233_b920_000fb46c480f-------", "anchor_text": "Jehill Parikh"}, {"url": "https://jehillparikh.medium.com/u-nets-with-attention-c8d7e9bf2416?source=read_next_recirc-----65593eb14aed----2---------------------d1d543ac_0e2e_4233_b920_000fb46c480f-------", "anchor_text": "U-Nets with attentionU-Net are popular NN architecture which are employed for many applications and were initially developed for medical image segmentation."}, {"url": "https://jehillparikh.medium.com/u-nets-with-attention-c8d7e9bf2416?source=read_next_recirc-----65593eb14aed----2---------------------d1d543ac_0e2e_4233_b920_000fb46c480f-------", "anchor_text": "\u00b72 min read\u00b7Nov 15, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2Fc8d7e9bf2416&operation=register&redirect=https%3A%2F%2Fjehillparikh.medium.com%2Fu-nets-with-attention-c8d7e9bf2416&user=Jehill+Parikh&userId=c972081b627e&source=-----c8d7e9bf2416----2-----------------clap_footer----d1d543ac_0e2e_4233_b920_000fb46c480f-------", "anchor_text": ""}, {"url": "https://jehillparikh.medium.com/u-nets-with-attention-c8d7e9bf2416?source=read_next_recirc-----65593eb14aed----2---------------------d1d543ac_0e2e_4233_b920_000fb46c480f-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc8d7e9bf2416&operation=register&redirect=https%3A%2F%2Fjehillparikh.medium.com%2Fu-nets-with-attention-c8d7e9bf2416&source=-----65593eb14aed----2-----------------bookmark_preview----d1d543ac_0e2e_4233_b920_000fb46c480f-------", "anchor_text": ""}, {"url": "https://medium.com/mlearning-ai/understanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f?source=read_next_recirc-----65593eb14aed----3---------------------d1d543ac_0e2e_4233_b920_000fb46c480f-------", "anchor_text": ""}, {"url": "https://medium.com/@martin-thissen?source=read_next_recirc-----65593eb14aed----3---------------------d1d543ac_0e2e_4233_b920_000fb46c480f-------", "anchor_text": ""}, {"url": "https://medium.com/@martin-thissen?source=read_next_recirc-----65593eb14aed----3---------------------d1d543ac_0e2e_4233_b920_000fb46c480f-------", "anchor_text": "Martin Thissen"}, {"url": "https://medium.com/mlearning-ai?source=read_next_recirc-----65593eb14aed----3---------------------d1d543ac_0e2e_4233_b920_000fb46c480f-------", "anchor_text": "MLearning.ai"}, {"url": "https://medium.com/mlearning-ai/understanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f?source=read_next_recirc-----65593eb14aed----3---------------------d1d543ac_0e2e_4233_b920_000fb46c480f-------", "anchor_text": "Understanding and Coding the Attention Mechanism \u2014 The Magic Behind TransformersIn this article, I\u2019ll give you an introduction to the attention mechanism and show you how to code the attention mechanism yourself."}, {"url": "https://medium.com/mlearning-ai/understanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f?source=read_next_recirc-----65593eb14aed----3---------------------d1d543ac_0e2e_4233_b920_000fb46c480f-------", "anchor_text": "\u00b712 min read\u00b7Dec 6, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fmlearning-ai%2Ffe707a85cc3f&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fmlearning-ai%2Funderstanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f&user=Martin+Thissen&userId=f99c73950195&source=-----fe707a85cc3f----3-----------------clap_footer----d1d543ac_0e2e_4233_b920_000fb46c480f-------", "anchor_text": ""}, {"url": "https://medium.com/mlearning-ai/understanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f?source=read_next_recirc-----65593eb14aed----3---------------------d1d543ac_0e2e_4233_b920_000fb46c480f-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffe707a85cc3f&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fmlearning-ai%2Funderstanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f&source=-----65593eb14aed----3-----------------bookmark_preview----d1d543ac_0e2e_4233_b920_000fb46c480f-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----65593eb14aed--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----65593eb14aed--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----65593eb14aed--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----65593eb14aed--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----65593eb14aed--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----65593eb14aed--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----65593eb14aed--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----65593eb14aed--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----65593eb14aed--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----65593eb14aed--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}