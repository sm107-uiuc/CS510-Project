{"url": "https://towardsdatascience.com/text-classification-on-disaster-tweets-with-lstm-and-word-embedding-df35f039c1db", "time": 1683013281.283069, "path": "towardsdatascience.com/text-classification-on-disaster-tweets-with-lstm-and-word-embedding-df35f039c1db/", "webpage": {"metadata": {"title": "Text Classification on Disaster Tweets with LSTM and Word Embedding | by Emmanuella Anggi | Towards Data Science", "h1": "Text Classification on Disaster Tweets with LSTM and Word Embedding", "description": "In this post, I will elaborate on how to use fastText and GloVe as word embedding on LSTM model for text classification. I got interested in Word Embedding while doing my paper on Natural Language\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/@jonathan_hui/nlp-bert-transformer-7f0ac397f524", "anchor_text": "further reading on BERT", "paragraph_index": 1}, {"url": "https://www.kaggle.com/c/nlp-getting-started", "anchor_text": "here", "paragraph_index": 3}, {"url": "https://fasttext.cc/docs/en/english-vectors.html", "anchor_text": "fastText", "paragraph_index": 9}, {"url": "https://nlp.stanford.edu/projects/glove/", "anchor_text": "Glove", "paragraph_index": 9}, {"url": "https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/)", "anchor_text": "more about embedding matrix", "paragraph_index": 13}, {"url": "https://github.com/emmanuellaanggi", "anchor_text": "GitHub", "paragraph_index": 18}], "all_paragraphs": ["This was my first Kaggle notebook and I thought why not write it on Medium too?", "In this post, I will elaborate on how to use fastText and GloVe as word embedding on LSTM model for text classification. I got interested in Word Embedding while doing my paper on Natural Language Generation. It showed that embedding matrix for the weight on embedding layer improved the performance of the model. But since it was NLG, the measurement was subjective. And I only used fastText too. So in this article, I want to see how each method (with fastText and GloVe and without) affects to the prediction. On my Github code, I also compare the result with CNN. The dataset that i use here is from one of competition on Kaggle, consisted of tweets and labelled with whether the tweet is using disastrous words to inform a real disaster or merely just used it metaphorically. Honestly, on first seeing this dataset, I immediately thought about BERT and its ability to understand way better than what I proposed on this article (further reading on BERT).", "But anyway, in this article I will focus on fastText and GloVe.", "The data consisted of 7613 tweets (columns Text) with label (column Target) whether they were talking about a real disaster or not. With 3271 rows informing real disaster and 4342 rows informing not real disaster. The data shared on kaggle competition, and if you want to learn more about the data you can read it here.", "Example of real disaster word in a text :", "\u201c Forest fire near La Ronge Sask. Canada \u201c", "Example of the use of disaster word but not about disaster:", "\u201cThese boxes are ready to explode! Exploding Kittens finally arrived! gameofkittens #explodingkittens\u201d", "The data will be divided for training (6090 rows) and testing (1523 rows) then proceed to pre-processing. We will only be using the text and target columns.", "The first step on working both with fastText and Glove is downloading each of pre-trained model. I used Google Colab to prevent the use of big memory on my laptop, so I downloaded it with request library and unzip it directly on the notebook.", "I used the biggest pre-trained model from both word embedding. fastText model gave 2 million word vectors (600B tokens) and GloVe gave 2.2 million word vectors (840B tokens), both trained on Common Crawl.", "Step 2. Load Pre-trained model to Word Vectors", "FastText gave the format to load the word vectors and so I used that to load both models.", "Embedding matrix will be used in embedding layer for the weight of each word in training data. It\u2019s made by enumerating each unique word in the training dataset that existed in tokenized word index and locate the embedding weight with the weight from fastText orGloVe (more about embedding matrix).", "But there is a possibility that there are words that aren\u2019t in the vectors such as typos or abbreviation or usernames. Those words will be stored in a list and we can compare the performance of handling words from fastText and GloVe", "Number of null word embeddings on fastText is 9175 and on GloVe is 9186. Can be assumed that fastText handle more words even when the pre-trained was trained on fewer words.", "You can do fine-tuning on hyper-parameters or architecture, but I\u2019m going to use the very simple one with Embedding Layer, LSTM layer, Dense layer, and Drop out Layer.", "fastText gave the best performance with accuracy for about 83% while GloVe gave 81% accuracy. The difference on the performance isn\u2019t so significant but to compare it with the performance of model without word embedding (68%), we can see the significant use of Word Embedding on embedding layer weight.", "For more about the training performance, detail code, and if you want to apply it on a different dataset, you can see the full code on my GitHub.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fdf35f039c1db&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-classification-on-disaster-tweets-with-lstm-and-word-embedding-df35f039c1db&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-classification-on-disaster-tweets-with-lstm-and-word-embedding-df35f039c1db&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-classification-on-disaster-tweets-with-lstm-and-word-embedding-df35f039c1db&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-classification-on-disaster-tweets-with-lstm-and-word-embedding-df35f039c1db&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----df35f039c1db--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----df35f039c1db--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://emmanuellaanggi.medium.com/?source=post_page-----df35f039c1db--------------------------------", "anchor_text": ""}, {"url": "https://emmanuellaanggi.medium.com/?source=post_page-----df35f039c1db--------------------------------", "anchor_text": "Emmanuella Anggi"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2b61d2c09f4d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-classification-on-disaster-tweets-with-lstm-and-word-embedding-df35f039c1db&user=Emmanuella+Anggi&userId=2b61d2c09f4d&source=post_page-2b61d2c09f4d----df35f039c1db---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdf35f039c1db&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-classification-on-disaster-tweets-with-lstm-and-word-embedding-df35f039c1db&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdf35f039c1db&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-classification-on-disaster-tweets-with-lstm-and-word-embedding-df35f039c1db&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.istockphoto.com/photo/disaster-area-with-tornado-gm172448852-23567577", "anchor_text": "https://www.istockphoto.com/photo/disaster-area-with-tornado-gm172448852-23567577"}, {"url": "https://github.com/emmanuellaanggi/disaster_tweet_sentiment", "anchor_text": "Github"}, {"url": "https://medium.com/@jonathan_hui/nlp-bert-transformer-7f0ac397f524", "anchor_text": "further reading on BERT"}, {"url": "https://www.kaggle.com/c/nlp-getting-started", "anchor_text": "here"}, {"url": "https://fasttext.cc/docs/en/english-vectors.html", "anchor_text": "fastText"}, {"url": "https://nlp.stanford.edu/projects/glove/", "anchor_text": "Glove"}, {"url": "https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/)", "anchor_text": "more about embedding matrix"}, {"url": "https://github.com/emmanuellaanggi", "anchor_text": "GitHub"}, {"url": "https://medium.com/tag/text-classification?source=post_page-----df35f039c1db---------------text_classification-----------------", "anchor_text": "Text Classification"}, {"url": "https://medium.com/tag/word-embeddings?source=post_page-----df35f039c1db---------------word_embeddings-----------------", "anchor_text": "Word Embeddings"}, {"url": "https://medium.com/tag/nlp?source=post_page-----df35f039c1db---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/data-science?source=post_page-----df35f039c1db---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/lstm?source=post_page-----df35f039c1db---------------lstm-----------------", "anchor_text": "Lstm"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fdf35f039c1db&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-classification-on-disaster-tweets-with-lstm-and-word-embedding-df35f039c1db&user=Emmanuella+Anggi&userId=2b61d2c09f4d&source=-----df35f039c1db---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fdf35f039c1db&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-classification-on-disaster-tweets-with-lstm-and-word-embedding-df35f039c1db&user=Emmanuella+Anggi&userId=2b61d2c09f4d&source=-----df35f039c1db---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdf35f039c1db&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-classification-on-disaster-tweets-with-lstm-and-word-embedding-df35f039c1db&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----df35f039c1db--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fdf35f039c1db&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-classification-on-disaster-tweets-with-lstm-and-word-embedding-df35f039c1db&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----df35f039c1db---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----df35f039c1db--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----df35f039c1db--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----df35f039c1db--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----df35f039c1db--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----df35f039c1db--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----df35f039c1db--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----df35f039c1db--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----df35f039c1db--------------------------------", "anchor_text": ""}, {"url": "https://emmanuellaanggi.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://emmanuellaanggi.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Emmanuella Anggi"}, {"url": "https://emmanuellaanggi.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "30 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2b61d2c09f4d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-classification-on-disaster-tweets-with-lstm-and-word-embedding-df35f039c1db&user=Emmanuella+Anggi&userId=2b61d2c09f4d&source=post_page-2b61d2c09f4d--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F683451178098&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-classification-on-disaster-tweets-with-lstm-and-word-embedding-df35f039c1db&newsletterV3=2b61d2c09f4d&newsletterV3Id=683451178098&user=Emmanuella+Anggi&userId=2b61d2c09f4d&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}