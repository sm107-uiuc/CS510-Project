{"url": "https://towardsdatascience.com/categorical-features-parameters-in-catboost-4ebd1326bee5", "time": 1683010649.495992, "path": "towardsdatascience.com/categorical-features-parameters-in-catboost-4ebd1326bee5/", "webpage": {"metadata": {"title": "Categorical features parameters in CatBoost | by Mariia Garkavenko | Towards Data Science", "h1": "Categorical features parameters in CatBoost", "description": "CatBoost is an open-sourced gradient boosting library. One of the differences between CatBoost and other gradient boosting libraries is its advanced processing of the categorical features (in fact\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.kaggle.com/lepchenkov/usedcarscatalog", "anchor_text": "https://www.kaggle.com/lepchenkov/usedcarscatalog", "paragraph_index": 10}, {"url": "https://catboost.ai/docs/concepts/cli-installation.html", "anchor_text": "here", "paragraph_index": 68}, {"url": "https://github.com/catboost/catboost/blob/master/catboost/tutorials/cmdline_tutorial/cmdline_tutorial.md", "anchor_text": "this tutorial", "paragraph_index": 68}], "all_paragraphs": ["CatBoost is an open-sourced gradient boosting library. One of the differences between CatBoost and other gradient boosting libraries is its advanced processing of the categorical features (in fact \u201cCat\u201d in the package name stands not for a \ud83d\udc31 but for \u201cCATegorical\u201d).", "CatBoost deals with the categorical data quite well out-of-the-box. However, it also has a huge number of training parameters, which provide fine control over the categorical features preprocessing. In this tutorial, we are going to learn how to use these parameters for the greater good. The tutorial is split into the following sections:", "\u0421ategorical feature is a feature that has a discrete set of values called categories that are not comparable by < or > to each other. In real-world datasets, we quite often deal with categorical data. The cardinality of a categorical feature, i.e. the number of different values that the feature can take varies drastically among features and datasets \u2014 from just a few to thousands and millions of distinct values. The values of a categorical feature can be distributed almost uniformly and there might be values with a frequency different by the orders of magnitude. To be used in gradient boosting categorical features need to be transformed into some form that can be handled by a decision tree, for example to numbers. In the next section, we are going to briefly go through the most popular in machine learning methods of transforming categorical features values into numbers. Standard approaches to categorical features preprocessing", "CatBoost supports some traditional methods of categorical data preprocessing, such as One-hot Encoding and Frequency Encoding. However one of the signatures of this package is its original solution for categorical features encoding.", "The core idea behind CatBoost categorical features preprocessing is Ordered Target Encoding: a random permutation of the dataset is performed and then target encoding of some type (for example just computing mean of the target for objects of this category) is performed on each example using only the objects that are placed before the current object.", "Generally transforming categorical features to numerical features in CatBoost includes the following steps:", "3. Encoding the categorical feature values.", "CatBoost creates four permutations of the training objects and for each permutation, a separate model is trained. Three models are used for the tree structure selection and the fourth is used to compute the leaves values of the final model that we save. At each iteration one of the three models is chosen randomly; this model is used to choose the new tree structure and to calculate the leaves values for all the four models.", "Using several models for tree structure selection enhances the robustness of the categorical features encoding. If in one permutation an object is close to the beginning of the dataset and the statistics for encoding are calculated on a small number of objects in the other two permutations it may be closer to the end of the dataset and many objects will be used to compute the statistics.", "Another important point is that CatBoost can create new categorical features combining the existing ones. And it will actually do so unless you explicitly tell it not to :) Treatment of the original features and the created features can be controlled separately by the settings simple_ctr and combinations_ctr respectively (we will talk about them in detail).", "For the experiments in this tutorial, we are going to use https://www.kaggle.com/lepchenkov/usedcarscatalog", "This dataset consists of the old cars\u2019 descriptions and their characteristics \u2014 both numerical, such as mileage, production year, etc and categorical, such as color, manufacturer name, model name, etc.", "Our goal is to solve the regression task, i.e. to predict the price of an old car.", "Let us see how many unique values each categorical variable has:", "Here is the target value distribution:", "First, we are going to roughly estimate the number of trees and the learning rate required that are sufficient for this task.", "Now we are going to write a simple function that tests CatBoost performance on 3-fold cross-validation given the parameters and returns the full list of parameters for the last model. Optionally this function compares the model\u2019s metrics with the results of the model trained with the default categorical features parameters.", "We will fix the number of estimators at 4500 and the learning rate at 0.1.", "The amount of parameters related to categorical features processing in CatBoost is overwhelming. Here is a hopefully the full list:", "The three parameters simple_ctr, combinations_ctr, and per_feature_ctr are complex parameters that control the second and the third steps of categorical features processing. We will talk about them more in the next sections.", "First, we test the out-of-the-box CatBoost categorical features processing.", "We will save the metrics of the model with the default categorical features parameters for further comparison.", "The first thing we try is to make CatBoost use one-hot encoding for all our categorical features (the max categorical feature cardinality in our dataset is 1118 < 2000). The documentation says, that for the features for which one-hot encoding is used no other encodings are computed.", "This parameter influences the model size if training data has categorical features.", "The information regarding categorical features makes a great contribution to the final size of the model. The mapping from the categorical feature value hash to some statistic values is stored for each categorical feature that is used in the model. The size of this mapping for a particular feature depends on the number of unique values that this feature takes.", "Therefore, the potential weight of a categorical feature can be taken into account in the final model when choosing a split in a tree to reduce the final size of the model. When choosing the best split, all split scores are calculated and then the split with the best score is chosen. But before choosing the split with the best score, all scores change according to the following formula:", "s_new is the new score for the split by some categorical feature or combination feature, s_old is the old score for the split by the feature, u is the number of unique values of the feature, U is the maximum of all values among all features and M is the value of the model_size_reg parameter.", "This regularization works slightly differently on GPU: feature combinations are regularized more aggressively than on CPU. For CPU cost of a combination is equal to the number of different feature values in these combinations that are present in the training dataset. On GPU cost of a combination is equal to the number of all possible different values of this combination. For example, if the combination contains two categorical features c1 and c2, then the cost will be #categories in c1 * #categories in c2, even though many of the values from this combination might not be present in the dataset.", "Let us try to set model size regularization coefficient to 0 \u2014 thus we allow our model to use as many categorical features and its combinations as it wants.", "To check how the size of the model is affected by this setting we will write a function that given parameters dictionary will train a model, save it in a file and return the model\u2019s weight:", "As we can see the model with the strong regularization is almost 13 times smaller than the model without regularization.", "Feature combinations: Note that any combination of several categorical features could be considered as a new one. For example, assume that the task is music recommendation and we have two categorical features: user ID and musical genre. Some user prefers, say, rock music. When we convert user ID and musical genre to numerical features we lose this information. A combination of two features solves this problem and gives a new powerful feature. However, the number of combinations grows exponentially with the number of categorical features in the dataset and it is not possible to consider all of them in the algorithm. When constructing a new split for the current tree, CatBoost considers combinations in a greedy way. No combinations are considered for the first split in the tree. For the next splits, CatBoost combines all combinations and categorical features present in the current tree with all categorical features in the dataset. Combination values are converted to numbers on the fly. CatBoost also generates combinations of numerical and categorical features in the following way: all the splits selected in the tree are considered as categorical with two values and used in combinations in the same way as categorical ones.", "The maximum number of features that can be combined. Each resulting combination consists of one or more categorical features and can optionally contain binary features in the following form: \u201cnumeric feature > value\u201d. For the regression task on CPU, the default value is 4.", "Although it is not mentioned in the documentation, this parameter value has to be less or equal to 15. (Because this parameter should be less than the max gradient boosting tree depth).", "As we can see on our dataset the difference in the model\u2019s accuracy is not significant. To check how the size of the model is affected we will use our function that weights a model.", "As can be seen, the model that can combine up to 6 features weights 6 times more than the model that does not combine features at all.", "With this setting on we do not perform random permutations during the Transforming categorical features to numerical. This might be useful when the objects of our dataset are already ordered by time. If a Timestamp type column is present in the input data it is used to determine the order of objects.", "Both simple_ctr and combinations_ctr are complex parameters that provide regulation of the categorical features encodings types. While simple_ctr is responsible for processing the categorical features initially present in the dataset, combinations_ctr affects the encoding of the new features, that CatBoost creates by combining the existing features. The available methods of encodings and possible values of simple_ctr and combinations_ctr are the same, so we are not going to look at them separately. But of course, you can always tune them separately on your task!", "Target quantization is transforming float target values to int target values using some borders. We will first consider the target encoding methods that do not require such a transformation.", "The first option FloatTargetMeanValue is the most straightforward approach. Each value of the categorical variable is replaced with the mean of the target over the objects of the same category that are placed before the current object.", "The second option is FeatureFreq. The categorical feature values are replaced with the frequencies of the category in the dataset. Again only the objects placed before the current objects are used.", "We have already discussed Counter method in section \u201cDefault parameters\u201d because by default this method is used to create feature encodings. It is worth noticing, that if we directly pass Counter into simple_ctr and/or combinations_ctr CatBoost will use only Counter features encodings.", "Let us say we have calculated encodings for our categorical variable. These encodings are floats and they are comparable: in case of Counter the larger encoding value corresponds to the more frequent category. However, if we have a large number of categories the difference between close categories encodings may be caused by noise and we do not want our model to differentiate between close categories. For this reason, we transform our float encoding into int encoding \ud835\udc56\u2208[0,\ud835\udc59]i\u2208[0,l]. By default CtrBorderCount=15 setting means that \ud835\udc59=14(15\u22121)l=14(15\u22121). We can try to use a bigger value:", "The second method BinarizedTargetMeanValue is very similar to target encoding, except that instead of the sum over the exact target values we use the sum of the values of the beans. Which corresponds to the following formula:", "While using the BinarizedTargetMeanValue method we can also finetune Prior and CtrBorderCount(the number of borders for quantization the category feature encoding). By default CtrBorderCount=15 and 0, 0.5 and 1 Prior values are used to build three different encodings.", "Now we proceed to the settings of the encodings methods that require target quantization. The first choice is Borders vs. Buckets. The difference between the two is pretty simple. Both are described by the following formula:", "for i in [0, k-1] in case of Borders and for i in [0, k] in case of Buckets:", "where k is the number of borders regulated by parameter TargetBorderCount,", "totalCount is the number of objects of the same category. prior is defined by the parameter prior. The only difference is that for Borders countInClass is the number of the objects of the category with the discretized target value greater than i while for Buckets countInClass is the number of the objects of the category with the discretized target value equal to i.", "Let us see a small example: we have objects of two categories shown as suns and moons. We will compute the categorical feature encodings in case of borders and buckets.", "Borders: We have two borders(which corresponds to TargetBorderCount=2), so we need to calculate 2 encodings. Let us say our Prior is 0.5", "Buckets: i in [0, k] creates k+1 buckets. So the same value of parameter TargetBorderCount=2 creates more features from each categorical feature if we choose Buckets.", "Important note! This example just serves to illustrate the difference between Borders and Buckets and the whole dataset is used to compute countInClass and totalCount. In reality, CatBoost uses only the objects placed before the current object are used.", "Let us see if it makes any difference in practice:", "An attentive reader may remember that by default CatBoost creates some features using Borders splits and also some features using Counter method. When we explicitly pass the Borders option, Counter method is not used.", "Generally, it is recommended to use Borders for the regression task and Buckets for the multiclassification task.", "The number of borders or buckets is can be controlled with the TargetBorderCount parameter. By default we have only one border, let us see if having more borders helps:", "By default, CatBoost uses several encoding techniques to encode each categorical feature.", "We can always check the parameters used by our model with get_all_params() method.", "The next thing I would like to talk about in this tutorial is using different encoding methods for different features with the parameter per_feature_ctr. It might be useful in cases when you know that one of your features is more important than the others. We can, for example, increase the number of target borders for model_name feature:", "The parameter determines whether to use validation dataset(provided through parameter eval_set of fit method) to estimate categories frequencies with Counter. By default, it is Full and the objects from validation dataset are used; Pass SkipTest value to ignore the objects from the validation set In our score_catboost_model function we don't give to CatBoost the validation dataset at all during training so to check this method effect we will use train/test split.", "The maximum number of borders to use in target quantization for categorical features that need it. The default for the regression task is 1.", "Let us try a rather big number of borders:", "This parameter regulates the number of the most common categorical feature values that are used by the model. If we have n unique categories and ctr_leaf_count_limit=m we preserve the categorical feature value only for objects from most frequent categories. For the objects from the remaining categories, we replace categorical feature value with None.", "The default value of this parameter is None -- all the categorical features values are preserved.", "Oops! On our dataset, it ruins the model performance.", "With this setting on the previous parameter ctr_leaf_count_limit affects only the categorical features, that CatBoost creates by combining the initial features, and the initial categorical features present in the dataset are not affected. When parameter ctr_leaf_count_limit is None parameter store_all_simple_ctr has no effect.", "It is quite common to use several encodings for a categorical feature. For instance, CatBoost creates 4 different encodings for each categorical feature by default (see \u201cDefault value of simple_ctr and combinations_ctr\u201d section). When we call get_feature_importances method we get aggregated across all the encodings importance for the categorical feature. That is because in practice we usually just want to compare the overall usefulness of the different features present in our dataset.", "However, what if we want to know which encodings worked best for us? For that, we would need to get Internal Feature Importance. Currently, it is available only in the command-line version of CatBoost library. You can find details about the installation here and an example of how to train a model with the command-line version in this tutorial.", "To train a model with the command-line version we first need to create a column description file:", "And then create an Internal Feature Importance file: catboost fstr -m model.bin --cd train.cd --fstr-type InternalFeatureImportance -o feature_strength.tsv", "The contents of this file in our case are the following:", "An interesting observation is that for some features like model_name the most useful are the encodings of Border type, while for other features e.g. manufacturer_name the most useful encoding is obtained with Counter method.", "Another way of getting some insight into how your model works is training with logging_level=Info parameter. This setting allows us to see the feature splits chosen for each tree:", "For numeric features the format is the following:", "feature name, index of the chosen split, split score", "feature name, prior, target border, encoding type, categorical feature border, split score", "For convenience, categorical features names are written in brackets {}", "In our tutorial, we were working on the regression task, so I would like to make several notes on categorical parameter tuning on binary classification and multiclassification tasks.", "Congratulations to everyone who finished reading this tutorial :) As we saw the number of tunable parameters related to categorical features processing in CatBoost package is quite impressive. We learned how to control all of them, and I very much hope that this knowledge will help you to achieve the best results on your tasks involving categorical data!", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F4ebd1326bee5&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcategorical-features-parameters-in-catboost-4ebd1326bee5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcategorical-features-parameters-in-catboost-4ebd1326bee5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcategorical-features-parameters-in-catboost-4ebd1326bee5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcategorical-features-parameters-in-catboost-4ebd1326bee5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----4ebd1326bee5--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4ebd1326bee5--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@mariia.garkavenko?source=post_page-----4ebd1326bee5--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@mariia.garkavenko?source=post_page-----4ebd1326bee5--------------------------------", "anchor_text": "Mariia Garkavenko"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fddf32aedf465&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcategorical-features-parameters-in-catboost-4ebd1326bee5&user=Mariia+Garkavenko&userId=ddf32aedf465&source=post_page-ddf32aedf465----4ebd1326bee5---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4ebd1326bee5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcategorical-features-parameters-in-catboost-4ebd1326bee5&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4ebd1326bee5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcategorical-features-parameters-in-catboost-4ebd1326bee5&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@chuttersnap?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "chuttersnap"}, {"url": "https://unsplash.com/s/photos/cars?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://www.kaggle.com/lepchenkov/usedcarscatalog", "anchor_text": "https://www.kaggle.com/lepchenkov/usedcarscatalog"}, {"url": "https://catboost.ai/docs/concepts/cli-installation.html", "anchor_text": "here"}, {"url": "https://github.com/catboost/catboost/blob/master/catboost/tutorials/cmdline_tutorial/cmdline_tutorial.md", "anchor_text": "this tutorial"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----4ebd1326bee5---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/gradient-boosting?source=post_page-----4ebd1326bee5---------------gradient_boosting-----------------", "anchor_text": "Gradient Boosting"}, {"url": "https://medium.com/tag/catboost?source=post_page-----4ebd1326bee5---------------catboost-----------------", "anchor_text": "Catboost"}, {"url": "https://medium.com/tag/categorical-data?source=post_page-----4ebd1326bee5---------------categorical_data-----------------", "anchor_text": "Categorical Data"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4ebd1326bee5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcategorical-features-parameters-in-catboost-4ebd1326bee5&user=Mariia+Garkavenko&userId=ddf32aedf465&source=-----4ebd1326bee5---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4ebd1326bee5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcategorical-features-parameters-in-catboost-4ebd1326bee5&user=Mariia+Garkavenko&userId=ddf32aedf465&source=-----4ebd1326bee5---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4ebd1326bee5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcategorical-features-parameters-in-catboost-4ebd1326bee5&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4ebd1326bee5--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F4ebd1326bee5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcategorical-features-parameters-in-catboost-4ebd1326bee5&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----4ebd1326bee5---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----4ebd1326bee5--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----4ebd1326bee5--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----4ebd1326bee5--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----4ebd1326bee5--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----4ebd1326bee5--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----4ebd1326bee5--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----4ebd1326bee5--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----4ebd1326bee5--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@mariia.garkavenko?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@mariia.garkavenko?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Mariia Garkavenko"}, {"url": "https://medium.com/@mariia.garkavenko/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "50 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fddf32aedf465&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcategorical-features-parameters-in-catboost-4ebd1326bee5&user=Mariia+Garkavenko&userId=ddf32aedf465&source=post_page-ddf32aedf465--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F2ad498baf418&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcategorical-features-parameters-in-catboost-4ebd1326bee5&newsletterV3=ddf32aedf465&newsletterV3Id=2ad498baf418&user=Mariia+Garkavenko&userId=ddf32aedf465&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}