{"url": "https://towardsdatascience.com/how-to-implement-prioritized-experience-replay-for-a-deep-q-network-a710beecd77b", "time": 1683010304.620325, "path": "towardsdatascience.com/how-to-implement-prioritized-experience-replay-for-a-deep-q-network-a710beecd77b/", "webpage": {"metadata": {"title": "How to implement Prioritized Experience Replay for a Deep Q-Network | by Guillaume Crab\u00e9 | Towards Data Science", "h1": "How to implement Prioritized Experience Replay for a Deep Q-Network", "description": "In this article, we will use the OpenAI environment called Lunar Lander to train an agent to play as well as a human! To do that, we will implement a version of the Deep Q-Network algorithm called\u2026"}, "outgoing_paragraph_urls": [{"url": "https://github.com/Guillaume-Cr", "anchor_text": "https://github.com/Guillaume-Cr", "paragraph_index": 34}], "all_paragraphs": ["In this article, we will use the OpenAI environment called Lunar Lander to train an agent to play as well as a human! To do that, we will implement a version of the Deep Q-Network algorithm called Prioritized Experience Replay.", "To have a sense of what we want to accomplish, let\u2019s watch an untrained agent play the game. While the goal is landing between the two yellow flags, we can see that the agent still has a lot to learn!", "To begin with, let\u2019s refresh a bit our memory and place things into context. What is Deep Q-Network (DQN) and why do we use it? Globally, what kind of problems do we want to solve?", "The deep Q-network belongs to the family of the reinforcement learning algorithms, which means we place ourselves in the case where an environment is able to interact with an agent. The agent is able to take an action which will bring it from one state into another one. The environment will then provide a reward for attaining this new state, which can be positive or negative (penalty). The problem that we want to solve is being able to choose the best action for every state so that we maximize our total cumulative reward.", "For Reinforcement Learning algorithms to work, given a state, the action that will provide the best cumulative reward should not depend on the pasts visited states. This framework is called a Markov Decision Process. By visiting states multiple times and by updating our expected cumulative reward with the one we actually obtain, we are able to find out the best action to take for every state of the environment. This is the basis of the Q-Network algorithm.", "Now, this is very fine when we have a finite number of states, for example when an agent moves through a grid where the state is defined by the case its located at. The problem that we wish to solve now is the case of non-finite state variables (or actions). For example, a robot arm for which the environment state is a list of joint positions and velocities. The states being non-finite, it is very unlikely that we are going to visit a state multiple times, thus making it impossible to update the estimation of the best action to take. We need something that can, given two known states close enough to our current state, predict what would be the best action to take in our current state. You guessed it, the solution is some form of interpolation. When linear interpolation simply consists in \u201cdrawing a line between two states\u201d, we need to be able to predict with a higher degree of complexity. That\u2019s where neural network comes onto the stage. Neural networks give us the possibility to predict the best action to take given known states (and their optimal actions) with a non-linear model. And here it is, the Deep Q-Network. DQN posed several implementation problems, related to the training part of the neural network. The \u201ctrick\u201d is called experience replay, which basically means that we episodically stop visiting the environment to first collect some data about the past visited states, and then train our neural network on the collected experiences. In other words, it\u2019s alternating between phases of exploration and phases of training, decoupling the two allowing the neural network the converge towards an optimal solution.", "Now that we have a good understanding of what brought us to a Q-Network, let the fun begins. In this article, we want to implement a variant of the DQN named Prioritized Experience Replay (see publication link). The concept is quite simple: when we sample experiences to feed the Neural Network, we assume that some experiences are more valuable than others. In a uniform sampling DQN, all the experiences have the same probability to be sampled. As a result, every experience will be used about the same number of times at the end of the training. If we sample with weights, we can make it so that some experiences which are more beneficial get sampled more times on average. Now how do we distribute the weights for each experience? The publication advises us to compute a sampling probability which is proportional to the loss obtained after the forward pass of the neural network. This is equivalent to say that we want to keep the experiences which led to an important difference between the expected reward and the reward that we actually got, or in other terms, we want to keep the experiences that made the neural network learn a lot.", "Alright, now that we got the concept, it\u2019s time to implement on a real case scenario. We will try to solve the OpenAI gym environment called \u201cLunar-lander\u201d. In this environment, the agent is a spaceship undergoing gravity which can take 4 different actions: do nothing or fire left, right, or bottom engine. We get rewarded if the spaceship lands at the correct location, and penalized if the lander crashes. We also get a small penalty each time we use the bottom throttle, to avoid converging towards a situation where the AI would keep the lander in the air.", "Now we can question our approach to this problem. Why do we want to use Deep Q-Network here? The states of this environment are described by 8 variables: x, y coordinates and velocities, rotation and angular velocity of the lander, and two boolean variables to state whether the legs of the lander are in contact with the ground. As we can see, most of the variables are continuous, so a discrete approach of the Q-Network would be inadequate, and we need to be able to interpolate the total reward we expect to get at one state to choose the best action, here with a neural network.", "Now comes another question, how do prioritizing some experiences may help us to obtain better or faster results in this scenario? When we begin training our algorithm, it is very probable that the lander will just crash most of the times. However, it might be that the lander will be able to touch the ground without crashing, or land correctly on rare occasions. In that case, the difference between the expected result (negative reward) and the actual output (positive reward) would be significant, leading to a much higher probability for this experience to be sampled. In a sense, we wish to use this experience multiple times to train the neural network as an example of what is working and what direction we should take to improve the weights of the network.", "Great, we are now sure that our approach is valid. Let\u2019s dig into the details of the implementation. We will focus on the class `ReplayBuffer` as it contains most of the implementation related to the Prioritized Experience Replay, but the rest of the code is available on GitHub. The goal that we will set is to improve the rapidity of the algorithm (to be able to solve the environment with fewer episodes), without sacrificing running time due to additional computation complexity. To do that, we will be careful about the types of containers we will use to store our data, as well as how we access and sort out data. The publication cites two ways to store the priorities, one with a regular container and one with sum trees, a custom data type that can grant write and access over a priority with complexity o(1). We will try to focus here on the implementation with a regular container, as it seems more challenging to optimize so as to reduce complexity, providing with a good coding exercise!", "We assume here that the implementation of the Deep Q-Network is already done, that is we already have an agent class, which role is to manage the training by saving the experiences in the replay buffer at each step and to train the neural network episodically. The neural network is also already defined, here we chose a neural network with two hidden layers of respective size 256 and 128 neurons with ReLu activation, and a final linear activation layer.", "In the uniform sampling DQN, we randomly sample through the experiences with a linear distribution, which means we only need one container to store the experiences without any need for additional computation. For Prioritized Experience Replay, we do need to associate every experience with additional information, its priority, probability and weight.", "The priority is updated according to the loss obtained after the forward pass of the neural network. The probability is computed out of the experiences priorities, while the weight (correcting the bias introduced by not uniformly sampling during the neural network backward pass) is computed out of the probabilities. The paper introduces two more hyper-parameters alpha and beta, which control how much we want to prioritize: at the end of the training, we want to sample uniformly to avoid overfitting due to some experiences being constantly prioritized. The equations can be found below:", "According to the authors, the weights can be neglected in the case of Prioritized Experience Replay only, but are mandatory when associated with dual Q-network, another DQN implementation. The weights will still be implemented here for a potential usage in combination with a dual Q-network.", "So compared to the uniform DQN we now have 3 values to associate with the experiences. Worse than that, we need to be able to update these variables. In terms of implementation, it means that after randomly sampling our experiences, we still need to remember from where we took these experiences. Concretely, that is remembering the index of the experience in the container when we sample it (Ah, if only we had pointers). So we now get 4 variables to associate. The container that we choose is a dictionary. Actually two dictionaries, one for the experiences themselves and one for the associated data, this is fine since we need to remember the index anyway. Because we need to find the data back after processing them in the neural network, a dictionary is a good fit since the complexity of its accessor is of magnitude o(1) as we don\u2019t need to browse the whole container. For both dictionaries, the values are in form of named tuples, which makes the code clearer. We also add a small for loop to initialize the dictionaries indexes.", "Next, let\u2019s dissect the probably most computationally expensive step, the random sampling. It is expensive because, in order to sample with weights, we probably need to sort our container containing the probabilities. To sample, we use the random.choices function, let\u2019s see how this is implemented.", "If we browse the Python documentation for the function bisect we can see this: \u201cThis module provides support for maintaining a list in sorted order without having to sort the list after each insertion\u201d. Bingo! No need to look further into the code, the function does need to sort the container at least once every time we call random.choices, which is equivalent to a complexity of magnitude o(n). Our dictionary being of size 10e5, that\u2019s far from being negligible. We can\u2019t really afford to sort the container every sample, as we sample every four steps. A solution to go around this problem is to sample multiple batches at once for multiple neural network trainings in prevision. We saw that random.choices is implemented with the bissect function which makes sure that the container is only sorted once, so sampling more batches does not add any complexity.", "Let\u2019s see how this has been implemented in the ReplayBuffer class:", "Here the variables update_mem_every and update_nn_every represent respectively how often we want to compute a new set of experience batches and how often we want to train the network. The current_batch variable represents which batch is currently used to feed the neural network and is here reset to 0.", "We are now able to sample experiences with probability weights efficiently. Let\u2019s take a look at the PER algorithm to understand how to include our sampling in the bigger picture.", "We can notice two things that could be tricky for computation complexity optimization: being able to remember the maximum priority and the maximum weight for each step. This might seem easy to do, basically just comparing the newly updated values with the max at each step. But that\u2019s forgetting that the container is of fixed size, meaning that each step we will also delete an experience to be able to add one more. Now what if we delete the maximum, how do we find the second highest value? Should we always keep track of the order of the values in the container? Of course not! This would mean o(n) complexity at each step. In practice, we can simply find the maximum each time the maximum value gets deleted. So we keep track of the max, then compare every deleted entry with it. Usually, experiences to be deleted already have been used couple of times, so their priority should be low, so as the chances that it is actually the maximum value. So we are fine with sorting the container once in a while. See code below at line 9:", "To point out, we also have a variable named priorities_sum_alpha. As can be seen in the definition of the sampling probability, the sum of all the recorded experiences priorities to the power alpha needs to be computed each time. And for sure we don\u2019t want to compute this value from scratch each time so we keep track of it and update it upon addition/deletion of an experience.", "Our code is pretty much optimized, overall we should have a complexity of o(n/T), T being the number of batches we sample at once. Of course, the complexity depends on that parameter and we can play with it to find out which value would lead to the best efficiency.", "Time to test out our implementation! We run two tests, one with the prioritized experience replay implementation, another one with the uniform sampling DQN. We plot the obtained graph shown as below:", "No, this is not a mistake, the uniform sampling is outperforming the prioritized sampling! Both of the algorithms were run with the same hyper-parameters so the results can be compared. And we find out that by using prioritized sampling we are able to solve the environment in about 800 episodes while we can do it in about 500 in the case of uniform sampling. Though we went through the theory and saw that prioritizing experiences would be beneficial! Few reasons that could explain what went wrong here:", "As a matter of fact, we tried tweaking the algorithm so as to prioritize the positive experiences only. The reasoning behind that is, when learning how to play, the algorithm would crash much more than it would land correctly, and since we can crash on a much wider area than we can land, we would tend to remember much more crashing experiences than anything else. For that purpose, we tried to following adaptation: we look at the signed difference between the neural networks actual output and the expected value. If it\u2019s positive, we actually got a better reward than what we expected! Then we apply a ReLu function to assign the difference to be 0 if negative, else do nothing. In the replay buffer, so as to not just delete purely the experiences with a negative difference, we assign them with average priority.", "In theory, that would result in simply prioritizing a bit more the experiences with high positive reward difference (landing). In practice, that\u2019s a different story\u2026 The algorithm does not even converge anymore! Truth be told, prioritizing experiences is a dangerous game to play, it is easy to create bias as well as prioritizing the same experiences over and over leading to overfitting the network for a subset of experiences and failing to learn the game properly. The publication does advise to anneal the value of alpha (controlling how much you rely on priority) to 0 so that we tend towards uniform sampling, which we use, but in our case, it only results in forcing the algorithm to converge after enough iterations.", "Even though the algorithm does not lead to better learning performances, we can still verify that our other goal, reducing computation complexity, is met. To that end, we will use the uniform sampling algorithm which we know solves the environment, and a modified version of the prioritized experience implementation where the parameter alpha is assigned to 0. This way, we do sample uniformly while keeping the complexity of prioritizing experiences: we still need to sample with weights, update priorities for each training batch and so on. Since the two experiments are similar, we can safely directly compare the training duration", "As we can see, our implementation does increase the overall computation time to solve the environment from 2426s to 3161s, which corresponds to approximately a 34% increase. This is an acceptable price to pay given the complexity of what we intend to do (access and modify elements of a container at each iteration, order a container to sample from it frequently). Looking at the graph, it seems that until 300 episodes, both algorithms require about the same time to process but diverge later. This is understandable given the fact that the container of 10e5 elements becomes full at about this stage. Remember that tiny detail about having to update all the container if we remove the highest priority value, which is okay since it almost never happens? Well here, all the priorities are the same so it does happen every time once the container is full. So to look at a real comparison we can limit ourselves to the first 300 experiences which see little difference between the two implementations! To note, the publication mention that their implementation with sum trees lead to an additional computation time of about 3%. It seems that our implementation can provide similar results which is rather satisfying.", "Of course, these results depend on the hyper-parameters chosen for the prioritized experience replay implementation, namely how many batches do you want to sample at once and how frequently do you want to update the parameters alpha and beta (which need to update every single probability in the buffer). The higher these two values get, the faster the algorithm would compute, but that would probably have a non-negligible impact on the training. Since our algorithm does not provide benefits on this part, it is hard to define optimal parameters, but it should be possible to benchmark a set of parameters and decide what is the best overall compromise.", "What can we conclude in this experiment? First that we were able to implement the prioritized experience replay for deep Q-network with almost no additional computation complexity. Second, that this implementation seems not improving the agent\u2019s learning efficiency for this environment. So what could we do next?", "Last but not least, let\u2019s observe a trained agent play the game! We deserve that after all of that gruesome computation. Of course, we use the trained agent from the prioritized memory replay implementation, it took more time but it\u2019s still trained well enough!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Robotics Software Engineer, dreaming of creating robots with intelligence beyond the scope of the human brain. https://github.com/Guillaume-Cr"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fa710beecd77b&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-implement-prioritized-experience-replay-for-a-deep-q-network-a710beecd77b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-implement-prioritized-experience-replay-for-a-deep-q-network-a710beecd77b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-implement-prioritized-experience-replay-for-a-deep-q-network-a710beecd77b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-implement-prioritized-experience-replay-for-a-deep-q-network-a710beecd77b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----a710beecd77b--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a710beecd77b--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://gcrabemp.medium.com/?source=post_page-----a710beecd77b--------------------------------", "anchor_text": ""}, {"url": "https://gcrabemp.medium.com/?source=post_page-----a710beecd77b--------------------------------", "anchor_text": "Guillaume Crab\u00e9"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fea42d028f128&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-implement-prioritized-experience-replay-for-a-deep-q-network-a710beecd77b&user=Guillaume+Crab%C3%A9&userId=ea42d028f128&source=post_page-ea42d028f128----a710beecd77b---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa710beecd77b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-implement-prioritized-experience-replay-for-a-deep-q-network-a710beecd77b&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa710beecd77b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-implement-prioritized-experience-replay-for-a-deep-q-network-a710beecd77b&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@spacex?utm_source=medium&utm_medium=referral", "anchor_text": "SpaceX"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://github.com/Guillaume-Cr/lunar_lander_per", "anchor_text": "https://github.com/Guillaume-Cr/lunar_lander_per"}, {"url": "https://arxiv.org/abs/1511.05952", "anchor_text": "https://arxiv.org/abs/1511.05952"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----a710beecd77b---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----a710beecd77b---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----a710beecd77b---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa710beecd77b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-implement-prioritized-experience-replay-for-a-deep-q-network-a710beecd77b&user=Guillaume+Crab%C3%A9&userId=ea42d028f128&source=-----a710beecd77b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa710beecd77b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-implement-prioritized-experience-replay-for-a-deep-q-network-a710beecd77b&user=Guillaume+Crab%C3%A9&userId=ea42d028f128&source=-----a710beecd77b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa710beecd77b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-implement-prioritized-experience-replay-for-a-deep-q-network-a710beecd77b&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a710beecd77b--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fa710beecd77b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-implement-prioritized-experience-replay-for-a-deep-q-network-a710beecd77b&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----a710beecd77b---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----a710beecd77b--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----a710beecd77b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----a710beecd77b--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----a710beecd77b--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----a710beecd77b--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----a710beecd77b--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----a710beecd77b--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----a710beecd77b--------------------------------", "anchor_text": ""}, {"url": "https://gcrabemp.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://gcrabemp.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Guillaume Crab\u00e9"}, {"url": "https://gcrabemp.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "96 Followers"}, {"url": "https://github.com/Guillaume-Cr", "anchor_text": "https://github.com/Guillaume-Cr"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fea42d028f128&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-implement-prioritized-experience-replay-for-a-deep-q-network-a710beecd77b&user=Guillaume+Crab%C3%A9&userId=ea42d028f128&source=post_page-ea42d028f128--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Faa70df854abc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-implement-prioritized-experience-replay-for-a-deep-q-network-a710beecd77b&newsletterV3=ea42d028f128&newsletterV3Id=aa70df854abc&user=Guillaume+Crab%C3%A9&userId=ea42d028f128&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}