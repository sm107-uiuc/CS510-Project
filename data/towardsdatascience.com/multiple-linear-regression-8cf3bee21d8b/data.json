{"url": "https://towardsdatascience.com/multiple-linear-regression-8cf3bee21d8b", "time": 1683007258.9007628, "path": "towardsdatascience.com/multiple-linear-regression-8cf3bee21d8b/", "webpage": {"metadata": {"title": "Multiple Linear Regression. A complete study \u2014 Model Interpretation\u2026 | by Sangeet Aggarwal | Towards Data Science", "h1": "Multiple Linear Regression", "description": "Linear Regression, one of the most popular and discussed models, is certainly the gateway to go deeper into Machine Learning (ML). Such a simplistic, straightforward approach to modeling is worth\u2026"}, "outgoing_paragraph_urls": [{"url": "http://faculty.marshall.usc.edu/gareth-james/ISL/data.html", "anchor_text": "Advertising data", "paragraph_index": 14}, {"url": "http://faculty.marshall.usc.edu/gareth-james/ISL/Advertising.csv", "anchor_text": "here", "paragraph_index": 14}, {"url": "https://towardsdatascience.com/simple-linear-regression-35b3d940950e", "anchor_text": "Simple Linear Regression", "paragraph_index": 15}, {"url": "https://towardsdatascience.com/simple-linear-regression-35b3d940950e/#32ff", "anchor_text": "Residual Sum of Squares", "paragraph_index": 21}, {"url": "https://towardsdatascience.com/simple-linear-regression-35b3d940950e", "anchor_text": "Simple Linear Regression", "paragraph_index": 26}, {"url": "https://towardsdatascience.com/simple-linear-regression-35b3d940950e/#626f", "anchor_text": "correlated", "paragraph_index": 27}, {"url": "https://github.com/datasciencewithsan/Multiple-Linear-Regression", "anchor_text": "here", "paragraph_index": 65}, {"url": "https://www.datacamp.com/?tap_a=5644-dce66f&tap_s=910084-843f05&utm_medium=affiliate&utm_source=sangeetaggarwal", "anchor_text": "link", "paragraph_index": 67}], "all_paragraphs": ["Linear Regression, one of the most popular and discussed models, is certainly the gateway to go deeper into Machine Learning (ML). Such a simplistic, straightforward approach to modeling is worth learning as one of your first steps into ML.", "Before moving forward, let us recall that Linear Regression can be broadly classified into two categories.", "If you are new to regression, then I strongly suggest you first read about Simple Linear Regression from the link below, where you would understand the underlying maths behind and the approach to this model using interesting data and hands-on coding.", "We will see how multiple input variables together influence the output variable, while also learning how the calculations differ from that of Simple LR model. We will also build a regression model using Python.", "At last, we will go deeper into Linear Regression and will learn things like Collinearity, Hypothesis Testing, Feature Selection, and much more.", "Now one might wonder, we could also use simple linear regression to study our output against all independent variables separately. That would have made lives much easier right?", "\u201c To predict the outcome from multiple input variables. Duh!\u201d. But, is that it? Well, hold that thought.", "Consider this, suppose you have to estimate the price of a certain house you want to buy. You know the floor area, the age of the house, its distance from your workplace, the crime rate of the place, etc.", "Now, some of these factors will affect the price of the house positively. For example more the area, the more the price. On the other hand, factors like distance from the workplace, and the crime rate can influence your estimate of the house negatively (unless you are a rich criminal with interest in Machine Learning looking for a hideout, yeah I don\u2019t think so).", "Disadvantages of Simple Linear Regression \u2192 Running separate simple linear regressions will lead to different outcomes when we are interested in just one. Besides that, there may be an input variable that is itself correlated with or dependent on some other predictor. This can cause wrong predictions and unsatisfactory results.", "This is where Multiple Linear Regression comes into the picture.", "Here, Y is the output variable, and X terms are the corresponding input variables. Notice that this equation is just an extension of Simple Linear Regression, and each predictor has a corresponding slope coefficient (\u03b2).", "The first \u03b2 term (\u03b2o) is the intercept constant and is the value of Y in absence of all predictors (i.e when all X terms are 0). It may or may or may not hold any significance in a given regression problem. It\u2019s generally there to give a relevant nudge to the line/plane of regression.", "Let\u2019s now understand this with the help of some data.", "We are going to use Advertising data which is available on the site of USC Marshall School of Business. You can download it here.", "If you have read my post on Simple Linear Regression, then you are already familiar with this data. If you haven\u2019t, let me give you a quick brief.", "The advertising data set consists of the sales of a product in 200 different markets, along with advertising budgets for three different media: TV, radio, and newspaper. Here\u2019s how it looks like:", "The first row of the data says that the advertising budgets for TV, radio, and newspaper were $230.1k, $37.8k, and $69.2k respectively, and the corresponding number of units that were sold was 22.1k (or 22,100).", "In Simple Linear Regression, we can see how each advertising medium affects sales when applied without the other two media. However, in practice, all three might be working together to impact net sales. We did not consider the combined effect of these media on sales.", "Multiple Linear Regression solves the problem by taking account of all the variables in a single expression. Hence, our Linear Regression model can now be expressed as:", "Finding the values of these constants(\u03b2) is what regression model does by minimizing the error function and fitting the best line or hyperplane (depending on the number of input variables).", "This is done by minimizing the Residual Sum of Squares (RSS), which is obtained by squaring the differences between actual and predicted outcomes.", "Because this method finds the least sum of squares, it is also known as the Ordinary Least Squares (OLS) method. In Python, there are two primary ways to implement the OLS algorithm.", "You should get the following output.", "I encourage you to run the regression model using Scikit Learn as well and find the above parameters using model.coef_ & model.intercept_. Did you see the same results?", "Now that we have these values, how to interpret them? Here\u2019s how:", "Let me tell you an interesting thing here. If we run Simple Linear Regression using just the newspaper budget against sales, we\u2019ll observe the coefficient value of around 0.055, which is quite significant in comparison to what we saw above. Now, why is that?", "To understand this, let\u2019s see how these variables are correlated with each other.", "Let\u2019s visualize these numbers using a heatmap.", "Here the dark squares represent a strong correlation (close to 1) while the lighter ones represent the weaker correlation(close to 0). That\u2019s the reason, all the diagonals are dark blue, as a variable is fully correlated with itself.", "Now, the thing worth noticing here is that the correlation between newspaper and radio is 0.35. This indicates a fair relationship between newspaper and radio budgets. Hence, it can be inferred that \u2192 when the radio budget is increased for a product, there\u2019s a tendency to spend more on newspapers as well.", "This is called collinearity and is referred to as a situation in which two or more input variables are linearly related.", "Hence, even though the Multiple Regression model shows no impact on sales by the newspaper, the Simple Regression model still does due to this multicollinearity and the absence of other input variables.", "Sales & Radio \u2192 probable causation", "Sales & Newspaper \u2192 transitive correlation", "Alright! We understood Linear Regression, we built the model and even interpreted the results. What we learned so far were the fundamentals of Linear Regression. However, while dealing with real-world problems, we generally go beyond this point to statistically analyze our model and do the necessary changes if required.", "One of the fundamental questions that should be answered while running Multiple Linear Regression is, whether or not, at least one of the predictors is useful in predicting the output.", "We saw that the three predictors TV, radio and newspaper had a different degree of linear relationship with the sales. But what if the relationship is just by chance and there is no actual impact on sales due to any of the predictors?", "The model can only give us numbers to establish a close enough linear relationship between the response variable and the predictors. However, it cannot prove the credibility of these relationships.", "To have some confidence, we take help from statistics and do something known as a Hypothesis Test. We start by forming a Null Hypothesis and a corresponding Alternative Hypothesis.", "Since our goal is to find if at least one predictor is useful in predicting the output, we are in a way hoping that at least one of the coefficients(not intercept) is non-zero, not just by a random chance but due to actual cause.", "To do this, we start by forming a Null Hypothesis: All the coefficients are equal to zero.", "Hence the Alternative Hypothesis would be: At least one coefficient is not zero. It is proved by rejecting the Null Hypothesis by finding strong statistical evidence.", "The hypothesis test is performed by using F-Statistic. The formula for this statistic contains Residual Sum of Squares (RSS) and the Total Sum of Squares (TSS), which we don\u2019t have to worry about because the Statsmodels package takes care of this. The summary of the OLS model that we fit above contains the summary of all such statistics and can be obtained with this simple line of code:", "If the value of F-statistic is equal to or very close to 1, then the results are in favor of the Null Hypothesis and we fail to reject it.", "But as we can see that the F-statistic is many folds larger than 1, thus providing strong evidence against the Null Hypothesis (that all coefficients are zero). Hence, we reject the Null Hypothesis and are confident that at least one predictor is useful in predicting the output.", "Note that F-statistic is not suitable when the number of predictors(p) is large, or if p is greater than the number of data samples (n).", "Hence, we can say that at least one of the three advertising agents is useful in predicting sales.", "But which one or which two are important? Are all of them important? To find this out, we will perform Feature Selection or variable selection. Now one way of doing this is trying all possible combinations i.e.", "Here, it still looks feasible to try all 7 combinations, but if there are more predictors, the number of combinations will increase exponentially. For example, by adding only one more predictor to our case study, the total combinations would become 15. Just imagine having a dozen predictors.", "Hence we need more efficient ways to perform Feature Selection.", "Two of the most popular approaches to do feature selection are:", "In this post, I\u2019ll walk you through the forward selection method. To begin with, let\u2019s understand how we are going to select or reject the added variable.", "We are going to use 2 measures to evaluate our new model after each addition: RSS and R\u00b2.", "We are already familiar with RSS which is the Residual Sum of Squares and is calculated by squaring the difference between actual outputs and predicted outcomes. It should be minimum for the model to perform well.", "R\u00b2 is the measure of the degree to which variance in data is explained by the model. Mathematically, it\u2019s the square of the correlation between actual and predicted outcomes. R\u00b2 closer to 1 indicates that the model is good and explains the variance in data well. A value closer to zero indicates a poor model.", "Luckily, it\u2019s calculated for us by the OLS module in Statsmodels. So let\u2019s begin.", "Let\u2019s first evaluate models with single predictors one by one, starting with TV.", "We observe that for model_TV, the RSS is least and R\u00b2 value is the most among all the models. Hence we select model_TV as our base model to move forward.", "Now, we will add the radio and newspaper one by one and check the new values.", "As we can see that our values have improved tremendously. RSS has decreased and R\u00b2 has increased further, as compared to model_TV. It\u2019s a good sign. Let\u2019s now check the same for TV and newspaper.", "The values have improved by adding newspaper too, but not as much as with the radio. Hence, at this step, we will proceed with the TV & radio model and will observe the difference when we add newspaper to this model.", "The values have not improved with any significance. Hence, it\u2019s imperative to not add newspaper and finalize the model with TV and radio as selected features.", "So our final model can be expressed as below:", "Plotting the variables TV, radio, and sales in the 3D graph, we can visualize how our model has fit a regression plane to the data.", "That\u2019s it for Multiple Linear Regression. You can find the full code behind this post here. I hope you had a good time reading and learning. For more, stay tuned.", "If you are new to Data Science and Machine Learning and wondering where to begin your journey from, do check the link below, where I have mentioned step by step method to learn Data Science, with lots of sources for you to choose from.", "Can\u2019t wait? If you want to dive right into a course, check out the career tracks in Data Science that suits you, from the link below.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Enthusiast | I try to simplify Data Science and other concepts through my blogs"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F8cf3bee21d8b&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmultiple-linear-regression-8cf3bee21d8b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmultiple-linear-regression-8cf3bee21d8b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmultiple-linear-regression-8cf3bee21d8b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmultiple-linear-regression-8cf3bee21d8b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----8cf3bee21d8b--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8cf3bee21d8b--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://datasciencewithsan.medium.com/?source=post_page-----8cf3bee21d8b--------------------------------", "anchor_text": ""}, {"url": "https://datasciencewithsan.medium.com/?source=post_page-----8cf3bee21d8b--------------------------------", "anchor_text": "Sangeet Aggarwal"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdb3258338f2f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmultiple-linear-regression-8cf3bee21d8b&user=Sangeet+Aggarwal&userId=db3258338f2f&source=post_page-db3258338f2f----8cf3bee21d8b---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8cf3bee21d8b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmultiple-linear-regression-8cf3bee21d8b&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8cf3bee21d8b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmultiple-linear-regression-8cf3bee21d8b&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@steve_j?utm_source=medium&utm_medium=referral", "anchor_text": "Steve Johnson"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://towardsdatascience.com/simple-linear-regression-35b3d940950e", "anchor_text": "Simple Linear RegressionEverything you need to know about Simple Linear Regressiontowardsdatascience.com"}, {"url": "http://faculty.marshall.usc.edu/gareth-james/ISL/data.html", "anchor_text": "Advertising data"}, {"url": "http://faculty.marshall.usc.edu/gareth-james/ISL/Advertising.csv", "anchor_text": "here"}, {"url": "https://towardsdatascience.com/simple-linear-regression-35b3d940950e", "anchor_text": "Simple Linear Regression"}, {"url": "https://towardsdatascience.com/simple-linear-regression-35b3d940950e/#32ff", "anchor_text": "Residual Sum of Squares"}, {"url": "https://towardsdatascience.com/simple-linear-regression-35b3d940950e", "anchor_text": "Simple Linear Regression"}, {"url": "https://towardsdatascience.com/simple-linear-regression-35b3d940950e/#626f", "anchor_text": "correlated"}, {"url": "https://medium.com/u/db3258338f2f?source=post_page-----8cf3bee21d8b--------------------------------", "anchor_text": "Sangeet Aggarwal"}, {"url": "https://github.com/datasciencewithsan/Multiple-Linear-Regression", "anchor_text": "here"}, {"url": "https://towardsdatascience.com/data-science-from-scratch-4343d63c1c66", "anchor_text": "Data science from scratchHow to step into Data Science as a complete beginnertowardsdatascience.com"}, {"url": "https://www.datacamp.com/?tap_a=5644-dce66f&tap_s=910084-843f05&utm_medium=affiliate&utm_source=sangeetaggarwal", "anchor_text": "link"}, {"url": "https://www.datacamp.com/?tap_a=5644-dce66f&tap_s=910084-843f05&utm_medium=affiliate&utm_source=sangeetaggarwal", "anchor_text": "Learn R, Python & Data Science OnlineLearn Data Science from the comfort of your browser, at your own pace with DataCamp's video tutorials &amp; coding\u2026www.datacamp.com"}, {"url": "https://medium.com/tag/data-science?source=post_page-----8cf3bee21d8b---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----8cf3bee21d8b---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/hypothesis-testing?source=post_page-----8cf3bee21d8b---------------hypothesis_testing-----------------", "anchor_text": "Hypothesis Testing"}, {"url": "https://medium.com/tag/feature-selection?source=post_page-----8cf3bee21d8b---------------feature_selection-----------------", "anchor_text": "Feature Selection"}, {"url": "https://medium.com/tag/regression?source=post_page-----8cf3bee21d8b---------------regression-----------------", "anchor_text": "Regression"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8cf3bee21d8b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmultiple-linear-regression-8cf3bee21d8b&user=Sangeet+Aggarwal&userId=db3258338f2f&source=-----8cf3bee21d8b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8cf3bee21d8b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmultiple-linear-regression-8cf3bee21d8b&user=Sangeet+Aggarwal&userId=db3258338f2f&source=-----8cf3bee21d8b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8cf3bee21d8b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmultiple-linear-regression-8cf3bee21d8b&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8cf3bee21d8b--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F8cf3bee21d8b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmultiple-linear-regression-8cf3bee21d8b&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----8cf3bee21d8b---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----8cf3bee21d8b--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----8cf3bee21d8b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----8cf3bee21d8b--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----8cf3bee21d8b--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----8cf3bee21d8b--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----8cf3bee21d8b--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----8cf3bee21d8b--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----8cf3bee21d8b--------------------------------", "anchor_text": ""}, {"url": "https://datasciencewithsan.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://datasciencewithsan.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Sangeet Aggarwal"}, {"url": "https://datasciencewithsan.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "169 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdb3258338f2f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmultiple-linear-regression-8cf3bee21d8b&user=Sangeet+Aggarwal&userId=db3258338f2f&source=post_page-db3258338f2f--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F3b6740e12074&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmultiple-linear-regression-8cf3bee21d8b&newsletterV3=db3258338f2f&newsletterV3Id=3b6740e12074&user=Sangeet+Aggarwal&userId=db3258338f2f&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}