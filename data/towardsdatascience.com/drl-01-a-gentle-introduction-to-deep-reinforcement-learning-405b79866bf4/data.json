{"url": "https://towardsdatascience.com/drl-01-a-gentle-introduction-to-deep-reinforcement-learning-405b79866bf4", "time": 1683007404.1107562, "path": "towardsdatascience.com/drl-01-a-gentle-introduction-to-deep-reinforcement-learning-405b79866bf4/", "webpage": {"metadata": {"title": "A gentle introduction to Deep Reinforcement Learning | by Jordi TORRES.AI | Towards Data Science", "h1": "A gentle introduction to Deep Reinforcement Learning", "description": "Deep Reinforcement Learning (DRL), a very fast-moving field, is the combination of Reinforcement Learning and Deep Learning. It is also the most trending type of Machine Learning because it can solve\u2026"}, "outgoing_paragraph_urls": [{"url": "https://torres.ai/deep-reinforcement-learning-explained-series/", "anchor_text": "Deep Reinforcement Learning Explained", "paragraph_index": 0}, {"url": "https://torres.ai/deep-reinforcement-learning-explained-series/", "anchor_text": "series", "paragraph_index": 0}, {"url": "https://learning.mpi-sws.org/mlss2016/", "anchor_text": "Machine Learning Summer School", "paragraph_index": 3}, {"url": "http://joschu.net/index.html", "anchor_text": "John Schulman", "paragraph_index": 3}, {"url": "http://www.cs.berkeley.edu/", "anchor_text": "UC Berkeley", "paragraph_index": 3}, {"url": "http://www.openai.com/", "anchor_text": "OpenAI", "paragraph_index": 3}, {"url": "https://en.wikipedia.org/wiki/Stuart_J._Russell", "anchor_text": "Stuart Rusell", "paragraph_index": 12}, {"url": "https://en.wikipedia.org/wiki/Peter_Norvig", "anchor_text": "Peter Norvig", "paragraph_index": 12}, {"url": "https://en.wikipedia.org/wiki/AI_winter", "anchor_text": "AI winter", "paragraph_index": 15}, {"url": "https://gym.openai.com/envs/FrozenLake-v0/", "anchor_text": "Frozen-Lake", "paragraph_index": 46}, {"url": "https://gym.openai.com", "anchor_text": "Gym", "paragraph_index": 46}, {"url": "https://gym.openai.com", "anchor_text": "Gym", "paragraph_index": 47}, {"url": "https://gym.openai.com/envs/FrozenLake-v0/", "anchor_text": "Frozen-Lake", "paragraph_index": 49}, {"url": "https://colab.research.google.com/", "anchor_text": "Colab", "paragraph_index": 56}, {"url": "https://github.com/openai/gym", "anchor_text": "here", "paragraph_index": 56}, {"url": "http://gym.openai.com/docs/", "anchor_text": "here", "paragraph_index": 63}, {"url": "https://medium.com/@SmartLabAI/a-brief-overview-of-imitation-learning-8a8a75c44a9c", "anchor_text": "imitation learning", "paragraph_index": 74}, {"url": "https://towardsdatascience.com/drl-02-formalization-of-a-reinforcement-learning-problem-108b52ebfd9a", "anchor_text": "next post", "paragraph_index": 88}, {"url": "https://towardsdatascience.com/drl-02-formalization-of-a-reinforcement-learning-problem-108b52ebfd9a", "anchor_text": "next post", "paragraph_index": 88}, {"url": "https://www.upc.edu/en", "anchor_text": "UPC Barcelona Tech", "paragraph_index": 89}, {"url": "https://www.bsc.es/", "anchor_text": "Barcelona Supercomputing Center", "paragraph_index": 89}, {"url": "https://torres.ai/deep-reinforcement-learning-explained-series/", "anchor_text": "series", "paragraph_index": 90}, {"url": "https://twitter.com/hashtag/StayAtHome?src=hashtag_click", "anchor_text": "#StayAtHome", "paragraph_index": 91}, {"url": "https://torres.ai", "anchor_text": "https://torres.ai", "paragraph_index": 93}], "all_paragraphs": ["This is the first post of the series \u201cDeep Reinforcement Learning Explained\u201d; an introductory series that gradually and with a practical approach introduces the reader to the basic concepts and methods used in modern Deep Reinforcement Learning.", "Deep Reinforcement Learning (DRL), a very fast-moving field, is the combination of Reinforcement Learning and Deep Learning. It is also the most trending type of Machine Learning because it can solve a wide range of complex decision-making tasks that were previously out of reach for a machine to solve real-world problems with human-like intelligence.", "Today I\u2019m starting a series about Deep Reinforcement Learning that will bring the topic closer to the reader. The purpose is to review the field from specialized terms and jargons to fundamental concepts and classical algorithms in the area, that newbies would not get lost while starting in this amazing area.", "My first serious contact with Deep Reinforcement Learning was in Cadiz (Spain), during the Machine Learning Summer School in 2016. I attended the three days seminar of John Schulman (at that time from UC Berkeley and cofounder of OpenAI) about Deep Reinforcement Learning.", "It was awesome, but I also have to confess that it was tremendously difficult for me to follow John\u2019s explanations. It\u2019s been a long time since then, and thanks to working with Xavier Gir\u00f3 and Ph.D. students like Victor Campos and MPh.D.am Bellver, I\u2019ve been able to move forward and enjoy the subject.", "But even though several years have passed since then, I sincerely believe that the taxonomy of different approaches to Reinforcement Learning that he presented is still a good scheme to organize knowledge for beginners.", "Dynamic Programming is actually what most reinforcement learning courses in textbooks start. I will do that, but before, as John did in his seminar, I will introduce the Cross-Entropy method, a sort of evolutionary algorithm, although most books do not deal with it. It will go very well with this first method to introduce deep learning in reinforcement learning, Deep Reinforcement Learning, because it is a straightforward method to implement, and it works surprisingly well.", "With this method, we will be able to do a convenient review of how Deep Learning and Reinforcement Learning collaborate before entering the more classical approaches of treating an RL problem without considering DL such as Dynamic Programming, Monte Carlo, Temporal Difference Learning following the order of the vast majority of academic books on the subject. We will then dedicate the last part of this series to the most fundamental algorithms (not the state of the art because it is pervasive) of DL + RL as Policy Gradient Methods.", "Specifically, in this first publication, we will briefly present what Deep Reinforcement Learning is and the basic terms used in this research and innovation area.", "I think that Deep Reinforcement Learning is one of the most exciting fields in Artificial Intelligence. It\u2019s marrying the power and the ability of deep neural networks to represent and comprehend the world with the ability to act on that understanding. Let\u2019s see if I\u2019m able to share that excitement. Here we go!", "Exciting news in Artificial Intelligence (AI) has just happened in recent years. For instance, AlphaGo defeated the best professional human player in the game of Go. Or last year, for example, our friend Oriol Vinyals and his team in DeepMind showed the AlphaStar Agent beat professional players at the game of StarCraft II. Or a few months later, OpenAI\u2019s Dota-2-playing bot became the first AI system to beat the world champions in an e-sports game. All these systems have in common that they use Deep Reinforcement Learning (DRL). But what are AI and DRL?", "We have to take a step back to look at the types of learning. Sometimes the terminology itself can confuse us with the fundamentals. Artificial Intelligence, the main field of computer science in which Reinforcement Learning (RL) falls into, is a discipline concerned with creating computer programs that display humanlike \u201cintelligence\u201d.", "What do we mean when we talk about Artificial Intelligence? Artificial intelligence (AI) is a vast area. Even an authoritative AI textbook Artificial Intelligence, a modern approach written by Stuart Rusell and Peter Norvig, does not give a precise definition and discuss definitions of AI from different perspectives:", "Without a doubt, this book is the best starting point to have a global vision of the subject. But trying to make a more general approach (purpose of this series), we could accept a simple definition in which by Artificial Intelligence we refer to that intelligence shown by machines, in contrast to the natural intelligence of humans. In this sense, a possible concise and general definition of Artificial Intelligence could be the effort to automate intellectual tasks usually performed by humans.", "As such, the area of artificial intelligence is a vast scientific field that covers many areas of knowledge related to machine learning; even many more approaches are not always cataloged as Machine Learning is included by my university colleagues who are experts in the subject. Besides, over time, as computers have been increasingly able to \u201cdo things\u201d, tasks or technologies considered \u201csmart\u201d have been changing.", "Furthermore, since the 1950s, Artificial Intelligence has experienced several waves of optimism, followed by disappointment and loss of funding and interest (periods known as AI winter), followed by new approaches, success, and financing. Moreover, during most of its history, Artificial Intelligence research has been dynamically divided into subfields based on technical considerations or concrete mathematical tools and with research communities that sometimes did not communicate sufficiently with each other.", "Machine Learning (ML) is in itself a large field of research and development. In particular, Machine Learning could be defined as the subfield of Artificial Intelligence that gives computers the ability to learn without being explicitly programmed, that is, without requiring the programmer to indicate the rules that must be followed to achieve their task; the computers do them automatically.", "Generalizing, we can say that Machine Learning consists of developing a prediction \u201calgorithm\u201d for a particular use case for each problem. These algorithms learn from the data to find patterns or trends to understand what the data tell us, and in this way, build a model to predict and classify the elements.", "Given the maturity of the research area in Machine Learning, there are many well-established approaches to Machine Learning. Each of them uses a different algorithmic structure to optimize the predictions based on the received data. Machine Learning is a broad field with a complex taxonomy of algorithms that are grouped, in general, into three main categories:", "Orthogonal to this categorization, we can consider a powerful approach to ML, called Deep Learning (DL), a topic of which we have discussed extensively in previous posts. Remember that Deep Learning algorithms are based on artificial neural networks, whose algorithmic structures allow models composed of multiple processing layers to learn data representations with various abstraction levels.", "DL is not a separate ML branch, so it\u2019s not a different task than those described above. DL is a collection of techniques and methods for using neural networks to solve ML tasks, either Supervised Learning, Unsupervised Learning, or Reinforcement Learning. We can represent it graphically in Figure 1.", "Deep Learning is one of the best tools that we have today to handle unstructured environments; they can learn from large amounts of data or discover patterns. But this is not decision-making; it is a recognition problem. Reinforcement Learning provides this feature.", "Reinforcement Learning can solve the problems using a variety of ML methods and techniques, from decision trees to SVMs, to neural networks. However, in this series, we only use neural networks; this is what the \u201cdeep\u201d part of DRL refers to, after all. However, neural networks are not necessarily the best solution to every problem. For instance, neural networks are very data-hungry and challenging to interpret. Still, without doubt, neural networks are at this moment one of the most powerful techniques available, and their performance is often the best.", "In this section, we provide a brief first approach to RL, due it is essential for a good understanding of deep reinforcement learning, a particular type of RL, with deep neural networks for state representation and/or function approximation for value function, policy, and so on.", "Learning by interacting with our Environment is probably the first approach that comes to our mind when we think about the nature of learning. It is the way we intuit that an infant learns. And we know that such interactions are undoubtedly an essential source of knowledge about our environment and ourselves throughout people\u2019s lives, not just infants. For example, when we are learning to drive a car, we are entirely aware of how the environment responds to what we do, and we also seek to influence what happens in our environment through our actions. Learning from the interaction is a fundamental concept that underlies almost all learning theories and is the foundation of Reinforcement Learning.", "The approach of Reinforcement Learning is much more focused on goal-directed learning from interaction than are other approaches to Machine Learning. The learning entity is not told what actions to take, but instead must discover for itself which actions produce the greatest reward, its goal, by testing them by \u201ctrial and error.\u201d Furthermore, these actions can affect not only the immediate reward but also the future ones, \u201cdelayed rewards\u201d, since the current actions will determine future situations (how it happens in real life). These two characteristics, \u201ctrial and error\u201d search and \u201cdelayed reward\u201d, are two distinguishing characteristics of reinforcement learning that we will cover throughout this series of posts.", "Reinforcement Learning (RL) is a field that is influenced by a variety of other well-established fields that tackle decision-making problems under uncertainty. For instance, Control Theory studies ways to control complex known dynamical systems; however, the dynamics of the systems we try to control are usually known in advance, unlike the case of DRL, which is not known in advance. Another field can be Operations Research that also studies decision-making under uncertainty but often contemplates much larger action spaces than those commonly seen in RL.", "As a result, there is a synergy between these fields, which is undoubtedly positive for science advancement. But it also brings some inconsistencies in terminologies, notations, and so on. That is why in this section, we will provide a detailed introduction to terminologies and notations that we will use throughout the series.", "Reinforcement Learning is essentially a mathematical formalization of a decision-making problem that we will introduce later in this series.", "In Reinforcement Learning there are two core components:", "For example, in the tic-tac-toe game, we can consider that the Agent is one of the players, and the Environment includes the board game and the other player.", "These two core components continuously interact so that the Agent attempts to influence the Environment through actions, and the Environment reacts to the Agent\u2019s actions. How the environment reacts to specific actions is defined by a model that may or may not be known by the Agent, and this differentiates two circumstances:", "The Environment is represented by a set of variables related to the problem (very dependent on the type of problem we want to solve). This set of variables and all the possible values they can take are referred to as the state space. A state is an instantiation of the state space, a set of values the variables take.", "Due that we are considering that the Agent doesn\u2019t have access to the actual full state of the Environment, it is usually called observation, the part of the state that the Agent can observe. However, we will often see in the literature observations and states being used interchangeably, so we will do this in this series of posts.", "At each state, the Environment makes available a set of actions, from which the Agent will choose an action. The Agent influences the Environment through these actions, and the Environment may change states as a response to the Agent\u2019s action. The function responsible for this mapping is called in the literature transition function or transition probabilities between states.", "The Environment commonly has a well-defined task and may provide to the Agent a reward signal as a direct answer to the Agent\u2019s actions. This reward is feedback on how well the last action contributes to achieving the task to be performed by the Environment. The function responsible for this mapping is called the reward function. As we will see later, the Agent\u2019s goal is to maximize the overall reward it receives, and so rewards are the motivation the Agent needs to act in the desired behavior.", "Let\u2019s summarize in the following Figure the concepts introduced earlier in the Reinforcement Learning cycle:", "Generally speaking, Reinforcement Learning is basically about turning this Figure into a mathematical formalism.", "The cycle begins with the Agent observing the Environment (step 1) and receiving a state and a reward. The Agent uses this state and reward for deciding the next action to take (step 2). The Agent then sends an action to the Environment in an attempt to control it in a favorable way (step 3). Finally, the environment transitions, and its internal state changes as a consequence due to the previous state and the Agent\u2019s action (step 4). Then, the cycle repeats.", "The task the Agent is trying to solve may or may not have a natural ending. Tasks that have a natural ending, such as a game, are called episodic tasks. Conversely, tasks that do not, are called continuous tasks, for example learning forward motion. The sequence of time steps from the beginning to the end of an episodic task is called an episode.", "As we will see, Agents may take several time steps and episodes to learn how to solve a task. The sum of rewards collected in a single episode is called a return. Agents are often designed to maximize the return.", "One of the limitations is that these rewards are not disclosed to the Agent until the end of an episode, which we introduced earlier as \u201cdelayed reward\u201d. For example, in the game of tic-tac-toe the rewards for each movement (action) are not known until the end of the game. It would be a positive reward if the agent won the game (because the agent had achieved the overall desired outcome) or a negative reward (penalties) if the agent had lost the game.", "Another important characteristic, and challenge in Reinforcement Learning, is the trade-off between \u201cexploration\u201d and \u201cexploitation\u201d. Trying to obtain many rewards, an Agent must prefer actions that it has tried in the past and knows that will be effective actions in producing reward. But to discover such actions, paradoxically, it has to try actions that it has not selected never before.", "In summary, an Agent has to exploit what it has already experienced to obtain as much reward as possible, but at the same time, it also has to explore to make select better action in the future. The exploration-exploitation dilemma is a crucial topic and still an unsolved research topic. We will talk about this trade-off later in this series.", "Let\u2019s strengthen our understanding of Reinforcement Learning by looking at a simple example, a Frozen Lake (very slippery) where our agent can skate:", "The Frozen-Lake Environment that we will use as an example is an ice skating rink, divided into 16 cells (4x4), and as shown in the figure below, some of the cells have broken the ice. The skater named Agent begins to skate in the top-left position, and its goal is to reach the bottom-right place avoiding falling into the four holes in the track.", "The described example is coded as the Frozen-Lake Environment from Gym. With this example of Environment, we will review and clarify the RL terminology introduced until now. It will also be useful for future posts in this series to have this example.", "OpenAI is an artificial intelligence (AI) research organization that provides a famous toolkit called Gym for training a reinforcement learning agent to develop and compare RL algorithms. Gym offers a variety of environments for training an RL agent ranging from classic control tasks to Atari game environments. We can train our RL agent to learn in these simulated environments using various RL algorithms. Throughout the series, we will use the Gym toolkit to build and evaluate reinforcement learning algorithms for several classic control tasks such as Cart-Pole balancing or mountain car climbing.", "Gym also provides 59 Atari game environments, including Pong, Space Invaders, Air Raid, Asteroids, Centipede, Ms. Pac-Man, etc. Training our reinforcement learning agent to play Atari games is an interesting as well as challenging task. Later in this series, we will train our DQN reinforcement learning agent to play Atari Pong game environment.", "Let\u2019s introduce as an example one of the most straightforward environments called Frozen-Lake environment.", "Frozen-Lake Environment is from the so-called grid-world category when the Agent lives in a grid of size 4x4 (has 16 cells), which means a state space composed of 16 states (0\u201315) in the i, j coordinates of the grid-world.", "In Frozen-Lake, the Agent always starts at a top-left position, and its goal is to reach the bottom-right position of the grid. There are four holes in the fixed cells of the grid, and if the Agent gets into those holes, the episode ends, and the reward obtained is zero. If the Agent reaches the destination cell, it receives a reward of +1, and the episode ends. The following Figure shows a visual representation of the Frozen-Lake Environment:", "To reach the goal, the Agent has an action space composed of four directions movements: up, down, left, and right. We also know that there is a fence around the lake, so if the Agent tries to move out of the grid world, it will just bounce back to the cell from which it tried to move.", "Because the lake is frozen, the world is slippery, so the Agent\u2019s actions do not always turn out as expected \u2014 there is a 33% chance that it will slip to the right or the left. If we want the Agent to move left, for example, there is a 33% probability that it will, indeed, move left, a 33% chance that it will end up in the cell above, and a 33% chance that it will end up in the cell below.", "This behavior of the Environment is reflected in the transition function or transition probabilities presented before. However, at this point, we do not need to go into more detail on this function and leave it for later.", "As a summary, we could represent all this information visually in the following Figure:", "Let\u2019s look at how this Environment is represented in Gym. I suggest to use the Colab offered by Google to execute the code described in this post (Gym package is already installed). If you prefer to use your Python programming environment, you can install Gym using the steps provided here.", "The first step is to import Gym:", "Then, specify the game from Gym you want to use. We will use the Frozen-Lake game:", "The environment of the game can be reset to the initial state using:", "And, to see a view of the game state, we can use:", "The surface rendered by render()is presented using a grid like the following:", "Where the highlighted character indicates the position of the Agent in the current time step and", "The official documentation can be found here to see the detailed usage and explanation of Gym toolkit.", "For the moment, we will create the most straightforward Agent that we can make that only does random actions. For this purpose, we will use the action_space.sample() that samples a random action from the action space.", "Assume that we allow a maximum of 10 iterations; the following code can be our \u201cdumb\u201d Agent:", "If we run this code, it will output something like the following lines, where we can observe the Timestep, the action, and the Environment state:", "In general, it is challenging, if not almost impossible, to find an episode of our \u201cdumb\u201d Agent in which, with randomly selected actions, it can overcome the obstacles and reach the goal cell. So how could we build an Agent to pursue it?. This is what we will present in the next installment of this series, where we will further formalize the problem and build a new Agent version that can learn to reach the goal cell.", "To finish this post, let\u2019s review the basis of Reinforcement Learning for a moment, comparing it with other learning methods.", "In supervised learning, the system learns from training data that consists of a labeled pair of inputs and outputs. So, we train the model (Agent) using the training data in such a way that the model can generalize its learning to new unseen data (the labeled pairs of inputs and outputs guide the model in learning the given task).", "Let\u2019s understand the difference between supervised and reinforcement learning with an example. Imagine we want to train a model to play chess using supervised learning. In this case, we will train the model to learn using a training dataset that includes all the moves a player can make in each state, along with labels indicating whether it is a good move or not. Whereas in the case of RL, our agent will not be given any sort of training data; instead, we just provide a reward to the agent for each action it performs. Then, the agent will learn by interacting with the environment, and it will choose its actions based on the reward it gets.", "Similar to supervised learning, in unsupervised learning, we train the model based on the training data. But in the case of unsupervised learning, the training data does not contain any labels. And this leads to a common misconception that RL is a kind of unsupervised learning due we don\u2019t have labels as input data. But it is not. In unsupervised learning, the model learns the hidden structure in the input data, whereas, in RL, the model learns by maximizing the reward.", "A classic example is a movie recommendation system that wants to recommend a new movie to the user. With unsupervised learning, the model (agent) will find movies similar to the film the user (or users with a profile similar to the user) has viewed before and recommend new movies to the user. Instead, with Reinforcement Learning, the agent continually receives feedback from the user. This feedback represents rewards (a reward could be time spent watching a movie, time spent watching trailers, how many movies in a row have he watched, and so on). Based on the rewards, an RL agent will understand the user\u2019s movie preference and then suggest new movies accordingly. It is essential to notice that an RL agent can know if the user\u2019s movie preference changes and suggest new movies according to the user\u2019s changed movie preference dynamically.", "We can think that we don\u2019t have data in Reinforcement Learning as we have in Supervised or Unsupervised Learning. However, the data is actually the Environment because if you interact with this Environment, then data (trajectories) can be created, which are sequences of observations and actions. Then we can do some learning on top, and that\u2019s basically the core of Reinforcement Learning.", "Sometimes, we can use extra data from people or trajectories that exist, for instance, in imitation learning. We might actually just observe a bunch of people playing the game, and we don\u2019t need to know precisely how the Environment works. Sometimes we have explicitly given a data set, as a sort of a supervised data set, but in the pure Reinforcement Learning setting, the only data is the Environment.", "Reinforcement Learning has evolved rapidly over the past few years with a wide range of applications. One of the primary reasons for this evolution is the combination of Reinforcement Learning and Deep Learning. This is why we focus this series on presenting the basic state-of-the-art Deep Reinforcement Learning algorithms (DRL).", "The media has tended to focus on applications where DRL defeat humans at games, with examples as I mentioned at the beginning of this post: AlphaGo defeated the best professional human player in the game of Go; AlphaStar beat professional players at the game of StarCraft II; OpenAI\u2019s Dota-2-playing bot beat the world champions in an e-sports game.", "Fortunately, there are many real-life applications of DRL. One of the well known is in the area of driverless cars. In manufacturing, intelligent robots are trained using DRL to place objects in the right position, reducing labor costs, and increasing productivity. Another popular application of RL is dynamic pricing that allows changing the price of products based on demand and supply. Also, in a recommendation system, RL is used to build a recommendation system where the user\u2019s behavior continually changes.", "In today\u2019s business activities, DRL is used extensively in supply chain management, demand forecasting, inventory management, handling warehouse operations, etc. DRL is also widely used in financial portfolio management, predicting, and trading in commercial transaction markets. DRL has been commonly used in several Natural Language Processing (NLP) tasks, such as abstractive text summarization, chatbots, etc.", "Many recent research papers suggest applications of DRL in healthcare, education systems, smart cities, among many others. In summary, no business sector is left untouched by DRL.", "DRL agents can sometimes control hazardous real-life Environments, like robots or cars, which increases the risk of making incorrect choices. There is an important field called safe RL that attempts to deal with this risk, for instance, learning a policy that maximizes rewards while operating within predefined safety constraints.", "Also, DRL agents are also at risk from an attack, like any other software system. But DRL adds a few new attack vectors over and above traditional machine learning systems because, in general, we are dealing with systems much more complex to understand and model.", "Considering the safety and security of DRL systems are outside the introductory scope of this post. Still, I would like the reader to be aware of it and if in the future you put a DRL system into operation, keep in mind that you should treat this point in more depth.", "Artificial Intelligence is definitely penetrating society, like electricity, what will we expect? The future we will \u201cinvent\u201d is a choice we make jointly, not something that happens. We are in a position of power. With DRL, we have the power and authority to automate decisions and entire strategies.", "This is good! But as in most things in life, where there is light, can be the shadow, and DRL technology is hazardous in the wrong hands. I ask you that as engineers consider what we are building: Could our DRL system accidentally add bias? How does this affect individuals?. Or how does our solution affect the climate due to its energy consumption? Can be our DRL solution unintended used? Or use?. Or, in accordance with our ethics, can it have a type of use that we could consider nefarious?", "We must mull over the imminent adoption of Artificial Intelligence and its impact. Were we to go on to build Artificial Intelligence without regard to our responsibility of preventing its misuse, we can never expect to see Artificial Intelligence help humanity prosper.", "All of us, who are working or want to work on these topics, cannot shy away from our responsibility, because otherwise, we will regret it in the future.", "We started the post by understanding the basic idea of RL. We learned that RL is a trial and error learning process and the learning in RL happens based on a reward. We presented the difference between RL and the other ML paradigms. Finally, we looked into some real-life applications of RL and thought about the safety, security, and ethics of DRL.", "In the next post, we will learn about the Markov Decision Process (MDP) and how the RL environment can be modeled as an MDP. Next, we will review several important fundamental concepts involved in RL. See you in the next post!", "by UPC Barcelona Tech and Barcelona Supercomputing Center", "A relaxed introductory series that gradually and with a practical approach introduces the reader to this exciting technology that is the real enabler of the latest disruptive advances in the field of Artificial Intelligence.", "I started to write this series in May, during the period of lockdown in Barcelona. Honestly, writing these posts in my spare time helped me to #StayAtHome because of the lockdown. Thank you for reading this publication in those days; it justifies the effort I made.", "Disclaimers \u2014 These posts were written during this period of lockdown in Barcelona as a personal distraction and dissemination of scientific knowledge, in case it could be of help to someone, but without the purpose of being an academic reference document in the DRL area. If the reader needs a more rigorous document, the last post in the series offers an extensive list of academic resources and books that the reader can consult. The author is aware that this series of posts may contain some errors and suffers from a revision of the English text to improve it if the purpose were an academic document. But although the author would like to improve the content in quantity and quality, his professional commitments do not leave him free time to do so. However, the author agrees to refine all those errors that readers can report as soon as he can.", "Professor at UPC Barcelona Tech & Barcelona Supercomputing Center. Research focuses on Supercomputing & Artificial Intelligence https://torres.ai @JordiTorresAI"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F405b79866bf4&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdrl-01-a-gentle-introduction-to-deep-reinforcement-learning-405b79866bf4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdrl-01-a-gentle-introduction-to-deep-reinforcement-learning-405b79866bf4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdrl-01-a-gentle-introduction-to-deep-reinforcement-learning-405b79866bf4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdrl-01-a-gentle-introduction-to-deep-reinforcement-learning-405b79866bf4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/tagged/deep-r-l-explained", "anchor_text": "Deep Reinforcement Learning Explained \u2014 01"}, {"url": "https://torres-ai.medium.com/?source=post_page-----405b79866bf4--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----405b79866bf4--------------------------------", "anchor_text": ""}, {"url": "https://torres-ai.medium.com/?source=post_page-----405b79866bf4--------------------------------", "anchor_text": "Jordi TORRES.AI"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F497013a3c715&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdrl-01-a-gentle-introduction-to-deep-reinforcement-learning-405b79866bf4&user=Jordi+TORRES.AI&userId=497013a3c715&source=post_page-497013a3c715----405b79866bf4---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----405b79866bf4--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F405b79866bf4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdrl-01-a-gentle-introduction-to-deep-reinforcement-learning-405b79866bf4&user=Jordi+TORRES.AI&userId=497013a3c715&source=-----405b79866bf4---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F405b79866bf4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdrl-01-a-gentle-introduction-to-deep-reinforcement-learning-405b79866bf4&source=-----405b79866bf4---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://torres.ai/deep-reinforcement-learning-explained-series/", "anchor_text": "Deep Reinforcement Learning Explained"}, {"url": "https://torres.ai/deep-reinforcement-learning-explained-series/", "anchor_text": "series"}, {"url": "https://medium.com/aprendizaje-por-refuerzo/1-introducci\u00f3n-al-aprendizaje-por-refuerzo-92c9239aed90", "anchor_text": "Spanish version"}, {"url": "https://medium.com/aprendizaje-por-refuerzo/1-introducci%C3%B3n-al-aprendizaje-por-refuerzo-92c9239aed90", "anchor_text": "1. Introducci\u00f3n al aprendizaje por refuerzoAcceso abierto al cap\u00edtulo 1 del libro Introducci\u00f3n al aprendizaje por refuerzo profundomedium.com"}, {"url": "https://learning.mpi-sws.org/mlss2016/", "anchor_text": "Machine Learning Summer School"}, {"url": "http://joschu.net/index.html", "anchor_text": "John Schulman"}, {"url": "http://www.cs.berkeley.edu/", "anchor_text": "UC Berkeley"}, {"url": "http://www.openai.com/", "anchor_text": "OpenAI"}, {"url": "http://joschu.net/index.html", "anchor_text": "John Schulman"}, {"url": "https://learning.mpi-sws.org/mlss2016/", "anchor_text": "Machine Learning Summer School"}, {"url": "https://en.wikipedia.org/wiki/Stuart_J._Russell", "anchor_text": "Stuart Rusell"}, {"url": "https://en.wikipedia.org/wiki/Peter_Norvig", "anchor_text": "Peter Norvig"}, {"url": "https://en.wikipedia.org/wiki/AI_winter", "anchor_text": "AI winter"}, {"url": "https://en.wikipedia.org/wiki/Dynamic_programming", "anchor_text": "Dynamic Programming"}, {"url": "https://gym.openai.com/envs/FrozenLake-v0/", "anchor_text": "Frozen-Lake"}, {"url": "https://gym.openai.com", "anchor_text": "Gym"}, {"url": "https://gym.openai.com", "anchor_text": "Gym"}, {"url": "https://gym.openai.com/envs/FrozenLake-v0/", "anchor_text": "Frozen-Lake"}, {"url": "https://colab.research.google.com/", "anchor_text": "Colab"}, {"url": "https://github.com/openai/gym", "anchor_text": "here"}, {"url": "http://gym.openai.com/docs/", "anchor_text": "here"}, {"url": "https://medium.com/@SmartLabAI/a-brief-overview-of-imitation-learning-8a8a75c44a9c", "anchor_text": "imitation learning"}, {"url": "https://towardsdatascience.com/drl-02-formalization-of-a-reinforcement-learning-problem-108b52ebfd9a", "anchor_text": "next post"}, {"url": "https://towardsdatascience.com/drl-02-formalization-of-a-reinforcement-learning-problem-108b52ebfd9a", "anchor_text": "next post"}, {"url": "https://www.upc.edu/en", "anchor_text": "UPC Barcelona Tech"}, {"url": "https://www.bsc.es/", "anchor_text": "Barcelona Supercomputing Center"}, {"url": "https://torres.ai/deep-reinforcement-learning-explained-series/", "anchor_text": "series"}, {"url": "https://torres.ai/deep-reinforcement-learning-explained-series/", "anchor_text": "Deep Reinforcement Learning Explained - Jordi TORRES.AIContent of this series"}, {"url": "https://twitter.com/hashtag/StayAtHome?src=hashtag_click", "anchor_text": "#StayAtHome"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----405b79866bf4---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----405b79866bf4---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----405b79866bf4---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/deep-r-l-explained?source=post_page-----405b79866bf4---------------deep_r_l_explained-----------------", "anchor_text": "Deep R L Explained"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----405b79866bf4---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F405b79866bf4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdrl-01-a-gentle-introduction-to-deep-reinforcement-learning-405b79866bf4&user=Jordi+TORRES.AI&userId=497013a3c715&source=-----405b79866bf4---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F405b79866bf4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdrl-01-a-gentle-introduction-to-deep-reinforcement-learning-405b79866bf4&user=Jordi+TORRES.AI&userId=497013a3c715&source=-----405b79866bf4---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F405b79866bf4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdrl-01-a-gentle-introduction-to-deep-reinforcement-learning-405b79866bf4&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://torres-ai.medium.com/?source=post_page-----405b79866bf4--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----405b79866bf4--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F497013a3c715&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdrl-01-a-gentle-introduction-to-deep-reinforcement-learning-405b79866bf4&user=Jordi+TORRES.AI&userId=497013a3c715&source=post_page-497013a3c715----405b79866bf4---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F9fb911e344f9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdrl-01-a-gentle-introduction-to-deep-reinforcement-learning-405b79866bf4&newsletterV3=497013a3c715&newsletterV3Id=9fb911e344f9&user=Jordi+TORRES.AI&userId=497013a3c715&source=-----405b79866bf4---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://torres-ai.medium.com/?source=post_page-----405b79866bf4--------------------------------", "anchor_text": "Written by Jordi TORRES.AI"}, {"url": "https://torres-ai.medium.com/followers?source=post_page-----405b79866bf4--------------------------------", "anchor_text": "2.1K Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----405b79866bf4--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://torres.ai", "anchor_text": "https://torres.ai"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F497013a3c715&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdrl-01-a-gentle-introduction-to-deep-reinforcement-learning-405b79866bf4&user=Jordi+TORRES.AI&userId=497013a3c715&source=post_page-497013a3c715----405b79866bf4---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F9fb911e344f9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdrl-01-a-gentle-introduction-to-deep-reinforcement-learning-405b79866bf4&newsletterV3=497013a3c715&newsletterV3Id=9fb911e344f9&user=Jordi+TORRES.AI&userId=497013a3c715&source=-----405b79866bf4---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-bellman-equation-59258a0d3fa7?source=author_recirc-----405b79866bf4----0---------------------52ccf7f1_d6bc_47fc_9cb9_8be6fd24cd59-------", "anchor_text": ""}, {"url": "https://torres-ai.medium.com/?source=author_recirc-----405b79866bf4----0---------------------52ccf7f1_d6bc_47fc_9cb9_8be6fd24cd59-------", "anchor_text": ""}, {"url": "https://torres-ai.medium.com/?source=author_recirc-----405b79866bf4----0---------------------52ccf7f1_d6bc_47fc_9cb9_8be6fd24cd59-------", "anchor_text": "Jordi TORRES.AI"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----405b79866bf4----0---------------------52ccf7f1_d6bc_47fc_9cb9_8be6fd24cd59-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-bellman-equation-59258a0d3fa7?source=author_recirc-----405b79866bf4----0---------------------52ccf7f1_d6bc_47fc_9cb9_8be6fd24cd59-------", "anchor_text": "The Bellman EquationV-function and Q-function Explained"}, {"url": "https://towardsdatascience.com/the-bellman-equation-59258a0d3fa7?source=author_recirc-----405b79866bf4----0---------------------52ccf7f1_d6bc_47fc_9cb9_8be6fd24cd59-------", "anchor_text": "\u00b712 min read\u00b7Jun 11, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F59258a0d3fa7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-bellman-equation-59258a0d3fa7&user=Jordi+TORRES.AI&userId=497013a3c715&source=-----59258a0d3fa7----0-----------------clap_footer----52ccf7f1_d6bc_47fc_9cb9_8be6fd24cd59-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-bellman-equation-59258a0d3fa7?source=author_recirc-----405b79866bf4----0---------------------52ccf7f1_d6bc_47fc_9cb9_8be6fd24cd59-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "5"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F59258a0d3fa7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-bellman-equation-59258a0d3fa7&source=-----405b79866bf4----0-----------------bookmark_preview----52ccf7f1_d6bc_47fc_9cb9_8be6fd24cd59-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----405b79866bf4----1---------------------52ccf7f1_d6bc_47fc_9cb9_8be6fd24cd59-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----405b79866bf4----1---------------------52ccf7f1_d6bc_47fc_9cb9_8be6fd24cd59-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----405b79866bf4----1---------------------52ccf7f1_d6bc_47fc_9cb9_8be6fd24cd59-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----405b79866bf4----1---------------------52ccf7f1_d6bc_47fc_9cb9_8be6fd24cd59-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----405b79866bf4----1---------------------52ccf7f1_d6bc_47fc_9cb9_8be6fd24cd59-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----405b79866bf4----1---------------------52ccf7f1_d6bc_47fc_9cb9_8be6fd24cd59-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----52ccf7f1_d6bc_47fc_9cb9_8be6fd24cd59-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----405b79866bf4----1---------------------52ccf7f1_d6bc_47fc_9cb9_8be6fd24cd59-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----405b79866bf4----1-----------------bookmark_preview----52ccf7f1_d6bc_47fc_9cb9_8be6fd24cd59-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----405b79866bf4----2---------------------52ccf7f1_d6bc_47fc_9cb9_8be6fd24cd59-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----405b79866bf4----2---------------------52ccf7f1_d6bc_47fc_9cb9_8be6fd24cd59-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----405b79866bf4----2---------------------52ccf7f1_d6bc_47fc_9cb9_8be6fd24cd59-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----405b79866bf4----2---------------------52ccf7f1_d6bc_47fc_9cb9_8be6fd24cd59-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----405b79866bf4----2---------------------52ccf7f1_d6bc_47fc_9cb9_8be6fd24cd59-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----405b79866bf4----2---------------------52ccf7f1_d6bc_47fc_9cb9_8be6fd24cd59-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----2-----------------clap_footer----52ccf7f1_d6bc_47fc_9cb9_8be6fd24cd59-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----405b79866bf4----2---------------------52ccf7f1_d6bc_47fc_9cb9_8be6fd24cd59-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----405b79866bf4----2-----------------bookmark_preview----52ccf7f1_d6bc_47fc_9cb9_8be6fd24cd59-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/deep-q-network-dqn-ii-b6bf911b6b2c?source=author_recirc-----405b79866bf4----3---------------------52ccf7f1_d6bc_47fc_9cb9_8be6fd24cd59-------", "anchor_text": ""}, {"url": "https://torres-ai.medium.com/?source=author_recirc-----405b79866bf4----3---------------------52ccf7f1_d6bc_47fc_9cb9_8be6fd24cd59-------", "anchor_text": ""}, {"url": "https://torres-ai.medium.com/?source=author_recirc-----405b79866bf4----3---------------------52ccf7f1_d6bc_47fc_9cb9_8be6fd24cd59-------", "anchor_text": "Jordi TORRES.AI"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----405b79866bf4----3---------------------52ccf7f1_d6bc_47fc_9cb9_8be6fd24cd59-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/deep-q-network-dqn-ii-b6bf911b6b2c?source=author_recirc-----405b79866bf4----3---------------------52ccf7f1_d6bc_47fc_9cb9_8be6fd24cd59-------", "anchor_text": "Deep Q-Network (DQN)-IIExperience Replay and Target Networks"}, {"url": "https://towardsdatascience.com/deep-q-network-dqn-ii-b6bf911b6b2c?source=author_recirc-----405b79866bf4----3---------------------52ccf7f1_d6bc_47fc_9cb9_8be6fd24cd59-------", "anchor_text": "\u00b714 min read\u00b7Aug 15, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb6bf911b6b2c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-q-network-dqn-ii-b6bf911b6b2c&user=Jordi+TORRES.AI&userId=497013a3c715&source=-----b6bf911b6b2c----3-----------------clap_footer----52ccf7f1_d6bc_47fc_9cb9_8be6fd24cd59-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/deep-q-network-dqn-ii-b6bf911b6b2c?source=author_recirc-----405b79866bf4----3---------------------52ccf7f1_d6bc_47fc_9cb9_8be6fd24cd59-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb6bf911b6b2c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-q-network-dqn-ii-b6bf911b6b2c&source=-----405b79866bf4----3-----------------bookmark_preview----52ccf7f1_d6bc_47fc_9cb9_8be6fd24cd59-------", "anchor_text": ""}, {"url": "https://torres-ai.medium.com/?source=post_page-----405b79866bf4--------------------------------", "anchor_text": "See all from Jordi TORRES.AI"}, {"url": "https://towardsdatascience.com/?source=post_page-----405b79866bf4--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----405b79866bf4----0---------------------ec19d1a1_4921_482f_837d_ad1fff56c102-------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=read_next_recirc-----405b79866bf4----0---------------------ec19d1a1_4921_482f_837d_ad1fff56c102-------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=read_next_recirc-----405b79866bf4----0---------------------ec19d1a1_4921_482f_837d_ad1fff56c102-------", "anchor_text": "Wouter van Heeswijk, PhD"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----405b79866bf4----0---------------------ec19d1a1_4921_482f_837d_ad1fff56c102-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----405b79866bf4----0---------------------ec19d1a1_4921_482f_837d_ad1fff56c102-------", "anchor_text": "Proximal Policy Optimization (PPO) ExplainedThe journey from REINFORCE to the go-to algorithm in continuous control"}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----405b79866bf4----0---------------------ec19d1a1_4921_482f_837d_ad1fff56c102-------", "anchor_text": "\u00b713 min read\u00b7Nov 29, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fabed1952457b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fproximal-policy-optimization-ppo-explained-abed1952457b&user=Wouter+van+Heeswijk%2C+PhD&userId=33f45c9ab481&source=-----abed1952457b----0-----------------clap_footer----ec19d1a1_4921_482f_837d_ad1fff56c102-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----405b79866bf4----0---------------------ec19d1a1_4921_482f_837d_ad1fff56c102-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fabed1952457b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fproximal-policy-optimization-ppo-explained-abed1952457b&source=-----405b79866bf4----0-----------------bookmark_preview----ec19d1a1_4921_482f_837d_ad1fff56c102-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----405b79866bf4----1---------------------ec19d1a1_4921_482f_837d_ad1fff56c102-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----405b79866bf4----1---------------------ec19d1a1_4921_482f_837d_ad1fff56c102-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----405b79866bf4----1---------------------ec19d1a1_4921_482f_837d_ad1fff56c102-------", "anchor_text": "The PyCoach"}, {"url": "https://artificialcorner.com/?source=read_next_recirc-----405b79866bf4----1---------------------ec19d1a1_4921_482f_837d_ad1fff56c102-------", "anchor_text": "Artificial Corner"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----405b79866bf4----1---------------------ec19d1a1_4921_482f_837d_ad1fff56c102-------", "anchor_text": "You\u2019re Using ChatGPT Wrong! Here\u2019s How to Be Ahead of 99% of ChatGPT UsersMaster ChatGPT by learning prompt engineering."}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----405b79866bf4----1---------------------ec19d1a1_4921_482f_837d_ad1fff56c102-------", "anchor_text": "\u00b77 min read\u00b7Mar 17"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fartificial-corner%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&user=The+PyCoach&userId=fb44e21903f3&source=-----886a50dabc54----1-----------------clap_footer----ec19d1a1_4921_482f_837d_ad1fff56c102-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----405b79866bf4----1---------------------ec19d1a1_4921_482f_837d_ad1fff56c102-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "276"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&source=-----405b79866bf4----1-----------------bookmark_preview----ec19d1a1_4921_482f_837d_ad1fff56c102-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----405b79866bf4----0---------------------ec19d1a1_4921_482f_837d_ad1fff56c102-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData?source=read_next_recirc-----405b79866bf4----0---------------------ec19d1a1_4921_482f_837d_ad1fff56c102-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData?source=read_next_recirc-----405b79866bf4----0---------------------ec19d1a1_4921_482f_837d_ad1fff56c102-------", "anchor_text": "Andrew Austin"}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----405b79866bf4----0---------------------ec19d1a1_4921_482f_837d_ad1fff56c102-------", "anchor_text": "AI Anyone Can Understand Part 1: Reinforcement LearningReinforcement learning is a way for machines to learn by trying different things and seeing what works best. For example, a robot could\u2026"}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----405b79866bf4----0---------------------ec19d1a1_4921_482f_837d_ad1fff56c102-------", "anchor_text": "\u00b74 min read\u00b7Dec 11, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F6c3b3d623a2d&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40MoneyAndData%2Fai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d&user=Andrew+Austin&userId=42d388912d13&source=-----6c3b3d623a2d----0-----------------clap_footer----ec19d1a1_4921_482f_837d_ad1fff56c102-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----405b79866bf4----0---------------------ec19d1a1_4921_482f_837d_ad1fff56c102-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6c3b3d623a2d&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40MoneyAndData%2Fai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d&source=-----405b79866bf4----0-----------------bookmark_preview----ec19d1a1_4921_482f_837d_ad1fff56c102-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----405b79866bf4----1---------------------ec19d1a1_4921_482f_837d_ad1fff56c102-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----405b79866bf4----1---------------------ec19d1a1_4921_482f_837d_ad1fff56c102-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----405b79866bf4----1---------------------ec19d1a1_4921_482f_837d_ad1fff56c102-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----405b79866bf4----1---------------------ec19d1a1_4921_482f_837d_ad1fff56c102-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----405b79866bf4----1---------------------ec19d1a1_4921_482f_837d_ad1fff56c102-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----405b79866bf4----1---------------------ec19d1a1_4921_482f_837d_ad1fff56c102-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----1-----------------clap_footer----ec19d1a1_4921_482f_837d_ad1fff56c102-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----405b79866bf4----1---------------------ec19d1a1_4921_482f_837d_ad1fff56c102-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----405b79866bf4----1-----------------bookmark_preview----ec19d1a1_4921_482f_837d_ad1fff56c102-------", "anchor_text": ""}, {"url": "https://medium.datadriveninvestor.com/feature-importance-with-deep-neural-network-for-cryptocurrencies-f06191e2d562?source=read_next_recirc-----405b79866bf4----2---------------------ec19d1a1_4921_482f_837d_ad1fff56c102-------", "anchor_text": ""}, {"url": "https://byfintech.medium.com/?source=read_next_recirc-----405b79866bf4----2---------------------ec19d1a1_4921_482f_837d_ad1fff56c102-------", "anchor_text": ""}, {"url": "https://byfintech.medium.com/?source=read_next_recirc-----405b79866bf4----2---------------------ec19d1a1_4921_482f_837d_ad1fff56c102-------", "anchor_text": "Bruce Yang ByFinTech"}, {"url": "https://medium.datadriveninvestor.com/?source=read_next_recirc-----405b79866bf4----2---------------------ec19d1a1_4921_482f_837d_ad1fff56c102-------", "anchor_text": "DataDrivenInvestor"}, {"url": "https://medium.datadriveninvestor.com/feature-importance-with-deep-neural-network-for-cryptocurrencies-f06191e2d562?source=read_next_recirc-----405b79866bf4----2---------------------ec19d1a1_4921_482f_837d_ad1fff56c102-------", "anchor_text": "Feature Importance with Deep Neural Network for CryptocurrenciesA FinRL-Meta Tutorial for NeurIPS 2022 Datasets and Benchmarks"}, {"url": "https://medium.datadriveninvestor.com/feature-importance-with-deep-neural-network-for-cryptocurrencies-f06191e2d562?source=read_next_recirc-----405b79866bf4----2---------------------ec19d1a1_4921_482f_837d_ad1fff56c102-------", "anchor_text": "\u00b710 min read\u00b7Jan 2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fdatadriveninvestor%2Ff06191e2d562&operation=register&redirect=https%3A%2F%2Fmedium.datadriveninvestor.com%2Ffeature-importance-with-deep-neural-network-for-cryptocurrencies-f06191e2d562&user=Bruce+Yang+ByFinTech&userId=a878fc45fb3f&source=-----f06191e2d562----2-----------------clap_footer----ec19d1a1_4921_482f_837d_ad1fff56c102-------", "anchor_text": ""}, {"url": "https://medium.datadriveninvestor.com/feature-importance-with-deep-neural-network-for-cryptocurrencies-f06191e2d562?source=read_next_recirc-----405b79866bf4----2---------------------ec19d1a1_4921_482f_837d_ad1fff56c102-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff06191e2d562&operation=register&redirect=https%3A%2F%2Fmedium.datadriveninvestor.com%2Ffeature-importance-with-deep-neural-network-for-cryptocurrencies-f06191e2d562&source=-----405b79866bf4----2-----------------bookmark_preview----ec19d1a1_4921_482f_837d_ad1fff56c102-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/rainbow-dqn-the-best-reinforcement-learning-has-to-offer-166cb8ed2f86?source=read_next_recirc-----405b79866bf4----3---------------------ec19d1a1_4921_482f_837d_ad1fff56c102-------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=read_next_recirc-----405b79866bf4----3---------------------ec19d1a1_4921_482f_837d_ad1fff56c102-------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=read_next_recirc-----405b79866bf4----3---------------------ec19d1a1_4921_482f_837d_ad1fff56c102-------", "anchor_text": "Wouter van Heeswijk, PhD"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----405b79866bf4----3---------------------ec19d1a1_4921_482f_837d_ad1fff56c102-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/rainbow-dqn-the-best-reinforcement-learning-has-to-offer-166cb8ed2f86?source=read_next_recirc-----405b79866bf4----3---------------------ec19d1a1_4921_482f_837d_ad1fff56c102-------", "anchor_text": "Rainbow DQN \u2014 The Best Reinforcement Learning Has to Offer?What happens if the most successful techniques in Deep Q-Learning are combined into a single algorithm?"}, {"url": "https://towardsdatascience.com/rainbow-dqn-the-best-reinforcement-learning-has-to-offer-166cb8ed2f86?source=read_next_recirc-----405b79866bf4----3---------------------ec19d1a1_4921_482f_837d_ad1fff56c102-------", "anchor_text": "\u00b711 min read\u00b7Dec 8, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F166cb8ed2f86&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frainbow-dqn-the-best-reinforcement-learning-has-to-offer-166cb8ed2f86&user=Wouter+van+Heeswijk%2C+PhD&userId=33f45c9ab481&source=-----166cb8ed2f86----3-----------------clap_footer----ec19d1a1_4921_482f_837d_ad1fff56c102-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/rainbow-dqn-the-best-reinforcement-learning-has-to-offer-166cb8ed2f86?source=read_next_recirc-----405b79866bf4----3---------------------ec19d1a1_4921_482f_837d_ad1fff56c102-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F166cb8ed2f86&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frainbow-dqn-the-best-reinforcement-learning-has-to-offer-166cb8ed2f86&source=-----405b79866bf4----3-----------------bookmark_preview----ec19d1a1_4921_482f_837d_ad1fff56c102-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----405b79866bf4--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----405b79866bf4--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----405b79866bf4--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----405b79866bf4--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----405b79866bf4--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----405b79866bf4--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----405b79866bf4--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----405b79866bf4--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----405b79866bf4--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----405b79866bf4--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}