{"url": "https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a", "time": 1683001422.47735, "path": "towardsdatascience.com/illustrated-self-attention-2d627e33b20a/", "webpage": {"metadata": {"title": "Illustrated: Self-Attention. A step-by-step guide to self-attention\u2026 | by Raimi Karim | Towards Data Science", "h1": "Illustrated: Self-Attention", "description": "This article walks you through the mathematical operations in a self-attention module. Includes illustrations and code."}, "outgoing_paragraph_urls": [{"url": "https://colab.research.google.com/drive/1rPk3ohrmVclqhH7uQ7qys4oznDdAhpzF", "anchor_text": "here", "paragraph_index": 0}, {"url": "https://medium.com/u/3f2bb9b4510b?source=post_page-----2d627e33b20a--------------------------------", "anchor_text": "Manuel Romero", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3#ba24", "anchor_text": "score functions", "paragraph_index": 26}, {"url": "https://en.wikipedia.org/wiki/Softmax_function", "anchor_text": "softmax", "paragraph_index": 27}, {"url": "https://pytorch.org/", "anchor_text": "PyTorch", "paragraph_index": 37}, {"url": "https://pytorch.org/docs/master/nn.html#multiheadattention", "anchor_text": "nn.MultiheadAttention", "paragraph_index": 40}, {"url": "https://en.wikipedia.org/wiki/Neural_machine_translation", "anchor_text": "neural machine translation", "paragraph_index": 41}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "Attention Is All You Need", "paragraph_index": 44}, {"url": "https://medium.com/@remykarem/membership", "anchor_text": "here", "paragraph_index": 45}, {"url": "https://twitter.com/remykarem", "anchor_text": "@remykarem", "paragraph_index": 47}, {"url": "http://remykarem.medium.com/membership", "anchor_text": "remykarem.medium.com/membership", "paragraph_index": 49}], "all_paragraphs": ["The illustrations are best viewed on the Desktop. A Colab version can be found here (thanks to Manuel Romero!).", "Changelog:30 Dec 2022 \u2014 Use Medium\u2019s new code block for syntax highlighting12 Jan 2022 \u2014 Improve clarity5 Jan 2022 \u2014 Fix typos and improve clarity", "What do BERT, RoBERTa, ALBERT, SpanBERT, DistilBERT, SesameBERT, SemBERT, SciBERT, BioBERT, MobileBERT, TinyBERT and CamemBERT all have in common? And I\u2019m not looking for the answer \u201cBERT\u201d \ud83e\udd2d.", "Answer: self-attention \ud83e\udd17. We are not only talking about architectures bearing the name \u201cBERT\u2019 but, more correctly, Transformer-based architectures. Transformer-based architectures, which are primarily used in modelling language understanding tasks, eschew recurrence in neural networks and instead trust entirely on self-attention mechanisms to draw global dependencies between inputs and outputs. But what\u2019s the math behind this?", "That\u2019s what we\u2019re going to find out today. The main content of this post is to walk you through the mathematical operations involved in a self-attention module. By the end of this article, you should be able to write or code a self-attention module from scratch.", "This article does not aim to provide the intuitions and explanations behind the different numerical representations and mathematical operations in the self-attention module. It also does not seek to demonstrate the why\u2019s and how-exactly\u2019s of self-attention in Transformers (I believe there\u2019s a lot out there already). Note that the difference between attention and self-attention is also not detailed in this article.", "Now let\u2019s get on to it!", "If you think that self-attention is similar, the answer is yes! They fundamentally share the same concept and many common mathematical operations.", "A self-attention module takes in n inputs and returns n outputs. What happens in this module? In layman\u2019s terms, the self-attention mechanism allows the inputs to interact with each other (\u201cself\u201d) and find out who they should pay more attention to (\u201cattention\u201d). The outputs are aggregates of these interactions and attention scores.", "The illustrations are divided into the following steps:", "NoteIn practice, the mathematical operations are vectorised, i.e. all the inputs undergo the mathematical operations together. We\u2019ll see this later in the Code section.", "We start with 3 inputs for this tutorial, each with dimension 4.", "Every input must have three representations (see diagram below). These representations are called key (orange), query (red), and value (purple). For this example, let\u2019s take that we want these representations to have a dimension of 3. Because every input has a dimension of 4, each set of the weights must have a shape of 4\u00d73.", "NoteWe\u2019ll see later that the dimension of value is also the output dimension.", "To obtain these representations, every input (green) is multiplied with a set of weights for keys, a set of weights for querys (I know that\u2019s not the correct spelling), and a set of weights for values. In our example, we initialise the three sets of weights as follows.", "NotesIn a neural network setting, these weights are usually small numbers, initialised randomly using an appropriate random distribution like Gaussian, Xavier and Kaiming distributions. This initialisation is done once before training.", "Step 3: Derive key, query and value", "Now that we have the three sets of weights, let\u2019s obtain the key, query and value representations for every input.", "Use the same set of weights to get the key representation for Input 2:", "Use the same set of weights to get the key representation for Input 3:", "A faster way is to vectorise the above operations:", "Let\u2019s do the same to obtain the value representations for every input:", "NotesIn practice, a bias vector may be added to the product of matrix multiplication.", "Step 4: Calculate attention scores for Input 1", "To obtain attention scores, we start with taking a dot product between Input 1\u2019s query (red) with all keys (orange), including itself. Since there are 3 key representations (because we have 3 inputs), we obtain 3 attention scores (blue).", "Notice that we only use the query from Input 1. Later we\u2019ll work on repeating this same step for the other querys.", "NoteThe above operation is known as dot product attention, one of the several score functions. Other score functions include scaled dot product and additive/concat.", "Take the softmax across these attention scores (blue).", "Note that we round off to 1 decimal place here for readability.", "Step 6: Multiply scores with values", "The softmaxed attention scores for each input (blue) is multiplied by its corresponding value (purple). This results in 3 alignment vectors (yellow). In this tutorial, we\u2019ll refer to them as weighted values.", "Step 7: Sum weighted values to get Output 1", "Take all the weighted values (yellow) and sum them element-wise:", "The resulting vector [2.0, 7.0, 1.5] (dark green) is Output 1, which is based on the query representation from Input 1 interacting with all other keys, including itself.", "Step 8: Repeat for Input 2 & Input 3", "Now that we\u2019re done with Output 1, we repeat Steps 4 to 7 for Output 2 and Output 3. I trust that I can leave you to work out the operations yourself \ud83d\udc4d\ud83c\udffc.", "NotesThe dimension of query and key must always be the same because of the dot product score function. However, the dimension of value may be different from query and key. The resulting output will consequently follow the dimension of value.", "Here is the code in PyTorch \ud83e\udd17, a popular deep learning framework in Python. To enjoy the APIs for @ operator, .T and None indexing in the following code snippets, make sure you\u2019re on Python\u22653.6 and PyTorch 1.3.1. Just follow along and copy-paste these in a Python/IPython REPL or Jupyter Notebook.", "Step 3: Derive key, query and value", "Step 6: Multiply scores with values", "NotePyTorch has provided an API for this called nn.MultiheadAttention. However, this API requires that you feed in key, query and value PyTorch tensors. Moreover, the outputs of this module undergo a linear transformation.", "So, where do we go from here? Transformers! Indeed we live in exciting times of deep learning research and high compute resources. The transformer is the incarnation from Attention Is All You Need, originally born to perform neural machine translation. Researchers picked up from here, reassembling, cutting, adding and extending the parts, and extending its usage to more language tasks.", "Here I will briefly mention how we can extend self-attention to a Transformer architecture.", "That\u2019s all folks! Hope you find the content easy to digest. Is there something that you think I should add or elaborate on further in this article? Do drop a comment! Also, do check out an illustration I created for attention below", "Attention Is All You Need (arxiv.org)", "If you like my content and haven\u2019t already subscribed to Medium, subscribe via my referral link here! NOTE: A portion of your membership fees will be apportioned to me as referral fees.", "Special thanks to Xin Jie, Serene, Ren Jie, Kevin and Wei Yih for ideas, suggestions and corrections to this article.", "Follow me on Twitter @remykarem for digested articles and other tweets on AI, ML, Deep Learning and Python.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "\ud83c\uddf8\ud83c\uddec Software Engineer at GovTech \u2022 Master of Computing AI at NUS \u2022 Subscribe at remykarem.medium.com/membership"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F2d627e33b20a&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fillustrated-self-attention-2d627e33b20a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fillustrated-self-attention-2d627e33b20a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fillustrated-self-attention-2d627e33b20a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fillustrated-self-attention-2d627e33b20a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----2d627e33b20a--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2d627e33b20a--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://remykarem.medium.com/?source=post_page-----2d627e33b20a--------------------------------", "anchor_text": ""}, {"url": "https://remykarem.medium.com/?source=post_page-----2d627e33b20a--------------------------------", "anchor_text": "Raimi Karim"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc2958659896a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fillustrated-self-attention-2d627e33b20a&user=Raimi+Karim&userId=c2958659896a&source=post_page-c2958659896a----2d627e33b20a---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2d627e33b20a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fillustrated-self-attention-2d627e33b20a&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2d627e33b20a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fillustrated-self-attention-2d627e33b20a&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://colab.research.google.com/drive/1rPk3ohrmVclqhH7uQ7qys4oznDdAhpzF", "anchor_text": "here"}, {"url": "https://medium.com/u/3f2bb9b4510b?source=post_page-----2d627e33b20a--------------------------------", "anchor_text": "Manuel Romero"}, {"url": "https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3#ba24", "anchor_text": "score functions"}, {"url": "https://en.wikipedia.org/wiki/Softmax_function", "anchor_text": "softmax"}, {"url": "https://pytorch.org/", "anchor_text": "PyTorch"}, {"url": "https://pytorch.org/docs/master/nn.html#multiheadattention", "anchor_text": "nn.MultiheadAttention"}, {"url": "https://en.wikipedia.org/wiki/Neural_machine_translation", "anchor_text": "neural machine translation"}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "Attention Is All You Need"}, {"url": "https://jalammar.github.io/illustrated-transformer/", "anchor_text": "The Illustrated Transformer"}, {"url": "https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3", "anchor_text": "Attn: Illustrated Attention"}, {"url": "https://medium.com/@remykarem/membership", "anchor_text": "here"}, {"url": "https://twitter.com/remykarem", "anchor_text": "@remykarem"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----2d627e33b20a---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----2d627e33b20a---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/tag/transformers?source=post_page-----2d627e33b20a---------------transformers-----------------", "anchor_text": "Transformers"}, {"url": "https://medium.com/tag/attention?source=post_page-----2d627e33b20a---------------attention-----------------", "anchor_text": "Attention"}, {"url": "https://medium.com/tag/nlp?source=post_page-----2d627e33b20a---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2d627e33b20a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fillustrated-self-attention-2d627e33b20a&user=Raimi+Karim&userId=c2958659896a&source=-----2d627e33b20a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2d627e33b20a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fillustrated-self-attention-2d627e33b20a&user=Raimi+Karim&userId=c2958659896a&source=-----2d627e33b20a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2d627e33b20a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fillustrated-self-attention-2d627e33b20a&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2d627e33b20a--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F2d627e33b20a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fillustrated-self-attention-2d627e33b20a&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----2d627e33b20a---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----2d627e33b20a--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----2d627e33b20a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----2d627e33b20a--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----2d627e33b20a--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----2d627e33b20a--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----2d627e33b20a--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----2d627e33b20a--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----2d627e33b20a--------------------------------", "anchor_text": ""}, {"url": "https://remykarem.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://remykarem.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Raimi Karim"}, {"url": "https://remykarem.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "2.2K Followers"}, {"url": "http://remykarem.medium.com/membership", "anchor_text": "remykarem.medium.com/membership"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc2958659896a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fillustrated-self-attention-2d627e33b20a&user=Raimi+Karim&userId=c2958659896a&source=post_page-c2958659896a--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F307a18475417&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fillustrated-self-attention-2d627e33b20a&newsletterV3=c2958659896a&newsletterV3Id=307a18475417&user=Raimi+Karim&userId=c2958659896a&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}