{"url": "https://towardsdatascience.com/dimension-reduction-techniques-with-python-f36ca7009e5c", "time": 1682996776.323926, "path": "towardsdatascience.com/dimension-reduction-techniques-with-python-f36ca7009e5c/", "webpage": {"metadata": {"title": "Dimension Reduction Techniques with Python | by Chris Kuo/Dr. Dataman | Towards Data Science", "h1": "Dimension Reduction Techniques with Python", "description": "A high-dimensional dataset is a dataset that has a great number of columns (or variables). Such a dataset presents many mathematical or computational challenges. The good news is that variables (or\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/anomaly-detection-with-autoencoder-b4cdce4866a6", "anchor_text": "Anomaly Detection with Autoencoders Made Easy", "paragraph_index": 4}, {"url": "https://medium.com/@Dataman.ai/dataman-learning-paths-build-your-skills-drive-your-career-e1aee030ff6e", "anchor_text": "Dataman Learning Paths \u2014 Build Your Skills, Drive Your Career", "paragraph_index": 5}, {"url": "https://medium.com/dataman-in-ai/a-wide-choice-for-modeling-multi-class-classifications-d97073ff4ec8", "anchor_text": "A Wide Variety of models for Multi-class Classification", "paragraph_index": 6}, {"url": "https://towardsdatascience.com/avoid-these-deadly-modeling-mistakes-that-may-cost-you-a-career-b9b686d89f2c", "anchor_text": "Avoid These Deadly Modeling Mistakes that May Cost You a Career", "paragraph_index": 11}, {"url": "https://towardsdatascience.com/anomaly-detection-with-pyod-b523fc47db9", "anchor_text": "Anomaly Detection with PyOD", "paragraph_index": 17}, {"url": "https://en.wikipedia.org/wiki/Radial_basis_function", "anchor_text": "Radial Basis Function", "paragraph_index": 21}, {"url": "https://en.wikipedia.org/wiki/Support-vector_machine", "anchor_text": "Support Vector Machine (SVM)", "paragraph_index": 21}, {"url": "https://www.kaggle.com/piyushgoyal443/red-wine-dataset#wineQualityInfo.txt", "anchor_text": "Red Wine Quality", "paragraph_index": 26}, {"url": "https://medium.com/dataman-in-ai/a-wide-choice-for-modeling-multi-class-classifications-d97073ff4ec8", "anchor_text": "A Wide Variety of models for Multi-class Classification", "paragraph_index": 29}, {"url": "https://yutsumura.com/how-to-diagonalize-a-matrix-step-by-step-explanation/", "anchor_text": "here", "paragraph_index": 32}, {"url": "http://www.cs.toronto.edu/~hinton/absps/tsne.pdf", "anchor_text": "Laurens van der Maaten and Geoggrey Hinton", "paragraph_index": 37}, {"url": "https://distill.pub/2016/misread-tsne/", "anchor_text": "how to use t-SNE more effectively", "paragraph_index": 44}, {"url": "https://sps.columbia.edu/faculty/chris-kuo", "anchor_text": "https://sps.columbia.edu/faculty/chris-kuo", "paragraph_index": 46}], "all_paragraphs": ["Why Do We Need to Reduce Dimensionality?", "A high-dimensional dataset is a dataset that has a great number of columns (or variables). Such a dataset presents many mathematical or computational challenges. The good news is that variables (or called features) are often correlated \u2014 high-dimensional data are dominated \u201csuperficially\u201d by a small number of simple variables. We can find a subset of the variables to represent the same level of information in the data or transform the variables into a new set of variables without losing much information. Although high-power computing can somehow handle high-dimensional data, in many applications it is still necessary to reduce the dimensionality of the original data.", "Principal Component Analysis (PCA) is probably the most popular technique when we think of dimension reduction. In this article, I will start with PCA, then go on to introduce other dimension-reduction techniques. Python code will be included in each technique.", "Dimensionality Reduction Can Also Find Outliers", "Data scientists can use dimension-reduction techniques to identify anomalies. Why? Don\u2019t we just want to reduce the dimensionality? The intuition lies in outliers themselves. D.M.Hawkins said:\u201d An outlier is an observation which deviates so much from the other observations as to arouse suspicions that it was generated by a different mechanism.\u201d Once dimensions are reduced to fewer principal dimensions, patterns are identified and then the outliers are revealed. We can say outlier detection is a by-product of dimension reduction, as described in the article \u201cAnomaly Detection with Autoencoders Made Easy\u201d.", "I have written articles on a variety of data science topics. For ease of use, you can bookmark my summary post \u201cDataman Learning Paths \u2014 Build Your Skills, Drive Your Career\u201d which lists the links to all articles.", "It is worth mentioning that in real life many classification problems are multi-class. If you ever face the need to model a multi-class problem, see my post \u201cA Wide Variety of models for Multi-class Classification\u201d.", "The idea of principal component analysis (PCA) is to reduce the dimensionality of a dataset consisting of a large number of related variables while retaining as much variance in the data as possible. PCA finds a set of new variables that the original variables are just their linear combinations. The new variables are called Principal Components (PCs). These principal components are orthogonal: In a 3-D case, the principal components are perpendicular to each other. X can not be represented by Y or Y cannot be presented by Z.", "Figure (A) shows the intuition of PCA: it \u201crotates\u201d the axes to line up better with your data. The first principal component will capture most of the variance in the data, then followed by the second, third, and so on. As a result, the new data will have fewer dimensions.", "Let\u2019s use the iris dataset to illustrate PCA:", "Notice this IRIS dataset comes with the target variable. In PCA, you only transform the X variables without the target Y variable.", "Standardization: All the variables should be on the same scale before applying PCA, otherwise, a feature with large values will dominate the result. This point is further explained in my post \u201cAvoid These Deadly Modeling Mistakes that May Cost You a Career\u201d. Below I use StandardScaler in scikit-learn to standardize the dataset\u2019s features onto the unit scale (mean = 0 and variance = 1).", "There are four features in the original data. So PCA will provide the same number of principal components.", "What are the variances explained by each of the principal components? Use pca.explained_variance_ratio_ to return a vector of the variance:", "It shows the first principal component accounts for 72.22% variance, and the second, third and fourth account for 23.9%, 3.68%, and 0.51% variance respectively. We can say 72.22 + 23.9 = 96.21% of the information is captured by the first and second principal components. We often want to keep only the significant features and drop the insignificant ones. A rule of thumb is to keep the top principal components that capture significant variance and ignore the small ones.", "We can plot the results using the first two components. Let\u2019s append the target variable y to the new data x_pca:", "The outcome shows the data are separable in the new space.", "How do we use PCA to detect outliers? Let me give you the intuition. After the transformation, the \u201cnormal\u201d data points will align along the eigenvectors (new axes) with small eigenvalues. The outliers are far away from the eigenvectors with large eigenvalues. Therefore the distances between each data point to the eigenvectors become a measure for the outlier. A large distance indicates an anomaly. For more information, see \u201cAnomaly Detection with PyOD\u201d.", "PCA applies linear transformation, which is just its limitation. Kernel PCA extends PCA to non-linearity. It first maps the original data to some nonlinear feature space (usually a higher dimension), then applies PCA to extract the principal components in that space. This can be understood in Figure (B). The graph on the left shows the blue and red dots can not be separated using any linear transformation. But if all the dots are projected onto a 3D space, the result becomes linearly separable! We then apply PCA to separate the components.", "Where does intuition come from? Why does component separation become easier in a higher-dimensional space? This has to go back to the Vapnik-Chervonenkis (VC) theory. It says mapping into a higher dimensional space often provides greater classification power.", "The following Python code makes a circle plot consisting of red and blue dots. There is no way to separate the red and blue dots with a line (linear separation).", "However, when we project the circle to a higher dimensional space and separate using PCA, the data observations against the first and second principal components are separable! Below is the result that the dots are plotted against the first and second principal components. I draw a line to separate the red and blue dots. In KernelPCA we specify kernel=\u2019rbf\u2019, which is the Radial Basis Function, or the Euclidean distance. The RBFs are commonly used as a kernel in machine learning techniques such as the Support Vector Machine (SVM).", "If we specify the kernel to be \u201clinear\u201d as the code below (KernelPCA(kernel=\u2019linear\u2019), it becomes the standard PCA with only linear transformation, and the red and blue dots are not separable.", "The origin of LDA is different from PCA. PCA is an unsupervised learning method that transforms the original features into a set of new features. We do not care about whether the new set of features can provide the best discriminatory power for the target variable. In contrast, Linear Discriminant Analysis (LDA) seeks to preserve as much discriminatory power as possible for the dependent variable, while projecting the original data matrix onto a lower-dimensional space. LDA is a type of supervised learning technique. It utilizes the classes in the dependent variable to divide the space of predictors into regions. All the regions should have linear boundaries. Thus the name linear comes from. The model predicts that all observations within a region belong to the same class of the dependent variable.", "LDA achieves the above goal in three major steps. First, it calculates the separability between different classes of the dependent variable, which is called the between-class variance, as shown in (1) of Figure LDA. Second, it calculates the distance between the mean and the samples of each class, which is called the within-class variance, as shown in (2). Then it constructs the lower-dimensional space with this criterion: maximizing the between-class variance and minimizing the within-class variance. The solution to this criterion is to compute the eigenvalues and eigenvectors. The resulting eigenvectors represent the directions of the new space, and the corresponding eigenvalues represent the length of the eigenvectors. Thus, each eigenvector represents one axis of the LDA space, and the eigenvalue represents the length of that eigenvector.", "The intuition is like the following: If the two classes can be truly separated, it will be ideal that the two classes are as far as possible, and at the same time each class is as homogeneous as possible. This intuition can be formulated in three steps:", "I will use the \u201cRed Wine Quality\u201d dataset in the Kaggle competition. This dataset has 11 input variables and one output variable \u2018quality\u2019.", "The following code performs PCA and LDA.", "Then plot both the results of PCA and LDA:", "LDA is suitable for a multi-class classification problem. If you even face the need to model a multi-class problem, see my post \u201cA Wide Variety of models for Multi-class Classification\u201d.", "SVD is a data summary method similar to PCA. It extracts important features from data. But there is one more advantage of SVD: reconstructing the original dataset into a small dataset. The SVD has wide applications in image compression. For example, if you have a 32*32 = 1,024-pixel image, the SVD can summarize it into 66 pixels. The 66 pixels can represent the 32*32 pixel images without missing any important information. The SVD is fundamental in linear algebra, but it seems \u201cnot nearly as famous as it should be\u201d. This great comment is in the classic textbook \u201cLinear Algebra and Its Applications\u201d by Gilbert Strang.", "To introduce the SVD properly, let\u2019s start with matrix operation. If A is a symmetric real n \u00d7 n matrix, there exists an orthogonal matrix V and a diagonal D such that", "Columns V are eigenvectors for A and the diagonal entries of D are the eigenvalues of A. This process is called the Eigenvalue Decomposition, or EVD, for matrix A. It tells us how to choose orthonormal bases so that the transformation is represented by a matrix with the simplest possible form, that is, diagonal. (For readers who would like to go over the steps to diagonalize a matrix, here is a good example.) The term orthonormal means two vectors are perpendicular.", "Extending the symmetric matrix, the SVD works with any real m \u00d7 n matrix A. Given a real m\u00d7 n matrix A, there exists an orthogonal m \u00d7 m matrix U, an orthogonal matrix m \u00d7 m V, and a diagonal m \u00d7 n matrix \u03a3 such that", "Note that an orthogonal matrix is a square matrix such that the product of itself and its inverse matrix is an identity matrix. A diagonal matrix is a matrix in which the entries other than the diagonal are all zero.", "Below I will again use the iris dataset to show you how to apply the SVD.", "You can compare the result of SVD to that of PCA. Both achieve similar outcomes.", "t-SNE is developed by Laurens van der Maaten and Geoggrey Hinton. It is a machine learning algorithm for visualization that presents embedding high-dimensional data in a low-dimensional space of two or three dimensions.", "What is the best way to present the above three-dimensional Swiss roll to two-dimensional? Intuitively we want to \u201cunroll\u201d the Swiss roll into a flat cake. In mathematics, it means similar points will become nearby points and dissimilar points will become distant points.", "Figure (C) shows another example. It is a 3-dimensional tetrahedron with data points clustering in the vertex corners. If we just collapse the 3-dimensional graph to a 2-dimensional graph like Panel (A) does, it does not work well because group (A) becomes the center cluster. In contrast, Panel (B) is probably a better 2-D exhibit that preserves the far distances between Cluster (A)-(E) while keeping the local distances of points in each cluster. t-SNE, a nonlinear dimension reduction technique, is designed to preserve the local neighborhoods. If a set of points cluster together on a t-SNE plot, we can be fairly certain that these points are close to each other.", "t-SNE models the similarities among points. How does it define similarities? First, it is defined by the Euclidean distance between points Xi and Xj. Second, it is defined as the conditional probability that \u201cthe similarity of data point i to point j is the conditional probability p that point i would pick data j as its neighbor if other neighbors were picked according to their probabilities under a Gaussian distribution.\u201d In the following conditional expression, if point j is closer to point i than other points, it has a higher probability (notice the negative sign) to be chosen.", "t-SNE aims to match the above conditional probability p between j and i as well as possible by a low-dimensional space q between points Yi and Yj, as shown below. The probability q follows a fat-tailed Student-t distribution, thus the \u201ct\u201d in t-SNE comes from.", "The next step is to find Yi such that the distribution q will be as close to the distribution p as possible. t-SNE uses the gradient decent technique, an optimization technique, to find the values.", "Below I demonstrate how the t-SNE technique is used with the iris dataset.", "For more information, this post \u201chow to use t-SNE more effectively\u201d provides more discussions.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "The Dataman articles are my reflections on data science and teaching notes at Columbia University https://sps.columbia.edu/faculty/chris-kuo"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Ff36ca7009e5c&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdimension-reduction-techniques-with-python-f36ca7009e5c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdimension-reduction-techniques-with-python-f36ca7009e5c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdimension-reduction-techniques-with-python-f36ca7009e5c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdimension-reduction-techniques-with-python-f36ca7009e5c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----f36ca7009e5c--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f36ca7009e5c--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://dataman-ai.medium.com/?source=post_page-----f36ca7009e5c--------------------------------", "anchor_text": ""}, {"url": "https://dataman-ai.medium.com/?source=post_page-----f36ca7009e5c--------------------------------", "anchor_text": "Chris Kuo/Dr. Dataman"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F319122a619c6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdimension-reduction-techniques-with-python-f36ca7009e5c&user=Chris+Kuo%2FDr.+Dataman&userId=319122a619c6&source=post_page-319122a619c6----f36ca7009e5c---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff36ca7009e5c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdimension-reduction-techniques-with-python-f36ca7009e5c&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff36ca7009e5c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdimension-reduction-techniques-with-python-f36ca7009e5c&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/anomaly-detection-with-autoencoder-b4cdce4866a6", "anchor_text": "Anomaly Detection with Autoencoders Made Easy"}, {"url": "https://medium.com/@Dataman.ai/dataman-learning-paths-build-your-skills-drive-your-career-e1aee030ff6e", "anchor_text": "Dataman Learning Paths \u2014 Build Your Skills, Drive Your Career"}, {"url": "https://medium.com/dataman-in-ai/a-wide-choice-for-modeling-multi-class-classifications-d97073ff4ec8", "anchor_text": "A Wide Variety of models for Multi-class Classification"}, {"url": "https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data", "anchor_text": "https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"}, {"url": "https://towardsdatascience.com/avoid-these-deadly-modeling-mistakes-that-may-cost-you-a-career-b9b686d89f2c", "anchor_text": "Avoid These Deadly Modeling Mistakes that May Cost You a Career"}, {"url": "https://towardsdatascience.com/anomaly-detection-with-pyod-b523fc47db9", "anchor_text": "Anomaly Detection with PyOD"}, {"url": "https://dataman-ai.medium.com/membership", "anchor_text": "Join Medium with my referral link - Chris Kuo/Dr. DatamanRead every story from Chris Kuo/Dr. Dataman. Your membership fee directly supports Chris Kuo/Dr. Dataman and other\u2026dataman-ai.medium.com"}, {"url": "https://en.wikipedia.org/wiki/Radial_basis_function", "anchor_text": "Radial Basis Function"}, {"url": "https://en.wikipedia.org/wiki/Support-vector_machine", "anchor_text": "Support Vector Machine (SVM)"}, {"url": "https://www.kaggle.com/piyushgoyal443/red-wine-dataset#wineQualityInfo.txt", "anchor_text": "Red Wine Quality"}, {"url": "https://medium.com/dataman-in-ai/a-wide-choice-for-modeling-multi-class-classifications-d97073ff4ec8", "anchor_text": "A Wide Variety of models for Multi-class Classification"}, {"url": "https://yutsumura.com/how-to-diagonalize-a-matrix-step-by-step-explanation/", "anchor_text": "here"}, {"url": "https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data", "anchor_text": "https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"}, {"url": "http://www.cs.toronto.edu/~hinton/absps/tsne.pdf", "anchor_text": "Laurens van der Maaten and Geoggrey Hinton"}, {"url": "https://distill.pub/2016/misread-tsne/", "anchor_text": "how to use t-SNE more effectively"}, {"url": "https://dataman-ai.medium.com/membership", "anchor_text": "Join Medium with my referral link - Chris Kuo/Dr. DatamanRead every story from Chris Kuo/Dr. Dataman. Your membership fee directly supports Chris Kuo/Dr. Dataman and other\u2026dataman-ai.medium.com"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----f36ca7009e5c---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/dimensionality-reduction?source=post_page-----f36ca7009e5c---------------dimensionality_reduction-----------------", "anchor_text": "Dimensionality Reduction"}, {"url": "https://medium.com/tag/pca?source=post_page-----f36ca7009e5c---------------pca-----------------", "anchor_text": "Pca"}, {"url": "https://medium.com/tag/data-visualization?source=post_page-----f36ca7009e5c---------------data_visualization-----------------", "anchor_text": "Data Visualization"}, {"url": "https://medium.com/tag/data-science?source=post_page-----f36ca7009e5c---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff36ca7009e5c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdimension-reduction-techniques-with-python-f36ca7009e5c&user=Chris+Kuo%2FDr.+Dataman&userId=319122a619c6&source=-----f36ca7009e5c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff36ca7009e5c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdimension-reduction-techniques-with-python-f36ca7009e5c&user=Chris+Kuo%2FDr.+Dataman&userId=319122a619c6&source=-----f36ca7009e5c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff36ca7009e5c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdimension-reduction-techniques-with-python-f36ca7009e5c&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f36ca7009e5c--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Ff36ca7009e5c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdimension-reduction-techniques-with-python-f36ca7009e5c&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----f36ca7009e5c---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----f36ca7009e5c--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----f36ca7009e5c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----f36ca7009e5c--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----f36ca7009e5c--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----f36ca7009e5c--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----f36ca7009e5c--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----f36ca7009e5c--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----f36ca7009e5c--------------------------------", "anchor_text": ""}, {"url": "https://dataman-ai.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://dataman-ai.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Chris Kuo/Dr. Dataman"}, {"url": "https://dataman-ai.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "4.3K Followers"}, {"url": "https://sps.columbia.edu/faculty/chris-kuo", "anchor_text": "https://sps.columbia.edu/faculty/chris-kuo"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F319122a619c6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdimension-reduction-techniques-with-python-f36ca7009e5c&user=Chris+Kuo%2FDr.+Dataman&userId=319122a619c6&source=post_page-319122a619c6--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Ff55a75d0fe3f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdimension-reduction-techniques-with-python-f36ca7009e5c&newsletterV3=319122a619c6&newsletterV3Id=f55a75d0fe3f&user=Chris+Kuo%2FDr.+Dataman&userId=319122a619c6&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}