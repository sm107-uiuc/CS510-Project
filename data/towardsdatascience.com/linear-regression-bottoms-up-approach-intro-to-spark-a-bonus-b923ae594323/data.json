{"url": "https://towardsdatascience.com/linear-regression-bottoms-up-approach-intro-to-spark-a-bonus-b923ae594323", "time": 1683001440.05775, "path": "towardsdatascience.com/linear-regression-bottoms-up-approach-intro-to-spark-a-bonus-b923ae594323/", "webpage": {"metadata": {"title": "Linear Regression: Bottoms-up Approach | by Hemanth Devarapati | Towards Data Science", "h1": "Linear Regression: Bottoms-up Approach", "description": "This article walks through the nuances of linear regression and will include data analysis with spark as an add-on. This will serve, as a refresher for seasoned data scientists, as a deep learning\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.kaggle.com/sohier/calcofi", "anchor_text": "data set", "paragraph_index": 2}, {"url": "https://towardsdatascience.com/a-neanderthals-guide-to-apache-spark-in-python-9ef1f156d427", "anchor_text": "click me", "paragraph_index": 5}, {"url": "http://mathworld.wolfram.com/NormalEquation.html", "anchor_text": "normal equation", "paragraph_index": 16}, {"url": "https://medium.com/onfido-tech/machine-learning-101-be2e0a86c96a", "anchor_text": "example", "paragraph_index": 18}, {"url": "http://ruder.io/optimizing-gradient-descent/index.html#adam", "anchor_text": "here", "paragraph_index": 28}, {"url": "https://github.com/hdev7/medium-linear-regression-article-source-code", "anchor_text": "code", "paragraph_index": 29}, {"url": "http://people.duke.edu/~rnau/testing.htm", "anchor_text": "Duke", "paragraph_index": 36}, {"url": "http://www-bcf.usc.edu/~gareth/ISL/", "anchor_text": "Introduction to Statistical Learning", "paragraph_index": 46}, {"url": "https://www.youtube.com/watch?v=TIgfjmp-4BA", "anchor_text": "A good video on this", "paragraph_index": 67}, {"url": "http://scikit-learn.org/stable/modules/grid_search.html", "anchor_text": "here", "paragraph_index": 71}, {"url": "http://scikit-learn.org/stable/modules/cross_validation.html", "anchor_text": "documentation", "paragraph_index": 72}, {"url": "https://www.linkedin.com/in/hemanthsaid/", "anchor_text": "LinkedIn", "paragraph_index": 76}], "all_paragraphs": ["What is in it for you?", "This article walks through the nuances of linear regression and will include data analysis with spark as an add-on. This will serve, as a refresher for seasoned data scientists, as a deep learning material for beginners, and as a reference guide to budding practitioners.", "To meet the objective I have picked a data set that is not too trivial and at the same time not too intimidating. We will be focusing on determining the relationship between water salinity and temperature. This dataset has around 1 billion records and is Over 60 years of oceanographic data.", "The post will be big. Don\u2019t worry I won\u2019t bug you with weird maths but will focus on deriving insights and it will be fun I promise.", "You might want to use colab if you are running the entire dataset. Depending on your laptop capacity.", "Feel free to skip to the next session. If you are interested in getting started with spark and distributed computing, in general, click me", "This is just to get a feel of how pyspark works.", "Modeling will mean using a machine learning technique to learn from data the relationship between a set of features and what we hope to predict. Before going into the model building we should know about cost function. This is a very important concept which helps in building good models.", "How might we handle this issue? A decent spot to begin one\u2019s reasoning is: say we create numerous models to foresee our objective, how might we pick the best one? When we decide this, our objective is then to minimize/maximize that value.", "It is extremely useful if you can reduce your problem to a single evaluation metric because then it makes it very easy to iterate on model development. Sometimes it isn\u2019t extremely clear what you want your model to maximize/minimize. We will discuss this challenge more, but for now, we will stick with pretty standard evaluation functions used today.", "So for this problem, I would propose the following evaluation metric: mean squared error (MSE). To understand MSE, let\u2019s define some terminology:", "In English, for each point, we subtract our predicted value from the actual. Then, since we don\u2019t care about the direction of the error, we square the difference. Lastly, we take the mean of all these values. Basically, we are saying that we want the average distance between our predictions and our actuals to be small.", "You might be wondering, why we squared the value instead of taking the absolute value. It turns out that for some of the following math, squaring the value works out nicely. Also, it is the MSE estimate. This does have the effect, though, of weighting large errors more in our average since we are squaring all the differences.", "This depends on your loss function. In many circumstances it makes sense to give more weight to points further away from the mean \u2014 that is, being off by 10 is more than twice as bad as being off by 5. In such cases, RMSE is a more appropriate measure of error. If being off by ten is just twice as bad as being off by 5, then MAE is more appropriate.", "Now that we have our cost function, how do we find a way to minimize it? we will be reviewing the Linear Regression model. The model is as follows:", "Where j is the number of predictors we have and the \u03b2 values are our coefficients with \u03b20 being the intercept. Basically, our model is a linear combination of our predictors with an intercept.", "Now that we have a model and a cost function, our challenge becomes finding the \u03b2 values for our model that minimize MSE for our data. For linear regression, there is actually a closed-form solution called the normal equation. Though, we are going to use a different technique that is more common in machine learning \u2014 gradient descent.", "Gradient descent is a technique we borrow from optimization. It is a very simple, yet powerful algorithm that can be used to find the minimum of a function.", "This technique will find the global minimum if a function is convex, if not, we can only prove that it will find a local minimum. Let\u2019s take a look at an example.", "The first question we need to answer is: is the cost function convex? Let\u2019s take a look:", "What we have done above is taken a range of coefficient values for salinity and for each one calculated the MSE on our data(Temperature as target). If we then plot these we get the above curve \u2014 looks pretty convex! And in fact, it turns out that our MSE function with our linear regression model will always be convex! That means we can use gradient descent to find the optimal coefficients for our model!", "One reason that gradient descent is more common than the normal equation for machine learning is that it scales much better as we increase the number of features. It is also a general optimization technique that pops up all over machine learning, so understanding how it works is extremely valuable.", "Gradients are just the partial derivatives with respect to the coefficients. For each coefficient we have, we will need to calculate the derivative of our MSE with respect to that coefficient. Let\u2019s get started!", "Now, let\u2019s expand it out for our simple example with an intercept and a single variable, Salnty:", "Now, for the derivative of this with respect to \u03b20, we get:", "The implementation can be found in the notebook.", "The learning rate is a hyper-parameter used to determine how far we step away from the direction of the gradient. How do you know what value to pick? Typically, many values can be tried and here are a few that I believe are suggested by Andrew Ng: .001, .003, .01, .03, .1, .3, 1, 3", "Choosing a value that is too small, leads to slower convergence. Choosing a value too large can result in over-shooting the minimum and diverging.", "There are also other gradient descent optimizers that are more sophisticated and adapt the learning rate overtime for you. This is also something you can do on your own where you slowly decay the learning rate over time. You can read more about different optimizers here.", "In my code, I simply choose to run our loop 10,000 times. Why 10,000. No real reason other than I was pretty sure it was long enough to converge. That is typically not the best practice. A few better ideas are:", "When working with gradient descent you want all of your data normalized. Subtract from the mean and divide by the standard deviation for all your training features. This typically makes the training faster and reduces the chances of getting stuck in local optimum if your cost function is not convex.", "Normalization is a good technique to use when you do not know the distribution of your data or when you know the distribution is not Gaussian (a bell curve). Normalization is useful when your data has varying scales and the algorithm you are using does not make assumptions about the distribution of your data, such as k-nearest neighbors and artificial neural networks. Min-max scaler. It is sensitive to outliers.", "Standardization assumes that your data has a Gaussian (bell curve) distribution. This does not strictly have to be true, but the technique is more effective if your attribute distribution is Gaussian. Standardization is useful when your data has varying scales and the algorithm you are using does make assumptions about your data having a Gaussian distribution, such as linear regression, logistic regression, and linear discriminant analysis.", "Scaling using median and quantiles consists of subtracting the median to all the observations and then dividing by the interquartile difference. It Scales features using statistics that are robust to outliers.", "The gradient descent I have shown here is a vanilla form, which means each coefficient update uses all of the data to calculate the gradients. There is also stochastic gradient descent which only uses 1 row of data to update the coefficients in each loop. This is much more scalable as you only have to look at one data row at a time but is also much more random as you are trying to navigate using a gradient calculated on only a single data point. This introduces a new term as well: epoch which is the number of times you want to loop over your data.", "Another type of gradient descent is of mini-batch gradient descent. This form is a compromise between the two where you choose a batch size of say 32 (or even better a batch schedule which starts with small batches and increases over the number of epochs) and each iteration of your gradient descent gets to use 32 random rows of data to calculate the gradient with. This offers some scalability and some randomness. This randomness turns out is actually useful for cost functions which are not convex (deep learning) as it can help the model escape local minimum. This is the most common method for non-convex cost functions.", "Whenever you are dealing with a model it is good to be aware of the assumptions it is making. I was going to write up a section on this here, but Duke already did an excellent job.", "Now that we understand a bit of the theory and implementation, let\u2019s turn to a software library to actually run a linear regression on our data. It is very useful for learning to write models from scratch, but it a practice you are usually much better off using a tested and widely used library.", "Remember to scale your data \u2014 very important!", "Sklearn is a great package. It provides many models and all of them have a fit and predict function. You call fit on the X and y data to train the model and the predict on new features to get a predicted value. Sklearn also provides a lot of metrics you can use for evaluation such as MSE. Here I output root MSE (RMSE) because that gets us back into the original scale of our target, which I find easier to understand.", "With our SGDRegressor tol tells the model when to stop iterating and eta0 is our initial learning rate.", "If you remember our plot of Salnty against our target(T_degC), there looked a little bit of a polynomial relationship (this might be because of the scale of the plot as well). Linear regression fits linear relationships, but if you add polynomial features, such as Salnty\u00b2 you can fit more complex relationships. Sklearn makes this easy:", "The polynomial features command generated a new feature matrix consisting of all polynomial combinations of the features with a degree less than or equal to the specified degree (in our example 2). We then scaled these data and feed them to our model. And we got a slightly improved RMSE. Excellent! Note, though, that these results are not very valid because they are applied to our testing data. Later in this post, we will look at how to better evaluate the generalizability of our models.", "Linear regression is one of the models that you need to be careful with when you have categorical data. If you have a feature with values 1, 2, and 3 that actually mean Male, Female, No Response. You don\u2019t want to give it to the model this way even though they are numbers. If you did, the model would assign that feature a coefficient \u2014 perhaps 0.1. That would mean that being a female boosts the prediction by .1 and No response by .2. But maybe female should boost the score by 1.2 and No response by only .001. To account for this, you should convert these values to dummy variables so that each value can have its own weight. Unless you are confident there is a constant relationship in the order of your numeric values.", "Linear regression is a great statistical model that has been around for a long time. There are many statistical techniques that one can use to evaluate and interpret it. We will not cover them all and in fact, will mostly focus on very simple methods that are perhaps more common in machine learning than statistics.", "Intro to stat learning book linear regression points:", "This is just an intro. Refer Introduction to Statistical Learning for better understanding.", "First, let\u2019s take a look at the coefficients our model has learned:", "What are these coefficients? They represent the average change in the temperature for one unit of change in the feature while holding other features in the model constant(In our case we only have one). For example, a unit increase in Salinity decreases our target (Temperature) by 2.096.", "This is really nice! We can perhaps say that if you want to increase the temperature, decreasing salinity might be a place to start. I say might because linear regression is looking at correlations. In our data, this definitely appears to be the case, but that by itself does not mean that these features have a causal relationship. It could be a good place to look for a causal relationship, though, and does represent relationships that were seen in the data.", "Often in machine learning, it is very useful to have a confidence interval around your estimates. There are different ways to do this, but one fairly general method is using bootstrap.", "Bootstrap is a random sample with replacement of our data and this sample is of the same size as the original data. This is a way of generating multiple views of the same data. Let\u2019s create 1000 bootstraps of our data. After we are done sampling we can get the coefficients for each of these datasets by feeding into the model.", "This is pretty nice! Now we can say with strong confidence that the actual coefficient on Salnty is negative and almost certainly between -2.2 and -2.1", "Up to this point, we have been training on all the data that we have. This might make sense because we want to maximize the utility of our data by using as much as possible for training. On the other hand, though, it makes it hard to evaluate how well our model is doing. The reason for this is because if we just calculate our MSE score using data that the model was trained on, we might find that we introduced to data it was not trained on, it performs quite poorly.", "This idea is called overfitting. Basically, when a model performs much better on the data it was trained on as opposed to new data, it has overfitted to something unique to the training data that doesn\u2019t generalize.", "The other side of this is called bias. A model has a high bias when it really just doesn\u2019t do a good job fitting to the data. In this case, the MSE will be high for both the training data and data not seen during training.", "In machine learning, there is always a trade-off between bias and variance. As your models become more complex, there is a stronger risk of overfitting to your training data.", "Now that we know there are issues with only looking at MSE on our training data, what can we do to better judge generalizability? As well as diagnose overfitting and bias? Typically, we split our data into two sets: a training set and a testing set.", "You can leverage train_test_split from sklearn.model_selection", "Excellent! Now we have RMSE on both our training and testing data. And both are pretty close, which suggests we don\u2019t have an overfitting problem. Are they both low, though? Which would suggest a high bias.", "One way to look into this is by plotting the learning curve. A learning curve plots our error function (MSE) with various amounts of data used for training. Here is our plot:", "You can see that with less than 250 million training examples the training MSE is quite good and the cross-validation is quite bad (we have not talked about cross-validation yet, so think of that as testing for now). If we only had that much data then, it would look like a high variance problem. As we increase our data, we begin to improve both of our scores and they become very close, which suggests we don\u2019t have a high variance problem.", "This graph looks more like we have a high bias problem since our two curves are very close and flattening out. It is hard to say for sure, though, because we may have just reached the best MSE possible. In that case, this wouldn\u2019t be a high bias problem. It would only be a problem if our curves flattened out with an MSE higher than optimal. In real life, you don\u2019t know what the optimal MSE is, so you have to theorize a bit as to whether you think decreasing bias would improve your score \u2014 or just try it!", "So, now that you have diagnosed your bias or variance problem, how do you fix them?", "Earlier we mentioned this phrase: cross-validation. Let\u2019s talk about that now. So far, we have learned that it is a good idea to split your data into training and testing sets to better understand how well the model is actually doing. That is great, but imagine we want to test multiple different models or test different parameters to our model \u2014 for example, a different learning rate or tolerance. How would we decide which model or which parameter is best? Would we train everything on the training data and test everything on our testing data? Hopefully, you see that this doesn\u2019t make sense because then we would essentially be in the same place we were before without a way to test how well we do with never before seen data. So \u2014 we want to keep our testing set untainted in the sense that in a perfect world we would only run our tests on it after we have done all of our experimentation and think we have found the very best model.", "It sounds like we need a third set of data \u2014 a validation set. Basically, what we can do is break down our training data into two sets: a training set and a validation set. All models will be trained on the training set and then tested on our validation set. We then take the model that does the best on validation and see how well it does on testing. Our testing results represent how well we think our model would do with unseen data \u2014 and then we are done.", "Note: the assumption here is that our testing and validation sets are a representative sample of our population. Often, we randomly sample our available data into our three sets, but it is always good to confirm that these sets are good representations. Otherwise, you will find that your model that worked well in validation and testing performs poorly in production.", "In practice, instead of creating a single validation set we often use k-fold cross-validation. What this does is we choose a value of k, say 3. We then take our training data and split it into 3 folds. We randomly select 2 folds to train on and then use the remaining for testing. We then repeat this 2 more times, for a total of 3 times such that all observations are used for both training and validation, and each observation is used for validation exactly once. We would then average all the three scores (in our case MSE) to get a score for a particular model. We can then repeat this process for multiple models to find the best one. A good video on this", "This process is very easy with sklearn:", "Here we actually used Randomized search which typically is better than searching over all possible values. Often, you want to try many different parameters for many different knobs and grid searching over everything is not efficient. Usually, you want to use a randomized search as we did above. Though, since we only had a small number of values we forced it to be a grid-search by setting n_iter_search to the number of values we wanted to try.", "We also set cv=3 to have 3 folds and used negative MSE because the CV functions in sklearn try to maximize value.", "You can learn more about the random search and grid search here:", "Also, sklearn has many other CV functions that are useful especially if you want to test different models with the same folds. Here is some documentation", "As a means to account for high variance models, I mentioned regularization. You can think of regularization as a method used to penalize a model from learning complex relationships. For linear regression, that takes the form of three popular methods. All of these methods are centered around the idea of limiting how large the coefficients on our features can be. The idea being that if we overestimate the impact of a predictor (a large coefficient) it is likely that we are overfitting. Note: we can still have large coefficients. Regularization just says that the decrease in MSE has to justify the increase in coefficient magnitudes.", "Each of these methods takes a weighting factor that tells you how much you should weight the regularization term in the cost function. In sklearn, it is called alpha. An alpha of zero would add no penalty, while a high alpha would penalize the model a lot for having large coefficients. You can use cross-validation to discover a good value for alpha.", "I hope you have learned something in the article. This is a good starting point. I suggest you should read more about the concepts and consolidate them. Most of the time the inclination is towards learning fancy things as they are buzzed around. But building basics is always a great starting point. A ton of business applications just needs a quality regression model for point prediction.", "If you need any help you can get in touch: LinkedIn", "Hands-on machine learning by Geron Aurelien", "I love data and writing is my first love. This blend of both worlds brings out a purpose."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fb923ae594323&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-bottoms-up-approach-intro-to-spark-a-bonus-b923ae594323&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-bottoms-up-approach-intro-to-spark-a-bonus-b923ae594323&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-bottoms-up-approach-intro-to-spark-a-bonus-b923ae594323&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-bottoms-up-approach-intro-to-spark-a-bonus-b923ae594323&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/@hemanthsaid7?source=post_page-----b923ae594323--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b923ae594323--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@hemanthsaid7?source=post_page-----b923ae594323--------------------------------", "anchor_text": "Hemanth Devarapati"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffd994668b1ec&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-bottoms-up-approach-intro-to-spark-a-bonus-b923ae594323&user=Hemanth+Devarapati&userId=fd994668b1ec&source=post_page-fd994668b1ec----b923ae594323---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----b923ae594323--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb923ae594323&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-bottoms-up-approach-intro-to-spark-a-bonus-b923ae594323&user=Hemanth+Devarapati&userId=fd994668b1ec&source=-----b923ae594323---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb923ae594323&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-bottoms-up-approach-intro-to-spark-a-bonus-b923ae594323&source=-----b923ae594323---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://unsplash.com/@kmuza?utm_source=medium&utm_medium=referral", "anchor_text": "Carlos Muza"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://www.kaggle.com/sohier/calcofi", "anchor_text": "data set"}, {"url": "https://towardsdatascience.com/a-neanderthals-guide-to-apache-spark-in-python-9ef1f156d427", "anchor_text": "click me"}, {"url": "http://mathworld.wolfram.com/NormalEquation.html", "anchor_text": "normal equation"}, {"url": "https://medium.com/onfido-tech/machine-learning-101-be2e0a86c96a", "anchor_text": "example"}, {"url": "http://ruder.io/optimizing-gradient-descent/index.html#adam", "anchor_text": "here"}, {"url": "https://github.com/hdev7/medium-linear-regression-article-source-code", "anchor_text": "code"}, {"url": "http://people.duke.edu/~rnau/testing.htm", "anchor_text": "Duke"}, {"url": "http://www-bcf.usc.edu/~gareth/ISL/", "anchor_text": "Introduction to Statistical Learning"}, {"url": "https://www.youtube.com/watch?v=TIgfjmp-4BA", "anchor_text": "A good video on this"}, {"url": "http://scikit-learn.org/stable/modules/grid_search.html", "anchor_text": "here"}, {"url": "http://scikit-learn.org/stable/modules/cross_validation.html", "anchor_text": "documentation"}, {"url": "https://github.com/hdev7/medium-linear-regression-article-source-code", "anchor_text": "Github"}, {"url": "https://www.linkedin.com/in/hemanthsaid/", "anchor_text": "LinkedIn"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----b923ae594323---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----b923ae594323---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/linear-regression?source=post_page-----b923ae594323---------------linear_regression-----------------", "anchor_text": "Linear Regression"}, {"url": "https://medium.com/tag/regulation?source=post_page-----b923ae594323---------------regulation-----------------", "anchor_text": "Regulation"}, {"url": "https://medium.com/tag/gradient-descent?source=post_page-----b923ae594323---------------gradient_descent-----------------", "anchor_text": "Gradient Descent"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb923ae594323&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-bottoms-up-approach-intro-to-spark-a-bonus-b923ae594323&user=Hemanth+Devarapati&userId=fd994668b1ec&source=-----b923ae594323---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb923ae594323&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-bottoms-up-approach-intro-to-spark-a-bonus-b923ae594323&user=Hemanth+Devarapati&userId=fd994668b1ec&source=-----b923ae594323---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb923ae594323&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-bottoms-up-approach-intro-to-spark-a-bonus-b923ae594323&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/@hemanthsaid7?source=post_page-----b923ae594323--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b923ae594323--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffd994668b1ec&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-bottoms-up-approach-intro-to-spark-a-bonus-b923ae594323&user=Hemanth+Devarapati&userId=fd994668b1ec&source=post_page-fd994668b1ec----b923ae594323---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Ffd994668b1ec%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-bottoms-up-approach-intro-to-spark-a-bonus-b923ae594323&user=Hemanth+Devarapati&userId=fd994668b1ec&source=-----b923ae594323---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/@hemanthsaid7?source=post_page-----b923ae594323--------------------------------", "anchor_text": "Written by Hemanth Devarapati"}, {"url": "https://medium.com/@hemanthsaid7/followers?source=post_page-----b923ae594323--------------------------------", "anchor_text": "33 Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----b923ae594323--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffd994668b1ec&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-bottoms-up-approach-intro-to-spark-a-bonus-b923ae594323&user=Hemanth+Devarapati&userId=fd994668b1ec&source=post_page-fd994668b1ec----b923ae594323---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Ffd994668b1ec%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-bottoms-up-approach-intro-to-spark-a-bonus-b923ae594323&user=Hemanth+Devarapati&userId=fd994668b1ec&source=-----b923ae594323---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/do-you-know-k-nearest-neighbors-can-also-be-used-for-regression-tasks-117da22bcac3?source=author_recirc-----b923ae594323----0---------------------0b060583_6807_47bd_a6a6_db98d5521e20-------", "anchor_text": ""}, {"url": "https://medium.com/@hemanthsaid7?source=author_recirc-----b923ae594323----0---------------------0b060583_6807_47bd_a6a6_db98d5521e20-------", "anchor_text": ""}, {"url": "https://medium.com/@hemanthsaid7?source=author_recirc-----b923ae594323----0---------------------0b060583_6807_47bd_a6a6_db98d5521e20-------", "anchor_text": "Hemanth Devarapati"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----b923ae594323----0---------------------0b060583_6807_47bd_a6a6_db98d5521e20-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/do-you-know-k-nearest-neighbors-can-also-be-used-for-regression-tasks-117da22bcac3?source=author_recirc-----b923ae594323----0---------------------0b060583_6807_47bd_a6a6_db98d5521e20-------", "anchor_text": "Do you know K_Nearest_Neighbors can also be used for regression tasks?Yes, it is! Let\u2019s understand this simple algorithm"}, {"url": "https://towardsdatascience.com/do-you-know-k-nearest-neighbors-can-also-be-used-for-regression-tasks-117da22bcac3?source=author_recirc-----b923ae594323----0---------------------0b060583_6807_47bd_a6a6_db98d5521e20-------", "anchor_text": "\u00b76 min read\u00b7Nov 28, 2019"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F117da22bcac3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdo-you-know-k-nearest-neighbors-can-also-be-used-for-regression-tasks-117da22bcac3&user=Hemanth+Devarapati&userId=fd994668b1ec&source=-----117da22bcac3----0-----------------clap_footer----0b060583_6807_47bd_a6a6_db98d5521e20-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/do-you-know-k-nearest-neighbors-can-also-be-used-for-regression-tasks-117da22bcac3?source=author_recirc-----b923ae594323----0---------------------0b060583_6807_47bd_a6a6_db98d5521e20-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F117da22bcac3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdo-you-know-k-nearest-neighbors-can-also-be-used-for-regression-tasks-117da22bcac3&source=-----b923ae594323----0-----------------bookmark_preview----0b060583_6807_47bd_a6a6_db98d5521e20-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----b923ae594323----1---------------------0b060583_6807_47bd_a6a6_db98d5521e20-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----b923ae594323----1---------------------0b060583_6807_47bd_a6a6_db98d5521e20-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----b923ae594323----1---------------------0b060583_6807_47bd_a6a6_db98d5521e20-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----b923ae594323----1---------------------0b060583_6807_47bd_a6a6_db98d5521e20-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----b923ae594323----1---------------------0b060583_6807_47bd_a6a6_db98d5521e20-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----b923ae594323----1---------------------0b060583_6807_47bd_a6a6_db98d5521e20-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----0b060583_6807_47bd_a6a6_db98d5521e20-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----b923ae594323----1---------------------0b060583_6807_47bd_a6a6_db98d5521e20-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----b923ae594323----1-----------------bookmark_preview----0b060583_6807_47bd_a6a6_db98d5521e20-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----b923ae594323----2---------------------0b060583_6807_47bd_a6a6_db98d5521e20-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----b923ae594323----2---------------------0b060583_6807_47bd_a6a6_db98d5521e20-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----b923ae594323----2---------------------0b060583_6807_47bd_a6a6_db98d5521e20-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----b923ae594323----2---------------------0b060583_6807_47bd_a6a6_db98d5521e20-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----b923ae594323----2---------------------0b060583_6807_47bd_a6a6_db98d5521e20-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----b923ae594323----2---------------------0b060583_6807_47bd_a6a6_db98d5521e20-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----2-----------------clap_footer----0b060583_6807_47bd_a6a6_db98d5521e20-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----b923ae594323----2---------------------0b060583_6807_47bd_a6a6_db98d5521e20-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----b923ae594323----2-----------------bookmark_preview----0b060583_6807_47bd_a6a6_db98d5521e20-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/logistic-regression-bottoms-up-approach-feature-engineering-ideology-a-bonus-81807fa881be?source=author_recirc-----b923ae594323----3---------------------0b060583_6807_47bd_a6a6_db98d5521e20-------", "anchor_text": ""}, {"url": "https://medium.com/@hemanthsaid7?source=author_recirc-----b923ae594323----3---------------------0b060583_6807_47bd_a6a6_db98d5521e20-------", "anchor_text": ""}, {"url": "https://medium.com/@hemanthsaid7?source=author_recirc-----b923ae594323----3---------------------0b060583_6807_47bd_a6a6_db98d5521e20-------", "anchor_text": "Hemanth Devarapati"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----b923ae594323----3---------------------0b060583_6807_47bd_a6a6_db98d5521e20-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/logistic-regression-bottoms-up-approach-feature-engineering-ideology-a-bonus-81807fa881be?source=author_recirc-----b923ae594323----3---------------------0b060583_6807_47bd_a6a6_db98d5521e20-------", "anchor_text": "logistic regression: bottoms-up approach. (Feature engineering ideology \u2014 a bonus)Quick Question?"}, {"url": "https://towardsdatascience.com/logistic-regression-bottoms-up-approach-feature-engineering-ideology-a-bonus-81807fa881be?source=author_recirc-----b923ae594323----3---------------------0b060583_6807_47bd_a6a6_db98d5521e20-------", "anchor_text": "\u00b716 min read\u00b7Nov 24, 2019"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F81807fa881be&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-bottoms-up-approach-feature-engineering-ideology-a-bonus-81807fa881be&user=Hemanth+Devarapati&userId=fd994668b1ec&source=-----81807fa881be----3-----------------clap_footer----0b060583_6807_47bd_a6a6_db98d5521e20-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/logistic-regression-bottoms-up-approach-feature-engineering-ideology-a-bonus-81807fa881be?source=author_recirc-----b923ae594323----3---------------------0b060583_6807_47bd_a6a6_db98d5521e20-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F81807fa881be&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-bottoms-up-approach-feature-engineering-ideology-a-bonus-81807fa881be&source=-----b923ae594323----3-----------------bookmark_preview----0b060583_6807_47bd_a6a6_db98d5521e20-------", "anchor_text": ""}, {"url": "https://medium.com/@hemanthsaid7?source=post_page-----b923ae594323--------------------------------", "anchor_text": "See all from Hemanth Devarapati"}, {"url": "https://towardsdatascience.com/?source=post_page-----b923ae594323--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----b923ae594323----0---------------------b1da5bce_12b8_4fd6_8c30_92ebd83eeb72-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----b923ae594323----0---------------------b1da5bce_12b8_4fd6_8c30_92ebd83eeb72-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----b923ae594323----0---------------------b1da5bce_12b8_4fd6_8c30_92ebd83eeb72-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----b923ae594323----0---------------------b1da5bce_12b8_4fd6_8c30_92ebd83eeb72-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----b923ae594323----0---------------------b1da5bce_12b8_4fd6_8c30_92ebd83eeb72-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----b923ae594323----0---------------------b1da5bce_12b8_4fd6_8c30_92ebd83eeb72-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----0-----------------clap_footer----b1da5bce_12b8_4fd6_8c30_92ebd83eeb72-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----b923ae594323----0---------------------b1da5bce_12b8_4fd6_8c30_92ebd83eeb72-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----b923ae594323----0-----------------bookmark_preview----b1da5bce_12b8_4fd6_8c30_92ebd83eeb72-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----b923ae594323----1---------------------b1da5bce_12b8_4fd6_8c30_92ebd83eeb72-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----b923ae594323----1---------------------b1da5bce_12b8_4fd6_8c30_92ebd83eeb72-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----b923ae594323----1---------------------b1da5bce_12b8_4fd6_8c30_92ebd83eeb72-------", "anchor_text": "The PyCoach"}, {"url": "https://artificialcorner.com/?source=read_next_recirc-----b923ae594323----1---------------------b1da5bce_12b8_4fd6_8c30_92ebd83eeb72-------", "anchor_text": "Artificial Corner"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----b923ae594323----1---------------------b1da5bce_12b8_4fd6_8c30_92ebd83eeb72-------", "anchor_text": "You\u2019re Using ChatGPT Wrong! Here\u2019s How to Be Ahead of 99% of ChatGPT UsersMaster ChatGPT by learning prompt engineering."}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----b923ae594323----1---------------------b1da5bce_12b8_4fd6_8c30_92ebd83eeb72-------", "anchor_text": "\u00b77 min read\u00b7Mar 17"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fartificial-corner%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&user=The+PyCoach&userId=fb44e21903f3&source=-----886a50dabc54----1-----------------clap_footer----b1da5bce_12b8_4fd6_8c30_92ebd83eeb72-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----b923ae594323----1---------------------b1da5bce_12b8_4fd6_8c30_92ebd83eeb72-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "276"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&source=-----b923ae594323----1-----------------bookmark_preview----b1da5bce_12b8_4fd6_8c30_92ebd83eeb72-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/exploratory-data-analysis-the-ultimate-workflow-a82b1d21f747?source=read_next_recirc-----b923ae594323----0---------------------b1da5bce_12b8_4fd6_8c30_92ebd83eeb72-------", "anchor_text": ""}, {"url": "https://medium.com/@arthurmello_?source=read_next_recirc-----b923ae594323----0---------------------b1da5bce_12b8_4fd6_8c30_92ebd83eeb72-------", "anchor_text": ""}, {"url": "https://medium.com/@arthurmello_?source=read_next_recirc-----b923ae594323----0---------------------b1da5bce_12b8_4fd6_8c30_92ebd83eeb72-------", "anchor_text": "Arthur Mello"}, {"url": "https://levelup.gitconnected.com/?source=read_next_recirc-----b923ae594323----0---------------------b1da5bce_12b8_4fd6_8c30_92ebd83eeb72-------", "anchor_text": "Level Up Coding"}, {"url": "https://levelup.gitconnected.com/exploratory-data-analysis-the-ultimate-workflow-a82b1d21f747?source=read_next_recirc-----b923ae594323----0---------------------b1da5bce_12b8_4fd6_8c30_92ebd83eeb72-------", "anchor_text": "Exploratory Data Analysis: The Ultimate WorkflowExplore the true potential of your data with Python"}, {"url": "https://levelup.gitconnected.com/exploratory-data-analysis-the-ultimate-workflow-a82b1d21f747?source=read_next_recirc-----b923ae594323----0---------------------b1da5bce_12b8_4fd6_8c30_92ebd83eeb72-------", "anchor_text": "\u00b716 min read\u00b7Apr 20"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fgitconnected%2Fa82b1d21f747&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fexploratory-data-analysis-the-ultimate-workflow-a82b1d21f747&user=Arthur+Mello&userId=9d32d5e0ac40&source=-----a82b1d21f747----0-----------------clap_footer----b1da5bce_12b8_4fd6_8c30_92ebd83eeb72-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/exploratory-data-analysis-the-ultimate-workflow-a82b1d21f747?source=read_next_recirc-----b923ae594323----0---------------------b1da5bce_12b8_4fd6_8c30_92ebd83eeb72-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "5"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa82b1d21f747&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fexploratory-data-analysis-the-ultimate-workflow-a82b1d21f747&source=-----b923ae594323----0-----------------bookmark_preview----b1da5bce_12b8_4fd6_8c30_92ebd83eeb72-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/wanna-break-into-data-science-in-2023-think-twice-26842e9a87fe?source=read_next_recirc-----b923ae594323----1---------------------b1da5bce_12b8_4fd6_8c30_92ebd83eeb72-------", "anchor_text": ""}, {"url": "https://dwiuzila.medium.com/?source=read_next_recirc-----b923ae594323----1---------------------b1da5bce_12b8_4fd6_8c30_92ebd83eeb72-------", "anchor_text": ""}, {"url": "https://dwiuzila.medium.com/?source=read_next_recirc-----b923ae594323----1---------------------b1da5bce_12b8_4fd6_8c30_92ebd83eeb72-------", "anchor_text": "Albers Uzila"}, {"url": "https://levelup.gitconnected.com/?source=read_next_recirc-----b923ae594323----1---------------------b1da5bce_12b8_4fd6_8c30_92ebd83eeb72-------", "anchor_text": "Level Up Coding"}, {"url": "https://levelup.gitconnected.com/wanna-break-into-data-science-in-2023-think-twice-26842e9a87fe?source=read_next_recirc-----b923ae594323----1---------------------b1da5bce_12b8_4fd6_8c30_92ebd83eeb72-------", "anchor_text": "Wanna Break into Data Science in 2023? Think Twice!It won\u2019t be smooth sailing for you"}, {"url": "https://levelup.gitconnected.com/wanna-break-into-data-science-in-2023-think-twice-26842e9a87fe?source=read_next_recirc-----b923ae594323----1---------------------b1da5bce_12b8_4fd6_8c30_92ebd83eeb72-------", "anchor_text": "\u00b711 min read\u00b7Dec 23, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fgitconnected%2F26842e9a87fe&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwanna-break-into-data-science-in-2023-think-twice-26842e9a87fe&user=Albers+Uzila&userId=159e5ce51250&source=-----26842e9a87fe----1-----------------clap_footer----b1da5bce_12b8_4fd6_8c30_92ebd83eeb72-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/wanna-break-into-data-science-in-2023-think-twice-26842e9a87fe?source=read_next_recirc-----b923ae594323----1---------------------b1da5bce_12b8_4fd6_8c30_92ebd83eeb72-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "12"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F26842e9a87fe&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwanna-break-into-data-science-in-2023-think-twice-26842e9a87fe&source=-----b923ae594323----1-----------------bookmark_preview----b1da5bce_12b8_4fd6_8c30_92ebd83eeb72-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----b923ae594323----2---------------------b1da5bce_12b8_4fd6_8c30_92ebd83eeb72-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----b923ae594323----2---------------------b1da5bce_12b8_4fd6_8c30_92ebd83eeb72-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----b923ae594323----2---------------------b1da5bce_12b8_4fd6_8c30_92ebd83eeb72-------", "anchor_text": "Alexander Nguyen"}, {"url": "https://levelup.gitconnected.com/?source=read_next_recirc-----b923ae594323----2---------------------b1da5bce_12b8_4fd6_8c30_92ebd83eeb72-------", "anchor_text": "Level Up Coding"}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----b923ae594323----2---------------------b1da5bce_12b8_4fd6_8c30_92ebd83eeb72-------", "anchor_text": "Why I Keep Failing Candidates During Google Interviews\u2026They don\u2019t meet the bar."}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----b923ae594323----2---------------------b1da5bce_12b8_4fd6_8c30_92ebd83eeb72-------", "anchor_text": "\u00b74 min read\u00b7Apr 13"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fgitconnected%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&user=Alexander+Nguyen&userId=a148fd75c2e9&source=-----dc8f865b2c19----2-----------------clap_footer----b1da5bce_12b8_4fd6_8c30_92ebd83eeb72-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----b923ae594323----2---------------------b1da5bce_12b8_4fd6_8c30_92ebd83eeb72-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "89"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&source=-----b923ae594323----2-----------------bookmark_preview----b1da5bce_12b8_4fd6_8c30_92ebd83eeb72-------", "anchor_text": ""}, {"url": "https://medium.com/pipeline-a-data-engineering-resource/3-data-science-projects-that-got-me-12-interviews-and-1-that-got-me-in-trouble-f376682b4e21?source=read_next_recirc-----b923ae594323----3---------------------b1da5bce_12b8_4fd6_8c30_92ebd83eeb72-------", "anchor_text": ""}, {"url": "https://zachl-quinn.medium.com/?source=read_next_recirc-----b923ae594323----3---------------------b1da5bce_12b8_4fd6_8c30_92ebd83eeb72-------", "anchor_text": ""}, {"url": "https://zachl-quinn.medium.com/?source=read_next_recirc-----b923ae594323----3---------------------b1da5bce_12b8_4fd6_8c30_92ebd83eeb72-------", "anchor_text": "Zach Quinn"}, {"url": "https://medium.com/pipeline-a-data-engineering-resource?source=read_next_recirc-----b923ae594323----3---------------------b1da5bce_12b8_4fd6_8c30_92ebd83eeb72-------", "anchor_text": "Pipeline: A Data Engineering Resource"}, {"url": "https://medium.com/pipeline-a-data-engineering-resource/3-data-science-projects-that-got-me-12-interviews-and-1-that-got-me-in-trouble-f376682b4e21?source=read_next_recirc-----b923ae594323----3---------------------b1da5bce_12b8_4fd6_8c30_92ebd83eeb72-------", "anchor_text": "3 Data Science Projects That Got Me 12 Interviews. And 1 That Got Me in Trouble.3 work samples that got my foot in the door, and 1 that almost got me tossed out."}, {"url": "https://medium.com/pipeline-a-data-engineering-resource/3-data-science-projects-that-got-me-12-interviews-and-1-that-got-me-in-trouble-f376682b4e21?source=read_next_recirc-----b923ae594323----3---------------------b1da5bce_12b8_4fd6_8c30_92ebd83eeb72-------", "anchor_text": "\u00b77 min read\u00b7Aug 29, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fpipeline-a-data-engineering-resource%2Ff376682b4e21&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fpipeline-a-data-engineering-resource%2F3-data-science-projects-that-got-me-12-interviews-and-1-that-got-me-in-trouble-f376682b4e21&user=Zach+Quinn&userId=4364d136bdda&source=-----f376682b4e21----3-----------------clap_footer----b1da5bce_12b8_4fd6_8c30_92ebd83eeb72-------", "anchor_text": ""}, {"url": "https://medium.com/pipeline-a-data-engineering-resource/3-data-science-projects-that-got-me-12-interviews-and-1-that-got-me-in-trouble-f376682b4e21?source=read_next_recirc-----b923ae594323----3---------------------b1da5bce_12b8_4fd6_8c30_92ebd83eeb72-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "46"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff376682b4e21&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fpipeline-a-data-engineering-resource%2F3-data-science-projects-that-got-me-12-interviews-and-1-that-got-me-in-trouble-f376682b4e21&source=-----b923ae594323----3-----------------bookmark_preview----b1da5bce_12b8_4fd6_8c30_92ebd83eeb72-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----b923ae594323--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----b923ae594323--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----b923ae594323--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----b923ae594323--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----b923ae594323--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----b923ae594323--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----b923ae594323--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----b923ae594323--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----b923ae594323--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----b923ae594323--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}