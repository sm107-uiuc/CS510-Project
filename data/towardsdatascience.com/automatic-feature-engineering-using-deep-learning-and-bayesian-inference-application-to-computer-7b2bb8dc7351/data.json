{"url": "https://towardsdatascience.com/automatic-feature-engineering-using-deep-learning-and-bayesian-inference-application-to-computer-7b2bb8dc7351", "time": 1682993233.643737, "path": "towardsdatascience.com/automatic-feature-engineering-using-deep-learning-and-bayesian-inference-application-to-computer-7b2bb8dc7351/", "webpage": {"metadata": {"title": "Automatic feature engineering using deep learning and Bayesian inference | by Hamaad Shah | Towards Data Science", "h1": "Automatic feature engineering using deep learning and Bayesian inference", "description": "We will explore the use of deep learning and Bayesian inference for automatic feature engineering, specifically autoencoders. The idea is to automatically learn a set of features from, potentially\u2026"}, "outgoing_paragraph_urls": [{"url": "https://github.com/hamaadshah/autoencoders_keras", "anchor_text": "https://github.com/hamaadshah/autoencoders_keras", "paragraph_index": 70}], "all_paragraphs": ["We will explore the use of deep learning and Bayesian inference for automatic feature engineering, specifically autoencoders. The idea is to automatically learn a set of features from, potentially noisy, raw data that can be useful in supervised learning tasks such as in computer vision and insurance. In this manner we avoid the manual process of handcrafted feature engineering by learning a set of features automatically, i.e., representation learning, that can help in solving certain tasks such as image recognition and insurance loss risk prediction.", "We will use the MNIST dataset for this purpose where the raw data is a 2 dimensional tensor of pixel intensities per image. The image is our unit of analysis: We will predict the probability of each class for each image. This is a multiclass classification task and we will use the accuracy score to assess model performance on the test fold.", "Some examples of handcrafted feature engineering for the computer vision task perhaps might be using Gabor filters.", "We will use a synthetic dataset where the raw data is a 2 dimensional tensor of historical policy level information per policy-period combination: Per unit this will be a 4 by 3 dimensional tensor, i.e., 4 historical time periods and 3 transactions types. The policy-period combination is our unit of analysis: We will predict the probability of loss for time period 5 in the future \u2014 think of this as a potential renewal of the policy for which we need to predict whether it would make a loss for us or not hence affecting whether we decided to renew the policy and / or adjust the renewal premium to take into account the additional risk. This is a binary class classification task and we will use the AUROC score to assess model performance.", "Some examples of handcrafted feature engineering for the insurance task perhaps might be using column or row averages.", "The synthetic insurance financial transactions dataset was coded in R. All the rest of the work is done in Python.", "Please note the similarities between the raw data for the computer vision task and the raw data for the insurance task. Our main goal here is to learn a good representation of this raw data using automatic feature engineering via deep learning and Bayesian inference.", "We will use the Python machine learning library scikit-learn for data transformation and the classification task. Note that we will code the autoencoders as scikit-learn transformers such that they can be readily used by scikit-learn pipelines. The deep learners will be coded using Keras with the TensorFlow backend. We also use an external GPU, i.e., GTX 1070, on a MacBook Pro.", "We run the MNIST dataset without using an autoencoder. The 2 dimensional tensor of pixel intensities per image for MNIST images are of dimension 28 by 28. We reshape them as a 1 dimensional tensor of dimension 784 per image. Therefore we have 784 features for this supervised learning task per image.", "The accuracy score for the MNIST classification task without autoencoders: 92.000000%.", "We use a PCA filter that picks the number of components that explain 99% of the variation.", "The accuracy score for the MNIST classification task with PCA: 91.430000%.", "An autoencoder is an unsupervised learning technique where the objective is to learn a set of features that can be used to reconstruct the input data.", "Our input data is X. An encoder function E maps this to a set of K features. A decoder function D uses the set of K features to reconstruct the input data.", "Lets denote the reconstructed data as follows.", "The goal is to learn the encoding and decoding functions such that we minimize the difference between the input data and the reconstructed data. An example for an objective function for this task can be the Mean Squared Error (MSE).", "We learn the encoding and decoding functions by minimizing the MSE using the parameters that define the encoding and decoding functions: The gradient of the MSE with respect to the parameters are calculated using the chain rule, i.e., backpropagation, and used to update the parameters via an optimization algorithm such as Stochastic Gradient Descent (SGD).", "Lets assume we have a single layer autoencoder using the Exponential Linear Unit (ELU) activation function, batch normalization, dropout and the Adaptive Moment (Adam) optimization algorithm. B is the batch size, K is the number of features.", "The accuracy score for the MNIST classification task with an autoencoder: 96.940000%.", "The idea here is to add some noise to the data and try to learn a set of robust features that can reconstruct the non-noisy data from the noisy data. The MSE objective functions is as follows.", "The accuracy score for the MNIST classification task with a denoising autoencoder: 96.930000%.", "So far we have used flattened or reshaped raw data. Such a 1 dimensional tensor of pixel intensities per image might not take into account useful spatial features that the 2 dimensional tensor might contain. To overcome this problem, we introduce the concept of convolution filters, considering first their 1 dimensional version and then their 2 dimensional version.", "The ideas behind convolution filters are closely related to handcrafted feature engineering: One can view the handcrafted features as simply the result of a predefined convolution filter, i.e., a convolution filter that has not been learnt based on the raw data at hand.", "Suppose we have raw transactions data per some unit of analysis, i.e., mortgages, that will potentially help us in classifying a unit as either defaulted or not defaulted. We will keep this example simple by only allowing the transaction values to be either $100 or $0. The raw data per unit spans 5 time periods while the defaulted label is for the next period, i.e., period 6. Here is an example of a raw data for a particular unit:", "Suppose further that if the average transaction value is $20 then we will see a default in period 6 for this particular mortgage unit. Otherwise we do not see a default in period 6. The average transaction value is an example of a handcrafted feature: A predefined handcrafted feature that has not been learnt in any manner. It has been arrived at via domain knowledge of credit risk. Denote this as H(x).", "The idea of learning such a feature is an example of a 1 dimensional convolution filter. As follows:", "Assuming that H(x) is the correct representation of the raw data for this supervised learning task then the optimal set of parameters learnt via supervised learning for the convolution filter defined above, or perhaps unsupervised learning and then transferred to the supervised learning task, i.e., transfer learning, is [0.2, 0.2, 0.2, 0.2, 0.2]:", "This is a simple example however this clearly illusrates the principle behind using deep learning for automatic feature engineering or representation learning. One of the main benefits of learning such a representation in an unsupervised manner is that the same representation can then be used for multiple supervised learning tasks: Transfer learning. This is a principled manner of learning a representation from raw data.", "To summarize the 1 dimensional convolution filter for our simple example is defined as:", "This automates feature engineering however introduces architecture engineering where different architectures consisting of various convolution filters, activation functions, batch normalization layers, dropout layers and pooling operators can be stacked together in a pipeline in order to learn a good representation of the raw data. One usually creates an ensemble of such architectures.", "The goal behind convolutional autoencoders is to use convolution filters, activation functions, batch normalization layers, dropout layers and pooling operators to create an encoder function which will learn a good representation of our raw data. The decoder will also use a similar set of layers as the encoder to reconstruct the raw data with one exception: Instead of using a pooling operator it will use an upsampling operator. The basic idea behind the upsampling operator is to repeat an element a certain number of times say size 4: One can view this as the inverse operator to the pooling operator. The pooling operator is essentially a downsampling operator and the upsampling operator is simply the inverse of that in some sense.", "The accuracy score for the MNIST classification task with a 1 dimensional convolutional autoencoder: 97.570000%.", "Given our mortgage default example a potentially more useful deep learning architecture might be the Recurrent Neural Network (RNN), specifically their state of the art variant the Long Short Term Memory (LSTM) network. The goal is to explicitly take into account the sequential nature of the raw data.", "The gradients in a RNN depend on the parameter matrices defined for the model. Simply put these parameter matrices can end up being multiplied many times over and hence cause two major problems for learning: Exploding and vanishing gradients. If the spectral radius of the parameter matrices, i.e., the maximum absolute value of the eigenvalues of a matrix, is more than 1 then gradients can become large enough, i.e., explode in value, such that learning diverges and similarly if the spectral radius is less than 1 then gradients can become small, i.e., vanish in value, such that the next best transition for the parameters cannot be reliably calculated. Appropriate calculation of the gradient is important for estimating the optimal set of parameters that define a machine learning method and the LSTM network overcomes these problems in a vanilla RNN. We now define the LSTM network for 1 time step, i.e., 1 memory cell.", "We calculate the value of the input gate, the value of the memory cell state at time period t where f(x) is some activation function and the value of the forget gate:", "The forget gate controls the amount the LSTM remembers, i.e., the value of the memory cell state at time period t-1 where \u2a02 is the hadamard product:", "With the updated state of the memory cell we calculate the value of the outputs gate and finally the output value itself:", "We can have a wide variety of LSTM architectures such as the convolutional LSTM where note that we replace the matrix multiplication operators in the input gate, the initial estimate of the memory cell state, the forget gate and the output gate by the convolution operator *:", "Another popular variant is the peephole LSTM where the gates are allowed to peep at the memory cell state:", "The goal for the sequence to sequence autoencoder is to create a representation of the raw data using a LSTM as an encoder. This representation will be a sequence of vectors learnt from a sequence of raw data vectors. The final vector of the representation is our encoded representation, also called a context vector. This context vector is repeated as many times as the length of the sequence such that it can be used as an input to a decoder which is yet another LSTM. The decoder LSTM will use this context vector to recontruct the sequence of raw data vectors. If the context vector is useful in the recontruction task then it can be further used for other tasks such as predicting default risk as given in our example.", "The accuracy score for the MNIST classification task with a sequence to sequence autoencoder: 97.600000%.", "We now combine Bayesian inference with deep learning by using variational inference to train an autoencoder. This moves us towards generative modelling which can have further use cases in semi-supervised learning. The other benefit of training using Bayesian inference is that we can be more robust to higher capacity deep learners.", "The accuracy score for the MNIST classification task with a variational autoencoder: 96.520000%.", "For 2 dimensional convolution filters the idea is similar as for the 1 dimensional convolution filters. We will stick to our previously mentioned banking example to illustrate this point.", "In the 2 dimensional tensor of raw transactions data now we have 5 historical time periods, i.e., the rows, and 3 different transaction types, i.e., the columns. We will use a kernel of size 2 by 3 to extract useful features from the raw data. The choice of such a kernel means that we are interested in finding a feature map across all 3 transaction types and 2 historical time periods. We will use a stride length of 1 and a valid convolution to extract features over different patches of the raw data. The following will illustrate this point.", "The principles and ideas apply to 2 dimensional convolution filters as they do for their 1 dimensional counterparts there we will not repeat them here.", "The accuracy score for the MNIST classification task with a 2 dimensional convolutional autoencoder: 98.860000%.", "We now proceed to run the insurance model without any handcrafted or deep learning based feature engineering: Just raw data.", "The AUROC score for the insurance classification task without autoencoders: 92.206261%.", "We now proceed to run the insurance model without any handcrafted or deep learning based feature engineering however with a PCA filter that picks the number of components that explain 99% of the variation.", "The AUROC score for the insurance classification task with PCA: 91.128859%.", "In this case we have created some handcrafted features which we believe provide a useful representation of the raw data for the insurance model.", "The AUROC score for the insurance classification task with handcrafted features: 93.610635%.", "We use the aforementioned handcrafted features and a PCA filter that picks the number of components that explain 99% of the variation.", "The AUROC score for the insurance classification task with handcrafted features and PCA: 93.160377%.", "In this case we use vanilla autoencoders to learn a good representation of the raw data such that we can obtain an uplift, in terms of AUROC, for the supervised learning task.", "The AUROC score for the insurance classification task with an autoencoder: 93.932247%.", "In this case we use denoising autoencoders to learn a good representation of the raw data such that we can obtain an uplift, in terms of AUROC, for the supervised learning task.", "The AUROC score for the insurance classification task with a denoising autoencoder: 93.712479%.", "In this case we use sequence to sequence autoencoders, taking into account the time series nature, i.e., sequential nature, of the raw transactions data, to learn a good representation of the raw data such that we can obtain an uplift, in terms of AUROC, for the supervised learning task.", "The AUROC score for the insurance classification task with a sequence to sequence autoencoder: 91.418310%.", "In this case we use 1 dimensional convolutional autoencoders to learn a good representation of the raw data such that we can obtain an uplift, in terms of AUROC, for the supervised learning task.", "The AUROC score for the insurance classification task with a 1 dimensional convolutional autoencoder: 91.509434%.", "In this case we use 2 dimensional convolutional autoencoders to learn a good representation of the raw data such that we can obtain an uplift, in terms of AUROC, for the supervised learning task.", "The AUROC score for the insurance classification task with a 2 dimensional convolutional autoencoder: 92.645798%.", "In this case we use variational autoencoders to learn a good representation of the raw data such that we can obtain an uplift, in terms of AUROC, for the supervised learning task.", "The AUROC score for the insurance classification task with a variational autoencoder: 90.871569%.", "As expected, the best score achieved here is by a 2 dimensional convolutional autoencoder.", "The best score achieved on this task is by a vanilla autoencoder. This highlights the automation of feature engineering via deep learning: I believe it was Ian Goodfellow who said that a learnt representation is better than a handcrafted representation.", "Note that the sequence to sequence and convolutional autoencoders did not do well on this task simply because of the manner in which I generated the synthetic transactions data: Should the data have been from a process more amenable to sequence to sequence or convolutional autoencoders it is highly likely that these architectures would\u2019ve performed better.", "The code I\u2019ve written for this article is available on my GitHub here: https://github.com/hamaadshah/autoencoders_keras", "We have shown how to use deep learning and Bayesian inference to learn a good representation of raw data, i.e., 1 or 2 dimensional tensors per unit of analysis, that can then perhaps be used for supervised learning tasks in the domain of computer vision and insurance. This moves us away from manual handcrafted feature engineering towards automatic feature engineering, i.e., representation learning. This does introduce architecture engineering however that can be automated as well perhaps by the use of genetic algorithms or reinforcement learning \u2014 a topic for another paper perhaps.", "Finally, I would like to emphasize that the same code used for solving the computer vision task was used to solve the insurance task. In both tasks automatic feature engineering via deep learning had the best performance despite the fact that we were not explicitly looking for the best state of the art architecture possible. This provides us with a powerful way of automating machine learning tasks end-to-end.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Operational Data Team Lead at Trade Nation and guest speaker at the University of Oxford DCE."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F7b2bb8dc7351&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomatic-feature-engineering-using-deep-learning-and-bayesian-inference-application-to-computer-7b2bb8dc7351&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomatic-feature-engineering-using-deep-learning-and-bayesian-inference-application-to-computer-7b2bb8dc7351&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomatic-feature-engineering-using-deep-learning-and-bayesian-inference-application-to-computer-7b2bb8dc7351&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomatic-feature-engineering-using-deep-learning-and-bayesian-inference-application-to-computer-7b2bb8dc7351&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----7b2bb8dc7351--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----7b2bb8dc7351--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@littlemarble?source=post_page-----7b2bb8dc7351--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@littlemarble?source=post_page-----7b2bb8dc7351--------------------------------", "anchor_text": "Hamaad Shah"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd4620e7f535&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomatic-feature-engineering-using-deep-learning-and-bayesian-inference-application-to-computer-7b2bb8dc7351&user=Hamaad+Shah&userId=d4620e7f535&source=post_page-d4620e7f535----7b2bb8dc7351---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7b2bb8dc7351&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomatic-feature-engineering-using-deep-learning-and-bayesian-inference-application-to-computer-7b2bb8dc7351&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7b2bb8dc7351&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomatic-feature-engineering-using-deep-learning-and-bayesian-inference-application-to-computer-7b2bb8dc7351&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://github.com/hamaadshah/autoencoders_keras", "anchor_text": "https://github.com/hamaadshah/autoencoders_keras"}, {"url": "https://arxiv.org/abs/1312.6114", "anchor_text": "https://arxiv.org/abs/1312.6114"}, {"url": "https://arxiv.org/abs/1801.07848", "anchor_text": "https://arxiv.org/abs/1801.07848"}, {"url": "http://scikit-learn.org/stable/#", "anchor_text": "http://scikit-learn.org/stable/#"}, {"url": "https://towardsdatascience.com/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1", "anchor_text": "https://towardsdatascience.com/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1"}, {"url": "https://stackoverflow.com/questions/42177658/how-to-switch-backend-with-keras-from-tensorflow-to-theano", "anchor_text": "https://stackoverflow.com/questions/42177658/how-to-switch-backend-with-keras-from-tensorflow-to-theano"}, {"url": "https://blog.keras.io/building-autoencoders-in-keras.html", "anchor_text": "https://blog.keras.io/building-autoencoders-in-keras.html"}, {"url": "https://keras.io", "anchor_text": "https://keras.io"}, {"url": "https://www.cs.cornell.edu/courses/cs1114/2013sp/sections/S06_convolution.pdf", "anchor_text": "https://www.cs.cornell.edu/courses/cs1114/2013sp/sections/S06_convolution.pdf"}, {"url": "http://deeplearning.net/tutorial/lstm.html", "anchor_text": "http://deeplearning.net/tutorial/lstm.html"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----7b2bb8dc7351---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----7b2bb8dc7351---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/bayesian-machine-learning?source=post_page-----7b2bb8dc7351---------------bayesian_machine_learning-----------------", "anchor_text": "Bayesian Machine Learning"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----7b2bb8dc7351---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/feature-engineering?source=post_page-----7b2bb8dc7351---------------feature_engineering-----------------", "anchor_text": "Feature Engineering"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7b2bb8dc7351&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomatic-feature-engineering-using-deep-learning-and-bayesian-inference-application-to-computer-7b2bb8dc7351&user=Hamaad+Shah&userId=d4620e7f535&source=-----7b2bb8dc7351---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7b2bb8dc7351&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomatic-feature-engineering-using-deep-learning-and-bayesian-inference-application-to-computer-7b2bb8dc7351&user=Hamaad+Shah&userId=d4620e7f535&source=-----7b2bb8dc7351---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7b2bb8dc7351&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomatic-feature-engineering-using-deep-learning-and-bayesian-inference-application-to-computer-7b2bb8dc7351&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----7b2bb8dc7351--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F7b2bb8dc7351&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomatic-feature-engineering-using-deep-learning-and-bayesian-inference-application-to-computer-7b2bb8dc7351&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----7b2bb8dc7351---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----7b2bb8dc7351--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----7b2bb8dc7351--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----7b2bb8dc7351--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----7b2bb8dc7351--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----7b2bb8dc7351--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----7b2bb8dc7351--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----7b2bb8dc7351--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----7b2bb8dc7351--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@littlemarble?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@littlemarble?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Hamaad Shah"}, {"url": "https://medium.com/@littlemarble/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "321 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd4620e7f535&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomatic-feature-engineering-using-deep-learning-and-bayesian-inference-application-to-computer-7b2bb8dc7351&user=Hamaad+Shah&userId=d4620e7f535&source=post_page-d4620e7f535--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Fd4620e7f535%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomatic-feature-engineering-using-deep-learning-and-bayesian-inference-application-to-computer-7b2bb8dc7351&user=Hamaad+Shah&userId=d4620e7f535&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}