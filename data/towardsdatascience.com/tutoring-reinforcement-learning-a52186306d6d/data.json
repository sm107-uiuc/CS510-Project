{"url": "https://towardsdatascience.com/tutoring-reinforcement-learning-a52186306d6d", "time": 1683009045.3153799, "path": "towardsdatascience.com/tutoring-reinforcement-learning-a52186306d6d/", "webpage": {"metadata": {"title": "Tutoring Reinforcement Learning. RL agents start from scratch, knowing\u2026 | by Mauricio Fadel Argerich | Towards Data Science", "h1": "Tutoring Reinforcement Learning", "description": "Reinforcement Learning (RL) has been shown to achieve amazing results in several tasks such as video games, robotics and recommender systems in recente years. However, these successful results come\u2026"}, "outgoing_paragraph_urls": [{"url": "http://ceur-ws.org/Vol-2600/short9.pdf", "anchor_text": "here", "paragraph_index": 0}, {"url": "http://ceur-ws.org/Vol-2600/short9.pdf", "anchor_text": "Tutor4RL", "paragraph_index": 4}, {"url": "https://www.aaai-make.info", "anchor_text": "AAAI-MAKE: Combining Machine Learning and Knowledge Engineering in Practice", "paragraph_index": 4}, {"url": "http://ceur-ws.org/Vol-2600/short9.pdf", "anchor_text": "here", "paragraph_index": 4}, {"url": "https://github.com/keras-rl/keras-rl", "anchor_text": "Keras-RL", "paragraph_index": 11}, {"url": "https://www.cl.uni-heidelberg.de/courses/ws17/reinforcement/MnihETAL15.pdf", "anchor_text": "DQN agent (Mnih et al. 2015)", "paragraph_index": 11}, {"url": "https://gym.openai.com/envs/Breakout-v0/", "anchor_text": "Breakout on OpenAI Gym", "paragraph_index": 11}, {"url": "https://www.linkedin.com/in/maufadel/", "anchor_text": "https://www.linkedin.com/in/maufadel/", "paragraph_index": 16}], "all_paragraphs": ["This story is based on a paper I co-authored and you can find here.", "Reinforcement Learning (RL) has been shown to achieve amazing results in several tasks such as video games, robotics and recommender systems in recente years. However, these successful results come after training RL agents for millions of iterations. Until then, the agent\u2019s performance will be far from great. In fact, when the agent starts, its behavior is random while exploring the different actions it can take, and even after several iterations and gaining some experience, the agent will often make mistakes because of variability of its actions in the environment and unseen states that may arise at any time.", "In environments like video games, this might not be a problem (even though it means you need time and some serious computing resources), but in real world systems this is a serious challenge. You might think of a robot that uses RL to learn how to move: the robot might take days to learn how to properly move and it might damage itself if a movement is dangerous.", "As humans, we do not learn everything from scratch and if we would, we would not be far from cave men. All the progress we\u2019ve made has been achieved through learning from others achievements and improving upon them. Couldn\u2019t we do something similar with RL? Couldn\u2019t we teach the agent some general rules that it can apply while learning from its own experience?", "With my colleagues, we have been working on this idea and we have developed an approach we called Tutor4RL. We have published it at AAAI-MAKE: Combining Machine Learning and Knowledge Engineering in Practice this year and you can see it here (unfortunately, the spring venue was canceled this year but papers were published.) Tutor4RL is still work in progress, so here I present our approach and initial results we\u2019ve had so far.", "As I have said, slow learning is a common challenge in RL and many methods have emerged to address it:", "We want our agent to perform well (or at least decently) from the start, but this is very hard when we do not have access to the environment beforehand, and if we make any assumptions on the environment and they\u2019re wrong, the agent\u2019s performance will suffer, not only initially but throughout its whole life. However, this doesn\u2019t mean we cannot give some useful information the agent could be able to use once it is deployed in the environment.", "In the end, when we learn something we are not given all the details of every situation we will encounter. Instead, we can learn from theoretical ideas and hints other persons give us. Can a RL agent do this?", "In an attempt to do this, we\u2019ve modified the RL framework as shown in the figure below:", "We\u2019ve added a component we call the Tutor which contains external knowledge in the form of an ensemble of knowledge functions. These functions are normal programmable functions that take as input the state and reward and output a vector with a value for each action, in a similar way to how a policy maps states to actions with Q-values. There are two types of knowledge functions:", "The agent can ask the tutor for its knowledge when it\u2019s uncertain about what to do, for example, in its initial steps. The tutor will reply with an ensemble of its functions that the agent will then use to choose what action to execute and learn from this experience. In this way, the tutor guides the agent but the agent can always find if a suggestion from the tutor is wrong or, by using some exploration mechanism such as \u03b5-greedy, can also find if there is a better action than the tutor\u2019s choice. However, constraint functions are always applied to both, guide functions and the agent\u2019s policy, so it provides a safety layer to avoid serious errors that might put the agent in danger or have a great negative impact in the task.", "We have implemented a prototype of Tutor4RL using Keras-RL on Python. We have applied Tutor4RL to a DQN agent (Mnih et al. 2015), to play Breakout on OpenAI Gym. We used one simple guide function, that tells the agent to move in the direction to where the ball is, if the ball is not directly above the bar (not that in this test, we did not use constraint functions). In addition, we have implemented a very simple approach to control when the agent will use the tutor\u2019s guides: we define a parameter called \u03c4 that is used in the same way as \u03b5 is used in \u03b5-greedy exploration. When \u03c4 is greater than a sample taken from U(0,1) distribution, we use the tutor\u2019s output, otherwise, we use the policy\u2019s output. We initialize the agent with \u03c4=1 and decrease it linearly over time, so when the agent starts, the tutor\u2019s output is used heavily but this decreases as the agent gathers more experience.", "Below, you can see the reward achieved by the DQN agent with Tutor4RL compared against a standard, plain DQN agent. The reward in Breakout is directly the score achieved on the game, so the more blocks you break, the higher score and the higher the reward. As you can see, the DQN agent with Tutor4RL shows an initial good performance thanks to the tutor guide while the plain DQN agent struggles. It takes about 1.3 million iterations for the plain DQN agent to catch up and in iteration 1.5 million the tutor ceases to be used completely. Note that before this, the tutor is used intermittently, depending on the value of \u03c4 that decreases in each iteration. After that, we can see both agents have a similar performance, showing the tutored DQN agent also learned but avoided the rough start of the plain DQN agent.", "Tutor4RL has proven to be able to help the agent to start with some knowledge of its environment, improving its performance in its initial steps. However, Tutor4RL is still work in progress and several things can be improved:", "As always, thanks for reading! I hope you found this approach interesting and I look forward to hearing your feedback. I\u2019m sure there are more ways in which we can improve Tutor4RL.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Scientist | Information Systems Engineer. Research Scientist of AI. More about me on https://www.linkedin.com/in/maufadel/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fa52186306d6d&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftutoring-reinforcement-learning-a52186306d6d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftutoring-reinforcement-learning-a52186306d6d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftutoring-reinforcement-learning-a52186306d6d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftutoring-reinforcement-learning-a52186306d6d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----a52186306d6d--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a52186306d6d--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@mauriciofadelargerich?source=post_page-----a52186306d6d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@mauriciofadelargerich?source=post_page-----a52186306d6d--------------------------------", "anchor_text": "Mauricio Fadel Argerich"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb3931df7d193&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftutoring-reinforcement-learning-a52186306d6d&user=Mauricio+Fadel+Argerich&userId=b3931df7d193&source=post_page-b3931df7d193----a52186306d6d---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa52186306d6d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftutoring-reinforcement-learning-a52186306d6d&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa52186306d6d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftutoring-reinforcement-learning-a52186306d6d&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "http://ceur-ws.org/Vol-2600/short9.pdf", "anchor_text": "here"}, {"url": "http://ceur-ws.org/Vol-2600/short9.pdf", "anchor_text": "Tutor4RL"}, {"url": "https://www.aaai-make.info", "anchor_text": "AAAI-MAKE: Combining Machine Learning and Knowledge Engineering in Practice"}, {"url": "http://ceur-ws.org/Vol-2600/short9.pdf", "anchor_text": "here"}, {"url": "http://ceur-ws.org/Vol-2600/short9.pdf", "anchor_text": "original publication"}, {"url": "https://gym.openai.com/envs/Breakout-v0/", "anchor_text": "Breakout from OpenAI Gym"}, {"url": "https://github.com/keras-rl/keras-rl", "anchor_text": "Keras-RL"}, {"url": "https://www.cl.uni-heidelberg.de/courses/ws17/reinforcement/MnihETAL15.pdf", "anchor_text": "DQN agent (Mnih et al. 2015)"}, {"url": "https://gym.openai.com/envs/Breakout-v0/", "anchor_text": "Breakout on OpenAI Gym"}, {"url": "http://ceur-ws.org/Vol-2600/short9.pdf", "anchor_text": "original publication"}, {"url": "https://arxiv.org/pdf/1702.01182.pdf", "anchor_text": "Kahn et al. (2017)"}, {"url": "https://link.springer.com/article/10.1007/s00778-019-00552-1", "anchor_text": "Snorkel (Ratner et al. (2019))"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----a52186306d6d---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----a52186306d6d---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/programming?source=post_page-----a52186306d6d---------------programming-----------------", "anchor_text": "Programming"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----a52186306d6d---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/data-science?source=post_page-----a52186306d6d---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa52186306d6d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftutoring-reinforcement-learning-a52186306d6d&user=Mauricio+Fadel+Argerich&userId=b3931df7d193&source=-----a52186306d6d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa52186306d6d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftutoring-reinforcement-learning-a52186306d6d&user=Mauricio+Fadel+Argerich&userId=b3931df7d193&source=-----a52186306d6d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa52186306d6d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftutoring-reinforcement-learning-a52186306d6d&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a52186306d6d--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fa52186306d6d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftutoring-reinforcement-learning-a52186306d6d&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----a52186306d6d---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----a52186306d6d--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----a52186306d6d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----a52186306d6d--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----a52186306d6d--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----a52186306d6d--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----a52186306d6d--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----a52186306d6d--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----a52186306d6d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@mauriciofadelargerich?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@mauriciofadelargerich?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Mauricio Fadel Argerich"}, {"url": "https://medium.com/@mauriciofadelargerich/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "298 Followers"}, {"url": "https://www.linkedin.com/in/maufadel/", "anchor_text": "https://www.linkedin.com/in/maufadel/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb3931df7d193&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftutoring-reinforcement-learning-a52186306d6d&user=Mauricio+Fadel+Argerich&userId=b3931df7d193&source=post_page-b3931df7d193--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F9d5b675a4898&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftutoring-reinforcement-learning-a52186306d6d&newsletterV3=b3931df7d193&newsletterV3Id=9d5b675a4898&user=Mauricio+Fadel+Argerich&userId=b3931df7d193&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}