{"url": "https://towardsdatascience.com/learn-nlp-the-stanford-way-lesson-2-7447f2c12b36", "time": 1683017445.89066, "path": "towardsdatascience.com/learn-nlp-the-stanford-way-lesson-2-7447f2c12b36/", "webpage": {"metadata": {"title": "Learn NLP the Stanford Way \u2014 Lesson 2 | by Thiago Candido | Towards Data Science", "h1": "Learn NLP the Stanford Way \u2014 Lesson 2", "description": "In the previous post, we introduced NLP. To find out word meanings with the Python programming language, we used the NLTK package and worked our way into word embeddings using the gensim package and\u2026"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/abs/1301.3781", "anchor_text": "[1]", "paragraph_index": 2}, {"url": "https://arxiv.org/abs/1310.4546", "anchor_text": "[2]", "paragraph_index": 2}, {"url": "https://en.wikipedia.org/wiki/Logistic_function", "anchor_text": "logistic function", "paragraph_index": 11}, {"url": "https://en.wikipedia.org/wiki/Multinomial_logistic_regression", "anchor_text": "multinomial logistic regression", "paragraph_index": 11}, {"url": "https://en.wikipedia.org/wiki/Activation_function", "anchor_text": "activation function", "paragraph_index": 11}, {"url": "https://en.wikipedia.org/wiki/Artificial_neural_network", "anchor_text": "neural network", "paragraph_index": 11}, {"url": "https://en.wikipedia.org/wiki/Probability_distribution", "anchor_text": "probability distribution", "paragraph_index": 11}, {"url": "https://en.wikipedia.org/wiki/Luce%27s_choice_axiom", "anchor_text": "Luce\u2019s choice axiom", "paragraph_index": 11}, {"url": "https://en.wikipedia.org/wiki/Softmax_function", "anchor_text": "Wikipedia", "paragraph_index": 11}, {"url": "https://yinwenpeng.wordpress.com/2013/09/26/hierarchical-softmax-in-neural-network-language-model/", "anchor_text": "this", "paragraph_index": 13}, {"url": "https://arxiv.org/abs/1310.4546", "anchor_text": "3", "paragraph_index": 14}, {"url": "https://deepnote.com/project/a0986055-32dd-4f1e-b536-e62a47a91318#%2FLesson%202.ipynb", "anchor_text": "here", "paragraph_index": 16}, {"url": "https://radimrehurek.com/gensim/", "anchor_text": "gensim", "paragraph_index": 17}, {"url": "https://radimrehurek.com/gensim/models/word2vec.html", "anchor_text": "ource", "paragraph_index": 20}, {"url": "https://nlp.stanford.edu/pubs/glove.pdf", "anchor_text": "GloVe: Global Vectors for Word Representation", "paragraph_index": 21}, {"url": "https://nlp.stanford.edu/projects/glove/", "anchor_text": "Stanford GloVe", "paragraph_index": 22}, {"url": "https://radimrehurek.com/gensim/scripts/glove2word2vec.html", "anchor_text": "script", "paragraph_index": 27}, {"url": "https://arxiv.org/abs/1601.03764", "anchor_text": "4", "paragraph_index": 30}, {"url": "https://twitter.com/ogaihtcandido", "anchor_text": "Twitter", "paragraph_index": 33}], "all_paragraphs": ["In the previous post, we introduced NLP. To find out word meanings with the Python programming language, we used the NLTK package and worked our way into word embeddings using the gensim package and Word2vec.", "Since we only touched the Word2Vec technique from a 10,000-feet overview, we are now going to dive deeper into the training method to create a Word2vec model.", "The Word2vec (Mikolov et al. 2013)[1][2] is not a singular technique or algorithm. It\u2019s actually a family of neural network architectures and optimization techniques that can produce good results learning embeddings for large datasets.", "The network architectures are shallow, composed of two layers, and are trained to produce vector representations of words given their context.", "The two model variations that can be used are:", "The CBOW model is based on trying to predict a central word from the context words around it.", "We select a few words from a fixed-size window \u2014 the authors recommend this technique around a size of 5 \u2014 create a dictionary containing the words and their frequencies, and train the model by predicting the central word from the bag of words. The CBOW model doesn\u2019t take into consideration the order of the words inside the \u201cbag.\u201d", "With the Skip-gram model, we predict outside words given a central context word. It works in the opposite way of the CBOW model. With this method, the authors recommend using a window of size 10.", "On performance and accuracy: The CBOW model is faster than the Skip-gram, but the Skip-gram architecture works better with infrequent words.", "While the word embeddings created by the network can express the relationships between words, the network itself presents scalability issues. Depending on the vocabulary size, the number of operations needed to calculate the network's output layer is huge. Here are some techniques that are frequently used with Word2vec networks:", "The hierarchical softmax technique, proposed by Morin and Bengio[1], is applied due to the sheer size of regular vocabularies. In a regular neural network output layer, using the softmax function, the computing power needed to address the probability distribution of a full-sized vocabulary in any given language would be extremely large. We can formalize this by giving a size V vocabulary; we can denote the complexity using O(V).", "The softmax function is a generalization of the logistic function to multiple dimensions. It is used in multinomial logistic regression and is often used as the last activation function of a neural network to normalize the output of a network to a probability distribution over predicted output classes, based on Luce\u2019s choice axiom. \u2014 Wikipedia (in case you were wondering)", "With hierarchical softmax, the complexity is O(log2(V)) instead of O(V). That is achieved through the use of a multi-layer binary tree to calculate the probability of each word.", "If you are searching for a more technical, in-depth explanation, I recommend you this blog post.", "The intuition behind negative sampling, presented by Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean[3] is only to update a subset of weights in the training process, affecting only the target \u2014 positive \u2014 word and a few of the non-related \u2014 negative \u2014 words, chosen using a \u201cunigram distribution,\u201d in which more frequent words are preferred to be selected as negative examples.", "Using negative sampling, the computational cost is dramatically lower than the regular softmax version since it transforms a multi-classification task into a few binary classification tasks.", "You can duplicate my Deepnote notebook here and follow me as I walk through this project for the best experience.", "We will use gensim, a Python library, to create different Word2vec models from the same corpus, just passing different parameters to the Word2Vec class constructor.", "First, we import the necessary packages and download the corpus:", "Then we can create different Word2Vec models using the downloaded corpus and different parameters:", "The following parameters are passed to the constructor to define the training algorithm and optimization technique used (source) :", "GloVe: Global Vectors for Word Representation, presented by Jeffrey Pennington, Richard Socher, and Christopher D. Manning, is another model mainly based on word embeddings.", "GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space. \u2014 Stanford GloVe", "The main intuition is to scan through the whole corpus and compute the co-occurrence statistics for each word given a context. You can picture a matrix, with rows being the words and columns being the different contexts. Then you would reduce the dimensionality of each row to represent a word vector by factoring the matrix.", "GloVe combines two model families: the local context window method and the global matrix factorization method.", "The main difference between the two is that while the Word2vec model uses local contexts and a shallow neural network, the GloVe model is based on local and global word co-occurrence and uses the matrix factorization method.", "To use GloVe with gensim is really easy. You can use the api package to download a trained GloVe model.", "You can also convert a GloVe model to a Word2Vec model in gensim using the glove2word2vec script.", "Now we know how to create and use word embeddings created by the Word2vec and GloVe models. But still \u2014 are those vectors enough to represent words accurately in different contexts?", "Common or long-lived words can have several meanings. How can we create embeddings that can capture all meanings from a word?", "Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, Andrej Risteski [4] propose a solution by representing different senses of the same word using a linear superposition \u2014 meaning the creation of a word embedding based on a weighted average of each sense embedding and its frequency.", "Considering that the vector embedding space is high dimensional and sparse, we can reconstruct the different sense vectors from just the weighted average \u2014 or the linear superposition \u2014 of the senses.", "Next, we will discuss word window classification, neural networks, and PyTorch, topics of the Stanford course\u2019s second lecture. I hope you enjoyed reading this post.", "If you did, consider following me on Twitter.", "Thank you for your time. Take care, and keep coding!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "I help people to use their products and services data intelligently @eximiaco"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F7447f2c12b36&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearn-nlp-the-stanford-way-lesson-2-7447f2c12b36&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearn-nlp-the-stanford-way-lesson-2-7447f2c12b36&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearn-nlp-the-stanford-way-lesson-2-7447f2c12b36&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearn-nlp-the-stanford-way-lesson-2-7447f2c12b36&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----7447f2c12b36--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----7447f2c12b36--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://tgcandido.medium.com/?source=post_page-----7447f2c12b36--------------------------------", "anchor_text": ""}, {"url": "https://tgcandido.medium.com/?source=post_page-----7447f2c12b36--------------------------------", "anchor_text": "Thiago Candido"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9d2e6b913e29&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearn-nlp-the-stanford-way-lesson-2-7447f2c12b36&user=Thiago+Candido&userId=9d2e6b913e29&source=post_page-9d2e6b913e29----7447f2c12b36---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7447f2c12b36&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearn-nlp-the-stanford-way-lesson-2-7447f2c12b36&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7447f2c12b36&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearn-nlp-the-stanford-way-lesson-2-7447f2c12b36&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@kellysikkema?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Kelly Sikkema"}, {"url": "https://unsplash.com/s/photos/note?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://towardsdatascience.com/learn-nlp-the-stanford-way-lesson-1-3f1844265760", "anchor_text": "Learn NLP the Stanford way \u2014 Lesson 1An NLP introduction, Word Vectors, and an invitation for youtowardsdatascience.com"}, {"url": "https://arxiv.org/abs/1301.3781", "anchor_text": "[1]"}, {"url": "https://arxiv.org/abs/1310.4546", "anchor_text": "[2]"}, {"url": "https://www.researchgate.net/publication/332543231_Comparative_Study_of_Word_Embeddings_Models_and_Their_Usage_in_Arabic_Language_Applications", "anchor_text": "Source"}, {"url": "https://en.wikipedia.org/wiki/Logistic_function", "anchor_text": "logistic function"}, {"url": "https://en.wikipedia.org/wiki/Multinomial_logistic_regression", "anchor_text": "multinomial logistic regression"}, {"url": "https://en.wikipedia.org/wiki/Activation_function", "anchor_text": "activation function"}, {"url": "https://en.wikipedia.org/wiki/Artificial_neural_network", "anchor_text": "neural network"}, {"url": "https://en.wikipedia.org/wiki/Probability_distribution", "anchor_text": "probability distribution"}, {"url": "https://en.wikipedia.org/wiki/Luce%27s_choice_axiom", "anchor_text": "Luce\u2019s choice axiom"}, {"url": "https://en.wikipedia.org/wiki/Softmax_function", "anchor_text": "Wikipedia"}, {"url": "https://fasttext.cc/docs/en/english-vectors.html", "anchor_text": "libraries"}, {"url": "https://qr.ae/pNaWBW", "anchor_text": "Source"}, {"url": "https://yinwenpeng.wordpress.com/2013/09/26/hierarchical-softmax-in-neural-network-language-model/", "anchor_text": "this"}, {"url": "https://arxiv.org/abs/1310.4546", "anchor_text": "3"}, {"url": "https://stackoverflow.com/a/56401065", "anchor_text": "Source"}, {"url": "https://deepnote.com/project/a0986055-32dd-4f1e-b536-e62a47a91318#%2FLesson%202.ipynb", "anchor_text": "here"}, {"url": "https://radimrehurek.com/gensim/", "anchor_text": "gensim"}, {"url": "https://radimrehurek.com/gensim/models/word2vec.html", "anchor_text": "ource"}, {"url": "https://nlp.stanford.edu/pubs/glove.pdf", "anchor_text": "GloVe: Global Vectors for Word Representation"}, {"url": "https://nlp.stanford.edu/projects/glove/", "anchor_text": "Stanford GloVe"}, {"url": "https://radimrehurek.com/gensim/scripts/glove2word2vec.html", "anchor_text": "script"}, {"url": "https://arxiv.org/abs/1601.03764", "anchor_text": "4"}, {"url": "https://twitter.com/ogaihtcandido", "anchor_text": "Twitter"}, {"url": "https://web.stanford.edu/class/cs224n/slides/cs224n-2020-lecture02-wordvecs2.pdf", "anchor_text": "CS 224N Lecture 2 Slides"}, {"url": "https://www.youtube.com/watch?v=kEMJRjEdNzM", "anchor_text": "CS 224N Lecture 2 Video"}, {"url": "https://deepnote.com/", "anchor_text": "Deepnote"}, {"url": "https://radimrehurek.com/gensim/", "anchor_text": "Gensim"}, {"url": "https://medium.com/tag/python?source=post_page-----7447f2c12b36---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----7447f2c12b36---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----7447f2c12b36---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/nlp?source=post_page-----7447f2c12b36---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/programming?source=post_page-----7447f2c12b36---------------programming-----------------", "anchor_text": "Programming"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7447f2c12b36&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearn-nlp-the-stanford-way-lesson-2-7447f2c12b36&user=Thiago+Candido&userId=9d2e6b913e29&source=-----7447f2c12b36---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7447f2c12b36&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearn-nlp-the-stanford-way-lesson-2-7447f2c12b36&user=Thiago+Candido&userId=9d2e6b913e29&source=-----7447f2c12b36---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7447f2c12b36&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearn-nlp-the-stanford-way-lesson-2-7447f2c12b36&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----7447f2c12b36--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F7447f2c12b36&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearn-nlp-the-stanford-way-lesson-2-7447f2c12b36&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----7447f2c12b36---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----7447f2c12b36--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----7447f2c12b36--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----7447f2c12b36--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----7447f2c12b36--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----7447f2c12b36--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----7447f2c12b36--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----7447f2c12b36--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----7447f2c12b36--------------------------------", "anchor_text": ""}, {"url": "https://tgcandido.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://tgcandido.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Thiago Candido"}, {"url": "https://tgcandido.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "319 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9d2e6b913e29&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearn-nlp-the-stanford-way-lesson-2-7447f2c12b36&user=Thiago+Candido&userId=9d2e6b913e29&source=post_page-9d2e6b913e29--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F66796b3e33e0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearn-nlp-the-stanford-way-lesson-2-7447f2c12b36&newsletterV3=9d2e6b913e29&newsletterV3Id=66796b3e33e0&user=Thiago+Candido&userId=9d2e6b913e29&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}