{"url": "https://towardsdatascience.com/a-pair-of-interrelated-neural-networks-in-dqn-f0f58e09b3c4", "time": 1683004923.943553, "path": "towardsdatascience.com/a-pair-of-interrelated-neural-networks-in-dqn-f0f58e09b3c4/", "webpage": {"metadata": {"title": "A pair of interrelated neural networks in Deep Q-Network | by Rafael Stekolshchik | Towards Data Science", "h1": "A pair of interrelated neural networks in Deep Q-Network", "description": "In DQN and Double DQN models, comparing two interrelated neural networks is crucial. Last section contains several Python code tips."}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/abs/1312.5602", "anchor_text": "Playing Atari with Deep RL", "paragraph_index": 2}, {"url": "https://www.nature.com/articles/nature14236", "anchor_text": "DeepMind demonstrated", "paragraph_index": 2}, {"url": "https://github.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/blob/master/Cartpole-Deep-Q-Learning/CartPole-v0_DQN_Pytorch.ipynb", "anchor_text": "CartPole", "paragraph_index": 9}, {"url": "https://www.ri.cmu.edu/publications/issues-in-using-function-approximation-for-reinforcement-learning/", "anchor_text": "Thrun and Schwartz (1993)", "paragraph_index": 18}, {"url": "https://arxiv.org/abs/1509.06461", "anchor_text": "Hasselt et. al.,", "paragraph_index": 18}, {"url": "https://github.com/Rafael1s/Deep-Reinforcement-Learning-Udacity", "anchor_text": "github directory", "paragraph_index": 41}, {"url": "https://towardsdatascience.com/how-the-bellman-equation-works-in-deep-reinforcement-learning-5301fe41b25a", "anchor_text": "my previous paper", "paragraph_index": 41}], "all_paragraphs": ["We will follow a few steps that have been taken in the fight against correlations and overestimations in the development of the DQN and Double DQN algorithms. As an example of the DQN and Double DQN applications, we present the training results for the CartPole-v0 and CartPole-v1 environments. The last section contains some tips on PyTorch tensors.", "From lookup table to neural network", "Until around 2013, Deep Q-Learning was a Reinforcement Learning (RL) method that worked only with a lookup table. The success of neural networks in Computer Vision has sparked interest in trying them out in RL. The paper \u201cPlaying Atari with Deep RL\u201d (V.Mnih et al., 2013, DeepMind) presented the first successful Deep Learning model using a neural network function approximation. In 2015, DeepMind demonstrated that the Deep Q-network agent, receiving only the row pixel data and the game score as inputs, was able to exceed the performance of all previous algorithms.", "After these two DeepMind works, the lookup tables were replaced by neural networks and a Q-Learning became a Deep Q-Learning, or, equivalently, Deep Q-Network (DQN). In fact, it was a breakthrough in RL agent training.", "The DQN is the algorithm that combines Q-learning with neural networks.", "Reinforcement Learning is known as unstable when a neural network is used as a function approximation. The reasons of this instability are as follows:", "used in the DQN key equation, eq. (2).", "Pair of Q-Networks: local and target", "One of the important components of the DQN algorithm in the fight against correlations is the use of the target network (q_target). The target network, (with parameters \u03b8*) is the same as the local network except that its parameters are copied every \u03c4 steps from the local network (q_local), so that then \u03b8*_t = \u03b8_t, and kept fixed on all other steps. In looks as follows:", "In CartPole it was set \u03c4 = TARGET_UPDATE = 10.", "Comparing two neural networks representing the same Q-table and finding the point at which these networks are very close is the basic part of the DQN algorithm.", "By networks q_local and q_target the tensors current estimate Q(s_t, a_t) and alternative estimate G_t, see (1), are calculated. Further, in the function learn() of the class Agent", "Experience replay \u2014 a biologically inspired mechanism", "Another thing that DQN uses to reduce correlations is the experience replay mechanism, which puts data into a specific memory storage and randomly receives data from the memory storage.", "The DQN \u03b5-greedy mechanism promotes a reasonable proportion between exploration and exploitation. This mechanism provides the parameter \u03b5, where 0 < \u03b5 < 1, which allows controlling this proportion. For any \u03b5, with probability 1-\u03b5, exploitation is chosen. The exploitation means, that the agent finds the following action by maximizing the Q-value over all possible actions for the given state, see the relevant lines from the function get_action():", "But how to choose epsilon? One popular option is the annealing \u03b5-greedy algorithm. For any episode i, the action is greedy with probability 1-\u03b5.", "In eq.(3), \u03b5_m is the minimal value of \u03b5 , which must be achieved in the episode number M\u03b5. By (3) if i = 0 we have\u03b5 = 1; if i = 1 then \u03b5 is close to 1. For example, for M\u03b5 = 50 and \u03b5_m =0.01, we have \u03b5 = 0.98. Then exploitation is selected with probability 0.02, and exploration is chosen with probability 0.98, i.e., a quite random value for action number will be obtained with probability 0.98. Thus, for first episodes, the action will be chosen very randomly, this is exploration.", "If i = M\u03b5, we get \u03b5 = \u03b5_m. For our example,\u03b5 = 0.01. Then exploitation is chosen with probability 0.99, and random values for the action number will be obtained only for 1% of cases. For episodes from 0 to M\u03b5, the value of \u03b5 is decreasing from 1 to \u03b5_m, and fixed at \u03b5_m thereafter.", "The DQN algorithm is known to overestimate action values. For the first time, overestimations of DQN were investigated by Thrun and Schwartz (1993). They give an example in which these overestimations asymptotically lead to sub-optimal policies. Suppose, for some state s\u2019 , the true value on all actions a is Q(s\u2019, a) = 0 and the estimate value for Q-values are distributed some above and below zero. Then the maximum of these estimates > 0 that is the overestimate for true value. In 2015, (Hasselt et. al., DeepMind) shown that estimation errors can drive the estimates up and away from the true optimal values. They supposed the solution that reduces the overestimation: Double DQN.", "What is the reason of overestimations? The problem is in max operator in eqs. (1) and (2). Suppose, the evaluation value for Q(S_t, a) is already overestimated. Then the action value obtained in eqs. (1) or (2) becomes even more overestimated. The TD-target G_t in eq.(1) can be rewritten to TD-target G^Q_t as follows:", "Here, \u03b8_t is the set of weights of the network q_target, see Table 1 above.", "The Double DQN solution is in decoupling the selection of action argmax_a from the evaluation Q(S_{t+1}, argmax_a), see (4). This is done using another network with the set of weights \u03b8\u2019_t :", "By (5) the greedy policy (argmax) is estimated by the neural network q_local (weights \u03b8_t). The network q_target (weights \u03b8'_t) is used to fairly evaluate of this policy. This solution is the main idea of the Double DQN.", "Loss function for Double DQN agent", "For the Double DQN agent case, the tensors Q(s_t, a_t) and G_t are calculated as for the DQN, see Tables 1 ans 2.", "The main difference between Table 1 and Table 2 is the calculation of the tensor Q_target_next (evaluation of the Q-value):", "A pole is attached by an joint to a cart, which moves along a track. The system is controlled by applying a force of +1 or -1 to the cart.", "The CartPole is a binary classification problem", "The dimension of the CartPole observation space is 4, since there are 4 features that form the input: cart coordinate, velocity, pole\u2019s angle from vertical and its derivative (pole \u201cfalling\u201d velocity ). The CartPole is a binary classification problem because at each time step the agent chooses between moving left or right. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every time step that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center.", "The environment CartPole-v0 is considered solved if the getting average reward > 195 over 100 consecutive trials; CartPole-v1 is considered solved if the getting average reward > 475 over 100 consecutive trials.", "Training experiments for CartPole-v0 and CartPole-v1", "Here are the results of my experiments with DQN and Double DQN, obtained during training CartPole-v0 and CartPole-v1. For all cases, LEARNING_RATE = 0.001. The greedy parameter\u03b5is changed from 1 to \u03b5_m = 0.01", "For CartPole-v0 we put M\u03b5= 200;for CartPole-v1, we put M\u03b5= 150. Recall that M\u03b5 is the number of episode where \u03b5 achieves the minimal value\u03b5_m.", "If M\u03b5 is set too large, then the choice of \u03b5 will be performed for a long time in conditions of high probability (> \u03b5_m) of exploration. In other words, for a long time \u03b5 will be carried out without information accumulated in the neural network. This means that choosing between moving left or right , we can be mistaken in half the cases for a very long time.", "If M\u03b5 is set too small, then the choice of \u03b5 will be performed for a long time under conditions of high probability (= \u03b5_m) of exploitation. This can be very bad in the early stages of neural network training because the choice of action using argmax will be made from the neural network, which is still very crude. Then in many cases, the chosen action will be mistaken.", "In developing the DQN and Double DQN algorithms, three steps were taken in the fight against correlations and overestimations: (1) target and local networks, (2) experience replay mechanism, (3) decoupling the selection from the evaluation. These mechanisms have been developed with substantial use of two interrelated neural networks.", "The PyTorch function no_grad() excludes some elements from the gradient calculation. It is used when there is confidence that the back-propagation process is not performed. This function reduces memory consumption, see get_action(). A similar effect occurs when using the detach() function. The with statement clarifies code corresponding to try...finally blocks.", "clears old gradients from the last step (otherwise the gradients will be just accumulated from all loss.backward() calls)", "This function returns a new tensor, the same as the original tensor, but of a different shape. Trying to remove the function view(1,1) in get_action(), we get the different shapes of the action tensor in two branches of get_action().Then in learn() function we get batch.action that consists of tensors of various shapes. This is failure. The function view(1,1) changes the shape from tensor([a]) to tensor([[a]]). Parameters 1,1 mean the number of elements in each dimension. For example, view(1,1,1,1,1) meanstensor([[[[[a]]]]]).", "Concatenates the given tuple of tensors into the single tensor. For example, in learn() function, batch.state is the tuple of 64 tensors of shape [1,4]. Function torch.cat transforms this tuple into the single tensor states of the shape [64,4] as follows:", "Why do we use reshape(-1) to find the Q_targets_nexttensor, see Table 2? In learn() function we compare two tensors: Q_targets.unsqueeze(1) and Q_expected. If we don\u2019t use reshape function, then by Table 3 these tensors have different shape, then comparison is failure.", "For other Deep Reinforcement Learning projects, see my github directory. For interrelations between the Bellman equation and neural networks, see my previous paper. The same article provides more tips on PyTorch.", "[5] S.Karagiannakos, Taking Deep Q Networks a step further, (2018), TheAISummer", "[7] R.Stekolshchik, How does the Bellman equation work in Deep RL?, (2020), TowardsDataScience", "[9] S.Thrun and A.Schwartz, Issues in Using Function Approximation for Reinforcement Learning, (1993),  Carnegie Mellon University, The Robotics Institute [10] F.Mutsch, CartPole with Q-Learning \u2014 First experiences with OpenAI Gym (2017), muetsch.io", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Ph.D. in Math, Algorithm and SW developer, Researcher. Fan of Deep Learning and Neural Networks. @r.stekol"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Ff0f58e09b3c4&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-pair-of-interrelated-neural-networks-in-dqn-f0f58e09b3c4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-pair-of-interrelated-neural-networks-in-dqn-f0f58e09b3c4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-pair-of-interrelated-neural-networks-in-dqn-f0f58e09b3c4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-pair-of-interrelated-neural-networks-in-dqn-f0f58e09b3c4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----f0f58e09b3c4--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f0f58e09b3c4--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://r-stekol.medium.com/?source=post_page-----f0f58e09b3c4--------------------------------", "anchor_text": ""}, {"url": "https://r-stekol.medium.com/?source=post_page-----f0f58e09b3c4--------------------------------", "anchor_text": "Rafael Stekolshchik"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F57ce87a178e5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-pair-of-interrelated-neural-networks-in-dqn-f0f58e09b3c4&user=Rafael+Stekolshchik&userId=57ce87a178e5&source=post_page-57ce87a178e5----f0f58e09b3c4---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff0f58e09b3c4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-pair-of-interrelated-neural-networks-in-dqn-f0f58e09b3c4&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff0f58e09b3c4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-pair-of-interrelated-neural-networks-in-dqn-f0f58e09b3c4&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://arxiv.org/abs/1312.5602", "anchor_text": "Playing Atari with Deep RL"}, {"url": "https://www.nature.com/articles/nature14236", "anchor_text": "DeepMind demonstrated"}, {"url": "https://github.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/blob/master/Cartpole-Deep-Q-Learning/CartPole-v0_DQN_Pytorch.ipynb", "anchor_text": "CartPole"}, {"url": "https://www.ri.cmu.edu/publications/issues-in-using-function-approximation-for-reinforcement-learning/", "anchor_text": "Thrun and Schwartz (1993)"}, {"url": "https://arxiv.org/abs/1509.06461", "anchor_text": "Hasselt et. al.,"}, {"url": "https://github.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/tree/master/Cartpole-Deep-Q-Learning", "anchor_text": "CartPole with DQN"}, {"url": "https://github.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/tree/master/Cartpole-Double-Deep-Q-Learning", "anchor_text": "CartPole with Double DQN"}, {"url": "https://github.com/Rafael1s/Deep-Reinforcement-Learning-Udacity", "anchor_text": "github directory"}, {"url": "https://towardsdatascience.com/how-the-bellman-equation-works-in-deep-reinforcement-learning-5301fe41b25a", "anchor_text": "my previous paper"}, {"url": "https://towardsdatascience.com/dqn-part-1-vanilla-deep-q-networks-6eb4a00febfb", "anchor_text": "https://towardsdatascience.com/dqn-part-1-vanilla-deep-q-networks-6eb4a00febfb"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----f0f58e09b3c4---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/deep-q-learning?source=post_page-----f0f58e09b3c4---------------deep_q_learning-----------------", "anchor_text": "Deep Q Learning"}, {"url": "https://medium.com/tag/pytorch?source=post_page-----f0f58e09b3c4---------------pytorch-----------------", "anchor_text": "Pytorch"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----f0f58e09b3c4---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/dqn?source=post_page-----f0f58e09b3c4---------------dqn-----------------", "anchor_text": "Dqn"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff0f58e09b3c4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-pair-of-interrelated-neural-networks-in-dqn-f0f58e09b3c4&user=Rafael+Stekolshchik&userId=57ce87a178e5&source=-----f0f58e09b3c4---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff0f58e09b3c4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-pair-of-interrelated-neural-networks-in-dqn-f0f58e09b3c4&user=Rafael+Stekolshchik&userId=57ce87a178e5&source=-----f0f58e09b3c4---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff0f58e09b3c4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-pair-of-interrelated-neural-networks-in-dqn-f0f58e09b3c4&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f0f58e09b3c4--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Ff0f58e09b3c4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-pair-of-interrelated-neural-networks-in-dqn-f0f58e09b3c4&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----f0f58e09b3c4---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----f0f58e09b3c4--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----f0f58e09b3c4--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----f0f58e09b3c4--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----f0f58e09b3c4--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----f0f58e09b3c4--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----f0f58e09b3c4--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----f0f58e09b3c4--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----f0f58e09b3c4--------------------------------", "anchor_text": ""}, {"url": "https://r-stekol.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://r-stekol.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Rafael Stekolshchik"}, {"url": "https://r-stekol.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "306 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F57ce87a178e5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-pair-of-interrelated-neural-networks-in-dqn-f0f58e09b3c4&user=Rafael+Stekolshchik&userId=57ce87a178e5&source=post_page-57ce87a178e5--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fbc96182db31e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-pair-of-interrelated-neural-networks-in-dqn-f0f58e09b3c4&newsletterV3=57ce87a178e5&newsletterV3Id=bc96182db31e&user=Rafael+Stekolshchik&userId=57ce87a178e5&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}