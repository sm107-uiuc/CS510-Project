{"url": "https://towardsdatascience.com/attaining-attention-in-deep-learning-a712f93bdb1e", "time": 1683006999.142227, "path": "towardsdatascience.com/attaining-attention-in-deep-learning-a712f93bdb1e/", "webpage": {"metadata": {"title": "Understanding Attention In Deep Learning (NLP) | By Ria K. | Towards Data Science", "h1": "Understanding Attention In Deep Learning", "description": "In this beginner friendly article, I will discuss how we gave an ML model the ability to focus aka attention and its impact on performance on various ML problems as we discuss the paper \"Attention is All you Need\"."}, "outgoing_paragraph_urls": [{"url": "https://medium.com/analytics-vidhya/the-price-of-free-d662016a5229?source=friends_link&sk=924bc170af02df5b9c38b6e99965a136", "anchor_text": "previous articles", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/back-propagation-721bfcc94e34?source=friends_link&sk=acbff34da1a05019364dff8893317331", "anchor_text": "here", "paragraph_index": 20}, {"url": "https://towardsdatascience.com/transformers-89034557de14", "anchor_text": "Transformer", "paragraph_index": 22}, {"url": "https://medium.com/@ria.kulshrestha16/keeping-up-with-the-berts-5b7beb92766", "anchor_text": "BERT", "paragraph_index": 22}, {"url": "https://medium.com/@ria.kulshrestha16", "anchor_text": "here", "paragraph_index": 24}, {"url": "https://twitter.com/ree_____ree", "anchor_text": "Twitter", "paragraph_index": 25}], "all_paragraphs": ["In one of my previous articles, I talk about how attention is one of the most important assets \u2014 we as humans \u2014 have. How we choose to spend our time and on what we choose to focus on during that time determines the outcome for almost all our endeavors.", "In this article, we will discuss how we gave an ML model the ability to focus and the impact it had on its performance.", "Let\u2019s go over a task \u2014 popularly solved in many NLP models \u2014 translation. A word to word translation doesn\u2019t work in most cases as most languages don\u2019t share a common sentence structure. A simple example:", "The way NLP models usually go about such tasks is by capturing all the information in the input sentence \u2014the details of objects, how objects are related to each other \u2014 in an intermediate state and then using this intermediary information and expressing it in the output language. The size of the vector used for this intermediary state i.e. the state capturing all the information from input sequence \u2014 before we start decoding the output sequence\u2014 is fixed.", "The intermediate state plays a paramount role in this entire process. Its ability to remember all the information passed in the input sentence is critical to the quality of decoded output. Whether it is a translation or a QnA task, in which the input is a question and a paragraph and the model needs to predict the answer to that question based on the paragraph or, any other sequence to sequence modeling operation, the intermediary state continues to be the most crucial piece of the puzzle.", "It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way \u2014 in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only.", "- A Tale of Two Cities, Charles Dickens.", "Now I ask you to just memorize, not even translate, this sentence after going over it once, from left to right, and I limit the number of words that you can write in your notes. Not so easy, is it?", "How about memorizing this sentence about cars brimming with facts that I totally made up? Disclaimer: The numbers and use cases appearing in the following statement are fictitious, any sense they make is purely coincidental.", "This car is 2m in height \u2014 so lean on my friend, 12m in width \u2014 not very hug-able unfortunately, with a 8m wheelbase \u2014 whatever use that information is for, a turning radius of just 0.1m \u2014 making it easier to turn away from all your problems, a boot-space of 200 litres \u2014 for all the luggage you\u2019ll carry for a trip and never use, a ground clearance of 0.25m \u2014 in case you ever seek refuge under the car, a 6 cylinder engine with 5 valves \u2014 a spec you\u2019ll only ever use while showing off and lastly has a dual overhead camshaft because big cars comes with big words. -Yours truly :)", "Jokes aside, in this case not only you have to remember all the numbers I randomly threw in there, but also the features \u2014shown in bold \u2014 it corresponds to. Mess it up at one place and you have got it all wrong.", "As evident by now, in cases with very long sentences as input, the intermediate state fails and is not sufficient to capture all the information. Often it will forget the first part by the time it completes processing the whole input. To improve the model\u2019s performance and relieve the intermediate state from being solely responsible for encoding all the information available to the decoder in a fixed length vector and from being a potential bottleneck, we use the concept of attention. With this new approach, the information can be spread throughout the sequence of annotations \u2014 encoder hidden states, which can be selectively retrieved by the decoder accordingly.", "Attention mechanism tries to overcome the information bottleneck of the intermediary state by allowing the decoder model to access all the hidden states, rather than a single vector \u2014 aka intermediary state \u2014 build out of the encoder\u2019s last hidden state, while predicting each output.", "The input to a cell in decoder now gets the following values:", "The global alignment weights are important because they tell us which annotations(s) to focus on for the next output. The weights will and should vary in each time steps of the decoder model. They are calculated by using a feed forward neural network.", "The sequence of steps is as follows:", "We know we use a feed forward neural network that outputs these global alignment weights \u237a\u2096 \u2c7c. The purpose of these weights is to reflect the importance of each annotation h\u2c7c w.r.t. the previous hidden state in deciding the next state H\u2096. This, in a way, allows the model to decide which parts of the input to attend to. The bigger the weight, the more attention it gets. So the next question is, what are its inputs, and how do we train it?", "We input both decoder hidden state and the annotations in our neural network to predict a single value \u2014 e\u2096 \u2c7c as the authors of the paper liked to call it, the \u201cassociated energy\u201d\u2014 signifying the importance of the annotations in next decoder step H\u2096. We repeat this process for all annotations. Once, we have associated energies corresponding to all annotations, we do a softmax to obtain the global alignment weights \u237a\u2096 \u2c7c.", "Step 2, mentioned before, can now be broken down as follows:", "The alignment model directly computes a soft alignment \u2014 considers all inputs, which allows the gradient of the loss function \u2014 calculated for final outputs of the entire sequence to sequence model \u2014 to be back-propagated through. This gradient is used to train the alignment model as well as the whole translation model jointly.", "Let \u237a\u1d62 \u2c7c be a probability that the target word Y\u1d62 is translated from a source word X\u2c7c . Then, the i-th context vector C\u1d62 is the expected annotation over all the annotations with probabilities \u237a\u1d62 \u2c7c . When loss at each time step is back-propagated, we calculate the gradient for all three inputs \u2014 previous hidden state, the previous output and the context vector. (If this is a little confusing for you, please read more about back-propagation here) All the gradients flowing back in a time step shall be added together before we calculate the gradients for its inputs.", "The gradient at each time step for C\u2096 is used as the loss for the feed forward neural network we use to predict global alignment weights.", "If you liked this article and want to explore more about Transformer and BERT (out now), make sure to check out my articles on the same. \ud83d\ude04\u270c\ufe0f", "I\u2018m glad you made it till the end of this article. \ud83c\udf89I hope your reading experience was as enriching as the one I had writing this.\ud83d\udc96", "Do check out my other articles here. \ud83d\udd0d", "If you want to reach out to me, my medium of choice would be Twitter. \ud83d\udcec", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fa712f93bdb1e&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fattaining-attention-in-deep-learning-a712f93bdb1e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fattaining-attention-in-deep-learning-a712f93bdb1e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fattaining-attention-in-deep-learning-a712f93bdb1e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fattaining-attention-in-deep-learning-a712f93bdb1e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----a712f93bdb1e--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a712f93bdb1e--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://ria-kulshrestha.medium.com/?source=post_page-----a712f93bdb1e--------------------------------", "anchor_text": ""}, {"url": "https://ria-kulshrestha.medium.com/?source=post_page-----a712f93bdb1e--------------------------------", "anchor_text": "Ria Kulshrestha"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F406aa3cbd38e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fattaining-attention-in-deep-learning-a712f93bdb1e&user=Ria+Kulshrestha&userId=406aa3cbd38e&source=post_page-406aa3cbd38e----a712f93bdb1e---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa712f93bdb1e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fattaining-attention-in-deep-learning-a712f93bdb1e&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa712f93bdb1e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fattaining-attention-in-deep-learning-a712f93bdb1e&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@haneenkrimly?utm_source=medium&utm_medium=referral", "anchor_text": "Haneen Krimly"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://medium.com/analytics-vidhya/the-price-of-free-d662016a5229?source=friends_link&sk=924bc170af02df5b9c38b6e99965a136", "anchor_text": "previous articles"}, {"url": "https://arxiv.org/pdf/1409.0473.pdf", "anchor_text": "the original paper"}, {"url": "https://arxiv.org/pdf/1409.0473.pdf", "anchor_text": "The original paper."}, {"url": "https://towardsdatascience.com/back-propagation-721bfcc94e34?source=friends_link&sk=acbff34da1a05019364dff8893317331", "anchor_text": "here"}, {"url": "https://towardsdatascience.com/transformers-89034557de14", "anchor_text": "Transformers"}, {"url": "https://medium.com/@ria.kulshrestha16/keeping-up-with-the-berts-5b7beb92766", "anchor_text": "Google\u2019s BERT"}, {"url": "https://arxiv.org/pdf/1907.11692.pdf", "anchor_text": "RoBERTa"}, {"url": "https://towardsdatascience.com/transformers-89034557de14", "anchor_text": "TransformersOr as I like to call it Attention on Steroids. \ud83d\udc89\ud83d\udc8atowardsdatascience.com"}, {"url": "https://towardsdatascience.com/keeping-up-with-the-berts-5b7beb92766", "anchor_text": "Keeping up with the BERTsThe most popular family in NLP town.towardsdatascience.com"}, {"url": "https://towardsdatascience.com/transformers-89034557de14", "anchor_text": "Transformer"}, {"url": "https://medium.com/@ria.kulshrestha16/keeping-up-with-the-berts-5b7beb92766", "anchor_text": "BERT"}, {"url": "https://arxiv.org/pdf/1409.0473.pdf", "anchor_text": "Original Paper"}, {"url": "https://towardsdatascience.com/intuitive-understanding-of-attention-mechanism-in-deep-learning-6c9482aecf4f", "anchor_text": "ntuitive Understanding Of Attention Mechanism In Deep Learning"}, {"url": "https://www.analyticsvidhya.com/blog/2019/11/comprehensive-guide-attention-mechanism-deep-learning/", "anchor_text": "A Comprehensive Guide to Attention Mechanism in Deep Learning for Everyone"}, {"url": "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/nmt_with_attention/nmt_with_attention.ipynb", "anchor_text": "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/nmt_with_attention/nmt_with_attention.ipynb"}, {"url": "https://towardsdatascience.com/back-propagation-721bfcc94e34", "anchor_text": "Yes, you should listen to Andrej Karpathy, and understand Back propagation"}, {"url": "https://towardsdatascience.com/evaluation-of-an-nlp-model-latest-benchmarks-90fd8ce6fae5", "anchor_text": "Evaluation of an NLP model \u2014 latest benchmarks"}, {"url": "https://towardsdatascience.com/machine-learning-in-computational-fluid-dynamics-7018941414b9", "anchor_text": "Machine Learning in Computational Fluid Dynamics"}, {"url": "https://medium.com/@ria.kulshrestha16/keeping-up-with-the-berts-5b7beb92766", "anchor_text": "Keeping Up With The BERTs"}, {"url": "https://medium.com/@ria.kulshrestha16", "anchor_text": "here"}, {"url": "https://twitter.com/ree_____ree", "anchor_text": "Twitter"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----a712f93bdb1e---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----a712f93bdb1e---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/ai?source=post_page-----a712f93bdb1e---------------ai-----------------", "anchor_text": "AI"}, {"url": "https://medium.com/tag/attention?source=post_page-----a712f93bdb1e---------------attention-----------------", "anchor_text": "Attention"}, {"url": "https://medium.com/tag/data-science?source=post_page-----a712f93bdb1e---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa712f93bdb1e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fattaining-attention-in-deep-learning-a712f93bdb1e&user=Ria+Kulshrestha&userId=406aa3cbd38e&source=-----a712f93bdb1e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa712f93bdb1e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fattaining-attention-in-deep-learning-a712f93bdb1e&user=Ria+Kulshrestha&userId=406aa3cbd38e&source=-----a712f93bdb1e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa712f93bdb1e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fattaining-attention-in-deep-learning-a712f93bdb1e&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a712f93bdb1e--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fa712f93bdb1e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fattaining-attention-in-deep-learning-a712f93bdb1e&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----a712f93bdb1e---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----a712f93bdb1e--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----a712f93bdb1e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----a712f93bdb1e--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----a712f93bdb1e--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----a712f93bdb1e--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----a712f93bdb1e--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----a712f93bdb1e--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----a712f93bdb1e--------------------------------", "anchor_text": ""}, {"url": "https://ria-kulshrestha.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://ria-kulshrestha.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ria Kulshrestha"}, {"url": "https://ria-kulshrestha.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "768 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F406aa3cbd38e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fattaining-attention-in-deep-learning-a712f93bdb1e&user=Ria+Kulshrestha&userId=406aa3cbd38e&source=post_page-406aa3cbd38e--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fccdcb0ec19a4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fattaining-attention-in-deep-learning-a712f93bdb1e&newsletterV3=406aa3cbd38e&newsletterV3Id=ccdcb0ec19a4&user=Ria+Kulshrestha&userId=406aa3cbd38e&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}