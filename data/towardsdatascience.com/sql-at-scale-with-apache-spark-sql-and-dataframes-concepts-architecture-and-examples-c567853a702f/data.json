{"url": "https://towardsdatascience.com/sql-at-scale-with-apache-spark-sql-and-dataframes-concepts-architecture-and-examples-c567853a702f", "time": 1682993857.223356, "path": "towardsdatascience.com/sql-at-scale-with-apache-spark-sql-and-dataframes-concepts-architecture-and-examples-c567853a702f/", "webpage": {"metadata": {"title": "SQL at Scale with Apache Spark SQL and DataFrames \u2014 Concepts, Architecture and Examples | by Dipanjan (DJ) Sarkar | Towards Data Science", "h1": "SQL at Scale with Apache Spark SQL and DataFrames \u2014 Concepts, Architecture and Examples", "description": "This article covers detailed concepts pertaining to Spark, SQL and DataFrames. Besides this we also cover a hands-on case study around working with SQL at scale using Spark SQL and DataFrames. If the\u2026"}, "outgoing_paragraph_urls": [{"url": "https://opensource.com/", "anchor_text": "opensource.com", "paragraph_index": 0}, {"url": "http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html", "anchor_text": "KDD 99 Cup Data", "paragraph_index": 4}, {"url": "http://databricks.com", "anchor_text": "Databricks Cloud Platform", "paragraph_index": 4}, {"url": "http://people.csail.mit.edu/matei/papers/2015/sigmod_spark_sql.pdf", "anchor_text": "Relational Data Processing in Spark", "paragraph_index": 11}, {"url": "https://code.fb.com/core-data/apache-spark-scale-a-60-tb-production-use-case", "anchor_text": "\u2018Apache Spark @Scale: A 60 TB+ production use case\u2019", "paragraph_index": 11}, {"url": "https://www2.eecs.berkeley.edu/Pubs/TechRpts/2011/EECS-2011-82.pdf", "anchor_text": "\u2018Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing\u2019", "paragraph_index": 14}, {"url": "https://towardsdatascience.com/@faviovazquez?source=post_header_lockup", "anchor_text": "Favio V\u00e1zquez", "paragraph_index": 16}, {"url": "https://towardsdatascience.com/deep-learning-with-apache-spark-part-1-6d397c16abd", "anchor_text": "article on Deep Learning With Apache Spark", "paragraph_index": 16}, {"url": "https://databricks.com/blog/2016/01/04/introducing-apache-spark-datasets.html", "anchor_text": "Spark Dataset", "paragraph_index": 19}, {"url": "https://databricks.com/glossary/what-are-datasets", "anchor_text": "Datasets", "paragraph_index": 19}, {"url": "https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html", "anchor_text": "an excellent article from Databricks!", "paragraph_index": 29}, {"url": "https://databricks.com/try-databricks", "anchor_text": "Try Databricks page", "paragraph_index": 31}, {"url": "https://databricks.com/signup#signup/community", "anchor_text": "sign up", "paragraph_index": 31}, {"url": "https://databricks.com/signup#signup/community", "anchor_text": "community edition", "paragraph_index": 31}, {"url": "https://databricks.com/spark/getting-started-with-apache-spark", "anchor_text": "this short but useful tutorial", "paragraph_index": 33}, {"url": "http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html", "anchor_text": "KDD Cup 1999", "paragraph_index": 34}, {"url": "http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html", "anchor_text": "KDD Cup 1999", "paragraph_index": 35}, {"url": "https://spark.apache.org/docs/latest/rdd-programming-guide.html#resilient-distributed-datasets-rdds", "anchor_text": "Resilient Distributed Dataset (RDD)", "paragraph_index": 40}, {"url": "http://kdd.ics.uci.edu/databases/kddcup99/kddcup.names", "anchor_text": "here", "paragraph_index": 45}, {"url": "http://kdd.ics.uci.edu/databases/kddcup99/task.html", "anchor_text": "here", "paragraph_index": 45}, {"url": "https://attack.mitre.org/techniques/T1188/", "anchor_text": "multihop attacks", "paragraph_index": 66}, {"url": "https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/3137082781873852/3704545280501166/1264763342038607/latest.html", "anchor_text": "import my notebook", "paragraph_index": 71}, {"url": "https://github.com/dipanjanS/data_science_for_all/tree/master/tds_spark_sql_intro", "anchor_text": "my GitHub repository", "paragraph_index": 72}, {"url": "https://docs.databricks.com/spark/latest/spark-sql/index.html", "anchor_text": "Spark SQL from Databricks", "paragraph_index": 73}, {"url": "https://databricks.com/blog/2015/02/02/an-introduction-to-json-support-in-spark-sql.html", "anchor_text": "JSON support in Spark SQL", "paragraph_index": 74}, {"url": "https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html", "anchor_text": "Window Functions in Spark SQL", "paragraph_index": 75}, {"url": "https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/3137082781873852/3704545280501166/1264763342038607/latest.html", "anchor_text": "this link", "paragraph_index": 77}, {"url": "https://github.com/dipanjanS/data_science_for_all/tree/master/tds_spark_sql_intro", "anchor_text": "my GitHub", "paragraph_index": 78}, {"url": "http://nbviewer.jupyter.org/github/dipanjanS/data_science_for_all/blob/master/tds_spark_sql_intro/Working%20with%20SQL%20at%20Scale%20-%20Spark%20SQL%20Tutorial.ipynb", "anchor_text": "Jupyter Notebook", "paragraph_index": 79}, {"url": "https://towardsdatascience.com/", "anchor_text": "TDS", "paragraph_index": 80}, {"url": "https://www.linkedin.com/in/dipanzan/", "anchor_text": "LinkedIn", "paragraph_index": 80}, {"url": "https://www.linkedin.com/in/durba-dutta-bhaumik-44532ab1/", "anchor_text": "Durba", "paragraph_index": 81}], "all_paragraphs": ["This article covers detailed concepts pertaining to Spark, SQL and DataFrames. Besides this we also cover a hands-on case study around working with SQL at scale using Spark SQL and DataFrames. If the article seems a bit overwhelming or really long to you at the first glance, feel free to check out the same in more bite-sized chunks in the following links at opensource.com", "I hope this helps you out on your own journey with Spark and SQL!", "Relational Databases are here to stay, regardless of the hype as well as the advent of newer databases often popularly termed as \u2018NoSQL\u2019 databases. The simple reason is that these databases enforce essential structure, constraints and provide a nice declarative language to query data, which we love \u2014 SQL! However, scale has always been a problem with relational databases. Most enterprises now in the 21st century are loaded with rich data stores and repositories, and want to take maximum advantage of their \u2018Big Data\u2019 for actionable insights. Relational databases might be popular, but they don\u2019t scale very well unless we invest in a proper Big Data management strategy. This involves thinking about potential data sources, data volume, constraints, schemas, ETL (extract-transform-load), access and querying patterns and much more!", "This article will cover some excellent advances made for leveraging the power of relational databases, but \u2018at scale,\u2019 using some of the newer components from Apache Spark \u2014 Spark SQL and DataFrames. Most notably, we will cover the following topics.", "Thus, we will be looking at the major challenges and motivation for people working so hard, and investing time in building new components in Apache Spark, so that we could perform SQL at scale. We will also understand major architecture, interfaces, features and performance benchmarks for Spark SQL and DataFrames. Lastly, but most importantly, we will cover a real-world case study on analyzing intrusion attacks based on KDD 99 Cup Data using Spark SQL and DataFrames by leveraging Databricks Cloud Platform for Spark!", "Relational data stores are easy to build and query. Also, users as well as developers often prefer writing easy to interpret, declarative queries in a human-like readable language such as SQL. However, as data starts increasing in volume and variety, the relational approach does not scale well enough for building big data applications and analytical system. Following are some major challenges.", "Big Data Analytics is not something which was just invented yesterday! We have had success in this domain with Hadoop and the Map-Reduce paradigm. This was powerful, but often slow, and also gave users a low-level, procedural programming interface which required people to write a lot of code for even very simple data transformations. However, once Spark was released, it really revolutionized the way big data analytics was done with focus on in-memory computing, fault tolerance, high-level abstractions and ease of use.", "From then, several frameworks and systems like Hive, Pig and Shark (which evolved into Spark SQL) provided rich relational interfaces and declarative querying mechanisms to Big Data stores. The challenge remained that these tools were either relational or procedural-based and we couldn\u2019t have the best of both worlds.", "However in the real-world, most data and analytical pipelines might involve a combination of relational and procedural code. Thus, forcing users to choose either one ends up complicating things and increasing user efforts in developing, building and maintaining different applications and systems. Apache Spark SQL builds on the previously mentioned SQL-on-Spark effort, called Shark. Instead of forcing users to pick between a relational or a procedural API, Spark SQL tries to enable users to seamlessly intermix the two and perform data querying, retrieval and analysis at scale on Big Data.", "Spark SQL essentially tries to bridge the gap between the two models we mentioned previously \u2014 the relational and procedural models by two major components.", "Essentially, Spark SQL leverages the power of Spark to perform distributed, robust, in-memory computations at massive scale on Big Data. Spark SQL provides state-of-the-art SQL performance, and also maintains compatibility with all existing structures and components supported by Apache Hive (a popular Big Data Warehouse framework) including data formats, user-defined functions (UDFs) and the metastore. Besides this, it also helps in ingesting a wide variety of data formats from Big Data sources and enterprice data warehouses like JSON, Hive, Parquet and so on, and perform a combination of relational and procedural operations for more complex, advanced analytics.", "Let\u2019s look at some of the interesting facts about Spark SQL, it\u2019s usage, adoption and goals, some of which I will shamelessly once again copy from the excellent and original paper on Relational Data Processing in Spark. Spark SQL was first released in May 2014, and is perhaps now one of the most actively developed components in Spark. Apache Spark is definitely the most active open source project for big data processing, with hundreds of contributors. Besides being just an open-source project, Spark SQL has actually started seeing mainstream industry adoption! It has already been deployed in very large scale environments. An excellent case-study has been mentioned by Facebook where they talk about \u2018Apache Spark @Scale: A 60 TB+ production use case\u2019 \u2014 Here, they were doing data preparation for entity ranking and their Hive jobs used to take several days and had many challenges, but they were able to successfully able to scale and increase performance using Spark. Do check out the interesting challenges they faced in this journey!", "Another interesting fact is that 2/3rd of customers of Databricks Cloud (hosted service running Spark), use Spark SQL within other programming languages. We will also showcase a hands-on case study using Spark SQL on Databricks in this article. Stay tuned for that! The major goals for Spark SQL as defined by it\u2019s creators are as follows.", "We will now take a look at the key features and architecture around Spark SQL and DataFrames. Some key concepts to keep in mind here would be around the Spark eco-system which has been constantly evolving over time.", "RDDs or \u201cResilient Distributed Datasets\u201d is perhaps the biggest contributor behind all the success stories of Spark. It is basically a data structure or rather , a distributed memory abstraction to be more precise, that allows programmers to perform in-memory computations on large distributed clusters while retaining aspects like fault tolerance. You can also parallelize a lot of computations, transformations and track the whole lineage of transformations, which can help in efficiently recomputing lost data. Spark enthusiasts do read the excellent paper around RDDs, \u2018Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing\u2019. Also, Spark works with the concept of drivers and workers as depicted in the following figure.", "You can typically create an RDD by reading in data from files, databases, parallelizing existing collections or even transformations. Typically transformations are operations which can be used to transform the data into different aspects and dimensions depending on the way we want to wrangle and process the data. They are also lazily evaluated meaning that even if you define a transformation, the results are not computed till you apply an action which typically requires a result to be returned to the driver program (and it computed all applied transformations then!).", "Shout out to fellow data scientist and friend Favio V\u00e1zquez and his excellent article on Deep Learning With Apache Spark from which I got some excellent ideas and content including the preceding figure. Do check it out!", "Now that we know about the general architecture of how Spark works, let\u2019s take a closer look into Spark SQL. Typically, Spark SQL runs as a library on top of Spark, as we have seen in the figure covering the Spark eco-system. The following figure gives a more detailed peek into the typical achitecture and interfaces of Spark SQL.", "The figure clearly shows us the various SQL interfaces, which can be accessed through JDBC/ODBC or through a command-line console, as well as the DataFrame API integrated into Spark\u2019s supported programming languages (we will be using Python!). The DataFrame API is very powerful and allows users to finally intermix procedural and relational code! Advanced functions like UDFs (user defined functions) can also be exposed in SQL, which can be used by BI tools.", "Spark DataFrames are very interesting and help us leverage the power of Spark SQL and combine its procedural paradigms as needed. A Spark DataFrame is basically a distributed collection of rows (Row types) with the same schema. It is basically a Spark Dataset organized into named columns. A point to note here is that Datasets, are an extension of the DataFrame API that provides a type-safe, object-oriented programming interface. Hence, they are available only in Java and Scala and we will, therefore, be focusing on DataFrames.", "A DataFrame is equivalent to a table in a relational database (but with more optimizations under the hood), and can also be manipulated in similar ways to the \u201cnative\u201d distributed collections in Spark (RDDs). Spark DataFrames have some interesting properties, some of which are mentioned below.", "This should give you enough perspective on Spark SQL, DataFrames, essential features, concepts, architecture and interfaces. Let\u2019s wrap up this section by taking a look at performance benchmarks.", "Releasing a new feature without the right optimizations can be deadly, and the folks who built Spark did tons of performance tests and benchmarking! Let\u2019s take a look at some interesting results. The first figure showcasing some results is depicted below.", "In these experiments, they compared the performance of Spark SQL against Shark and Impala using the AMPLab big data benchmark, which uses a web analytics workload developed by Pavlo et al. The benchmark contains four types of queries with different parameters performing scans, aggregation, joins and a UDF-based MapReduce job. The dataset was 110 GB of data after compression using the columnar Parquet format. We see that in all queries, Spark SQL is substantially faster than Shark, and generally competitive with Impala. The Catalyst optimizer is responsible for this, which reduces CPU overhead (we shall cover this briefly). This feature makes Spark SQL competitive with the C++ and LLVM-based Impala engine in many of these queries. The largest gap from Impala is in query 3a where Impala chooses a better join plan, because the selectivity of the queries makes one of the tables very small.", "Following graphs show some more performance benchmarks for DataFrames and regular Spark APIs and Spark + SQL.", "Finally, the following graph shows a nice benchmark result of DataFrames vs. RDDs in different languages, which gives an interesting perspective on how optimized DataFrames can be!", "Why is Spark SQL so fast and optimized? The reason is, because of a new extensible optimizer, Catalyst, based on functional programming constructs in Scala. While we won\u2019t go into too extensive details about Catalyst here, it is worth a mention since it helps in optimizing DataFrame operations and queries.", "Catalyst\u2019s extensible design has two purposes.", "Catalyst supports both rule-based and cost-based optimization. While extensible optimizers have been proposed in the past, they have typically required a complex domain-specific language to specify rules. Usually, this leads to having a significant learning curve and maintenance burden. In contrast, Catalyst uses standard features of the Scala programming language, such as pattern-matching, to let developers use the full programming language while still making rules easy to specify.", "At its core, Catalyst contains a general library for representing trees and applying rules to manipulate them. On top of this framework, it has libraries specific to relational query processing (e.g., expressions, logical query plans), and several sets of rules that handle different phases of query execution: analysis, logical optimization, physical planning, and code generation to compile parts of queries to Java bytecode. Interested in knowing more details about Catalyst and doing a deep-dive? You can check out an excellent article from Databricks!", "We will now do a simple tutorial based on a real-world dataset to look at how to use Spark SQL. We will be using Spark DataFrames but the focus will be more on using SQL. I will be covering a detailed discussion around Spark DataFrames and common operations in a separate article. I love using cloud services for my machine learning, deep learning and even Big Data Analytics needs. Instead of painfully setting up your own Spark cluster, use one of the best in the Cloud! We will be using the Databricks Platform for our Spark needs! Databricks is a company founded by the creators of Apache Spark, that aims to help clients with cloud-based big data processing using Spark.", "The simplest way (and free of charge) is to go to the Try Databricks page and sign up for an account using the community edition where you get a cloud-based cluster, which is a single node cluster with 6 GB and unlimited notebooks, not bad for a free version! I definitely recommend using the Databricks Platform if you have serious needs for analyzing Big Data!", "Let\u2019s get started with our case study now, feel free to create a new notebook from your home screen in Databricks or your own Spark cluster as depicted in the following snapshot.", "You can also import my notebook containing the entire tutorial but do run every cell and play around with it and explore instead of just reading through it. Unsure of how to use Spark on Databricks? Follow this short but useful tutorial and get started today!", "This tutorial will familiarize you with essential Spark capabilities to deal with structured data often obtained from databases or flat files. We will explore typical ways of querying and aggregating relational data by leveraging concepts of DataFrames and SQL using Spark. We will work on an interesting dataset from the KDD Cup 1999 and try to query the data using high level abstrations like the dataframe which has already been a hit in popular data analysis tools like R and Python. We will also look at how easy it is to build data queries using the SQL language, which you have learnt, and retrieve insightful information from our data. This also happens at scale without us having to do a lot more since Spark distributes these data structures efficiently in the back-end, which makes our queries scalable and as efficient as possible. We start by loading some basic dependencies.", "We will use data from the KDD Cup 1999, which is the data set used for The Third International Knowledge Discovery and Data Mining Tools Competition, which was held in conjunction with KDD-99 The Fifth International Conference on Knowledge Discovery and Data Mining. The competition task was to build a network intrusion detector, a predictive model capable of distinguishing between bad connections, called intrusions or attacks, and good, normal connections. This database contains a standard set of data to be audited, which includes a wide variety of intrusions simulated in a military network environment.", "We will be using the reduced dataset kddcup.data_10_percent.gz containing nearly half a million nework interactions, since we would be downloading this Gzip file from the web locally, and then work on the same. If you have a good, stable internet connection, feel free to download and work with the full dataset available as kddcup.data.gz.", "Dealing with datasets retrieved from the web can be a bit tricky in Databricks. Fortunately, we have some excellent utility packages like dbutils which help in making our job easier. Let's take a quick look at some essential functions for this module.", "We will now leverage the python urllib library to extract the KDD Cup 99 data from their web repository, store it in a temporary location, and then move it to the Databricks filesystem, which can enable easy access to this data for analysis", "Note: If you skip this step and download the data directly, you may end up getting a InvalidInputException: Input path does not exist error", "Now that we have our data stored in the Databricks filesystem, let\u2019s load up our data from the disk into Spark\u2019s traditional abstracted data structure, the Resilient Distributed Dataset (RDD)", "You can also verify the type of data structure of our data (RDD) using the following code.", "A Spark DataFrame is an interesting data structure representing a distributed collecion of data. Typically the entry point into all SQL functionality in Spark is the SQLContext class. To create a basic instance of this call, all we need is a SparkContext reference. In Databricks, this global context object is available as sc for this purpose.", "Each entry in our RDD is a comma-separated line of data which we first need to split before we can parse and build our dataframe.", "We can use the following code to check the total number of potential columns in our dataset.", "The KDD 99 Cup data consists of different attributes captured from connection data. The full list of attributes in the data can be obtained here and further details pertaining to the description for each attribute\\column can be found here. We will just be using some specific columns from the dataset, the details of which are specified as follows.", "We will be extracting the following columns based on their positions in each datapoint (row) and build a new RDD as follows.", "Now that our data is neatly parsed and formatted, let\u2019s build our DataFrame!", "You can also now check out the schema of our dataframe using the following code.", "We can leverage the registerTempTable() function to build a temporaty table to run SQL commands on our DataFrame at scale! A point to remember is that the lifetime of this temp table is tied to the session. It creates an in-memory table that is scoped to the cluster in which it was created. The data is stored using Hive's highly-optimized, in-memory columnar format.", "You can also check out saveAsTable() which creates a permanent, physical table stored in S3 using the Parquet format. This table is accessible to all clusters. The table metadata including the location of the file(s) is stored within the Hive metastore.", "Let\u2019s look at a few examples of how we can run SQL queries on our table based off our dataframe. We will start with some simple queries and then look at aggregations, filters, sorting, sub-queries and pivots in this tutorial.", "Let\u2019s look at how we can get the total number of connections based on the type of connectivity protocol. First, we will get this information using normal DataFrame DSL syntax to perform aggregations.", "Can we also use SQL to perform the same aggregation? Yes, we can leverage the table we built earlier for this!", "You can clearly see, that you get the same results and you do not need to worry about your background infrastructure or how the code is executed. Just write simple SQL!", "We will now run a simple aggregation to check the total number of connections based on good (normal) or bad (intrusion attacks) types.", "We have a lot of different attack types. We can visualize this in the form of a bar chart. The simplest way is to use the excellent interface options in the Databricks notebook itself!", "This gives us the following nice looking bar chart! Which you can customize further by clicking on Plot Options as needed.", "Another way is to write the code yourself to do it. You can extract the aggregated data as a pandas DataFrame and then plot it as a regular bar chart.", "Let\u2019s look at which protocols are most vulnerable to attacks now based on the following SQL query.", "Well, looks like ICMP connections followed by TCP connections have had the maximum attacks!", "Let\u2019s take a look at some statistical measures pertaining to these protocols and attacks for our connection requests.", "Looks like average amount of data being transmitted in TCP requests are much higher which is not surprising. Interestingly, attacks have a much higher average payload of data being transmitted from the source to the destination.", "Let\u2019s take a closer look at TCP attacks given that we have more relevant data and statistics for the same. We will now aggregate different types of TCP attacks based on service, attack type and observe different metrics.", "There are a lot of attack types and the preceding output shows a specific section of the same.", "We will now filter some of these attack types by imposing some constraints based on duration, file creations, root accesses in our query.", "Interesting to see multihop attacks being able to get root accesses to the destination hosts!", "Let\u2019s try to get all the TCP attacks based on service and attack type such that the overall mean duration of these attacks is greater than zero (> 0). For this, we can do an inner query with all aggregation statistics and then extract the relevant queries and apply a mean duration filter in the outer query as shown below.", "This is nice! Now an interesting way to also view this data is to use a pivot table where one attribute represents rows, and another one represents columns. Let\u2019s see if we can leverage Spark DataFrames to do this!", "Here, we will build upon the previous DataFrame object we obtained where we aggregated attacks based on type and service. For this, we can leverage the power of Spark DataFrames and the DataFrame DSL.", "We get a nice neat pivot table showing all the occurrences based on service and attack type!", "I would encourage you to go out and play with Spark SQL and DataFrames, you can even import my notebook and play with it yourself in your own account.", "Feel free to refer to my GitHub repository also for all the code and notebooks used in this article. What we didn\u2019t cover here includes the following.", "There are plenty of articles\\tutorials available online so I would recommend you to check them out. Some useful resources for you to check out include, the complete guide to Spark SQL from Databricks.", "Thinking of working with JSON data but unsure of using Spark SQL. They support it! Check out this excellent guide to JSON support in Spark SQL.", "Interested in advanced concepts like window functions and ranks in SQL? Check out this excellent article on Window Functions in Spark SQL", "I will also write a followup article to this covering some of these concepts in an intuitive way which should be easy for you to understand. Stay tuned!", "You can directly access my notebook on Databricks by going to this link and import it directly and play around with it", "All the code and resources for this tutorial is available on my GitHub", "You can also access my tutorial as a Jupyter Notebook in case you want to use it offline.", "Have feedback for me? Or interested in working with me on research, data science, artificial intelligence or even publishing an article on TDS? You can reach out to me on LinkedIn.", "Thanks to Durba for editing this article.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fc567853a702f&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsql-at-scale-with-apache-spark-sql-and-dataframes-concepts-architecture-and-examples-c567853a702f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsql-at-scale-with-apache-spark-sql-and-dataframes-concepts-architecture-and-examples-c567853a702f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsql-at-scale-with-apache-spark-sql-and-dataframes-concepts-architecture-and-examples-c567853a702f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsql-at-scale-with-apache-spark-sql-and-dataframes-concepts-architecture-and-examples-c567853a702f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----c567853a702f--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c567853a702f--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://djsarkar.medium.com/?source=post_page-----c567853a702f--------------------------------", "anchor_text": ""}, {"url": "https://djsarkar.medium.com/?source=post_page-----c567853a702f--------------------------------", "anchor_text": "Dipanjan (DJ) Sarkar"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6278d12b0682&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsql-at-scale-with-apache-spark-sql-and-dataframes-concepts-architecture-and-examples-c567853a702f&user=Dipanjan+%28DJ%29+Sarkar&userId=6278d12b0682&source=post_page-6278d12b0682----c567853a702f---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc567853a702f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsql-at-scale-with-apache-spark-sql-and-dataframes-concepts-architecture-and-examples-c567853a702f&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc567853a702f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsql-at-scale-with-apache-spark-sql-and-dataframes-concepts-architecture-and-examples-c567853a702f&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://opensource.com/", "anchor_text": "opensource.com"}, {"url": "https://opensource.com/article/19/3/sql-scale-apache-spark-sql-and-dataframes", "anchor_text": "Scaling relational databases with Apache Spark SQL and DataFrames"}, {"url": "https://opensource.com/article/19/3/apache-spark-and-dataframes-tutorial", "anchor_text": "How to use Spark SQL: A hands-on tutorial"}, {"url": "https://www.slideshare.net/databricks/largescale-data-science-in-apache-spark-20", "anchor_text": "https://www.slideshare.net/databricks/largescale-data-science-in-apache-spark-20"}, {"url": "http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html", "anchor_text": "KDD 99 Cup Data"}, {"url": "http://databricks.com", "anchor_text": "Databricks Cloud Platform"}, {"url": "http://people.csail.mit.edu/matei/papers/2015/sigmod_spark_sql.pdf", "anchor_text": "Relational Data Processing in Spark"}, {"url": "https://code.fb.com/core-data/apache-spark-scale-a-60-tb-production-use-case", "anchor_text": "\u2018Apache Spark @Scale: A 60 TB+ production use case\u2019"}, {"url": "https://code.fb.com/core-data/apache-spark-scale-a-60-tb-production-use-case/", "anchor_text": "Apache Spark @Scale: A 60 TB+ production use case - Facebook CodeFacebook often uses analytics for data-driven decision making. Over the past few years, user and product growth has\u2026code.fb.com"}, {"url": "https://www2.eecs.berkeley.edu/Pubs/TechRpts/2011/EECS-2011-82.pdf", "anchor_text": "\u2018Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing\u2019"}, {"url": "https://towardsdatascience.com/@faviovazquez?source=post_header_lockup", "anchor_text": "Favio V\u00e1zquez"}, {"url": "https://towardsdatascience.com/deep-learning-with-apache-spark-part-1-6d397c16abd", "anchor_text": "article on Deep Learning With Apache Spark"}, {"url": "https://towardsdatascience.com/deep-learning-with-apache-spark-part-1-6d397c16abd", "anchor_text": "Deep Learning With Apache Spark \u2014 Part 1First part on a full discussion on how to do Distributed Deep Learning with Apache Spark. This part: What is Spark\u2026towardsdatascience.com"}, {"url": "https://databricks.com/blog/2016/01/04/introducing-apache-spark-datasets.html", "anchor_text": "Spark Dataset"}, {"url": "https://databricks.com/glossary/what-are-datasets", "anchor_text": "Datasets"}, {"url": "https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html", "anchor_text": "an excellent article from Databricks!"}, {"url": "https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html", "anchor_text": "Deep Dive into Spark SQL's Catalyst OptimizerSpark SQL is one of the newest and most technically involved components of Spark. It powers both SQL queries and the\u2026databricks.com"}, {"url": "https://databricks.com/try-databricks", "anchor_text": "Try Databricks page"}, {"url": "https://databricks.com/signup#signup/community", "anchor_text": "sign up"}, {"url": "https://databricks.com/signup#signup/community", "anchor_text": "community edition"}, {"url": "https://databricks.com/spark/getting-started-with-apache-spark", "anchor_text": "this short but useful tutorial"}, {"url": "https://databricks.com/spark/getting-started-with-apache-spark", "anchor_text": "Apache Spark Tutorial: Getting Started with Apache Spark TutorialApache Spark is a powerful open-source processing engine built around speed, ease of use, and sophisticated analytics\u2026databricks.com"}, {"url": "http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html", "anchor_text": "KDD Cup 1999"}, {"url": "http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html", "anchor_text": "KDD Cup 1999"}, {"url": "https://spark.apache.org/docs/latest/rdd-programming-guide.html#resilient-distributed-datasets-rdds", "anchor_text": "Resilient Distributed Dataset (RDD)"}, {"url": "http://kdd.ics.uci.edu/databases/kddcup99/kddcup.names", "anchor_text": "here"}, {"url": "http://kdd.ics.uci.edu/databases/kddcup99/task.html", "anchor_text": "here"}, {"url": "https://attack.mitre.org/techniques/T1188/", "anchor_text": "multihop attacks"}, {"url": "https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/3137082781873852/3704545280501166/1264763342038607/latest.html", "anchor_text": "import my notebook"}, {"url": "https://github.com/dipanjanS/data_science_for_all/tree/master/tds_spark_sql_intro", "anchor_text": "my GitHub repository"}, {"url": "https://docs.databricks.com/spark/latest/spark-sql/index.html", "anchor_text": "Spark SQL from Databricks"}, {"url": "https://docs.databricks.com/spark/latest/spark-sql/index.html", "anchor_text": "SQL Guide - Databricks DocumentationView Azure Databricks documentation Azure docsdocs.databricks.com"}, {"url": "https://databricks.com/blog/2015/02/02/an-introduction-to-json-support-in-spark-sql.html", "anchor_text": "JSON support in Spark SQL"}, {"url": "https://databricks.com/blog/2015/02/02/an-introduction-to-json-support-in-spark-sql.html", "anchor_text": "An introduction to JSON support in Spark SQLIn this blog post, we introduce Spark SQL's JSON support, a feature we have been working on at Databricks to make it\u2026databricks.com"}, {"url": "https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html", "anchor_text": "Window Functions in Spark SQL"}, {"url": "https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html", "anchor_text": "Introducing Window Functions in Spark SQLIn this blog post, we introduce the new window function feature that was added in Apache Spark 1.4. Window functions\u2026databricks.com"}, {"url": "https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/3137082781873852/3704545280501166/1264763342038607/latest.html", "anchor_text": "this link"}, {"url": "https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/3137082781873852/3704545280501166/1264763342038607/latest.html", "anchor_text": "Working with SQL at Scale - Spark SQL Tutorial - DatabricksBy Dipanjan (DJ) Sarkardatabricks-prod-cloudfront.cloud.databricks.com"}, {"url": "https://github.com/dipanjanS/data_science_for_all/tree/master/tds_spark_sql_intro", "anchor_text": "my GitHub"}, {"url": "https://github.com/dipanjanS/data_science_for_all/tree/master/tds_spark_sql_intro", "anchor_text": "dipanjanS/data_science_for_allCode and resources for my blog and articles to share Data Science and AI knowledge and learnings with everyone \u2026github.com"}, {"url": "http://nbviewer.jupyter.org/github/dipanjanS/data_science_for_all/blob/master/tds_spark_sql_intro/Working%20with%20SQL%20at%20Scale%20-%20Spark%20SQL%20Tutorial.ipynb", "anchor_text": "Jupyter Notebook"}, {"url": "http://nbviewer.jupyter.org/github/dipanjanS/data_science_for_all/blob/master/tds_spark_sql_intro/Working%20with%20SQL%20at%20Scale%20-%20Spark%20SQL%20Tutorial.ipynb", "anchor_text": "Jupyter Notebook ViewerSpark SQL brings native support for SQL to Spark and streamlines the process of querying data stored both in RDDs\u2026nbviewer.jupyter.org"}, {"url": "https://towardsdatascience.com/", "anchor_text": "TDS"}, {"url": "https://www.linkedin.com/in/dipanzan/", "anchor_text": "LinkedIn"}, {"url": "https://www.linkedin.com/in/dipanzan/", "anchor_text": "Dipanjan Sarkar \u2014 Data Scientist \u2014 Intel Corporation | LinkedInView Dipanjan Sarkar\u2019s profile on LinkedIn, the world\u2019s largest professional community. Dipanjan has 6 jobs listed on\u2026www.linkedin.com"}, {"url": "https://www.linkedin.com/in/durba-dutta-bhaumik-44532ab1/", "anchor_text": "Durba"}, {"url": "https://medium.com/tag/big-data?source=post_page-----c567853a702f---------------big_data-----------------", "anchor_text": "Big Data"}, {"url": "https://medium.com/tag/data-science?source=post_page-----c567853a702f---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/analytics?source=post_page-----c567853a702f---------------analytics-----------------", "anchor_text": "Analytics"}, {"url": "https://medium.com/tag/programming?source=post_page-----c567853a702f---------------programming-----------------", "anchor_text": "Programming"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----c567853a702f---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc567853a702f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsql-at-scale-with-apache-spark-sql-and-dataframes-concepts-architecture-and-examples-c567853a702f&user=Dipanjan+%28DJ%29+Sarkar&userId=6278d12b0682&source=-----c567853a702f---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc567853a702f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsql-at-scale-with-apache-spark-sql-and-dataframes-concepts-architecture-and-examples-c567853a702f&user=Dipanjan+%28DJ%29+Sarkar&userId=6278d12b0682&source=-----c567853a702f---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc567853a702f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsql-at-scale-with-apache-spark-sql-and-dataframes-concepts-architecture-and-examples-c567853a702f&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c567853a702f--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fc567853a702f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsql-at-scale-with-apache-spark-sql-and-dataframes-concepts-architecture-and-examples-c567853a702f&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----c567853a702f---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----c567853a702f--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----c567853a702f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----c567853a702f--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----c567853a702f--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----c567853a702f--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----c567853a702f--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----c567853a702f--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----c567853a702f--------------------------------", "anchor_text": ""}, {"url": "https://djsarkar.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://djsarkar.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Dipanjan (DJ) Sarkar"}, {"url": "https://djsarkar.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "10.4K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6278d12b0682&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsql-at-scale-with-apache-spark-sql-and-dataframes-concepts-architecture-and-examples-c567853a702f&user=Dipanjan+%28DJ%29+Sarkar&userId=6278d12b0682&source=post_page-6278d12b0682--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fa34c887aa0f4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsql-at-scale-with-apache-spark-sql-and-dataframes-concepts-architecture-and-examples-c567853a702f&newsletterV3=6278d12b0682&newsletterV3Id=a34c887aa0f4&user=Dipanjan+%28DJ%29+Sarkar&userId=6278d12b0682&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}