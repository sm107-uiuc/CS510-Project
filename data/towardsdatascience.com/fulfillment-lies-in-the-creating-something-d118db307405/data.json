{"url": "https://towardsdatascience.com/fulfillment-lies-in-the-creating-something-d118db307405", "time": 1682993973.177815, "path": "towardsdatascience.com/fulfillment-lies-in-the-creating-something-d118db307405/", "webpage": {"metadata": {"title": "Fulfillment lies in the creating something | by Abhinav Singh | Towards Data Science", "h1": "Fulfillment lies in the creating something", "description": "In a paper in 2014 Ian Goodfellow, a Ph.D. Student at the University of Montreal introduced a paper named Generative Adversarial Nets along with his mentors at Mila including Yoshua Bengio and Aaron\u2026"}, "outgoing_paragraph_urls": [{"url": "http://www.iro.umontreal.ca/~bengioy/yoshua_en/index.html", "anchor_text": "Yoshua Bengio", "paragraph_index": 1}, {"url": "https://aaroncourville.wordpress.com/", "anchor_text": "Aaron Courville", "paragraph_index": 1}, {"url": "https://blog.openai.com/generative-models/", "anchor_text": "Gan\u2019s by OpenAI", "paragraph_index": 6}, {"url": "https://opensource.com/article/17/9/representation-learning", "anchor_text": "representation learning", "paragraph_index": 7}, {"url": "https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence", "anchor_text": "KL divergence", "paragraph_index": 9}, {"url": "https://arxiv.org/pdf/1511.06434.pdf", "anchor_text": "here", "paragraph_index": 23}, {"url": "https://towardsdatascience.com/up-sampling-with-transposed-convolution-9ae4f2df52d0", "anchor_text": "blog post", "paragraph_index": 25}, {"url": "https://github.com/tensorlayer", "anchor_text": "tensorlayer", "paragraph_index": 26}, {"url": "https://github.com/SullyChen", "anchor_text": "Sully Chen", "paragraph_index": 26}, {"url": "http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization", "anchor_text": "here", "paragraph_index": 28}, {"url": "https://github.com/SullyChen", "anchor_text": "Sully Chen", "paragraph_index": 40}, {"url": "https://github.com/Abhinavfreecodecamp/DCGAN", "anchor_text": "here", "paragraph_index": 48}, {"url": "https://medium.com/@jonathan_hui/gan-why-it-is-so-hard-to-train-generative-advisory-networks-819a86b3750b", "anchor_text": "blog post", "paragraph_index": 51}, {"url": "https://medium.com/@jonathan_hui", "anchor_text": "Jonathan hui", "paragraph_index": 51}, {"url": "https://en.wikipedia.org/wiki/Deepfake", "anchor_text": "deep fakes", "paragraph_index": 52}, {"url": "https://github.com/Abhinavfreecodecamp/DCGAN", "anchor_text": "here", "paragraph_index": 53}], "all_paragraphs": ["Note: The reader should have a basic understanding of deep learning.", "In a paper in 2014 Ian Goodfellow, a Ph.D. Student at the University of Montreal introduced a paper named Generative Adversarial Nets along with his mentors at Mila including Yoshua Bengio and Aaron Courville. This paper got citations over 5000 and got reviewed by all major pioneers of deep learning. Read below what Yann Lecun has to say about Gan\u2019s.", "Generative Adversarial Networks is the most interesting idea in machine learning in last ten years \u2014 Yann Lecun (Facebook AI Director)", "This is a clear indication that Generative Adversarial Networks does something really interesting. From now we will refer Generative Adversarial networks as Gan\u2019s. Gan\u2019s are interesting as they can learn to approximate data distribution(aka mimic real data), which is powerful as now it can generate images, audios clips, videos which do not exists in reality. In a sense, you can assume Gan\u2019s are artists. One of the famous quotes by Richard Feynman which explains the intuition behind Gan\u2019s.", "What I cannot create, I do not understand \u2014 Richard Feynman( American theoretical physicist)", "The results generated by gan\u2019s are pretty powerful, In October 2018 Christie\u2019s digital house a French company sold a portrait(fig1.0) in an auction for $432,000 which was generated by a Gan.", "This section is a summarization of blog post on Gan\u2019s by OpenAI. The Idea is if Gan\u2019s becomes better at creating data then it will also be able to understand the data which is present in the world much better then any other algorithms. As they learn the underlying representations of the data they then will understand it better. Gan\u2019s belong to the class of Generative models and are based on the approach of differentiable generator networks, so they are one of the ways to achieve this, other some popular way\u2019s are:", "More on these models later, but the idea remains same all these architectures are trying to learn the underlying representations which define the data. Generative models usually belong to an active area of research called representation learning i.e learning representations from unlabeled data. All the above works differently but still all of them try to learn probability distribution nonetheless.", "The idea behind Gan\u2019s is intuitive to understand, it consists of two architectures a Discriminator which estimates the probability that the sample is real and a Generator which tries to generate data close to real one. With time the Discriminator gets better in discriminating and Generator gets better in generating samples that are more close to real samples (fig 1.1 for illustration). The Goals of the two models are opposite as when Generator is maximizing the probability of Discriminator making a mistake at the same time Discriminator tries its best to predict the correct label either real or fake. This idea becomes also interesting if you think carefully that one model is helping other to get better.", "Consider the fig. 1.2. Here p(x) (Blue one in right) is the true distribution of the image and p\u03b8(x) (Green one) are points taken from Gaussian distribution passed through our neural network. Initially, the points will be completely random but as with training, our model parameter\u2019s \u03b8 are optimized to minimize the difference between the two distributions (By using KL divergence or any other means).", "Both Generator and Discriminator can be same architectures or different, In the Initial paper of Gan\u2019s, they used Multilayer perceptron by providing random noise as input to the generator and another Multilayer Perceptron model is used as a discriminator which detects, is sample a generated one or real. In the later section of this article, we will be building our own Generative neural net by using a convolution-based architecture known as DCGAN( Deep Convolutional Generative Adversarial Networks) but first Let\u2019s put the mathematics behind Gan\u2019s into its correct position. I will try to make math as simple as possible but make sure your basics are in correct order.", "The Goal is to learn the generator\u2019s distribution Pg which defines the real data x, by passing random noise z as input through a neural net G(z ; \u03b8g) where \u03b8g are parameters of the Generator neural network, think of any neural network as a, Universal function approximator, we will also define another neural network D(x ; \u03b8d) the discriminator, this network will give probability d(x)[between 0 to 1] that x came from real data rather than Pg, while G is trained to minimize log(1 \u2212 D(G(z))) the D is trained to predict the correct label. The minimization term of G which is log(1 \u2212 D(G(z))) makes sense as initially, the probability D(G(z)) will be low as the generator has barely started generating samples and it will be easy for discriminator to predict if the sample is real or fake so overall result of log(1 \u2212 D(G(z))) is large. So when G\u2019s objective is to minimize, the D\u2019s objective is to maximize, consider this whole situation as a min-max game V(D,G)", "First, Ex\u223cpdata(x) [log (D(x))] \u2014 Expectation of [maximizing that the generated sample x is real].", "Second, Ez\u223cpz(z) [log(1 \u2212 D(G(z)))] \u2014 Expectation of [minimizing that the generated sample is not being real.]", "The above equation is essentially a min-max game where one is trying to minimize and other one to maximize it. Let\u2019s understand this min-max game situation by taking example of L2 regularization in machine learning. In regularization the the regularizing term is preventing the weights to be very small but at the same time the optimizer is minimizing the overall objective function which generally results in small values of weights. This situation is similar to Gan\u2019s min-max situation but the difference being, In gan\u2019s both minimization and maximization is controlled by optimizer itself.", "Ideally, the best value of D(x) would be 0.5 it means it fails to discriminate between the real sample or the fake one i.e", "The idea of simultaneously training D(x; \u03b8d) and G(z; \u03b8g) looks good in theory but in practice, we usually train D(x; \u03b8d) for k times for each time we train G(z; \u03b8g) so that the discriminator is always stronger than generator.", "The idea is simple to train the network for some number of iteration until the results look satisfactory.", "Repeat following steps for N iterations:", "Step 1: for each of n iteration, K iterations :1.select a mini-batch of noise samples (z1....zm) 2.select a mini-batch of real-data samples (r1....rm)3.train the discriminator with using max-min loss function [eq(1)] over m samples.", "Step 2: For the next iteration : 1.select a mini-batch (z1.....zm) from p(z)2.train the generator by minimizing the log probability of generator making mistake.", "The initial paper has mentioned the use of SGD for training but it is recommended to use more powerful optimizers like Adam, Adagrad, rms-prop etc.", "DCGAN or Deep Convolutional Generative Adversarial Networks is a big improvement over previous Gan\u2019s. Though in theory, the architecture of Gan\u2019s looks simple in practice, but it\u2019s really hard to train a Gan. Reason being, the gradients has to flow to through both discriminator and generator and due to this fact, the training of Gan\u2019s becomes unstable. We will cover these topics in later part of this article but first, let\u2019s try to understand the architecture of DCGAN\u2019s.", "The DCGAN\u2019s are itself a class of deep convolutions based architecture\u2019s consists of two convolution neural nets with the following constraints (as per original paper, read: here). Following constraints are based after performing an extensive model search.", "The architecture proposed in the original paper of DCGAN contains one input layer generating noise along with four deconvolution layers in the generator and four convolution layer along with an output layer in discriminator.", "if you are not aware of Transposed Convolution then check out this blog post by Naoki Shibuya.", "The code for this demonstration taken from tensorlayer original implementation of DCGAN with changes to work with Sully Chen data-set and layer-wise visualizations to really understand how network is learning. The data-set was built to train a self-driving car, it contains 63k images of road driving with corresponding steering angle and other values like gas peddle etc. They main idea behind using this data-set is to see how good the Gan\u2019s can mimic real driving scenes like other pedestrians, vehicles etc. If you look at bigger picture, this experiment is great as now instead of relying on a camera attached in front of your car to capture images of the roads, you can generate endless amount of data just by sitting in a room. Although how good the idea to train a self-driving car with generated images is? I\u2019ll leave it for further discussion.", "Note: All the weights of the model are initialized using Glorot initializer. Typical Glorot initialization looks something like this,", "if you want to learn more about Glorot initializer or why it is used, click here.", "Let\u2019s look at the code for Generator. The flow of the code is as follows:", "1. Defining generator parameters like output image size, filter parameters, gamma initializing.", "2. Input layer is of 100 dimension i.e 100 random numbers generated from uniform distribution with mean = 0 and standard_dev = 1.0.", "5. Normalizing using Batch Normalizing with gamma sampled from uniform distribution with mean = 1.0, std_dev=0.02 with ReLu activation.", "10. DeConv2d or simply transposed convolution layer with kernel =(5,5), stride = (h=2,w=2) with depth 64 *4 with zero padding with tanh activation.", "I have to say the results looks much promising than expected. Before this let\u2019s see how real data images look like (samples below are random from real dataset, the idea is to see how good gans are in understanding data).", "As you can see the samples from 1st epoch model is just able to get essence of data but as the number of epoches increases the output gets better and in 10th epoches it becomes really better. The Network is trained on 30 epoches, let\u2019s visualize those. Images arranged from epoches 11 to 30 each in a row.", "Approaching towards last epoch the results start to capture even tiny architecture like tyres, bushes etc. In-spite of good results, still the generator is sometimes unable to capture blurry edges like between roads and mountain resulting in fusion of two. Still results are completely acceptable.", "Convergence of Gan\u2019s are unstable and this is directly reflected in variance of loss values. Below provided a glimpse of the loss values of our gan architecture.", "We can clearly observe how much the discriminator and generator loss values are going up and down. Problems associated with training of gans are discussed in later part of this blog.", "In most applications we see, involving neural networks, they act as a black box i.e giving data from one side and getting output. We usually don\u2019t understand what is going inside a network even at abstract level, so I decided to take a step in this direction and trying to understand what is happening at output of each layer in both generator and discriminator, how the network is generating or discriminating the data.", "Note: All the Layer-wise output are obtained before normalizing after training network with 30 epochs on Sully Chen.", "Layer 1 : As expected initially the network only generates some noise.", "Layer 2: The network starts generating some patterns.", "Layer 4: Final patterns upon which tanh activation will be applied.", "Layer 1 : We know lower levels are used to extract simple features like horizontal or vertical edge detection, same is observed in our experiment.", "Layer 2: Output of 2nd layer is done upon output of 1st. Not much can be understood by us but all this important for network itself.", "Layer 3: Size gets smaller and the prediction becomes relevant to network only.", "Layer 4: It\u2019s hard to make any interpretation when the network get\u2019s deeper, but that\u2019s how network does it.", "These visualization gives us some basic understanding of how networks predicts and what is happening at output of each layer, but we have just scratched the surface it\u2019s much more to learn what these networks are doing. The full code for this development can be found on my github here.", "In Theory, the working of gan\u2019s sounds good and it seems it should converge to global minima, but in practice when both Generator G and Discriminator D are represented as neural network the chance of convergence goes down dramatically and in most cases the convergence will stop at saddle point.", "Following problems are majorly observed while training Gan\u2019s", "Stabilizing Gan\u2019s is still an active area of research in deep learning community, many academic institutes and organizations are working in this area. If you are interested in exploring more about the current problems associated with Gan\u2019s checkout this excellent blog post by Jonathan hui.", "Though Gan\u2019s were built keeping in mind to synthesize data and essentially making models to learn more about the representation that constitutes real world data for good but unfortunately most current use of Gan\u2019s were not so friendly and many were even disastrous one of famous example being the deep fakes, the idea is create fake images of people saying or doing something. These attempts can cause serious damage to both society and country. In-spite of this there are some cool applications of gan\u2019s,", "Generative Adversarial networks are one of the most promising technique in deep learning currently. They are built upon idea which is extremely simple and feels most generic. Our, seems to be a good experiment with gan\u2019s, I\u2019ll be writing more on this in my upcomming blogs. Complete code for this can be found here.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fd118db307405&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffulfillment-lies-in-the-creating-something-d118db307405&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffulfillment-lies-in-the-creating-something-d118db307405&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffulfillment-lies-in-the-creating-something-d118db307405&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffulfillment-lies-in-the-creating-something-d118db307405&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----d118db307405--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d118db307405--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@abhinav199530singh?source=post_page-----d118db307405--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@abhinav199530singh?source=post_page-----d118db307405--------------------------------", "anchor_text": "Abhinav Singh"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd0fe2ea90f2b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffulfillment-lies-in-the-creating-something-d118db307405&user=Abhinav+Singh&userId=d0fe2ea90f2b&source=post_page-d0fe2ea90f2b----d118db307405---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd118db307405&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffulfillment-lies-in-the-creating-something-d118db307405&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd118db307405&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffulfillment-lies-in-the-creating-something-d118db307405&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "http://www.iro.umontreal.ca/~bengioy/yoshua_en/index.html", "anchor_text": "Yoshua Bengio"}, {"url": "https://aaroncourville.wordpress.com/", "anchor_text": "Aaron Courville"}, {"url": "http://www.artfixdaily.com/news_feed/2018/08/21/461-ai-generated-artwork-heads-to-christies-in-a-first-ever-sale", "anchor_text": "here"}, {"url": "https://blog.openai.com/generative-models/", "anchor_text": "Gan\u2019s by OpenAI"}, {"url": "https://www.doc.ic.ac.uk/~js4416/163/website/autoencoders/variational.html", "anchor_text": "Variational Autoencoders"}, {"url": "https://en.wikipedia.org/wiki/Recurrent_neural_network", "anchor_text": "Rnn\u2019s"}, {"url": "http://colah.github.io/posts/2015-08-Understanding-LSTMs/", "anchor_text": "LSTM\u2019s"}, {"url": "https://arxiv.org/abs/1601.06759", "anchor_text": "PixelRnn\u2019s"}, {"url": "http://deeplearning.net/tutorial/rnnrbm.html", "anchor_text": "RNN-RBM for sequential output"}, {"url": "https://en.wikipedia.org/wiki/Convolutional_Deep_Belief_Networks", "anchor_text": "Convolution Boltzmann machines"}, {"url": "https://opensource.com/article/17/9/representation-learning", "anchor_text": "representation learning"}, {"url": "https://www.researchgate.net/figure/General-block-diagram-of-generative-adversarial-networks-GANs_fig1_317388182", "anchor_text": "here"}, {"url": "https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence", "anchor_text": "KL divergence"}, {"url": "https://blog.openai.com/generative-models/", "anchor_text": "here"}, {"url": "https://arxiv.org/abs/1406.2661", "anchor_text": "here"}, {"url": "https://arxiv.org/pdf/1406.2661.pdf", "anchor_text": "here"}, {"url": "https://arxiv.org/pdf/1511.06434.pdf", "anchor_text": "here"}, {"url": "https://gluon.mxnet.io/_images/dcgan.png", "anchor_text": "here"}, {"url": "https://towardsdatascience.com/up-sampling-with-transposed-convolution-9ae4f2df52d0", "anchor_text": "blog post"}, {"url": "https://github.com/tensorlayer", "anchor_text": "tensorlayer"}, {"url": "https://github.com/SullyChen", "anchor_text": "Sully Chen"}, {"url": "http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization", "anchor_text": "here"}, {"url": "https://github.com/SullyChen/driving-datasets", "anchor_text": "source"}, {"url": "https://github.com/SullyChen", "anchor_text": "Sully Chen"}, {"url": "https://github.com/Abhinavfreecodecamp/DCGAN", "anchor_text": "here"}, {"url": "https://www.deeplearningbook.org/", "anchor_text": "deep learning book"}, {"url": "https://medium.com/@jonathan_hui/gan-why-it-is-so-hard-to-train-generative-advisory-networks-819a86b3750b", "anchor_text": "blog post"}, {"url": "https://medium.com/@jonathan_hui", "anchor_text": "Jonathan hui"}, {"url": "https://en.wikipedia.org/wiki/Deepfake", "anchor_text": "deep fakes"}, {"url": "https://github.com/Abhinavfreecodecamp/DCGAN", "anchor_text": "here"}, {"url": "https://arxiv.org/abs/1406.2661", "anchor_text": "https://arxiv.org/abs/1406.2661"}, {"url": "https://arxiv.org/abs/1511.06434", "anchor_text": "https://arxiv.org/abs/1511.06434"}, {"url": "https://github.com/tensorlayer/dcgan", "anchor_text": "https://github.com/tensorlayer/dcgan"}, {"url": "https://github.com/InFoCusp/tf_cnnvis", "anchor_text": "https://github.com/InFoCusp/tf_cnnvis"}, {"url": "https://blog.openai.com/generative-models/", "anchor_text": "Generative Models blog by openAI"}, {"url": "https://medium.com/@jonathan_hui/gan-why-it-is-so-hard-to-train-generative-advisory-networks-819a86b3750b", "anchor_text": "https://medium.com/@jonathan_hui/gan-why-it-is-so-hard-to-train-generative-advisory-networks-819a86b3750b"}, {"url": "https://github.com/carpedm20/DCGAN-tensorflow/blob/master/model.py", "anchor_text": "github/carpedm20/DCGAN-tensorflow"}, {"url": "https://appliedaicourse.com", "anchor_text": "AppliedAI Course"}, {"url": "http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization", "anchor_text": "andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization"}, {"url": "https://www.deeplearningbook.org/", "anchor_text": "www.deeplearningbook.org/"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----d118db307405---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----d118db307405---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/generative-adversarial?source=post_page-----d118db307405---------------generative_adversarial-----------------", "anchor_text": "Generative Adversarial"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----d118db307405---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/convolutional-network?source=post_page-----d118db307405---------------convolutional_network-----------------", "anchor_text": "Convolutional Network"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd118db307405&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffulfillment-lies-in-the-creating-something-d118db307405&user=Abhinav+Singh&userId=d0fe2ea90f2b&source=-----d118db307405---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd118db307405&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffulfillment-lies-in-the-creating-something-d118db307405&user=Abhinav+Singh&userId=d0fe2ea90f2b&source=-----d118db307405---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd118db307405&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffulfillment-lies-in-the-creating-something-d118db307405&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d118db307405--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fd118db307405&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffulfillment-lies-in-the-creating-something-d118db307405&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----d118db307405---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----d118db307405--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----d118db307405--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----d118db307405--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----d118db307405--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----d118db307405--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----d118db307405--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----d118db307405--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----d118db307405--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@abhinav199530singh?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@abhinav199530singh?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Abhinav Singh"}, {"url": "https://medium.com/@abhinav199530singh/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "53 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd0fe2ea90f2b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffulfillment-lies-in-the-creating-something-d118db307405&user=Abhinav+Singh&userId=d0fe2ea90f2b&source=post_page-d0fe2ea90f2b--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Fd0fe2ea90f2b%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffulfillment-lies-in-the-creating-something-d118db307405&user=Abhinav+Singh&userId=d0fe2ea90f2b&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}