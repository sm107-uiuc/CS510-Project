{"url": "https://towardsdatascience.com/alexnet-8b05c5eb88d4", "time": 1683011745.662699, "path": "towardsdatascience.com/alexnet-8b05c5eb88d4/", "webpage": {"metadata": {"title": "AlexNet. Let\u2019s understand and code it! | by Abhishek Verma | Towards Data Science", "h1": "AlexNet", "description": "AlexNet the first convolutional network has been explained with its code in TensorFlow Python. ILSVRC challenge winner and the first neural network to work greatly on computer vision."}, "outgoing_paragraph_urls": [{"url": "https://colab.research.google.com/", "anchor_text": "COLAB", "paragraph_index": 48}, {"url": "https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf", "anchor_text": "paper", "paragraph_index": 49}, {"url": "https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf", "anchor_text": "paper", "paragraph_index": 51}, {"url": "https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf", "anchor_text": "paper", "paragraph_index": 53}, {"url": "https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf", "anchor_text": "paper", "paragraph_index": 55}, {"url": "https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf", "anchor_text": "paper", "paragraph_index": 59}, {"url": "https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf", "anchor_text": "paper", "paragraph_index": 64}, {"url": "https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf", "anchor_text": "paper", "paragraph_index": 64}, {"url": "https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf", "anchor_text": "paper", "paragraph_index": 65}, {"url": "https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf", "anchor_text": "paper", "paragraph_index": 65}, {"url": "https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf", "anchor_text": "paper", "paragraph_index": 66}], "all_paragraphs": ["In the autumnal September of 2012, AlexNet first competed in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) and showed the abnormal prowess of GPUs in deep learning. The spark that lit the whole area of deep learning in image was this. Created by Alex Krizhevsky along with the big names of deep learning today, Ilya Sutskever and Geoffrey Hinton. This is the first paper anybody who enters the arena of deep learning reads.", "So, let\u2019s review this paper and see how can we recreate to bring it to its former glory.", "Due to paucity of GPU memory at the time the network was designed, it had to be trained by combining 2 GPUs.", "The success of the network lies in the tricks they have used.", "This is one of the neat tricks they used.", "Let\u2019s first take a look at ReLU.", "The best thing about ReLU is that there will be learning even if a minuscule amount of training samples have some activation. But, the downside is they are unbounded. So, they keep their weights in check and also, to bring out better features, we use local response normalization.", "This formula looks scary at first sight, so, let\u2019s understand that.", "First of all, inside the sum operator, lies the activation, we square it to nullify the effects of positive-negative.", "Now, let\u2019s see, what is the range of the sum operator, given a n, it iterates n/2 to its left and n/2 to its right, while taking care of the bounds i.e. 0 and N-1.", "This is further multiplied by a factor, \u03b1, to diminish its value in comparison to the numerator, in order to preserve the value of the activation in the numerator (if \u03b1 is high, then, the activation in the numerator will be diminished, thus, leading to a vanishing gradient, if it\u2019s too low, it will lead to exploding gradients).", "k is added in order to prevent division by zero error.", "Finally, \u03b2 is put as an exponent to decide the effect of this local response on the activation in question, a higher \u03b2 will penalize the activation w.r.t to its neighbours more while a lower \u03b2 will not affect the activation in question much w.r.t. to its neighbours.", "So, Local Response Normalization helps us to bring out those activations that are performing much better w.r.t. to its neighbours. This helps to increase the efficiency of the network also, as some neurons will be acting as the core of the network, so, computation will be fast.", "Also, it helps diminishing redundant features, suppose, many adjacent neurons are having high values, then, they will be normalized by the Local Response Normalization, thus, suppressing the redundant, continuous features.", "Thus, local response normalization also creates competition between adjacent neurons to learn better to differentiate themselves from others in their neighbourhood. So, you can call it as a competitive normalization.", "Local Response Normalization is not used anymore as we prefer Batch Normalization, which works at a batch level to get rid of internal covariate shift.", "Batch Normalization is trainable and has a regularization effect as compared to Local Response Normalization which is not trainable and has no regularization effect.", "This is the next cool trick they have used.", "Normally, we use non-overlapping pooling, something like this:", "But, in AlexNet, overlapping pooling was used.", "Let\u2019s understand the intuition behind it.", "Given an image, we infer the shape, the boundary of the objects in that image, because of the clear-cut distinction, we can make between two sides of the boundary. This also helps us decide where the object is in the image.", "Let\u2019s take the example of the image above. Our attention is focused on the object of the image i.e. in this case, the girl. We can distinctly locate the girl in the image because of the boundary we can draw around the girl in our mind.", "This is the spatial information in the image that we infer.", "In the case of non-overlapping pooling, we can see that because of the disjointedness of the pooling, a certain amount of spatial information will be lost as all the high values will be captured.", "But, in the case of overlapping pooling, we can see that along with the highs, some areas where the density of low values is high, they will also be conserved. The spatial information is a consequence of both, so, in case of overlapping pooling, it will be conserved more.", "When the network starts focusing on a particular feature and then, any deviation from it leads to a wrong prediction. This hypersenstivity leads to poor generalization.", "Thus, if non-overlapping pooling is used, then, the network will keep on focusing the dominant features only leading to catastrophic overfitting.", "But, in the case of overlapping pooling as spatial information is conserved, the network won\u2019t overfit easily.", "Nowadays I haven\u2019t seen this technique used prevalently. The most plausible reason would be batch normalization which has a regularization effect and also, prevents overfitting. So, we go with the normal non-overlapping pooling in order to save space.", "The third trick they used is data augmentation.", "We want our neural networks to generalize well, so, we augment our data by doing some simple operations and on-the-fly i.e. the augmented image is generated while training (just like in AlexNet).", "AlexNet uses image translations and horizontal reflection. Out of the 256x256 images they had, they took random patches of 224x224 along with their horizontal reflections. The act of taking random patches is thus image translation. As for horizontal flipping, let\u2019s see this example:", "The second of augmentation that they have used is one that I had not seen elsewhere. They altered the intensities of the RGB channels in training examples.", "They first performed PCA on the RGB pixels of the entire ImageNet training dataset. They extracted the principal components for each of the channels. They then add a random fraction of these principal components into each pixel of the image.", "What it does to the image is that it changes the colour and intensity of the illumination. Thus, it exploits a property of natural images that the label of the object is invariant of the illumination parameters. A dog is a dog in bright white light and yellow lamp, period!", "This is my first tryst with this augmentation and haven\u2019t seen it being used. Because of the huge size of data today, it seems inefficient to go through the pain of finding the principal components, when we have other random operations for augmentation today such as zooming, skewing etc.", "This is the fourth trick they used. Honestly, this needs no introduction, as it is the de facto method to reduce overfitting in neural networks today.", "Dropout is randomly switching off some neurons, so that, each neuron is forced to learn features that are not dependent on its neighbours, thus leading to more robust features.", "Finally, let\u2019s have a tally as to what helped this network the most:", "If you are confused about what is a top-1 and top-5 error, then, we know that ImageNet has a total of 1000 classes. The normal way is to predict the class which corresponds to the highest value in the final layer and that is termed here as top-1. Another way is to take the top-k approach (in this case, k = 5), here, we take the prediction as the classes that correspond to units in final layer with the top-5 highest values. If out of these 5, anyone matches the ground truth, then, we consider it a successful prediction.", "The optimizer they have used SGD with momentum which is still used as of today but needs a proper training schedule and is thus hard to train. So, the de facto optimizer is Adam. A momentum of 0.9 has been used in the case of AlexNet.", "The training batch size is 128, which is good and in line with all the advice about deep learning. Higher number of samples in batches lead to better models.", "Weight decay also has been used with a value at 0.0005. Weight decay is still in vogue today and used in models to improve their performance.", "Weight initialization is done using zero-mean Gaussian distribution with a standard deviation of 0.01. In case of bias initialization, they have initialized the biases with 1 in case of second, fourth, fifth and the higher dense layers while other layers\u2019 biases are initialized with 0. This initialization helps the network initially by giving ReLU positive inputs.", "Learning rate was set initially at 0.01 and reduced by a factor of 10 whenever the validation error rate stopped improving.", "We have finally learnt everything about AlexNet.", "If you want to try this code, I suggest COLAB by Google. This uses TensorFlow 2.2.0. I have used the simple MNIST dataset here and resized it to fit to AlexNet inputs. Why? Because if we input 28x28 images available MNIST, this won\u2019t compile. Try it! After the first convolution, there is nothing left to apply max pool on.", "Now, I have fully tried to incorporate as much as I could from the paper. First of all, I am not using two stems because this is not the ancient times when there was a paucity of GPU space. So, I am using a single stem.", "Before line 37 in the gist, I have loaded the MNIST data and reshaped it. I take a very small part of MNIST because loading it whole and resizing it overflows the RAM leading to kernel being dumped.", "At line 38, I have defined the first layer using tf.keras.layers.Conv2D . The number of filters is 96, the filter size is 11x11, activation is relu , and the last unfamiliar kernel_initializer I have used is to initialize the weights using a Gaussian distribution with mean 0 and standard deviation of 0.01, as mentioned in the paper. If you are wondering about the biases, by default, they are set to zero.", "At line 42, we have the tf.keras.layers.MaxPooling2D , in which I have set the filter size to be 3x3 and stride to be (2, 2).", "At line 44, we again have tf.keras.layers.Conv2D . Here, one thing has changed, I have also used a bias_initializer which hold the value ones . This is to set biases 1 at this layer as mentioned in the paper.", "At line 48, we have a tf.keras.layers.MaxPooling2D just like line 42.", "At line 50, we have tf.keras.layers.Conv2D . The number of filters is 384, the filter size is 3x3, activation is relu , kernel_initializer has been used to initialize the weights using a Gaussian distribution with mean 0 and standard deviation 0.01, as mentioned in the paper and lastly bias_initializer has been used to set biases to 1 as mentioned in the paper.", "At lines 53 and 57, the same pattern is followed as line 50.", "At line 61, we have a tf.keras.layers.MaxPooling2D just like line 42.", "At line 63, a Flatten layer has been used to get rid of the extra dimensions. This is the transformation that happens:", "At line 65, Dense layer has been used with 4096 units (in paper, each stem has 2048 units), kernel_initializer has been used to initialize weight with a Gaussian distribution of mean 0 and standard deviation 0.01 and bias_initializer has been used to initialize biases with 1, as mentioned in the paper.", "At line 69, Dropout has been used and set to 0.5. In the paper, the value of dropout is not mentioned specifically in relation to the network, but, in the discussion about dropout, they talk about this value, hence, I have used it. There is no explicit mention of the dropout used.", "At line 71, the same pattern as line 65 is followed.", "At line 75, the same pattern as line 69 is followed.", "At line 77, we have the prediction layer, here, we have 10 classes as compared to the 1000 in ImageNet, which was in use in AlexNet. Dense layer with activation softmax and 10 units has been used to do the predictions. Also, kernel_initializer has been used to initialize the weights with a Gaussian distribution of mean 0 and standard deviation 0.01. The biases are implicitly 0.", "At line 83, we have the model.compile , here we decide the optimizer, the loss and the metrics of the model. The authors have used SGD with momentum 0.9 and weight decay 0.0005, but the functionality of weight decay with SGD is not available in TensorFlow. So, I have only used SGD with momentum 0.9. The learning rate of 0.01 has been used according to paper. The metrics in the paper were top-1 (which is equivalent to accuracy) and top-5. Both the metrics have been set in this line. The loss is categorical_crossentropy which is used for multi-class classification.", "At line 89, I have used ReduceLROnPaleau to reduce the learning rate when the validation error doesn\u2019t improve. For how many epochs do they see if it is changing or not is not mentioned in the paper. This is set by patience attribute in the function used. The factor attribute allows us to divide the learning rate by 10 when it doesn\u2019t improve as mentioned in the paper. The min_lr shows the lower bound of the learning rate and this has been set to the minimum learning rate, the authors had while training.", "At line 93, we fit the model. A batch size of 128 and number of epochs 90, has been set in accordance with the paper.", "Today, we reviewed and understood the first groundbreaking network that brought GPUs into the light. This network single-handedly spurred the era of image deep learning. It brought GPUs into vogue and they are here to stay.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "I am a Data Scientist. I like to write about concepts related to deep learning. Leave in the comments what you want me to write on next."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F8b05c5eb88d4&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Falexnet-8b05c5eb88d4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Falexnet-8b05c5eb88d4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Falexnet-8b05c5eb88d4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Falexnet-8b05c5eb88d4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----8b05c5eb88d4--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8b05c5eb88d4--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://deeptechtalker.medium.com/?source=post_page-----8b05c5eb88d4--------------------------------", "anchor_text": ""}, {"url": "https://deeptechtalker.medium.com/?source=post_page-----8b05c5eb88d4--------------------------------", "anchor_text": "Abhishek Verma"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F18373f6fd34a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Falexnet-8b05c5eb88d4&user=Abhishek+Verma&userId=18373f6fd34a&source=post_page-18373f6fd34a----8b05c5eb88d4---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8b05c5eb88d4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Falexnet-8b05c5eb88d4&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8b05c5eb88d4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Falexnet-8b05c5eb88d4&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://goodstock.photos/", "anchor_text": "Good Stock Photos"}, {"url": "https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf", "anchor_text": "https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf"}, {"url": "https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf", "anchor_text": "https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf"}, {"url": "https://medium.com/@deeptechtalker", "anchor_text": "Abhishek Verma"}, {"url": "https://medium.com/@deeptechtalker", "anchor_text": "Abhishek Verma"}, {"url": "https://unsplash.com/@michaeldam?utm_source=medium&utm_medium=referral", "anchor_text": "Michael Dam"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://medium.com/@deeptechtalker", "anchor_text": "Abhishek Verma"}, {"url": "https://medium.com/@deeptechtalker", "anchor_text": "Abhishek Verma"}, {"url": "https://medium.com/@deeptechtalker", "anchor_text": "Abhishek Verma"}, {"url": "https://colab.research.google.com/", "anchor_text": "COLAB"}, {"url": "https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf", "anchor_text": "paper"}, {"url": "https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf", "anchor_text": "paper"}, {"url": "https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf", "anchor_text": "paper"}, {"url": "https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf", "anchor_text": "paper"}, {"url": "https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf", "anchor_text": "paper"}, {"url": "https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf", "anchor_text": "paper"}, {"url": "https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf", "anchor_text": "paper"}, {"url": "https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf", "anchor_text": "paper"}, {"url": "https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf", "anchor_text": "paper"}, {"url": "https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf", "anchor_text": "paper"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----8b05c5eb88d4---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----8b05c5eb88d4---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/ai?source=post_page-----8b05c5eb88d4---------------ai-----------------", "anchor_text": "AI"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----8b05c5eb88d4---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/tag/learning?source=post_page-----8b05c5eb88d4---------------learning-----------------", "anchor_text": "Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8b05c5eb88d4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Falexnet-8b05c5eb88d4&user=Abhishek+Verma&userId=18373f6fd34a&source=-----8b05c5eb88d4---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8b05c5eb88d4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Falexnet-8b05c5eb88d4&user=Abhishek+Verma&userId=18373f6fd34a&source=-----8b05c5eb88d4---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8b05c5eb88d4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Falexnet-8b05c5eb88d4&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8b05c5eb88d4--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F8b05c5eb88d4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Falexnet-8b05c5eb88d4&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----8b05c5eb88d4---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----8b05c5eb88d4--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----8b05c5eb88d4--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----8b05c5eb88d4--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----8b05c5eb88d4--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----8b05c5eb88d4--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----8b05c5eb88d4--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----8b05c5eb88d4--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----8b05c5eb88d4--------------------------------", "anchor_text": ""}, {"url": "https://deeptechtalker.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://deeptechtalker.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Abhishek Verma"}, {"url": "https://deeptechtalker.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "396 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F18373f6fd34a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Falexnet-8b05c5eb88d4&user=Abhishek+Verma&userId=18373f6fd34a&source=post_page-18373f6fd34a--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F6854bb98687&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Falexnet-8b05c5eb88d4&newsletterV3=18373f6fd34a&newsletterV3Id=6854bb98687&user=Abhishek+Verma&userId=18373f6fd34a&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}