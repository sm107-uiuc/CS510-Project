{"url": "https://towardsdatascience.com/playing-cards-with-reinforcement-learning-1-3-c2dbabcf1df0", "time": 1683007555.2430441, "path": "towardsdatascience.com/playing-cards-with-reinforcement-learning-1-3-c2dbabcf1df0/", "webpage": {"metadata": {"title": "Playing cards with Reinforcement Learning | by Matyas Amrouche | Towards Data Science", "h1": "Playing cards with Reinforcement Learning", "description": "Reinforcement Learning explained with a simple card game example. 3 common approaches to learn the optimal policy: Monte-Carlo, Sarsa and Value Approximation."}, "outgoing_paragraph_urls": [{"url": "https://www.davidsilver.uk/wp-content/uploads/2020/03/Easy21-Johannes.pdf", "anchor_text": "assignment", "paragraph_index": 3}, {"url": "https://github.com/Matyyas/Easy21", "anchor_text": "notebook", "paragraph_index": 11}, {"url": "https://github.com/Matyyas/Easy21", "anchor_text": "notebook", "paragraph_index": 13}, {"url": "https://github.com/Matyyas/Easy21", "anchor_text": "notebook", "paragraph_index": 23}, {"url": "http://r", "anchor_text": "NB", "paragraph_index": 24}, {"url": "http://incompleteideas.net/book/first/ebook/node76.html", "anchor_text": "equivalent", "paragraph_index": 24}, {"url": "https://www.davidsilver.uk/teaching/", "anchor_text": "topics", "paragraph_index": 25}, {"url": "http://r", "anchor_text": "NB", "paragraph_index": 26}, {"url": "http://r", "anchor_text": "NB", "paragraph_index": 34}, {"url": "https://www.youtube.com/watch?v=2pWv7GOvuf0", "anchor_text": "youtube", "paragraph_index": 37}, {"url": "https://github.com/dennybritz/reinforcement-learning", "anchor_text": "repository", "paragraph_index": 37}], "all_paragraphs": ["Today, we are going to play cards! Well, not exactly\u2026 We are going to build a bot to play a card game like a pro \ud83d\ude0e\ud83d\udd25More specifically the Easy21 game, a variant of Blackjack. I picked this project from David Silver\u2019s Reinforcement Learning (RL) assignment at UCL. I believe it is a fun way to catch some fundamental RL concepts with a real and concrete application that makes sense to everyone:", "Try to beat the dealer in any situation \ud83d\udcaa", "This post is a personal note I use to remind me and illustrate some core RL concepts. So, I would advise a complete beginner in RL to spend some time getting familiar with RL\u2019s vocabulary as I don\u2019t dwell too much on the basics.", "In short, Easy21 is more \u201ccomplex\u201d than BlackJack, because you can subtract cards\u2019 value. You may want to check Fig 1. below to see a full game sequence example (called an episode in RL) or look at the assignment for a full description.", "Now that we better understand what\u2019s going on in Easy21, let\u2019s briefly introduce some fundamental notions of RL to give a more formal context to our problem. RL is about finding the optimal policy for an agent in a given environment. In plain English: RL is about finding the best strategy for a player given the Easy21 rules.", "In RL, an agent learns by reward assignment. For each terminal state in Easy21, winning state (player score > dealer score), loosing state (player score < dealer score) or a draw state (player score = dealer score) is associated a reward, respectively +1, -1 and 0.", "Thanks to the reward, the agent learns what is called the Q value function. In short, the Q value function is the expected reward for a given state-action pair. In plain english, the Q value is the estimation of the potential reward in a particular situation. This Q value function is the beating heart of RL \ud83d\udc98", "Once the Q value function calculated, our agent will know which actions, hit or stick, has the highest expected reward given a game state. Remember in Fig. 1, in the state: {player score = 13, dealer score = 9}, the player sticked and lost. Maybe it was not the optimal action to take, maybe it was\u2026 How one can live with such uncertainty... Keep the faith, RL will tell us \ud83d\udd2e", "This post covers 3 common approaches to estimate the best strategy: Monte-Carlo Control, SARSA(\u03bb) and Value Approximation.", "Enough chit chat, let\u2019s do some real RL \ud83d\ude80", "If you better understand with code, you can directly go to the commented notebook \ud83d\udc47", "First thing first, we must define our environment where our agent will learn which action (hit or stick) to pick in a given state in order to beat the dealer.The environment simply encodes the rules of Easy21. Refer to the notebook to see the implementation details.", "That\u2019s great! But now that we have an environment for our agent, how do we learn the optimal policy to follow inside it?", "In this section, we will explain in plain english what\u2019s going on under the hood of the Monte-Carlo Control (MCC) approach. However, you can go and check the notebook to dive into the details of the implementation.", "Using the MCC approach, we first start to sample an episode \u2194\ufe0f play a game of Easy21 using a \u201ccurrent\u201d strategy, and look at the reward of the terminal state. Then, very simply, we update the Q value function (expected reward) of each state-action pair encountered in the sampled episode regarding the reward observed at the end of the episode.", "The above update step seen from the inside of the MCC agent\u2019s head: \u201cif the played game was a win (resp. loss), the values of the state-action sequence I have taken should be increased (resp. decreased)\u201d.", "Given that the Q value function has been updated, we can build a \u201cnew\u201d strategy upon it and we are ready to sample a new episode using this \u201cnew\u201d strategy.", "Now repeat this sampling/updating procedure until you reach a number of fixed episodes or until the new strategy doesn\u2019t change from the current one\u2026 And here you have a nice piece of optimal Q value function \ud83d\udc68\u200d\ud83c\udf73", "Fig 2. shows the different optimal Q value functions obtained if we choose to sample 1000, 100 000 or 1 000 000 episodes. We logically see that the more samples, the smoother is the optimal Q value function.", "From the above calculated optimal Q value function (the redder the higher expected reward), the agent can now identify which action to pick in any given state to optimize his chances to end in a winning terminal state. That\u2019s the Control my friends \ud83d\udc4a", "Remember Fig 1, in the state: {player score = 13, dealer score = 9}, the player sticked and lost\u2026 Now, our trained agent \ud83e\udd16 knows he would have a higher chance of winning it he hits instead, as we can see in Fig 3.", "With this optimal policy above our agent knows exactly which action to pick in any possible state to optimize his chances to beat the dealer \ud83d\udca5\ud83e\udd4a", "To sum-up, MCC learns by sampling episodes \u2194\ufe0f playing multiple games of Easy21, the more episodes it samples, the better it learns. It might be ok in a simple game as Easy21, however in deeper environment sampling one million or more episodes might be infeasible\u2026 To come over this major drawback, we need to start learning from incomplete episodes.", "Even though the algorithm\u2019s name sounds like some obscure Parseltongue formula \u26a1\ud83d\udc0d, SARSA is simply the acronym of State, Action \u27a1\ufe0f Reward, State\u2019, Action\u2019 which is related to the way the algorithm updates the Q value function towards optimality. The \u03bb is a model\u2019s parameter that we will soon further develop. As usual, you can directly go to the notebook if you better understand with code.", "NB: There are two views for the SARSA approach, a forward and a backward view, which are both equivalent. In this post we will only focus on the backward view, as it is the implementation we used.", "\ud83d\udd0e There are two major concepts at the heart of SARSA algorithm that we will briefly review before diving into the approach (I leave it to the reader to deepen those topics if interested).", "NB: The Q value update is made using an existing estimation, this is why SARSA is also called a bootstrapping method in literature.", "Rather than choosing between TD learning or Monte-Carlo sampling, SARSA(\u03bb) uses the trace factor \u03bb, scaling from 0 to 1, to combine the best of the two concepts. If \u03bb=1 we come back to the classic MCC approach (previously seen in section II) and if \u03bb=0 we make a simple (one step) TD learning update. It\u2019s our job to find and pick the best \u03bb to make the agent learn the most rapidly the optimal policy without damaging its learning.", "Compared to Monte-Carlo, an agent learning with SARSA doesn\u2019t need to sample as much episodes to converge to the optimal policy thanks to its ability to learn from incomplete episodes. That\u2019s pretty useful when episodes are very long and sampling to many of them starts to become computationally too intensive. Check on Fig.6 if you don\u2019t believe me! \ud83d\udc47", "What could still possibly go wrong with all this RL arsenal we have just deployed? \ud83e\udd14 Actually, there is one last thing we have not yet considered\u2026In Easy21, there are 10*21*2 = 420 different state-action pairs. However, in real life, environments often have thousands of possible states and actions or even continuous states and actions domains. In such environments it is not possible anymore to store or explicitly calculate each state-action combination hence the need of a Q value function approximation to free us from this curse of dimensionality.", "At last, a title that makes sense \ud83d\ude4c ! Value Approximation (VA) surprisingly means exactly what it suggests: we are going to approximate the Q value function, rather than computing it for all the possible state-action pairs.", "In order to approximate our Q value, we will need to reduce the range of possibilities. To do so, we can imagine that a Easy21 expert player, with a deep knowledge of the game, guides us to build the below features (Fig. 7). This expert knows what dealer\u2019s score intervals and player\u2019s score intervals are meaningful to distinguish to play the game like a pro. As always in ML, the feature engineering step is directly related to the quality of the field expert\u2019s knowledge and might be non-trivial regarding the environment complexity.", "Remembering all the above approaches, we could see the Q value function as a lookup table for every state-action pair. Now, we are modeling Q using 3 \u2217 6 \u2217 2 = 36 possible combination of the engineered features, which is much less than our previous 420 possible state-action pairs.", "The Q value function is not a lookup table anymore, but a function depending on some weights. Now, we need to learn the best weights to get our approximation function as close as possible to the optimal Q value function.Therefore, the \u201cclassic\u201d update step seen in previous sections is replaced by an update of the weights \u03b8 of the linear function. Pretty straightforward, right?", "NB: In our case we implemented a linear value approximation. However, one can consider to use a non-linear approach and enter the amazing field of Deep Reinforcement Learning which approximates Q value function using neural networks. Using neural networks leads to higher representational capabilities and makes features engineering useless. To see more in the next episode\u2026", "Et voil\u00e0 \ud83d\udc4c! Our Reinforcement Learning journey is now over, you are one of the brave to reach the end of this post and your soul will undoubtedly go to Valhalla \ud83d\ude4c", "Ressources:- [1] The post\u2019s notebook (all the code/plots are available there):", "- [2] David Silver\u2019s full online and free RL course at UCL (youtube)- [3] This amazing Github repository gathering most known RL algorithms", "ML & NLP for Search Relevance @ Leboncoin \ud83d\udce6"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fc2dbabcf1df0&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplaying-cards-with-reinforcement-learning-1-3-c2dbabcf1df0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplaying-cards-with-reinforcement-learning-1-3-c2dbabcf1df0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplaying-cards-with-reinforcement-learning-1-3-c2dbabcf1df0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplaying-cards-with-reinforcement-learning-1-3-c2dbabcf1df0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://matyasamrouche.medium.com/?source=post_page-----c2dbabcf1df0--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c2dbabcf1df0--------------------------------", "anchor_text": ""}, {"url": "https://matyasamrouche.medium.com/?source=post_page-----c2dbabcf1df0--------------------------------", "anchor_text": "Matyas Amrouche"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fad07e8173fb2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplaying-cards-with-reinforcement-learning-1-3-c2dbabcf1df0&user=Matyas+Amrouche&userId=ad07e8173fb2&source=post_page-ad07e8173fb2----c2dbabcf1df0---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----c2dbabcf1df0--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc2dbabcf1df0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplaying-cards-with-reinforcement-learning-1-3-c2dbabcf1df0&user=Matyas+Amrouche&userId=ad07e8173fb2&source=-----c2dbabcf1df0---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc2dbabcf1df0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplaying-cards-with-reinforcement-learning-1-3-c2dbabcf1df0&source=-----c2dbabcf1df0---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://www.montecarlosbm.com/fr/casino-monaco/casino-monte-carlo", "anchor_text": "source"}, {"url": "https://www.davidsilver.uk/wp-content/uploads/2020/03/Easy21-Johannes.pdf", "anchor_text": "assignment"}, {"url": "https://github.com/Matyyas/Easy21", "anchor_text": "Matyyas/Easy21Implementation of various RL algorithms to learn to play Easy21 - Matyyas/Easy21github.com"}, {"url": "https://github.com/Matyyas/Easy21", "anchor_text": "notebook"}, {"url": "https://github.com/Matyyas/Easy21", "anchor_text": "notebook"}, {"url": "https://github.com/Matyyas/Easy21", "anchor_text": "notebook"}, {"url": "http://r", "anchor_text": "NB"}, {"url": "http://incompleteideas.net/book/first/ebook/node76.html", "anchor_text": "equivalent"}, {"url": "https://www.davidsilver.uk/teaching/", "anchor_text": "topics"}, {"url": "https://www.davidsilver.uk/wp-content/uploads/2020/03/MC-TD.pdf", "anchor_text": "source"}, {"url": "http://r", "anchor_text": "NB"}, {"url": "https://www.davidsilver.uk/wp-content/uploads/2020/03/control.pdf", "anchor_text": "source"}, {"url": "http://r", "anchor_text": "NB"}, {"url": "https://github.com/Matyyas/Easy21", "anchor_text": "Matyyas/Easy21Implementation of various RL algorithms to learn to play Easy21 - Matyyas/Easy21github.com"}, {"url": "https://www.youtube.com/watch?v=2pWv7GOvuf0", "anchor_text": "youtube"}, {"url": "https://github.com/dennybritz/reinforcement-learning", "anchor_text": "repository"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----c2dbabcf1df0---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----c2dbabcf1df0---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----c2dbabcf1df0---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----c2dbabcf1df0---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/ai?source=post_page-----c2dbabcf1df0---------------ai-----------------", "anchor_text": "AI"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc2dbabcf1df0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplaying-cards-with-reinforcement-learning-1-3-c2dbabcf1df0&user=Matyas+Amrouche&userId=ad07e8173fb2&source=-----c2dbabcf1df0---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc2dbabcf1df0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplaying-cards-with-reinforcement-learning-1-3-c2dbabcf1df0&user=Matyas+Amrouche&userId=ad07e8173fb2&source=-----c2dbabcf1df0---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc2dbabcf1df0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplaying-cards-with-reinforcement-learning-1-3-c2dbabcf1df0&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://matyasamrouche.medium.com/?source=post_page-----c2dbabcf1df0--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c2dbabcf1df0--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fad07e8173fb2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplaying-cards-with-reinforcement-learning-1-3-c2dbabcf1df0&user=Matyas+Amrouche&userId=ad07e8173fb2&source=post_page-ad07e8173fb2----c2dbabcf1df0---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F5ead0b158385&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplaying-cards-with-reinforcement-learning-1-3-c2dbabcf1df0&newsletterV3=ad07e8173fb2&newsletterV3Id=5ead0b158385&user=Matyas+Amrouche&userId=ad07e8173fb2&source=-----c2dbabcf1df0---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://matyasamrouche.medium.com/?source=post_page-----c2dbabcf1df0--------------------------------", "anchor_text": "Written by Matyas Amrouche"}, {"url": "https://matyasamrouche.medium.com/followers?source=post_page-----c2dbabcf1df0--------------------------------", "anchor_text": "158 Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----c2dbabcf1df0--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fad07e8173fb2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplaying-cards-with-reinforcement-learning-1-3-c2dbabcf1df0&user=Matyas+Amrouche&userId=ad07e8173fb2&source=post_page-ad07e8173fb2----c2dbabcf1df0---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F5ead0b158385&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplaying-cards-with-reinforcement-learning-1-3-c2dbabcf1df0&newsletterV3=ad07e8173fb2&newsletterV3Id=5ead0b158385&user=Matyas+Amrouche&userId=ad07e8173fb2&source=-----c2dbabcf1df0---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/short-text-topic-modeling-70e50a57c883?source=author_recirc-----c2dbabcf1df0----0---------------------91734a54_6b66_45f5_b6db_8fee139f6e70-------", "anchor_text": ""}, {"url": "https://matyasamrouche.medium.com/?source=author_recirc-----c2dbabcf1df0----0---------------------91734a54_6b66_45f5_b6db_8fee139f6e70-------", "anchor_text": ""}, {"url": "https://matyasamrouche.medium.com/?source=author_recirc-----c2dbabcf1df0----0---------------------91734a54_6b66_45f5_b6db_8fee139f6e70-------", "anchor_text": "Matyas Amrouche"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----c2dbabcf1df0----0---------------------91734a54_6b66_45f5_b6db_8fee139f6e70-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/short-text-topic-modeling-70e50a57c883?source=author_recirc-----c2dbabcf1df0----0---------------------91734a54_6b66_45f5_b6db_8fee139f6e70-------", "anchor_text": "Short Text Topic ModelingIntuition and (some) maths to understand Topic Modeling on short texts"}, {"url": "https://towardsdatascience.com/short-text-topic-modeling-70e50a57c883?source=author_recirc-----c2dbabcf1df0----0---------------------91734a54_6b66_45f5_b6db_8fee139f6e70-------", "anchor_text": "\u00b710 min read\u00b7Aug 22, 2019"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F70e50a57c883&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fshort-text-topic-modeling-70e50a57c883&user=Matyas+Amrouche&userId=ad07e8173fb2&source=-----70e50a57c883----0-----------------clap_footer----91734a54_6b66_45f5_b6db_8fee139f6e70-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/short-text-topic-modeling-70e50a57c883?source=author_recirc-----c2dbabcf1df0----0---------------------91734a54_6b66_45f5_b6db_8fee139f6e70-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "14"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F70e50a57c883&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fshort-text-topic-modeling-70e50a57c883&source=-----c2dbabcf1df0----0-----------------bookmark_preview----91734a54_6b66_45f5_b6db_8fee139f6e70-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----c2dbabcf1df0----1---------------------91734a54_6b66_45f5_b6db_8fee139f6e70-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----c2dbabcf1df0----1---------------------91734a54_6b66_45f5_b6db_8fee139f6e70-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----c2dbabcf1df0----1---------------------91734a54_6b66_45f5_b6db_8fee139f6e70-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----c2dbabcf1df0----1---------------------91734a54_6b66_45f5_b6db_8fee139f6e70-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----c2dbabcf1df0----1---------------------91734a54_6b66_45f5_b6db_8fee139f6e70-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----c2dbabcf1df0----1---------------------91734a54_6b66_45f5_b6db_8fee139f6e70-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----91734a54_6b66_45f5_b6db_8fee139f6e70-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----c2dbabcf1df0----1---------------------91734a54_6b66_45f5_b6db_8fee139f6e70-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----c2dbabcf1df0----1-----------------bookmark_preview----91734a54_6b66_45f5_b6db_8fee139f6e70-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----c2dbabcf1df0----2---------------------91734a54_6b66_45f5_b6db_8fee139f6e70-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----c2dbabcf1df0----2---------------------91734a54_6b66_45f5_b6db_8fee139f6e70-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----c2dbabcf1df0----2---------------------91734a54_6b66_45f5_b6db_8fee139f6e70-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----c2dbabcf1df0----2---------------------91734a54_6b66_45f5_b6db_8fee139f6e70-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----c2dbabcf1df0----2---------------------91734a54_6b66_45f5_b6db_8fee139f6e70-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----c2dbabcf1df0----2---------------------91734a54_6b66_45f5_b6db_8fee139f6e70-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----2-----------------clap_footer----91734a54_6b66_45f5_b6db_8fee139f6e70-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----c2dbabcf1df0----2---------------------91734a54_6b66_45f5_b6db_8fee139f6e70-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----c2dbabcf1df0----2-----------------bookmark_preview----91734a54_6b66_45f5_b6db_8fee139f6e70-------", "anchor_text": ""}, {"url": "https://medium.com/swlh/tips-to-deploy-python-package-on-aws-lambda-fbf4bed4dc87?source=author_recirc-----c2dbabcf1df0----3---------------------91734a54_6b66_45f5_b6db_8fee139f6e70-------", "anchor_text": ""}, {"url": "https://matyasamrouche.medium.com/?source=author_recirc-----c2dbabcf1df0----3---------------------91734a54_6b66_45f5_b6db_8fee139f6e70-------", "anchor_text": ""}, {"url": "https://matyasamrouche.medium.com/?source=author_recirc-----c2dbabcf1df0----3---------------------91734a54_6b66_45f5_b6db_8fee139f6e70-------", "anchor_text": "Matyas Amrouche"}, {"url": "https://medium.com/swlh?source=author_recirc-----c2dbabcf1df0----3---------------------91734a54_6b66_45f5_b6db_8fee139f6e70-------", "anchor_text": "The Startup"}, {"url": "https://medium.com/swlh/tips-to-deploy-python-package-on-aws-lambda-fbf4bed4dc87?source=author_recirc-----c2dbabcf1df0----3---------------------91734a54_6b66_45f5_b6db_8fee139f6e70-------", "anchor_text": "Tips to deploy python package on AWS LambdaAnd keep your git clean!"}, {"url": "https://medium.com/swlh/tips-to-deploy-python-package-on-aws-lambda-fbf4bed4dc87?source=author_recirc-----c2dbabcf1df0----3---------------------91734a54_6b66_45f5_b6db_8fee139f6e70-------", "anchor_text": "\u00b72 min read\u00b7Jun 5, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fswlh%2Ffbf4bed4dc87&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fswlh%2Ftips-to-deploy-python-package-on-aws-lambda-fbf4bed4dc87&user=Matyas+Amrouche&userId=ad07e8173fb2&source=-----fbf4bed4dc87----3-----------------clap_footer----91734a54_6b66_45f5_b6db_8fee139f6e70-------", "anchor_text": ""}, {"url": "https://medium.com/swlh/tips-to-deploy-python-package-on-aws-lambda-fbf4bed4dc87?source=author_recirc-----c2dbabcf1df0----3---------------------91734a54_6b66_45f5_b6db_8fee139f6e70-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffbf4bed4dc87&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fswlh%2Ftips-to-deploy-python-package-on-aws-lambda-fbf4bed4dc87&source=-----c2dbabcf1df0----3-----------------bookmark_preview----91734a54_6b66_45f5_b6db_8fee139f6e70-------", "anchor_text": ""}, {"url": "https://matyasamrouche.medium.com/?source=post_page-----c2dbabcf1df0--------------------------------", "anchor_text": "See all from Matyas Amrouche"}, {"url": "https://towardsdatascience.com/?source=post_page-----c2dbabcf1df0--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----c2dbabcf1df0----0---------------------66bcce52_55ce_4ca1_89c3_9066a7e534a2-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData?source=read_next_recirc-----c2dbabcf1df0----0---------------------66bcce52_55ce_4ca1_89c3_9066a7e534a2-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData?source=read_next_recirc-----c2dbabcf1df0----0---------------------66bcce52_55ce_4ca1_89c3_9066a7e534a2-------", "anchor_text": "Andrew Austin"}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----c2dbabcf1df0----0---------------------66bcce52_55ce_4ca1_89c3_9066a7e534a2-------", "anchor_text": "AI Anyone Can Understand Part 1: Reinforcement LearningReinforcement learning is a way for machines to learn by trying different things and seeing what works best. For example, a robot could\u2026"}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----c2dbabcf1df0----0---------------------66bcce52_55ce_4ca1_89c3_9066a7e534a2-------", "anchor_text": "\u00b74 min read\u00b7Dec 11, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F6c3b3d623a2d&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40MoneyAndData%2Fai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d&user=Andrew+Austin&userId=42d388912d13&source=-----6c3b3d623a2d----0-----------------clap_footer----66bcce52_55ce_4ca1_89c3_9066a7e534a2-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----c2dbabcf1df0----0---------------------66bcce52_55ce_4ca1_89c3_9066a7e534a2-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6c3b3d623a2d&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40MoneyAndData%2Fai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d&source=-----c2dbabcf1df0----0-----------------bookmark_preview----66bcce52_55ce_4ca1_89c3_9066a7e534a2-------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----c2dbabcf1df0----1---------------------66bcce52_55ce_4ca1_89c3_9066a7e534a2-------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk?source=read_next_recirc-----c2dbabcf1df0----1---------------------66bcce52_55ce_4ca1_89c3_9066a7e534a2-------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk?source=read_next_recirc-----c2dbabcf1df0----1---------------------66bcce52_55ce_4ca1_89c3_9066a7e534a2-------", "anchor_text": "Steve Roberts"}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----c2dbabcf1df0----1---------------------66bcce52_55ce_4ca1_89c3_9066a7e534a2-------", "anchor_text": "State Values and Policy Evaluation in 5 minutesAn Introduction to Reinforcement Learning"}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----c2dbabcf1df0----1---------------------66bcce52_55ce_4ca1_89c3_9066a7e534a2-------", "anchor_text": "\u00b75 min read\u00b7Jan 11"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2Ff3e00f3c1a50&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40tinkertytonk%2Fstate-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50&user=Steve+Roberts&userId=6b6735266652&source=-----f3e00f3c1a50----1-----------------clap_footer----66bcce52_55ce_4ca1_89c3_9066a7e534a2-------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----c2dbabcf1df0----1---------------------66bcce52_55ce_4ca1_89c3_9066a7e534a2-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff3e00f3c1a50&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40tinkertytonk%2Fstate-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50&source=-----c2dbabcf1df0----1-----------------bookmark_preview----66bcce52_55ce_4ca1_89c3_9066a7e534a2-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----c2dbabcf1df0----0---------------------66bcce52_55ce_4ca1_89c3_9066a7e534a2-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----c2dbabcf1df0----0---------------------66bcce52_55ce_4ca1_89c3_9066a7e534a2-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----c2dbabcf1df0----0---------------------66bcce52_55ce_4ca1_89c3_9066a7e534a2-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----c2dbabcf1df0----0---------------------66bcce52_55ce_4ca1_89c3_9066a7e534a2-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----c2dbabcf1df0----0---------------------66bcce52_55ce_4ca1_89c3_9066a7e534a2-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----c2dbabcf1df0----0---------------------66bcce52_55ce_4ca1_89c3_9066a7e534a2-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----0-----------------clap_footer----66bcce52_55ce_4ca1_89c3_9066a7e534a2-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----c2dbabcf1df0----0---------------------66bcce52_55ce_4ca1_89c3_9066a7e534a2-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----c2dbabcf1df0----0-----------------bookmark_preview----66bcce52_55ce_4ca1_89c3_9066a7e534a2-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----c2dbabcf1df0----1---------------------66bcce52_55ce_4ca1_89c3_9066a7e534a2-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----c2dbabcf1df0----1---------------------66bcce52_55ce_4ca1_89c3_9066a7e534a2-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----c2dbabcf1df0----1---------------------66bcce52_55ce_4ca1_89c3_9066a7e534a2-------", "anchor_text": "The PyCoach"}, {"url": "https://artificialcorner.com/?source=read_next_recirc-----c2dbabcf1df0----1---------------------66bcce52_55ce_4ca1_89c3_9066a7e534a2-------", "anchor_text": "Artificial Corner"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----c2dbabcf1df0----1---------------------66bcce52_55ce_4ca1_89c3_9066a7e534a2-------", "anchor_text": "You\u2019re Using ChatGPT Wrong! Here\u2019s How to Be Ahead of 99% of ChatGPT UsersMaster ChatGPT by learning prompt engineering."}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----c2dbabcf1df0----1---------------------66bcce52_55ce_4ca1_89c3_9066a7e534a2-------", "anchor_text": "\u00b77 min read\u00b7Mar 17"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fartificial-corner%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&user=The+PyCoach&userId=fb44e21903f3&source=-----886a50dabc54----1-----------------clap_footer----66bcce52_55ce_4ca1_89c3_9066a7e534a2-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----c2dbabcf1df0----1---------------------66bcce52_55ce_4ca1_89c3_9066a7e534a2-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "276"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&source=-----c2dbabcf1df0----1-----------------bookmark_preview----66bcce52_55ce_4ca1_89c3_9066a7e534a2-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----c2dbabcf1df0----2---------------------66bcce52_55ce_4ca1_89c3_9066a7e534a2-------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=read_next_recirc-----c2dbabcf1df0----2---------------------66bcce52_55ce_4ca1_89c3_9066a7e534a2-------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=read_next_recirc-----c2dbabcf1df0----2---------------------66bcce52_55ce_4ca1_89c3_9066a7e534a2-------", "anchor_text": "Wouter van Heeswijk, PhD"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----c2dbabcf1df0----2---------------------66bcce52_55ce_4ca1_89c3_9066a7e534a2-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----c2dbabcf1df0----2---------------------66bcce52_55ce_4ca1_89c3_9066a7e534a2-------", "anchor_text": "Proximal Policy Optimization (PPO) ExplainedThe journey from REINFORCE to the go-to algorithm in continuous control"}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----c2dbabcf1df0----2---------------------66bcce52_55ce_4ca1_89c3_9066a7e534a2-------", "anchor_text": "\u00b713 min read\u00b7Nov 29, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fabed1952457b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fproximal-policy-optimization-ppo-explained-abed1952457b&user=Wouter+van+Heeswijk%2C+PhD&userId=33f45c9ab481&source=-----abed1952457b----2-----------------clap_footer----66bcce52_55ce_4ca1_89c3_9066a7e534a2-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----c2dbabcf1df0----2---------------------66bcce52_55ce_4ca1_89c3_9066a7e534a2-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fabed1952457b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fproximal-policy-optimization-ppo-explained-abed1952457b&source=-----c2dbabcf1df0----2-----------------bookmark_preview----66bcce52_55ce_4ca1_89c3_9066a7e534a2-------", "anchor_text": ""}, {"url": "https://medium.datadriveninvestor.com/finrl-meta-market-environments-and-benchmarks-for-data-driven-financial-reinforcement-learning-7af8e747c4bd?source=read_next_recirc-----c2dbabcf1df0----3---------------------66bcce52_55ce_4ca1_89c3_9066a7e534a2-------", "anchor_text": ""}, {"url": "https://byfintech.medium.com/?source=read_next_recirc-----c2dbabcf1df0----3---------------------66bcce52_55ce_4ca1_89c3_9066a7e534a2-------", "anchor_text": ""}, {"url": "https://byfintech.medium.com/?source=read_next_recirc-----c2dbabcf1df0----3---------------------66bcce52_55ce_4ca1_89c3_9066a7e534a2-------", "anchor_text": "Bruce Yang ByFinTech"}, {"url": "https://medium.datadriveninvestor.com/?source=read_next_recirc-----c2dbabcf1df0----3---------------------66bcce52_55ce_4ca1_89c3_9066a7e534a2-------", "anchor_text": "DataDrivenInvestor"}, {"url": "https://medium.datadriveninvestor.com/finrl-meta-market-environments-and-benchmarks-for-data-driven-financial-reinforcement-learning-7af8e747c4bd?source=read_next_recirc-----c2dbabcf1df0----3---------------------66bcce52_55ce_4ca1_89c3_9066a7e534a2-------", "anchor_text": "FinRL-Meta: Market Environments and Benchmarks for Data-Driven Financial Reinforcement LearningNeurIPS 2022 Datasets and Benchmarks."}, {"url": "https://medium.datadriveninvestor.com/finrl-meta-market-environments-and-benchmarks-for-data-driven-financial-reinforcement-learning-7af8e747c4bd?source=read_next_recirc-----c2dbabcf1df0----3---------------------66bcce52_55ce_4ca1_89c3_9066a7e534a2-------", "anchor_text": "\u00b79 min read\u00b7Nov 13, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fdatadriveninvestor%2F7af8e747c4bd&operation=register&redirect=https%3A%2F%2Fmedium.datadriveninvestor.com%2Ffinrl-meta-market-environments-and-benchmarks-for-data-driven-financial-reinforcement-learning-7af8e747c4bd&user=Bruce+Yang+ByFinTech&userId=a878fc45fb3f&source=-----7af8e747c4bd----3-----------------clap_footer----66bcce52_55ce_4ca1_89c3_9066a7e534a2-------", "anchor_text": ""}, {"url": "https://medium.datadriveninvestor.com/finrl-meta-market-environments-and-benchmarks-for-data-driven-financial-reinforcement-learning-7af8e747c4bd?source=read_next_recirc-----c2dbabcf1df0----3---------------------66bcce52_55ce_4ca1_89c3_9066a7e534a2-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7af8e747c4bd&operation=register&redirect=https%3A%2F%2Fmedium.datadriveninvestor.com%2Ffinrl-meta-market-environments-and-benchmarks-for-data-driven-financial-reinforcement-learning-7af8e747c4bd&source=-----c2dbabcf1df0----3-----------------bookmark_preview----66bcce52_55ce_4ca1_89c3_9066a7e534a2-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----c2dbabcf1df0--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----c2dbabcf1df0--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----c2dbabcf1df0--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----c2dbabcf1df0--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----c2dbabcf1df0--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----c2dbabcf1df0--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----c2dbabcf1df0--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----c2dbabcf1df0--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----c2dbabcf1df0--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----c2dbabcf1df0--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}