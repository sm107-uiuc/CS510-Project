{"url": "https://towardsdatascience.com/decision-trees-as-you-should-have-learned-them-99862469493e", "time": 1683018285.687754, "path": "towardsdatascience.com/decision-trees-as-you-should-have-learned-them-99862469493e/", "webpage": {"metadata": {"title": "Decision Trees: As You Should Have Learned Them | by Mauricio Letelier | Towards Data Science", "h1": "Decision Trees: As You Should Have Learned Them", "description": "If you always wanted to learn decision trees, just by reading this, you\u2019ve received a beautiful opportunity, a stroke of luck, I might say. But as entrepreneurs declare, \u201cit\u2019s not enough to be in the\u2026"}, "outgoing_paragraph_urls": [{"url": "https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html", "anchor_text": "scikit-learn documentation", "paragraph_index": 9}, {"url": "https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity", "anchor_text": "Wikipedia\u2019s", "paragraph_index": 19}, {"url": "https://scikit-learn.org/stable/modules/tree.html", "anchor_text": "documentation", "paragraph_index": 71}, {"url": "https://stackoverflow.com/questions/46756606/what-does-splitter-attribute-in-sklearns-decisiontreeclassifier-do/46759065", "anchor_text": "Stackoverflow", "paragraph_index": 71}, {"url": "http://www.linkedin.com/in/maletelier", "anchor_text": "Linkedin", "paragraph_index": 72}, {"url": "https://twitter.com/maletelier", "anchor_text": "Twitter", "paragraph_index": 72}, {"url": "https://www.linkedin.com/in/maletelier", "anchor_text": "https://www.linkedin.com/in/maletelier", "paragraph_index": 74}], "all_paragraphs": ["If you always wanted to learn decision trees, just by reading this, you\u2019ve received a beautiful opportunity, a stroke of luck, I might say. But as entrepreneurs declare, \u201cit\u2019s not enough to be in the right place; you also need to take the opportunity when it comes\u201d. So, consider giving yourself some time and get comfortable, because what\u2019s coming it\u2019s not the shortest guide, but it might be the best.", "Decision trees are the foundation of the most well performing algorithms on Kaggle competitions and real-life. An indicator of this is that you are certainly going to collide with a \u201cmax_depth\u201d on almost every ensemble. That\u2019s precisely the reason why the story you are about to read uses the most relevant hyperparameters to build its narrative.", "I never really learn decision trees until I decided to start looking at every hyperparameter. After studying them in detail, all the fog in my mind slowly vanished. I wrote this thinking of the guide I wish I had when I started looking for information, so it\u2019s intended to be very easy to follow.", "Don\u2019t get me wrong. I don\u2019t want to look like a smug repeating the typical smarty phrase \u201cyou should know all the foundations before doing anything\u201d because I don\u2019t believe in it that much.", "The learning process is not linear, and if you want to train your first model, maybe it\u2019s better for you just to know what a random forest is and go for it. It\u2019s a significant step!. But after your first model training, I\u2019m sure that you will realize that some problems may arise.", "You will have neither the compute infrastructure nor the time to try every hyperparameter combination. Understanding what\u2019s happening behind the scenes when you tweak them will give you a smarter starting point.", "I can assure you, and remember this one, the next time you define your hyperparameter range, you will start hypothesizing on the best combination, checking which one worked the best. That exercise will improve your skills a lot more than just a blindfolded grid-search.", "For this guide, you don\u2019t need any prior knowledge of decision trees. That\u2019s the magic. The algorithm will be built upon the hyperparameters. We can kill two birds with one shot: learn how decision trees work and how the hyperparameters modify their behavior. I really encourage you to read every hyperparameter because we will be building blocks on the algorithm\u2019s underlying mechanics while we review them.", "Just for the sake of the conversation, I want to clarify the parameters used are those on the scikit-learn implementation for classification. The idea is to mix intuition and math to create the perfect learning combination for you.", "Note: At the time this is being written, there are 12 parameters not deprecated on the scikit-learn documentation. As there are so many, we will focus just on 6. But if you support this, I will certainly come back with the others. Also, the word hyperparameter and parameter will be used interchangeably.", "Decision trees are implemented on scikit using an optimized version of the CART algorithm. The procedure is to ask questions about the data to split it into two groups. This is very important because other implementations can split the data into more than two groups at once.", "This happens over and over until one of our beloved parameters stops this process or when the data cannot be split anymore (1 sample). Before the first question, the place where we have all the data is called the \u201croot\u201d. After that, every subset of samples is a \u201cnode\u201d if there is a split after it. If this node can\u2019t be split, we will call it a \u201cleaf\u201d.", "On the image, you can see the overall structure. The illustration and title are on the left of its corresponding boxes. If you don\u2019t understand what\u2019s happening here, don\u2019t hesitate to keep reading, everything will be clear after some minutes.", "Ok, that\u2019s a pretty general approximation, but you might have some fair questions like: \u201chow to decide the variable for the split?\u201d, \u201chow to choose the threshold for the split?\u201d, \u201ccan I use the same feature again?\u201d or \u201cis it the depth related to the number of splits?\u201d, among many others.", "The goal of what you are going to read is to answer those questions and a lot more. On every (hyper)parameter, you will discover new clues that will help you to actually see how trees grow. And, if someday someone gives you some data, a pencil, and a piece of paper, you will be well equipped to solve the problem by yourself (super useful!).", "Are you ready? Let\u2019s do it!", "This parameter isn\u2019t a \u201cgame-changer\u201d but it\u2019s pretty useful to dispel how the basic mechanics of decision trees work. As the old saying says, \u201clet\u2019s start by the beginning\u201d.", "As we saw, on every iteration, we have to determine which feature we will select to ask the question to our data. Because of that, we should have a method to rank different options.", "The functions to measure the quality of the split that we will review are two: Gini and entropy.", "Gini: a great way to understand it is by checking Wikipedia\u2019s definition \u201cGini impurity is a measure of how often a randomly chosen element from the set would be incorrectly labeled if it was randomly labeled according to the distribution of labels in the subset\u201d.", "To better explain this concept, we will use the most original example that I could think of: balls. Imagine a case where we have 5 balls, we know the weight of these balls, they can be big or small and red or green.", "The goal is to predict the color of the balls based on their weight and size. Now is when we should start using our Gini measurement to choose if we will split our data based on the ball\u2019s weight or size.", "Before the mathematical formulation, let\u2019s try to figure out what this measurement is trying to do. Let\u2019s replicate the Gini impurity definition for this particular case in our \u201croot node\u201d. First, we need to randomly select an element, and after that, we need to label it following the original distribution.", "In this case, we selected the big green ball of 4 kilos, and we should label it as red with a probability of 0.6 and as green in the remaining 0.4. The problem is that we can\u2019t calculate it with just one example. The Gini impurity aims to determine how often, after this procedure, we would end up with an incorrectly labeled sample as a whole.", "A possible solution could be to simulate this process and repeating it enough times to get a good estimate. We can try that with the following code.", "You will probably get something like that number. That\u2019s the probability of mislabeling a sample, and we\u2019ve used none other information than the distribution of the target value.", "The calculation is needed for every iteration, and doing it in that way can be a bit slow, but don\u2019t be scared. We can do better than that. If we go back to our flowchart, we can use probabilities to create a deterministic formula.", "As you can see the probability of getting it wrong is the sum of the probability of choosing a green sample labeled as red plus the opposite case.", "Awesome, we just calculate our first Gini impurity! Now we can make an extra effort to generalize what we just did for n classes. The first step is to select the probability of choosing the first class (P(Class1)), and then multiply it by the probability of mislabeling it, which is (1-P(Class1)). As we saw in our most recent case, we have to sum this calculation for every class.", "Unbelievable! Now we can say that we really understand how we calculate the Gini impurity. After seeing all of this information, we can assert that this is an attempt to measure how mislabeled our node will be. The algorithm will select the variable which minimizes it.", "Ok, we are getting closer to taking our first decision, but nobody said it would be easy. We still have to solve some other minor issues. First, how we deal with different variable types?.", "Well, categorical variables must be \u201cone-hot-encoded\u201d which means transforming categories into binary variables. Therefore the split is pretty intuitive. For continuous variables is a little different: the selection of the threshold should be optimized to create the best possible split.", "To show you the importance of selecting the exact number of the split for a continuous feature, I will show you two possibles splits based on the weight. You will see a significant difference between choosing a \u201cgood\u201d or a \u201cbad\u201d split using the same variable!. We can start using 3.5 kilos to split the balls and calculate the Gini.", "Ok, we have learned how to calculate one particular split, but what to do when we have a split with two nodes (which will always be the case when we want to deliberate between splits). As you saw in the figure, we have to weight them based on the samples belonging to each node. Now let\u2019s try with 2.5 kilos as our split criteria.", "It looks better, and it\u2019s objectively better. The algorithm chooses the best Gini impurity to do this split, and now we know how to compare them. In case you are still a little skeptical about all the beautiful balls we just saw, we can invoke scikit-learn for you to believe me.", "The visualization is showing the variable used; \u201cX[0]\u201d which is the weight for this case. It also shows the Gini values for every node, the number of samples, and equals to \u201cvalue\u201d you can see the distribution of the classes (green at the left and red at the right). Everything was exactly as we expected it to be.", "I just earn some credibility, don\u2019t I?. To take advantage of this ephemeral moment of trust, I will show you an interesting last thing before we move on. As you know, we have another feature: the size. That\u2019s a binary variable, and because of that, it\u2019s easy to determine the threshold to be selected (any value between 0 and 1).", "Wow! The size is as good as the weight. So, why did scikit chose the latter?. Don\u2019t worry. The implementation doesn\u2019t have any preferences. If there is a tie, it will make a random choice, and that\u2019s why the \u201crandom_state\u201d becomes relevant. This parameter controls the randomness of decisions like these ones. Changing the parameter from 42 to 40 will do the trick.", "\u201cX[1]\u201d represents the weight, and the threshold was obviously set to 0.5. We learned that the same tree can take a different path if we don\u2019t clarify the \u201crandom_state\u201d. There can be ties among variables!.", "After the last magical trick, we can close the Gini criteria discussion to give way to our next guest: entropy", "Entropy: If the Gini impurity tries to measure how mislabeled the node will be, the entropy will try to estimate how uncertain the outcome is.", "The history behind entropy is fantastic, starts with thermodynamics, advances to information theory to reach us here, on decision trees. It\u2019s incredible enough to deserve its own blog post (in which I\u2019m working). So, to avoid straying so far off the road, I will straight away drop the formula. I know it might come out of nowhere, but it\u2019s the price for a goal-oriented post.", "For our initial level, we know that the probabilities are 0.6 and 0.4 for red and green balls, respectively. We can calculate the entropy for that case.", "You can try on your own combinations like [1,0] or [0.5,0.5]. With the first, the entropy is 0, zero uncertainty, for the latter is 1. You can see that our 0.97 is very close to the full uncertainty scenario.", "If we change the criterion parameter to \u201centropy\u201d, scikit will display it on the visualizations.", "Just to summarize, we can compare how Gini and entropy will fluctuate given every possible combination for 5 balls. Below a little gift for you, the calculation \u201cfrom scratch\u201d of our two different possible parameters.", "In the last section, we assumed that the algorithm would look for the best split every time we have a continuous variable. But \u201csplitter\u201d gives us some flexibility on this assumption. There are two options: \u201cbest\u201d and \u201crandom\u201d. The default parameter is \u201cbest\u201d.", "When choosing \u201cbest\u201d, unsurprisingly, the algorithm will choose the best split \u2014 like in the last section \u2014 , but when choosing \u201crandom\u201d, the split will be just a random number. To show this clearly, I will present the same example we used before, but this time using only the weight as the explanatory variable.", "As you can see, the threshold was set to 3.975, giving us an unfortunate split. It\u2019s important to say that this \u201cpoorly\u201d split is not always a bad thing. Now you know that the algorithm looks by default the best split on every iteration, but that\u2019s not necessarily the optimal way to find the global optimum. Using a random split could lead to some variability, which sometimes helps us find a better tree.", "To understand how \u201cmax_depth\u201d works, we have to look at the tree\u2019s internal structure. Do you remember that we never let the tree grow more than one split? That was because we force the algorithm to stay on a max depth of 1. The depth is not the number of splits. It\u2019s the maximum number of generations after the root node.", "If you think that a node receiving an arrow from another is a daughter, setting the max depth to one will allow the root node to be, at tops, a mother. If we set the parameter to two, the root node can\u2019t go further than a grandmother, and that logic goes on and on.", "For this and the following sections, we will use a new example, I know that you will miss the balls, but the new one will do a better job to explain what\u2019s coming (and it\u2019s about balls too). Imagine a simple classification case where you only have one feature (weight in ounces) and four classes (types of balls). To make this even easier, let\u2019s assume a direct relation between them: when the variable is within a specific range, it will always belong to the same class (as the last image).", "As the code snippet shows, if our feature is between 0 and 3, the class will always be a golf ball, the same behavior is expected for the other cases. So let\u2019s understand how \u201cmax_depth\u201d impacts the model. Let\u2019s train the first one using a \u201cmax_depth\u201d of 1.", "The model defined the optimal threshold (10.495). The classes on the left side belonged to our 0 or 1, and those on the right to 2 and 3. The model hadn\u2019t the chance to create another step to really separate them. So let\u2019s try a \u201cmax_depth\u201d of 2 and see what is happening.", "The first important thing for you to notice is that variables can be repeated, so using a \u201cmax_depth\u201d of the same size of the number of features does not necessarily cover every base. If a variable is the best split in a particular scenario, it can show up independent of its first appearances.", "Using a \u201cmax_depth\u201d of 2 it's actually enough in this case to separate perfectly every branch. But there is no rule of thumb to determine how profound the tree should go. Let\u2019s go for the exact same case, but changing the test split from 0.2 to 0.33.", "In this case, setting a \u201cmax_depth\u201d of 2 wasn\u2019t enough to separate every case. This was due to the first split, which happened in a bigger threshold. Using a \u201cmax_depth\u201d of 3 will do the trick in this case.", "Here comes the second important thing to learn, subtle changes in the structure of the data may result in different depths of the decision tree, even with just one variable. This is something critical because forces us to cover wider ranges if the variables have a lot of information like in this case.", "It\u2019s impossible to close this section without covering underfitting and overfitting. Underfitting is when the tree is not complex enough to capture the real relationship between the variables. For instance, when we set \u201cmax_depth\u201d to 1, the model didn\u2019t learn how to differentiate between baseball and golf balls.", "Overfitting is when the model becomes so complex that it starts capturing noise instead of understanding the data\u2019s real underlying structure. Our data didn\u2019t have any noise. Because of that, overfitting wasn\u2019t possible. In general, there is a lot of noise in the data, so letting \u201cmax_depth\u201d to fly so freely might lead to overfitting.", "Min impurity decrease is a way to control if a split it\u2019s really worth it. There are times that, if we keep splitting our nodes, it will do more harm than good. If we don\u2019t use this parameter, the model could tend to overfit the data.", "Imagine that, when we were weighing the balls, by mistake, three baseball balls were weighted instead of one. The data was stored with a value of 21.1, and the class was set to 1. Let\u2019s go back to our example, without any restriction on \u201cmin_impurity_decrease\u201d.", "The model now thinks that every weight higher than 21.024, it\u2019s a baseball ball. If you think about it, it\u2019s certainly a mistake, maybe a new basketball ball is a little higher than 21 ounces, but the model will mistakenly predict it as a baseball ball.", "But \u201cmin_impurity_decrease\u201d can save the day. As you can see, every node has its own Gini value. This parameter allows us to choose if the split it\u2019s worth it or it doesn\u2019t. For the last example, the Gini value of the problematic node was 0.01, which appears to be very low to do a split. If we set this parameter to 0.01 or higher, we limit the split, solving our overfitting problem.", "With the new changes, the model accepts a little impurity on the node, but we know that the reward is that it can effectively ignore the random noise that was added to the data. Our new tree is able to pass the test that the previous one couldn\u2019t.", "The trade-off between underfitting and overfitting might look like a d\u00e9j\u00e0 vu, but setting a value that it\u2019s not so high and not extremely low, is the same balance that we wanted to achieve with the \u201cmax_depth\u201d, and it will be the same on our last hyperparameter: \u201cmin_samples_split\u201d.", "\u201cMin_samples_split\u201d depends a lot more on the number of the initial samples than the last one. This parameter can be set to an integer, and if that is the case, the tree will decide to move forward with the split if the parameter is lower than the number of samples of the node. Otherwise, the node will become a leaf.", "If a fraction is given, it will multiply the fraction per the total number of samples at the root, and that will be the number to be set for the algorithm. For our very last example, we could see that the last useful node was split with 399 samples. Using any value between 202 and 399 will be enough to prevent overfitting, obtaining the same tree as before.", "We learned quite a lot today, but it\u2019s always a great idea to see what we\u2019ve accomplished. The highlights today were:", "Good enough for a single post, don\u2019t you think?.", "I really hope that this blog was somehow interesting to you. Tell me in the comments if you liked it!", "[1] Scikit-learn decision trees documentation.[2] Stackoverflow post.", "I\u2019m also on Linkedin and Twitter. I will gladly talk with you!In case you feel like reading a little more, check out some of my recent posts:", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Chilean \ud83c\udde8\ud83c\uddf1 | Quant Finance \ud83d\udcc8 | Azure Data Scientist Associate \u2601\ufe0f | https://www.linkedin.com/in/maletelier \ud83d\udc68\u200d\ud83d\udcbc"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F99862469493e&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecision-trees-as-you-should-have-learned-them-99862469493e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecision-trees-as-you-should-have-learned-them-99862469493e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecision-trees-as-you-should-have-learned-them-99862469493e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecision-trees-as-you-should-have-learned-them-99862469493e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----99862469493e--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----99862469493e--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://maletelier.medium.com/?source=post_page-----99862469493e--------------------------------", "anchor_text": ""}, {"url": "https://maletelier.medium.com/?source=post_page-----99862469493e--------------------------------", "anchor_text": "Mauricio Letelier"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1697a1a4db50&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecision-trees-as-you-should-have-learned-them-99862469493e&user=Mauricio+Letelier&userId=1697a1a4db50&source=post_page-1697a1a4db50----99862469493e---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F99862469493e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecision-trees-as-you-should-have-learned-them-99862469493e&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F99862469493e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecision-trees-as-you-should-have-learned-them-99862469493e&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@lazyomar?utm_source=medium&utm_medium=referral", "anchor_text": "Omar Ram"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html", "anchor_text": "scikit-learn documentation"}, {"url": "https://www.flaticon.com/free-icon/root_3239230?term=root&page=1&position=17&related_item_id=3239230", "anchor_text": "1"}, {"url": "https://www.flaticon.com/free-icon/modeling_1158219?term=node&page=1&position=9&related_item_id=1158219", "anchor_text": "2"}, {"url": "https://www.flaticon.com/free-icon/leaf_3039835?term=leaf&page=1&position=1&related_item_id=3039835", "anchor_text": "3"}, {"url": "https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity", "anchor_text": "Wikipedia\u2019s"}, {"url": "https://www.flaticon.com/free-icon/golf-ball-with-dents_33846?term=golf&page=1&position=36&related_item_id=33846", "anchor_text": "1"}, {"url": "https://stock.adobe.com/images/id/112587854?as_campaign=Flaticon&as_content=api&as_audience=srp&tduid=1a33fa0c43fa5eb82bbf6301c388aaa7&as_channel=affiliate&as_campclass=redirect&as_source=arvato", "anchor_text": "2"}, {"url": "https://www.flaticon.com/free-icon/american-football_521789?term=american%20football&page=1&position=4&related_item_id=521789", "anchor_text": "3"}, {"url": "https://www.flaticon.com/free-icon/basketball-ball_889289?term=basketball&page=1&position=5&related_item_id=889289", "anchor_text": "4"}, {"url": "https://scikit-learn.org/stable/modules/tree.html", "anchor_text": "documentation"}, {"url": "https://stackoverflow.com/questions/46756606/what-does-splitter-attribute-in-sklearns-decisiontreeclassifier-do/46759065", "anchor_text": "Stackoverflow"}, {"url": "https://maletelier.medium.com/", "anchor_text": ""}, {"url": "http://www.linkedin.com/in/maletelier", "anchor_text": "Linkedin"}, {"url": "https://twitter.com/maletelier", "anchor_text": "Twitter"}, {"url": "https://towardsdatascience.com/become-a-machine-learning-engineer-from-inside-of-your-jupyter-notebook-76a42b65c8f4", "anchor_text": "Become a Machine Learning Engineer (From Inside Of Your Jupyter Notebook)Manage infrastructure, train your models & deploy them.towardsdatascience.com"}, {"url": "https://towardsdatascience.com/probabilistic-machine-learning-approach-to-trading-macd-business-understanding-6b81f465aef6", "anchor_text": "Probabilistic Machine Learning Approach to Trading + MACD Business UnderstandingAt the end of the article, you will know how to structure a trading problem in a probabilistic way. Also, you will\u2026towardsdatascience.com"}, {"url": "https://medium.com/tag/data-science?source=post_page-----99862469493e---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----99862469493e---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/programming?source=post_page-----99862469493e---------------programming-----------------", "anchor_text": "Programming"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----99862469493e---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/algorithms?source=post_page-----99862469493e---------------algorithms-----------------", "anchor_text": "Algorithms"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F99862469493e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecision-trees-as-you-should-have-learned-them-99862469493e&user=Mauricio+Letelier&userId=1697a1a4db50&source=-----99862469493e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F99862469493e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecision-trees-as-you-should-have-learned-them-99862469493e&user=Mauricio+Letelier&userId=1697a1a4db50&source=-----99862469493e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F99862469493e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecision-trees-as-you-should-have-learned-them-99862469493e&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----99862469493e--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F99862469493e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecision-trees-as-you-should-have-learned-them-99862469493e&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----99862469493e---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----99862469493e--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----99862469493e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----99862469493e--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----99862469493e--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----99862469493e--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----99862469493e--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----99862469493e--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----99862469493e--------------------------------", "anchor_text": ""}, {"url": "https://maletelier.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://maletelier.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Mauricio Letelier"}, {"url": "https://maletelier.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "504 Followers"}, {"url": "https://www.linkedin.com/in/maletelier", "anchor_text": "https://www.linkedin.com/in/maletelier"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1697a1a4db50&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecision-trees-as-you-should-have-learned-them-99862469493e&user=Mauricio+Letelier&userId=1697a1a4db50&source=post_page-1697a1a4db50--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fae1f467aa193&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdecision-trees-as-you-should-have-learned-them-99862469493e&newsletterV3=1697a1a4db50&newsletterV3Id=ae1f467aa193&user=Mauricio+Letelier&userId=1697a1a4db50&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}