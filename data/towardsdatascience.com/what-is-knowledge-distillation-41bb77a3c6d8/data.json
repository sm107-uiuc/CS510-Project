{"url": "https://towardsdatascience.com/what-is-knowledge-distillation-41bb77a3c6d8", "time": 1682993980.433867, "path": "towardsdatascience.com/what-is-knowledge-distillation-41bb77a3c6d8/", "webpage": {"metadata": {"title": "What is Knowledge Distillation?. Knowledge distillation is a fascinating\u2026 | by Maxence Prevost | Towards Data Science", "h1": "What is Knowledge Distillation?", "description": "Knowledge distillation is a fascinating concept, we\u2019ll cover briefly why we need it, how it works."}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/pdf/1503.02531.pdf", "anchor_text": "Distilling the Knowledge in a Neural Network", "paragraph_index": 4}, {"url": "https://www.desmos.com/calculator/gdcy4dvaje", "anchor_text": "here", "paragraph_index": 9}, {"url": "https://github.com/Ujjwal-9/Knowledge-Distillation/blob/master/knowledge_distillation_for_mobilenet.ipynb", "anchor_text": "implementation", "paragraph_index": 17}], "all_paragraphs": ["Today\u2019s models can be quite large, here are some of the top models for the ImageNet dataset:", "It seems fair to say that simple computer vision models weigh easily ~100Mo. A hundred Mo just to be able to make an inference isn\u2019t a viable solution for an end product. A remote API can do the trick, but now your product needs to add encryption, you need to store and upload data, the user needs to have a reliable internet connection to have a decent speed. We can train a narrower network, they\u2019ll probably fit in a small memory. But chances are they won\u2019t be good enough at extracting complex features.", "And we\u2019re not talking about ensembles. Ensembles are a great way to extract a lot of knowledge from the training data. But at test time it can be too expensive to run a hundred different models in parallel. The knowledge per parameter ratio is quite low.", "In the end a model can have great score at training time, but we might want to: lower its size (for embedded systems), increase inference speed or simply reduce complexity. Geoffrey Hinton talks about reducing its \u201cmemory foot print\u201d:", "Many insects have a larval form that is optimized for extracting energy and nutrients from the environment and a completely different adult form that is optimized for the very different requirements of traveling and reproduction. In large-scale machine learning, we typically use very similar models for the training stage and the deployment stage despite their very different requirements (\u2026) ( Distilling the Knowledge in a Neural Network)", "Training a smaller model from a larger one is called knowledge distillation.", "The authors continue that we are identifying knowledge with the values of the weights which makes it \u201chard to see how we can change the form of the model but keep the same knowledge\u201d. And remind us that we can see knowledge as a mapping from input to output.", "Knowledge distillation\u2019s goal is to transfer the knowledge from one performant and heavy teacher to a more compact student.", "To do so, we look at the teacher\u2019s softmax layer, magnify it and the student learns how to produce them. We need to magnify because the softmax layer will smash down to zero the least probable classes and rises close to one the most probable (like one hot vector). We can also keep the relative probabilities between classes, where a motocycle and a bicycle share more similarities on the softmax layer rather than a book. We can do it by raising the temperature T.", "To transfer knowledge, a student is trained on the soften probabilities (T>>1) produced by a larger teacher. When the temperature T is smaller than one, the most expected classes will impact the most the final probability. Similarly, when increasing the temperature the probabilities will be softer/flattened across classes -you can have here an intuition of the influence of temperature on a single exp().", "First the teacher\u2019s temperature is increased until a certain point. Then the student is trained to copy its teacher\u2019s soft probabilities.", "Training on soft targets has several advantages: more information can be extracted from a single sample, training can be done on fewer examples, no need for labeled data", "The softmax of a multi-class classifier will give you higher probabilities for similar images. A rose may have similar soft probabilities with a tulip rather than a labrador. Similarly, two different classes are present in the same image, we might see it on the output. So more information are extracted from each training sample.", "This is a consequence from the first point, the model can be trained on fewer training examples than the teacher. The learning is also faster because there are more constraints on the student. It needs to target multiple (soft) outputs rather than a single (hard) one.", "Since the student learns from soft targets only, by relative similarities between classes, it can be trained on a unlabelled dataset, using only the master has an on-fly \u201csoft labeler\u201d. But in practice, the dataset can be the same as the teacher.", "Distillation loss is generally in two forms: matching function values, matching derivatives or both, corresponding to a regression problem with different orders:", "We can also try to increase the influence of the prediction by adding directly the hard loss:", "You can see a cool implementation here.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "ML Engineer drinking this data soup sip by sip \ud83c\udf5c"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F41bb77a3c6d8&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-knowledge-distillation-41bb77a3c6d8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-knowledge-distillation-41bb77a3c6d8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-knowledge-distillation-41bb77a3c6d8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-knowledge-distillation-41bb77a3c6d8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----41bb77a3c6d8--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----41bb77a3c6d8--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@maxpv?source=post_page-----41bb77a3c6d8--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@maxpv?source=post_page-----41bb77a3c6d8--------------------------------", "anchor_text": "Maxence Prevost"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd2a77aff36b1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-knowledge-distillation-41bb77a3c6d8&user=Maxence+Prevost&userId=d2a77aff36b1&source=post_page-d2a77aff36b1----41bb77a3c6d8---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F41bb77a3c6d8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-knowledge-distillation-41bb77a3c6d8&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F41bb77a3c6d8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-knowledge-distillation-41bb77a3c6d8&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://arxiv.org/pdf/1503.02531.pdf", "anchor_text": "Distilling the Knowledge in a Neural Network"}, {"url": "https://www.desmos.com/calculator/gdcy4dvaje", "anchor_text": "here"}, {"url": "https://github.com/Ujjwal-9/Knowledge-Distillation/blob/master/knowledge_distillation_for_mobilenet.ipynb", "anchor_text": "implementation"}, {"url": "https://www.youtube.com/watch?v=EK61htlw8hY?t=650", "anchor_text": "TTIC Geoffrey Hilton \u2014 Dark Knowledge"}, {"url": "https://arxiv.org/pdf/1503.02531v1.pdf", "anchor_text": "Knowledge Distillation paper"}, {"url": "https://www.youtube.com/watch?v=oQr0gODUiZo", "anchor_text": "IEE Security Symposium, Papernot"}, {"url": "https://data-soup.github.io/blog/", "anchor_text": "data-soup.github.io/blog/"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----41bb77a3c6d8---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----41bb77a3c6d8---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----41bb77a3c6d8---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F41bb77a3c6d8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-knowledge-distillation-41bb77a3c6d8&user=Maxence+Prevost&userId=d2a77aff36b1&source=-----41bb77a3c6d8---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F41bb77a3c6d8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-knowledge-distillation-41bb77a3c6d8&user=Maxence+Prevost&userId=d2a77aff36b1&source=-----41bb77a3c6d8---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F41bb77a3c6d8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-knowledge-distillation-41bb77a3c6d8&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----41bb77a3c6d8--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F41bb77a3c6d8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-knowledge-distillation-41bb77a3c6d8&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----41bb77a3c6d8---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----41bb77a3c6d8--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----41bb77a3c6d8--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----41bb77a3c6d8--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----41bb77a3c6d8--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----41bb77a3c6d8--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----41bb77a3c6d8--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----41bb77a3c6d8--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----41bb77a3c6d8--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@maxpv?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@maxpv?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Maxence Prevost"}, {"url": "https://medium.com/@maxpv/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "24 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd2a77aff36b1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-knowledge-distillation-41bb77a3c6d8&user=Maxence+Prevost&userId=d2a77aff36b1&source=post_page-d2a77aff36b1--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Fd2a77aff36b1%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-knowledge-distillation-41bb77a3c6d8&user=Maxence+Prevost&userId=d2a77aff36b1&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}