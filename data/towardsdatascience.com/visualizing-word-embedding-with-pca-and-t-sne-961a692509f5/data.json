{"url": "https://towardsdatascience.com/visualizing-word-embedding-with-pca-and-t-sne-961a692509f5", "time": 1683014513.9760058, "path": "towardsdatascience.com/visualizing-word-embedding-with-pca-and-t-sne-961a692509f5/", "webpage": {"metadata": {"title": "Visualizing Word Embedding with PCA and t-SNE | by Ruben Winastwan | Towards Data Science", "h1": "Visualizing Word Embedding with PCA and t-SNE", "description": "When you hear the word \u2018tea\u2019 and \u2018coffee\u2019, what would you think of these two words? Probably you will say that they both are beverages, which contain a certain amount of caffeine. The point is, we\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/why-do-we-use-embeddings-in-nlp-2f20e1b632d2", "anchor_text": "in this article", "paragraph_index": 4}, {"url": "https://towardsdatascience.com/introduction-to-word-embeddings-4cf857b12edc", "anchor_text": "this article", "paragraph_index": 4}, {"url": "https://nlp.stanford.edu/projects/glove/", "anchor_text": "here", "paragraph_index": 7}, {"url": "https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c", "anchor_text": "this article", "paragraph_index": 14}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html", "anchor_text": "the documentation", "paragraph_index": 30}, {"url": "https://github.com/marcellusruben/Word_Embedding_Visualization", "anchor_text": "here", "paragraph_index": 35}], "all_paragraphs": ["When you hear the word \u2018tea\u2019 and \u2018coffee\u2019, what would you think of these two words? Probably you will say that they both are beverages, which contain a certain amount of caffeine. The point is, we can easily recognize that these two words are associated with one another. However, when we supply the word \u2018tea\u2019 and \u2018coffee\u2019 to the computer, it can\u2019t recognize the association between these two words like we do.", "Word is not something that the computer will naturally understand. For a computer to understand the meaning behind a word, the word needs to be encoded into a numeric form. This is where the technique behind the word embedding will be useful.", "Word embedding is a technique commonly applied in Natural Language Processing to transform words into numeric values in the form of vectors. These vectors will occupy the embedding space with certain dimensions.", "If two words have a similar context, like the \u2018tea\u2019 and \u2018coffee\u2019 do, then the distance between these two words in embedding space will be close to one another, while the distance to other words that have a different context will be further apart.", "In this article, I\u2019m going to show you the step-by-step on how to visualize the word embedding. Since the focus of this article is not to explain in great detail about the fundamental theory behind the word embedding, you can read more about the theory in this article and this article.", "To visualize the word embedding, we are going to use common dimensionality reduction techniques such as PCA and t-SNE. To map the words into their vector representations in embedding space, the pre-trained word embedding GloVe will be implemented.", "Before visualizing the word embedding, normally we need to train the model first. However, word embedding is very expensive computationally to train. Hence, the pre-trained word embedding model is normally used instead. It contains the tokens and their associated vector representations in embedding space.", "GloVe is a popular pre-trained word embedding model developed by Stanford researchers beside word2vec which is developed by Google. In this article, the GloVe pre-trained word embedding is implemented and you can download it here. Meanwhile, we can use Gensim library to load the pre-trained word embedding model. You can install the library using pip command as follows.", "As the first step, we need to convert the GloVe file format into word2vec file format. With word2vec file format, we can load the pre-trained word embedding model into the memory using Gensim library. As this file loading will take some time each time this command is called, then it will be better if we use a separate Python file just for this purpose.", "Now that we have a Python file just to load the pre-trained model, next we can call it in another Python file to generate the most similar words according to input words. The input words can be any words according to your preference.", "After we type the input word, the next thing would be to create a code to read it. Then, we need to specify the number of similar words based on each input word that will be generated by the model. Finally, we store the result of similar words in a list. Below is the code to do this purpose.", "As an example, let\u2019s say that we want to find out the 5 most similar words associated with \u2018school\u2019. Hence, \u2018school\u2019 would be our input word. With the code above, we would get the words \u2018college\u2019, \u2018schools\u2019, \u2018elementary\u2019, \u2018students\u2019, and \u2018student\u2019 as the result.", "By now, we have the input word and the similar words generated based on it. Next, it\u2019s time for us to visualize them in the embedding space.", "With the pre-trained model, each word now can be mapped into embedding space with its vector representation. However, the word embedding comes in a very high dimensionality, which means it is impossible to visualize how words occupy the embedding space.", "A method such as Principal Component Analysis (PCA) is normally implemented to reduce the dimensionality of word embedding. In short, PCA is a feature extraction technique \u2014 it combines the variables, and then it drops the least important variables while still retains the valuable parts of the variables. If you want to dig deeper into PCA, I would recommend this article.", "With the PCA, we can visualize the word embedding either in 2D or 3D. Thus, let\u2019s create the code to visualize the word embedding using the model that we have called in the code chunk above. In the code below, only 3D visualization is shown. To visualize the PCA in 2D, only minor changes should be applied. You can find the parts that you need to change in the commented sections of the code.", "As an example, let\u2019s say that we want to visualize the top 5 most similar words associated with \u2018ball\u2019, \u2018school\u2019, and \u2018food\u2019. With the code that we\u2019ve created so far, below is the example of the visualization in 2D.", "And below is the visualization of the same set of words in 3D.", "From the visualization, now we can see the pattern regarding the space these set of words occupied. The words associated with \u2018ball\u2019 are placed close to each other as they have a similar context. Meanwhile, the distance between them and the words associated with \u2018school\u2019 and \u2018food\u2019 is further apart as they have a different context.", "Aside from PCA, another dimensionality reduction technique that is commonly used is t-Distributed Stochastic Neighbor Embedding or t-SNE. The difference between PCA and t-SNE is the fundamental technique they both implement to reduce the dimensionality.", "PCA is a linear dimension reduction method. The data in high dimensional space are mapped linearly into low dimensional space while maximizing the variance of the data. Meanwhile, t-SNE is a non-linear dimension reduction method. With t-SNE, the algorithm calculates the similarity in both high dimensional space and low dimensional space. Next, the similarity difference in both spaces is minimized using an optimization method, for example gradient descend method.", "The code to visualize the word embedding with t-SNE is very similar with the one with PCA. In the code below, only 3D visualization is shown. To visualize the t-SNE in 2D, only minor changes should be applied. You can find the parts that you need to change in the commented sections of the code.", "With the same example as in the PCA visualization, i.e the visualization of top-5 most similar words associated with \u2018ball\u2019, \u2018school\u2019, and \u2018food\u2019, below is the visualization result in 2D.", "And below is the visualization of the same set of words in 3D.", "Same as the one with PCA, note how the words that have similar context are placed close within each other while the words that have different context are placed further apart.", "So far, we have successfully created a Python script to visualize the word embedding in 2D or 3D with either PCA or t-SNE. Next, we can create a Python script to build a web app for a better user experience.", "The web app enables us to visualize the word embedding with a lot of functionality and interactivity. As an example, the user can type their own input words and they can also choose the top-n most similar words associated with each input word that will be returned.", "The web app can be created using Dash or Streamlit. In this article, I\u2019m going to show you how to build a simple interactive web app to visualize the word embedding with Streamlit.", "First, we will use all of the Python code that we have created before and put them into one Python script. Next, we can start to create several user input parameters as follows:", "Next, we need to take consideration of parameters that will come up in case we decide to use t-SNE. In t-SNE, there are a few parameters that we can tweak to get the best visualization result. These parameters are the perplexity, the learning rate, and the number of optimization iteration. There is no single best value for each of these parameters in each case, hence the best solution would be to let the user specify these values.", "Since we are using Scikit-learn, we can refer to the documentation to find out the default value of these parameters. The default value of perplexity is 30, but we can tweak the value between 5 to 50. The default value of the learning rate is 300, but we can tweak the value between 10 to 1000. Finally, the default value of the number of iteration is 1000, but we can tweak the value as less as 250. We can use slider attribute to create these parameter values.", "Now we have covered all of the necessary part to build our web app. Finally, we can wrap things up together into one whole complete script as below.", "You can now run the web app using Conda prompt. Within the prompt, go to the directory of your Python script and type the following command:", "Next, a browser window will pop-up automatically, where you can access your web app locally. Below is the snapshot of what you can do with the web app.", "And that\u2019s it! You\u2019ve created a simple web app with a lot of interactivity to visualize word embedding with PCA or t-SNE.", "If you want to see the whole code of this word embedding visualization, you can access it on my GitHub page here.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Science || Machine Learning || Computer Vision || NLP"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F961a692509f5&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvisualizing-word-embedding-with-pca-and-t-sne-961a692509f5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvisualizing-word-embedding-with-pca-and-t-sne-961a692509f5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvisualizing-word-embedding-with-pca-and-t-sne-961a692509f5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvisualizing-word-embedding-with-pca-and-t-sne-961a692509f5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----961a692509f5--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----961a692509f5--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@marcellusruben?source=post_page-----961a692509f5--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@marcellusruben?source=post_page-----961a692509f5--------------------------------", "anchor_text": "Ruben Winastwan"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5dae9da73c9b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvisualizing-word-embedding-with-pca-and-t-sne-961a692509f5&user=Ruben+Winastwan&userId=5dae9da73c9b&source=post_page-5dae9da73c9b----961a692509f5---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F961a692509f5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvisualizing-word-embedding-with-pca-and-t-sne-961a692509f5&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F961a692509f5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvisualizing-word-embedding-with-pca-and-t-sne-961a692509f5&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/tagged/hands-on-tutorials", "anchor_text": "Hands-on Tutorials"}, {"url": "https://unsplash.com/@francesco_ungaro?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Francesco Ungaro"}, {"url": "https://unsplash.com/s/photos/letter-word?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://towardsdatascience.com/why-do-we-use-embeddings-in-nlp-2f20e1b632d2", "anchor_text": "in this article"}, {"url": "https://towardsdatascience.com/introduction-to-word-embeddings-4cf857b12edc", "anchor_text": "this article"}, {"url": "https://nlp.stanford.edu/projects/glove/", "anchor_text": "here"}, {"url": "https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c", "anchor_text": "this article"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html", "anchor_text": "the documentation"}, {"url": "https://github.com/marcellusruben/Word_Embedding_Visualization", "anchor_text": "here"}, {"url": "https://medium.com/tag/data-science?source=post_page-----961a692509f5---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----961a692509f5---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/nlp?source=post_page-----961a692509f5---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/dimensionality-reduction?source=post_page-----961a692509f5---------------dimensionality_reduction-----------------", "anchor_text": "Dimensionality Reduction"}, {"url": "https://medium.com/tag/hands-on-tutorials?source=post_page-----961a692509f5---------------hands_on_tutorials-----------------", "anchor_text": "Hands On Tutorials"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F961a692509f5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvisualizing-word-embedding-with-pca-and-t-sne-961a692509f5&user=Ruben+Winastwan&userId=5dae9da73c9b&source=-----961a692509f5---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F961a692509f5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvisualizing-word-embedding-with-pca-and-t-sne-961a692509f5&user=Ruben+Winastwan&userId=5dae9da73c9b&source=-----961a692509f5---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F961a692509f5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvisualizing-word-embedding-with-pca-and-t-sne-961a692509f5&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----961a692509f5--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F961a692509f5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvisualizing-word-embedding-with-pca-and-t-sne-961a692509f5&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----961a692509f5---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----961a692509f5--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----961a692509f5--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----961a692509f5--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----961a692509f5--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----961a692509f5--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----961a692509f5--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----961a692509f5--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----961a692509f5--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@marcellusruben?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@marcellusruben?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ruben Winastwan"}, {"url": "https://medium.com/@marcellusruben/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "925 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5dae9da73c9b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvisualizing-word-embedding-with-pca-and-t-sne-961a692509f5&user=Ruben+Winastwan&userId=5dae9da73c9b&source=post_page-5dae9da73c9b--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F46c6747bd93b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvisualizing-word-embedding-with-pca-and-t-sne-961a692509f5&newsletterV3=5dae9da73c9b&newsletterV3Id=46c6747bd93b&user=Ruben+Winastwan&userId=5dae9da73c9b&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}