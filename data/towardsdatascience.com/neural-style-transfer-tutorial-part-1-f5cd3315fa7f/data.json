{"url": "https://towardsdatascience.com/neural-style-transfer-tutorial-part-1-f5cd3315fa7f", "time": 1682993898.3947651, "path": "towardsdatascience.com/neural-style-transfer-tutorial-part-1-f5cd3315fa7f/", "webpage": {"metadata": {"title": "Neural Style Transfer Tutorial -Part 1 | by Vamshik Shetty | Towards Data Science", "h1": "Neural Style Transfer Tutorial -Part 1", "description": "In this article, you will be learning using a bottom-up approach we will start from the basic foundation of neural style. We\u2019ll go through what it exactly is, for beginners, and why it works. This\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/u/f17122844cb9?source=post_page-----f5cd3315fa7f--------------------------------", "anchor_text": "Pawan Sasanka Ammanamanchi", "paragraph_index": 0}, {"url": "https://ayearofai.com/rohan-lenny-2-convolutional-neural-networks-5f4cd480a60b", "anchor_text": "here", "paragraph_index": 1}, {"url": "https://arxiv.org/abs/1508.06576", "anchor_text": "Leon A. Gatys\u2019 paper, A Neural Algorithm of Artistic Style", "paragraph_index": 31}, {"url": "https://www.linkedin.com/in/vamshik-shetty/", "anchor_text": "LinkedIn", "paragraph_index": 35}], "all_paragraphs": ["In this article, you will be learning using a bottom-up approach we will start from the basic foundation of neural style. We\u2019ll go through what it exactly is, for beginners, and why it works. This article is the first of an ongoing series and I will be co-authoring it with Pawan Sasanka Ammanamanchi. There is no end in sight for this series, and we will try to ultimately cover research at the helm of this field and keep adding new works. The prerequisites for following through are:", "2]. Basic Understanding of how Convolutional Networks work. We suggest this article here, if you don\u2019t know much about them.", "During the past few years we\u2019ve seen a slew of apps like prisma and other similar apps popping up which style your photos in a way wherein they look like paintings. Offering you a variety of beautiful styles some of which are paintings by famous artists like Starry Night by Van Gogh. Trying to explain this concept just with words might be difficult.", "As you can see in Figure 1, there are two input images namely content image and style image that are used to a generate a new image called stylized image. A few things to notice about this image is that it has the same content as the content image and has a style similar to that of the style image. It looks good and we are pretty sure it\u2019s not achieved by overlapping these two images so how do we get here what is the math behind this idea? To answer these question we need to take a step back and focus on what does a convolution neural network actually learn? What are these Convolution layers truly encoding in form of feature maps or kernels that lets them do this or stating this in another way What representations do CNNs learn when we input an image, let\u2019s try to understand that first.", "Convolutional Neural Networks were originally created for classification of images and have lately been used in a variety of other tasks like Image Segmentation, Neural Style and other computer vision and Natural Language Processing tasks as well. CNNs are one of the most interpretable models in Deep Learning because of our ability to visual their representations and understand what they might be learning.", "What insights can convolutional neural network provide?", "In this section I want to share some intuition of how deeper layer of CNN represent an image and how we can use this computed representation which only makes sense to the model to our advantage in developing neural style transfer. Let\u2019s take VGG16 architecture for understanding this representation in hidden layers.", "Using the above architecture as reference, let\u2019s consider the 1st convolution layer of vgg16 which uses 3x3 kernel and trains 64 feature maps to generate an image representation of 224x224x64 by taking a 3 channel image of size 224x224 as input. if you are bit confused look below", "Let\u2019s assume while training this 64 feature maps they may have learnt to detect simple patterns, such that some neural units activate when they see a straight line or even for some other type of pattern which might not make any sense to a human eye but has huge value to this model. This \u201cDetection\u201d of straight lines or some pattern is called as learning a feature representation.", "Now let\u2019s consider the 10th convolution layer of vgg16 which uses a 3x3 kernel with 512 feature maps to train and finally generates a output of 28X28x512 image representation, just for sake of simplicity let\u2019s assume that there are certain units in this 10th layer which gets activated by an image containing circles like wheel of a car or there might be some which get activated by an image having some pattern similar to three intersecting lines etc.", "It\u2019s safe to assume that CNN does not learn to encode what image is but it actually learns to encode what image represents or what contents are visible in the image and due to inherent nonlinear nature of neural networks has we go from shallow layers to deeper layers the hidden units become capable to detect more and more complex feature from a given image.", "How these image representations help in style transfer?", "Well this nature of encoding representations itself is the key to style transfer it is used to calculate loss between the generated image with respect to content and style image. As training the model over a ten thousands of images per class the model is able to generate similar feature representation for many different images given they belong to same class or have similar content or style. Hence it makes sense to use the difference in value of feature representation of generated image w.r.t content and style image to guide the iterations through which we produce the generated image itself but how do we make sure that content image (C) and generated image (G) are similar with respect to their content and not style, while on other hand how do we make sure that generated image only inherits similar style representation from style image (S) and not the entire style image itself. This is solved by dividing the loss function into two parts, one is the Content loss and the other is the Style loss and soon enough we will understand how they are different from each other and how they overcome the problems which we have put forth.", "Well as you can see in the above equation there are two things we need to calculate to get overall loss i.e content loss and style loss, alpha and beta hyperparameters which are used to provide weights to each type of loss i.e these parameters can be thought of simply as knobs to control how much of the content/style we want to inherit in the generated image. So let\u2019s get to understand what each of this loss term entails.", "What are the inputs to this loss function displayed above? We have no idea how the final output might look. So the naive approach of supervised learning might not work. The answer lies in the image below.", "During each iteration all the three images i.e. content image, style image and generated image are passed through the vgg16 model. The value of the hidden unit\u2019s activation which encode feature representation of the given image at certain layers are taken as input to these loss functions, in simpler terms you can directly think of this as taking the output of the Layers in the VGG16 network, there isn\u2019t any hard and fast rule on selection of layers. Another thing to add here, Initially we randomly initialize the generated image if you look at it then it\u2019s nothing more than a matrix of random noise of shape same as content image. With each iteration we change the generated image in effort to minimize the overall loss L.", "Note: Here after each Convolution layer, it\u2019s output is passed through relu as it\u2019s activation function, you can also check in figure 2 where each Convolution Block is represented as [ Convolution + Relu ]", "Content Loss is easy to calculate, let\u2019s take the feature representation of only one of the layers, let\u2019s consider 7th convolution layer of vgg16. To calculate the content loss we pass both content image and generated image through vgg16 and get the activation values (i.e outputs) of 7th conv layer for both of these images which has Relu for its activation, we will denote this layer\u2019s output generally as relu_3_3 because it is the output of third conv layer of third set/block of convolutions (check figure 2 & 6 for reference). Finally we find the L2 Norm of element wise subtraction between these two activation matrices as, This will help to preserve the original content in the generated image by making sure to minimize the difference in feature representation which logically focuses on the difference between content of both the images.", "To put this loss in mathematical form or a equation which we can compute. Let\u2019s say we have function Content loss which takes in three arguments as input that are content image C, generated image G and the layer L whose activation\u2019s we are going use to compute loss. Now let\u2019s denote each activation layer of content image as a[ L ]( C ) and activation layer of generated image as a[ L ]( G ).", "Now let\u2019s look at the style loss, while calculating the style loss we will consider feature representation of many convolution layers from shallow to deeper layers of the model. Unlike content loss we can\u2019t just find the difference in activation units, What we need is a way to find the correlation between these activations across different channels of the same layer and to do this we need something called as the Gram Matrix.", "I will try to build up the foundation needed to understand gram matrix with an example, So let\u2019s consider that we pass our style image though vgg16 and we get the activation values from 7th layer which generates the feature representation matrix of size 56x56x256, you can refer to figure 2 which describes architecture of vgg16. Now let\u2019s takes a closer look at this output.", "In this 3-D array, there are 256 channels of size 56x56 each. Now let\u2019s assume that there is channel \u2018A\u2018 whose activation units may get activated when they come across an image section containing black and brown strips and then there is a channel \u2018B\u2019 whose activation units may get activated when they come across something similar to an eyeball.", "Note: Here units getting activated refer to them having considerably huge value compared to zero after passing through relu.", "If both of these channels \u2018A\u2019 & \u2018B\u2019 activate together for the same input, there\u2019s a high possibility that image might contain a face of a tiger ( because it had two channels with high values which activates for eyeball and brown black stripes ). Now if both of these channels are fired up with high activation values that means they would have high correlation compared to correlation between channel \u2018A\u2019 & \u2018C\u2019 where channel \u2018C\u2019 might get activated when it sees a diamond shaped pattern.So to get the correlation of all this channels w.r.t each other we need to calculate something called as gram matrix, we will use gram matrix to measure the degree of correlation between channels which later will act as a measure of the style itself. Now you might have understood the significance of gram matrix but to understand how we get the gram matrix of above mentioned 3-D array go through the image mentioned below.", "Now as you can see how each element of this gram matrix contains correlation measure of all the channels with respect to each other. Moving forward, how do we use this computed Gram matrix G to calculate the style loss. Let\u2019s denote the gram matrix of style image of layer L as GM[L](S) and gram matrix of generated image of same layer as GM[L](G). Both the gram matrix were computed from the same layer hence using the same number of channel leading it to be a matrix of size ch x ch, Now if we find sum of square difference or L2_norm of element subtraction of these two matrices and try to minimize it, Then this will eventually lead to minimizing the difference between the style of style image and the generated image. Think about it, it might take some time to settle in but when it does, you will be mesmerized by how simple yet effective this is.", "In the above equation, N subscript l represents the number of channel in the feature-map/output of layer l and M subscript l represents the height*width of the feature-map/output of layer l.", "While computing style loss we use multiple activation layers, that scenarios leads us to a possibility of assigning different weightages to each sub loss provided by different layers. below equation, sums what I just said pretty elegantly but in our case or most cases in general people give equal weightage for all the layers.", "Moving forward once you have both content and style loss add them up and use any optimizer to perform gradient descent to change generated image such that it decreases its loss after each iteration.", "This pretty much sums it up, I hope my explanation was sufficient to clear any doubts you had about the basics of neural style. Now you can dive into neural style\u2019s code, I will go through each line of my code and properly dissect it but below mentioned pseudo code pretty much sums everything up about the code that you are going to run and play with.", "The following content and style layers are used generally and suggested by the original paper but you can experiment with other layers as well.", "This is the essence of neural style. If you still have some doubts or want better explanation on any part of this article, comment below and I will try my level best to get back to you as soon as possible.", "Link. to original neural style transfer paper Leon A. Gatys\u2019 paper, A Neural Algorithm of Artistic Style.", "You can look forward to the following articles being written in coming future.", "Course on Neural Style Transfer with Tensorflow and pyTorch:", "Thank You for reading this article !", "If you believe that we are like minded people and should connect then you can find me on LinkedIn or you can email me at vamshikdshetty@gmail.com. If you have any thoughts, questions or feedback feel free to comment below I would love to hear from you.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "I love everything from machine learning to back-end design to distributed network. I believe my life\u2019s driving force is inherent nature of being Simply Curious."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Ff5cd3315fa7f&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-style-transfer-tutorial-part-1-f5cd3315fa7f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-style-transfer-tutorial-part-1-f5cd3315fa7f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-style-transfer-tutorial-part-1-f5cd3315fa7f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-style-transfer-tutorial-part-1-f5cd3315fa7f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----f5cd3315fa7f--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f5cd3315fa7f--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@vamshikdshetty?source=post_page-----f5cd3315fa7f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@vamshikdshetty?source=post_page-----f5cd3315fa7f--------------------------------", "anchor_text": "Vamshik Shetty"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fbc0ad784dd4e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-style-transfer-tutorial-part-1-f5cd3315fa7f&user=Vamshik+Shetty&userId=bc0ad784dd4e&source=post_page-bc0ad784dd4e----f5cd3315fa7f---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff5cd3315fa7f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-style-transfer-tutorial-part-1-f5cd3315fa7f&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff5cd3315fa7f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-style-transfer-tutorial-part-1-f5cd3315fa7f&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/u/f17122844cb9?source=post_page-----f5cd3315fa7f--------------------------------", "anchor_text": "Pawan Sasanka Ammanamanchi"}, {"url": "https://ayearofai.com/rohan-lenny-2-convolutional-neural-networks-5f4cd480a60b", "anchor_text": "here"}, {"url": "https://medium.com/@franky07724_57962/using-keras-pre-trained-models-for-feature-extraction-in-image-clustering-a142c6cdf5b1", "anchor_text": "Figure 2"}, {"url": "https://arxiv.org/pdf/1311.2901", "anchor_text": "Visualizing and understanding convolutional networks"}, {"url": "https://arxiv.org/abs/1508.06576", "anchor_text": "Leon A. Gatys\u2019 paper, A Neural Algorithm of Artistic Style"}, {"url": "https://towardsdatascience.com/neural-style-transfer-series-part-2-91baad306b24", "anchor_text": "PART \u2014 2 Implementation of Neural Style Transfer"}, {"url": "https://www.linkedin.com/in/vamshik-shetty/", "anchor_text": "LinkedIn"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----f5cd3315fa7f---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/convolution-neural-net?source=post_page-----f5cd3315fa7f---------------convolution_neural_net-----------------", "anchor_text": "Convolution Neural Net"}, {"url": "https://medium.com/tag/tutorial?source=post_page-----f5cd3315fa7f---------------tutorial-----------------", "anchor_text": "Tutorial"}, {"url": "https://medium.com/tag/neural-style-transfer?source=post_page-----f5cd3315fa7f---------------neural_style_transfer-----------------", "anchor_text": "Neural Style Transfer"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----f5cd3315fa7f---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff5cd3315fa7f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-style-transfer-tutorial-part-1-f5cd3315fa7f&user=Vamshik+Shetty&userId=bc0ad784dd4e&source=-----f5cd3315fa7f---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff5cd3315fa7f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-style-transfer-tutorial-part-1-f5cd3315fa7f&user=Vamshik+Shetty&userId=bc0ad784dd4e&source=-----f5cd3315fa7f---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff5cd3315fa7f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-style-transfer-tutorial-part-1-f5cd3315fa7f&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f5cd3315fa7f--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Ff5cd3315fa7f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-style-transfer-tutorial-part-1-f5cd3315fa7f&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----f5cd3315fa7f---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----f5cd3315fa7f--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----f5cd3315fa7f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----f5cd3315fa7f--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----f5cd3315fa7f--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----f5cd3315fa7f--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----f5cd3315fa7f--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----f5cd3315fa7f--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----f5cd3315fa7f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@vamshikdshetty?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@vamshikdshetty?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Vamshik Shetty"}, {"url": "https://medium.com/@vamshikdshetty/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "117 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fbc0ad784dd4e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-style-transfer-tutorial-part-1-f5cd3315fa7f&user=Vamshik+Shetty&userId=bc0ad784dd4e&source=post_page-bc0ad784dd4e--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F835a9720aa7f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-style-transfer-tutorial-part-1-f5cd3315fa7f&newsletterV3=bc0ad784dd4e&newsletterV3Id=835a9720aa7f&user=Vamshik+Shetty&userId=bc0ad784dd4e&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}