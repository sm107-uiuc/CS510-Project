{"url": "https://towardsdatascience.com/reinforcement-learning-td-%CE%BB-introduction-686a5e4f4e60", "time": 1683000100.219836, "path": "towardsdatascience.com/reinforcement-learning-td-%CE%BB-introduction-686a5e4f4e60/", "webpage": {"metadata": {"title": "Reinforcement Learning \u2014 TD(\u03bb) Introduction(1) | by Jeremy Zhang | Towards Data Science", "h1": "Reinforcement Learning \u2014 TD(\u03bb) Introduction(1)", "description": "In this article, we will be talking about TD(\u03bb), which is a generic reinforcement learning method that unifies both Monte Carlo simulation and 1-step TD method. We have been talking about TD method\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/zero-equals-false/n-step-td-method-157d3875b9cb", "anchor_text": "TD(n)", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/reinforcement-learning-generalisation-in-continuous-state-space-df943b04ebfa", "anchor_text": "semi-gradient method", "paragraph_index": 8}, {"url": "https://towardsdatascience.com/reinforcement-learning-generalisation-in-continuous-state-space-df943b04ebfa", "anchor_text": "here", "paragraph_index": 10}, {"url": "https://github.com/MJeremy2017/Reinforcement-Learning-Implementation/blob/master/RandomWalk(Lambda)/TD_Lambda.py", "anchor_text": "implement", "paragraph_index": 11}, {"url": "https://github.com/MJeremy2017/Reinforcement-Learning-Implementation/blob/master/RandomWalk(Lambda)/TD_Lambda.py", "anchor_text": "here", "paragraph_index": 22}], "all_paragraphs": ["In this article, we will be talking about TD(\u03bb), which is a generic reinforcement learning method that unifies both Monte Carlo simulation and 1-step TD method. We have been talking about TD method exhaustively, and if you remember, in TD(n) method, I have said it is also a unification of MC simulation and 1-step TD, but in TD(n), one has to keep track of all the traces along the way and update current estimation based on value n-step ahead, in TD(\u03bb), we are going to see a more elegant unification.", "In this article, we will be:", "TD(\u03bb) is, in fact, an extension of TD(n) method, remember that in TD(n), we have the accumulated reward of the following form:", "This value estimation up to step t+n is used to update the value on step t , and what TD(\u03bb) does is to averaging the value, for example, using", "as the target value. But instead of using direct weights, it uses \u03bb as a parameter to control the weight and make it sum up to 1:", "I bet the image looks familiar, an agent start from a state, by taking an action, it reaches next state, and after that it chooses another action, and the SARSA process goes on and on. So the first column is in fact TD(1) method, which is being assigned weigh of 1-\u03bb , and the second column is TD(2), which has a weight of (1-\u03bb)\u03bb , \u2026, until the last TD(n) is assigned weight of \u03bb^(T-t-1) (T is the length of the episode). Note that the weight decays as n increases and the total summation is 1. A more general form of TD(\u03bb) is:", "From the formula, we can see that when \u03bb = 1 , only the last term is kept and this is essentially Monte Carlo method, as the state, action process goes all the way to the end, when \u03bb = 0 , the term reduces to G_t:t+1, which is 1-step TD method, and for 0 < \u03bb < 1 , the method becomes a mixed of weighted TD(n) method.", "With the target G_t defined, we are now moving to our first algorithm definition. The summarised update formula can be defined as:", "The update rule is the same as general semi-gradient method, the only difference lies in the target value I stated above.", "Referring to an image from Sutton\u2019s book, this method is also called forward view learning algorithm, as at each state, the update process looks forward to value of G_t:t+1 , G_t:t+2 , \u2026, and based on a weighted value of which to update the current state.", "Now let\u2019s get to the implementation of the algorithm on the random walk example. We have learnt the random walk example here, but FYI, in random walk, an agent starts at the middle position, and at each step, it has equal probability to move one step either to the left or right(action policy is fixed), and by only ending at either left or most can it stop an episode.", "We are going to implement a 19-state random walk, and although the state space is in fact discrete, we can still apply generalisation algorithm on it.", "The value function is simple and explicit. We have 19 states and 2 ending states, so in total we have 21 states, and each state has a weight, which is essentially its value estimation. The value function returns the value of a specific state and learn function update current estimation based on the difference delta , which in this case is Gt \u2014 v(St, wt) ( alpha is learning rate).", "As this is not our first time implement a random walk, I will list some commonly shared functions together:", "At each state, an agent chooseAction \u2192 takeAction \u2192 giveReward and repeat until reach the end of the game.", "Again, the major difference lies in the playing process and computing delta for each state.", "At each episode, we will need to first keep track of all states till the game ends in order to get a forward view when updating the value of each state. The self.states records all states along the way, and self.reward keeps only the latest reward, as all rewards are 0 along the way except the final state.", "The second part is to update the state value estimation after the end of the game. Recall the formula:", "For each state at time t and step n , we need to compute the value of G_t:t+n , and combining them with decaying weights in order to get the target value of St . So the function gt2tn computes the value given t and n , which is defined as:", "and gtlambda in the previous code snippet is the target value. In addition, we also set atruncate value, when lambda_power is too small, we simply ignore the value. With target gtlambda and current value from valueFunc , we are able to compute the difference delta and update the estimation using function learn we defined above.", "Remember in TD(n) session, we applied n-step TD method on random walk with exactly same settings. Now with off-line \u03bb-Return introduced, let\u2019s compare the learning result of both:", "We can see that both RMS error curves are similar and the result is comparable. Generally, the best learning result usually occurs with intermediate learning rate and \u03bb value (I copied the image from Sutton\u2019s book, as this one is way more clear than the one I plotted).", "That is it for forward view update, please check out the full implementation here. In next post, we will be learning backward update, which is a more elegant TD(\u03bb) method by using eligibility trace.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Hmm\u2026I am a data scientist looking to catch up the tide\u2026"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F686a5e4f4e60&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-td-%CE%BB-introduction-686a5e4f4e60&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-td-%CE%BB-introduction-686a5e4f4e60&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-td-%CE%BB-introduction-686a5e4f4e60&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-td-%CE%BB-introduction-686a5e4f4e60&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----686a5e4f4e60--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----686a5e4f4e60--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://meatba11.medium.com/?source=post_page-----686a5e4f4e60--------------------------------", "anchor_text": ""}, {"url": "https://meatba11.medium.com/?source=post_page-----686a5e4f4e60--------------------------------", "anchor_text": "Jeremy Zhang"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff37783fc8c26&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-td-%CE%BB-introduction-686a5e4f4e60&user=Jeremy+Zhang&userId=f37783fc8c26&source=post_page-f37783fc8c26----686a5e4f4e60---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F686a5e4f4e60&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-td-%CE%BB-introduction-686a5e4f4e60&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F686a5e4f4e60&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-td-%CE%BB-introduction-686a5e4f4e60&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/zero-equals-false/n-step-td-method-157d3875b9cb", "anchor_text": "TD(n)"}, {"url": "https://towardsdatascience.com/reinforcement-learning-generalisation-in-continuous-state-space-df943b04ebfa", "anchor_text": "semi-gradient method"}, {"url": "https://towardsdatascience.com/reinforcement-learning-generalisation-in-continuous-state-space-df943b04ebfa", "anchor_text": "here"}, {"url": "https://github.com/MJeremy2017/Reinforcement-Learning-Implementation/blob/master/RandomWalk(Lambda)/TD_Lambda.py", "anchor_text": "implement"}, {"url": "https://github.com/MJeremy2017/Reinforcement-Learning-Implementation/blob/master/RandomWalk(Lambda)/TD_Lambda.py", "anchor_text": "here"}, {"url": "http://incompleteideas.net/book/the-book-2nd.html?source=post_page---------------------------", "anchor_text": "http://incompleteideas.net/book/the-book-2nd.html"}, {"url": "https://github.com/ShangtongZhang/reinforcement-learning-an-introduction", "anchor_text": "https://github.com/ShangtongZhang/reinforcement-learning-an-introduction"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----686a5e4f4e60---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----686a5e4f4e60---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----686a5e4f4e60---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/tag/towards-ai?source=post_page-----686a5e4f4e60---------------towards_ai-----------------", "anchor_text": "Towards Ai"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F686a5e4f4e60&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-td-%25CE%25BB-introduction-686a5e4f4e60&user=Jeremy+Zhang&userId=f37783fc8c26&source=-----686a5e4f4e60---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F686a5e4f4e60&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-td-%25CE%25BB-introduction-686a5e4f4e60&user=Jeremy+Zhang&userId=f37783fc8c26&source=-----686a5e4f4e60---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F686a5e4f4e60&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-td-%CE%BB-introduction-686a5e4f4e60&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----686a5e4f4e60--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F686a5e4f4e60&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-td-%CE%BB-introduction-686a5e4f4e60&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----686a5e4f4e60---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----686a5e4f4e60--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----686a5e4f4e60--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----686a5e4f4e60--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----686a5e4f4e60--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----686a5e4f4e60--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----686a5e4f4e60--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----686a5e4f4e60--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----686a5e4f4e60--------------------------------", "anchor_text": ""}, {"url": "https://meatba11.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://meatba11.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Jeremy Zhang"}, {"url": "https://meatba11.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.1K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff37783fc8c26&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-td-%CE%BB-introduction-686a5e4f4e60&user=Jeremy+Zhang&userId=f37783fc8c26&source=post_page-f37783fc8c26--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fcdbd8b83c584&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-td-%CE%BB-introduction-686a5e4f4e60&newsletterV3=f37783fc8c26&newsletterV3Id=cdbd8b83c584&user=Jeremy+Zhang&userId=f37783fc8c26&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}