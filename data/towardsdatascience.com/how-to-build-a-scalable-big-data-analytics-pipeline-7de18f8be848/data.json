{"url": "https://towardsdatascience.com/how-to-build-a-scalable-big-data-analytics-pipeline-7de18f8be848", "time": 1683007097.0216699, "path": "towardsdatascience.com/how-to-build-a-scalable-big-data-analytics-pipeline-7de18f8be848/", "webpage": {"metadata": {"title": "How to Build a Scalable Big Data Analytics Pipeline | by Nam Nguyen | Towards Data Science", "h1": "How to Build a Scalable Big Data Analytics Pipeline", "description": "Data is a vital element of today\u2019s innovative enterprise. Data-driven decision making allows corporations to adapt to an unpredictable world. The ability to report on the data is the spine of\u2026"}, "outgoing_paragraph_urls": [{"url": "https://docs.python-guide.org/scenarios/serialization/", "anchor_text": "Data serialization is the process of converting structured data to a format that allows the sharing or storage of the data in a form that allows the recovery of its original structure.", "paragraph_index": 10}, {"url": "https://en.wikipedia.org/wiki/Protocol_Buffers", "anchor_text": "Protocol buffers (or protobuf)", "paragraph_index": 13}, {"url": "https://auth0.com/blog/beating-json-performance-with-protobuf/", "anchor_text": "6 times faster than JSON.", "paragraph_index": 13}, {"url": "https://parquet.apache.org/", "anchor_text": "Apache Parquet", "paragraph_index": 17}, {"url": "https://stackoverflow.com/questions/36822224/what-are-the-pros-and-cons-of-parquet-format-compared-to-other-formats", "anchor_text": "Being a column-based storage format, Parquet offers better compression and therefore optimized I/O operations.", "paragraph_index": 17}, {"url": "https://hive.apache.org/", "anchor_text": "Apache Hive", "paragraph_index": 18}, {"url": "https://en.wikipedia.org/wiki/Apache_Hive", "anchor_text": "Hive provides a SQL-like queries language (HiveQL) to execute queries directly on HDFS.", "paragraph_index": 18}, {"url": "https://prestodb.io/", "anchor_text": "Presto", "paragraph_index": 18}, {"url": "https://www.g2.com/products/hadoop-hdfs/competitors/alternatives", "anchor_text": "you can outsource the whole system to a cloud-based platform", "paragraph_index": 19}, {"url": "https://cloud.google.com/bigquery", "anchor_text": "Google Big Query", "paragraph_index": 19}, {"url": "https://aws.amazon.com/", "anchor_text": "Amazon AWS", "paragraph_index": 19}, {"url": "https://azure.microsoft.com/", "anchor_text": "Microsoft Azure", "paragraph_index": 19}, {"url": "https://www.vertica.com/", "anchor_text": "Vertica", "paragraph_index": 21}, {"url": "https://imply.io/post/compare-apache-druid-to-vertica", "anchor_text": "However, Vertica shows some disadvantages of working with real-time data or high-latency analytics", "paragraph_index": 22}, {"url": "https://druid.apache.org/", "anchor_text": "Druid", "paragraph_index": 22}, {"url": "https://searchitoperations.techtarget.com/feature/Compare-Grafana-vs-Datadog-for-IT-monitoring", "anchor_text": "any basic IT monitoring tools", "paragraph_index": 24}, {"url": "https://grafana.com/", "anchor_text": "Grafana", "paragraph_index": 24}, {"url": "https://www.datadoghq.com/", "anchor_text": "Datadog", "paragraph_index": 24}], "all_paragraphs": ["Data is a vital element of today\u2019s innovative enterprise. Data-driven decision making allows corporations to adapt to an unpredictable world. The ability to report on the data is the spine of business analytics. With the unprecedented growth of data in the 21st century, big data is no longer a buzzword but a reality that companies have to face.", "Thou shalt love thy data as thyself", "Data expands exponentially and it requires at all times the scalability of data systems. Building a big data pipeline at scale along with the integration into existing analytics ecosystems would become a big challenge for those who are not familiar with either.", "To build a scalable big data analytics pipeline, you must first identify three critical factors:", "Either they are time-series or non-time-series, you must know the nature of your pipeline\u2019s input data. It would determine under what format you store your data, what you do when data is missing, and what technology you use in the rest of the pipeline.", "When building an analytics pipeline, you need to care about the end-users. Data Analysts use your pipeline to build a reporting dashboard or visualization. The output data needs to be accessible and manipulable given end-users\u2019 possible lack of strong technical expertise in data engineering. Nowadays, famous analytics engines ease the integration between big data ecosystems and analytics warehouses.", "How much data can the pipeline ingest?", "The scalability of your data system can decide the long-term viability of the business. There\u2019s nothing much alike between handling 100 GB and 1 TB a day. The hardware and software infrastructure must keep up with a sudden change in data volume. You don\u2019t want to overload your data system due to the organic growth of your business. Scale your data pipeline for the best!", "Data collection is the first and foremost module of a data pipeline where you have to assess the origin of your data. Are they coming from another data source or top-level applications? Will the data be structured or unstructured? Do you need to perform any data cleaning? We might think of big data as a chaotic volume of data, but actually, most big data are structured. Unstructured data will require additional techniques to build a data pipeline upon it.", "Your pipeline\u2019s architecture will vary in the method you choose to collect the data: either in batch or via streaming service. A batch processing pipeline demands an efficient storage system for I/O operations whilst a streaming processing one prefers a fault-intolerant transmission protocol.", "When it comes to structured data, either they are texts, numbers, images, to feed them into the pipeline, they must go through a requisite process: data serialization. Data serialization is the process of converting structured data to a format that allows the sharing or storage of the data in a form that allows the recovery of its original structure.", "Data serialization leads to a homogeneous data structure across the pipeline, thus keeping the consistency for all the data processing modules. XML, CSV, YAML, JSON are some of the most popular formats in data serialization. Serialized data is more optimized in terms of storage and transmission. Transferring data from one system to another might encounter incompatible problems, so a bit-wise communication ensures there is no information loss.", "JSON is quite handy to handle both flat and nested data structures across the Internet. It offers a human-readable format and high integrity with JVM systems. However, in big data processing, the use of JSON is less favored than others due to its unoptimized storage and lack of structure validation.", "Protocol buffers (or protobuf) is Google\u2019s internal mechanism for serializing structured data. With protobuf, you can define a generic schema and then perform the read/write operations with your favorite programming language. Think about a language-neural format like XML, but faster and smaller. Apart from the non-human-readable disadvantage, protobuf performs up to 6 times faster than JSON.", "Suppose you have the data collection modules up and running, where will you store all those data? It depends on many things: hardware resources, data management expertise, maintenance budget, etc. You need to make up your mind before deciding where to spend your money because this is a long-term play.", "Data is the new oil, so it\u2019s best to keep the oil in your backyard", "If you have big money, the best thing is setting up your own data infrastructure. Data is the new oil, so it\u2019s best to keep the oil in your backyard. Hire the best hardware engineers, assemble a proper data center, and build your pipeline upon it. Hadoop File System (HDFS) has always been the number one choice for in-house data architecture. It offers a tightly-integrated ecosystem with all tools and platforms available for data storage and ETL. It requires a minimum effort to set up a viable Hadoop stack. Its power lies in the capacity of horizontal scaling, which means bundling commodity hardware side by side to maximize performance and minimize costs.", "You can even go the extra mile by optimizing the storage format. Storing files under .txt or .csv format might not be the brightest idea under HDFS. Apache Parquet is a columnar format available to any project in Hadoop, and it is recommended by every single data engineer out there. Being a column-based storage format, Parquet offers better compression and therefore optimized I/O operations. Its only drawback is the constraint in schema modification, for example, adding or removing a column takes more effort with parquet.", "Coming from a SQL background, you can also set up a more accessible query system. The Apache Hive data warehouse software facilitates reading, writing, and managing large datasets residing in distributed storage using SQL. Hive provides a SQL-like queries language (HiveQL) to execute queries directly on HDFS. Even though it does not follow all the SQL standards, HiveQL still eases the querying process for those who don\u2019t speak Hadoop. Another common query engine is Presto which was largely developed by Facebook engineers.", "Again, if you don\u2019t have enough resources to build your own data warehouse, you can outsource the whole system to a cloud-based platform. Many famous tech companies offer all-in-one big data architectures such as Google Big Query, Amazon AWS, Microsoft Azure. By out-sourcing, you don\u2019t have to bother setting up or maintaining the ecosystem, but that brings the risks of not being able to control your pipeline. There is a compromise between high-cost, low-maintenance, and low-cost, high-maintenance. Nevertheless, you can place your bet on the expertise of tech giants in managing your pipeline.", "Hadoop ecosystem and its alternatives are favorable for a big data storage system, but they don\u2019t fit to be an analytics engine. They aren\u2019t built to execute fast queries. For analytics purposes, we execute frequently ad hoc queries thus demand a system that returns quick results. Subordinate storage needs to be built on an analytics engine.", "Vertica is a database management system designed for analytics at scale and fast query performance. It stores data in a columnar format and creates projections to distribute data across its nodes for high-speed queries. Vertica is widely used by many tech companies thanks to its reputation for providing a robust analytics engine and efficient querying system. Vertica can play the role of a database for numerous data-related external applications thanks to its easy integration using Java, Scala, Python, C++.", "However, Vertica shows some disadvantages of working with real-time data or high-latency analytics. Its constraints on changing schemas or modifying projections limit its use on data with rapid transformation. Druid is an open-source analytics database specifically designed for Online Analytics Processing (OLAP). Time-series data requires an optimized storage mechanism and fast aggregators. It contains mostly timestamps and metrics. Druid stores metrics as columns and partitions data based on indexes and metrics altogether for quick access, therefore, provides agile aggregation operations.", "After finishing data collection, storage, and visualization integration, you might want to plug and play. But there\u2019s one last thing is what to do in case of incidents. Where do you turn to when your pipeline crashes for no reason? That\u2019s the purpose of the whole monitoring process. It helps you to track, log, and observe your system\u2019s health and performance. Some tools even allow you to debug on the fly. With that said, a proper monitoring system is a must if you want to build a data pipeline that lasts. Here we distinguish between two kinds: IT monitoring and data monitoring.", "IT monitoring is necessary for any software development. It shows various system-related metrics such as CPU, disk usage, resource consumption, memory allocated, etc. You can look at an IT monitoring and say whether you can double, or triple the pipeline\u2019s capacity. With pre-optimized ecosystems like Hadoop or Vertica, we don\u2019t need to worry much about IT performance. You can choose any basic IT monitoring tools like Grafana or Datadog to set up a simple dashboard keeping track of your metrics.", "Data monitoring is as crucial as other modules in your big data analytics pipeline. It detects data-related issues like latency, missing data, inconsistent dataset. The quality of your data pipeline reflects the integrity of data circulating within your system. These metrics ensure a minimum or zero data loss transferring from one place to another without affecting the business outcomes. We cannot name all the metrics logged by data monitoring tools because each data pipeline has its specific needs hence specific tracking. If you are building a time-series data pipeline, focus on latency-sensitive metrics. In case your data comes in batches, make sure you track properly the transmission processes. Some data monitoring tools can help you to build a straightforward data monitoring dashboard, but to suit your particular uses, it\u2019s best to build one yourself.", "We spend quite some time talking about a basic end-to-end big data analytics pipeline, and I hope you have acquired some useful knowledge. There is no all-in-one formula for building a pipeline as such, but you can base on the fundamental blueprint to craft your own.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Engineer by day, writer by night. Write a draft like no one is watching. Edit the draft as if the whole world is going to read it."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F7de18f8be848&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-scalable-big-data-analytics-pipeline-7de18f8be848&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-scalable-big-data-analytics-pipeline-7de18f8be848&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-scalable-big-data-analytics-pipeline-7de18f8be848&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-scalable-big-data-analytics-pipeline-7de18f8be848&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----7de18f8be848--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----7de18f8be848--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://namnguyenwrite.medium.com/?source=post_page-----7de18f8be848--------------------------------", "anchor_text": ""}, {"url": "https://namnguyenwrite.medium.com/?source=post_page-----7de18f8be848--------------------------------", "anchor_text": "Nam Nguyen"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3db9e6680953&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-scalable-big-data-analytics-pipeline-7de18f8be848&user=Nam+Nguyen&userId=3db9e6680953&source=post_page-3db9e6680953----7de18f8be848---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7de18f8be848&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-scalable-big-data-analytics-pipeline-7de18f8be848&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7de18f8be848&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-scalable-big-data-analytics-pipeline-7de18f8be848&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.pexels.com/@goumbik?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels", "anchor_text": "Lukas"}, {"url": "https://www.pexels.com/photo/person-writing-on-notebook-669615/?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels", "anchor_text": "Pexels"}, {"url": "https://unsplash.com/@franki?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Franki Chamaki"}, {"url": "https://unsplash.com/photos/1K6IQsQbizI?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://docs.python-guide.org/scenarios/serialization/", "anchor_text": "Data serialization is the process of converting structured data to a format that allows the sharing or storage of the data in a form that allows the recovery of its original structure."}, {"url": "https://devopedia.org/data-serialization", "anchor_text": "https://devopedia.org/data-serialization"}, {"url": "https://en.wikipedia.org/wiki/Protocol_Buffers", "anchor_text": "Protocol buffers (or protobuf)"}, {"url": "https://auth0.com/blog/beating-json-performance-with-protobuf/", "anchor_text": "6 times faster than JSON."}, {"url": "https://developers.google.com/protocol-buffers/docs/javatutorial", "anchor_text": "https://developers.google.com/protocol-buffers/docs/javatutorial"}, {"url": "https://architecht.io/what-happened-to-hadoop-211aa52a297", "anchor_text": "https://architecht.io/what-happened-to-hadoop-211aa52a297"}, {"url": "https://parquet.apache.org/", "anchor_text": "Apache Parquet"}, {"url": "https://stackoverflow.com/questions/36822224/what-are-the-pros-and-cons-of-parquet-format-compared-to-other-formats", "anchor_text": "Being a column-based storage format, Parquet offers better compression and therefore optimized I/O operations."}, {"url": "https://unsplash.com/@jankolar?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Kolar.io"}, {"url": "https://unsplash.com/s/photos/database?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://hive.apache.org/", "anchor_text": "Apache Hive"}, {"url": "https://en.wikipedia.org/wiki/Apache_Hive", "anchor_text": "Hive provides a SQL-like queries language (HiveQL) to execute queries directly on HDFS."}, {"url": "https://prestodb.io/", "anchor_text": "Presto"}, {"url": "https://www.g2.com/products/hadoop-hdfs/competitors/alternatives", "anchor_text": "you can outsource the whole system to a cloud-based platform"}, {"url": "https://cloud.google.com/bigquery", "anchor_text": "Google Big Query"}, {"url": "https://aws.amazon.com/", "anchor_text": "Amazon AWS"}, {"url": "https://azure.microsoft.com/", "anchor_text": "Microsoft Azure"}, {"url": "https://www.vertica.com/", "anchor_text": "Vertica"}, {"url": "https://unsplash.com/@lukechesser?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Luke Chesser"}, {"url": "https://unsplash.com/s/photos/chart?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://imply.io/post/compare-apache-druid-to-vertica", "anchor_text": "However, Vertica shows some disadvantages of working with real-time data or high-latency analytics"}, {"url": "https://druid.apache.org/", "anchor_text": "Druid"}, {"url": "https://searchitoperations.techtarget.com/feature/Compare-Grafana-vs-Datadog-for-IT-monitoring", "anchor_text": "any basic IT monitoring tools"}, {"url": "https://grafana.com/", "anchor_text": "Grafana"}, {"url": "https://www.datadoghq.com/", "anchor_text": "Datadog"}, {"url": "https://www.percona.com/blog/2016/10/25/monitoring-os-metrics-amazon-rds-grafana/", "anchor_text": "percona"}, {"url": "https://medium.com/tag/big-data?source=post_page-----7de18f8be848---------------big_data-----------------", "anchor_text": "Big Data"}, {"url": "https://medium.com/tag/big-data-analytics?source=post_page-----7de18f8be848---------------big_data_analytics-----------------", "anchor_text": "Big Data Analytics"}, {"url": "https://medium.com/tag/data?source=post_page-----7de18f8be848---------------data-----------------", "anchor_text": "Data"}, {"url": "https://medium.com/tag/scalability?source=post_page-----7de18f8be848---------------scalability-----------------", "anchor_text": "Scalability"}, {"url": "https://medium.com/tag/data-pipeline?source=post_page-----7de18f8be848---------------data_pipeline-----------------", "anchor_text": "Data Pipeline"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7de18f8be848&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-scalable-big-data-analytics-pipeline-7de18f8be848&user=Nam+Nguyen&userId=3db9e6680953&source=-----7de18f8be848---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7de18f8be848&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-scalable-big-data-analytics-pipeline-7de18f8be848&user=Nam+Nguyen&userId=3db9e6680953&source=-----7de18f8be848---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7de18f8be848&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-scalable-big-data-analytics-pipeline-7de18f8be848&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----7de18f8be848--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F7de18f8be848&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-scalable-big-data-analytics-pipeline-7de18f8be848&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----7de18f8be848---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----7de18f8be848--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----7de18f8be848--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----7de18f8be848--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----7de18f8be848--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----7de18f8be848--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----7de18f8be848--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----7de18f8be848--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----7de18f8be848--------------------------------", "anchor_text": ""}, {"url": "https://namnguyenwrite.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://namnguyenwrite.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Nam Nguyen"}, {"url": "https://namnguyenwrite.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "655 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3db9e6680953&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-scalable-big-data-analytics-pipeline-7de18f8be848&user=Nam+Nguyen&userId=3db9e6680953&source=post_page-3db9e6680953--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fc555a6d39ade&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-a-scalable-big-data-analytics-pipeline-7de18f8be848&newsletterV3=3db9e6680953&newsletterV3Id=c555a6d39ade&user=Nam+Nguyen&userId=3db9e6680953&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}