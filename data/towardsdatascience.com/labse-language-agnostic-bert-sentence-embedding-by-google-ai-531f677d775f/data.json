{"url": "https://towardsdatascience.com/labse-language-agnostic-bert-sentence-embedding-by-google-ai-531f677d775f", "time": 1683013095.702386, "path": "towardsdatascience.com/labse-language-agnostic-bert-sentence-embedding-by-google-ai-531f677d775f/", "webpage": {"metadata": {"title": "LaBSE: Language-Agnostic BERT Sentence Embedding by Google AI | by Rohan Jagtap | Towards Data Science", "h1": "LaBSE: Language-Agnostic BERT Sentence Embedding by Google AI", "description": "Multilingual Embedding Models are the ones that map text from multiple languages to a shared vector space (or embedding space). This implies that in this embedding space, related or similar words\u2026"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/abs/2007.01852", "anchor_text": "Feng et. al.", "paragraph_index": 1}, {"url": "https://research.fb.com/downloads/laser-language-agnostic-sentence-representations/", "anchor_text": "LASER: Language-Agnostic SEntence Representations", "paragraph_index": 2}, {"url": "https://ai.googleblog.com/2019/07/multilingual-universal-sentence-encoder.html", "anchor_text": "m-USE: Multilingual Universal Sentence Encoder", "paragraph_index": 2}, {"url": "https://www.aclweb.org/anthology/W18-6317/", "anchor_text": "Translation Ranking", "paragraph_index": 2}, {"url": "https://arxiv.org/abs/1901.07291", "anchor_text": "XLM", "paragraph_index": 3}, {"url": "https://www.aclweb.org/anthology/W18-6317/", "anchor_text": "(Guo et. al.", "paragraph_index": 5}, {"url": "https://www.ijcai.org/Proceedings/2019/746", "anchor_text": "Yang et al.", "paragraph_index": 5}, {"url": "https://www.aclweb.org/anthology/W18-6317/", "anchor_text": "Guo et. al.", "paragraph_index": 6}, {"url": "https://www.aclweb.org/anthology/W18-6317/", "anchor_text": "Guo et. al.", "paragraph_index": 7}, {"url": "https://towardsdatascience.com/additive-margin-softmax-loss-am-softmax-912e11ce1c6b", "anchor_text": "this blog", "paragraph_index": 11}, {"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "Devlin et. al", "paragraph_index": 13}, {"url": "https://arxiv.org/abs/1901.07291", "anchor_text": "XLM (Conneau and Lample)", "paragraph_index": 13}, {"url": "https://medium.com/swlh/bert-pre-training-of-transformers-for-language-understanding-5214fba4a9af", "anchor_text": "refer to my blog", "paragraph_index": 14}, {"url": "https://tfhub.dev/google/LaBSE/1", "anchor_text": "TFHub here", "paragraph_index": 19}], "all_paragraphs": ["Multilingual Embedding Models are the ones that map text from multiple languages to a shared vector space (or embedding space). This implies that in this embedding space, related or similar words will lie closer to each other, and unrelated words will be distant (refer to the figure above).", "In this article, we will discuss LaBSE: Language-Agnostic BERT Sentence Embedding, recently proposed in Feng et. al. which is the state of the art in Sentence Embedding.", "The existing approaches mostly involve training the model on a large amount of parallel data. Models like LASER: Language-Agnostic SEntence Representations and m-USE: Multilingual Universal Sentence Encoder essentially map parallel sentences directly from one language to another to obtain the embeddings. They perform pretty well across a number of languages. However, they do not perform as good as dedicated bilingual modeling approaches such as Translation Ranking (which we are about to discuss). Moreover, due to limited training data (especially for low-resource languages) and limited model capacity, these models cease to support more languages.", "Recent advances in NLP suggest training a language model on a masked language modeling (MLM) or a similar pre-training objective and then fine-tuning it on downstream tasks. Models like XLM are extended on the MLM objective, but on a cross-lingual setting. These work great on the downstream tasks but produce poor sentence-level embeddings due to the lack of a sentence-level objective.", "Rather, the production of sentence embeddings from MLMs must be learned via fine-tuning, similar to other downstream tasks.", "The proposed architecture is based on a Bidirectional Dual-Encoder (Guo et. al.) with Additive Margin Softmax (Yang et al.) with improvements. In the next few sub-sections we will decode the model in-depth:", "First things first, Guo et. al. uses a translation ranking task which essentially ranks all the target sentences in order of their compatibility with the source. This is usually not \u2018all\u2019 the sentences but some \u2018K - 1\u2019 sentences. The objective is to maximize the compatibility between the source sentence and its real translation and minimize it with others (negative sampling).", "The dual-encoder architecture essentially uses parallel encoders to encode two sequences and then obtain a compatibility score between both the encodings using a dot-product. The model in Guo et. al. was essentially trained on a parallel corpus for the translation ranking task which was discussed in the previous section.", "As far as \u2018bidirectional\u2019 is concerned; it basically takes the compatibility scores in both the \u2018directions\u2019, i.e. from source to target as well as from target to source. For example, if the compatibility from source x to target y is denoted by \u0278(x_i, y_i), then the score \u0278(y_i, x_i) is also taken into account and the individual losses are summed.", "In vector spaces, classification boundaries can be pretty narrow, hence it can be difficult to separate the vectors. AMS suggests introducing a parameter m in the original softmax loss to increase the separability between the vectors.", "Notice how the parameter m is subtracted just from the positive sample and not the negative ones which is responsible for the classification boundary.", "You can refer to this blog if you\u2019re interested in getting a better understanding of AMS.", "The translation ranking task suggests using negative sampling for \u2018K - 1\u2019 other sentences that aren\u2019t potentially compatible translations of the source sentence. This is usually done by taking sentences from the rest of the batch. This in-batch negative sampling is depicted in the above figure (left). However, LaBSE leverages BERT as its encoder network. For heavy networks like these, it is infeasible to have batch sizes that are large enough to provide sufficient negative samples for training. Thus, the proposed approach leverages distributed training methods to share batches across different accelerators (GPUs) and broadcasting them in the end. Here, all the shared batches are considered as negative samples, and just the sentences in the local batch are considered for positive sampling. This is depicted in the above figure (right).", "Finally, as mentioned earlier, the proposed architecture uses BERT encoders and are pretrained on Masked Language Model (MLM) as in Devlin et. al. and Translation Language Model (TLM) objective as in XLM (Conneau and Lample). Moreover, these are trained using a 3-stage progressive stacking algorithm i.e. an L layered encoder is first trained for L / 4 layers, then L / 2 layers and then finally L layers.", "For more on BERT pre-training, you can refer to my blog.", "LaBSE clearly outperforms its competitors with a state of the art average accuracy of 83.7% on all languages.", "LaBSE was also able to produce decent results on the languages for which training data was not available (zero-shot).", "Fun Fact: The model uses a 500k vocabulary size to support 109 languages and provides cross-lingual support for even zero-shot cases.", "We discussed the Language-Agnostic BERT Sentence Embedding model and how pre-training approaches can be incorporated to obtain the state of the art sentence embeddings.", "The model is open-sourced at TFHub here.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Immensely interested in AI Research | I read papers and post my notes on Medium"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F531f677d775f&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flabse-language-agnostic-bert-sentence-embedding-by-google-ai-531f677d775f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flabse-language-agnostic-bert-sentence-embedding-by-google-ai-531f677d775f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flabse-language-agnostic-bert-sentence-embedding-by-google-ai-531f677d775f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flabse-language-agnostic-bert-sentence-embedding-by-google-ai-531f677d775f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----531f677d775f--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----531f677d775f--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://rojagtap.medium.com/?source=post_page-----531f677d775f--------------------------------", "anchor_text": ""}, {"url": "https://rojagtap.medium.com/?source=post_page-----531f677d775f--------------------------------", "anchor_text": "Rohan Jagtap"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F39646f947a4c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flabse-language-agnostic-bert-sentence-embedding-by-google-ai-531f677d775f&user=Rohan+Jagtap&userId=39646f947a4c&source=post_page-39646f947a4c----531f677d775f---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F531f677d775f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flabse-language-agnostic-bert-sentence-embedding-by-google-ai-531f677d775f&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F531f677d775f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flabse-language-agnostic-bert-sentence-embedding-by-google-ai-531f677d775f&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://ai.googleblog.com/2020/08/language-agnostic-bert-sentence.html", "anchor_text": "Google AI Blog"}, {"url": "https://arxiv.org/abs/2007.01852", "anchor_text": "Feng et. al."}, {"url": "https://research.fb.com/downloads/laser-language-agnostic-sentence-representations/", "anchor_text": "LASER: Language-Agnostic SEntence Representations"}, {"url": "https://ai.googleblog.com/2019/07/multilingual-universal-sentence-encoder.html", "anchor_text": "m-USE: Multilingual Universal Sentence Encoder"}, {"url": "https://www.aclweb.org/anthology/W18-6317/", "anchor_text": "Translation Ranking"}, {"url": "https://arxiv.org/abs/1901.07291", "anchor_text": "XLM"}, {"url": "https://arxiv.org/abs/2007.01852", "anchor_text": "LaBSE Paper"}, {"url": "https://arxiv.org/abs/2007.01852", "anchor_text": "LaBSE Paper"}, {"url": "https://www.aclweb.org/anthology/W18-6317/", "anchor_text": "(Guo et. al."}, {"url": "https://www.ijcai.org/Proceedings/2019/746", "anchor_text": "Yang et al."}, {"url": "https://ai.googleblog.com/2020/08/language-agnostic-bert-sentence.html", "anchor_text": "Google AI Blog"}, {"url": "https://www.aclweb.org/anthology/W18-6317/", "anchor_text": "Guo et. al."}, {"url": "https://www.aclweb.org/anthology/W18-6317/", "anchor_text": "Guo et. al."}, {"url": "https://www.aclweb.org/anthology/W18-6317/", "anchor_text": "Guo et. al."}, {"url": "https://www.ijcai.org/Proceedings/2019/746", "anchor_text": "Yang et al."}, {"url": "https://arxiv.org/abs/2007.01852", "anchor_text": "LaBSE Paper"}, {"url": "https://towardsdatascience.com/additive-margin-softmax-loss-am-softmax-912e11ce1c6b", "anchor_text": "this blog"}, {"url": "https://arxiv.org/abs/2007.01852", "anchor_text": "LaBSE Paper"}, {"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "Devlin et. al"}, {"url": "https://arxiv.org/abs/1901.07291", "anchor_text": "XLM (Conneau and Lample)"}, {"url": "https://medium.com/swlh/bert-pre-training-of-transformers-for-language-understanding-5214fba4a9af", "anchor_text": "refer to my blog"}, {"url": "https://github.com/facebookresearch/LASER/tree/master/data/tatoeba/v1", "anchor_text": "Tatoeba Datasets"}, {"url": "https://ai.googleblog.com/2020/08/language-agnostic-bert-sentence.html", "anchor_text": "Google AI Blog"}, {"url": "https://ai.googleblog.com/2020/08/language-agnostic-bert-sentence.html", "anchor_text": "Google AI Blog"}, {"url": "https://tfhub.dev/google/LaBSE/1", "anchor_text": "TFHub here"}, {"url": "https://arxiv.org/abs/2007.01852", "anchor_text": "https://arxiv.org/abs/2007.01852"}, {"url": "https://www.ijcai.org/Proceedings/2019/746", "anchor_text": "https://www.ijcai.org/Proceedings/2019/746"}, {"url": "https://www.aclweb.org/anthology/W18-6317/", "anchor_text": "https://www.aclweb.org/anthology/W18-6317/"}, {"url": "https://arxiv.org/abs/1901.07291", "anchor_text": "https://arxiv.org/abs/1901.07291"}, {"url": "https://medium.com/swlh/bert-pre-training-of-transformers-for-language-understanding-5214fba4a9af", "anchor_text": "BERT: Pre-Training of Transformers for Language UnderstandingUnderstanding Transformer-Based Self-Supervised Architecturesmedium.com"}, {"url": "https://towardsdatascience.com/additive-margin-softmax-loss-am-softmax-912e11ce1c6b", "anchor_text": "Additive Margin Softmax Loss (AM-Softmax)Understanding L-Softmax, A-Softmax, and AM-Softmaxtowardsdatascience.com"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----531f677d775f---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----531f677d775f---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----531f677d775f---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----531f677d775f---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----531f677d775f---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F531f677d775f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flabse-language-agnostic-bert-sentence-embedding-by-google-ai-531f677d775f&user=Rohan+Jagtap&userId=39646f947a4c&source=-----531f677d775f---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F531f677d775f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flabse-language-agnostic-bert-sentence-embedding-by-google-ai-531f677d775f&user=Rohan+Jagtap&userId=39646f947a4c&source=-----531f677d775f---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F531f677d775f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flabse-language-agnostic-bert-sentence-embedding-by-google-ai-531f677d775f&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----531f677d775f--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F531f677d775f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flabse-language-agnostic-bert-sentence-embedding-by-google-ai-531f677d775f&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----531f677d775f---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----531f677d775f--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----531f677d775f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----531f677d775f--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----531f677d775f--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----531f677d775f--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----531f677d775f--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----531f677d775f--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----531f677d775f--------------------------------", "anchor_text": ""}, {"url": "https://rojagtap.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://rojagtap.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Rohan Jagtap"}, {"url": "https://rojagtap.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "465 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F39646f947a4c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flabse-language-agnostic-bert-sentence-embedding-by-google-ai-531f677d775f&user=Rohan+Jagtap&userId=39646f947a4c&source=post_page-39646f947a4c--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fe51e2b6202c5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flabse-language-agnostic-bert-sentence-embedding-by-google-ai-531f677d775f&newsletterV3=39646f947a4c&newsletterV3Id=e51e2b6202c5&user=Rohan+Jagtap&userId=39646f947a4c&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}