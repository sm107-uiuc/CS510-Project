{"url": "https://towardsdatascience.com/neural-networks-intuitions-9-distance-metric-learning-dae7c3a0ebdf", "time": 1683012644.4731038, "path": "towardsdatascience.com/neural-networks-intuitions-9-distance-metric-learning-dae7c3a0ebdf/", "webpage": {"metadata": {"title": "Neural Networks Intuitions: 9. Distance Metric Learning | by Raghul Asokan | Towards Data Science", "h1": "Neural Networks Intuitions: 9. Distance Metric Learning", "description": "Welcome back to my series Neural Networks Intuitions. In this ninth segment, we will be looking into deep distance metric learning, the motivation behind using it, wide range of methods proposed and\u2026"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/pdf/1703.07464.pdf", "anchor_text": "No Fuss Distance Metric Learning using Proxies", "paragraph_index": 2}, {"url": "http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf", "anchor_text": "Dimensionality Reduction by Learning an Invariant Mapping", "paragraph_index": 18}, {"url": "https://arxiv.org/pdf/1503.03832.pdf", "anchor_text": "FaceNet: A Unified Embedding for Face Recognition and Clustering", "paragraph_index": 28}, {"url": "https://kpzhang93.github.io/papers/eccv2016.pdf", "anchor_text": "A Discriminative Feature Learning Approach for Deep Face Recognition", "paragraph_index": 36}], "all_paragraphs": ["Welcome back to my series Neural Networks Intuitions. In this ninth segment, we will be looking into deep distance metric learning, the motivation behind using it, wide range of methods proposed and its applications.", "Note: All techniques discussed in this article comes under Deep Metric Learning (DML) i.e distance metric learning using neural networks.", "Distance Metric Learning means learning a distance in a low dimensional space which is consistent with the notion of semantic similarity. (as given in [No Fuss Distance Metric Learning using Proxies])", "What does the above statement mean w.r.t image domain?", "It means learning a distance in a low dimensional space(non-input space) such that similar images in the input space result in similar representation(low distance) and dissimilar images result in varied representation(high distance).", "Okay, this sounds exactly what a classifier does. Isn\u2019t it? Yes.", "So how is this different from supervised image classification? Why different terminology?", "Metric learning addresses the problem of open-set setup in machine learning i.e generalize to new examples at test time.", "This is not possible by a feature-extractor followed by fully connected layer Classification network.", "This is a very important question. The answer is as follows:", "And do not aim to minimize intra-class distance which results in the desirable discriminative features.", "This aspect of learning discriminative features is what metric learning achieves.", "Before looking into the most widely used methods in metric learning, let us look into its applications to make the problem statement more concrete and why a standard classification approach may not be suitable.", "Applications of metric learning are as follows:", "Now let us see the prominent methods employed in metric learning:", "a. Siamese network with contrastive loss(pairs)", "b. Triple networks with triplet loss(triplets)", "To produce embeddings which are close in euclidean space(assuming) for similar images and far apart for dissimilar images.", "The paper Dimensionality Reduction by Learning an Invariant Mapping solves the problem by taking two images as input and outputs whether the image pair is similar or not.", "So now we need a loss function(incorporating the euclidean distance) which is zero for similar pairs and one for dissimilar pairs.", "That\u2019s exactly what the contrastive loss function does!", "Let (X1, X2) be the input image pair, Gw be the function mapping producing low dimensional representations(where w represents the parameters), Y be the ground truth label denoting similar or not and Dw represents the euclidean distance.", "Contrastive loss bear a resemblance to the traditional cross entropy loss. The first term in the loss function takes care of similar pairs(Y=0) such that Dw becomes 0. The second term takes care of dissimilar pairs(Y=1) such that Dw becomes at least m.", "Here m is the margin and m > 0.", "Why do we need to show dissimilar pairs during training? Why not simply minimize the loss function over a set of similar pairs?", "The authors answer that, \u201cThe contrastive term involving dissimilar pairs is crucial. Simply minimizing DW (X1, X2) over the set of all similar pairs will usually lead to a collapsed solution, since DW and the loss L could then be made zero by setting GW to a constant\u201d.", "Now that we have understood what Contrastive Loss function is, what is a Siamsese Network then?", "This architecture where the same network(i.e sharing same set of parameters) is used for extracting low dimensional representations for both images in a pair is called the Siamese architecture.", "The paper FaceNet: A Unified Embedding for Face Recognition and Clustering takes a similar approach as contrastive loss \u2014 except instead of dealing with pairs at every step, it considers a triplet of images.", "A triplet consists of an anchor, positive(similar to anchor) and negative(dissimilar to anchor) image.", "Let f be the function mapping producing low dimensional representation, xa be the anchor image, xp be the positive image, xn be the negative image and \u03b1 be the margin.", "Triplet loss ensures that the representation of an anchor image is closer to all images similar to it, than it is to any other negative image.", "The architecture where the same network(i.e sharing same set of parameters) is used for extracting low dimensional representations for all images in a triplet is called the Triplet architecture/network.", "So, how are the above problems tackled?", "By careful selection of training image pairs and triplets \u2014 offline or online and using a larger batch size.", "*Note: There are a wide range of techniques that helps in solving the sampling issue with contrastive and triplet methods which I hope to cover in my future articles :)", "The paper A Discriminative Feature Learning Approach for Deep Face Recognition solves the objective using the vanilla neural network classification by introducing a new loss called center loss in addition to the cross entropy loss i.e a joint supervision signal.", "This paper solves the problem mentioned at the start of the article \u2014 \u201cClassifiers do not minimize intra-class distance but only maxmize inter-class distance resulting in linearly separable features but not discriminative features\u201d.", "Center loss minimizes the distance between every class center and the class samples\u2019 representation \u2014 which makes sure that representations of samples in the same class stay similar in addition to preserving inter-class distance(taken care by CE loss).", "Let x be the input sample and c be the class center for that sample.", "The distance between class center(embeddings) and the sample embeddings are computed for every iteration and the weights are updated.", "That\u2019s all in this article on Deep Distance Metric Learning. I hope all of you got a good grasp on what the problem is and how it is being addressed through various interesting techniques :)", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Director of AI R&D at Infilect"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fdae7c3a0ebdf&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-intuitions-9-distance-metric-learning-dae7c3a0ebdf&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-intuitions-9-distance-metric-learning-dae7c3a0ebdf&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-intuitions-9-distance-metric-learning-dae7c3a0ebdf&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-intuitions-9-distance-metric-learning-dae7c3a0ebdf&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----dae7c3a0ebdf--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----dae7c3a0ebdf--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://raghul-719.medium.com/?source=post_page-----dae7c3a0ebdf--------------------------------", "anchor_text": ""}, {"url": "https://raghul-719.medium.com/?source=post_page-----dae7c3a0ebdf--------------------------------", "anchor_text": "Raghul Asokan"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe3b61249ec9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-intuitions-9-distance-metric-learning-dae7c3a0ebdf&user=Raghul+Asokan&userId=e3b61249ec9&source=post_page-e3b61249ec9----dae7c3a0ebdf---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdae7c3a0ebdf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-intuitions-9-distance-metric-learning-dae7c3a0ebdf&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdae7c3a0ebdf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-intuitions-9-distance-metric-learning-dae7c3a0ebdf&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://arxiv.org/pdf/1703.07464.pdf", "anchor_text": "No Fuss Distance Metric Learning using Proxies"}, {"url": "http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf", "anchor_text": "Dimensionality Reduction by Learning an Invariant Mapping"}, {"url": "http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf", "anchor_text": "Dimensionality Reduction by Learning an Invariant Mapping"}, {"url": "https://arxiv.org/pdf/1503.03832.pdf", "anchor_text": "FaceNet: A Unified Embedding for Face Recognition and Clustering"}, {"url": "https://kpzhang93.github.io/papers/eccv2016.pdf", "anchor_text": "A Discriminative Feature Learning Approach for Deep Face Recognition"}, {"url": "https://neptune.ai/blog/content-based-image-retrieval-with-siamese-networks", "anchor_text": "https://neptune.ai/blog/content-based-image-retrieval-with-siamese-networks"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----dae7c3a0ebdf---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/metric-learning?source=post_page-----dae7c3a0ebdf---------------metric_learning-----------------", "anchor_text": "Metric Learning"}, {"url": "https://medium.com/tag/siamese-networks?source=post_page-----dae7c3a0ebdf---------------siamese_networks-----------------", "anchor_text": "Siamese Networks"}, {"url": "https://medium.com/tag/triplet-loss?source=post_page-----dae7c3a0ebdf---------------triplet_loss-----------------", "anchor_text": "Triplet Loss"}, {"url": "https://medium.com/tag/representation-learning?source=post_page-----dae7c3a0ebdf---------------representation_learning-----------------", "anchor_text": "Representation Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fdae7c3a0ebdf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-intuitions-9-distance-metric-learning-dae7c3a0ebdf&user=Raghul+Asokan&userId=e3b61249ec9&source=-----dae7c3a0ebdf---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fdae7c3a0ebdf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-intuitions-9-distance-metric-learning-dae7c3a0ebdf&user=Raghul+Asokan&userId=e3b61249ec9&source=-----dae7c3a0ebdf---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdae7c3a0ebdf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-intuitions-9-distance-metric-learning-dae7c3a0ebdf&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----dae7c3a0ebdf--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fdae7c3a0ebdf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-intuitions-9-distance-metric-learning-dae7c3a0ebdf&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----dae7c3a0ebdf---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----dae7c3a0ebdf--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----dae7c3a0ebdf--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----dae7c3a0ebdf--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----dae7c3a0ebdf--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----dae7c3a0ebdf--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----dae7c3a0ebdf--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----dae7c3a0ebdf--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----dae7c3a0ebdf--------------------------------", "anchor_text": ""}, {"url": "https://raghul-719.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://raghul-719.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Raghul Asokan"}, {"url": "https://raghul-719.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "330 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe3b61249ec9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-intuitions-9-distance-metric-learning-dae7c3a0ebdf&user=Raghul+Asokan&userId=e3b61249ec9&source=post_page-e3b61249ec9--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fb225bcb64cc2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-intuitions-9-distance-metric-learning-dae7c3a0ebdf&newsletterV3=e3b61249ec9&newsletterV3Id=b225bcb64cc2&user=Raghul+Asokan&userId=e3b61249ec9&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}