{"url": "https://towardsdatascience.com/available-hyperparameter-optimization-techniques-dc60fb836264", "time": 1683002659.926097, "path": "towardsdatascience.com/available-hyperparameter-optimization-techniques-dc60fb836264/", "webpage": {"metadata": {"title": "Available hyperparameter-optimization techniques | by Gabriel Naya | Towards Data Science", "h1": "Available hyperparameter-optimization techniques", "description": "I have often received articles referring to automation searching for hyperparameter optimization. Once I have understood the intent and scope of these techniques, I have wondered how much a model can\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.kaggle.com/c/titanic/data", "anchor_text": "Kaggle\u2019s Titanic passenger dataset", "paragraph_index": 6}, {"url": "https://machinelearningmastery.com/author/jasonb/", "anchor_text": "Jason Brownlee", "paragraph_index": 9}, {"url": "https://towardsdatascience.com/@williamkoehrsen?source=post_page-----dfda59b72f8a----------------------", "anchor_text": "Will Koehrsen\u00b3", "paragraph_index": 9}, {"url": "https://towardsdatascience.com/automated-machine-learning-hyperparameter-tuning-in-python-dfda59b72f8a", "anchor_text": "excellent tutoria", "paragraph_index": 22}, {"url": "https://github.com/WillKoehrsen/hyperparameter-optimization/blob/master/Bayesian%20Hyperparameter%20Optimization%20of%20Gradient%20Boosting%20Machine.ipynb", "anchor_text": "code", "paragraph_index": 22}, {"url": "https://towardsdatascience.com/@williamkoehrsen?source=post_page-----dfda59b72f8a----------------------", "anchor_text": "Will Koehrsen", "paragraph_index": 22}, {"url": "https://github.com/hyperopt/hyperopt/wiki/FMin", "anchor_text": "the hyperopt documentation", "paragraph_index": 24}, {"url": "https://en.wikipedia.org/wiki/Binary_classification", "anchor_text": "binary classification", "paragraph_index": 30}, {"url": "https://en.wikipedia.org/wiki/True_positive", "anchor_text": "true positives", "paragraph_index": 30}, {"url": "https://en.wikipedia.org/wiki/True_negative", "anchor_text": "true negatives", "paragraph_index": 30}, {"url": "https://gnaya73.glitch.me/", "anchor_text": "https://gnaya73.glitch.me/", "paragraph_index": 33}], "all_paragraphs": ["I have often received articles referring to automation searching for hyperparameter optimization. Once I have understood the intent and scope of these techniques, I have wondered how much a model can be improved through this optimization.The aim of this article is then to investigate the different available optimization techniques, and test them on a simple example, compare them and see an overview of the improvements obtained.", "Hyperparameters are model adjustable parameters that must be tuned to obtain a model with optimal performance. Then, optimizing the hyperparameters of a model is a crucial task to increase the performance of the selected algorithm.", "We need to know, to some extent, the implication that each hyperparameter has in each algorithm and its possible values. Understanding in-depth the meaning of each of these hyperparameters in the different algorithms is something necessary and a huge task that sometimes implies knowing the way the algorithm works internally and the mathematics behind it. The content of this article does not reach that depth, although we will use different algorithms to analyze by selecting some hyperparameters in each of them.", "Anyone who has used any algorithm has probably already made some manual optimization attempts on the default set of values. This manual adjustment usually takes a long time, is not always done rigorously and makes it difficult to systematize the results", "Secondly, we can apply available automated and quite simple techniques such as Grid Search and Random Search, which usually give better results, but with a high cost of time and machine computation. We will apply both techniques to compare their results", "Finally, we will apply Bayesian optimization, which is a method to find the minimum of a function, using the hyperopt library of Python on the best of the tested algorithms. The implementation of this technique may not be so easy, but it can give us better results in performance or time than the previous ones.", "The dataset we will use for the exercise is Kaggle\u2019s Titanic passenger dataset, which is a binary classification exercise to predict which passengers have survived and which have not.", "Metric selected is accuracy\u00b9: the percentage of passengers correctly predicted. Accuracy = (TP+TN) / (Total)", "We are going to apply fundamental techniques of data engineering that will allow us to use the algorithms without errors (remember that it is not the objective of this article to obtain a good result, only to compare the performance when applying optimization techniques).", "We select eight algorithms to perform the work; this decision is based on reusing part of the work of Jason Brownlee\u00b2 and Will Koehrsen\u00b3 that have developed much of the code used here.", "Each of these models will be a different column in our results table:", "For each of these columns, we will try to apply the following optimization techniques:", "As mentioned above, the first row of our results table is the starting point of the analysis, taking the default values for the hyperparameters of each of the algorithms:", "\u201cGrid search works by trying every possible combination of parameters you want to try in your model. Those parameters are each tried in a series of cross-validation passes. This technique has been in vogue for the past several years as a way to tune your models\u201d \u2074.", "Now we must define a dictionary with the different parameters and their values for each algorithm", "Then, apply GridSearchCV for each one:", "The different algorithms have many more parameters that have not been deliberately including here, and you could force searches to many more values in each hyperparameter. This reduction has been developing according to the time and computing capacity that we are willing to invest.", "\u201cEnter randomized search. Consider trying every possible combination takes a lot of brute force computation. Data Scientists are an impatient bunch, so they adopted a faster technique: randomly sample from a range of parameters. The idea is that you will cover on the near-optimal set of parameters faster than grid search. This technique, however, is naive. It doesn\u2019t know or remember anything from its previous runs.\u201d \u2074", "For practical purposes, the code is identical to GridSearchCV by modifying:", "So far, if we check the results grid, we find that the application of automated search techniques has given good results.In some cases, like SGD or SVM algorithm has been much improved, from a 67\u201369% floor to 78\u201376%.The general trend has been to improve 1, 2, or 3 percentage points, obtaining better results with GridSearchCV than with RandomSearchCV, being the random application times better than those of the grid.", "Let\u2019s analyze the winning algorithm (Light Gradient Boost) in his version of GridSearchCV:", "To know the hyperparameters found in this version of the algorithm we use:", "I must confess that before I understood how to apply the Bayesian optimization technique, I felt quite lost and confused, to the point of being close to giving up. The documentation and examples both from the library and from other articles that I could take as references are quite vague, or too dull, or very outdated. All this until I found this excellent tutorial\u2075 and its code\u2076 by Will Koehrsen, which I advise you to review and try step by step since it is clear and exhaustive. Following him, the first thing we are going to do is to define our objective function that must return a dictionary at least with the tags \u2018loss\u2019 and \u2018status\u2019.", "Domain Space: The domain space represents the range of values we want to evaluate for each hyperparameter. Each iteration of the search, the Bayesian optimization algorithm will choose one value for each hyperparameter from the domain space. When we do random or grid search, the domain space is a grid. In Bayesian optimization the idea is the same except this space has probability distributions for each hyperparameter rather than discrete values.", "Here use different domain distribution types (look for complete distributions list in the hyperopt documentation):", "Finally, with all the code ready, we look for the best combination of parameters through the fmin function:", "It triggers the process of finding the best combination. In the original example, the variable MAX_EVALS was set to 500; due to performance issues for this exercise, it was reduced to 100, which may have affected the final result.", "Once the process is finishing, we can take the Trials object (bayes_trials in our case) and analyze its results:", "We can also load a dataframe from the CSV, and use the \u2018ast\u2019 library to transform a text into a dictionary and feed our final model with the best result.", "We have developed the complete code to apply three available optimization techniques, on eight different classification algorithms.The time and computational capacity required to apply these techniques is a topic to take into account; it seems necessary to perform this optimization when the state of the art of the selected model is high enough.Parameter optimization does not necessarily imply a continuing improvement in the results of the trained model, over the test data, since the selection of parameters could be generating overfitting.Finally, it should be noted that the improvements observed on the different models have generally been of a considerable magnitude, which leaves at least the door open to the possibility of increasing the performance of the algorithm through any of these techniques.", "In binary classification Accuracy is also used as a statistical measure of how well a binary classification test correctly identifies or excludes a condition. That is, the accuracy is the proportion of true results (both true positives and true negatives) among the total number of cases examined\u2026 The formula for quantifying binary accuracy is:", "where: TP = True positive; FP = False positive; TN = True negative; FN = False negative", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Machine learning enthusiast and research at Kreilabs Uruguay. My profile: https://gnaya73.glitch.me/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fdc60fb836264&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Favailable-hyperparameter-optimization-techniques-dc60fb836264&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Favailable-hyperparameter-optimization-techniques-dc60fb836264&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Favailable-hyperparameter-optimization-techniques-dc60fb836264&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Favailable-hyperparameter-optimization-techniques-dc60fb836264&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----dc60fb836264--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----dc60fb836264--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://gabriel-naya73.medium.com/?source=post_page-----dc60fb836264--------------------------------", "anchor_text": ""}, {"url": "https://gabriel-naya73.medium.com/?source=post_page-----dc60fb836264--------------------------------", "anchor_text": "Gabriel Naya"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4213edda07c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Favailable-hyperparameter-optimization-techniques-dc60fb836264&user=Gabriel+Naya&userId=4213edda07c&source=post_page-4213edda07c----dc60fb836264---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdc60fb836264&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Favailable-hyperparameter-optimization-techniques-dc60fb836264&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdc60fb836264&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Favailable-hyperparameter-optimization-techniques-dc60fb836264&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@vaniashows?utm_source=medium&utm_medium=referral", "anchor_text": "Vania Shows"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://www.kaggle.com/c/titanic/data", "anchor_text": "Kaggle\u2019s Titanic passenger dataset"}, {"url": "https://machinelearningmastery.com/author/jasonb/", "anchor_text": "Jason Brownlee"}, {"url": "https://towardsdatascience.com/@williamkoehrsen?source=post_page-----dfda59b72f8a----------------------", "anchor_text": "Will Koehrsen\u00b3"}, {"url": "https://towardsdatascience.com/automated-machine-learning-hyperparameter-tuning-in-python-dfda59b72f8a", "anchor_text": "excellent tutoria"}, {"url": "https://github.com/WillKoehrsen/hyperparameter-optimization/blob/master/Bayesian%20Hyperparameter%20Optimization%20of%20Gradient%20Boosting%20Machine.ipynb", "anchor_text": "code"}, {"url": "https://towardsdatascience.com/@williamkoehrsen?source=post_page-----dfda59b72f8a----------------------", "anchor_text": "Will Koehrsen"}, {"url": "https://github.com/hyperopt/hyperopt/wiki/FMin", "anchor_text": "the hyperopt documentation"}, {"url": "https://en.wikipedia.org/w/index.php?title=Accuracy_and_precision&action=edit&section=4", "anchor_text": "https://en.wikipedia.org/w/index.php?title=Accuracy_and_precision&action=edit&section=4"}, {"url": "https://en.wikipedia.org/wiki/Binary_classification", "anchor_text": "binary classification"}, {"url": "https://en.wikipedia.org/wiki/True_positive", "anchor_text": "true positives"}, {"url": "https://en.wikipedia.org/wiki/True_negative", "anchor_text": "true negatives"}, {"url": "https://machinelearningmastery.com/hyperparameters-for-classification-machine-learning-algorithms/", "anchor_text": "https://machinelearningmastery.com/hyperparameters-for-classification-machine-learning-algorithms/"}, {"url": "https://towardsdatascience.com/automated-machine-learning-hyperparameter-tuning-in-python-dfda59b72f8a", "anchor_text": "https://towardsdatascience.com/automated-machine-learning-hyperparameter-tuning-in-python-dfda59b72f8a"}, {"url": "https://medium.com/apprentice-journal/hyper-parameter-optimization-c9b78372447b", "anchor_text": "https://medium.com/apprentice-journal/hyper-parameter-optimization-c9b78372447b"}, {"url": "https://towardsdatascience.com/automated-machine-learning-hyperparameter-tuning-in-python-dfda59b72f8a", "anchor_text": "https://towardsdatascience.com/automated-machine-learning-hyperparameter-tuning-in-python-dfda59b72f8a"}, {"url": "https://github.com/WillKoehrsen/hyperparameter-optimization/blob/master/Bayesian%20Hyperparameter%20Optimization%20of%20Gradient%20Boosting%20Machine.ipynb", "anchor_text": "https://github.com/WillKoehrsen/hyperparameter-optimization/blob/master/Bayesian%20Hyperparameter%20Optimization%20of%20Gradient%20Boosting%20Machine.ipynb"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----dc60fb836264---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/hyperparameter-tuning?source=post_page-----dc60fb836264---------------hyperparameter_tuning-----------------", "anchor_text": "Hyperparameter Tuning"}, {"url": "https://medium.com/tag/programming?source=post_page-----dc60fb836264---------------programming-----------------", "anchor_text": "Programming"}, {"url": "https://medium.com/tag/classification?source=post_page-----dc60fb836264---------------classification-----------------", "anchor_text": "Classification"}, {"url": "https://medium.com/tag/data-science?source=post_page-----dc60fb836264---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fdc60fb836264&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Favailable-hyperparameter-optimization-techniques-dc60fb836264&user=Gabriel+Naya&userId=4213edda07c&source=-----dc60fb836264---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fdc60fb836264&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Favailable-hyperparameter-optimization-techniques-dc60fb836264&user=Gabriel+Naya&userId=4213edda07c&source=-----dc60fb836264---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdc60fb836264&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Favailable-hyperparameter-optimization-techniques-dc60fb836264&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----dc60fb836264--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fdc60fb836264&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Favailable-hyperparameter-optimization-techniques-dc60fb836264&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----dc60fb836264---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----dc60fb836264--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----dc60fb836264--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----dc60fb836264--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----dc60fb836264--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----dc60fb836264--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----dc60fb836264--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----dc60fb836264--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----dc60fb836264--------------------------------", "anchor_text": ""}, {"url": "https://gabriel-naya73.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://gabriel-naya73.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Gabriel Naya"}, {"url": "https://gabriel-naya73.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "78 Followers"}, {"url": "https://gnaya73.glitch.me/", "anchor_text": "https://gnaya73.glitch.me/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4213edda07c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Favailable-hyperparameter-optimization-techniques-dc60fb836264&user=Gabriel+Naya&userId=4213edda07c&source=post_page-4213edda07c--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F4213edda07c%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Favailable-hyperparameter-optimization-techniques-dc60fb836264&user=Gabriel+Naya&userId=4213edda07c&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}