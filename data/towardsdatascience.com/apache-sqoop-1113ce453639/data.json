{"url": "https://towardsdatascience.com/apache-sqoop-1113ce453639", "time": 1683007497.339704, "path": "towardsdatascience.com/apache-sqoop-1113ce453639/", "webpage": {"metadata": {"title": "Apache Sqoop. RDBMS to HDFS and back | by Prathamesh Nimkar | Towards Data Science", "h1": "Apache Sqoop", "description": "Getting data from RDBMS to HDFS and back"}, "outgoing_paragraph_urls": [{"url": "https://docs.cloudera.com/documentation/enterprise/5-4-x/topics/cdh_ig_jdbc_driver_install.html", "anchor_text": "Cloudera says that Sqoop doesn\u2019t ship with third-party JDBC drivers and must be installed separately", "paragraph_index": 12}, {"url": "https://medium.com/@prathamesh.nimkar/hdfs-commands-b8a745ca9318", "anchor_text": "HDFS\u2019 Name Node Web UI", "paragraph_index": 17}, {"url": "http://instance-1:8889/", "anchor_text": "http://instance-1:8889/", "paragraph_index": 17}, {"url": "https://sqoop.apache.org/docs/1.4.6/SqoopUserGuide.html", "anchor_text": "Apache Sqoop\u2019s user guide", "paragraph_index": 32}, {"url": "https://sqoop.apache.org/docs/1.4.6/SqoopUserGuide.html", "anchor_text": "Sqoop User Guide", "paragraph_index": 33}, {"url": "https://docs.cloudera.com/documentation/enterprise/6/6.3/topics/cm_mc_sqoop1_client.html", "anchor_text": "Managing the Sqoop Client", "paragraph_index": 34}, {"url": "https://cwiki.apache.org/confluence/display/SQOOP/Sqoop+Presentations+and+Blogs", "anchor_text": "Sqoop2 New Generation of Big Data \u2014 Apache Sqoop Workflow", "paragraph_index": 35}, {"url": "https://github.com/pratnimk/sqoop-big-data-analysis/raw/master/915662529_T_ONTIME_REPORTING.zip", "anchor_text": "here", "paragraph_index": 37}, {"url": "https://medium.com/@prathamesh.nimkar/bb6dc35c1a06?source=friends_link&sk=af5eab7956455ae93784c9a0ee8a9fb8", "anchor_text": "Here you go", "paragraph_index": 37}, {"url": "https://www.bts.gov/", "anchor_text": "here", "paragraph_index": 37}, {"url": "https://www.transtats.bts.gov/DL_SelectFields.asp?Table_ID=236&DB_Short_Name=On-Time", "anchor_text": "here", "paragraph_index": 37}, {"url": "https://www.transtats.bts.gov/Fields.asp?Table_ID=236", "anchor_text": "here", "paragraph_index": 37}, {"url": "https://www.linkedin.com/in/prathameshnimkar/", "anchor_text": "https://www.linkedin.com/in/prathameshnimkar/", "paragraph_index": 39}], "all_paragraphs": ["Across the globe, the most popular databases are SQL based. Considering this, it is paramount for any data lake to be able to pull data from an RDBMS database. Thus, Apache Sqoop was born.", "Apache Sqoop supports bi-directional movement of data between any RDBMS and HDFS, Hive or HBase, etc. But, structured data only. It works in a batch-loading format (similar to extraction in ETL applications), supporting extraction of large volume of data, to the tune of multiple terabytes that RDBMS\u2019 are simply, unable to handle.", "The above GIF is a workflow of the Sqoop process to load data into HDFS from RDBMS. The converse also holds true. Here\u2019s how it works:", "Initiator: The client submits a job to the Sqoop server to load data from source to target (i.e. RDBMS to HDFS, in this case). The connection pool, schema and metadata validation is done at this stage.", "Partitioner: The data is to be extracted now. However, for example, a table of size 1 TB cannot be handled as one chunk of data. So, we have a partitioner, wherein, the data is broken down into digestible chunks/blocks of data to be extracted in parallel. Please note, no data is stored here.", "Extractor: In this part of the framework, the data is actually moved from the source in specific chunks/blocks. Important to note that not all of the data is loaded into memory for extraction at once. We can think of it as micro-batch processing. Again, no data is stored here.", "Loader: The extracted data is now pushed to the target through the Loader phase of framework. Now the data is actually stored.", "Cleaner: This is just a clean-up activity to free-up the resources used.", "Sqoop 2.x was launched using Map Reduce as its primary processing framework. Sqoop with Spark is a possibility, which can be easily configured through Sqoop\u2019s plug-n-play or modular framework discussed above. That\u2019s the beauty/essence of this generic workflow.", "If you haven\u2019t completed the installations, now\u2019s the right time to do so!", "Let\u2019s use Sqoop to automatically create a new HDFS folder called structuredFlightDataset and import data into it.", "Please note, the above import command diagram and labels apply to the export command too with minor, obvious differences.", "This would throw a nice little error as mysql\u2019s java connector is missing on instance-1. Cloudera says that Sqoop doesn\u2019t ship with third-party JDBC drivers and must be installed separately. Furthermore, it needs a headless open-JDK and some access privileges on the MySQL database front.", "Now, we need to grant access from remote servers/hosts on MySQL:", "Login to MySQL, create users with specific IPs and provide the appropriate grants. Please note, MySQL no longer permits creation of users directly from GRANT commands. The username and passwords can be the same as long as the hostnames are different.Quick tip: Hostnames can be found here Cloudera Manager->Host->All Hosts", "Once that is done, you can re-run the Sqoop import command. First, let\u2019s analyze the logs in depth:", "Let\u2019s check the data imported into HDFS:", "This should give you 5 files. You can view them from HDFS\u2019 Name Node Web UI or Hue\u2019s Web UI \u2014 http://instance-1:8889/ . You may need to download the files to view. Please note _SUCCESS file contains nothing, its just a flag value. The part-m-00000/1/2/3 files appear as a csv of the actual data.", "In order to control how many parallel processes aka mappers/splits are created and executed, we can tweak the import command as such:", "The number of mappers parameter is just a suggestion to YARN. YARN may choose to completely ignore this suggestion.", "It gives 2 output files, namely, part-m-00000 & _SUCCESS flag. You\u2019ll notice it is a bit slower, but this is just 600k records. Imagine the significant difference in performance if it were 100 million records.", "Since mappers are threads, what if you provide twice as many mappers as host machines? Would that enable further performance improvements?", "As you\u2019ll note, there are no differences in the time taken but it results in 8 output files (part-m-00000\u20138) & 1 _SUCCESS flag.", "Best to keep the number of mappers the same as number of data nodes, where the processing can/will be run.", "An interesting question that arises here is, can we use Sqoop to load data into HDFS using SQL queries? Why yes, off course we can. In fact, we can have complex queries joining multiple tables as we see fit.", "Bonus Tip: This query is submitted to the database layer and so, downstream Map Reduce or Spark processing is saved if you can filter the required data at this layer itself. But, this isn\u2019t a best practice. The idea of a data lake is to get all the raw data in first.", "Loading from OLTP databases in daily batches requires the ability to perform incremental load, i.e. loading only the delta from the previous execution. There are essentially two ways this can be achieved:", "Now, for incremental import in \u201clastmodified\u201d mode, we have to provide the following parameters: \u2014 incremental \u2014 To instantiate incremental extraction process and it\u2019s mode \u2014 check-column \u2014 The date column from which the date should be checked \u2014 last-value \u2014 The value post which the data should be picked \u2014 append \u2014 Appending the new data", "Now, for incremental import in \u201cappend\u201d mode, we have to provide the following parameters: \u2014 incremental \u2014 To instantiate incremental extraction process and it\u2019s mode \u2014 check-column \u2014 The column for which the value should be checked \u2014 last-value \u2014 The value post which the data should be picked", "Sqoop doesn\u2019t create a table automatically, hence we\u2019ve to create one with its underlying structure i.e. columns and data types.", "Please note the export command can also be run with multiple mappers. Just as for import commands, it is best to limit number of mappers = number of hosts.", "Mapper and Reducer tasks of MapReduce jobs are run by YARN\u2019s Resource Manager (RM) in 2 separate containers distributed across several nodes. But, if your dataset is small or your job contains small mapper tasks or your job contains just 1 reducer task \u2014 we can set the Uber mode to TRUE. This forces the RM to run the mapper and reducer task sequentially in one single container or JVM, thus reducing overhead of launching new containers and of networking across multiple nodes for a small job. The job completes faster.", "For more commands, perhaps you\u2019d like to visit Apache Sqoop\u2019s user guide", "[1] Sqoop User Guide, Apache Sqoop, ASF", "[2] Managing the Sqoop Client, Apache Sqoop, Cloudera Documentation", "[4] A. Elmahrek, J. Cecho (2014), Sqoop2 New Generation of Big Data \u2014 Apache Sqoop Workflow, Apache Software Foundation", "Please note, I didn\u2019t download all of the columns to create the dataset. You can though.", "The structured cleansed dataset can be found here. Explanation with some data? Here you go.The Homepage of the original data set \u2014 here. The raw data \u2014 here. More details on the fields \u2014 here.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Tech Enthusiast \u2014 Data Engineering | Data Analytics | LinkedIN: https://www.linkedin.com/in/prathameshnimkar/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F1113ce453639&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fapache-sqoop-1113ce453639&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fapache-sqoop-1113ce453639&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fapache-sqoop-1113ce453639&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fapache-sqoop-1113ce453639&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----1113ce453639--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1113ce453639--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@prathamesh.nimkar?source=post_page-----1113ce453639--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@prathamesh.nimkar?source=post_page-----1113ce453639--------------------------------", "anchor_text": "Prathamesh Nimkar"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6f0e324a756e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fapache-sqoop-1113ce453639&user=Prathamesh+Nimkar&userId=6f0e324a756e&source=post_page-6f0e324a756e----1113ce453639---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1113ce453639&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fapache-sqoop-1113ce453639&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1113ce453639&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fapache-sqoop-1113ce453639&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://cwiki.apache.org/confluence/display/SQOOP/Sqoop+Presentations+and+Blogs", "anchor_text": "Apache Sqoop Wiki"}, {"url": "https://medium.com/@prathamesh.nimkar/cloudera-manager-on-google-cloud-3da9b4d64d74", "anchor_text": "Cloudera Manager on Google CloudCM 6.3.1 on GCPmedium.com"}, {"url": "https://medium.com/@prathamesh.nimkar/install-mysql-database-7d64f0207cf9", "anchor_text": "Install MySQL databaseInstalling MySQL database and loading the datamedium.com"}, {"url": "https://docs.cloudera.com/documentation/enterprise/5-4-x/topics/cdh_ig_jdbc_driver_install.html", "anchor_text": "Cloudera says that Sqoop doesn\u2019t ship with third-party JDBC drivers and must be installed separately"}, {"url": "https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-8.0.19-1.el7.noarch.rpm", "anchor_text": "https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-8.0.19-1.el7.noarch.rpm"}, {"url": "https://medium.com/@prathamesh.nimkar/hdfs-commands-b8a745ca9318", "anchor_text": "HDFS\u2019 Name Node Web UI"}, {"url": "http://instance-1:8889/", "anchor_text": "http://instance-1:8889/"}, {"url": "https://sqoop.apache.org/docs/1.4.6/SqoopUserGuide.html", "anchor_text": "Apache Sqoop\u2019s user guide"}, {"url": "https://sqoop.apache.org/docs/1.4.6/SqoopUserGuide.html", "anchor_text": "Sqoop User Guide"}, {"url": "https://docs.cloudera.com/documentation/enterprise/6/6.3/topics/cm_mc_sqoop1_client.html", "anchor_text": "Managing the Sqoop Client"}, {"url": "https://community.cloudera.com/t5/Support-Questions/What-is-Uber-mode/td-p/211160", "anchor_text": "What is Uber Mode?"}, {"url": "https://cwiki.apache.org/confluence/display/SQOOP/Sqoop+Presentations+and+Blogs", "anchor_text": "Sqoop2 New Generation of Big Data \u2014 Apache Sqoop Workflow"}, {"url": "https://github.com/pratnimk/sqoop-big-data-analysis/raw/master/915662529_T_ONTIME_REPORTING.zip", "anchor_text": "here"}, {"url": "https://medium.com/@prathamesh.nimkar/bb6dc35c1a06?source=friends_link&sk=af5eab7956455ae93784c9a0ee8a9fb8", "anchor_text": "Here you go"}, {"url": "https://www.bts.gov/", "anchor_text": "here"}, {"url": "https://www.transtats.bts.gov/DL_SelectFields.asp?Table_ID=236&DB_Short_Name=On-Time", "anchor_text": "here"}, {"url": "https://www.transtats.bts.gov/Fields.asp?Table_ID=236", "anchor_text": "here"}, {"url": "https://towardsdatascience.com/apache-yarn-zookeeper-61e17a958215", "anchor_text": "Apache YARN & ZookeeperAll about Resource Allocation and High Availabilitytowardsdatascience.com"}, {"url": "https://towardsdatascience.com/simplifying-hdfs-erasure-coding-9d9588975113", "anchor_text": "HDFS Erasure CodingReduce storage overhead significantly in your HDFS cluster by leveraging Erasure Codingtowardsdatascience.com"}, {"url": "https://medium.com/@prathamesh.nimkar/big-data-analytics-using-the-hadoop-ecosystem-411d629084d3", "anchor_text": "Big Data Analytics Pipeline using the Hadoop EcosystemLanding Pagemedium.com"}, {"url": "https://medium.com/tag/big-data?source=post_page-----1113ce453639---------------big_data-----------------", "anchor_text": "Big Data"}, {"url": "https://medium.com/tag/data-science?source=post_page-----1113ce453639---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/apache?source=post_page-----1113ce453639---------------apache-----------------", "anchor_text": "Apache"}, {"url": "https://medium.com/tag/hadoop?source=post_page-----1113ce453639---------------hadoop-----------------", "anchor_text": "Hadoop"}, {"url": "https://medium.com/tag/sql?source=post_page-----1113ce453639---------------sql-----------------", "anchor_text": "Sql"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1113ce453639&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fapache-sqoop-1113ce453639&user=Prathamesh+Nimkar&userId=6f0e324a756e&source=-----1113ce453639---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1113ce453639&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fapache-sqoop-1113ce453639&user=Prathamesh+Nimkar&userId=6f0e324a756e&source=-----1113ce453639---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1113ce453639&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fapache-sqoop-1113ce453639&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1113ce453639--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F1113ce453639&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fapache-sqoop-1113ce453639&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----1113ce453639---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----1113ce453639--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----1113ce453639--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----1113ce453639--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----1113ce453639--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----1113ce453639--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----1113ce453639--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----1113ce453639--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----1113ce453639--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@prathamesh.nimkar?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@prathamesh.nimkar?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Prathamesh Nimkar"}, {"url": "https://medium.com/@prathamesh.nimkar/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "186 Followers"}, {"url": "https://www.linkedin.com/in/prathameshnimkar/", "anchor_text": "https://www.linkedin.com/in/prathameshnimkar/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6f0e324a756e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fapache-sqoop-1113ce453639&user=Prathamesh+Nimkar&userId=6f0e324a756e&source=post_page-6f0e324a756e--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F4018850c89c8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fapache-sqoop-1113ce453639&newsletterV3=6f0e324a756e&newsletterV3Id=4018850c89c8&user=Prathamesh+Nimkar&userId=6f0e324a756e&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}