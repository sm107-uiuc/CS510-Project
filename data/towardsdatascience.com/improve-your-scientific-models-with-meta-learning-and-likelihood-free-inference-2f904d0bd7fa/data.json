{"url": "https://towardsdatascience.com/improve-your-scientific-models-with-meta-learning-and-likelihood-free-inference-2f904d0bd7fa", "time": 1682994266.972092, "path": "towardsdatascience.com/improve-your-scientific-models-with-meta-learning-and-likelihood-free-inference-2f904d0bd7fa/", "webpage": {"metadata": {"title": "Improve your Scientific Models with Meta-Learning and Likelihood-Free Inference | by Arthur Pesah | Towards Data Science", "h1": "Improve your Scientific Models with Meta-Learning and Likelihood-Free Inference", "description": "The role of statistics and machine learning in science is usually related to the second kind of inference, also called induction. Imagine for instance that you want to model the evolution of two\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/u/5808df5ce3e6?source=post_page-----2f904d0bd7fa--------------------------------", "anchor_text": "Antoine Wehenkel", "paragraph_index": 0}, {"url": "https://en.wikipedia.org/wiki/Lotka\u2013Volterra_equations", "anchor_text": "Lotka-Volterra differential equation", "paragraph_index": 3}, {"url": "https://towardsdatascience.com/probability-concepts-explained-maximum-likelihood-estimation-c7b4342fdbb1", "anchor_text": "this excellent introduction to the subject", "paragraph_index": 12}, {"url": "https://arxiv.org/abs/1606.04474", "anchor_text": "Learning to learn by gradient descent by gradient descent", "paragraph_index": 20}, {"url": "https://arxiv.org/abs/1811.12932", "anchor_text": "Recurrent Machines for Likelihood-Free Inference", "paragraph_index": 36}, {"url": "http://jmlr.org/papers/v15/wierstra14a.html", "anchor_text": "Natural Evolution Strategies", "paragraph_index": 37}], "all_paragraphs": ["Article jointly written by Arthur Pesah and Antoine Wehenkel", "There are usually two ways of coming up with a new scientific theory:", "The role of statistics and machine learning in science is usually related to the second kind of inference, also called induction.", "Imagine for instance that you want to model the evolution of two populations (let\u2019s say foxes and rabbits) in an environment. A simple model is the Lotka-Volterra differential equation: you consider the probability that an event such as \u201ca fox eating a rabbit\u201d, \u201ca rabbit being born\u201d, \u201ca fox being born\u201d, etc. happens in a small time interval, deduce a set of differential equations depending on those probabilities, and predict the evolution of the two animal populations by solving those equations. By comparing your prediction with the evolution of a real population, you can infer the best model parameters (probabilities) in this environment.", "Modern theories require simulations in order to be linked to observations. It can be either a simple differential equation solver as in the case of the Lotka-Volterra model, or a complex Monte-Carlo simulator as they use in particle physics for instance.", "By comparing the results of a simulation, i.e. the predictions of a model, with real data, it is then possible to know the correctness of your model and adjust it accordingly. If this process of going back and forth between the model and the experimental data is usually done manually, the question that any machine learning practitioner would ask is: can we do it automatically? Can we build a machine that takes a tweakable simulator and real data as input, and returns the version of the simulator that fits best some real data?", "That\u2019s the object of our recent work [1], where we trained a neural network to come up with the best sequence of simulator tweaks in order to approximate experimental data, capitalizing on the recent advances in the fields of likelihood-free inference and meta-learning.", "Let\u2019s rephrase our problem in a more formal way. We can model a simulator (also called generative model) by a stochastic function that takes some parameters \u03b8 and returns samples x drawn from a certain distribution (the so-called model).", "This formalism applies to any scientific theory that includes randomness, as it\u2019s very often the case in modern science (particle collisions are governed by the law of quantum physics which are intrinsically random, biological processes or chemical reactions often occur in a noisy environment, etc.).", "Experimental data consist in a set of points living in the same space as the output of the simulator. The goal of inference is then to find the parameters \u03b8 such that the simulator generate points as close as possible to the real data.", "Scientific fields using such simulators include:", "So how can we predict the parameters of a simulator given some real observations? Let\u2019s consider the simple example of a Gaussian simulator, that takes a vector \u03b8=(\u03bc,\u03c3) as parameters and returns samples from the Gaussian distribution \ud835\udcdd(\u03bc,\u03c3).", "The classical way to infer the parameters of such a simulator is called Maximum Likelihood Estimation (MLE). The likelihood of real data points X under a probability distribution P parametrized by \u03b8 (the simulator) is defined as P(X|\u03b8). It means that if most data points are located in high density regions, the likelihood will be high. Hence, the best parameters of a model are often the ones that maximize the likelihood of real data. If you are unfamiliar with likelihood-based inference, you can read this excellent introduction to the subject.", "If you have explicitly access to the underlying probability distribution of your simulator, as well as the gradient of this distribution, you can for instance perform a gradient descent in the parameter space, in order to maximize the likelihood and infer the best parameters of your model.", "However, many real-life simulators have an intractable likelihood, which means that the explicit probability distribution is too hard to compute (either analytically or numerically) . We must therefore find new ways to infer the optimal parameters without using neither the likelihood function nor its gradient.", "To sum it up, we have a black-box stochastic simulator that takes parameters and generates samples from an unknown probability distribution, as well as real data that we are able to compare to the generated ones. Our goal is to find the parameters that lead the simulator to generate data as close as possible to the real ones. This setting is called likelihood-free inference.", "Let\u2019s try to come up progressively with a method to solve our problem. The first thing we can do is to start from a random parameter, and simulate the corresponding data:", "By doing so, we can see how far our generated data are from the real data, but we have no way to know where to move in the parameter-space. Instead, let\u2019s simulate several parameters:", "To do so, we consider a distribution in the parameter-space, called proposal distribution and noted q(\u03b8|\u03c8). If we choose q to be a Gaussian distribution, we will have \u03c8=(\u03bc,\u03c3). The first step consists in initializing \u03c8 randomly. In the figure above, we considered \u03c8=\u03bc for simplicity. Then, we can perform the following step until convergence:", "The third step is the hard one. Intuitively, you would like to move \u03c8 towards the orange and green parameters, since the corresponding predictions (orange and green cloud of points) are the closest to the real data. A set of methods, called natural evolution strategies [3], allow you to link the performance of each \u03b8 (in terms of similarity between its predictions and the real data) to a direction in the parameter space. A recent paper [4] use for instance the similarity measure given by a Generative Adversarial Network (GAN) to find the best direction. Even though those algorithms perform well in the general case, one can wonder if for a given simulator, it is not possible to find a better algorithm that would exploit the particular properties of this simulator. That\u2019s where meta-learning comes into play!", "The idea behind meta-learning is to learn how to learn, and in our case to learn the optimization process. The main idea, introduced in the paper Learning to learn by gradient descent by gradient descent [2], is to use a recurrent neural network (RNN) to find the best descent direction at each iteration. Below is an example of sequence of points produced by a randomly initialized RNN:", "Each descent direction is random and the last point produced is far from the minimum. During training, it should learn to exploit the gradient information at each point in order to move toward the minimum, giving something like:", "So how to train it? Generate many functions whose minimum you know, and ask the RNN to minimize the distance between the last point of the sequence and the real minimum.", "In the case of likelihood-free inference, the RNN should return a sequence of proposal parameters \u03c8, given the real observations and the generated cloud of points at each step.", "Now, same question as for learning to learn by gradient descent, how do we train the RNN? Here, the trick is to generate many random parameters \u03b8 and to pretend for each one that it is the \u201ctrue parameter\u201d. We can then simulate each \u03b8 generated and obtain a set of \u201creal observations\u201d. The RNN can then be trained by passing it those real observations, looking at its final proposal distribution, and comparing it to the true parameter (that we know because we have generated it).", "Let\u2019s go through an example to clarify all of that. In the figures below, the proposal distribution is represented in color (red = high density). At the beginning, the RNN is initialized randomly and we evaluate it on our first generated true parameter \u03b8 (in red).", "We can then backpropagate the loss into the RNN. After repeating this process for 200 different \u201ctrue parameters\u201d, this is what is should look like:", "We can see that it has learnt to exploit the information of the observation space to move towards the good parameter.", "We evaluated our model on different toy simulators, some where we the likelihood is known and some where it is not.", "The first simulator takes a parameter \u03bb and draw samples from a Poisson distribution P(\u03bb). The goal of this example was to see if we obtain comparable performance as the maximum likelihood estimator. Here are the results:", "We can see that the performance are comparable, even though we didn\u2019t give our model access to the likelihood.", "To evaluate our model in a true likelihood-free setting, we considered a simplified particle physics model that simulate the collision of an electron and a positron turning into a muon and an antimuon.", "The parameters of the simulator are the energy of the incoming particles and the Fermi constant, and the output is the angle between the two muons.", "To evaluate our method, we compared the real observations with the ones generated by the last parameter found. Here are the results:", "We saw what likelihood-free inference is and how meta-learning can be used to solve it by learning the best sequence of simulator tweaks to fit a model to the reality.", "As most meta-learning models, a limitation is that it is hard to train. We had trouble scaling our method to more complex simulators, since meta-training requires a lot of simulator calls, which might be very slow in real-world settings. However, as the field of meta-learning makes progress, we hope that new methods will emerge to alleviate this problem and make it more scalable.", "[1] A. Pesah, A. Wehenkel and G. Louppe, Recurrent Machines for Likelihood-Free Inference (2018), NeurIPS 2018 Workshop on Meta-Learning", "[3] D. Wierstra, T. Schaul, T. Glasmachers, Y. Sun, J. Peter, J. Schmidhuber, Natural Evolution Strategies (2014), Journal of Machine Learning Research (JMLR).", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Master's student in theoretical physics at KTH. Machine learning enthusiast, physics and maths lover, computer geek."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F2f904d0bd7fa&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimprove-your-scientific-models-with-meta-learning-and-likelihood-free-inference-2f904d0bd7fa&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimprove-your-scientific-models-with-meta-learning-and-likelihood-free-inference-2f904d0bd7fa&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimprove-your-scientific-models-with-meta-learning-and-likelihood-free-inference-2f904d0bd7fa&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimprove-your-scientific-models-with-meta-learning-and-likelihood-free-inference-2f904d0bd7fa&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----2f904d0bd7fa--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2f904d0bd7fa--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@arthur.pesah?source=post_page-----2f904d0bd7fa--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@arthur.pesah?source=post_page-----2f904d0bd7fa--------------------------------", "anchor_text": "Arthur Pesah"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc66e091abf91&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimprove-your-scientific-models-with-meta-learning-and-likelihood-free-inference-2f904d0bd7fa&user=Arthur+Pesah&userId=c66e091abf91&source=post_page-c66e091abf91----2f904d0bd7fa---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2f904d0bd7fa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimprove-your-scientific-models-with-meta-learning-and-likelihood-free-inference-2f904d0bd7fa&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2f904d0bd7fa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimprove-your-scientific-models-with-meta-learning-and-likelihood-free-inference-2f904d0bd7fa&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://arxiv.org/abs/1811.12932", "anchor_text": "Recurrent Machines for Likelihood-Free Inference"}, {"url": "http://metalearning.ml", "anchor_text": "NeurIPS 2018 Workshop on Meta-Learning"}, {"url": "https://medium.com/u/5808df5ce3e6?source=post_page-----2f904d0bd7fa--------------------------------", "anchor_text": "Antoine Wehenkel"}, {"url": "https://en.wikipedia.org/wiki/Lotka\u2013Volterra_equations", "anchor_text": "Lotka-Volterra differential equation"}, {"url": "https://en.wikipedia.org/wiki/Coalescent_theory", "anchor_text": "Coalescent theory"}, {"url": "https://en.wikipedia.org/wiki/Standard_Model", "anchor_text": "Standard Model"}, {"url": "https://en.wikipedia.org/wiki/Hodgkin%E2%80%93Huxley_model", "anchor_text": "Hodgkin-Huxley model"}, {"url": "https://towardsdatascience.com/probability-concepts-explained-maximum-likelihood-estimation-c7b4342fdbb1", "anchor_text": "this excellent introduction to the subject"}, {"url": "https://arxiv.org/abs/1606.04474", "anchor_text": "Learning to learn by gradient descent by gradient descent"}, {"url": "https://arxiv.org/abs/1811.12932", "anchor_text": "Recurrent Machines for Likelihood-Free Inference"}, {"url": "https://arxiv.org/abs/1606.04474", "anchor_text": "Learning to learn gradient descent by gradient descent"}, {"url": "http://jmlr.org/papers/v15/wierstra14a.html", "anchor_text": "Natural Evolution Strategies"}, {"url": "https://arxiv.org/abs/1707.07113", "anchor_text": "Adversarial Variational Optimization of Non-Differentiable Simulator"}, {"url": "https://www.lhc-france.fr/spip.php?article870", "anchor_text": "LHC France"}, {"url": "https://www.wallpaperflare.com/fox-eating-rabbit-vector-art-animals-minimalism-representation-wallpaper-qodee", "anchor_text": "Fox eating rabbit"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----2f904d0bd7fa---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/science?source=post_page-----2f904d0bd7fa---------------science-----------------", "anchor_text": "Science"}, {"url": "https://medium.com/tag/physics?source=post_page-----2f904d0bd7fa---------------physics-----------------", "anchor_text": "Physics"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----2f904d0bd7fa---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/meta-learning?source=post_page-----2f904d0bd7fa---------------meta_learning-----------------", "anchor_text": "Meta Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2f904d0bd7fa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimprove-your-scientific-models-with-meta-learning-and-likelihood-free-inference-2f904d0bd7fa&user=Arthur+Pesah&userId=c66e091abf91&source=-----2f904d0bd7fa---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2f904d0bd7fa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimprove-your-scientific-models-with-meta-learning-and-likelihood-free-inference-2f904d0bd7fa&user=Arthur+Pesah&userId=c66e091abf91&source=-----2f904d0bd7fa---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2f904d0bd7fa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimprove-your-scientific-models-with-meta-learning-and-likelihood-free-inference-2f904d0bd7fa&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2f904d0bd7fa--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F2f904d0bd7fa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimprove-your-scientific-models-with-meta-learning-and-likelihood-free-inference-2f904d0bd7fa&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----2f904d0bd7fa---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----2f904d0bd7fa--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----2f904d0bd7fa--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----2f904d0bd7fa--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----2f904d0bd7fa--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----2f904d0bd7fa--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----2f904d0bd7fa--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----2f904d0bd7fa--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----2f904d0bd7fa--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@arthur.pesah?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@arthur.pesah?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Arthur Pesah"}, {"url": "https://medium.com/@arthur.pesah/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "604 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc66e091abf91&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimprove-your-scientific-models-with-meta-learning-and-likelihood-free-inference-2f904d0bd7fa&user=Arthur+Pesah&userId=c66e091abf91&source=post_page-c66e091abf91--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Fc66e091abf91%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimprove-your-scientific-models-with-meta-learning-and-likelihood-free-inference-2f904d0bd7fa&user=Arthur+Pesah&userId=c66e091abf91&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}