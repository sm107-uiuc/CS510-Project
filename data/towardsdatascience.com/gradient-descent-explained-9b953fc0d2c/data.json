{"url": "https://towardsdatascience.com/gradient-descent-explained-9b953fc0d2c", "time": 1683007842.3309171, "path": "towardsdatascience.com/gradient-descent-explained-9b953fc0d2c/", "webpage": {"metadata": {"title": "Gradient Descent Explained. A comprehensive guide to Gradient\u2026 | by Daksh Trehan | Towards Data Science", "h1": "Gradient Descent Explained", "description": "Gradient Descent is an optimizing algorithm used in Machine/ Deep Learning algorithms. Gradient Descent with Momentum and Nesterov Accelerated Gradient Descent are advanced versions of Gradient Descent. Stochastic GD, Batch GD, Mini-Batch GD is also discussed in this article."}, "outgoing_paragraph_urls": [{"url": "https://mailchi.mp/b535943b5fff/daksh-trehan-weekly-newsletter", "anchor_text": "Daksh Trehan\u2019s Weekly Newsletter", "paragraph_index": 31}, {"url": "https://towardsdatascience.com/gradient-descent-algorithm-and-its-variants-10f652806a3", "anchor_text": "Gradient Descent Algorithm and Its Variants", "paragraph_index": 35}, {"url": "https://towardsdatascience.com/@ImadPhd?source=post_page-----10f652806a3----------------------", "anchor_text": "Imad Dabbura", "paragraph_index": 35}, {"url": "https://towardsdatascience.com/learning-parameters-part-2-a190bef2d12", "anchor_text": "Learning Parameters, Part 2: Momentum-Based & Nesterov Accelerated Gradient Descent", "paragraph_index": 36}, {"url": "https://ruder.io/optimizing-gradient-descent/index.html#momentum", "anchor_text": "An overview of gradient descent optimization algorithms", "paragraph_index": 37}, {"url": "https://towardsdatascience.com/understanding-the-mathematics-behind-gradient-descent-dde5dc9be06e", "anchor_text": "Understanding the Mathematics behind Gradient Descent", "paragraph_index": 38}, {"url": "https://padhai.onefourthlabs.in/courses/dl-feb-2019", "anchor_text": "Deep Learning", "paragraph_index": 39}, {"url": "https://medium.com/datadriveninvestor/determining-perfect-fit-for-your-ml-model-339459eef670", "anchor_text": "Determining perfect fit for your ML Model.", "paragraph_index": 41}, {"url": "https://medium.com/towards-artificial-intelligence/serving-data-science-to-a-rookie-b03af9ea99a2", "anchor_text": "Serving Data Science to a Rookie.", "paragraph_index": 42}, {"url": "https://levelup.gitconnected.com/relating-machine-learning-techniques-to-real-life-4dafd626fdff", "anchor_text": "Relating Machine Learning techniques to Real Life.", "paragraph_index": 43}], "all_paragraphs": ["Optimization refers to the task of minimizing/maximizing an objective function f(x) parameterized by x. In machine/deep learning terminology, it\u2019s the task of minimizing the cost/loss function J(w) parameterized by the model\u2019s parameters w \u2208 R^d. Optimization algorithms (in the case of minimization) have one of the following goals:", "Gradient Descent is an optimizing algorithm used in Machine/ Deep Learning algorithms. The goal of Gradient Descent is to minimize the objective convex function f(x) using iteration.", "For ease, let\u2019s take a simple linear model.", "A machine learning model always wants low error with maximum accuracy, in order to decrease error we will intuit our algorithm that you\u2019re doing something wrong that is needed to be rectified, that would be done through Gradient Descent.", "We need to minimize our error, in order to get pointer to minima we need to walk some steps that are known as alpha(learning rate).", "A derivative is a term that comes from calculus and is calculated as the slope of the graph at a particular point. The slope is described by drawing a tangent line to the graph at the point. So, if we are able to compute this tangent line, we might be able to compute the desired direction to reach the minima.", "Learning rate must be chosen wisely as:1. if it is too small, then the model will take some time to learn.2. if it is too large, model will converge as our pointer will shoot and we\u2019ll not be able to get to minima.", "Vanilla gradient descent, however, can\u2019t guarantee good convergence, due to following reasons:", "In simple words, every step we take towards minima tends to decrease our slope, now if we visualize, in steep region of curve derivative is going to be large therefore steps taken by our model too would be large but as we will enter gentle region of slope our derivative will decrease and so will the time to reach minima.", "If we consider, Simple Gradient Descent completely relies only on calculation i.e. if there are 10000 steps, then our model would try to implement Simple Gradient Descent for 10000 times that would be obviously too much time consuming and computationally expensive.", "In laymen language, suppose a man is walking towards his home but he don\u2019t know the way so he ask for direction from by passer, now we expect him to walk some distance and then ask for direction but man is asking for direction at every step he takes, that is obviously more time consuming, now compare man with Simple Gradient Descent and his goal with minima.", "In order to avoid drawbacks of vanilla Gradient Descent, we introduced momentum based Gradient Descent where the goal is to lower the computation time and that can be achieved when we introduce the concept of experience i.e. the confidence using previous steps.", "Pseudocode for momentum based Gradient Descent:", "In this way rather than computing new steps again and again we are averaging the decay and as decay increases its effect in decision making decreases and thus the older the step less effect on decision making.More the history more bigger steps will be taken.", "Even in the gentle region, momentum based Gradient Descent is taking large steps due to the momentum it is burdening along.", "But due to larger steps it overshoots its goal by longer distance as it oscillate around minima due to steep slope, but despite such hurdles it is faster than vanilla Gradient Descent.", "In simple words, suppose a man want to reach destination that is 1200m far and he doesn\u2019t know the path, so he decided that after every 250m he will ask for direction, now if he asked direction for 5 times he would\u2019ve travelled 1250m that\u2019s he has already passed his goal and to achieve that goal he would need to trace his steps back. Similar is the case of Momentum based GD where due to high experience our model is taking larger steps that is leading to overshooting and hence missing the goal but to achieve minima model have to trace back its steps.", "To overcome the problems of momentum based Gradient Descent we use NAG, in this we move first and then compute gradient so that if our oscillations overshoots then it must be insignificant as compared to that of Momentum Based Gradient Descent.", "Nesterov accelerated Gradient(NAG) is a way to provide history to our momentum. We can now adequately look forward by computing the angle not w.r.t. to our present parameters \u03b8.", "In this, learning happens on every example:", "a. Easy to fit in memoryb. Computationally fastc. Efficient for large dataset", "a. Due to frequent updates steps taken towards minima are very noisy.b. Noise can make it large to wait.c. Frequent updates are computationally expensive.", "It is a greedy approach where we have to sum over all examples for each update.", "a. Less noisy stepsb. produces stable GD convergence.c. Computationally efficient as all resources aren\u2019t used for single sample but rather for all training samples", "a. Additional memory might be needed.b. It can take long to process large database.c. Approximate gradients", "Instead of going over all examples, Mini-batch Gradient Descent sums up over lower number of examples based on the batch size.", "It is sum of both Batch Gradient Descent and Stochastic Gradient Descent.", "a. Easy fit in memory.b. Computationally efficient.c. Stable error go and convergence.", "In case of sparse data, we would experience sparse ON(1) features and more frequent OFF(0) features, now, most of the time gradient update will be NULL as derivative is zero in most cases and when it will be one, the steps would be too small to reach minima.", "For frequent features we require low learning rate, but for high features we require high learning rate.", "So, in order to boost our model for sparse nature data, we need to chose adaptive learning rate.", "If you like this article, please consider subscribing to my newsletter: Daksh Trehan\u2019s Weekly Newsletter.", "Hopefully, this article has not only increased your understanding of Gradient Descent but also made you realize machine learning is not difficult and is already happening in your daily life.", "As always, thank you so much for reading, and please share this article if you found it useful! :)", "To know more about parameters optimization techniques, follow :-", "[1] Gradient Descent Algorithm and Its Variants by Imad Dabbura", "[2] Learning Parameters, Part 2: Momentum-Based & Nesterov Accelerated Gradient Descent by Akshay L Chandra", "[3] An overview of gradient descent optimization algorithms by Sebastian Ruder", "[4] Understanding the Mathematics behind Gradient Descent by Parul Pandey.", "[5] Deep Learning (padhAI) by Dr. Mitesh Khapra and Dr. Pratyush Kumar", "The cover template is designed by me on canva.com, the source is mentioned on every visual, and the un-mentioned visuals are from my notebook.", "Determining perfect fit for your ML Model.", "Serving Data Science to a Rookie.", "Relating Machine Learning techniques to Real Life.", "Follow for further Machine Learning/ Deep Learning blogs.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F9b953fc0d2c&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-explained-9b953fc0d2c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-explained-9b953fc0d2c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-explained-9b953fc0d2c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-explained-9b953fc0d2c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----9b953fc0d2c--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----9b953fc0d2c--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://dakshtrehan.medium.com/?source=post_page-----9b953fc0d2c--------------------------------", "anchor_text": ""}, {"url": "https://dakshtrehan.medium.com/?source=post_page-----9b953fc0d2c--------------------------------", "anchor_text": "Daksh Trehan"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7c111171a79b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-explained-9b953fc0d2c&user=Daksh+Trehan&userId=7c111171a79b&source=post_page-7c111171a79b----9b953fc0d2c---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9b953fc0d2c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-explained-9b953fc0d2c&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9b953fc0d2c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-explained-9b953fc0d2c&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://algorithmia.com/blog/introduction-to-optimizers", "anchor_text": "Source"}, {"url": "https://cs231n.github.io/neural-networks-3/", "anchor_text": "Source"}, {"url": "https://padhai.onefourthlabs.in/courses/dl-feb-2019", "anchor_text": "Source"}, {"url": "https://nhannguyen95.github.io/coursera-deep-learning-course-2-week-2/", "anchor_text": "Source"}, {"url": "https://stats.stackexchange.com/questions/179915/whats-the-difference-between-momentum-based-gradient-descent-and-nesterovs-acc", "anchor_text": "Source"}, {"url": "https://stats.stackexchange.com/questions/179915/whats-the-difference-between-momentum-based-gradient-descent-and-nesterovs-acc", "anchor_text": "Source"}, {"url": "https://datascience.stackexchange.com/questions/52884/possible-for-batch-size-of-neural-network-to-be-too-small", "anchor_text": "Source"}, {"url": "https://mailchi.mp/b535943b5fff/daksh-trehan-weekly-newsletter", "anchor_text": "Daksh Trehan\u2019s Weekly Newsletter"}, {"url": "https://towardsdatascience.com/parameters-optimization-explained-876561853de0", "anchor_text": "Parameters Optimization ExplainedA brief yet descriptive guide to Gradient Descent, ADAM, ADAGRAD, RMSProptowardsdatascience.com"}, {"url": "https://towardsdatascience.com/gradient-descent-algorithm-and-its-variants-10f652806a3", "anchor_text": "Gradient Descent Algorithm and Its Variants"}, {"url": "https://towardsdatascience.com/@ImadPhd?source=post_page-----10f652806a3----------------------", "anchor_text": "Imad Dabbura"}, {"url": "https://towardsdatascience.com/learning-parameters-part-2-a190bef2d12", "anchor_text": "Learning Parameters, Part 2: Momentum-Based & Nesterov Accelerated Gradient Descent"}, {"url": "https://ruder.io/optimizing-gradient-descent/index.html#momentum", "anchor_text": "An overview of gradient descent optimization algorithms"}, {"url": "https://towardsdatascience.com/understanding-the-mathematics-behind-gradient-descent-dde5dc9be06e", "anchor_text": "Understanding the Mathematics behind Gradient Descent"}, {"url": "https://padhai.onefourthlabs.in/courses/dl-feb-2019", "anchor_text": "Deep Learning"}, {"url": "http://www.dakshtrehan.com", "anchor_text": "www.dakshtrehan.com"}, {"url": "https://www.linkedin.com/in/dakshtrehan/", "anchor_text": "https://www.linkedin.com/in/dakshtrehan/"}, {"url": "https://www.instagram.com/_daksh_trehan_/", "anchor_text": "https://www.instagram.com/_daksh_trehan_/"}, {"url": "https://github.com/dakshtrehan", "anchor_text": "https://github.com/dakshtrehan"}, {"url": "https://towardsdatascience.com/detecting-covid-19-using-deep-learning-262956b6f981", "anchor_text": "Detecting COVID-19 using Deep Learning."}, {"url": "https://towardsdatascience.com/logistic-regression-explained-ef1d816ea85a", "anchor_text": "Logistic Regression Explained"}, {"url": "https://medium.com/towards-artificial-intelligence/linear-regression-explained-f5cc85ae2c5c", "anchor_text": "Linear Regression Explained"}, {"url": "https://medium.com/datadriveninvestor/determining-perfect-fit-for-your-ml-model-339459eef670", "anchor_text": "Determining perfect fit for your ML Model."}, {"url": "https://medium.com/towards-artificial-intelligence/serving-data-science-to-a-rookie-b03af9ea99a2", "anchor_text": "Serving Data Science to a Rookie."}, {"url": "https://levelup.gitconnected.com/relating-machine-learning-techniques-to-real-life-4dafd626fdff", "anchor_text": "Relating Machine Learning techniques to Real Life."}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----9b953fc0d2c---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----9b953fc0d2c---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/gradient-descent?source=post_page-----9b953fc0d2c---------------gradient_descent-----------------", "anchor_text": "Gradient Descent"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----9b953fc0d2c---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/optimization?source=post_page-----9b953fc0d2c---------------optimization-----------------", "anchor_text": "Optimization"}, {"url": "http://creativecommons.org/publicdomain/zero/1.0/", "anchor_text": "No rights reserved"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9b953fc0d2c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-explained-9b953fc0d2c&user=Daksh+Trehan&userId=7c111171a79b&source=-----9b953fc0d2c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9b953fc0d2c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-explained-9b953fc0d2c&user=Daksh+Trehan&userId=7c111171a79b&source=-----9b953fc0d2c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9b953fc0d2c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-explained-9b953fc0d2c&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----9b953fc0d2c--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F9b953fc0d2c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-explained-9b953fc0d2c&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----9b953fc0d2c---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----9b953fc0d2c--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----9b953fc0d2c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----9b953fc0d2c--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----9b953fc0d2c--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----9b953fc0d2c--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----9b953fc0d2c--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----9b953fc0d2c--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----9b953fc0d2c--------------------------------", "anchor_text": ""}, {"url": "https://dakshtrehan.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://dakshtrehan.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Daksh Trehan"}, {"url": "https://dakshtrehan.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "341 Followers"}, {"url": "http://www.dakshtrehan.com", "anchor_text": "www.dakshtrehan.com"}, {"url": "http://www.linkedin.com/in/dakshtrehan", "anchor_text": "www.linkedin.com/in/dakshtrehan"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7c111171a79b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-explained-9b953fc0d2c&user=Daksh+Trehan&userId=7c111171a79b&source=post_page-7c111171a79b--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fe1e144c9162&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-explained-9b953fc0d2c&newsletterV3=7c111171a79b&newsletterV3Id=e1e144c9162&user=Daksh+Trehan&userId=7c111171a79b&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}