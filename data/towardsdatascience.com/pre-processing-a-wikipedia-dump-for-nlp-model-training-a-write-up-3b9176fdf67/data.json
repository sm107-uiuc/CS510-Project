{"url": "https://towardsdatascience.com/pre-processing-a-wikipedia-dump-for-nlp-model-training-a-write-up-3b9176fdf67", "time": 1683002023.656321, "path": "towardsdatascience.com/pre-processing-a-wikipedia-dump-for-nlp-model-training-a-write-up-3b9176fdf67/", "webpage": {"metadata": {"title": "Pre-processing a Wikipedia dump for NLP model training \u2014 a write-up | by Steven van de Graaf | Towards Data Science", "h1": "Pre-processing a Wikipedia dump for NLP model training \u2014 a write-up", "description": "Wikipedia dumps are used frequently in modern NLP research for model training, especially with transformers like BERT, RoBERTa, XLNet, XLM, etc. As such, for any aspiring NLP researcher intent on\u2026"}, "outgoing_paragraph_urls": [{"url": "https://dumps.wikimedia.org/", "anchor_text": "Wikipedia dumps", "paragraph_index": 0}, {"url": "https://dumps.wikimedia.org/enwiki/latest/", "anchor_text": "here", "paragraph_index": 1}, {"url": "https://github.com/attardi/wikiextractor/", "anchor_text": "WikiExtractor,", "paragraph_index": 4}, {"url": "https://github.com/microsoft/BlingFire", "anchor_text": "BlingFire tokenizer", "paragraph_index": 6}], "all_paragraphs": ["Wikipedia dumps are used frequently in modern NLP research for model training, especially with transformers like BERT, RoBERTa, XLNet, XLM, etc. As such, for any aspiring NLP researcher intent on getting to grips with models like these themselves, this write-up presents a complete picture (and code) of everything involved in downloading, extracting, cleaning and pre-processing a Wikipedia dump.", "Wikipedia dumps are freely available in multiple formats in many languages. For the English language Wikipedia, a full list of all available formats of the latest dump can be found here.", "As we\u2019re primarily interested in text data, for the purposes of this write-up, we\u2019ll download such a dump (that contains solely pages and articles) in a compressed XML-format, using the code below:", "To download the latest Wikipedia dump for the English language, for example, simply run the following command in your terminal: ./download_wiki_dump.sh en", "The Wikipedia dump we\u2019ve just downloaded is not ready to be pre-processed (sentence-tokenized and one sentence-per-line) just yet. First, we need to extract and clean the dump, which can easily be accomplished with WikiExtractor, using the code below:", "To extract and clean the Wikipedia dump we\u2019ve just downloaded, for example, simply run the following command in your terminal: ./extract_and_clean_wiki_dump.sh enwiki-latest-pages-articles.xml.bz2", "Now that we have successfully downloaded, extracted and cleaned a Wikipedia dump, we can begin to pre-process it. Practically, this means sentence-tokenizing the articles, as well as writing them one-sentence-per-line to a single text file, which can be accomplished using Microsoft\u2019s blazingly fast BlingFire tokenizer, using the code below:", "To pre-process the Wikipedia dump we\u2019ve just extracted and cleaned, for example, simply run the following command in your terminal: python3 preprocess_wiki_dump.py enwiki-latest-pages-articles.txt", "And that\u2019s it, you\u2019re done! \ud83d\ude4c You can now start experimenting with the latest and greatest in NLP yourself, using your freshly created Wikipedia corpus. \ud83e\udd17", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Graduate student in Artificial Intelligence @UvA_Amsterdam with multiple years of experience in Python and VBA development."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F3b9176fdf67&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpre-processing-a-wikipedia-dump-for-nlp-model-training-a-write-up-3b9176fdf67&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpre-processing-a-wikipedia-dump-for-nlp-model-training-a-write-up-3b9176fdf67&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpre-processing-a-wikipedia-dump-for-nlp-model-training-a-write-up-3b9176fdf67&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpre-processing-a-wikipedia-dump-for-nlp-model-training-a-write-up-3b9176fdf67&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----3b9176fdf67--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3b9176fdf67--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@sgraaf?source=post_page-----3b9176fdf67--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@sgraaf?source=post_page-----3b9176fdf67--------------------------------", "anchor_text": "Steven van de Graaf"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2853d5aeff85&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpre-processing-a-wikipedia-dump-for-nlp-model-training-a-write-up-3b9176fdf67&user=Steven+van+de+Graaf&userId=2853d5aeff85&source=post_page-2853d5aeff85----3b9176fdf67---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3b9176fdf67&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpre-processing-a-wikipedia-dump-for-nlp-model-training-a-write-up-3b9176fdf67&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3b9176fdf67&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpre-processing-a-wikipedia-dump-for-nlp-model-training-a-write-up-3b9176fdf67&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.wikiwand.com/en/Bert_(Sesame_Street)", "anchor_text": "Wikipedia entry for Bert"}, {"url": "https://dumps.wikimedia.org/", "anchor_text": "Wikipedia dumps"}, {"url": "https://dumps.wikimedia.org/enwiki/latest/", "anchor_text": "here"}, {"url": "https://github.com/attardi/wikiextractor/", "anchor_text": "WikiExtractor,"}, {"url": "https://github.com/microsoft/BlingFire", "anchor_text": "BlingFire tokenizer"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----3b9176fdf67---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/nlp?source=post_page-----3b9176fdf67---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/transformers?source=post_page-----3b9176fdf67---------------transformers-----------------", "anchor_text": "Transformers"}, {"url": "https://medium.com/tag/dataset?source=post_page-----3b9176fdf67---------------dataset-----------------", "anchor_text": "Dataset"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3b9176fdf67&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpre-processing-a-wikipedia-dump-for-nlp-model-training-a-write-up-3b9176fdf67&user=Steven+van+de+Graaf&userId=2853d5aeff85&source=-----3b9176fdf67---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3b9176fdf67&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpre-processing-a-wikipedia-dump-for-nlp-model-training-a-write-up-3b9176fdf67&user=Steven+van+de+Graaf&userId=2853d5aeff85&source=-----3b9176fdf67---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3b9176fdf67&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpre-processing-a-wikipedia-dump-for-nlp-model-training-a-write-up-3b9176fdf67&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3b9176fdf67--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F3b9176fdf67&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpre-processing-a-wikipedia-dump-for-nlp-model-training-a-write-up-3b9176fdf67&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----3b9176fdf67---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----3b9176fdf67--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----3b9176fdf67--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----3b9176fdf67--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----3b9176fdf67--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----3b9176fdf67--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----3b9176fdf67--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----3b9176fdf67--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----3b9176fdf67--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@sgraaf?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@sgraaf?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Steven van de Graaf"}, {"url": "https://medium.com/@sgraaf/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "41 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2853d5aeff85&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpre-processing-a-wikipedia-dump-for-nlp-model-training-a-write-up-3b9176fdf67&user=Steven+van+de+Graaf&userId=2853d5aeff85&source=post_page-2853d5aeff85--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F27561262aa3f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpre-processing-a-wikipedia-dump-for-nlp-model-training-a-write-up-3b9176fdf67&newsletterV3=2853d5aeff85&newsletterV3Id=27561262aa3f&user=Steven+van+de+Graaf&userId=2853d5aeff85&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}