{"url": "https://towardsdatascience.com/automating-doom-with-deep-q-learning-an-implementation-in-tensorflow-db03c1b03a9c", "time": 1683008499.154476, "path": "towardsdatascience.com/automating-doom-with-deep-q-learning-an-implementation-in-tensorflow-db03c1b03a9c/", "webpage": {"metadata": {"title": "Automating Doom with Deep Q-Learning: An Implementation in Tensorflow. | by Adrian Yijie Xu | Towards Data Science", "h1": "Automating Doom with Deep Q-Learning: An Implementation in Tensorflow.", "description": "Online learning methods are a dynamic family of reinforcement learning algorithms behind many of the achievements in general AI over the past decade. Belonging to the sample-based learning class of\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-automating-pong-in-using-a-policy-model-an-implementation-b71f64c158ff", "anchor_text": "sample-based learning", "paragraph_index": 0}, {"url": "https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-understanding-blackjack-strategy-through-monte-carlo-88c9b85194ed", "anchor_text": "offline counterparts", "paragraph_index": 0}, {"url": "https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-navigating-cliffworld-with-sarsa-and-q-learning-cc3c36eb5830", "anchor_text": "theory", "paragraph_index": 1}, {"url": "https://towardsdatascience.com/automating-pac-man-with-deep-q-learning-an-implementation-in-tensorflow-ca08e9891d9c", "anchor_text": "practical implementations", "paragraph_index": 1}, {"url": "https://towardsdatascience.com/automating-pac-man-with-deep-q-learning-an-implementation-in-tensorflow-ca08e9891d9c", "anchor_text": "past articles", "paragraph_index": 1}, {"url": "https://towardsdatascience.com/optimized-deep-q-learning-for-automated-atari-space-invaders-an-implementation-in-tensorflow-2-0-80352c744fdc", "anchor_text": "some", "paragraph_index": 2}, {"url": "https://towardsdatascience.com/automating-pac-man-with-deep-q-learning-an-implementation-in-tensorflow-ca08e9891d9c", "anchor_text": "previous", "paragraph_index": 2}, {"url": "https://github.com/shakenes/vizdoomgym", "anchor_text": "Vizdoomgym", "paragraph_index": 3}, {"url": "https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-markov-decision-processes-policies-value-functions-94f7389e1e82", "anchor_text": "Bellman Optimality Equations.", "paragraph_index": 7}, {"url": "https://github.com/EXJUSTICE/GradientCrescent", "anchor_text": "GradientCrescent Github.", "paragraph_index": 10}, {"url": "https://towardsdatascience.com/automating-pac-man-with-deep-q-learning-an-implementation-in-tensorflow-ca08e9891d9c", "anchor_text": "Atari", "paragraph_index": 10}, {"url": "https://towardsdatascience.com/optimized-deep-q-learning-for-automated-atari-space-invaders-an-implementation-in-tensorflow-2-0-80352c744fdc", "anchor_text": "environments", "paragraph_index": 10}, {"url": "https://arxiv.org/pdf/1605.02097.pdf", "anchor_text": "original Vizdoom paper", "paragraph_index": 36}, {"url": "https://medium.com/@adrianitsaxu", "anchor_text": "GradientCrescent", "paragraph_index": 41}, {"url": "https://github.com/EXJUSTICE/GradientCrescent", "anchor_text": "Github", "paragraph_index": 41}], "all_paragraphs": ["Online learning methods are a dynamic family of reinforcement learning algorithms behind many of the achievements in general AI over the past decade. Belonging to the sample-based learning class of reinforcement learning approaches, online learning methods allow for the determination of state values simply through repeated observations, eliminating the need for transition dynamics. Unlike their offline counterparts, online learning approaches allow for the incremental updates of the values of states and actions during an environmental episode, allowing for constant, incremental performance improvements to be observed.", "Beyond Temporal Difference learning (TD), we\u2019ve discussed the theory and practical implementations of Q-learning, an evolution of TD designed to allow for incremental estimations and improvement of state-action values. Q-learning has been made famous as becoming the backbone of reinforcement learning approaches to simulated game environments, such as those observed in OpenAI\u2019s gyms. As we\u2019ve already covered theoretical aspects of Q-learning in past articles, they will not be repeated here.", "The rapid intra-episodic responsiveness that characterize online learning approaches such as TD make them suitable for highly dynamic environments,where the values of states and actions is continuously updated throughout time through sets of estimates. Perhaps most notably, TD is the foundation of Q-learning, a more advanced algorithm used to train agents tackling game environments such as those observed in the OpenAI Atari gyms, as covered in some of our previous implementations.", "In this article, we\u2019ll explore how Q-learning can be applied to training an agent to play the classic FPS game Doom, through the use of the open-source OpenAI gym wrapper library Vizdoomgym. We\u2019ll guide you through setting up your first agent, and lay the foundations of future work.", "Recall that in Temporal Difference learning, we observed that an agent behaves cyclically in an environment, through sequence of States (S), Actions (A), and (Rewards).", "During TD, we can update the value of the previous state as soon as we reach the next state. We can further expand the scope of our model to include state-action values, in an approach termed SARSA, an on-policy TD control algorithm for estimating action values. During SARSA, we continuously estimate the action-values across incremental timesteps under a given policy, while using this knowledge to modify the policy towards an action-value greedy alternative.", "Let\u2019s compare the state-action and state-value TD update equations:", "Q-learning differs from SARSA by forcing a selection of the action with the current highest action value during an update, in a similar approach to what\u2019s observed with Bellman Optimality Equations. We can inspect SARSA and Q-learning next to the Bellman and Bellman Optimality Equations, below:", "You may be wondering about how ensure complete exploration of our state-action space, given the need to constantly select actions for a state with the highest existing action-value. In theory, we could be avoiding the optimal action simply by failing to evaluate it in the first place. To encourage exploration, we can use a decaying e-greedy policy, essentially forcing the agent to select an apparent sub-optimal action at a decaying probability in order to learn more about its value. By using a decay value, we can limit exploration once all of the states have been evaluated, after which we\u2019ll permanently select the optimal actions for each state.", "With our theory covered, let\u2019s get started on our implementation.", "Our Google Colaboratory implementation is written in Python utilizing Tensorflow Core, and can be found on the GradientCrescent Github. Readers following our publication will find the code similar to our previous implementations for Atari environments. As the implementation for this approach is quite convoluted, let\u2019s summarize the order of actions required:", "1. We define our Deep Q-learning neural network. This is a CNN that takes in-game screen images and outputs the probabilities of each of the actions, or Q-values, in the Ms-Pacman gamespace. To acquire a tensor of probabilitieses, we do not include any activation function in our final layer.", "2. As Q-learning require us to have knowledge of both the current and next states, we need to start with data generation. We feed preprocessed input images of the game space, representing initial states s, into the network, and acquire the initial probability distribution of actions, or Q-values. Before training, these values will appear random and sub-optimal.", "3. With our tensor of probabilities, we then select the action with the current highest probability using the argmax() function, and use it to build an epsilon greedy policy.", "4. Using our policy, we\u2019ll then select the action a, and evaluate our decision in the gym environment to receive information on the new state s\u2019, the reward r, and whether the episode has been finished.", "5. We store this combination of information in a buffer in the list form <s,a,r,s\u2019,d>, and repeat steps 2\u20134 for a preset number of times to build up a large enough buffer dataset.", "6. Once step 5 has finished,we move to generate our target y-values, R\u2019 and A\u2019, that are required for the loss calculation. While the former is simply discounted from R, we obtain the A\u2019 by feeding S\u2019 into our network.", "7. With all of our components in place, we can then calculate the loss to train our network.", "8. Once training has finished, we\u2019ll evaluate the performance of our agent under a new game episode, and record the performance.", "Let\u2019s get started. With Tensorflow 2 in use for Colaboratory environments, we\u2019ve converted our code to be TF2 compliant, using the new compat package. Note that this code is not TF2 native.", "Let\u2019s by importing all of the necessary packages, including the OpenAI and Vizdoomgym environments and Tensorflow Core:", "Next, we define a preprocessing function to normalize and resize the observations from our gym environment and convert them into one-dimensional tensors.", "Next, let\u2019s initialize the gym environment. We\u2019ll be using Vizdoomgym\u2019s health gathering scenario, where the objective is to collect as many health tokens as possible in order to stay alive while navigating a square room with a damaging acidic floor.", "We can inspect a screen of gameplay, and also view the 3 actions available within the gamespace, namely to turn left, right, or to move forward. Naturally, this information is not available to our agent.", "We can take this chance to compare our original and preprocessed input images:", "Next, we introduce input frame stacking and frame composition into our preprocessing pipeline, two techniques introduced by Deepmind in 2015. These methods serve to provide a temporal and motion reference for our inputs, respectively.", "We apply frame composition by by taking two of our input frames, and returning an element-wise maximum summation maxframe of the two. These composed frames are then stored in a deque or \u201cstack\u201d, which automatically removes older entries as new entries are introduced.", "Next, let\u2019s define our model, a deep Q-network. This is essentially a three layer convolutional network that takes preprocessed input obserations, with the generated flattened output fed to a fully-connected layer, generating probabilities of taking each action in the game space as an output. Note there are no activation layers here, as the presence of one would result in a binary output distribution.", "Let\u2019s also take this chance to define our hyperparameters for our model and training process. Note that the X_shape is now (None, 60, 80, 4), on account of our stacked frames.", "Recall, that Q-learning requires us to select actions with the highest action-values. To ensure that we still visit every single possible state-action combination, we\u2019ll have our agent follow a decaying epsilon-greedy policy, with an exploration rate of 5%.", "Recall from the equations above, that the update function for Q-learning requires the following:", "To supply these parameters in meaningful quantities, we need to evaluate our current policy following a set of parameters and store all of the variables in a buffer, from which we\u2019ll draw data in minibatches during training. Let\u2019s go ahead and create our buffer and a simple sampling function:", "Next, let\u2019s copy the weight parameters of our original network into a target network. This dual-network approach allows us to generate data during the training process using an existing policy while still optimizing our parameters for the next policy iteration, reducing loss oscillations.", "Finally, we\u2019ll also define our loss. This is simply the squared difference of our target action (with the highest action value) and our predicted action. We\u2019ll use an ADAM optimizer to minimize our loss during training.", "With all of our code defined, let\u2019s run our network and go over the training process. We\u2019ve defined most of this in the initial summary, but let\u2019s recall for posterity.", "Once training is complete, we can plot the reward distribution against incremental episodes. The first 1000 episodes are shown below:", "This result is acceptable, with the beginnings of improvement being visible, given how the original Vizdoom paper suggests that hundreds of thousands of episodes would be needed for a more significant improvement to be observed.", "Finally, we can take our list of frames and feed it to the scikit-video library to generate a video sequence output for inspection:", "Let\u2019s view our agent in action!", "Note how the agent pauses when it spots a health pack, before proceeding to move towards it. Just for fun, we also trained an agent for the basic scenario, where the objective is to hit the monster as soon as possible. While we achieved a best time of ca. 1.3 seconds, we\u2019ll show one of the earlier episodes below.", "That wraps up this implementation on Q-learning. In our next article, we\u2019ll move on to tackling more complex Doom scenarios with more advanced Q-learning approaches, and switch our code over to Pytorch, one of the most popular languages in AI research.", "We hope you enjoyed this article, and hope you check out the many other articles on GradientCrescent, covering applied and theoretical aspects of AI. To stay up to date with the latest updates on GradientCrescent, please consider following the publication and following our Github repository.", "White et. al, Fundamentals of Reinforcement Learning, University of Alberta", "Silva et. al, Reinforcement Learning, UCL", "Makarov et. al, Deep Reinforcement Learning with VizDoom First-Person Shooter, HSE", "Kempka et. al, ViZDoom: A Doom-based AI Research Platform for Visual Reinforcement Learning, PUT", "Ravichandiran et. al, Hands-On Reinforcement Learning with Python", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fdb03c1b03a9c&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomating-doom-with-deep-q-learning-an-implementation-in-tensorflow-db03c1b03a9c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomating-doom-with-deep-q-learning-an-implementation-in-tensorflow-db03c1b03a9c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomating-doom-with-deep-q-learning-an-implementation-in-tensorflow-db03c1b03a9c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomating-doom-with-deep-q-learning-an-implementation-in-tensorflow-db03c1b03a9c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----db03c1b03a9c--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----db03c1b03a9c--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@adrianitsaxu?source=post_page-----db03c1b03a9c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@adrianitsaxu?source=post_page-----db03c1b03a9c--------------------------------", "anchor_text": "Adrian Yijie Xu"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc834a59b6354&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomating-doom-with-deep-q-learning-an-implementation-in-tensorflow-db03c1b03a9c&user=Adrian+Yijie+Xu&userId=c834a59b6354&source=post_page-c834a59b6354----db03c1b03a9c---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdb03c1b03a9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomating-doom-with-deep-q-learning-an-implementation-in-tensorflow-db03c1b03a9c&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdb03c1b03a9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomating-doom-with-deep-q-learning-an-implementation-in-tensorflow-db03c1b03a9c&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-automating-pong-in-using-a-policy-model-an-implementation-b71f64c158ff", "anchor_text": "sample-based learning"}, {"url": "https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-understanding-blackjack-strategy-through-monte-carlo-88c9b85194ed", "anchor_text": "offline counterparts"}, {"url": "https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-navigating-cliffworld-with-sarsa-and-q-learning-cc3c36eb5830", "anchor_text": "theory"}, {"url": "https://towardsdatascience.com/automating-pac-man-with-deep-q-learning-an-implementation-in-tensorflow-ca08e9891d9c", "anchor_text": "practical implementations"}, {"url": "https://towardsdatascience.com/automating-pac-man-with-deep-q-learning-an-implementation-in-tensorflow-ca08e9891d9c", "anchor_text": "past articles"}, {"url": "https://towardsdatascience.com/optimized-deep-q-learning-for-automated-atari-space-invaders-an-implementation-in-tensorflow-2-0-80352c744fdc", "anchor_text": "some"}, {"url": "https://towardsdatascience.com/automating-pac-man-with-deep-q-learning-an-implementation-in-tensorflow-ca08e9891d9c", "anchor_text": "previous"}, {"url": "https://github.com/shakenes/vizdoomgym", "anchor_text": "Vizdoomgym"}, {"url": "https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-markov-decision-processes-policies-value-functions-94f7389e1e82", "anchor_text": "Bellman Optimality Equations."}, {"url": "https://github.com/EXJUSTICE/GradientCrescent", "anchor_text": "GradientCrescent Github."}, {"url": "https://towardsdatascience.com/automating-pac-man-with-deep-q-learning-an-implementation-in-tensorflow-ca08e9891d9c", "anchor_text": "Atari"}, {"url": "https://towardsdatascience.com/optimized-deep-q-learning-for-automated-atari-space-invaders-an-implementation-in-tensorflow-2-0-80352c744fdc", "anchor_text": "environments"}, {"url": "https://arxiv.org/pdf/1605.02097.pdf", "anchor_text": "original Vizdoom paper"}, {"url": "https://medium.com/@adrianitsaxu", "anchor_text": "GradientCrescent"}, {"url": "https://github.com/EXJUSTICE/GradientCrescent", "anchor_text": "Github"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----db03c1b03a9c---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----db03c1b03a9c---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----db03c1b03a9c---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/doom?source=post_page-----db03c1b03a9c---------------doom-----------------", "anchor_text": "Doom"}, {"url": "https://medium.com/tag/openai?source=post_page-----db03c1b03a9c---------------openai-----------------", "anchor_text": "OpenAI"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fdb03c1b03a9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomating-doom-with-deep-q-learning-an-implementation-in-tensorflow-db03c1b03a9c&user=Adrian+Yijie+Xu&userId=c834a59b6354&source=-----db03c1b03a9c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fdb03c1b03a9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomating-doom-with-deep-q-learning-an-implementation-in-tensorflow-db03c1b03a9c&user=Adrian+Yijie+Xu&userId=c834a59b6354&source=-----db03c1b03a9c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdb03c1b03a9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomating-doom-with-deep-q-learning-an-implementation-in-tensorflow-db03c1b03a9c&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----db03c1b03a9c--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fdb03c1b03a9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomating-doom-with-deep-q-learning-an-implementation-in-tensorflow-db03c1b03a9c&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----db03c1b03a9c---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----db03c1b03a9c--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----db03c1b03a9c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----db03c1b03a9c--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----db03c1b03a9c--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----db03c1b03a9c--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----db03c1b03a9c--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----db03c1b03a9c--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----db03c1b03a9c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@adrianitsaxu?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@adrianitsaxu?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Adrian Yijie Xu"}, {"url": "https://medium.com/@adrianitsaxu/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "604 Followers"}, {"url": "https://github.com/EXJUSTICE/", "anchor_text": "https://github.com/EXJUSTICE/"}, {"url": "https://www.linkedin.com/in/yijie-xu-0174a325/", "anchor_text": "https://www.linkedin.com/in/yijie-xu-0174a325/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc834a59b6354&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomating-doom-with-deep-q-learning-an-implementation-in-tensorflow-db03c1b03a9c&user=Adrian+Yijie+Xu&userId=c834a59b6354&source=post_page-c834a59b6354--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F362de3a1de04&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomating-doom-with-deep-q-learning-an-implementation-in-tensorflow-db03c1b03a9c&newsletterV3=c834a59b6354&newsletterV3Id=362de3a1de04&user=Adrian+Yijie+Xu&userId=c834a59b6354&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}