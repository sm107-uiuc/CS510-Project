{"url": "https://towardsdatascience.com/what-is-cross-entropy-3bdb04c13616", "time": 1683015936.900639, "path": "towardsdatascience.com/what-is-cross-entropy-3bdb04c13616/", "webpage": {"metadata": {"title": "What is Cross Entropy?. A brief explanation on cross-entropy\u2026 | by Anjali Bhardwaj | Towards Data Science", "h1": "What is Cross Entropy?", "description": "A couple of weeks ago, I made a pretty big decision. It was late at night, and I was lying in my bed thinking about how I spent my day. Because I have always been one to analyze my choices, I asked\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/@anjalibhardwaj2700", "anchor_text": "follow me", "paragraph_index": 2}, {"url": "https://www.udacity.com/course/deep-learning-pytorch--ud188", "anchor_text": "Deep Learning with PyTorch", "paragraph_index": 2}, {"url": "https://medium.com/@anjalibhardwaj2700", "anchor_text": "my last articles", "paragraph_index": 3}, {"url": "https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/", "anchor_text": "loss function", "paragraph_index": 4}, {"url": "https://www.khanacademy.org/math/ap-statistics/probability-ap/probability-multiplication-rule/a/general-multiplication-rule#:~:text=When%20we%20calculate%20probabilities%20involving,probability%20of%20the%20second%20event.", "anchor_text": "General Multiplication Rule in Probability", "paragraph_index": 8}, {"url": "https://www.rapidtables.com/math/algebra/Logarithm.html", "anchor_text": "log rules", "paragraph_index": 11}, {"url": "https://www.youtube.com/watch?v=keDswcqkees&feature=emb_logo", "anchor_text": "check this video out!", "paragraph_index": 24}, {"url": "https://classroom.udacity.com/courses/ud188/lessons/b4ca7aaa-b346-43b1-ae7d-20d27b2eab65/concepts/760235e0-a3ec-4e56-8cdb-56d762886690", "anchor_text": "Deep Learning with PyTorch.", "paragraph_index": 29}, {"url": "https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/", "anchor_text": "loss function", "paragraph_index": 31}, {"url": "https://stackoverflow.com/questions/41990250/what-is-cross-entropy", "anchor_text": "This is a really good form on what is cross-entropy.", "paragraph_index": 33}, {"url": "https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html", "anchor_text": "cheat sheet on Cross-Entropy", "paragraph_index": 34}, {"url": "http://www.linkedin.com/in/anjalibhardwaj2700", "anchor_text": "My LinkedIn!", "paragraph_index": 35}, {"url": "https://medium.com/@anjalibhardwaj2700", "anchor_text": "Follow my medium page for more", "paragraph_index": 36}], "all_paragraphs": ["Cross Entropy is a loss function often used in classification problems.", "A couple of weeks ago, I made a pretty big decision. It was late at night, and I was lying in my bed thinking about how I spent my day. Because I have always been one to analyze my choices, I asked myself two really important questions.", "After giving these ideas some thought, I realized the answer to both of these questions was No. No, I shouldn\u2019t stop eating fries before bed. No, I wasn\u2019t spending my time the way I wanted. So a couple of weeks ago I made the choice to learn everything I possibly could about the growing field of deep learning. I also made the choice to write about everything I learn (so if you are interested in the journey make sure to follow me). These articles are inspired by a course by Udacity called, Deep Learning with PyTorch. I highly recommend you check it out.", "If you are here from my last articles, you already know that we are trying to find ways to best classify data. To best classify data we want to maximize the probability of a model and minimize its error function. These two things are inversely related. Cross-entropy measures the performance of a classification model based on the probability and error, where the more likely (or the bigger the probability) of something is, the lower the cross-entropy. Let\u2019s look deeper into this.", "Cross entropy is a loss function that can be used to quantify the difference between two probability distributions. This can be best explained through an example.", "Suppose, we had two models, A and B, and we wanted to find out which model is better,", "Note: The numbers near the data points represent the probability that the dot is that color. For example, the probability that the blue dot on the top of the graph in Model A is blue, is 0.4.", "Intuitively, we know that Model B is better as the red dots are on the red distribution and the blue dots are on the blue distribution. But how can we make a model predict this?", "One way is to take the probabilities of every dot in Model A and multiply them together. This will give the total probability of the model, as we know from the General Multiplication Rule in Probability. We can do a similar thing for Model B,", "As you can see from the image above the model that is most likely is Model B as the probability is higher. So cool! We can find out which model is better using probability.", "Except\u2026 there are some problems with this model. As you might have guessed, if we had many data points this would cause the resulting probability to be very small. Additionally, if we had changed one data point the resulting probability would have changed drastically.", "In short, using products is not the best idea. So how can we fix this? One way is to use sums instead. If you remember your log rules, there is a way to relate the product to a sum,", "Let\u2019s apply this rule to our probabilities,", "Now, this almost looks good but let\u2019s get rid of the negatives by making the logs negative and let\u2019s also calculate the total.", "The use of negative logs on probabilities is what is known as the cross-entropy, where a high number means bad models and a low number means a good model.", "When we calculate the log for each data point, we actually get the error function for each point. For example, the error function for the point 0.2 in Model A is -ln(0.2), which is equal to 1.61. Notice that the points that are misclassified have large values, therefore have large errors.", "So let\u2019s understand cross-entropy a little more. What cross-entropy is really saying is if you have events and probabilities, how likely is it that the events happen based on the probabilities? If it is very likely, we have a small cross-entropy and if it is not likely we have a high cross-entropy. We will see this more after an example", "For example, if we take the probability that there is a gift behind three doors and we have a table that looks like the following,", "Here we can see that if the cross-entropy is large then the probability the event will happen is low and vise versa.", "Note that we describe the third door as 1-p which means 1 minus the probability there is a gift. This will give us the probability that there is no gift. Also, note that y describes how many gifts are behind the door.", "So the cross-entropy can be described by the following formula,", "Note: This is another way of describing the negative log.", "This formula calculated the entropy for the situation above. It calculated the negative logarithm which is the cross-entropy.", "What is useful about this function is that it can also be written in this form,", "Note: This formula is only for Binary Cross-Entropy. If you are interested in Multi-Class Cross Entropy check this video out!", "This function allows for two functions, p(x) and q(x). Where we can describe p(x) as the probability wanted, and q(x) as the actual probability.", "Therefore, the cross-entropy formula describes how closely the predicted distribution is to the true distribution.", "Overall, as we can see the cross-entropy is simply a way to measure the probability of a model. The cross-entropy is useful as it can describe how likely a model is and the error function of each data point. It can also be used to describe a predicted outcome compare to the true outcome. So hopefully you are starting to see how powerful this is!", "Let\u2019s check out how we can code this in python!", "This code is taken straight from the Udacity course, Deep Learning with PyTorch.", "This simple code takes in two inputs and returns the cross-entropy.", "What is cross-entropy?Cross entropy is a loss function that is used to quantify the difference between two probability distributions.", "This AI can transform your face into a Disney character!", "This is a really good form on what is cross-entropy.", "A perfect cheat sheet on Cross-Entropy", "My LinkedIn! Please feel free to connect with me, I love talking about artificial intelligence!", "Follow my medium page for more!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "20 year old interested in space technologies and deep learning"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F3bdb04c13616&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-cross-entropy-3bdb04c13616&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-cross-entropy-3bdb04c13616&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-cross-entropy-3bdb04c13616&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-cross-entropy-3bdb04c13616&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----3bdb04c13616--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3bdb04c13616--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@anjalibhardwaj2700?source=post_page-----3bdb04c13616--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@anjalibhardwaj2700?source=post_page-----3bdb04c13616--------------------------------", "anchor_text": "Anjali Bhardwaj"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa15e0aa2557d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-cross-entropy-3bdb04c13616&user=Anjali+Bhardwaj&userId=a15e0aa2557d&source=post_page-a15e0aa2557d----3bdb04c13616---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3bdb04c13616&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-cross-entropy-3bdb04c13616&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3bdb04c13616&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-cross-entropy-3bdb04c13616&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://imgflip.com/memegenerator", "anchor_text": "ImgFlip"}, {"url": "https://medium.com/@anjalibhardwaj2700", "anchor_text": "follow me"}, {"url": "https://www.udacity.com/course/deep-learning-pytorch--ud188", "anchor_text": "Deep Learning with PyTorch"}, {"url": "https://medium.com/@anjalibhardwaj2700", "anchor_text": "my last articles"}, {"url": "https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/", "anchor_text": "loss function"}, {"url": "https://www.khanacademy.org/math/ap-statistics/probability-ap/probability-multiplication-rule/a/general-multiplication-rule#:~:text=When%20we%20calculate%20probabilities%20involving,probability%20of%20the%20second%20event.", "anchor_text": "General Multiplication Rule in Probability"}, {"url": "https://www.rapidtables.com/math/algebra/Logarithm.html", "anchor_text": "log rules"}, {"url": "https://www.youtube.com/watch?v=keDswcqkees&feature=emb_logo", "anchor_text": "check this video out!"}, {"url": "https://classroom.udacity.com/courses/ud188/lessons/b4ca7aaa-b346-43b1-ae7d-20d27b2eab65/concepts/760235e0-a3ec-4e56-8cdb-56d762886690", "anchor_text": "Deep Learning with PyTorch."}, {"url": "https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/", "anchor_text": "loss function"}, {"url": "https://stackoverflow.com/questions/41990250/what-is-cross-entropy", "anchor_text": "This is a really good form on what is cross-entropy."}, {"url": "https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html", "anchor_text": "cheat sheet on Cross-Entropy"}, {"url": "http://www.linkedin.com/in/anjalibhardwaj2700", "anchor_text": "My LinkedIn!"}, {"url": "https://medium.com/@anjalibhardwaj2700", "anchor_text": "Follow my medium page for more"}, {"url": "https://medium.com/tag/cross-entropy?source=post_page-----3bdb04c13616---------------cross_entropy-----------------", "anchor_text": "Cross Entropy"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----3bdb04c13616---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----3bdb04c13616---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----3bdb04c13616---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3bdb04c13616&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-cross-entropy-3bdb04c13616&user=Anjali+Bhardwaj&userId=a15e0aa2557d&source=-----3bdb04c13616---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3bdb04c13616&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-cross-entropy-3bdb04c13616&user=Anjali+Bhardwaj&userId=a15e0aa2557d&source=-----3bdb04c13616---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3bdb04c13616&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-cross-entropy-3bdb04c13616&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3bdb04c13616--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F3bdb04c13616&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-cross-entropy-3bdb04c13616&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----3bdb04c13616---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----3bdb04c13616--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----3bdb04c13616--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----3bdb04c13616--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----3bdb04c13616--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----3bdb04c13616--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----3bdb04c13616--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----3bdb04c13616--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----3bdb04c13616--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@anjalibhardwaj2700?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@anjalibhardwaj2700?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Anjali Bhardwaj"}, {"url": "https://medium.com/@anjalibhardwaj2700/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "109 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa15e0aa2557d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-cross-entropy-3bdb04c13616&user=Anjali+Bhardwaj&userId=a15e0aa2557d&source=post_page-a15e0aa2557d--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fbbd04a7e4532&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-cross-entropy-3bdb04c13616&newsletterV3=a15e0aa2557d&newsletterV3Id=bbd04a7e4532&user=Anjali+Bhardwaj&userId=a15e0aa2557d&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}