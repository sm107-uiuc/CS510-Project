{"url": "https://towardsdatascience.com/implementing-a-deepmind-baseline-starcraft-reinforcement-learning-agent-7b1ef6f41b52", "time": 1682993605.253942, "path": "towardsdatascience.com/implementing-a-deepmind-baseline-starcraft-reinforcement-learning-agent-7b1ef6f41b52/", "webpage": {"metadata": {"title": "Implementing a DeepMind Baseline StarCraft Reinforcement Learning Agent | by Ray Heberer | Towards Data Science", "h1": "Implementing a DeepMind Baseline StarCraft Reinforcement Learning Agent", "description": "Implementing deep learning models described in research papers is a challenging yet illuminating experience that shows just how much information can be compressed into a handful of sentences or\u2026"}, "outgoing_paragraph_urls": [{"url": "http://www.rayheberer.ai/post/sc2-lessons/", "anchor_text": "gotten to know the ropes", "paragraph_index": 1}, {"url": "https://arxiv.org/abs/1708.04782", "anchor_text": "StarCraft II: A New Challenge for Reinforcement Learning", "paragraph_index": 1}, {"url": "https://arxiv.org/abs/1602.01783", "anchor_text": "Asynchronous Methods for Deep Reinforcement Learning", "paragraph_index": 2}, {"url": "https://github.com/rayheberer/SC2Agents/blob/master/agents/actor-critic.py", "anchor_text": "here", "paragraph_index": 3}, {"url": "https://en.wikipedia.org/wiki/Chain_rule_(probability)", "anchor_text": "general product rule", "paragraph_index": 14}, {"url": "https://www.tensorflow.org/api_docs/python/tf/stop_gradient", "anchor_text": "tf.stop_gradient", "paragraph_index": 28}], "all_paragraphs": ["Implementing deep learning models described in research papers is a challenging yet illuminating experience that shows just how much information can be compressed into a handful of sentences or paragraphs.", "This week, after having gotten to know the ropes of the StarCraft II python API and reinforcement learning environment, I set out to build the first of the three baseline agents described in StarCraft II: A New Challenge for Reinforcement Learning .", "Specifically, an Advantage Actor-Critic agent that estimates state values and optimal policies using a convolutional neural network architecture based on that in Asynchronous Methods for Deep Reinforcement Learning. Due to resource constraints, I did not go so far as to implement the \u201cAsynchronous\u201d part of A3C, meaning I only run one agent at a time, instead of many in parallel.", "In this post I will highlight the sections from these two research papers informing the agent\u2019s design which contained the most information relevant to the technical implementation. I hope that this will serve as a useful example for how to translate researcher-speak into code. My full implementation can be found here.", "Broadly speaking, a deep reinforcement learning agent needs to fulfill five requirements:", "This is just an arbitrary breakdown, but I find it to be helpful intuitively and will use it to guide the structure of this article.", "Thus, the main observations come as sets of feature layers which are rendered at N \u00d7 M pixels\u2026 In addition to the screen and minimap, the human interface for the game provides various non-spatial observations.", "For an agent to interface with PySC2, it must have a step(self, obs) method that is called every timestep. In addition it may have a reset(self) method that is called at the beginning of each episode.", "I like to use separate classes for my deep neural networks, as shown below. The inputs, which come in the form of feature layers, correspond relatively straightforwardly to placeholders in a Tensorflow graph. The API exposes them in arrays of shape [channels, y, x], so later on in my network I make sure to permute these dimensions appropriately so that they can be fed into convolutional layers, and often make use of np.expand_dims when feeding a batch of 1 into the graph.", "We embed all feature layers containing categorical values into a continuous space which is equivalent to using a one-hot encoding in the channel dimension followed by a 1 \u00d7 1 convolution. We also re-scale numerical features with a logarithmic transformation as some of them such as hit-points or minerals might attain substantially high values.", "While I found the language of \u201cembedding\u201d to be a little intimidating, the details that followed proved to be simpler to implement. More so than applying the one-hot encoding followed by the convolutional layer, I found that inferring whether a feature was categorical or numerical to be the more laborious task.", "It processes screen and minimap feature layers with\u2026 two layers with 16, 32 filters of size 8, 4 and stride 4, 2 respectively. The non-spatial features vector is processed by a linear layer with a tanh non-linearity. The results are concatenated and sent through a linear layer with a ReLU activation.", "Though these paragraphs of the paper correspond to some of the longest chunks of code, they are easily implemented because of the powerful Deep Learning frameworks available, of which Tensorflow is my tool of choice here.", "\u2026we propose to represent the policy in an auto-regressive manner, utilising the chain rule:", "One point of confusion for me here was regarding the \u201cchain rule.\u201d It refers to the general product rule from probability theory, not the rule from calculus that also goes by the name \u201cchain rule.\u201d", "In most of our experiments we found it sufficient to model sub-actions independently\u2026 For spatial actions (coordinates) we independently model policies to select (discretised) x and y coordinates.", "Basically, because any given action in StarCraft II (called function identifiers) might require a variable number of arguments, an efficient way to represent policies over both function identifiers and arguments is to have a policy over each type independently, where the probability of any argument given a function identifier which doesn\u2019t require it effectively becomes zero.", "There are around 10 general types of function identifiers in PySC2, each requiring a different sequence of arguments. When implementing this in the Tensorflow graph, I found that using a dictionary was a good way to keep track of all the output layers. I also built a dictionary for placeholders that would be used later on in training the network.", "To ensure that unavailable actions are never chosen by our agents, we mask out the function identifier choice of a0 such that only the proper subset can be sampled, imitating how a player randomly clicking buttons on the UI would play. We implement this by masking out actions and renormalising the probability distribution over a0.", "While in my first pass, I implemented this masking outside of the Tensorflow graph, during the actual selection of actions, it could just as well be implemented within the graph, and in a future update I may move it there.", "In A3C, we cut the trajectory and run backpropagation after K = 40 forward steps of a network or if a terminal signal is received.", "This sentence actually inspired this entire post. Initially having no idea what it meant to \u201ccut the trajectory\u201d I embarked on a journey of knowledge. The following two quotes come from the original A3C paper.", "Like our variant of n-step Q-learning, our variant of actor-critic also operates in the forward view and uses the same mix of n-step returns to update both the policy and the value-function.", "Of course, I had skipped directly to the section on A3C agents, but I knew that the trail was getting hotter.", "The algorithm then computes gradients for n-step Q-learning updates for each of the state-action pairs encountered since the last update. Each n-step update uses the longest possible n-step return resulting in a one-step update for the last state, a two-step update for the second last state, and so on for a total of up to t_max updates. The accumulated updates are applied in a single gradient step.", "And at last, everything is made clear! No, not really. This was still a highly dense piece of researcher-speak, but with the help of some pseudo-code in the appendix, I finally understood something about how the targets used to train the network would be produced.", "The A3C gradient is defined as follows:", "I intend to write about this in the future, but there was so much to unpack in this equation that I thought it would be fun to include. One thing that is interesting is that the policy and value gradients represent qualitatively different things, but smashing them together with simple addition still manages to produce a viable optimization objective (provided with some hyperparameters knobs to turn).", "Also interesting to me was that the \u201cadvantage\u201d \u2014 the difference between observed returns and estimated values which the gradient components are scaled by \u2014 acts as a constant factor though it includes a network output for value in its calculation. This meant that I had to use tf.stop_gradient so that the weights wouldn\u2019t just update to shift value estimates in a way that gamed the equation.", "I have underlined the portions of the two guiding research papers which were among the most important, and showed how they mapped onto code. This project has left me with new respect for just how much researchers are able to condense mountains of information into sentences so short they could be shared on Twitter.", "Though my commentary has been brief \u2014 and deliberately so, for I wished to emphasize the relationship between the scientific reporting and the code \u2014 I plan to follow up with an article that delves into the meaning of all of these concepts in more depth.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Scientist at Proximate Research. A little interested in a lot of things, a lot interested in a little."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F7b1ef6f41b52&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-a-deepmind-baseline-starcraft-reinforcement-learning-agent-7b1ef6f41b52&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-a-deepmind-baseline-starcraft-reinforcement-learning-agent-7b1ef6f41b52&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-a-deepmind-baseline-starcraft-reinforcement-learning-agent-7b1ef6f41b52&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-a-deepmind-baseline-starcraft-reinforcement-learning-agent-7b1ef6f41b52&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----7b1ef6f41b52--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----7b1ef6f41b52--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://rayheberer.medium.com/?source=post_page-----7b1ef6f41b52--------------------------------", "anchor_text": ""}, {"url": "https://rayheberer.medium.com/?source=post_page-----7b1ef6f41b52--------------------------------", "anchor_text": "Ray Heberer"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd6efdc3f1390&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-a-deepmind-baseline-starcraft-reinforcement-learning-agent-7b1ef6f41b52&user=Ray+Heberer&userId=d6efdc3f1390&source=post_page-d6efdc3f1390----7b1ef6f41b52---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7b1ef6f41b52&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-a-deepmind-baseline-starcraft-reinforcement-learning-agent-7b1ef6f41b52&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7b1ef6f41b52&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-a-deepmind-baseline-starcraft-reinforcement-learning-agent-7b1ef6f41b52&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "http://www.rayheberer.ai/post/sc2-lessons/", "anchor_text": "gotten to know the ropes"}, {"url": "https://arxiv.org/abs/1708.04782", "anchor_text": "StarCraft II: A New Challenge for Reinforcement Learning"}, {"url": "https://arxiv.org/abs/1602.01783", "anchor_text": "Asynchronous Methods for Deep Reinforcement Learning"}, {"url": "https://github.com/rayheberer/SC2Agents/blob/master/agents/actor-critic.py", "anchor_text": "here"}, {"url": "https://arxiv.org/abs/1708.04782", "anchor_text": "https://arxiv.org/abs/1708.04782"}, {"url": "https://en.wikipedia.org/wiki/Chain_rule_(probability)", "anchor_text": "general product rule"}, {"url": "https://www.tensorflow.org/api_docs/python/tf/stop_gradient", "anchor_text": "tf.stop_gradient"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----7b1ef6f41b52---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----7b1ef6f41b52---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----7b1ef6f41b52---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/tensorflow?source=post_page-----7b1ef6f41b52---------------tensorflow-----------------", "anchor_text": "TensorFlow"}, {"url": "https://medium.com/tag/starcraft?source=post_page-----7b1ef6f41b52---------------starcraft-----------------", "anchor_text": "Starcraft"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7b1ef6f41b52&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-a-deepmind-baseline-starcraft-reinforcement-learning-agent-7b1ef6f41b52&user=Ray+Heberer&userId=d6efdc3f1390&source=-----7b1ef6f41b52---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7b1ef6f41b52&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-a-deepmind-baseline-starcraft-reinforcement-learning-agent-7b1ef6f41b52&user=Ray+Heberer&userId=d6efdc3f1390&source=-----7b1ef6f41b52---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7b1ef6f41b52&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-a-deepmind-baseline-starcraft-reinforcement-learning-agent-7b1ef6f41b52&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----7b1ef6f41b52--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F7b1ef6f41b52&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-a-deepmind-baseline-starcraft-reinforcement-learning-agent-7b1ef6f41b52&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----7b1ef6f41b52---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----7b1ef6f41b52--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----7b1ef6f41b52--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----7b1ef6f41b52--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----7b1ef6f41b52--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----7b1ef6f41b52--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----7b1ef6f41b52--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----7b1ef6f41b52--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----7b1ef6f41b52--------------------------------", "anchor_text": ""}, {"url": "https://rayheberer.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://rayheberer.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ray Heberer"}, {"url": "https://rayheberer.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "362 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd6efdc3f1390&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-a-deepmind-baseline-starcraft-reinforcement-learning-agent-7b1ef6f41b52&user=Ray+Heberer&userId=d6efdc3f1390&source=post_page-d6efdc3f1390--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Facb0180c171c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-a-deepmind-baseline-starcraft-reinforcement-learning-agent-7b1ef6f41b52&newsletterV3=d6efdc3f1390&newsletterV3Id=acb0180c171c&user=Ray+Heberer&userId=d6efdc3f1390&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}