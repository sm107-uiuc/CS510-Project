{"url": "https://towardsdatascience.com/interviewing-the-1-5b-gpt-2-model-by-openai-b7f30fbbb8a6", "time": 1683002966.600887, "path": "towardsdatascience.com/interviewing-the-1-5b-gpt-2-model-by-openai-b7f30fbbb8a6/", "webpage": {"metadata": {"title": "Interviewing the 1.5B GPT-2 model by OpenAI | by Charles Yang | Towards Data Science", "h1": "Interviewing the 1.5B GPT-2 model by OpenAI", "description": "Using the 1558MB version of OpenAI\u2019s GPT-2 model (the largest version) and Max Woolf\u2019s gpt2-simple package on github powered by Google colaboratory, I generated the following text with data science\u2026"}, "outgoing_paragraph_urls": [{"url": "https://openai.com/blog/better-language-models/", "anchor_text": "OpenAI\u2019s GPT-2 model", "paragraph_index": 0}, {"url": "https://github.com/minimaxir/gpt-2-simple", "anchor_text": "gpt2-simple package", "paragraph_index": 0}, {"url": "https://github.com/charlesxjyang/Medium-GPT2", "anchor_text": "here", "paragraph_index": 0}, {"url": "https://huggingface.co/openai-detector/", "anchor_text": "GPT-2 output detector", "paragraph_index": 12}, {"url": "https://github.com/charlesxjyang/Medium-GPT2", "anchor_text": "here", "paragraph_index": 13}, {"url": "https://arxiv.org/pdf/1910.01108.pdf", "anchor_text": "arxiv", "paragraph_index": 13}, {"url": "https://medium.com/huggingface/distilbert-8cf3380435b5", "anchor_text": "medium post", "paragraph_index": 13}, {"url": "https://github.com/huggingface/transformers/tree/master/examples/distillation", "anchor_text": "github", "paragraph_index": 13}, {"url": "https://github.com/minimaxir/gpt-2-simple", "anchor_text": "Max Woolf\u2019s gpt2-simple github package", "paragraph_index": 14}, {"url": "https://keras.io/", "anchor_text": "keras", "paragraph_index": 16}, {"url": "https://openai.com/blog/gpt-2-1-5b-release/", "anchor_text": "1", "paragraph_index": 17}, {"url": "https://openai.com/blog/better-language-models/", "anchor_text": "2", "paragraph_index": 17}, {"url": "https://openai.com/blog/gpt-2-6-month-follow-up/", "anchor_text": "3", "paragraph_index": 17}, {"url": "https://www.nature.com/articles/s41586-019-1335-8", "anchor_text": "science abstracts for chemistry", "paragraph_index": 19}, {"url": "https://github.com/huggingface/transformers", "anchor_text": "transformers", "paragraph_index": 20}, {"url": "https://github.com/charlesxjyang/Medium-GPT2", "anchor_text": "my repo", "paragraph_index": 20}, {"url": "https://minimaxir.com/2019/09/howto-gpt2/", "anchor_text": "Max Woolf\u2019s great introductory blog", "paragraph_index": 20}, {"url": "http://ml4sci.substack.com/", "anchor_text": "ml4sci.substack.com", "paragraph_index": 21}], "all_paragraphs": ["Using the 1558MB version of OpenAI\u2019s GPT-2 model (the largest version) and Max Woolf\u2019s gpt2-simple package on github powered by Google colaboratory, I generated the following text with data science focused prompts. The github repo for this project is located here. All text in bold is human written. Scroll down for a meta-analysis about the implications of this technology and some more about how I created this.", "Data Science is a set of techniques for conducting complex, multidimensional analysis of large data sets. Data science is a broad field, encompassing a wide variety of disciplines, from mathematics and statistics to computer science and machine learning.", "In the near future, artificial intelligence will be so powerful that it will be able to outperform humans at almost any task, and this will have a profound impact on society. But the impact will not be good. It will mean that humans will have less and less control over their own lives.", "Machine learning is a process of training computers to understand and predict data. The latest version of the popular Python programming language now includes a library that can be used to train neural networks. The technique can be used to train neural networks to predict a person\u2019s gender based on the input data.", "Learning how to code is a great way to learn to write software. However, there are a lot of people who can\u2019t code, or aren\u2019t good at it. I think the biggest reason why people don\u2019t learn how to code is that they don\u2019t think they can. I\u2019ve heard a lot of people say that they don\u2019t think they can learn to code because they don\u2019t have a computer science degree. This is just a huge misconception.", "Neural Networks are able to learn from their experiences and generalize from them to improve on their previous behavior. However, the problem with neural networks is that they are not well suited to tasks that require continuous learning. The problem is that neural networks are not able to learn from their experiences and generalize from them to improve on their previous behavior. This is because neural networks are not able to remember the past.", "Google\u2019s Android software, which powers more than a third of the world\u2019s smartphones, is also the most popular operating system on the planet, with more than 700 million users worldwide. The report found that the majority of Android users worldwide are accessing the software through their smartphones, and that 80% of those users are accessing the software through their smartphones. In Europe, Android is the most popular operating system, with more than a third of the European smartphone users accessing the software through their smartphones.", "OpenAI\u2019s original goal was to develop a general purpose AI that could tackle problems in the areas of self-driving cars, robotics, and healthcare. The company\u2019s goal has evolved into a more general one: to make it easier for people to build AI systems.", "OpenAI, the company behind the Google-owned DeepMind AI project, has published a white paper outlining its vision for the future of AI. The paper, which is titled \u201cA Primer on the Future of AI,\u201d was co-authored by Yann LeCun, a professor at the University of Montreal, and Shane Legg, a researcher at the company\u2019s AI lab.", "Deep Learning (ML) is an extremely important tool for machine learning. It\u2019s the technology behind the algorithms that allow us to recognize and understand images, videos, and text. The world is full of data that\u2019s structured in a way that makes it easy to recognize patterns. This data can be of any size. The most common example is text, where a sentence might be structured like this: \u201cI saw a snake\u201d", "Analysis of Generated Text and Model Training", "The 1558MB version of GPT-2 is able to generate coherent text for a variety of prompts. Interestingly, it is able to describe neural networks, deep learning, machine learning, and data science fairly accurately. However, these language models still lack intrinsic models of the world. For instance, it generates (ML) as the incorrect abbreviation of Deep Learning; it believes OpenAI founded DeepMind, despite also calling it \u201cGoogle-owned\u201d; there is currently no paper titled \u201cA primer on the future of AI\u201d and Yann Lecun is a professor at NYU, not Montreal. Clearly, GPT-2 is able to learn frequent pairings between phrases, but does not actually have a clear relational model of the world.", "Detecting synthetic generated text by human review is far too time-consuming. Hugging Face, a NLP research startup, has open-sourced a GPT-2 output detector, which performs very well on the above generated text, classifying the above generated text as \u2018fake\u2019 GPT-2 produced text with greater than 99% confidence. Interestingly, the detector is fairly good even when one makes slight, cosmetic alterations to the text, suggesting that such detectors can even detect \u2018edited\u2019 fake text. The development of such \u201cDeepfake detectors\u201d provides a small bulwark against a potential flood of fake written literature, although one can imagine there are already researchers developing more difficult-to-detect generative models.", "When I first set out on this project, I actually tried to first fine-tune a 355MB GPT-2 model on a dataset of Medium articles that had data science relevant tags, but it turns out that the fine-tuned 355MB model actually performs worse than the non-fine-tuned 1558MB model (You can find my github repo with the scraped data here). This is probably due to the greater language understanding of the larger model and it\u2019s ability to synthesize long-term coherent text. Unfortunately, it is not feasible to fine-tune the 1558MB model on a single GPU due to RAM constraints, although some interesting future work could be fine-tuning distilled versions of large models which could fit on a single GPU[arxiv, medium post, github]. What\u2019s surprising is that the 1558MB version is still fairly \u201cknowledgeable\u201d about data science and machine learning, with no fine-tuning on a domain-specific dataset.", "Max Woolf\u2019s gpt2-simple github package allows users to spin up a single google colaboratory notebook, load various sizes of the GPT-2 model, fine-tune it on the complimentary GPU provided by google colaboratory, and generate synthetic text passages, in just a few short lines of code. The fact that such high-level software exists and is easily used by any user is due to the convergence of two powerful trends in deep learning: easy, cheap access to specialized hardware and high-level abstraction programming paradigms.", "Google Colaboratory provides a complimentary NVIDIA T40 or K80 GPU with any instance of a google colaboratory notebook. The financial incentive for Google doing so is to encourage users to adopt their own proprietary google colaboratory notebooks over jupyter notebook, an open-source software. The low cost of providing so much hardware is possible due to the scalability of cloud computing (as well as Google\u2019s deep pockets). As a result, anyone with an internet connection can now access a server-based high-end GPU and a fair amount of compute power with Google Colaboratory. This freely available compute enables any individual to train and utilize very large and powerful deep learning models.", "In addition to cheap compute, one of the other core drivers of deep learning innovation is the introduction of high-level programming paradigms and packages for creating deep learning models. For instance, keras is a high-level package that is an API for lower-level neural network implementation packages such as Tensorflow and Pytorch. With keras, it is easy for anyone to create simple neural networks in just a few lines of code. Such software helps to accelerate research and innovation by lowering the barrier of entry for anyone to play around with such tools. Similarly, gpt2-simple abstracts away all the model training by using standardized NLP fine-tuning procedures, as well as providing nice wrapper functions for generating text.", "The convergence of these two trends, as well as the incredible improvements in NLP architecture and performance recently, now allows anyone to synthesize human-like text. The implications of such a model architecture have already been discussed by OpenAI[1,2,3].", "One can easily imagine that text generation models such as GPT-2 will one day be able to write entire Medium articles about data science, programming, machine learning, etc. When that arrives, we will have to face some thorny questions. For example, who has authorship rights? The programmer who implemented and ran the code? The authors who created the model? The collective authors of the text in the fine-tuning corpus?", "As realistic text-generation models such as GPT-2 become more prevalent and commonplace (especially with the progression of high-level software implementations), we will see more and more examples of fine-tuned NLP models trained on domain-specific datasets. Although the larger, not fine-tuned 1558MB GPT-2 version performed better than the smaller, fine-tuned 355MB GPT-2 version, one can imagine domains with text that are much more specific and different from the large corpus that GPT-2 was trained on e.g. science abstracts for chemistry", "For me, I embarked on this project over my winter break to learn more about these exciting breakthroughs during what has been called the year of NLP, as well as to familiarize myself with the software environment for NLP models, most notably, Hugging Face\u2019s transformers package. I had some hands-on experience with fine-tuning GPT-2 on a text corpus that I created, dealing with RAM issues on a single GPU, and explored knowledge distillation methods. If you want to try it out yourself, check out my repo or Max Woolf\u2019s great introductory blog. As always, use and share responsibly.", "I\u2019ve recently started a free newsletter on applications of Machine Learning and AI in scientific fields and engineering problems (ml4sci). You can find it on ml4sci.substack.com. Feel free to drop a suggested article or topic and if you really like what you see, please subscribe!", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fb7f30fbbb8a6&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterviewing-the-1-5b-gpt-2-model-by-openai-b7f30fbbb8a6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterviewing-the-1-5b-gpt-2-model-by-openai-b7f30fbbb8a6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterviewing-the-1-5b-gpt-2-model-by-openai-b7f30fbbb8a6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterviewing-the-1-5b-gpt-2-model-by-openai-b7f30fbbb8a6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----b7f30fbbb8a6--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b7f30fbbb8a6--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@charlesyang_32909?source=post_page-----b7f30fbbb8a6--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@charlesyang_32909?source=post_page-----b7f30fbbb8a6--------------------------------", "anchor_text": "Charles Yang"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F17bd64d00f1f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterviewing-the-1-5b-gpt-2-model-by-openai-b7f30fbbb8a6&user=Charles+Yang&userId=17bd64d00f1f&source=post_page-17bd64d00f1f----b7f30fbbb8a6---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb7f30fbbb8a6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterviewing-the-1-5b-gpt-2-model-by-openai-b7f30fbbb8a6&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb7f30fbbb8a6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterviewing-the-1-5b-gpt-2-model-by-openai-b7f30fbbb8a6&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/syncedreview/openai-releases-1-5-billion-parameter-gpt-2-model-c34e97da56c0", "anchor_text": "https://medium.com/syncedreview/openai-releases-1-5-billion-parameter-gpt-2-model-c34e97da56c0"}, {"url": "https://openai.com/blog/better-language-models/", "anchor_text": "OpenAI\u2019s GPT-2 model"}, {"url": "https://github.com/minimaxir/gpt-2-simple", "anchor_text": "gpt2-simple package"}, {"url": "https://github.com/charlesxjyang/Medium-GPT2", "anchor_text": "here"}, {"url": "https://huggingface.co/openai-detector/", "anchor_text": "GPT-2 output detector"}, {"url": "https://github.com/charlesxjyang/Medium-GPT2", "anchor_text": "here"}, {"url": "https://arxiv.org/pdf/1910.01108.pdf", "anchor_text": "arxiv"}, {"url": "https://medium.com/huggingface/distilbert-8cf3380435b5", "anchor_text": "medium post"}, {"url": "https://github.com/huggingface/transformers/tree/master/examples/distillation", "anchor_text": "github"}, {"url": "https://github.com/minimaxir/gpt-2-simple", "anchor_text": "Max Woolf\u2019s gpt2-simple github package"}, {"url": "https://keras.io/", "anchor_text": "keras"}, {"url": "https://openai.com/blog/gpt-2-1-5b-release/", "anchor_text": "1"}, {"url": "https://openai.com/blog/better-language-models/", "anchor_text": "2"}, {"url": "https://openai.com/blog/gpt-2-6-month-follow-up/", "anchor_text": "3"}, {"url": "https://www.nature.com/articles/s41586-019-1335-8", "anchor_text": "science abstracts for chemistry"}, {"url": "https://github.com/huggingface/transformers", "anchor_text": "transformers"}, {"url": "https://github.com/charlesxjyang/Medium-GPT2", "anchor_text": "my repo"}, {"url": "https://minimaxir.com/2019/09/howto-gpt2/", "anchor_text": "Max Woolf\u2019s great introductory blog"}, {"url": "http://ml4sci.substack.com/", "anchor_text": "ml4sci.substack.com"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----b7f30fbbb8a6---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----b7f30fbbb8a6---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/interview?source=post_page-----b7f30fbbb8a6---------------interview-----------------", "anchor_text": "Interview"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----b7f30fbbb8a6---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/openai?source=post_page-----b7f30fbbb8a6---------------openai-----------------", "anchor_text": "OpenAI"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb7f30fbbb8a6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterviewing-the-1-5b-gpt-2-model-by-openai-b7f30fbbb8a6&user=Charles+Yang&userId=17bd64d00f1f&source=-----b7f30fbbb8a6---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb7f30fbbb8a6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterviewing-the-1-5b-gpt-2-model-by-openai-b7f30fbbb8a6&user=Charles+Yang&userId=17bd64d00f1f&source=-----b7f30fbbb8a6---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb7f30fbbb8a6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterviewing-the-1-5b-gpt-2-model-by-openai-b7f30fbbb8a6&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b7f30fbbb8a6--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fb7f30fbbb8a6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterviewing-the-1-5b-gpt-2-model-by-openai-b7f30fbbb8a6&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----b7f30fbbb8a6---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----b7f30fbbb8a6--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----b7f30fbbb8a6--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----b7f30fbbb8a6--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----b7f30fbbb8a6--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----b7f30fbbb8a6--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----b7f30fbbb8a6--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----b7f30fbbb8a6--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----b7f30fbbb8a6--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@charlesyang_32909?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@charlesyang_32909?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Charles Yang"}, {"url": "https://medium.com/@charlesyang_32909/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "141 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F17bd64d00f1f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterviewing-the-1-5b-gpt-2-model-by-openai-b7f30fbbb8a6&user=Charles+Yang&userId=17bd64d00f1f&source=post_page-17bd64d00f1f--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F587595457611&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterviewing-the-1-5b-gpt-2-model-by-openai-b7f30fbbb8a6&newsletterV3=17bd64d00f1f&newsletterV3Id=587595457611&user=Charles+Yang&userId=17bd64d00f1f&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}