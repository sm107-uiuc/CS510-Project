{"url": "https://towardsdatascience.com/word-and-character-based-lstms-12eb65f779c2", "time": 1683017512.1558871, "path": "towardsdatascience.com/word-and-character-based-lstms-12eb65f779c2/", "webpage": {"metadata": {"title": "Word and Character Based LSTM Models | by Ruthu S Sanketh | Towards Data Science", "h1": "Word and Character Based LSTM Models", "description": "If you\u2019re not familiar with the NLTK library and data preprocessing, take a look at this article. If you\u2019re interested in language models and how to build them, read this article. If you\u2019re familiar\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/text-preprocessing-with-nltk-9de5de891658", "anchor_text": "this", "paragraph_index": 0}, {"url": "https://medium.com/swlh/language-modelling-with-nltk-20eac7e70853", "anchor_text": "this", "paragraph_index": 0}, {"url": "https://github.com/ruthussanketh/natural-language-processing/blob/main/word-and-character-LSTM/corpus.txt", "anchor_text": "here", "paragraph_index": 5}, {"url": "https://colab.research.google.com/", "anchor_text": "Google Colab", "paragraph_index": 18}, {"url": "https://github.com/ruthussanketh/natural-language-processing/tree/main/word-and-character-LSTM", "anchor_text": "here", "paragraph_index": 29}], "all_paragraphs": ["If you\u2019re not familiar with the NLTK library and data preprocessing, take a look at this article. If you\u2019re interested in language models and how to build them, read this article. If you\u2019re familiar with NLP and language models, continue reading!", "Long-short-term memory models or LSTMs are used to solve the problem of short term memory by using gates that regulate the flow of information. These models have mechanisms that decide whether or not to keep information, thereby being able to retain important information over a long time. Due to their ability to learn long-term dependencies, they are extensively used in machine translation, speech recognition, handwriting recognition and generation, language modeling and translation, speech synthesis, and many other deep learning tasks.", "Sequence classification is a predictive modeling problem where there is some sequence of inputs over space or time and we wish to predict a category for the sequence. The sequences can vary in length, be comprised of a very large vocabulary of input symbols and may require the model to learn long-term dependencies between symbols in the input sequence, making this a challenging problem for RNNs but easily solvable by LSTMs.", "In this article, we will look at building word based as well as character based LSTM models, and compare the next word predictions of the two. We will also look at different parameters that can be changed while training the models and analyze which combination gives us better results.", "First we import the required toolkits and libraries.", "Now we perform basic preprocessing, such as lowering, etc., on the dataset. We also extract some statistics about the dataset such as the number of unique tokens, which can later be passed as a parameter while training our model. The dataset used in this article can be found here.", "Now that we know what our data looks like after preprocessing, we can set the hyperparameters of our word based LSTM model. We also tokenize our dataset into sentences and obtain sequences of length 50 tokens each from the sentences.", "We have to prepare our data into X and Y columns. Since our sequence length is 50, we take every 50 tokens as the input and the 51st token as the output. We also one-hot encode the output variable Y.", "Let us now pass the required parameters to our model and compile it. We use word embeddings, which is a technique where words are encoded as real-valued vectors in a high dimensional space, such that the similarity between words in terms of meaning translates to closeness in the vector space, using an Embedded Layer provided by Keras.", "We will map each word onto a 100 length real valued vector, using an embedding dimension of 100 as defined by our hyperparameters. Bidirectional LSTMs, which we are using, are an extension of traditional LSTMs that improve model performance by training two instead of one LSTM on the input sequence.", "We train the model with 5 epochs and a train-validation split of 80\u201320%. Increasing the number of epochs might improve the model performance up to a certain number of epochs.", "Now that we have a trained model, we can use it to get the next word predictions. To do this, first we have to create a word map from the predictions to the actual word using which we can find the word corresponding to a certain prediction. We then write a function to return the next n words greedily.", "Let us look at the next word predictions by picking a random seed as the input and predicting the next 10 words greedily, and also by specifying an unseen input string.", "We see that word based LSTM model in general has a very poor prediction of next words, regardless of whether the data is seen or unseen. Predictions may be made better by training the model for a larger number of epochs.", "Let us train 2 character based models with different hyperparameters and then compare the outputs to analyze which hyperparameters are best suited to next character prediction.", "First we use the preprocessed data to create the raw data for the model. Periods have not been removed for better results. Sometimes, not removing other special characters such as apostrophes, commas, etc., can also give better results. Feel free to try it out! We then create a mapping of unique characters to integers and vice versa.", "Now we prepare the dataset where the input is a sequence of 100 characters and the target is next character. We also one-hot encode the output variable Y.", "Let us now pass the required parameters to our model and compile it. We use word embeddings again. We will map each word onto a 100 length real valued vector, using an embedding dimension of 100 as defined by our hyperparameters. We also use a dropout layer which is a regularization method to prevent overfitting.", "We now train the model using 20 epochs, and a batch size of 128. Using a local machine usually makes this process long and requires a lot of runtime, so I would suggest using Google Colab with a GPU runtime.", "We need to write a function to be able to get the actual character from the prediction.", "Let us now look at the next character predictions by picking a random seed as the input and predicting the next 200 characters greedily, and also by specifying an unseen input string.", "We see that the character generation is better, and has more overall word dependency captured in the seen data than the unseen data. In both the cases, the generated text has nearly the same degree of syntactical accuracy, but the semantic meaning of the overall text is higher in the seen data generation.", "We use the same data that was used in the first model. Let us pass the required parameters to our model and compile it. We use word embeddings again. We will map each word onto a 100 length real valued vector, using an embedding dimension of 100 as defined by our hyperparameters. We also use 2 dropout layers.", "We now train the model using 20 epochs, and a batch size of 64.", "We now write a function to get the actual character from the prediction.", "Let us look at the next character predictions by picking a random seed as the input and predicting the next 200 characters greedily, and also by specifying an unseen input string.", "We see that the character generation is better, has higher overall word dependency captured in the seen data than the unseen data, as well as higher semantic meaning, just like Model 1. Model 2 is comparatively less overfitted than Model 1, in terms of unseen as well as seen data prediction. The unseen data prediction of this model is thus slightly better than that of Model 1.", "Model 2 has a higher accuracy, as well as semantic meaning and captures word dependencies better than the Model 1 for unseen data, whereas Model 1 makes slightly better predictions on the seen data.", "Some differences between Model 1 and Model 2 are -", "I hope this article was a good introduction to word and character based LSTM models, how to build as well as analyze them without needing a lot of mathematical knowledge of their working. Varying hyperparameters, the number of layers, as well the number of training epochs can help improve model performance. The entire code and dataset used in this article can be found here.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Senior at IIT Kharagpur. Passionate about tech that is shaping the future. Spend most of my time reading :)"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F12eb65f779c2&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-and-character-based-lstms-12eb65f779c2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-and-character-based-lstms-12eb65f779c2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-and-character-based-lstms-12eb65f779c2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-and-character-based-lstms-12eb65f779c2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----12eb65f779c2--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----12eb65f779c2--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://ruthussanketh.medium.com/?source=post_page-----12eb65f779c2--------------------------------", "anchor_text": ""}, {"url": "https://ruthussanketh.medium.com/?source=post_page-----12eb65f779c2--------------------------------", "anchor_text": "Ruthu S Sanketh"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F43c63c0677bc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-and-character-based-lstms-12eb65f779c2&user=Ruthu+S+Sanketh&userId=43c63c0677bc&source=post_page-43c63c0677bc----12eb65f779c2---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F12eb65f779c2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-and-character-based-lstms-12eb65f779c2&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F12eb65f779c2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-and-character-based-lstms-12eb65f779c2&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/text-preprocessing-with-nltk-9de5de891658", "anchor_text": "this"}, {"url": "https://medium.com/swlh/language-modelling-with-nltk-20eac7e70853", "anchor_text": "this"}, {"url": "https://unsplash.com/@markusspiske?utm_source=medium&utm_medium=referral", "anchor_text": "Markus Spiske"}, {"url": "https://unsplash.com/?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://github.com/ruthussanketh/natural-language-processing/blob/main/word-and-character-LSTM/corpus.txt", "anchor_text": "here"}, {"url": "https://colab.research.google.com/", "anchor_text": "Google Colab"}, {"url": "https://github.com/ruthussanketh/natural-language-processing/tree/main/word-and-character-LSTM", "anchor_text": "here"}, {"url": "https://www.lighttag.io/blog/character-level-NLP/", "anchor_text": "LightTag \u2014 Character Level NLP"}, {"url": "https://machinelearningmastery.com/develop-character-based-neural-language-model-keras/", "anchor_text": "How to Develop a Character-Based Neural Language Model in Keras"}, {"url": "https://towardsdatascience.com/pos-tagging-using-crfs-ea430c5fb78b", "anchor_text": "POS Tagging Using CRFs"}, {"url": "https://towardsdatascience.com/naive-bayes-and-lstm-based-classifier-models-63d521a48c20", "anchor_text": "Naive Bayes and LSTM Based Classifier Models"}, {"url": "https://medium.com/tag/nlp?source=post_page-----12eb65f779c2---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/text-preprocessing?source=post_page-----12eb65f779c2---------------text_preprocessing-----------------", "anchor_text": "Text Preprocessing"}, {"url": "https://medium.com/tag/lstm?source=post_page-----12eb65f779c2---------------lstm-----------------", "anchor_text": "Lstm"}, {"url": "https://medium.com/tag/language-model?source=post_page-----12eb65f779c2---------------language_model-----------------", "anchor_text": "Language Model"}, {"url": "https://medium.com/tag/keras?source=post_page-----12eb65f779c2---------------keras-----------------", "anchor_text": "Keras"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F12eb65f779c2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-and-character-based-lstms-12eb65f779c2&user=Ruthu+S+Sanketh&userId=43c63c0677bc&source=-----12eb65f779c2---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F12eb65f779c2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-and-character-based-lstms-12eb65f779c2&user=Ruthu+S+Sanketh&userId=43c63c0677bc&source=-----12eb65f779c2---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F12eb65f779c2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-and-character-based-lstms-12eb65f779c2&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----12eb65f779c2--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F12eb65f779c2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-and-character-based-lstms-12eb65f779c2&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----12eb65f779c2---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----12eb65f779c2--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----12eb65f779c2--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----12eb65f779c2--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----12eb65f779c2--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----12eb65f779c2--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----12eb65f779c2--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----12eb65f779c2--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----12eb65f779c2--------------------------------", "anchor_text": ""}, {"url": "https://ruthussanketh.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://ruthussanketh.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ruthu S Sanketh"}, {"url": "https://ruthussanketh.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "167 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F43c63c0677bc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-and-character-based-lstms-12eb65f779c2&user=Ruthu+S+Sanketh&userId=43c63c0677bc&source=post_page-43c63c0677bc--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F3d93ce75710a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-and-character-based-lstms-12eb65f779c2&newsletterV3=43c63c0677bc&newsletterV3Id=3d93ce75710a&user=Ruthu+S+Sanketh&userId=43c63c0677bc&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}