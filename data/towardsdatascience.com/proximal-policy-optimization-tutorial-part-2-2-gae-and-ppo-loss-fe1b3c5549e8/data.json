{"url": "https://towardsdatascience.com/proximal-policy-optimization-tutorial-part-2-2-gae-and-ppo-loss-fe1b3c5549e8", "time": 1682997652.050884, "path": "towardsdatascience.com/proximal-policy-optimization-tutorial-part-2-2-gae-and-ppo-loss-fe1b3c5549e8/", "webpage": {"metadata": {"title": "Proximal Policy Optimization Tutorial (Part 2/2: GAE and PPO loss) | by Chintan Trivedi | Towards Data Science", "h1": "Proximal Policy Optimization Tutorial (Part 2/2: GAE and PPO loss)", "description": "Welcome to the second part of the Reinforcement Learning math and code tutorial series. In the first part of this series, we saw how to setup the Google Football Environment and then implemented an\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/proximal-policy-optimization-tutorial-part-1-actor-critic-method-d53f9afffbf6", "anchor_text": "Proximal Policy Optimization Tutorial (Part 1: Actor-Critic Method)", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-tutorial-part-1-actor-critic-method-d53f9afffbf6", "anchor_text": "first part of this series", "paragraph_index": 1}, {"url": "https://gist.github.com/ChintanTrivedi/665dd40d21227fbf6ac646c99cb8976d#file-train-py", "anchor_text": "this code", "paragraph_index": 2}, {"url": "https://www.youtube.com/watch?v=WxQfQW48A4A", "anchor_text": "here", "paragraph_index": 19}, {"url": "https://www.youtube.com/watch?v=5P7I-xPq8u8", "anchor_text": "here", "paragraph_index": 19}, {"url": "https://medium.com/@chintan.t93", "anchor_text": "Medium", "paragraph_index": 20}, {"url": "https://github.com/ChintanTrivedi", "anchor_text": "GitHub", "paragraph_index": 20}, {"url": "http://youtube.com/c/DeepGamingAI", "anchor_text": "YouTube channel", "paragraph_index": 20}, {"url": "http://medium.com/deepgamingai", "anchor_text": "medium.com/deepgamingai", "paragraph_index": 22}], "all_paragraphs": ["Part 1 link: Proximal Policy Optimization Tutorial (Part 1: Actor-Critic Method)", "Welcome to the second part of the Reinforcement Learning math and code tutorial series. In the first part of this series, we saw how to setup the Google Football Environment and then implemented an Actor-Critic model framework to interact with and collect sample experiences from this game environment.", "Today, we will complete the rest of the tutorial by using the batch of sample experiences to train our model to score some goals in the game. In reference to this code we implemented last time, recall that we have collected the following information so far:", "Using this information,we can now go ahead and calculate advantages.", "Advantage can be defined as a way to measure how much better off we can be by taking a particular action when we are in a particular state. We want to use the rewards that we collected at each time step and calculate how much of an advantage we were able to obtain by taking the action that we took. So if we took a good action like shoot towards a goal, we want to calculate how much better off we were by taking that action, not only in the short run but also over a longer period of time. This way, even if we do not immediately score a goal in the next time step after shooting, we still look at few time steps after that action into the longer future to see if we scored a goal.", "In order to calculate this, we\u2019ll use an algorithm known as Generalized Advantage Estimation or GAE. So let\u2019s take a look at how this algorithm works using the batch of experiences we have collected.", "This is basically the GAE algorithm that can be implemented in our code as follows.", "Here\u2019s a line-by-line explanation of this algorithm in the video below.", "We now have everything we need in order to train our actor and critic models. So we will see how to use this information to calculate a custom PPO loss and use that loss for training the actor model.", "This is the most important part of the Proximal Policy Optimization algorithm. So let\u2019s first understand this loss function.", "Recall that \u03c0 indicates the policy that is defined by our Actor neural network model. By training this model, we want to improve this policy so that it gives us better and better actions over time. Now a major problem in some Reinforcement Learning approaches is that once our model adopts a bad policy, it only takes bad actions in the game, so we are unable to generate any good actions from there on leading us down an unrecoverable path in training. PPO tries to address this by only making small updates to the model in an update step, thereby stabilizing the training process. The PPO loss can be calculated as follows.", "This custom loss function can be defined with Keras using the following code.", "Here\u2019s the line-by-line explanation and implementation of this custom loss function in the video embedded below.", "Now we can finally start the model training. For this, let\u2019s use the fit function of Keras as follows.", "You should now be able to see on your screen the model taking different actions and collecting rewards from the environment. At the beginning of the training process, the actions may seem fairly random as the randomly initialized model is exploring the game environment.", "Ok, now lets implement some model evaluation code. This will tell us during model training how good the last updated version of the model is terms of successfully scoring a goal. So to evaluate that, we\u2019ll calculate average reward, defined as the mean of all the rewards we obtain by playing the game from scratch multiple times. If we score a goal in let\u2019s say 4 out of 5 games, our average reward will be 80%. This can be implemented as follows.", "The testing phase will look like the following once the model begins to learn which set of actions produce the best long-term rewards. In our case, hitting the ball to the right is observed to have produced the best rewards, hence our Actor model will produce the right direction and shoot actions as it\u2019s preferred output actions.", "The rest of the code used to tie everything together can be found here in the train.py script of this GitHub repository.", "If you want to learn this implementation with line-by-line explanation, you may watch the video below.", "I hope this tutorial could give you a good idea of the basic PPO algorithm. You can now move onto building upon this by executing multiple environments in parallel in order to collect more training samples and also solve more complicated game scenarios like full 11-vs-11 mode or scoring from corner kicks. Some useful references that can help with that, and also which I used for this particular tutorial, can be found here and here. Best of luck!", "Thank you for reading. If you liked this article, you may follow more of my work on Medium, GitHub, or subscribe to my YouTube channel.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "AI, ML for Digital Games Researcher. Founder at DG AI Research Lab, India. Visit our publication homepage medium.com/deepgamingai for weekly AI & Games content!"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Ffe1b3c5549e8&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fproximal-policy-optimization-tutorial-part-2-2-gae-and-ppo-loss-fe1b3c5549e8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fproximal-policy-optimization-tutorial-part-2-2-gae-and-ppo-loss-fe1b3c5549e8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fproximal-policy-optimization-tutorial-part-2-2-gae-and-ppo-loss-fe1b3c5549e8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fproximal-policy-optimization-tutorial-part-2-2-gae-and-ppo-loss-fe1b3c5549e8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----fe1b3c5549e8--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----fe1b3c5549e8--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@chintan.t93?source=post_page-----fe1b3c5549e8--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@chintan.t93?source=post_page-----fe1b3c5549e8--------------------------------", "anchor_text": "Chintan Trivedi"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcba121ffc3f5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fproximal-policy-optimization-tutorial-part-2-2-gae-and-ppo-loss-fe1b3c5549e8&user=Chintan+Trivedi&userId=cba121ffc3f5&source=post_page-cba121ffc3f5----fe1b3c5549e8---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffe1b3c5549e8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fproximal-policy-optimization-tutorial-part-2-2-gae-and-ppo-loss-fe1b3c5549e8&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffe1b3c5549e8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fproximal-policy-optimization-tutorial-part-2-2-gae-and-ppo-loss-fe1b3c5549e8&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-tutorial-part-1-actor-critic-method-d53f9afffbf6", "anchor_text": "Proximal Policy Optimization Tutorial (Part 1: Actor-Critic Method)"}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-tutorial-part-1-actor-critic-method-d53f9afffbf6", "anchor_text": "first part of this series"}, {"url": "https://gist.github.com/ChintanTrivedi/665dd40d21227fbf6ac646c99cb8976d#file-train-py", "anchor_text": "this code"}, {"url": "https://github.com/ChintanTrivedi/rl-bot-football", "anchor_text": "ChintanTrivedi/rl-bot-footballThis code implements a bare-bones version of the Proximal Policy Optimization (PPO) algorithm for the purpose of\u2026github.com"}, {"url": "https://www.youtube.com/watch?v=WxQfQW48A4A", "anchor_text": "here"}, {"url": "https://www.youtube.com/watch?v=5P7I-xPq8u8", "anchor_text": "here"}, {"url": "https://medium.com/@chintan.t93", "anchor_text": "Medium"}, {"url": "https://github.com/ChintanTrivedi", "anchor_text": "GitHub"}, {"url": "http://youtube.com/c/DeepGamingAI", "anchor_text": "YouTube channel"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----fe1b3c5549e8---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----fe1b3c5549e8---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----fe1b3c5549e8---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----fe1b3c5549e8---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/computer-vision-aide?source=post_page-----fe1b3c5549e8---------------computer_vision_aide-----------------", "anchor_text": "Computer Vision Aide"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffe1b3c5549e8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fproximal-policy-optimization-tutorial-part-2-2-gae-and-ppo-loss-fe1b3c5549e8&user=Chintan+Trivedi&userId=cba121ffc3f5&source=-----fe1b3c5549e8---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffe1b3c5549e8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fproximal-policy-optimization-tutorial-part-2-2-gae-and-ppo-loss-fe1b3c5549e8&user=Chintan+Trivedi&userId=cba121ffc3f5&source=-----fe1b3c5549e8---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffe1b3c5549e8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fproximal-policy-optimization-tutorial-part-2-2-gae-and-ppo-loss-fe1b3c5549e8&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----fe1b3c5549e8--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Ffe1b3c5549e8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fproximal-policy-optimization-tutorial-part-2-2-gae-and-ppo-loss-fe1b3c5549e8&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----fe1b3c5549e8---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----fe1b3c5549e8--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----fe1b3c5549e8--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----fe1b3c5549e8--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----fe1b3c5549e8--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----fe1b3c5549e8--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----fe1b3c5549e8--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----fe1b3c5549e8--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----fe1b3c5549e8--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@chintan.t93?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@chintan.t93?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Chintan Trivedi"}, {"url": "https://medium.com/@chintan.t93/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "3.4K Followers"}, {"url": "http://medium.com/deepgamingai", "anchor_text": "medium.com/deepgamingai"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcba121ffc3f5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fproximal-policy-optimization-tutorial-part-2-2-gae-and-ppo-loss-fe1b3c5549e8&user=Chintan+Trivedi&userId=cba121ffc3f5&source=post_page-cba121ffc3f5--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Ff8a6ddbb7d3e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fproximal-policy-optimization-tutorial-part-2-2-gae-and-ppo-loss-fe1b3c5549e8&newsletterV3=cba121ffc3f5&newsletterV3Id=f8a6ddbb7d3e&user=Chintan+Trivedi&userId=cba121ffc3f5&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}