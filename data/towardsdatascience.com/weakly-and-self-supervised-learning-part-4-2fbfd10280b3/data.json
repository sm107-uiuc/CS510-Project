{"url": "https://towardsdatascience.com/weakly-and-self-supervised-learning-part-4-2fbfd10280b3", "time": 1683012553.2649689, "path": "towardsdatascience.com/weakly-and-self-supervised-learning-part-4-2fbfd10280b3/", "webpage": {"metadata": {"title": "Weakly and Self-supervised Learning \u2014 Part 4 | by Andreas Maier | Towards Data Science", "h1": "Weakly and Self-supervised Learning \u2014 Part 4", "description": "In this lecture, we look into contrastive losses for self-supervised learning, how they can be combined with supervised learning and some even newer ideas."}, "outgoing_paragraph_urls": [{"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning", "paragraph_index": 0}, {"url": "http://autoblog.tf.fau.de/", "anchor_text": "Try it yourself!", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/weakly-and-self-supervised-learning-part-3-b8186679d55e", "anchor_text": "Previous Lecture", "paragraph_index": 1}, {"url": "https://youtu.be/zIDdTstAqWU", "anchor_text": "Watch this Video", "paragraph_index": 1}, {"url": "https://towardsdatascience.com/all-you-want-to-know-about-deep-learning-8d68dcffc258", "anchor_text": "Top Level", "paragraph_index": 1}, {"url": "https://towardsdatascience.com/graph-deep-learning-part-1-e9652e5c4681", "anchor_text": "Next Lecture", "paragraph_index": 1}, {"url": "https://medium.com/@akmaier", "anchor_text": "more essays here", "paragraph_index": 20}, {"url": "https://lme.tf.fau.de/teaching/free-deep-learning-resources/", "anchor_text": "here", "paragraph_index": 20}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj", "anchor_text": "Deep", "paragraph_index": 20}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Learning", "paragraph_index": 20}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj", "anchor_text": "Lecture", "paragraph_index": 20}, {"url": "https://www.youtube.com/c/AndreasMaierTV", "anchor_text": "YouTube", "paragraph_index": 20}, {"url": "https://twitter.com/maier_ak", "anchor_text": "Twitter", "paragraph_index": 20}, {"url": "https://www.facebook.com/andreas.maier.31337", "anchor_text": "Facebook", "paragraph_index": 20}, {"url": "https://www.linkedin.com/in/andreas-maier-a6870b1a6/", "anchor_text": "LinkedIn", "paragraph_index": 20}, {"url": "https://creativecommons.org/licenses/by/4.0/deed.de", "anchor_text": "Creative Commons 4.0 Attribution License", "paragraph_index": 20}, {"url": "http://autoblog.tf.fau.de/", "anchor_text": "AutoBlog", "paragraph_index": 20}], "all_paragraphs": ["These are the lecture notes for FAU\u2019s YouTube Lecture \u201cDeep Learning\u201d. This is a full transcript of the lecture video & matching slides. We hope, you enjoy this as much as the videos. Of course, this transcript was created with deep learning techniques largely automatically and only minor manual modifications were performed. Try it yourself! If you spot mistakes, please let us know!", "Previous Lecture / Watch this Video / Top Level / Next Lecture", "Welcome back to deep learning! So, today we want to discuss in the last part about weakly and self-supervised learning: a couple of new losses that can also help us with the self-supervision. So, let\u2019s see what we have on our slides: Today part number four and the idea is contrastive self-supervised learning.", "In contrastive learning, you try to formulate the learning problem as a kind of matching approach. So here, we have an example from supervised learning. The idea is then to match the correct animal with respect to other animals. The learning task is whether the animal is the same or a different one. This is actually a very powerful form of training because you can avoid a couple of disadvantages in generative or context models. For example, pixel-level losses could overly focus on pixel-based details and pixel-based objectives often assume pixel independence. This reduces the ability to model correlations or complex structures. Here, we essentially can then build abstract models that are also built in a kind of hierarchical way. Now, we have this supervised example but of course, this also works with many of the different pseudo-labels that we\u2019ve seen earlier.", "We can then build this contrastive loss. All that we need is the current sample x some positive sample x plus and then negative samples that are all from a different class. In addition, we need a similarity function s and this could, for example, be a cosine similarity. You could also have a trainable similarity function but generally, you can also use some of the standard similarity functions that we already discussed in this lecture. Furthermore, you want to apply then your network f(x) and compute the similarity. The goal is then to have a higher similarity between the positive sample and the sample under consideration and all the negative ones. This then leads to the contrastive loss. It\u2019s sometimes also called the info normalized cross-entropy loss. There are also other names in literature: The n-pair loss, consistency loss, and the ranking based NCE. It\u2019s essentially a cross-entropy loss for an n-way softmax classifier. The idea here is that you then have essentially the positive examples and this is then normalized by all of the examples. Here, I\u2019m splitting the two still in the first row, but you can then simply see that the split is actually a sum over all of the samples. This then yields a kind of softmax classifier.", "So minimizing the contrastive loss actually maximizes a lower bound on the mutual information between f(x) and f(xplus) as shown in those two references here. There is also a common variation that introduces a temperature hyperparameter that is shown in this example. So, you divide by an additional \u03c4.", "The contrastive losses are very effective and they have two very useful properties. On the one hand, they align. So, similar samples have similar features. On the other hand, they create uniformity because they preserve the maximal information.", "Let\u2019s look into an example of how this can be constructed. This is from [31]. Here, you can see that we can for example start with some sample x. So let\u2019s say you start with a mini-batch of n samples. Then, each sample is transformed with two different data augmentation operations. This leads to 2n augmented samples. So for every sample in the batch, you get two different augmentations. Here, we take t and t\u2019 that we apply to the same sample. This sample is then the same and your transformations t and t\u2019 are taken from a set of augmentations T. Now, you end up with one positive pair for each sample and 2(n-1) negative pairs. Because they\u2019re all different samples, you are able to compute a representation through the base encoder f(x) that then produces some h which is the actual feature representation that we\u2019re interested in. An example of f(x) could be a ResNet-50. On top of this, we have a representation head g that then does an additional dimensionality reduction. Note that both g and f are the same in both branches. So, you could argue this approach has considerable similarities to what is called a Siamese network. So, you then obtain two different sets z subscript i and z subscript j from g. With those z, you can compute your contrastive loss that is then expressed as the similarity between the two z over the temperature parameter \u03c4. You see, this is essentially the same contrastive loss that we have already seen earlier.", "Contrastive losses, of course, can also be combined in a supervised way. This then leads to supervised contrastive learning. Here, the idea is that if you just perform the self-supervised contrastive, you have positive examples versus the negative ones. With the supervised contrastive loss, you can then also embed additional class information.", "So, this has additional positive effects. Let\u2019s see how this is applied. We remain essentially with the same idea of training two coupled networks.", "So let\u2019s summarize a bit of what the difference between contrastive learning and supervised contrastive learning is. Well, in supervised learning, you would essentially have your encoder that is shown here as this gray block. Then, you end up with some description that is a 2048 dimensional vector and you further train a classification head that produces the class dog using the classical cross-entropy loss. Now, in contrastive learning, you would then expand on this. You would have essentially two coupled networks and you have two patches that are either the same or not the same with a different augmentation technique. You train these coupled weight-shared networks to produce 2048 dimensional representations. On top, you have this representation head and you use the contrastive loss on the representation layer itself. Now, if you combine the two supervised and contrastive, you essentially have the same setup as in the contrastive loss, but on the representation layer, you can then augment with an additional loss that works strictly supervised. There, you still have the typical softmax that goes to let\u2019s say a thousand classes and predicts dog in this example. You couple it on the representation layer to be able to fuse contrastive and supervised losses.", "So, the self-supervised has no knowledge about the class labels and it only knows about one positive example. The supervised has knowledge about all the class labels and has many positives per example. This then can be combined and you compute the loss between any sample z having the same class anchor. This then allows you to compute a loss between any sample z subscript j having the same class as the anchor z subscript i. So, the two classes are the same. This leads to the following loss: You can see this is still based on this kind of contrastive loss. Now, we explicitly use the cosine similarity just using the inner product of the two vectors. Also, you can see that we essentially add the red terms here that tell us to only use cases where we have different samples. So i is unequal to j and we want to use samples in this loss where we want to use samples in this loss, where the actual class membership is the same.", "There are a couple of additional things that you have to keep in mind. The vectors z need to be normalized. This means that you want to introduce a scaling where this w is essentially the output of the projection network and you scale it such that it has a unit norm. This then essentially leads to something that you could interpret as a built-in focal loss because the gradient with respect to w is going to be high for hard positives and negatives and small otherwise. So, for one positive and one negative, by the way, it turns out that this loss is proportional to the Euclidean distance squared between the actual observation and the positive minus the Euclidean distance squared between the actual observation and the negative. This is, by the way, also a very common contrastive loss in Siamese networks.", "Let\u2019s have a look at the hyperparameters. It turns out that the supervised contrastive loss is also very stable with respect to hyperparameters and you don\u2019t see these large variations as if you were using only supervised cross-entropy loss in terms of the learning rate, in terms of the optimizer, as well as in terms of the augmentation. If you\u2019re interested in the exact experimental details please have a look at [33].", "What else? Well, the training is about 50% slower than with the cross-entropy loss. It does improve over training with state-of-the-art data augmentation like CutMix and it enables unsupervised clustering in latent space. This allows them to correct for label noise. It also introduces new possibilities for semi-supervised learning and so on.", "Well, is that it? Are there no more ideas? Well, there\u2019s also some interesting idea that is bootstrapping self-supervised learning. You could argue that this is a paradigm change. This is a very new paper, but I\u2019m including it here because it has some very interesting ideas that are very different from the contrastive losses. So it\u2019s called bootstrap your own latent (BYOL) and the problem that they observe is that the choice of pairs is critical. So, often you need to have large batch sizes, memory banks, and custom mining strategies in order to get those pairs right. You also need to have the right augmentation strategy. In bootstrap your own latent, they don\u2019t need negative pairs. they don\u2019t need contrastive loss, and they\u2019re more resilient to changes in batch size and the set of image augmentations compared to contrastive counterparts.", "What\u2019s the idea? Well, they have a similar setup here. They have this kind of network that does the view, representation, projection, and prediction. Now, the idea is that they have an online and a target network. They interact and learn from each other. Now, of course, this is problematic because in theory there\u2019s a trivial solution possible like zero for all images. So, what they then use to counter that is they use a slow-moving average of the online network as the target network. This prevents from collapsing or weights to zero very quickly. As loss, they use the mean square error of the l2 normalized predictions. This is proportional to the cosine distance.", "It\u2019s quite interesting that with this very simple idea, they actually outperform the state-of-the-art and self-supervised learning approaches. So, you can see here colorization and different ideas that we talked about in the beginning and simCLR. They outperform them with their approach. They outperform even simCLR.", "It\u2019s quite interesting to see that also in terms of the number of parameters they are very close to supervised learning with respect to top-1 image accuracy on ImageNet. So, they get very close to state-of-the-art performance with very simple approaches. Actually I\u2019m quite interested in seeing whether this will also be transferable to other domains than ImageNet. In particular, it would be interesting to look into such approaches on medical data, for example. You see that with the self-supervised learning we\u2019re very very close to the state-of-the-art. This is a very active field of research. So, let\u2019s see how these results will develop in the next couple of months and years and which of the approaches will be considered state-of-the-art methods in a couple of months from now.", "Of course, we are now slowly going towards the end of our lecture. But there are still some things coming up next time. In particular, next time we want to talk about how to process graphs. So there\u2019s also something which is a very nice concept called graph convolutions and I want to introduce you to this topic in one of the next videos. Another emerging method that I still want to show is an idea of how to avoid learning everything from scratch and how to embed specific prior knowledge into deep networks. There are also some very cool approaches out there that I think deserve being presented in this lecture. So, I have plenty of references below. So, thank you very much for listening and looking forward to seeing you in the next video. Bye-bye!", "If you liked this post, you can find more essays here, more educational material on Machine Learning here, or have a look at our Deep LearningLecture. I would also appreciate a follow on YouTube, Twitter, Facebook, or LinkedIn in case you want to be informed about more essays, videos, and research in the future. This article is released under the Creative Commons 4.0 Attribution License and can be reprinted and modified if referenced. If you are interested in generating transcripts from video lectures try AutoBlog.", "I do research in Machine Learning. My positions include being Prof @FAU_Germany, President @DataDonors, and Board Member for Science & Technology @TimeMachineEU"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F2fbfd10280b3&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweakly-and-self-supervised-learning-part-4-2fbfd10280b3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweakly-and-self-supervised-learning-part-4-2fbfd10280b3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweakly-and-self-supervised-learning-part-4-2fbfd10280b3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweakly-and-self-supervised-learning-part-4-2fbfd10280b3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/tagged/fau-lecture-notes", "anchor_text": "FAU LECTURE NOTES"}, {"url": "https://akmaier.medium.com/?source=post_page-----2fbfd10280b3--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2fbfd10280b3--------------------------------", "anchor_text": ""}, {"url": "https://akmaier.medium.com/?source=post_page-----2fbfd10280b3--------------------------------", "anchor_text": "Andreas Maier"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb1444918afee&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweakly-and-self-supervised-learning-part-4-2fbfd10280b3&user=Andreas+Maier&userId=b1444918afee&source=post_page-b1444918afee----2fbfd10280b3---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----2fbfd10280b3--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2fbfd10280b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweakly-and-self-supervised-learning-part-4-2fbfd10280b3&user=Andreas+Maier&userId=b1444918afee&source=-----2fbfd10280b3---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2fbfd10280b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweakly-and-self-supervised-learning-part-4-2fbfd10280b3&source=-----2fbfd10280b3---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning"}, {"url": "http://autoblog.tf.fau.de/", "anchor_text": "Try it yourself!"}, {"url": "https://towardsdatascience.com/weakly-and-self-supervised-learning-part-3-b8186679d55e", "anchor_text": "Previous Lecture"}, {"url": "https://youtu.be/zIDdTstAqWU", "anchor_text": "Watch this Video"}, {"url": "https://towardsdatascience.com/all-you-want-to-know-about-deep-learning-8d68dcffc258", "anchor_text": "Top Level"}, {"url": "https://towardsdatascience.com/graph-deep-learning-part-1-e9652e5c4681", "anchor_text": "Next Lecture"}, {"url": "https://github.com/vvo/gifify", "anchor_text": "gifify"}, {"url": "https://youtu.be/b1UTUQpxPSY", "anchor_text": "YouTube"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://github.com/vvo/gifify", "anchor_text": "gifify"}, {"url": "https://youtu.be/6R_oJCq5qMw", "anchor_text": "YouTube"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://medium.com/@akmaier", "anchor_text": "more essays here"}, {"url": "https://lme.tf.fau.de/teaching/free-deep-learning-resources/", "anchor_text": "here"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj", "anchor_text": "Deep"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Learning"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj", "anchor_text": "Lecture"}, {"url": "https://www.youtube.com/c/AndreasMaierTV", "anchor_text": "YouTube"}, {"url": "https://twitter.com/maier_ak", "anchor_text": "Twitter"}, {"url": "https://www.facebook.com/andreas.maier.31337", "anchor_text": "Facebook"}, {"url": "https://www.linkedin.com/in/andreas-maier-a6870b1a6/", "anchor_text": "LinkedIn"}, {"url": "https://creativecommons.org/licenses/by/4.0/deed.de", "anchor_text": "Creative Commons 4.0 Attribution License"}, {"url": "http://autoblog.tf.fau.de/", "anchor_text": "AutoBlog"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----2fbfd10280b3---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----2fbfd10280b3---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----2fbfd10280b3---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----2fbfd10280b3---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/fau-lecture-notes?source=post_page-----2fbfd10280b3---------------fau_lecture_notes-----------------", "anchor_text": "Fau Lecture Notes"}, {"url": "http://creativecommons.org/licenses/by/4.0/", "anchor_text": "Some rights reserved"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2fbfd10280b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweakly-and-self-supervised-learning-part-4-2fbfd10280b3&user=Andreas+Maier&userId=b1444918afee&source=-----2fbfd10280b3---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2fbfd10280b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweakly-and-self-supervised-learning-part-4-2fbfd10280b3&user=Andreas+Maier&userId=b1444918afee&source=-----2fbfd10280b3---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2fbfd10280b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweakly-and-self-supervised-learning-part-4-2fbfd10280b3&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://akmaier.medium.com/?source=post_page-----2fbfd10280b3--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2fbfd10280b3--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb1444918afee&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweakly-and-self-supervised-learning-part-4-2fbfd10280b3&user=Andreas+Maier&userId=b1444918afee&source=post_page-b1444918afee----2fbfd10280b3---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fa5f0dee142a2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweakly-and-self-supervised-learning-part-4-2fbfd10280b3&newsletterV3=b1444918afee&newsletterV3Id=a5f0dee142a2&user=Andreas+Maier&userId=b1444918afee&source=-----2fbfd10280b3---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://akmaier.medium.com/?source=post_page-----2fbfd10280b3--------------------------------", "anchor_text": "Written by Andreas Maier"}, {"url": "https://akmaier.medium.com/followers?source=post_page-----2fbfd10280b3--------------------------------", "anchor_text": "2.2K Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----2fbfd10280b3--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb1444918afee&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweakly-and-self-supervised-learning-part-4-2fbfd10280b3&user=Andreas+Maier&userId=b1444918afee&source=post_page-b1444918afee----2fbfd10280b3---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fa5f0dee142a2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweakly-and-self-supervised-learning-part-4-2fbfd10280b3&newsletterV3=b1444918afee&newsletterV3Id=a5f0dee142a2&user=Andreas+Maier&userId=b1444918afee&source=-----2fbfd10280b3---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/codex/10-ideas-to-make-money-from-large-language-models-86f2cb31bb25?source=author_recirc-----2fbfd10280b3----0---------------------466b115c_bd80_47b4_9328_73c4ad72a036-------", "anchor_text": ""}, {"url": "https://akmaier.medium.com/?source=author_recirc-----2fbfd10280b3----0---------------------466b115c_bd80_47b4_9328_73c4ad72a036-------", "anchor_text": ""}, {"url": "https://akmaier.medium.com/?source=author_recirc-----2fbfd10280b3----0---------------------466b115c_bd80_47b4_9328_73c4ad72a036-------", "anchor_text": "Andreas Maier"}, {"url": "https://medium.com/codex?source=author_recirc-----2fbfd10280b3----0---------------------466b115c_bd80_47b4_9328_73c4ad72a036-------", "anchor_text": "CodeX"}, {"url": "https://medium.com/codex/10-ideas-to-make-money-from-large-language-models-86f2cb31bb25?source=author_recirc-----2fbfd10280b3----0---------------------466b115c_bd80_47b4_9328_73c4ad72a036-------", "anchor_text": "10 Ideas to Make Money from Large Language ModelsLarge Language Models work, but what can we do with them?"}, {"url": "https://medium.com/codex/10-ideas-to-make-money-from-large-language-models-86f2cb31bb25?source=author_recirc-----2fbfd10280b3----0---------------------466b115c_bd80_47b4_9328_73c4ad72a036-------", "anchor_text": "\u00b73 min read\u00b7Dec 11, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fcodex%2F86f2cb31bb25&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fcodex%2F10-ideas-to-make-money-from-large-language-models-86f2cb31bb25&user=Andreas+Maier&userId=b1444918afee&source=-----86f2cb31bb25----0-----------------clap_footer----466b115c_bd80_47b4_9328_73c4ad72a036-------", "anchor_text": ""}, {"url": "https://medium.com/codex/10-ideas-to-make-money-from-large-language-models-86f2cb31bb25?source=author_recirc-----2fbfd10280b3----0---------------------466b115c_bd80_47b4_9328_73c4ad72a036-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F86f2cb31bb25&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fcodex%2F10-ideas-to-make-money-from-large-language-models-86f2cb31bb25&source=-----2fbfd10280b3----0-----------------bookmark_preview----466b115c_bd80_47b4_9328_73c4ad72a036-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----2fbfd10280b3----1---------------------466b115c_bd80_47b4_9328_73c4ad72a036-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----2fbfd10280b3----1---------------------466b115c_bd80_47b4_9328_73c4ad72a036-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----2fbfd10280b3----1---------------------466b115c_bd80_47b4_9328_73c4ad72a036-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----2fbfd10280b3----1---------------------466b115c_bd80_47b4_9328_73c4ad72a036-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----2fbfd10280b3----1---------------------466b115c_bd80_47b4_9328_73c4ad72a036-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----2fbfd10280b3----1---------------------466b115c_bd80_47b4_9328_73c4ad72a036-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----466b115c_bd80_47b4_9328_73c4ad72a036-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----2fbfd10280b3----1---------------------466b115c_bd80_47b4_9328_73c4ad72a036-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----2fbfd10280b3----1-----------------bookmark_preview----466b115c_bd80_47b4_9328_73c4ad72a036-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----2fbfd10280b3----2---------------------466b115c_bd80_47b4_9328_73c4ad72a036-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----2fbfd10280b3----2---------------------466b115c_bd80_47b4_9328_73c4ad72a036-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----2fbfd10280b3----2---------------------466b115c_bd80_47b4_9328_73c4ad72a036-------", "anchor_text": "Jacob Marks, Ph.D."}, {"url": "https://towardsdatascience.com/?source=author_recirc-----2fbfd10280b3----2---------------------466b115c_bd80_47b4_9328_73c4ad72a036-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----2fbfd10280b3----2---------------------466b115c_bd80_47b4_9328_73c4ad72a036-------", "anchor_text": "How I Turned My Company\u2019s Docs into a Searchable Database with OpenAIAnd how you can do the same with your docs"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----2fbfd10280b3----2---------------------466b115c_bd80_47b4_9328_73c4ad72a036-------", "anchor_text": "15 min read\u00b7Apr 25"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&user=Jacob+Marks%2C+Ph.D.&userId=f7dc0c0eae92&source=-----4f2d34bd8736----2-----------------clap_footer----466b115c_bd80_47b4_9328_73c4ad72a036-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----2fbfd10280b3----2---------------------466b115c_bd80_47b4_9328_73c4ad72a036-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "20"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&source=-----2fbfd10280b3----2-----------------bookmark_preview----466b115c_bd80_47b4_9328_73c4ad72a036-------", "anchor_text": ""}, {"url": "https://medium.com/codex/gradient-descent-and-back-tracking-line-search-d8bd120bd625?source=author_recirc-----2fbfd10280b3----3---------------------466b115c_bd80_47b4_9328_73c4ad72a036-------", "anchor_text": ""}, {"url": "https://akmaier.medium.com/?source=author_recirc-----2fbfd10280b3----3---------------------466b115c_bd80_47b4_9328_73c4ad72a036-------", "anchor_text": ""}, {"url": "https://akmaier.medium.com/?source=author_recirc-----2fbfd10280b3----3---------------------466b115c_bd80_47b4_9328_73c4ad72a036-------", "anchor_text": "Andreas Maier"}, {"url": "https://medium.com/codex?source=author_recirc-----2fbfd10280b3----3---------------------466b115c_bd80_47b4_9328_73c4ad72a036-------", "anchor_text": "CodeX"}, {"url": "https://medium.com/codex/gradient-descent-and-back-tracking-line-search-d8bd120bd625?source=author_recirc-----2fbfd10280b3----3---------------------466b115c_bd80_47b4_9328_73c4ad72a036-------", "anchor_text": "Gradient Descent and Back-tracking Line SearchAn Introduction to Optimization using Gradient Descent"}, {"url": "https://medium.com/codex/gradient-descent-and-back-tracking-line-search-d8bd120bd625?source=author_recirc-----2fbfd10280b3----3---------------------466b115c_bd80_47b4_9328_73c4ad72a036-------", "anchor_text": "\u00b713 min read\u00b7Apr 10, 2021"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fcodex%2Fd8bd120bd625&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fcodex%2Fgradient-descent-and-back-tracking-line-search-d8bd120bd625&user=Andreas+Maier&userId=b1444918afee&source=-----d8bd120bd625----3-----------------clap_footer----466b115c_bd80_47b4_9328_73c4ad72a036-------", "anchor_text": ""}, {"url": "https://medium.com/codex/gradient-descent-and-back-tracking-line-search-d8bd120bd625?source=author_recirc-----2fbfd10280b3----3---------------------466b115c_bd80_47b4_9328_73c4ad72a036-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd8bd120bd625&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fcodex%2Fgradient-descent-and-back-tracking-line-search-d8bd120bd625&source=-----2fbfd10280b3----3-----------------bookmark_preview----466b115c_bd80_47b4_9328_73c4ad72a036-------", "anchor_text": ""}, {"url": "https://akmaier.medium.com/?source=post_page-----2fbfd10280b3--------------------------------", "anchor_text": "See all from Andreas Maier"}, {"url": "https://towardsdatascience.com/?source=post_page-----2fbfd10280b3--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://towardsdatascience.com/using-transformers-for-computer-vision-6f764c5a078b?source=read_next_recirc-----2fbfd10280b3----0---------------------9b0606e7_98c7_429f_a26c_3f09f25682e0-------", "anchor_text": ""}, {"url": "https://wolfecameron.medium.com/?source=read_next_recirc-----2fbfd10280b3----0---------------------9b0606e7_98c7_429f_a26c_3f09f25682e0-------", "anchor_text": ""}, {"url": "https://wolfecameron.medium.com/?source=read_next_recirc-----2fbfd10280b3----0---------------------9b0606e7_98c7_429f_a26c_3f09f25682e0-------", "anchor_text": "Cameron R. Wolfe"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----2fbfd10280b3----0---------------------9b0606e7_98c7_429f_a26c_3f09f25682e0-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/using-transformers-for-computer-vision-6f764c5a078b?source=read_next_recirc-----2fbfd10280b3----0---------------------9b0606e7_98c7_429f_a26c_3f09f25682e0-------", "anchor_text": "Using Transformers for Computer VisionAre Vision Transformers actually useful?"}, {"url": "https://towardsdatascience.com/using-transformers-for-computer-vision-6f764c5a078b?source=read_next_recirc-----2fbfd10280b3----0---------------------9b0606e7_98c7_429f_a26c_3f09f25682e0-------", "anchor_text": "\u00b713 min read\u00b7Oct 5, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6f764c5a078b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-transformers-for-computer-vision-6f764c5a078b&user=Cameron+R.+Wolfe&userId=28aa6026c553&source=-----6f764c5a078b----0-----------------clap_footer----9b0606e7_98c7_429f_a26c_3f09f25682e0-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/using-transformers-for-computer-vision-6f764c5a078b?source=read_next_recirc-----2fbfd10280b3----0---------------------9b0606e7_98c7_429f_a26c_3f09f25682e0-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "4"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6f764c5a078b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-transformers-for-computer-vision-6f764c5a078b&source=-----2fbfd10280b3----0-----------------bookmark_preview----9b0606e7_98c7_429f_a26c_3f09f25682e0-------", "anchor_text": ""}, {"url": "https://jehillparikh.medium.com/u-nets-with-attention-c8d7e9bf2416?source=read_next_recirc-----2fbfd10280b3----1---------------------9b0606e7_98c7_429f_a26c_3f09f25682e0-------", "anchor_text": ""}, {"url": "https://jehillparikh.medium.com/?source=read_next_recirc-----2fbfd10280b3----1---------------------9b0606e7_98c7_429f_a26c_3f09f25682e0-------", "anchor_text": ""}, {"url": "https://jehillparikh.medium.com/?source=read_next_recirc-----2fbfd10280b3----1---------------------9b0606e7_98c7_429f_a26c_3f09f25682e0-------", "anchor_text": "Jehill Parikh"}, {"url": "https://jehillparikh.medium.com/u-nets-with-attention-c8d7e9bf2416?source=read_next_recirc-----2fbfd10280b3----1---------------------9b0606e7_98c7_429f_a26c_3f09f25682e0-------", "anchor_text": "U-Nets with attentionU-Net are popular NN architecture which are employed for many applications and were initially developed for medical image segmentation."}, {"url": "https://jehillparikh.medium.com/u-nets-with-attention-c8d7e9bf2416?source=read_next_recirc-----2fbfd10280b3----1---------------------9b0606e7_98c7_429f_a26c_3f09f25682e0-------", "anchor_text": "\u00b72 min read\u00b7Nov 15, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2Fc8d7e9bf2416&operation=register&redirect=https%3A%2F%2Fjehillparikh.medium.com%2Fu-nets-with-attention-c8d7e9bf2416&user=Jehill+Parikh&userId=c972081b627e&source=-----c8d7e9bf2416----1-----------------clap_footer----9b0606e7_98c7_429f_a26c_3f09f25682e0-------", "anchor_text": ""}, {"url": "https://jehillparikh.medium.com/u-nets-with-attention-c8d7e9bf2416?source=read_next_recirc-----2fbfd10280b3----1---------------------9b0606e7_98c7_429f_a26c_3f09f25682e0-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc8d7e9bf2416&operation=register&redirect=https%3A%2F%2Fjehillparikh.medium.com%2Fu-nets-with-attention-c8d7e9bf2416&source=-----2fbfd10280b3----1-----------------bookmark_preview----9b0606e7_98c7_429f_a26c_3f09f25682e0-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----2fbfd10280b3----0---------------------9b0606e7_98c7_429f_a26c_3f09f25682e0-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----2fbfd10280b3----0---------------------9b0606e7_98c7_429f_a26c_3f09f25682e0-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----2fbfd10280b3----0---------------------9b0606e7_98c7_429f_a26c_3f09f25682e0-------", "anchor_text": "The PyCoach"}, {"url": "https://artificialcorner.com/?source=read_next_recirc-----2fbfd10280b3----0---------------------9b0606e7_98c7_429f_a26c_3f09f25682e0-------", "anchor_text": "Artificial Corner"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----2fbfd10280b3----0---------------------9b0606e7_98c7_429f_a26c_3f09f25682e0-------", "anchor_text": "You\u2019re Using ChatGPT Wrong! Here\u2019s How to Be Ahead of 99% of ChatGPT UsersMaster ChatGPT by learning prompt engineering."}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----2fbfd10280b3----0---------------------9b0606e7_98c7_429f_a26c_3f09f25682e0-------", "anchor_text": "\u00b77 min read\u00b7Mar 17"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fartificial-corner%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&user=The+PyCoach&userId=fb44e21903f3&source=-----886a50dabc54----0-----------------clap_footer----9b0606e7_98c7_429f_a26c_3f09f25682e0-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----2fbfd10280b3----0---------------------9b0606e7_98c7_429f_a26c_3f09f25682e0-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "276"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&source=-----2fbfd10280b3----0-----------------bookmark_preview----9b0606e7_98c7_429f_a26c_3f09f25682e0-------", "anchor_text": ""}, {"url": "https://medium.com/mlearning-ai/understanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f?source=read_next_recirc-----2fbfd10280b3----1---------------------9b0606e7_98c7_429f_a26c_3f09f25682e0-------", "anchor_text": ""}, {"url": "https://medium.com/@martin-thissen?source=read_next_recirc-----2fbfd10280b3----1---------------------9b0606e7_98c7_429f_a26c_3f09f25682e0-------", "anchor_text": ""}, {"url": "https://medium.com/@martin-thissen?source=read_next_recirc-----2fbfd10280b3----1---------------------9b0606e7_98c7_429f_a26c_3f09f25682e0-------", "anchor_text": "Martin Thissen"}, {"url": "https://medium.com/mlearning-ai?source=read_next_recirc-----2fbfd10280b3----1---------------------9b0606e7_98c7_429f_a26c_3f09f25682e0-------", "anchor_text": "MLearning.ai"}, {"url": "https://medium.com/mlearning-ai/understanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f?source=read_next_recirc-----2fbfd10280b3----1---------------------9b0606e7_98c7_429f_a26c_3f09f25682e0-------", "anchor_text": "Understanding and Coding the Attention Mechanism \u2014 The Magic Behind TransformersIn this article, I\u2019ll give you an introduction to the attention mechanism and show you how to code the attention mechanism yourself."}, {"url": "https://medium.com/mlearning-ai/understanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f?source=read_next_recirc-----2fbfd10280b3----1---------------------9b0606e7_98c7_429f_a26c_3f09f25682e0-------", "anchor_text": "\u00b712 min read\u00b7Dec 6, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fmlearning-ai%2Ffe707a85cc3f&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fmlearning-ai%2Funderstanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f&user=Martin+Thissen&userId=f99c73950195&source=-----fe707a85cc3f----1-----------------clap_footer----9b0606e7_98c7_429f_a26c_3f09f25682e0-------", "anchor_text": ""}, {"url": "https://medium.com/mlearning-ai/understanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f?source=read_next_recirc-----2fbfd10280b3----1---------------------9b0606e7_98c7_429f_a26c_3f09f25682e0-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffe707a85cc3f&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fmlearning-ai%2Funderstanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f&source=-----2fbfd10280b3----1-----------------bookmark_preview----9b0606e7_98c7_429f_a26c_3f09f25682e0-------", "anchor_text": ""}, {"url": "https://medium.com/@steinsfu/diffusion-model-clearly-explained-cd331bd41166?source=read_next_recirc-----2fbfd10280b3----2---------------------9b0606e7_98c7_429f_a26c_3f09f25682e0-------", "anchor_text": ""}, {"url": "https://medium.com/@steinsfu?source=read_next_recirc-----2fbfd10280b3----2---------------------9b0606e7_98c7_429f_a26c_3f09f25682e0-------", "anchor_text": ""}, {"url": "https://medium.com/@steinsfu?source=read_next_recirc-----2fbfd10280b3----2---------------------9b0606e7_98c7_429f_a26c_3f09f25682e0-------", "anchor_text": "Steins"}, {"url": "https://medium.com/@steinsfu/diffusion-model-clearly-explained-cd331bd41166?source=read_next_recirc-----2fbfd10280b3----2---------------------9b0606e7_98c7_429f_a26c_3f09f25682e0-------", "anchor_text": "Diffusion Model Clearly Explained!How does AI artwork work? Understanding the tech behind the rise of AI-generated art."}, {"url": "https://medium.com/@steinsfu/diffusion-model-clearly-explained-cd331bd41166?source=read_next_recirc-----2fbfd10280b3----2---------------------9b0606e7_98c7_429f_a26c_3f09f25682e0-------", "anchor_text": "\u00b77 min read\u00b7Dec 26, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2Fcd331bd41166&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40steinsfu%2Fdiffusion-model-clearly-explained-cd331bd41166&user=Steins&userId=a36be384d77d&source=-----cd331bd41166----2-----------------clap_footer----9b0606e7_98c7_429f_a26c_3f09f25682e0-------", "anchor_text": ""}, {"url": "https://medium.com/@steinsfu/diffusion-model-clearly-explained-cd331bd41166?source=read_next_recirc-----2fbfd10280b3----2---------------------9b0606e7_98c7_429f_a26c_3f09f25682e0-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcd331bd41166&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40steinsfu%2Fdiffusion-model-clearly-explained-cd331bd41166&source=-----2fbfd10280b3----2-----------------bookmark_preview----9b0606e7_98c7_429f_a26c_3f09f25682e0-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----2fbfd10280b3----3---------------------9b0606e7_98c7_429f_a26c_3f09f25682e0-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----2fbfd10280b3----3---------------------9b0606e7_98c7_429f_a26c_3f09f25682e0-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----2fbfd10280b3----3---------------------9b0606e7_98c7_429f_a26c_3f09f25682e0-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----2fbfd10280b3----3---------------------9b0606e7_98c7_429f_a26c_3f09f25682e0-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----2fbfd10280b3----3---------------------9b0606e7_98c7_429f_a26c_3f09f25682e0-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----2fbfd10280b3----3---------------------9b0606e7_98c7_429f_a26c_3f09f25682e0-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----3-----------------clap_footer----9b0606e7_98c7_429f_a26c_3f09f25682e0-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----2fbfd10280b3----3---------------------9b0606e7_98c7_429f_a26c_3f09f25682e0-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----2fbfd10280b3----3-----------------bookmark_preview----9b0606e7_98c7_429f_a26c_3f09f25682e0-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----2fbfd10280b3--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----2fbfd10280b3--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----2fbfd10280b3--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----2fbfd10280b3--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----2fbfd10280b3--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----2fbfd10280b3--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----2fbfd10280b3--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----2fbfd10280b3--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----2fbfd10280b3--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----2fbfd10280b3--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}