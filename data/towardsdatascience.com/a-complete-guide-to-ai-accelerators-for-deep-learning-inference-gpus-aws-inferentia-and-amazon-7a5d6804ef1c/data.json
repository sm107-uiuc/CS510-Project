{"url": "https://towardsdatascience.com/a-complete-guide-to-ai-accelerators-for-deep-learning-inference-gpus-aws-inferentia-and-amazon-7a5d6804ef1c", "time": 1683015474.011531, "path": "towardsdatascience.com/a-complete-guide-to-ai-accelerators-for-deep-learning-inference-gpus-aws-inferentia-and-amazon-7a5d6804ef1c/", "webpage": {"metadata": {"title": "A complete guide to AI accelerators for deep learning inference \u2014 GPUs, AWS Inferentia and Amazon Elastic Inference | by Shashank Prasanna | Towards Data Science", "h1": "A complete guide to AI accelerators for deep learning inference \u2014 GPUs, AWS Inferentia and Amazon Elastic Inference", "description": "An AI accelerator is a dedicated processor designed to accelerate machine learning computations. Machine learning, and particularly its subset, deep learning is primarily composed of a large number\u2026"}, "outgoing_paragraph_urls": [{"url": "https://developer.nvidia.com/blog/cuda-refresher-reviewing-the-origins-of-gpu-computing/", "anchor_text": "blog post from NVIDIA explains", "paragraph_index": 31}, {"url": "https://developer.nvidia.com/deep-learning-performance-training-inference", "anchor_text": "I\u2019ll refer you to NVIDIA\u2019s website for their latest training and inference benchmarks for popular models", "paragraph_index": 33}, {"url": "https://towardsdatascience.com/choosing-the-right-gpu-for-deep-learning-on-aws-d69c157d8c86?source=friends_link&sk=a7b056b6fdbed24ecb5e23f6ea8625cc", "anchor_text": "Choosing the right GPU for deep learning on AWS", "paragraph_index": 37}, {"url": "https://code.google.com/archive/p/cuda-convnet/", "anchor_text": "cuda-convnet", "paragraph_index": 41}, {"url": "https://github.com/shashankprasanna/ai-accelerators-examples/blob/main/gpu-tf-tensorrt-resnet50.ipynb", "anchor_text": "full implementation on GitHub", "paragraph_index": 53}, {"url": "https://perspectives.mvdirona.com/2018/11/aws-inferentia-machine-learning-processor/", "anchor_text": "AWS Inferentia Machine Learning Processor", "paragraph_index": 64}, {"url": "https://en.wikipedia.org/wiki/Systolic_array", "anchor_text": "systolic-array", "paragraph_index": 67}, {"url": "https://aws.amazon.com/blogs/machine-learning/improving-performance-for-deep-learning-based-object-detection-with-an-aws-neuron-compiled-yolov4-model-on-aws-inferentia/", "anchor_text": "YOLOv4 model", "paragraph_index": 70}, {"url": "https://aws.amazon.com/blogs/machine-learning/deploying-tensorflow-openpose-on-aws-inferentia-based-inf1-instances-for-significant-price-performance-improvements/", "anchor_text": "OpenPose", "paragraph_index": 70}, {"url": "https://github.com/aws/aws-neuron-sdk/tree/master/src/examples", "anchor_text": "examples for BERT and SSD for TensorFlow, MXNet and PyTorch", "paragraph_index": 70}, {"url": "https://github.com/aws/aws-neuron-sdk/tree/master/release-notes/neuron-cc-ops", "anchor_text": "supported operators for your framework", "paragraph_index": 71}, {"url": "https://github.com/aws/aws-neuron-sdk/blob/master/release-notes/neuron-cc-ops/neuron-cc-ops-tensorflow.md", "anchor_text": "TensorFlow ops accelerated on AWS Inferentia", "paragraph_index": 71}, {"url": "https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ThreadPoolExecutor", "anchor_text": "concurrent.futures.ThreadPoolExecutor", "paragraph_index": 84}, {"url": "https://github.com/aws/aws-neuron-sdk/blob/master/docs/neuron-install-guide.md", "anchor_text": "install the SDK and the neuron-accelerated frameworks and libraries", "paragraph_index": 88}, {"url": "https://github.com/shashankprasanna/ai-accelerators-examples/blob/main/inf1-neuron-sdk-resnet50.ipynb", "anchor_text": "in this Jupyter Notebook", "paragraph_index": 92}, {"url": "https://aws.amazon.com/machine-learning/elastic-inference/", "anchor_text": "Amazon Elastic Inference (EI)", "paragraph_index": 94}, {"url": "https://aws.amazon.com/blogs/machine-learning/category/artificial-intelligence/amazon-elastic-inference/", "anchor_text": "blog posts that talk about performance and cost savings", "paragraph_index": 102}, {"url": "https://docs.aws.amazon.com/dlami/latest/devguide/what-is-dlami.html", "anchor_text": "AWS Deep Learning AMI", "paragraph_index": 104}, {"url": "https://docs.aws.amazon.com/elastic-inference/latest/developerguide/ei-tensorflow.html", "anchor_text": "installing it manually,", "paragraph_index": 104}, {"url": "https://medium.com/@shashankprasanna", "anchor_text": "medium", "paragraph_index": 119}, {"url": "https://twitter.com/shshnkp", "anchor_text": "@shshnkp", "paragraph_index": 120}, {"url": "https://www.linkedin.com/in/shashankprasanna/", "anchor_text": "LinkedIn", "paragraph_index": 120}, {"url": "http://shashankprasanna.com", "anchor_text": "shashankprasanna.com", "paragraph_index": 121}], "all_paragraphs": ["An AI accelerator is a dedicated processor designed to accelerate machine learning computations. Machine learning, and particularly its subset, deep learning is primarily composed of a large number of linear algebra computations, (i.e. matrix-matrix, matrix-vector operations) and these operations can be easily parallelized. AI accelerators are specialized hardware designed to accelerate these basic machine learning computations and improve performance, reduce latency and reduce cost of deploying machine learning based applications.", "Let\u2019s say you have an ML model as part of your software application. The prediction step (or inference) is often the most time consuming part of your application that directly affects user experience. A model that takes several hundreds of milliseconds to generate text translations or apply filters to images or generate product recommendations, can drive users away from your \u201csluggish\u201d, \u201cslow\u201d, \u201cfrustrating to use\u201d app.", "By speeding up inference, you can reduce the overall application latency and deliver an app experience that can be described as \u201csmooth\u201d, \u201csnappy\u201d, and \u201cdelightful to use\u201d. And you can speed up inference by offloading ML model prediction computation to an AI accelerator.", "With great market needs comes great many product alternatives, so naturally there is more than one way to accelerate your ML models in the cloud.", "In this blog post, I\u2019ll explore three popular options:", "Choosing the right type of hardware acceleration for your workload can be a difficult choice to make. Through the rest of this post, I\u2019ll walk you through various considerations such as target throughput, latency, cost budget, model type and size, choice of framework, and others to help you make your decision. I\u2019ll also present plenty of code examples and discuss developer friendliness and ease of use with options.", "Disclaimer: Opinions and recommendations in this article are my own and do not reflect the views of my current or past employers.", "In the early days of computing (in the 70s and 80s), to speed up math computations on your computer, you paired a CPU (Central Processing Unit) with an FPU (floating-point unit) aka math coprocessor. The idea was simple \u2014 allow the CPU to offload complex floating point mathematical operations to a specially designed chip, so that the CPU could focus on executing the rest of the application program, run the operating system etc. Since the system had different types of processors (the CPU and the math coprocessor) the setup was sometimes referred to as heterogeneous computing.", "Fast forward to the 90s, and the CPUs got faster, better and more efficient, and started to ship with integrated floating-point hardware. The simpler system prevailed, and coprocessors and heterogeneous computing fell out of fashion for the regular user.", "Around the same time specific types of workloads started to get more complex. Designers demanded better graphics, engineers and scientists demanded faster computers for data processing, modeling and simulations. This meant there was some need (and a market) for high-performance processors that could accelerate \u201cspecial programs\u201d much faster than a CPU, freeing up the CPU to do other things. Computer graphics was an early example of workload being offloaded to a special processor. You may know this special processor by its common name, the venerable GPU.", "The early 2010s saw yet another class of workloads \u2014 deep learning, or machine learning with deep neural networks \u2014 that needed hardware acceleration to be viable, much like computer graphics. GPUs were already in the market and over the years have become highly programmable unlike the early GPUs which were fixed function processors. Naturally, ML practitioners started using GPUs to accelerate deep learning training and inference.", "Today\u2019s deep learning inference acceleration landscape is much more interesting. CPUs acquired support for advanced vector extensions (AVX-512) to accelerate matrix math computations common in deep learning. GPUs acquired new capabilities such as support for reduced precision arithmetic (FP16 and INT8) further accelerating inference.", "In addition to CPUs and GPUs, today you also have access to specialized hardware, with custom designed silicon built just for deep learning inference. These specialized processors, also called Application Specific Integrated Circuits or ASICs can be far more performant and cost effective compared to general purpose processors, if your workload is supported by the processor. A great example of such specialized processors is AWS Inferentia, a custom-designed ASIC by AWS for accelerating deep learning inference.", "The right choice of hardware acceleration for your application may not be obvious at first. In the next section, we\u2019ll discuss the benefits of each approach and considerations such as throughput, latency, cost and other factors that will affect your choice.", "It\u2019s hard to answer general questions such as \u201cis GPU better than CPU?\u201d or \u201cis CPU cheaper than a GPU\u201d or \u201cis an ASIC always faster than a GPU\u201d. There really isn\u2019t a single hardware solution that works well for every use case and the answer depends on your workload and several considerations:", "While considerations such as model support and target latency are objective, ease of use can be very subjective. Therefore, I caution against general recommendation that doesn\u2019t consider all of the above for your specific application. Such high level recommendations tends to be biased.", "One way to categorize AI accelerators is based on how programmable they are. On the \u201cfully programmable\u201d end of the spectrum there are CPUs. As general purpose processors, you can pretty much write custom code for your machine learning model with custom layers, architectures and operations.", "On the other end of the spectrum are ASICs such as AWS Inferentia that have a fixed set of supported operations exposed via it\u2019s AWS Neuron SDK compiler. Somewhere in between, but closer to ASICs are GPUs, that are far more programmable than ASICs, but far less general purpose than CPUs. There is always going to be some trade off between being general purpose and delivering performance.", "If you\u2019re pushing the boundaries of deep learning research with custom neural network operations, you may need to author custom code for custom operations. And you\u2019d typically do this in high level languages like Python.", "Most AI accelerators can\u2019t automatically accelerate custom code written in high level languages and therefore that piece of code will fall back to CPUs for execution, reducing the overall inference performance.", "NVIDIA GPUs have the advantage that if you want more performance out of your custom code you can reimplement them using CUDA programming language and run them on GPUs. But if your ASIC\u2019s compiler doesn\u2019t support the operations you need, then CPU fall back may result in lower performance.", "In general specialized processors such as AWS Inferentia tend to offer lower price/performance ratio and improve latency vs. general purpose processors. But in the world of AI acceleration, all solutions can be competitive, depending on the type of workload.", "GPUs are throughput processors, and can deliver high throughput for a specified latency. If latency is not critical (batch processing, offline inference) then GPU utilization can be kept high, making them the most cost effective option in the cloud. CPUs are not parallel throughput devices, but for real time inference of smaller models, CPUs can be the most cost effective, as long the inference latency is under your target latency budget. AWS Inferentia\u2019s performance and lower cost could make it the most cost effective and performant option vs both CPUs and GPUs if your model is fully supported by AWS Neuron SDK compiler for acceleration on AWS Inferentia.", "This is indeed a nuanced topic and is very workload dependent. In the subsequent sections we\u2019ll take a closer look at performance, latency and cost for each accelerator. If a specific choice doesn\u2019t work for you, no problem, it\u2019s easy to switch options in the cloud till you find the right option for you.", "To accelerate your models on AI accelerators, you typically have to go through a compilation step that analyzes the computational graph and optimizes it for the target hardware to get the best performance. When deploying on a CPU, the deep learning framework has everything you need, so additional SDKs and compilers are typically not required.", "If you\u2019re deploying to a GPU, you can rely on a deep learning framework to accelerate your model for inference, but you\u2019ll be leaving performance on the table. To get the most out of your GPU, you\u2019ll have to use a dedicated inference compiler such as NVIDIA TensorRT.", "In some cases, you can get over 10 times extra performance vs. using the deep learning framework (see figure). We\u2019ll see later in the code examples section, how you can reproduce these results.", "NVIDIA TensorRT is two things \u2014 inference compiler and a runtime engine. By compiling your model with TensorRT, you can get better performance and lower latency since it performs a number of optimizations such as graph optimization and quantizations. Likewise, when targeting AWS Inferentia, AWS Neuron SDK compiler will perform similar optimizations to get the most out of your AWS Inferentia processor.", "Let\u2019s dig a little deeper into each of these AI accelerator options", "You train your model on GPUs, so it\u2019s natural to consider GPUs for inference deployment. After all, GPUs substantially speed up deep learning training, and inference is just the forward pass of your neural network that\u2019s already accelerated on GPU. This is true, and GPUs are indeed an excellent hardware accelerator for inference.", "First, let\u2019s talk about what GPUs really are.", "GPUs are first and foremost throughput processors, as this blog post from NVIDIA explains. They were designed to exploit inherent parallelism in algorithms and accelerate them by computing them in parallel. GPUs started out as specialized processors for computer graphics, but today\u2019s GPUs have evolved into programmable processors, also called General Purpose GPU (GPGPU). They are still specialized parallel processors, but also highly programmable for a narrow range of applications which can be accelerated with parallelization.", "As it turns out, the high-performance computing (HPC) community had been using GPUs to accelerate linear algebra calculations long before deep learning. Deep neural networks computations are primarily composed of similar linear algebra computations, so a GPU for deep learning was a solution looking for a problem. It is no surprise that Alex Krizhevsky\u2019s AlexNet deep neural network that won the ImageNet 2012 competition and (re)introduced the world to deep learning was trained on readily available, programmable consumer GPUs by NVIDIA.", "GPUs have gotten much faster since then and I\u2019ll refer you to NVIDIA\u2019s website for their latest training and inference benchmarks for popular models. While these benchmarks are a good indication of what a GPU is capable of, your decision may hinge on other considerations discussed below.", "Since GPUs are throughput devices, if your objective is to maximize sheer throughput, they can deliver best in class throughput per desired latency, depending on the GPU type and model being deployed. An example of a use-case where GPUs absolutely shine is offline or batch inference. GPUs will also deliver some of the lowest latencies for prediction for small batches, but if you are unable to keep your GPU utilization at its maximum at all times, due to say sporadic inference request (fluctuating customer demand), your cost / inference request goes up (because you are delivering fewer requests for the same GPU instance cost). For these situations you\u2019re better off using Amazon Elastic Inference which lets you access just enough GPU acceleration for lower cost.", "In the example section we\u2019ll see comparision of GPU performance across different precisions (FP32, FP16, INT8).", "On AWS you can launch 18 different Amazon EC2 GPU instances with different NVIDIA GPUs, number of vCPUs, system memory and network bandwidth. Two of the most popular GPUs for deep learning inference are the NVIDIA T4 GPUs offered by G4 EC2 instance type and NVIDIA V100 GPUs offered by P3 EC2 instance type.", "For a fully summary of all GPU instance type of AWS read my earlier blog post: Choosing the right GPU for deep learning on AWS", "G4 instance type should be the go-to GPU instance for deep learning inference deployment.", "Based on the NVIDIA Turing architecture, NVIDIA T4 GPUs feature FP64, FP32, FP16, Tensor Cores (mixed-precision), and INT8 precision types. They also have 16 GB of GPU memory which can be plenty for most models and combined with reduced precision support.", "If you need more throughput or need more memory per GPU, then P3 instance types offer a more powerful NVIDIA V100 GPU and with p3dn.24xlarge instance size, you can get access to NVIDIA V100 with up to 32 GB of GPU memory for large models or large images or other datasets.", "Unlike ASICs such as AWS Inferentia which are fixed function processors, a developer can use NVIDIA\u2019s CUDA programming model to code up custom layers that can be accelerated on an NVIDIA GPU. This is exactly what Alex Krizhevsky did with AlexNet in 2012. He hand coded custom CUDA kernels to train his neural network on GPU. He called his framework cuda-convnet and you could say cuda-convnet was the very first deep learning framework. If you\u2019re pushing the boundary of deep learning and don\u2019t want to leave performance on the table a GPU is the best option for you.", "Programmability with performance is one of GPUs greatest strengths", "Of course, you don\u2019t need to write low-level GPU code to do deep learning. NVIDIA has made neural network primitives available via libraries such as cuDNN and cuBLAS and deep learning frameworks such as TensorFlow, PyTorch and MXNet use these libraries under the hood so you get GPU acceleration for free by simply using these frameworks. This is why GPUs score high marks for ease of use and programmability.", "If you really want to get the best performance out of your GPUs, NVIDIA offers TensorRT, a model compiler for inference deployment. Does additional optimizations to a trained model, and a full list is available on NVIDIA\u2019s TensorRT website. The key optimizations to note are:", "Deploying with FP16 is straight forward with NVIDIA TensorRT. The TensorRT compiler will automatically quantize your models during the compilation step.", "To deploy with INT8 precision, the weights and activations of the model need to be quantized so that floating point values can be converted into integers using appropriate ranges. You have two options.", "The following examples was tested on Amazon EC2 g4dn.xlarge using the following AWS Deep Learning AMI: Deep Learning AMI (Ubuntu 18.04) Version 35.0. To run TensorRT, I used the following NVIDIA TensorFlow Docker image: nvcr.io/nvidia/tensorflow:20.08-tf2-py3", "Dataset: ImageNet Validation dataset with 50000 test images, converted to TFRecordModel: TensorFlow implementation of ResNet50", "You can find the full implementation for the examples below on this Jupyter Notebook:", "TensorFlow\u2019s native GPU acceleration support just works out of the box, with no additional setup. You won\u2019t get the additional performance you can get with NVIDIA TensorRT, but you can\u2019t argue with how easy life becomes when things just work.", "Running inference with frameworks\u2019 native GPU support takes all of 3 lines of code:", "But you\u2019re really leaving performance on the table (some times 10x the performance). To increase the performance and utilization of your GPU, you have to use an inference compiler and runtime like NVIDIA TensorRT.", "The following code shows how to compile your model with TensorRT. You can find the full implementation on GitHub", "TensorRT compilation has the following steps:", "Below is a comparison of accuracy and performance of TensorFlow ResNet50 inference with:", "I measured not just performance but also accuracy, since reducing precision means there is information loss. On the ImageNet test dataset we see negligible loss in accuracy across all precisions, with minor boost in throughput. Your mileage may vary for your model.", "In Example 1, we tested the performance offline, but in most cases you\u2019ll be hosting your model in the cloud as an endpoint that client applications can submit inference requests to. One of the simplest ways of doing this is to use Amazon SageMaker hosting capabilities.", "This example was tested on Amazon SageMaker Studio Notebook. Run this notebook using the following Amazon SageMaker Studio conda environment: TensorFlow 2 CPU Optimized. The full implementation is available here:", "Hosting a model endpoint with SageMaker involves the following simple steps:", "Create model.tar.gz with the TensorFlow saved model:", "Upload model to S3 and deploy:", "You can test the model by invoking the endpoint as follows:", "AWS Inferentia is a custom silicon designed by Amazon for cost-effective, high-throughput, low latency inference.", "James Hamilton (VP and Distinguished Engineer at AWS) goes into further depth about ASICs, general purpose processors, AWS Inferentia and the economics surrounding them in his blog post: AWS Inferentia Machine Learning Processor, which I encourage you to read if you\u2019re interested in AI hardware.", "The idea of using specialized processors for specialized workloads is not new. The chip in your noise cancelling headphone and the video decoder in your DVD player are examples of specialized chips, sometimes also called an Application Specific Integrated Circuit (ASIC).", "ASICs have 1 job (or limited responsibilities) and are optimized to do it well. Unlike general purpose processors (CPUs) or programmable accelerators (GPU), large parts of the silicon are not dedicated to run arbitrary code.", "AWS Inferentia was purpose built to offer high inference performance at the lowest cost in the cloud. AWS Inferentia chips can be accessed via the Amazon EC2 Inf1 instances which come in different sizes with 1 AWS Inferentia chip per instance all the way up to 16 AWS Inferential chips per instance. Each AWS Inferentia chip has 4 NeuronCores and supports FP16, BF16 and INT8 data types. NeuronCore is a high-performance systolic-array matrix-multiply engine and each has a two stage memory hierarchy, a very large on-chip cache.", "In most cases, AWS Inferentia might be the best AI accelerator for your use case, if your model:", "If you have operators not supported by the AWS Neuron SDK, you can still deploy it successfully on Inf1 instances, but those operations will run on the host CPU and won\u2019t be accelerated on AWS Inferentia. As I stated earlier, every use case is different, so compile your model with AWS Neuron SDK and measure performance to make sure it meets your performance, latency and throughput needs.", "AWS has compared performance of AWS Inferentia vs. GPU instances for popular models, and reports lower cost for popular models: YOLOv4 model, OpenPose, and has provided examples for BERT and SSD for TensorFlow, MXNet and PyTorch. For real-time applications, AWS Inf1 instances are amongst the least expensive of all the acceleration options available on AWS and AWS Inferentia can deliver higher throughput at target latency and at lower cost compared to GPUs and CPUs. Ultimately your choice may depend on other factors discussed below.", "AWS Inferentia chip supports a fixed set of neural network operators exposed via the AWS Neuron SDK. When you compile a model to target AWS Inferentia using the AWS Neuron SDK, the compiler will check your model for supported operators for your framework. If an operator isn\u2019t supported or if the compiler determines that a specific operator is more efficient to execute on CPU, it\u2019ll partition the graph to include CPU partitions and AWS Inferentia partitions. The same is also true for Amazon Elastic Inference which we\u2019ll discuss in the next section. If you\u2019re using TensorFlow with AWS Inferentia here is a list of all TensorFlow ops accelerated on AWS Inferentia.", "If you trained your model in FP32 (single precision), AWS Neuron SDK compiler will automatically cast your FP32 model to BF16 to improve inference performance. If you instead, prefer to provide a model in FP16, either by training in FP16 or by performing post-training quantization, AWS Neuron SDK will directly use your FP16 weights. While INT8 is supported by the AWS Inferentia chip, the AWS Neuron SDK compiler currently does not provide a way to deploy with INT8 support.", "In most cases, AWS Neuron SDK makes AWS Inferentia really easy to use. A key difference in the user experience of using AWS Inferentia and GPUs is that AWS Inferentia lets you have more control over how each core is used.", "AWS Neuron SDK supports two ways to improve performance by utilizing all the NeuronCores: (1) batching and (2) pipelining. Since the AWS Neuron SDK compiler is an ahead-of-time compiler, you have to enable these options explicitly during the compilation stage.", "Let\u2019s take a look at what these are and how these work.", "When you compile a model with AWS Neuron SDK compiler with batch_size, greater than one, batching is enabled. During inference your model weights are stored in external memory, and as forward pass is initiated, a subset of layer weights, as determined by the neuron runtime, is copied to the on-chip cache. With the weights of this layer on the cache, forward pass is computed on the entire batch.", "After that the next set of layer weights are loaded into the cache, and the forward pass is computed on the entire batch. This process continues until all weights are used for inference computations. Batching allows for better amortization of the cost of reading weights from the external memory by running inference on large batches when the layers are still in cache.", "All of this happens behind the scenes and as a user, you just have to set a desired batch size using an example input, during compilation.", "Even though batch size is set at the compilation phase, with dynamic batching enabled, the model can accept variable sized batches. Internally the neuron runtime will break down the user batch size into compiled batch sizes and run inference.", "During batching, model weights are loaded to the on-chip cache from the external memory layer by layer. With pipelining, you can load the entire model weights into the on-chip cache of multiple cores. This can reduce the latency since the neuron runtime does not have to load the weights from external memory. Again all of this happens behind the scenes, as a user you just set the desired number of cores using \u2014-num-neuroncores during the compilation phase.", "Batching and pipelining can be used together. However, you have to try different combinations of pipelining cores and compiled batch sizes to determine what works best for your model.", "During the compilation step, all combinations of batch sizes and number of neuron cores (for pipelining), may not work. You will have to determine the working combinations of batch size and number of neuron cores by running a sweep of different values and monitoring compiler errors.", "Depending on how you compiled your model you can either:", "The least cost Amazon EC2 Inf1 instance type, inf1.xlarge has 1 AWS Inferentia chip with 4 NeuronCores. If you compiled your model to run on a single NeuronCore, tensorflow-neuron will automatically perform data parallel execution on all 4 NeuronCores. This is equivalent to replicating your model 4 times and loading it into each NeuronCore and running 4 Python threads to feed input to data to each core. Automatic data parallel execution does not work beyond 1 AWS Inferentia chip. If you want to replicate your model to all 16 NeuronCores on an inf1.6xlarge for example, you have to spawn multiple threads to feed all AWS Inferentia chips with data. In python you can use concurrent.futures.ThreadPoolExecutor.", "When you compile a model for multiple NeuronCores, the runtime will allocate different subgraphs to each NeuronCore (screenshot by author)", "AWS Neuron SDK allows you to group NeuronCores into logical groups. Each group could have 1 or more NeuronCores and could run a different model. For example if you\u2019re deploying on an inf1.6xlarge EC2 Inf1 instance, you have access to 4 Inferentia chips with 4 NeuronCores each i.e. a total of 16 NeuronCores. You could divide 16 NeuronCores into, let\u2019s say 3 groups. Group 1 has 8 NeuronCores and will run a model that uses pipelining to use all 8 cores. Group 2 uses 4 NeuronCores and runs 4 copies of a model compiled with 1 neuron core. Group 3 uses 4 NeuronCores and runs 2 copies of a model compiled with 2 neuron cores with pipelining. You can specify this configuration using the NEURONCORE_GROUP_SIZES environment variable, and you\u2019d set it to NEURONCORE_GROUP_SIZES=8,1,1,1,1,2,2", "After that you simply have to load the model in the specified sequence within a single python process, i.e. load the model that\u2019s compiled to use 8 cores first, then load the model that\u2019s compiled to use 1 core four times, and then use load the model that\u2019s compiled to use 2 cores, two times. The appropriate cores will be assigned to the model.", "AWS Neuron SDK comes pre-installed on AWS Deep Learning AMI, and you can also install the SDK and the neuron-accelerated frameworks and libraries TensorFlow, TensorFlow Serving, TensorBoard (with neuron support), MXNet and PyTorch.", "The following examples were tested on Amazon EC2 Inf1.xlarge and Deep Learning AMI (Ubuntu 18.04) Version 35.0.", "You can find the full implementation for the examples below on this Jupyter Notebook:", "In this example I compare 3 different options", "You can find the full implementation in this Jupyter Notebook. I\u2019ll just review the results here.", "The comparison below shows that you get the best throughput with option 2 (batch size = 1, no pipelining) on Inf1.xlarge instances. You can repeat this experiment with other combinations on large Inf1 instances.", "Amazon Elastic Inference (EI) allows you to add cost-effective variable-size GPU acceleration to a CPU-only instance without provisioning a dedicated GPU instance. To use Amazon EI, you simply provision a CPU-only instance such as Amazon EC2 C5 instance type, and choose from 6 different EI accelerator options at launch.", "The EI accelerator is not part of the hardware that makes up your CPU instance, instead, the EI accelerator is attached through the network using an AWS PrivateLink endpoint service which routes traffic from your instance to the Elastic Inference accelerator configured with your instance. All of this happens seamlessly behind the scenes when you use an EI enabled serving frameworks such as TensorFlow serving.", "Amazon EI uses GPUs to provide GPU acceleration, but unlike dedicated GPU instances, you can choose to add GPU acceleration that comes in 6 different accelerator sizes, that you can choose by Tera (trillion) Floating Point Operations per Second (TFLOPS) or GPU memory.", "As I discussed earlier, GPUs are primarily throughput devices, and when dealing with smaller batches, common with real-time applications, GPUs tend to get underutilized when you deploy models that don\u2019t need the full processing power or full memory of a GPU. Also, if you don\u2019t have sufficient demand or multiple models to serve and share the GPU, then a single GPU may not be cost effective as cost/inference would go up.", "You can choose from 6 different EI accelerators that offer 1\u20134 TFLOPS and 1\u20138 GB of GPU memory. Let\u2019s say you have a less computationally demanding model with a small memory footprint, you can attach the smallest EI accelerator such as eia1.medium that offers 1 TFLOPS of FP32 performance and 1 GB of GPU memory to a CPU instance. If you have a more demanding model, you could attach an eia2.xlarge EI accelerator with 4 TFLOPS performance and 8 GB GPU memory to a CPU instance.", "The cost of the CPU instance + EI accelerator would still be cheaper than a dedicated GPU instance, and can lower inference costs. You don\u2019t have to worry about maximizing the utilization of your GPU since you\u2019re adding just enough capacity to meet demand, without over-provisioning.", "Let\u2019s consider the following hypothetical scenario. Let\u2019s say your application can deliver a good customer experience if your total latency (app + network + model predictions) is under 200 ms. And let\u2019s say, with a G4 instance type you can get total latency down to 40 ms which is well within your target latency. You\u2019ve also tried deploying with a CPU-only C5 instance type you can only get total latency to 400 ms which does not meet your SLA requirements and results in poor customer experience.", "With Elastic Inference, you can network attach just enough GPU acceleration to a CPU instance. After exploring different EI accelerator sizes (say eia2.medium, eia2.large, eia2.xlarge), you and get your total latency down to 180 ms with an eia2.large EI accelerators, which is under the desired 200 ms mark. Since EI is significantly cheaper than provisioning a dedicated GPU instance, you save on your total deployment costs.", "Since the GPU acceleration is added via the network, EI adds some latency compared to a dedicated GPU instance, but will still be faster than a CPU-only instance, and more cost-effective than a dedicated GPU instance. A dedicated GPU instance will still deliver better inference performance vs EI, but if the extra performance doesn\u2019t improve your customer experience, with EI you will stay under the target latency SLA, deliver good customer experience, and save on overall deployment costs. AWS has a number of blog posts that talk about performance and cost savings compared to CPUs and GPU using popular deep learning frameworks.", "Amazon EI supports models trained on TensorFlow, Apache MXNet, Pytorch and ONNX models. After you launch an Amazon EC2 instance with Amazon EI attached, to access the accelerator you need an EI enabled framework such as TensorFlow, PyTorch or Apache MXNet.", "EI enabled frameworks come pre-installed on AWS Deep Learning AMI, but if you prefer installing it manually, a Python wheel file has also been made available.", "Most popular models such as Inception, ResNet, SSD, RCNN, GNMT have been tested to deliver cost saving benefits when deployed with Amazon EI. If you\u2019re deploying a custom model with custom operators, EI enabled framework, partitions the graph to run unsupported operators on the host CPU, and all support ops on the EI accelerator attached via the network. This makes using EI very simple.", "This example was tested on Amazon EC2 c5.2xlarge the following AWS Deep Learning AMI: Deep Learning AMI (Ubuntu 18.04) Version 35.0", "You can find the full implementation on this Jupyter Notebook here:", "Amazon EI enabled TensorFlow offers APIs that let you accelerate your models using EI accelerators, and behave just like TensorFlow API. As a developer you to make have minimal code changes.", "To load model, you just have to run the following code:", "If you have more than one EI accelerators attached to your instance, you can specify them using the accelerator_id argument. Simply replace your TensorFlow model object with eia_model and the rest of your script remains the same, and your model is now accelerated on Amazon EI.", "The following figure compares CPU-only inference vs. EI accelerated inference on the same CPU instance. In this example you see over 6 times speed up with an EI accelerator.", "If there is one thing I want you to take away from the blog post, it is this: Deployment needs are unique and there really is no one size fits all. Review your deployment goals, compare them with the discussions in the article, and test out all options. Cloud makes it easy to try before you commit.", "Keep these considerations in mind as you choose:", "If programmability is very important, and you have low performance targets, then CPU might just work for you.", "If programmability and performance is important, then you can develop custom CUDA kernels for custom ops that are accelerated on GPUs.", "If you want the lowest cost option, and your model is supported on AWS Inferentia, you can save on overall deployment costs.", "Ease of use is subjective, but nothing can beat native framework experience. But with a little bit of extra effort both AWS Neuron SDK for AWS Inferentia and NVIDIA TensorRT for NVIDIA GPUs can deliver higher performance, thereby reducing cost / inference.", "Thank you for reading. In this article I was only able to give you a glimpse of all the sample code we discussed in this article. If you want to reproduce the results visit the following GitHub repo:", "If you found this article interesting, please check out my other blog posts on medium.", "Want me to write on a specific machine learning topic? I\u2019d love to hear from you! Follow me on twitter (@shshnkp), LinkedIn or leave a comment below.", "Talking Engineer. Runner. Coffee Connoisseur. Formerly Machine learning @Meta, AWS, NVIDIA, MATLAB, posts are my own opinions. website: shashankprasanna.com"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F7a5d6804ef1c&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-complete-guide-to-ai-accelerators-for-deep-learning-inference-gpus-aws-inferentia-and-amazon-7a5d6804ef1c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-complete-guide-to-ai-accelerators-for-deep-learning-inference-gpus-aws-inferentia-and-amazon-7a5d6804ef1c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-complete-guide-to-ai-accelerators-for-deep-learning-inference-gpus-aws-inferentia-and-amazon-7a5d6804ef1c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-complete-guide-to-ai-accelerators-for-deep-learning-inference-gpus-aws-inferentia-and-amazon-7a5d6804ef1c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/@shashankprasanna?source=post_page-----7a5d6804ef1c--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----7a5d6804ef1c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@shashankprasanna?source=post_page-----7a5d6804ef1c--------------------------------", "anchor_text": "Shashank Prasanna"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe0c596ca35b5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-complete-guide-to-ai-accelerators-for-deep-learning-inference-gpus-aws-inferentia-and-amazon-7a5d6804ef1c&user=Shashank+Prasanna&userId=e0c596ca35b5&source=post_page-e0c596ca35b5----7a5d6804ef1c---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----7a5d6804ef1c--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7a5d6804ef1c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-complete-guide-to-ai-accelerators-for-deep-learning-inference-gpus-aws-inferentia-and-amazon-7a5d6804ef1c&user=Shashank+Prasanna&userId=e0c596ca35b5&source=-----7a5d6804ef1c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7a5d6804ef1c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-complete-guide-to-ai-accelerators-for-deep-learning-inference-gpus-aws-inferentia-and-amazon-7a5d6804ef1c&source=-----7a5d6804ef1c---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://developer.nvidia.com/blog/cuda-refresher-reviewing-the-origins-of-gpu-computing/", "anchor_text": "blog post from NVIDIA explains"}, {"url": "https://developer.nvidia.com/deep-learning-performance-training-inference", "anchor_text": "I\u2019ll refer you to NVIDIA\u2019s website for their latest training and inference benchmarks for popular models"}, {"url": "https://towardsdatascience.com/choosing-the-right-gpu-for-deep-learning-on-aws-d69c157d8c86?source=friends_link&sk=a7b056b6fdbed24ecb5e23f6ea8625cc", "anchor_text": "Choosing the right GPU for deep learning on AWS"}, {"url": "https://towardsdatascience.com/choosing-the-right-gpu-for-deep-learning-on-aws-d69c157d8c86?source=friends_link&sk=a7b056b6fdbed24ecb5e23f6ea8625cc", "anchor_text": "Choosing the right GPU for deep learning on AWS"}, {"url": "https://code.google.com/archive/p/cuda-convnet/", "anchor_text": "cuda-convnet"}, {"url": "https://github.com/shashankprasanna/ai-accelerators-examples/blob/main/gpu-tf-tensorrt-resnet50.ipynb", "anchor_text": "https://github.com/shashankprasanna/ai-accelerators-examples/blob/main/gpu-tf-tensorrt-resnet50.ipynb"}, {"url": "https://github.com/shashankprasanna/ai-accelerators-examples/blob/main/gpu-tf-tensorrt-resnet50.ipynb", "anchor_text": "full implementation on GitHub"}, {"url": "https://github.com/shashankprasanna/ai-accelerators-examples/blob/main/gpu-tf-tensorrt-resnet50.ipynb", "anchor_text": "https://github.com/shashankprasanna/ai-accelerators-examples/blob/main/gpu-tf-tensorrt-resnet50.ipynb"}, {"url": "https://github.com/shashankprasanna/ai-accelerators-examples/blob/main/sagemaker-tf-cpu-gpu-ei-resnet50.ipynb", "anchor_text": "https://github.com/shashankprasanna/ai-accelerators-examples/blob/main/sagemaker-tf-cpu-gpu-ei-resnet50.ipynb"}, {"url": "https://perspectives.mvdirona.com/2018/11/aws-inferentia-machine-learning-processor/", "anchor_text": "AWS Inferentia Machine Learning Processor"}, {"url": "https://en.wikipedia.org/wiki/Systolic_array", "anchor_text": "systolic-array"}, {"url": "https://github.com/aws/aws-neuron-sdk/tree/master/release-notes/neuron-cc-ops", "anchor_text": "supported by the AWS Neuron SDK"}, {"url": "https://aws.amazon.com/blogs/machine-learning/improving-performance-for-deep-learning-based-object-detection-with-an-aws-neuron-compiled-yolov4-model-on-aws-inferentia/", "anchor_text": "YOLOv4 model"}, {"url": "https://aws.amazon.com/blogs/machine-learning/deploying-tensorflow-openpose-on-aws-inferentia-based-inf1-instances-for-significant-price-performance-improvements/", "anchor_text": "OpenPose"}, {"url": "https://github.com/aws/aws-neuron-sdk/tree/master/src/examples", "anchor_text": "examples for BERT and SSD for TensorFlow, MXNet and PyTorch"}, {"url": "https://github.com/aws/aws-neuron-sdk/tree/master/release-notes/neuron-cc-ops", "anchor_text": "supported operators for your framework"}, {"url": "https://github.com/aws/aws-neuron-sdk/blob/master/release-notes/neuron-cc-ops/neuron-cc-ops-tensorflow.md", "anchor_text": "TensorFlow ops accelerated on AWS Inferentia"}, {"url": "https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ThreadPoolExecutor", "anchor_text": "concurrent.futures.ThreadPoolExecutor"}, {"url": "https://github.com/aws/aws-neuron-sdk/blob/master/docs/neuron-install-guide.md", "anchor_text": "install the SDK and the neuron-accelerated frameworks and libraries"}, {"url": "https://github.com/shashankprasanna/ai-accelerators-examples/blob/main/inf1-neuron-sdk-resnet50.ipynb", "anchor_text": "https://github.com/shashankprasanna/ai-accelerators-examples/blob/main/inf1-neuron-sdk-resnet50.ipynb"}, {"url": "https://github.com/shashankprasanna/ai-accelerators-examples/blob/main/inf1-neuron-sdk-resnet50.ipynb", "anchor_text": "in this Jupyter Notebook"}, {"url": "https://aws.amazon.com/machine-learning/elastic-inference/", "anchor_text": "Amazon Elastic Inference (EI)"}, {"url": "https://aws.amazon.com/blogs/machine-learning/category/artificial-intelligence/amazon-elastic-inference/", "anchor_text": "blog posts that talk about performance and cost savings"}, {"url": "https://docs.aws.amazon.com/dlami/latest/devguide/what-is-dlami.html", "anchor_text": "AWS Deep Learning AMI"}, {"url": "https://docs.aws.amazon.com/elastic-inference/latest/developerguide/ei-tensorflow.html", "anchor_text": "installing it manually,"}, {"url": "https://github.com/shashankprasanna/ai-accelerators-examples/blob/main/ei-tensorflow-resnet50.ipynb", "anchor_text": "https://github.com/shashankprasanna/ai-accelerators-examples/blob/main/ei-tensorflow-resnet50.ipynb"}, {"url": "https://github.com/shashankprasanna/ai-accelerators-examples", "anchor_text": "https://github.com/shashankprasanna/ai-accelerators-examples"}, {"url": "https://medium.com/@shashankprasanna", "anchor_text": "medium"}, {"url": "https://twitter.com/shshnkp", "anchor_text": "@shshnkp"}, {"url": "https://www.linkedin.com/in/shashankprasanna/", "anchor_text": "LinkedIn"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----7a5d6804ef1c---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/aws?source=post_page-----7a5d6804ef1c---------------aws-----------------", "anchor_text": "AWS"}, {"url": "https://medium.com/tag/gpu?source=post_page-----7a5d6804ef1c---------------gpu-----------------", "anchor_text": "Gpu"}, {"url": "https://medium.com/tag/data-science?source=post_page-----7a5d6804ef1c---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----7a5d6804ef1c---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7a5d6804ef1c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-complete-guide-to-ai-accelerators-for-deep-learning-inference-gpus-aws-inferentia-and-amazon-7a5d6804ef1c&user=Shashank+Prasanna&userId=e0c596ca35b5&source=-----7a5d6804ef1c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7a5d6804ef1c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-complete-guide-to-ai-accelerators-for-deep-learning-inference-gpus-aws-inferentia-and-amazon-7a5d6804ef1c&user=Shashank+Prasanna&userId=e0c596ca35b5&source=-----7a5d6804ef1c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7a5d6804ef1c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-complete-guide-to-ai-accelerators-for-deep-learning-inference-gpus-aws-inferentia-and-amazon-7a5d6804ef1c&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/@shashankprasanna?source=post_page-----7a5d6804ef1c--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----7a5d6804ef1c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe0c596ca35b5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-complete-guide-to-ai-accelerators-for-deep-learning-inference-gpus-aws-inferentia-and-amazon-7a5d6804ef1c&user=Shashank+Prasanna&userId=e0c596ca35b5&source=post_page-e0c596ca35b5----7a5d6804ef1c---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fd48ce4d9cb5c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-complete-guide-to-ai-accelerators-for-deep-learning-inference-gpus-aws-inferentia-and-amazon-7a5d6804ef1c&newsletterV3=e0c596ca35b5&newsletterV3Id=d48ce4d9cb5c&user=Shashank+Prasanna&userId=e0c596ca35b5&source=-----7a5d6804ef1c---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/@shashankprasanna?source=post_page-----7a5d6804ef1c--------------------------------", "anchor_text": "Written by Shashank Prasanna"}, {"url": "https://medium.com/@shashankprasanna/followers?source=post_page-----7a5d6804ef1c--------------------------------", "anchor_text": "681 Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----7a5d6804ef1c--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "http://shashankprasanna.com", "anchor_text": "shashankprasanna.com"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe0c596ca35b5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-complete-guide-to-ai-accelerators-for-deep-learning-inference-gpus-aws-inferentia-and-amazon-7a5d6804ef1c&user=Shashank+Prasanna&userId=e0c596ca35b5&source=post_page-e0c596ca35b5----7a5d6804ef1c---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fd48ce4d9cb5c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-complete-guide-to-ai-accelerators-for-deep-learning-inference-gpus-aws-inferentia-and-amazon-7a5d6804ef1c&newsletterV3=e0c596ca35b5&newsletterV3Id=d48ce4d9cb5c&user=Shashank+Prasanna&userId=e0c596ca35b5&source=-----7a5d6804ef1c---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-pytorch-2-0-accelerates-deep-learning-with-operator-fusion-and-cpu-gpu-code-generation-35132a85bd26?source=author_recirc-----7a5d6804ef1c----0---------------------11a077dc_b60b_43cd_92f9_f83081db9855-------", "anchor_text": ""}, {"url": "https://medium.com/@shashankprasanna?source=author_recirc-----7a5d6804ef1c----0---------------------11a077dc_b60b_43cd_92f9_f83081db9855-------", "anchor_text": ""}, {"url": "https://medium.com/@shashankprasanna?source=author_recirc-----7a5d6804ef1c----0---------------------11a077dc_b60b_43cd_92f9_f83081db9855-------", "anchor_text": "Shashank Prasanna"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----7a5d6804ef1c----0---------------------11a077dc_b60b_43cd_92f9_f83081db9855-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/how-pytorch-2-0-accelerates-deep-learning-with-operator-fusion-and-cpu-gpu-code-generation-35132a85bd26?source=author_recirc-----7a5d6804ef1c----0---------------------11a077dc_b60b_43cd_92f9_f83081db9855-------", "anchor_text": "How Pytorch 2.0 accelerates deep learning with operator fusion and CPU/GPU code-generationA primer on deep learning compiler technologies in PyTorch for graph capture, intermediate representations, operator fusion, and more"}, {"url": "https://towardsdatascience.com/how-pytorch-2-0-accelerates-deep-learning-with-operator-fusion-and-cpu-gpu-code-generation-35132a85bd26?source=author_recirc-----7a5d6804ef1c----0---------------------11a077dc_b60b_43cd_92f9_f83081db9855-------", "anchor_text": "17 min read\u00b7Apr 20"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F35132a85bd26&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-pytorch-2-0-accelerates-deep-learning-with-operator-fusion-and-cpu-gpu-code-generation-35132a85bd26&user=Shashank+Prasanna&userId=e0c596ca35b5&source=-----35132a85bd26----0-----------------clap_footer----11a077dc_b60b_43cd_92f9_f83081db9855-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-pytorch-2-0-accelerates-deep-learning-with-operator-fusion-and-cpu-gpu-code-generation-35132a85bd26?source=author_recirc-----7a5d6804ef1c----0---------------------11a077dc_b60b_43cd_92f9_f83081db9855-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F35132a85bd26&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-pytorch-2-0-accelerates-deep-learning-with-operator-fusion-and-cpu-gpu-code-generation-35132a85bd26&source=-----7a5d6804ef1c----0-----------------bookmark_preview----11a077dc_b60b_43cd_92f9_f83081db9855-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----7a5d6804ef1c----1---------------------11a077dc_b60b_43cd_92f9_f83081db9855-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----7a5d6804ef1c----1---------------------11a077dc_b60b_43cd_92f9_f83081db9855-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----7a5d6804ef1c----1---------------------11a077dc_b60b_43cd_92f9_f83081db9855-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----7a5d6804ef1c----1---------------------11a077dc_b60b_43cd_92f9_f83081db9855-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----7a5d6804ef1c----1---------------------11a077dc_b60b_43cd_92f9_f83081db9855-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----7a5d6804ef1c----1---------------------11a077dc_b60b_43cd_92f9_f83081db9855-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----11a077dc_b60b_43cd_92f9_f83081db9855-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----7a5d6804ef1c----1---------------------11a077dc_b60b_43cd_92f9_f83081db9855-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----7a5d6804ef1c----1-----------------bookmark_preview----11a077dc_b60b_43cd_92f9_f83081db9855-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----7a5d6804ef1c----2---------------------11a077dc_b60b_43cd_92f9_f83081db9855-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----7a5d6804ef1c----2---------------------11a077dc_b60b_43cd_92f9_f83081db9855-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----7a5d6804ef1c----2---------------------11a077dc_b60b_43cd_92f9_f83081db9855-------", "anchor_text": "Jacob Marks, Ph.D."}, {"url": "https://towardsdatascience.com/?source=author_recirc-----7a5d6804ef1c----2---------------------11a077dc_b60b_43cd_92f9_f83081db9855-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----7a5d6804ef1c----2---------------------11a077dc_b60b_43cd_92f9_f83081db9855-------", "anchor_text": "How I Turned My Company\u2019s Docs into a Searchable Database with OpenAIAnd how you can do the same with your docs"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----7a5d6804ef1c----2---------------------11a077dc_b60b_43cd_92f9_f83081db9855-------", "anchor_text": "15 min read\u00b7Apr 25"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&user=Jacob+Marks%2C+Ph.D.&userId=f7dc0c0eae92&source=-----4f2d34bd8736----2-----------------clap_footer----11a077dc_b60b_43cd_92f9_f83081db9855-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----7a5d6804ef1c----2---------------------11a077dc_b60b_43cd_92f9_f83081db9855-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "20"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&source=-----7a5d6804ef1c----2-----------------bookmark_preview----11a077dc_b60b_43cd_92f9_f83081db9855-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/choosing-the-right-gpu-for-deep-learning-on-aws-d69c157d8c86?source=author_recirc-----7a5d6804ef1c----3---------------------11a077dc_b60b_43cd_92f9_f83081db9855-------", "anchor_text": ""}, {"url": "https://medium.com/@shashankprasanna?source=author_recirc-----7a5d6804ef1c----3---------------------11a077dc_b60b_43cd_92f9_f83081db9855-------", "anchor_text": ""}, {"url": "https://medium.com/@shashankprasanna?source=author_recirc-----7a5d6804ef1c----3---------------------11a077dc_b60b_43cd_92f9_f83081db9855-------", "anchor_text": "Shashank Prasanna"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----7a5d6804ef1c----3---------------------11a077dc_b60b_43cd_92f9_f83081db9855-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/choosing-the-right-gpu-for-deep-learning-on-aws-d69c157d8c86?source=author_recirc-----7a5d6804ef1c----3---------------------11a077dc_b60b_43cd_92f9_f83081db9855-------", "anchor_text": "Choosing the right GPU for deep learning on AWSHow to choose the right Amazon EC2 GPU instance for deep learning training and inference \u2014 from best performance to the most\u2026"}, {"url": "https://towardsdatascience.com/choosing-the-right-gpu-for-deep-learning-on-aws-d69c157d8c86?source=author_recirc-----7a5d6804ef1c----3---------------------11a077dc_b60b_43cd_92f9_f83081db9855-------", "anchor_text": "\u00b725 min read\u00b7Jul 25, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd69c157d8c86&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fchoosing-the-right-gpu-for-deep-learning-on-aws-d69c157d8c86&user=Shashank+Prasanna&userId=e0c596ca35b5&source=-----d69c157d8c86----3-----------------clap_footer----11a077dc_b60b_43cd_92f9_f83081db9855-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/choosing-the-right-gpu-for-deep-learning-on-aws-d69c157d8c86?source=author_recirc-----7a5d6804ef1c----3---------------------11a077dc_b60b_43cd_92f9_f83081db9855-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "12"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd69c157d8c86&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fchoosing-the-right-gpu-for-deep-learning-on-aws-d69c157d8c86&source=-----7a5d6804ef1c----3-----------------bookmark_preview----11a077dc_b60b_43cd_92f9_f83081db9855-------", "anchor_text": ""}, {"url": "https://medium.com/@shashankprasanna?source=post_page-----7a5d6804ef1c--------------------------------", "anchor_text": "See all from Shashank Prasanna"}, {"url": "https://towardsdatascience.com/?source=post_page-----7a5d6804ef1c--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----7a5d6804ef1c----0---------------------21a7de52_dfc8_4cfe_a857_f703155109f8-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----7a5d6804ef1c----0---------------------21a7de52_dfc8_4cfe_a857_f703155109f8-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----7a5d6804ef1c----0---------------------21a7de52_dfc8_4cfe_a857_f703155109f8-------", "anchor_text": "Alexander Nguyen"}, {"url": "https://levelup.gitconnected.com/?source=read_next_recirc-----7a5d6804ef1c----0---------------------21a7de52_dfc8_4cfe_a857_f703155109f8-------", "anchor_text": "Level Up Coding"}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----7a5d6804ef1c----0---------------------21a7de52_dfc8_4cfe_a857_f703155109f8-------", "anchor_text": "Why I Keep Failing Candidates During Google Interviews\u2026They don\u2019t meet the bar."}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----7a5d6804ef1c----0---------------------21a7de52_dfc8_4cfe_a857_f703155109f8-------", "anchor_text": "\u00b74 min read\u00b7Apr 13"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fgitconnected%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&user=Alexander+Nguyen&userId=a148fd75c2e9&source=-----dc8f865b2c19----0-----------------clap_footer----21a7de52_dfc8_4cfe_a857_f703155109f8-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----7a5d6804ef1c----0---------------------21a7de52_dfc8_4cfe_a857_f703155109f8-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "91"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&source=-----7a5d6804ef1c----0-----------------bookmark_preview----21a7de52_dfc8_4cfe_a857_f703155109f8-------", "anchor_text": ""}, {"url": "https://medium.com/@snk.nitin/how-to-solve-cuda-out-of-memory-error-850bb247cfb2?source=read_next_recirc-----7a5d6804ef1c----1---------------------21a7de52_dfc8_4cfe_a857_f703155109f8-------", "anchor_text": ""}, {"url": "https://medium.com/@snk.nitin?source=read_next_recirc-----7a5d6804ef1c----1---------------------21a7de52_dfc8_4cfe_a857_f703155109f8-------", "anchor_text": ""}, {"url": "https://medium.com/@snk.nitin?source=read_next_recirc-----7a5d6804ef1c----1---------------------21a7de52_dfc8_4cfe_a857_f703155109f8-------", "anchor_text": "Nitin Kishore"}, {"url": "https://medium.com/@snk.nitin/how-to-solve-cuda-out-of-memory-error-850bb247cfb2?source=read_next_recirc-----7a5d6804ef1c----1---------------------21a7de52_dfc8_4cfe_a857_f703155109f8-------", "anchor_text": "How to solve CUDA Out of Memory error**Freeze frame, scratch that record and cue \u2014 \u2018The Who\u2019 intro**"}, {"url": "https://medium.com/@snk.nitin/how-to-solve-cuda-out-of-memory-error-850bb247cfb2?source=read_next_recirc-----7a5d6804ef1c----1---------------------21a7de52_dfc8_4cfe_a857_f703155109f8-------", "anchor_text": "\u00b77 min read\u00b7Nov 2, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F850bb247cfb2&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40snk.nitin%2Fhow-to-solve-cuda-out-of-memory-error-850bb247cfb2&user=Nitin+Kishore&userId=ef6a1cf849e2&source=-----850bb247cfb2----1-----------------clap_footer----21a7de52_dfc8_4cfe_a857_f703155109f8-------", "anchor_text": ""}, {"url": "https://medium.com/@snk.nitin/how-to-solve-cuda-out-of-memory-error-850bb247cfb2?source=read_next_recirc-----7a5d6804ef1c----1---------------------21a7de52_dfc8_4cfe_a857_f703155109f8-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F850bb247cfb2&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40snk.nitin%2Fhow-to-solve-cuda-out-of-memory-error-850bb247cfb2&source=-----7a5d6804ef1c----1-----------------bookmark_preview----21a7de52_dfc8_4cfe_a857_f703155109f8-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----7a5d6804ef1c----0---------------------21a7de52_dfc8_4cfe_a857_f703155109f8-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----7a5d6804ef1c----0---------------------21a7de52_dfc8_4cfe_a857_f703155109f8-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----7a5d6804ef1c----0---------------------21a7de52_dfc8_4cfe_a857_f703155109f8-------", "anchor_text": "The PyCoach"}, {"url": "https://artificialcorner.com/?source=read_next_recirc-----7a5d6804ef1c----0---------------------21a7de52_dfc8_4cfe_a857_f703155109f8-------", "anchor_text": "Artificial Corner"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----7a5d6804ef1c----0---------------------21a7de52_dfc8_4cfe_a857_f703155109f8-------", "anchor_text": "You\u2019re Using ChatGPT Wrong! Here\u2019s How to Be Ahead of 99% of ChatGPT UsersMaster ChatGPT by learning prompt engineering."}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----7a5d6804ef1c----0---------------------21a7de52_dfc8_4cfe_a857_f703155109f8-------", "anchor_text": "\u00b77 min read\u00b7Mar 17"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fartificial-corner%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&user=The+PyCoach&userId=fb44e21903f3&source=-----886a50dabc54----0-----------------clap_footer----21a7de52_dfc8_4cfe_a857_f703155109f8-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----7a5d6804ef1c----0---------------------21a7de52_dfc8_4cfe_a857_f703155109f8-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "276"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&source=-----7a5d6804ef1c----0-----------------bookmark_preview----21a7de52_dfc8_4cfe_a857_f703155109f8-------", "anchor_text": ""}, {"url": "https://tnmthai.medium.com/training-pytorch-models-on-a-mac-m1-and-m2-92d02c50b872?source=read_next_recirc-----7a5d6804ef1c----1---------------------21a7de52_dfc8_4cfe_a857_f703155109f8-------", "anchor_text": ""}, {"url": "https://tnmthai.medium.com/?source=read_next_recirc-----7a5d6804ef1c----1---------------------21a7de52_dfc8_4cfe_a857_f703155109f8-------", "anchor_text": ""}, {"url": "https://tnmthai.medium.com/?source=read_next_recirc-----7a5d6804ef1c----1---------------------21a7de52_dfc8_4cfe_a857_f703155109f8-------", "anchor_text": "Thai Tran"}, {"url": "https://tnmthai.medium.com/training-pytorch-models-on-a-mac-m1-and-m2-92d02c50b872?source=read_next_recirc-----7a5d6804ef1c----1---------------------21a7de52_dfc8_4cfe_a857_f703155109f8-------", "anchor_text": "Training PyTorch models on a Mac M1 and M2Metal acceleration"}, {"url": "https://tnmthai.medium.com/training-pytorch-models-on-a-mac-m1-and-m2-92d02c50b872?source=read_next_recirc-----7a5d6804ef1c----1---------------------21a7de52_dfc8_4cfe_a857_f703155109f8-------", "anchor_text": "\u00b78 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F92d02c50b872&operation=register&redirect=https%3A%2F%2Ftnmthai.medium.com%2Ftraining-pytorch-models-on-a-mac-m1-and-m2-92d02c50b872&user=Thai+Tran&userId=bb8b875b3e65&source=-----92d02c50b872----1-----------------clap_footer----21a7de52_dfc8_4cfe_a857_f703155109f8-------", "anchor_text": ""}, {"url": "https://tnmthai.medium.com/training-pytorch-models-on-a-mac-m1-and-m2-92d02c50b872?source=read_next_recirc-----7a5d6804ef1c----1---------------------21a7de52_dfc8_4cfe_a857_f703155109f8-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F92d02c50b872&operation=register&redirect=https%3A%2F%2Ftnmthai.medium.com%2Ftraining-pytorch-models-on-a-mac-m1-and-m2-92d02c50b872&source=-----7a5d6804ef1c----1-----------------bookmark_preview----21a7de52_dfc8_4cfe_a857_f703155109f8-------", "anchor_text": ""}, {"url": "https://medium.com/trigger-ai/mlops-in-practice-machine-learning-ml-model-deployment-patterns-part-1-ce7cb575feda?source=read_next_recirc-----7a5d6804ef1c----2---------------------21a7de52_dfc8_4cfe_a857_f703155109f8-------", "anchor_text": ""}, {"url": "https://medium.com/@weiyunna91?source=read_next_recirc-----7a5d6804ef1c----2---------------------21a7de52_dfc8_4cfe_a857_f703155109f8-------", "anchor_text": ""}, {"url": "https://medium.com/@weiyunna91?source=read_next_recirc-----7a5d6804ef1c----2---------------------21a7de52_dfc8_4cfe_a857_f703155109f8-------", "anchor_text": "YUNNA WEI"}, {"url": "https://medium.com/trigger-ai?source=read_next_recirc-----7a5d6804ef1c----2---------------------21a7de52_dfc8_4cfe_a857_f703155109f8-------", "anchor_text": "Efficient Data+AI Stack"}, {"url": "https://medium.com/trigger-ai/mlops-in-practice-machine-learning-ml-model-deployment-patterns-part-1-ce7cb575feda?source=read_next_recirc-----7a5d6804ef1c----2---------------------21a7de52_dfc8_4cfe_a857_f703155109f8-------", "anchor_text": "MLOps in Practice \u2014 Machine Learning (ML) model deployment patterns (Part 1)Machine Learning (ML) model serving and deployment is one of the most critical components of any solid ML solution architecture. This\u2026"}, {"url": "https://medium.com/trigger-ai/mlops-in-practice-machine-learning-ml-model-deployment-patterns-part-1-ce7cb575feda?source=read_next_recirc-----7a5d6804ef1c----2---------------------21a7de52_dfc8_4cfe_a857_f703155109f8-------", "anchor_text": "\u00b711 min read\u00b7Jan 26"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftrigger-ai%2Fce7cb575feda&operation=register&redirect=https%3A%2F%2Fmedium.com%2Ftrigger-ai%2Fmlops-in-practice-machine-learning-ml-model-deployment-patterns-part-1-ce7cb575feda&user=YUNNA+WEI&userId=4b47aa84fc4&source=-----ce7cb575feda----2-----------------clap_footer----21a7de52_dfc8_4cfe_a857_f703155109f8-------", "anchor_text": ""}, {"url": "https://medium.com/trigger-ai/mlops-in-practice-machine-learning-ml-model-deployment-patterns-part-1-ce7cb575feda?source=read_next_recirc-----7a5d6804ef1c----2---------------------21a7de52_dfc8_4cfe_a857_f703155109f8-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fce7cb575feda&operation=register&redirect=https%3A%2F%2Fmedium.com%2Ftrigger-ai%2Fmlops-in-practice-machine-learning-ml-model-deployment-patterns-part-1-ce7cb575feda&source=-----7a5d6804ef1c----2-----------------bookmark_preview----21a7de52_dfc8_4cfe_a857_f703155109f8-------", "anchor_text": ""}, {"url": "https://pub.towardsai.net/run-very-large-language-models-on-your-computer-390dd33838bb?source=read_next_recirc-----7a5d6804ef1c----3---------------------21a7de52_dfc8_4cfe_a857_f703155109f8-------", "anchor_text": ""}, {"url": "https://medium.com/@bnjmn_marie?source=read_next_recirc-----7a5d6804ef1c----3---------------------21a7de52_dfc8_4cfe_a857_f703155109f8-------", "anchor_text": ""}, {"url": "https://medium.com/@bnjmn_marie?source=read_next_recirc-----7a5d6804ef1c----3---------------------21a7de52_dfc8_4cfe_a857_f703155109f8-------", "anchor_text": "Benjamin Marie"}, {"url": "https://pub.towardsai.net/?source=read_next_recirc-----7a5d6804ef1c----3---------------------21a7de52_dfc8_4cfe_a857_f703155109f8-------", "anchor_text": "Towards AI"}, {"url": "https://pub.towardsai.net/run-very-large-language-models-on-your-computer-390dd33838bb?source=read_next_recirc-----7a5d6804ef1c----3---------------------21a7de52_dfc8_4cfe_a857_f703155109f8-------", "anchor_text": "Run Very Large Language Models on Your ComputerWith PyTorch and Hugging Face\u2019s device_map"}, {"url": "https://pub.towardsai.net/run-very-large-language-models-on-your-computer-390dd33838bb?source=read_next_recirc-----7a5d6804ef1c----3---------------------21a7de52_dfc8_4cfe_a857_f703155109f8-------", "anchor_text": "\u00b75 min read\u00b7Dec 22, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-artificial-intelligence%2F390dd33838bb&operation=register&redirect=https%3A%2F%2Fpub.towardsai.net%2Frun-very-large-language-models-on-your-computer-390dd33838bb&user=Benjamin+Marie&userId=ad2a414578b3&source=-----390dd33838bb----3-----------------clap_footer----21a7de52_dfc8_4cfe_a857_f703155109f8-------", "anchor_text": ""}, {"url": "https://pub.towardsai.net/run-very-large-language-models-on-your-computer-390dd33838bb?source=read_next_recirc-----7a5d6804ef1c----3---------------------21a7de52_dfc8_4cfe_a857_f703155109f8-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F390dd33838bb&operation=register&redirect=https%3A%2F%2Fpub.towardsai.net%2Frun-very-large-language-models-on-your-computer-390dd33838bb&source=-----7a5d6804ef1c----3-----------------bookmark_preview----21a7de52_dfc8_4cfe_a857_f703155109f8-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----7a5d6804ef1c--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----7a5d6804ef1c--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----7a5d6804ef1c--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----7a5d6804ef1c--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----7a5d6804ef1c--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----7a5d6804ef1c--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----7a5d6804ef1c--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----7a5d6804ef1c--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----7a5d6804ef1c--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----7a5d6804ef1c--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}