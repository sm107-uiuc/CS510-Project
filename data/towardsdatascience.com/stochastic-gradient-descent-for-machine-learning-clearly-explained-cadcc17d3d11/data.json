{"url": "https://towardsdatascience.com/stochastic-gradient-descent-for-machine-learning-clearly-explained-cadcc17d3d11", "time": 1683008417.0519629, "path": "towardsdatascience.com/stochastic-gradient-descent-for-machine-learning-clearly-explained-cadcc17d3d11/", "webpage": {"metadata": {"title": "Stochastic Gradient Descent for machine learning clearly explained | by Baptiste Monpezat | Towards Data Science", "h1": "Stochastic Gradient Descent for machine learning clearly explained", "description": "As you may know, supervised machine learning consists in finding a function, called a decision function, that best models the relation between input/output pairs of data. In order to find this\u2026"}, "outgoing_paragraph_urls": [{"url": "https://github.com/baptiste-monpezat/stochastic_gradient_descent", "anchor_text": "https://github.com/baptiste-monpezat/stochastic_gradient_descent", "paragraph_index": 34}], "all_paragraphs": ["As you may know, supervised machine learning consists in finding a function, called a decision function, that best models the relation between input/output pairs of data. In order to find this function, we have to formulate this learning problem into an optimization problem.", "Let\u2019s consider the following task: finding the best linear function that maps the input space, the variable X to the output space, the variable Y.", "As we try to model the relation between X and Y by a linear function, the set of functions that the learning algorithm is allowed to select is the following :", "The term b is the intercept, also called bias in machine learning. This set of functions is our hypothesis space.But how do we choose the values for the parameters a,b and how do we judge if it\u2019s a good guess or not?", "We define a function called a loss function that evaluates our choice in the context of the outcome Y.", "We define our loss as a squared loss (we could have chosen another loss function such as the absolute loss) :", "The squared loss penalizes the difference between the actual y outcome and the outcome estimated by choosing values for the set of parameters a,b. This loss function evaluates our choice on a single point, but we need to evaluate our decision function on all the training points.", "Thus, we compute the average of the square of the errors: the mean squared error.", "where n is the number of data points.This function, which depends on the parameters defining our hypothesis space, is called Empirical risk.", "Rn(a,b) is a quadratic function of the parameters, hence it's minimum always exists but may not be unique.", "Eventually, we reached our initial goal: formulating the learning problem into an optimization one!", "Indeed, all we have to do is to find the decision function, the a,b coefficients, that minimize this empirical risk.", "It would be the best decision function we could possibly produce: our target function.", "In the case of a simple linear regression, we could simply differentiate the empirical risk and compute the a,b coefficients that cancel the derivative. It is easier to use matrix notation to compute the solution. It is convenient to include the constant variable 1 in X and write parameters a and b as a single vector \u03b2. Thus, our linear model can be written as :", "and our loss function becomes :", "The vector beta minimizing our equation can be found by solving the following equation :", "Our linear regression has only two predictors (a and b), thus X is a n x 2 matrix (where n is the number of observations and 2 the number of predictors). As you can see, to solve the equation we need to calculate the matrix (X^T X) then invert it.", "In machine learning, the number of observations is often very high as well as the number of predictors. Consequently, this operation is very expensive in terms of calculation and memory.", "Gradient descent algorithm is an iterative optimization algorithm that allows us to find the solution while keeping the computational complexity low. We describe how it works in the next part of this article.", "Gradient descent algorithm can be illustrated by the following analogy. Imagine that you are lost in the mountains in the middle of the night. You can\u2019t see anything as it\u2019s pitch dark and you want to go back to the village located in the valley bottom (you are trying to find the local/global minimum of the mean squared error function). To survive, you develop the following strategy :", "Eventually, you will reach the valley bottom, or you will get stuck in a local minimum \u2026", "Now that you have understood the principle with this allegory, let\u2019s dive into the mathematics of gradient descent algorithm!For finding the a, b parameters that minimize the mean squared error, the algorithm can be implemented as follow :", "Then update values of a and b by subtracting the gradient multiplied by a step size :", "with \u03b7, our fixed step size.", "Compute the mean squared loss with the updated values of a and b.", "Repeat those steps until a stopping criterion is met. For instance, the decrease of the mean squared loss is lower than a threshold \u03f5.", "On the animation below, you can see the update of the parameter a performed by the gradient descent algorithm as well as the fitting of our linear regression model :", "As we are fitting a model with two predictors, we can visualize the gradient descent algorithm process in a 3D space!", "At every iteration of the gradient descent algorithm, we have to look at all our training points to compute the gradient.", "Thus, the time complexity of this algorithm is O(n). It will take a long time to compute for a very large data set. Maybe we could compute an estimate of the gradient instead of looking at all the data points: this algorithm is called minibatch gradient descent.", "Minibatch gradient descent consists in using a random subset of size N to determine step direction at each iteration.", "If we use a random subset of size N=1, it is called stochastic gradient descent. It means that we will use a single randomly chosen point to determine step direction.", "In the following animation, the blue line corresponds to stochastic gradient descent and the red one is a basic gradient descent algorithm.", "I hope this article has helped you understand this basic optimization algorithm, if you liked it or if you have any question don\u2019t hesitate to comment!", "You can find the code I made to implement stochastic gradient descent and create the animations on my GitHub: https://github.com/baptiste-monpezat/stochastic_gradient_descent.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Scientist | Having fun with data ! All models are wrong, but some are useful."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fcadcc17d3d11&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstochastic-gradient-descent-for-machine-learning-clearly-explained-cadcc17d3d11&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstochastic-gradient-descent-for-machine-learning-clearly-explained-cadcc17d3d11&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstochastic-gradient-descent-for-machine-learning-clearly-explained-cadcc17d3d11&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstochastic-gradient-descent-for-machine-learning-clearly-explained-cadcc17d3d11&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----cadcc17d3d11--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----cadcc17d3d11--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@baptiste.monpezat?source=post_page-----cadcc17d3d11--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@baptiste.monpezat?source=post_page-----cadcc17d3d11--------------------------------", "anchor_text": "Baptiste Monpezat"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffe51577b9797&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstochastic-gradient-descent-for-machine-learning-clearly-explained-cadcc17d3d11&user=Baptiste+Monpezat&userId=fe51577b9797&source=post_page-fe51577b9797----cadcc17d3d11---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcadcc17d3d11&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstochastic-gradient-descent-for-machine-learning-clearly-explained-cadcc17d3d11&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcadcc17d3d11&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstochastic-gradient-descent-for-machine-learning-clearly-explained-cadcc17d3d11&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://github.com/baptiste-monpezat/stochastic_gradient_descent/blob/master/Stochastic_Gradient_Descent.ipynb", "anchor_text": "GitHub Link"}, {"url": "https://github.com/baptiste-monpezat/stochastic_gradient_descent/blob/master/Stochastic_Gradient_Descent.ipynb", "anchor_text": "GitHub Link"}, {"url": "https://github.com/baptiste-monpezat/stochastic_gradient_descent/blob/master/Stochastic_Gradient_Descent.ipynb", "anchor_text": "GitHub Link"}, {"url": "https://github.com/baptiste-monpezat/stochastic_gradient_descent/blob/master/Stochastic_Gradient_Descent.ipynb", "anchor_text": "GitHub Link"}, {"url": "https://github.com/baptiste-monpezat/stochastic_gradient_descent/blob/master/Stochastic_Gradient_Descent.ipynb", "anchor_text": "GitHub Link"}, {"url": "https://github.com/baptiste-monpezat/stochastic_gradient_descent/blob/master/Stochastic_Gradient_Descent.ipynb", "anchor_text": "GitHub Link"}, {"url": "https://github.com/baptiste-monpezat/stochastic_gradient_descent", "anchor_text": "https://github.com/baptiste-monpezat/stochastic_gradient_descent"}, {"url": "https://baptiste-monpezat.github.io/blog/stochastic-gradient-descent-for-machine-learning-clearly-explained", "anchor_text": "https://baptiste-monpezat.github.io/blog/stochastic-gradient-descent-for-machine-learning-clearly-explained"}, {"url": "https://medium.com/tag/data-science?source=post_page-----cadcc17d3d11---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----cadcc17d3d11---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/stochastic-gradient?source=post_page-----cadcc17d3d11---------------stochastic_gradient-----------------", "anchor_text": "Stochastic Gradient"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----cadcc17d3d11---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----cadcc17d3d11---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcadcc17d3d11&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstochastic-gradient-descent-for-machine-learning-clearly-explained-cadcc17d3d11&user=Baptiste+Monpezat&userId=fe51577b9797&source=-----cadcc17d3d11---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcadcc17d3d11&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstochastic-gradient-descent-for-machine-learning-clearly-explained-cadcc17d3d11&user=Baptiste+Monpezat&userId=fe51577b9797&source=-----cadcc17d3d11---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcadcc17d3d11&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstochastic-gradient-descent-for-machine-learning-clearly-explained-cadcc17d3d11&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----cadcc17d3d11--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fcadcc17d3d11&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstochastic-gradient-descent-for-machine-learning-clearly-explained-cadcc17d3d11&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----cadcc17d3d11---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----cadcc17d3d11--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----cadcc17d3d11--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----cadcc17d3d11--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----cadcc17d3d11--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----cadcc17d3d11--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----cadcc17d3d11--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----cadcc17d3d11--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----cadcc17d3d11--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@baptiste.monpezat?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@baptiste.monpezat?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Baptiste Monpezat"}, {"url": "https://medium.com/@baptiste.monpezat/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "24 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffe51577b9797&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstochastic-gradient-descent-for-machine-learning-clearly-explained-cadcc17d3d11&user=Baptiste+Monpezat&userId=fe51577b9797&source=post_page-fe51577b9797--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Ffe51577b9797%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstochastic-gradient-descent-for-machine-learning-clearly-explained-cadcc17d3d11&user=Baptiste+Monpezat&userId=fe51577b9797&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}