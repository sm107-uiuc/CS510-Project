{"url": "https://towardsdatascience.com/13-solutions-to-multi-arm-bandit-problem-for-non-mathematicians-1b88b4c0b3fc", "time": 1682994649.7049868, "path": "towardsdatascience.com/13-solutions-to-multi-arm-bandit-problem-for-non-mathematicians-1b88b4c0b3fc/", "webpage": {"metadata": {"title": "13 Solutions to Multi-Arm Bandit Problem for Non-Mathematicians | by Jae Duk Seo | Towards Data Science", "h1": "13 Solutions to Multi-Arm Bandit Problem for Non-Mathematicians", "description": "The problem is simple, we have a slot machine with n number of arms. And we have limited numbers of trials on which arm we can pull, also we don\u2019t know which arms will give us the most amount of\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.michaelpacheco.net/blog/RL-multi-armed-bandit-2", "anchor_text": "blog post", "paragraph_index": 18}, {"url": "https://lilianweng.github.io/lil-log/2018/01/23/the-multi-armed-bandit-problem-and-its-solutions.html", "anchor_text": "blog post", "paragraph_index": 19}, {"url": "https://www.youtube.com/watch?time_continue=24&v=fIKkhoI1kF4", "anchor_text": "video", "paragraph_index": 19}, {"url": "https://homes.di.unimi.it/~cesabian/Pubblicazioni/ml-02.pdf", "anchor_text": "Auer, Cesa-Bianchi & Fisher", "paragraph_index": 21}, {"url": "https://eigenfoo.xyz/bayesian-bandits/", "anchor_text": "here", "paragraph_index": 23}, {"url": "https://peterroelants.github.io/posts/multi-armed-bandit-implementation/", "anchor_text": "here", "paragraph_index": 23}, {"url": "https://en.wikipedia.org/wiki/Conjugate_prior", "anchor_text": "conjugate prior", "paragraph_index": 24}, {"url": "https://en.wikipedia.org/wiki/Bernoulli_distribution", "anchor_text": "Bernoulli", "paragraph_index": 24}, {"url": "https://www.michaelpacheco.net/blog/RL-multi-armed-bandit-3", "anchor_text": "blog post.", "paragraph_index": 25}, {"url": "https://colab.research.google.com/drive/1k0CCyHZ2-KdLTLfUxom01mQiJwffHqy7", "anchor_text": "here", "paragraph_index": 36}, {"url": "https://github.com/JaeDukSeo/Daily-Neural-Network-Practice-2/blob/master/reinforcement_learning/bandit/z%20bandit%20blog%20.ipynb", "anchor_text": "here", "paragraph_index": 36}, {"url": "https://colab.research.google.com/drive/1cHAs8BKEoP5viaSjeHawOZmC8YdrkS-E", "anchor_text": "here", "paragraph_index": 37}, {"url": "https://github.com/JaeDukSeo/Daily-Neural-Network-Practice-2/blob/master/reinforcement_learning/bandit/z%20bandit%20blog%20large.ipynb", "anchor_text": "here", "paragraph_index": 37}, {"url": "http://banditalgs.com/", "anchor_text": "blog post.", "paragraph_index": 38}, {"url": "https://www.cs.mcgill.ca/~vkules/bandits.pdf?fbclid=IwAR0zRTBvxp0Eam47JTABWDodxJtPcm8QtIID04RBfsaZATWFTkJeXMywQR4", "anchor_text": "paper", "paragraph_index": 39}, {"url": "https://jaedukseo.me/", "anchor_text": "website", "paragraph_index": 41}], "all_paragraphs": ["The problem is simple, we have a slot machine with n number of arms. And we have limited numbers of trials on which arm we can pull, also we don\u2019t know which arms will give us the most amount of money.", "Assuming that the probability distribution does not change over time (meaning that this is a stationary problem)\u2026.", "Which arm should we pull? Should we pull the arm that gave us the most amount of reward in the past or should we explore in hopes of getting more optimal arm?", "There are multiple solutions to this problem, and usually, people measure regret in order to rank each solution. (Regret == simply put, the amount of penalty that we get for not pulling the optimal arm.). So to minimize regret we just have to pull the arm that has the highest probability of giving us a reward.", "But I wanted to look at an additional measurement, specifically, I will also take into account how well each algorithm estimates the probability distribution for each arm. (the probability that they will give us a reward). And see each of their performance on a smaller scale, in which we only have 12 arms, and a larger scale, in which we have 1000 arms.", "Finally, the aim of this post is to provide a simple implementation of each solution, for non-mathematicians (like me). Hence the theoretical guarantees and proofs are not discussed but, I have provided different links for people who wish to study this problem more in depth.", "Below is the list of methods that we are going to compare\u2026..", "1)Vectorized 2) Greedy3) e-Greedy4) Decay e-Greedy5) Linear Reward Inaction (Pursuit Methods)6) Linear Reward Penalty (Pursuit Methods)7) Upper Confidence Bounds8) Upper Confidence Bounds Tuned9) Thompson Sampling (Beta Distribution)10) Thompson Sampling (Uniform Distribution)11) Neural Network 12) Boltzmann Exploration (Softmax)13) Gradient Bandits", "Please note that this method isn\u2019t actually a solution. The idea behind the multi-arm bandit problem is how to optimally balance between exploration and exploitation.", "Here we are just counting how much reward we are getting for each arm, and dividing by the number of times we pulled each arm. (hence calculating the percentage of getting reward directly.)", "I just wanted to show a simple method to estimate the probability distribution.", "Here our strategy is just to pull the arm that gives us the most reward. However, the problem at the beginning is the fact that we don\u2019t know how each arm\u2019s probability is distributed.", "So I just decided to give some \u2018informative\u2019 prior by initializing our estimate distribution from a uniform distribution. (I guess this can be somewhat related to \u2018optimal initialization scheme\u2019 but that was not my intention, also initializing our prior to being all zero can work as well.).", "Similar concept to Greedy method, but with some probability epsilon, we explore and select a random bandit rather than choosing the arm that we think has the highest probability.", "Exactly the same idea as the e-Greedy method, but here we are slowly decreasing the epsilon. (probability in which we explore). Hence in time, this method will be exactly the same as the Greedy method.", "The general idea behind pursuit method is to maintain an explicit policy over the bandits and update them using empirical means. In the beginning, the probability distribution is initialized uniformly (meaning each arm has an equal chance of getting picked) but over time the distribution changes.", "Since for pursuit methods, the estimation is normalized to have the sum of one, so rescaling might be appropriate for fair comparison.", "Linear Reward Penalty expands the idea behind Linear Reward Inaction in a way we are also penalizing the arm if we do not get a reward.", "For both Linear Reward Penalty and Inaction my friend, Michael\u2019s, has written a more detailed blog post.", "Simply put UCB follows a simple principle, act \u2018optimistically in face of uncertainty\u2019. Another way of understanding this concept is to prefer the arm that has been pulled less frequently, hence we are not so certain about that arm. (For more information please read this blog post or watch this video.).", "This method is very attractive since it has strong theoretical guarantees.", "Auer, Cesa-Bianchi & Fisher, proposed the UCB1-Tuned algorithm in the same paper in which they proposed UCB1. Interestingly this algorithm gave better empirical results, but the authors themselves were not able to give theoretical regret bound.", "This method is a Bayesian inference approach in which we have set of prior distributions over each arm, and as we observe how much reward we are getting for each arm, we update our posterior.", "For more information regarding this method please click here or here.", "Please note that this method was done purely for entertainment purpose. Uniform distribution is not a conjugate prior to Bernoulli distribution. I just wanted to try this method out, overall, when compared to Beta distribution, uniform distribution tends to explore more.", "This method is also for entertainment purpose, it does not have any theoretical guarantees. I was inspired after reading this blog post. The difference is the fact that I used KL divergence as the objective function and used the Adam optimizer.", "The Softmax method picks each arm with a probability that is proportional to its average reward. Hence if an arm gives a large reward, it has a higher probability of being selected.", "One interesting idea related to this method is the fact that we have a temperature parameter that controls the degree of exploration.", "Gradient bandit uses gradient ascent to find the optimal arm to pull. Simply put we have a variable called mean reward, that tracks the mean of reward until certain time t. And if the bandit we pulled gives higher reward then the mean, we increase the probability that the arms is chosen and vice versa if otherwise.", "Left Image \u2192 Regret Over Time for each MethodRight Image \u2192 Aggregated Reward Over Time for each Method", "When the method\u2019s regret grows in logarithmic time, it is considered to be a solution to the bandit problem. And from the plots above, we can see that UCB1 Tuned gave us the optimal results.", "However, when we view the correlation matrix between the ground truth probability distribution, we can see that e-greedy and softmax policy had one to one correlation. (vector method is not considered to be a solution.).", "Quite a surprising result can be seen above, we can notice right away that simpler methods such as greedy and decay e-greedy methods are able to outperform more advanced solutions. (sampling methods took way much time.)", "I am quite surprised to see that the Neural network method is performing well.", "But when it comes to estimating the probability distribution of 1000 arms we can see that the softmax and UCB1 method gave us the most accurate estimations.", "It seems like decay e-greedy method is the optimal solution overall, in both large and small experiment setting.", "To access the code for smaller experiment please click here or here.", "To access the code for larger experiment please click here or here.", "This simple problem is still being studied hence, many more advanced solutions exist, for further reading please read this blog post.", "Also from this paper, it is again shown that simple strategy such as e-greedy method can outperform more advanced methods in traditional multi-arm bandit problem as well as give competitive results in real life clinical trials.", "While theoretical guarantees are extremely important it is quite surprising to observe other simpler methods giving much better results.", "For more articles please visit my website.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Exploring the intersection of AI, deep learning, and art. Passionate about pushing the boundaries of multi-media production and beyond. #AIArt"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F1b88b4c0b3fc&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F13-solutions-to-multi-arm-bandit-problem-for-non-mathematicians-1b88b4c0b3fc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2F13-solutions-to-multi-arm-bandit-problem-for-non-mathematicians-1b88b4c0b3fc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F13-solutions-to-multi-arm-bandit-problem-for-non-mathematicians-1b88b4c0b3fc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2F13-solutions-to-multi-arm-bandit-problem-for-non-mathematicians-1b88b4c0b3fc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----1b88b4c0b3fc--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1b88b4c0b3fc--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@jdseo?source=post_page-----1b88b4c0b3fc--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@jdseo?source=post_page-----1b88b4c0b3fc--------------------------------", "anchor_text": "Jae Duk Seo"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F70eb2d57a447&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F13-solutions-to-multi-arm-bandit-problem-for-non-mathematicians-1b88b4c0b3fc&user=Jae+Duk+Seo&userId=70eb2d57a447&source=post_page-70eb2d57a447----1b88b4c0b3fc---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1b88b4c0b3fc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F13-solutions-to-multi-arm-bandit-problem-for-non-mathematicians-1b88b4c0b3fc&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1b88b4c0b3fc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F13-solutions-to-multi-arm-bandit-problem-for-non-mathematicians-1b88b4c0b3fc&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://giphy.com/gifs/upvote-machine-upvotegifs-4eQFLKTo1Tymc", "anchor_text": "website"}, {"url": "https://www.michaelpacheco.net/blog/RL-multi-armed-bandit-2", "anchor_text": "blog post"}, {"url": "https://lilianweng.github.io/lil-log/2018/01/23/the-multi-armed-bandit-problem-and-its-solutions.html", "anchor_text": "blog post"}, {"url": "https://www.youtube.com/watch?time_continue=24&v=fIKkhoI1kF4", "anchor_text": "video"}, {"url": "https://homes.di.unimi.it/~cesabian/Pubblicazioni/ml-02.pdf", "anchor_text": "Auer, Cesa-Bianchi & Fisher"}, {"url": "https://eigenfoo.xyz/bayesian-bandits/", "anchor_text": "here"}, {"url": "https://peterroelants.github.io/posts/multi-armed-bandit-implementation/", "anchor_text": "here"}, {"url": "https://en.wikipedia.org/wiki/Conjugate_prior", "anchor_text": "conjugate prior"}, {"url": "https://en.wikipedia.org/wiki/Bernoulli_distribution", "anchor_text": "Bernoulli"}, {"url": "https://www.michaelpacheco.net/blog/RL-multi-armed-bandit-3", "anchor_text": "blog post."}, {"url": "https://colab.research.google.com/drive/1k0CCyHZ2-KdLTLfUxom01mQiJwffHqy7", "anchor_text": "here"}, {"url": "https://github.com/JaeDukSeo/Daily-Neural-Network-Practice-2/blob/master/reinforcement_learning/bandit/z%20bandit%20blog%20.ipynb", "anchor_text": "here"}, {"url": "https://colab.research.google.com/drive/1cHAs8BKEoP5viaSjeHawOZmC8YdrkS-E", "anchor_text": "here"}, {"url": "https://github.com/JaeDukSeo/Daily-Neural-Network-Practice-2/blob/master/reinforcement_learning/bandit/z%20bandit%20blog%20large.ipynb", "anchor_text": "here"}, {"url": "http://banditalgs.com/", "anchor_text": "blog post."}, {"url": "https://www.cs.mcgill.ca/~vkules/bandits.pdf?fbclid=IwAR0zRTBvxp0Eam47JTABWDodxJtPcm8QtIID04RBfsaZATWFTkJeXMywQR4", "anchor_text": "paper"}, {"url": "https://jaedukseo.me/", "anchor_text": "website"}, {"url": "http://www-anw.cs.umass.edu/~barto/courses/cs687/Chapter%202.pdf", "anchor_text": "http://www-anw.cs.umass.edu/~barto/courses/cs687/Chapter%202.pdf"}, {"url": "https://lilianweng.github.io/lil-log/2018/01/23/the-multi-armed-bandit-problem-and-its-solutions.html", "anchor_text": "https://lilianweng.github.io/lil-log/2018/01/23/the-multi-armed-bandit-problem-and-its-solutions.html"}, {"url": "https://itnext.io/reinforcement-learning-with-multi-arm-bandit-decf442e02d2", "anchor_text": "https://itnext.io/reinforcement-learning-with-multi-arm-bandit-decf442e02d2"}, {"url": "https://itnext.io/reinforcement-learning-with-multi-arm-bandit-part-2-831a43f22a47", "anchor_text": "https://itnext.io/reinforcement-learning-with-multi-arm-bandit-part-2-831a43f22a47"}, {"url": "https://github.com/ankonzoid/LearningX/blob/master/classical_RL/MAB/MAB.py", "anchor_text": "https://github.com/ankonzoid/LearningX/blob/master/classical_RL/MAB/MAB.py"}, {"url": "https://www.michaelpacheco.net/blog/RL-multi-armed-bandit-2", "anchor_text": "https://www.michaelpacheco.net/blog/RL-multi-armed-bandit-2"}, {"url": "https://www.michaelpacheco.net/blog/RL-multi-armed-bandit-3", "anchor_text": "https://www.michaelpacheco.net/blog/RL-multi-armed-bandit-3"}, {"url": "https://dataorigami.net/blogs/napkin-folding/79031811-multi-armed-bandits", "anchor_text": "https://dataorigami.net/blogs/napkin-folding/79031811-multi-armed-bandits"}, {"url": "https://peterroelants.github.io/posts/multi-armed-bandit-implementation/", "anchor_text": "https://peterroelants.github.io/posts/multi-armed-bandit-implementation/"}, {"url": "https://www.cs.mcgill.ca/~vkules/bandits.pdf", "anchor_text": "https://www.cs.mcgill.ca/~vkules/bandits.pdf"}, {"url": "https://itnext.io/reinforcement-learning-with-multi-arm-bandit-decf442e02d2", "anchor_text": "https://itnext.io/reinforcement-learning-with-multi-arm-bandit-decf442e02d2"}, {"url": "https://github.com/ankonzoid/LearningX/tree/master/classical_RL/MAB", "anchor_text": "https://github.com/ankonzoid/LearningX/tree/master/classical_RL/MAB"}, {"url": "https://www.michaelpacheco.net/blog/RL-multi-armed-bandit-3", "anchor_text": "https://www.michaelpacheco.net/blog/RL-multi-armed-bandit-3"}, {"url": "https://www.w3schools.com/html/tryit.asp?filename=tryhtml_lists_intro", "anchor_text": "https://www.w3schools.com/html/tryit.asp?filename=tryhtml_lists_intro"}, {"url": "https://github.com/bgalbraith/bandits/blob/master/bandits/policy.py", "anchor_text": "https://github.com/bgalbraith/bandits/blob/master/bandits/policy.py"}, {"url": "http://www-anw.cs.umass.edu/~barto/courses/cs687/Chapter%202.pdf", "anchor_text": "http://www-anw.cs.umass.edu/~barto/courses/cs687/Chapter%202.pdf"}, {"url": "https://github.com/bgalbraith/bandits/blob/master/bandits/policy.py", "anchor_text": "https://github.com/bgalbraith/bandits/blob/master/bandits/policy.py"}, {"url": "https://gist.github.com/yusugomori/cf7bce19b8e16d57488a", "anchor_text": "https://gist.github.com/yusugomori/cf7bce19b8e16d57488a"}, {"url": "https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.minimum.html", "anchor_text": "https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.minimum.html"}, {"url": "https://stackoverflow.com/questions/7125009/how-to-change-legend-size-with-matplotlib-pyplot", "anchor_text": "https://stackoverflow.com/questions/7125009/how-to-change-legend-size-with-matplotlib-pyplot"}, {"url": "https://seaborn.pydata.org/generated/seaborn.heatmap.html", "anchor_text": "https://seaborn.pydata.org/generated/seaborn.heatmap.html"}, {"url": "https://stackoverflow.com/questions/31087613/heat-map-seaborn-fmt-d-error", "anchor_text": "https://stackoverflow.com/questions/31087613/heat-map-seaborn-fmt-d-error"}, {"url": "https://stackoverflow.com/questions/43694368/data-order-in-seaborn-heatmap-from-pivot", "anchor_text": "https://stackoverflow.com/questions/43694368/data-order-in-seaborn-heatmap-from-pivot"}, {"url": "https://stackoverflow.com/questions/33104322/auto-adjust-font-size-in-seaborn-heatmap", "anchor_text": "https://stackoverflow.com/questions/33104322/auto-adjust-font-size-in-seaborn-heatmap"}, {"url": "https://homes.di.unimi.it/~cesabian/Pubblicazioni/ml-02.pdf", "anchor_text": "https://homes.di.unimi.it/~cesabian/Pubblicazioni/ml-02.pdf"}, {"url": "https://en.wikipedia.org/wiki/Conjugate_prior", "anchor_text": "https://en.wikipedia.org/wiki/Conjugate_prior"}, {"url": "http://banditalgs.com/", "anchor_text": "http://banditalgs.com/"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----1b88b4c0b3fc---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----1b88b4c0b3fc---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----1b88b4c0b3fc---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/python?source=post_page-----1b88b4c0b3fc---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/mathematics?source=post_page-----1b88b4c0b3fc---------------mathematics-----------------", "anchor_text": "Mathematics"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1b88b4c0b3fc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F13-solutions-to-multi-arm-bandit-problem-for-non-mathematicians-1b88b4c0b3fc&user=Jae+Duk+Seo&userId=70eb2d57a447&source=-----1b88b4c0b3fc---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1b88b4c0b3fc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F13-solutions-to-multi-arm-bandit-problem-for-non-mathematicians-1b88b4c0b3fc&user=Jae+Duk+Seo&userId=70eb2d57a447&source=-----1b88b4c0b3fc---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1b88b4c0b3fc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F13-solutions-to-multi-arm-bandit-problem-for-non-mathematicians-1b88b4c0b3fc&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1b88b4c0b3fc--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F1b88b4c0b3fc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F13-solutions-to-multi-arm-bandit-problem-for-non-mathematicians-1b88b4c0b3fc&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----1b88b4c0b3fc---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----1b88b4c0b3fc--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----1b88b4c0b3fc--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----1b88b4c0b3fc--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----1b88b4c0b3fc--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----1b88b4c0b3fc--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----1b88b4c0b3fc--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----1b88b4c0b3fc--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----1b88b4c0b3fc--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@jdseo?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@jdseo?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Jae Duk Seo"}, {"url": "https://medium.com/@jdseo/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "5.2K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F70eb2d57a447&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F13-solutions-to-multi-arm-bandit-problem-for-non-mathematicians-1b88b4c0b3fc&user=Jae+Duk+Seo&userId=70eb2d57a447&source=post_page-70eb2d57a447--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fd9ea20dd433a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F13-solutions-to-multi-arm-bandit-problem-for-non-mathematicians-1b88b4c0b3fc&newsletterV3=70eb2d57a447&newsletterV3Id=d9ea20dd433a&user=Jae+Duk+Seo&userId=70eb2d57a447&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}