{"url": "https://towardsdatascience.com/learning-parameters-part-5-65a2f3583f7d", "time": 1683000630.1787379, "path": "towardsdatascience.com/learning-parameters-part-5-65a2f3583f7d/", "webpage": {"metadata": {"title": "Learning Parameters, Part 5: AdaGrad, RMSProp, and Adam | by Akshay L Chandra | Towards Data Science", "h1": "Learning Parameters, Part 5: AdaGrad, RMSProp, and Adam", "description": "In part 4, we looked at some heuristics that can help us tune the learning rate and momentum better. In this final article of the series, let us look at a more principled way of adjusting the\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/learning-parameters-part-4-6a18d1d3000b", "anchor_text": "part 4", "paragraph_index": 0}, {"url": "https://medium.com/tag/learning-parameters/latest", "anchor_text": "series", "paragraph_index": 0}, {"url": "https://www.cse.iitm.ac.in/~miteshk/CS7015.html", "anchor_text": "CS7015: Deep Learning", "paragraph_index": 1}, {"url": "https://www.cse.iitm.ac.in/~miteshk/", "anchor_text": "Prof. Mitesh Khapra", "paragraph_index": 1}, {"url": "https://towardsdatascience.com/learning-parameters-part-1-eb3e8bb9ffbb", "anchor_text": "part-1", "paragraph_index": 8}, {"url": "https://www.youtube.com/watch?v=-0ZMU-gnm2g", "anchor_text": "here", "paragraph_index": 15}, {"url": "https://www.cse.iitm.ac.in/~miteshk/", "anchor_text": "Prof. Mitesh M Khapra", "paragraph_index": 18}, {"url": "https://www.cse.iitm.ac.in/~miteshk/CS7015.html", "anchor_text": "CS7015: Deep Learning", "paragraph_index": 18}, {"url": "http://M.Sc", "anchor_text": "M.Sc", "paragraph_index": 20}], "all_paragraphs": ["In part 4, we looked at some heuristics that can help us tune the learning rate and momentum better. In this final article of the series, let us look at a more principled way of adjusting the learning rate and give the learning rate a chance to adapt.", "Citation Note: Most of the content and figures in this blog are directly taken from Lecture 5 of CS7015: Deep Learning course offered by Prof. Mitesh Khapra at IIT-Madras.", "Consider the following simple perceptron network with sigmoid activation.", "It should be easy to see that given a single point (x, y), gradients of w would be the following:", "Gradient of f(x) w.r.t to a particular weight is clearly dependent on its corresponding input. If there are n points, we can just sum the gradients over all the n points to get the total gradient. This news is neither new nor special. But what would happen if the feature x2 is very sparse (i.e., if its value is 0 for most inputs)? It is fair to assume that \u2207w2 will be 0 for most inputs (see formula) and hence w2 will not get enough updates.", "Why should that bother us though? It is important to note that if at all x2 is both sparse as well as important, we would want to take the updates to w2 seriously. To make sure updates happen even when a particular input is sparse, can we have a different learning rate for each parameter which takes care of the frequency of the features? We sure can. I mean that is the whole point of this article.", "Decay the learning rate for parameters in proportion to their update history (more updates means more decay).", "It is clear from the update rule that history of the gradient is accumulated in v. The smaller the gradient accumulated, the smaller the v value will be, leading to a bigger learning rate (because v divides \u03b7).", "To see AdaGrad in action, we need to first create some data where one of the features is sparse. How would we do this to the toy network we used across all parts of the Learning Parameters series? Well, our network has just two parameters (w and b, see Motivation in part-1). Of these, the input/feature corresponding to b is always on, so we can\u2019t really make it sparse. So the only option is to make x sparse. Which is why we created 100 random (x,y) pairs and then roughly 80% of these pairs we set x to 0, making the feature for w sparse.", "Before we actually look at AdaGrad in action, please look at the other 3 optimizers above - vanilla GD(black), momentum (red), NAG (blue). There is something interesting that these 3 optimizers are doing for this dataset. Can you spot it? Feel free to pause and ponder. Answer: Initially, all three algorithms are moving mainly along the vertical (b) axis and there is very little movement along the horizontal (w) axis. Why? Because in our data, the feature corresponding to w is sparse and hence w undergoes very few updates. On the other hand, b is very dense and undergoes many updates. Such sparsity is very common in large neural networks containing 1000s of input features and hence we need to address it. Let us now look at AdaGrad in action.", "Voila! By using a parameter specific learning rate AdaGrad ensures that despite sparsity w gets a higher learning rate and hence larger updates. Furthermore, it also ensures that if b undergoes a lot of updates, its effective learning rate decreases because of the growing denominator. In practice, this does not work so well if we remove the square root from the denominator (something to ponder about). What\u2019s the flipside? Over time the effective learning rate for b will decay to an extent that there will be no further updates to b. Can we avoid this? RMSProp can!", "AdaGrad decays the learning rate very aggressively (as the denominator grows). As a result, after a while, the frequent parameters will start receiving very small updates because of the decayed learning rate. To avoid this why not decay the denominator and prevent its rapid growth.", "Everything is very similar to AdaGrad, except now we decay the denominator as well.", "What do you see? How is RMSProp different from AdaGrad? Feel free to pause and ponder. Answer: AdaGrad got stuck when it was close to convergence, it was no longer able to move in the vertical (b) directionbecause of the decayed learning rate. RMSProp overcomes this problem by being less aggressive on the decay.", "Do everything that RMSProp does to solve the denominator decay problem of AdaGrad. In addition to that, use a cumulative history of gradients.", "and a similar set of equations for b_t. Notice that the update rule for Adam is very similar to RMSProp, except we look at the cumulative history of gradients as well (m_t). Note that the third step in the update rule above is bias correction. Explanation by Prof. Mitesh M Khapra on why bias correction is necessary can be found here.", "Quite clearly, taking a cumulative history of gradients speeds it up. For this toy dataset, it appears to be overshooting (a little) but even then it converges way faster than the other optimizers.", "In this final article of the series, we looked at how gradient descent with adaptive learning rate can help speed up convergence in neural networks. Intuition, python code and visual illustration of three widely used optimizers \u2014 AdaGrad, RMSProp, and Adam are covered in this article. Adam combines the best properties of RMSProp and AdaGrad to work well even with noisy or sparse datasets.", "A lot of credit goes to Prof. Mitesh M Khapra and the TAs of CS7015: Deep Learning course by IIT Madras for such rich content and creative visualizations. I merely just compiled the provided lecture notes and lecture videos concisely.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "M.Sc. Student @ University of Freiburg"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F65a2f3583f7d&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-parameters-part-5-65a2f3583f7d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-parameters-part-5-65a2f3583f7d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-parameters-part-5-65a2f3583f7d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-parameters-part-5-65a2f3583f7d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----65a2f3583f7d--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----65a2f3583f7d--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@acl21?source=post_page-----65a2f3583f7d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@acl21?source=post_page-----65a2f3583f7d--------------------------------", "anchor_text": "Akshay L Chandra"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F202534492f47&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-parameters-part-5-65a2f3583f7d&user=Akshay+L+Chandra&userId=202534492f47&source=post_page-202534492f47----65a2f3583f7d---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F65a2f3583f7d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-parameters-part-5-65a2f3583f7d&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F65a2f3583f7d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-parameters-part-5-65a2f3583f7d&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/tag/learning-parameters/latest", "anchor_text": "Learning Parameters"}, {"url": "https://towardsdatascience.com/learning-parameters-part-4-6a18d1d3000b", "anchor_text": "part 4"}, {"url": "https://medium.com/tag/learning-parameters/latest", "anchor_text": "series"}, {"url": "https://www.cse.iitm.ac.in/~miteshk/CS7015.html", "anchor_text": "CS7015: Deep Learning"}, {"url": "https://www.cse.iitm.ac.in/~miteshk/", "anchor_text": "Prof. Mitesh Khapra"}, {"url": "https://towardsdatascience.com/learning-parameters-part-1-eb3e8bb9ffbb", "anchor_text": "Part-1"}, {"url": "https://towardsdatascience.com/learning-parameters-part-1-eb3e8bb9ffbb", "anchor_text": "part-1"}, {"url": "https://www.youtube.com/watch?v=-0ZMU-gnm2g", "anchor_text": "here"}, {"url": "https://www.cse.iitm.ac.in/~miteshk/", "anchor_text": "Prof. Mitesh M Khapra"}, {"url": "https://www.cse.iitm.ac.in/~miteshk/CS7015.html", "anchor_text": "CS7015: Deep Learning"}, {"url": "https://blog.paperspace.com/intro-to-optimization-momentum-rmsprop-adam/", "anchor_text": "Ayoosh Kathuria"}, {"url": "https://medium.com/tag/gradient-descent?source=post_page-----65a2f3583f7d---------------gradient_descent-----------------", "anchor_text": "Gradient Descent"}, {"url": "https://medium.com/tag/optimization?source=post_page-----65a2f3583f7d---------------optimization-----------------", "anchor_text": "Optimization"}, {"url": "https://medium.com/tag/adam?source=post_page-----65a2f3583f7d---------------adam-----------------", "anchor_text": "Adam"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----65a2f3583f7d---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/rmsprop?source=post_page-----65a2f3583f7d---------------rmsprop-----------------", "anchor_text": "Rmsprop"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F65a2f3583f7d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-parameters-part-5-65a2f3583f7d&user=Akshay+L+Chandra&userId=202534492f47&source=-----65a2f3583f7d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F65a2f3583f7d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-parameters-part-5-65a2f3583f7d&user=Akshay+L+Chandra&userId=202534492f47&source=-----65a2f3583f7d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F65a2f3583f7d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-parameters-part-5-65a2f3583f7d&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----65a2f3583f7d--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F65a2f3583f7d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-parameters-part-5-65a2f3583f7d&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----65a2f3583f7d---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----65a2f3583f7d--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----65a2f3583f7d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----65a2f3583f7d--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----65a2f3583f7d--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----65a2f3583f7d--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----65a2f3583f7d--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----65a2f3583f7d--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----65a2f3583f7d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@acl21?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@acl21?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Akshay L Chandra"}, {"url": "https://medium.com/@acl21/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.1K Followers"}, {"url": "http://M.Sc", "anchor_text": "M.Sc"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F202534492f47&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-parameters-part-5-65a2f3583f7d&user=Akshay+L+Chandra&userId=202534492f47&source=post_page-202534492f47--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F4c4c11d45430&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-parameters-part-5-65a2f3583f7d&newsletterV3=202534492f47&newsletterV3Id=4c4c11d45430&user=Akshay+L+Chandra&userId=202534492f47&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}