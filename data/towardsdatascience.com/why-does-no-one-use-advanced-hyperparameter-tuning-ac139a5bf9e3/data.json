{"url": "https://towardsdatascience.com/why-does-no-one-use-advanced-hyperparameter-tuning-ac139a5bf9e3", "time": 1683014873.4863858, "path": "towardsdatascience.com/why-does-no-one-use-advanced-hyperparameter-tuning-ac139a5bf9e3/", "webpage": {"metadata": {"title": "Why Does No One Use Advanced Hyperparameter Tuning? | by Liam Li | Towards Data Science", "h1": "Why Does No One Use Advanced Hyperparameter Tuning?", "description": "Hyperparameter tuning (HP tuning) is an integral part of the machine learning development process and can play a key role in maximizing a model\u2019s predictive performance. As a result, hyperparameter\u2026"}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/Hyperparameter_optimization", "anchor_text": "Hyperparameter tuning", "paragraph_index": 0}, {"url": "https://en.wikipedia.org/wiki/Hyperparameter_optimization", "anchor_text": "many software implementations of these algorithms can be found online", "paragraph_index": 0}, {"url": "http://gael-varoquaux.info/science/survey-of-machine-learning-experimental-methods-at-neurips2019-and-iclr2020.html", "anchor_text": "researchers still primarily use these simple methods", "paragraph_index": 1}, {"url": "https://arxiv.org/abs/1603.06560", "anchor_text": "Hyperband HP tuning algorithm", "paragraph_index": 1}, {"url": "https://determined.ai/product/", "anchor_text": "Determined\u2019s integrated platform for deep learning", "paragraph_index": 3}, {"url": "https://arxiv.org/abs/1603.06560", "anchor_text": "Hyperband for efficient HP tuning via early-stopping", "paragraph_index": 6}, {"url": "https://arxiv.org/abs/1810.05934", "anchor_text": "a paper published at MLSys", "paragraph_index": 7}, {"url": "https://docs.determined.ai/latest/topic-guides/hp-tuning-det/hp-adaptive-asha.html#topic-guides-hp-tuning-det-adaptive-asha", "anchor_text": "our adaptive HP tuning algorithm", "paragraph_index": 8}, {"url": "https://blog.ml.cmu.edu/2018/12/12/massively-parallel-hyperparameter-optimization/", "anchor_text": "this blog post on ASHA for more details", "paragraph_index": 8}, {"url": "https://docs.determined.ai/latest/topic-guides/checkpoints.html#checkpoint-garbage-collection", "anchor_text": "checkpoint gc", "paragraph_index": 10}, {"url": "https://docs.determined.ai/latest/reference/experiment-config.html", "anchor_text": "checkpoint policy", "paragraph_index": 10}, {"url": "https://arxiv.org/pdf/2004.08900.pdf", "anchor_text": "models require thousands of hours to train", "paragraph_index": 11}, {"url": "https://determined.ai/assets/images/resources/Recursion-Drug-Discovery-with-Determined-Training-Platform.pdf", "anchor_text": "24x speedup with Determined in a drug discovery application", "paragraph_index": 11}, {"url": "https://docs.determined.ai/latest/topic-guides/optimizing-distributed-training.html#advanced-optimizations", "anchor_text": "more advanced optimization options for distributed training", "paragraph_index": 13}, {"url": "https://docs.determined.ai/latest/topic-guides/aws.html", "anchor_text": "AWS", "paragraph_index": 16}, {"url": "https://docs.determined.ai/latest/how-to/installation/gcp.html", "anchor_text": "GCP", "paragraph_index": 16}, {"url": "https://determined.ai/blog/scale-your-model-development-on-a-budget/", "anchor_text": "make their compute budget go even further with spot/preemptible instances", "paragraph_index": 18}, {"url": "https://slurm.schedmd.com/overview.html", "anchor_text": "SLURM", "paragraph_index": 19}, {"url": "https://en.wikipedia.org/wiki/Oracle_Grid_Engine", "anchor_text": "Sun Grid Engine", "paragraph_index": 19}, {"url": "https://determined.ai/blog/reproducibility-in-ml/", "anchor_text": "reproducibility in machine learning", "paragraph_index": 21}, {"url": "https://www.wired.com/story/artificial-intelligence-confronts-reproducibility-crisis/", "anchor_text": "1", "paragraph_index": 21}, {"url": "https://sites.google.com/view/icml-reproducibility-workshop/home", "anchor_text": "2", "paragraph_index": 21}, {"url": "https://determined.ai/blog/warm-starting-deep-learning/", "anchor_text": "warmstart training from a checkpoint", "paragraph_index": 22}, {"url": "https://docs.determined.ai/latest/topic-guides/hp-tuning-det/hp-adaptive-asha.html#topic-guides-hp-tuning-det-adaptive-asha", "anchor_text": "fine-grained control over the behavior of adaptive", "paragraph_index": 26}, {"url": "https://docs.determined.ai/latest/topic-guides/hp-tuning-det/index.html#other-supported-methods", "anchor_text": "other HP tuning algorithms", "paragraph_index": 26}, {"url": "https://docs.determined.ai/latest/reference/experiment-config.html#hyperparameters", "anchor_text": "specifying the search space with the hyperparameters and associated ranges", "paragraph_index": 27}, {"url": "https://arxiv.org/pdf/1810.05934.pdf", "anchor_text": "ASHA paper", "paragraph_index": 29}, {"url": "https://arxiv.org/abs/1807.01774", "anchor_text": "BOHB", "paragraph_index": 29}, {"url": "https://en.wikipedia.org/wiki/Neural_architecture_search", "anchor_text": "neural architecture search", "paragraph_index": 29}, {"url": "https://arxiv.org/abs/1806.09055", "anchor_text": "well-studied search space", "paragraph_index": 29}, {"url": "https://github.com/determined-ai/determined/tree/master/examples/hp_search_benchmarks", "anchor_text": "here", "paragraph_index": 29}, {"url": "https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/rnn_ctxt.pdf", "anchor_text": "Penn Treebank Dataset", "paragraph_index": 31}, {"url": "https://en.wikipedia.org/wiki/Perplexity", "anchor_text": "perplexity", "paragraph_index": 31}, {"url": "https://www.cs.toronto.edu/~kriz/cifar.html", "anchor_text": "CIFAR-10", "paragraph_index": 32}, {"url": "https://determined.ai/blog/faster-nlp-with-deep-learning-distributed-training/", "anchor_text": "NLP example where we use Determined to speed up training by 44x", "paragraph_index": 34}, {"url": "https://arxiv.org/pdf/1902.07638.pdf", "anchor_text": "this paper", "paragraph_index": 35}, {"url": "https://arxiv.org/pdf/1802.03268", "anchor_text": "ENAS", "paragraph_index": 35}, {"url": "https://arxiv.org/pdf/1806.09055", "anchor_text": "DARTS", "paragraph_index": 35}, {"url": "https://arxiv.org/pdf/1810.05749", "anchor_text": "GHN", "paragraph_index": 35}, {"url": "https://arxiv.org/pdf/1812.09926", "anchor_text": "SNAS", "paragraph_index": 35}, {"url": "https://docs.determined.ai/latest/tutorials/model-registry.html#organizing-models", "anchor_text": "model registry", "paragraph_index": 36}, {"url": "https://docs.determined.ai/latest/how-to/hyperparameter-tuning.html#hyperparameter-tuning", "anchor_text": "this tutorial", "paragraph_index": 37}, {"url": "https://arxiv.org/pdf/1902.07638.pdf", "anchor_text": "ASHA to be a strong baseline for NAS", "paragraph_index": 38}, {"url": "https://docs.determined.ai/latest/how-to/install-main.html#install-cluster", "anchor_text": "install a Determined cluster", "paragraph_index": 38}], "all_paragraphs": ["Hyperparameter tuning (HP tuning) is an integral part of the machine learning development process and can play a key role in maximizing a model\u2019s predictive performance. As a result, hyperparameter tuning algorithms have been widely studied in academia, and many software implementations of these algorithms can be found online.", "However, there is a large gap between implementing the pseudocode presented in academic research papers and performing large-scale HP tuning as part of an end-to-end machine learning workflow. Even in the ML research community, where it is well known that modern HP tuning methods are far superior to random search and grid search, researchers still primarily use these simple methods because more advanced methods are too difficult to use. I have experienced this myself, as I moved from developing the Hyperband HP tuning algorithm as a grad student, to running HP tuning algorithms on large-scale problems at Google, to more recently integrating state-of-the-art (SOTA) HP tuning capabilities into Determined\u2019s training platform. In this blog post, I will share the insights I\u2019ve gained throughout my journey so that you can get a jump start on applying SOTA HP tuning to your most pressing ML problems.", "So why is it so hard to use advanced HP tuning techniques in practice? Based on my experience, the three main challenges to applying SOTA HP tuning methods as part of an end-to-end ML workflow are:", "Given the discussion above, it\u2019s clear that the HP tuning algorithm itself is just one part of applying HP tuning in practice; these challenges need to be addressed with other equally important capabilities like distributed training, cluster management, and usability. These capabilities are already supported within Determined\u2019s integrated platform for deep learning so that users can focus on model development without being bogged down in the operational complexity associated with deep learning.", "In the sections to follow, I will share how we have addressed these key challenges to applying SOTA HP tuning by leveraging our expertise and building upon existing capabilities in our integrated system. In particular, I will give a high-level overview of the following capabilities that we view to be necessary for advanced HP tuning in practice:", "With ever-larger models and longer training times, hyperparameter tuning algorithms that efficiently exploit massive parallelism are crucial for large-scale applications. Our two criteria when selecting an HP tuning algorithm were:", "In terms of efficiency, leading HP tuning algorithms exploit early-stopping to reduce the computational cost; the main idea is to allocate fewer resources to poor HP settings so that a quality configuration can be found sooner. In the course of my graduate studies, my collaborators and I introduced Hyperband for efficient HP tuning via early-stopping. Our results showed Hyperband to be over 20x faster than simple random search and demonstrated SOTA performance by beating previously SOTA Bayesian optimization methods. Since then, owing to its simplicity and theoretical soundness, Hyperband has become one of the most popular methods for HP tuning.", "For the second criterion, large-scale HP tuning algorithms need to be robust to commonplace disturbances in real-world computing clusters like stragglers and task failures. In this respect, while the Hyperband algorithm was amenable to parallelization, the synchronization steps in the algorithm introduced significant bottlenecks as the rate of failures increased. To address this issue, we recently introduced an improved algorithm for massively parallel HP tuning called asynchronous successive halving (ASHA) that achieves SOTA performance in a paper published at MLSys.", "Determined provides SOTA HP tuning functionality via our adaptive HP tuning algorithm, which builds upon ASHA to improve ease-of-use. Our adaptive algorithm exploits early-stopping to evaluate up to 100x more hyperparameter settings than brute force approaches like random and grid search (see this blog post on ASHA for more details). As ML practitioners, we are well aware that it\u2019s one thing to claim SOTA performance and another to demonstrate it with real-world experiments. For the curious, feel free to skip to our benchmarking results to see Determined\u2019s adaptive HP tuning algorithm in action.", "The ability to pause and resume training without wasting too much computation is critical to maximizing the efficiency of early-stopping based HP tuning methods; Determined\u2019s adaptive HP tuning algorithm is no exception. Along the way, thousands of trials are paused and resumed as the algorithm adaptively allocates training resources to more performant HP settings. Without support for efficient saving and restoring of stateful objects, a meaningful portion of the computation would be duplicated when the adaptive tuning algorithm decides to resume a trial for further training.", "To reach SOTA performance with Determined\u2019s adaptive HP tuning algorithm, we support efficient early-stopping in the same way we support fault tolerant-machine learning in general: by automatically saving models and other stateful objects so that we can resume training without losing too much computation after a failure. In lieu of Determined, users have to write boilerplate code to save and resume models correctly depending on storage location (e.g., AWS S3, Google Cloud Storage, or a distributed file system). This is highly non-trivial to do correctly and can be further complicated by surrounding requirements for distributed training and reproducibility. Determined handles these more complex use cases for you and allows you to specify a checkpoint policy to control the storage footprint of your experiments (see checkpoint gc and checkpoint policy).", "SOTA HP tuning alone is not sufficient for large-scale deep learning, especially if models require thousands of hours to train; it is simply intractable for users to wait weeks or months for an experiment to complete. Fortunately, training times can be reduced significantly with distributed training (e.g., 24x speedup with Determined in a drug discovery application).", "The combination of adaptive HP tuning and distributed training in Determined enables truly large-scale model development for cutting edge AI with near zero boilerplate code. Enabling distributed training in Determined is as easy as toggling a single experiment configuration field:", "With this configuration, each trial within an HP tuning experiment will use 64 GPUs to train a single HP setting. Behind the scenes, Determined handles the complexities associated with data sharding and communication of model parameters for you. Determined also supports more advanced optimization options for distributed training that can further speedup your experiments.", "Given that Determined\u2019s adaptive HP tuning algorithm can often find a high-quality HP configuration in approximately the time it takes to train a single model to convergence, distributed training makes HP tuning tractable for even the largest models.", "Running large-scale HP tuning experiments in a distributed fashion requires coordinating across multiple instances to execute workloads indicated by the HP tuning algorithm. In contrast to random search and grid search, intermediate results need to be communicated to the algorithm so that the algorithm state can be updated to generate future workloads. Setting up such a cluster is time-consuming, tedious, and often requires a separate solution for each compute platform. Determined handles much of the operational side of deep-learning by automatically provisioning resources on AWS/GCP to set up a cluster and then scheduling experiments on that cluster once it\u2019s launched.", "In Determined, users can launch an AWS or GCP cluster with a single command:", "With this command, Determined will create a cluster with the necessary networking between instances for HP tuning and distributed training. Then, Determined automatically scales instances up and down as necessary to train the active experiments.", "Users can also make their compute budget go even further with spot/preemptible instances which are often 3x cheaper. There is certainly the risk of wasting computation with spot/preemptible instances since they can be shut down when demand rises. With Determined, these risks are largely mitigated by the built-in support for saving and resuming experiments that we discussed in the previous section.", "First-in-first-out (FIFO) scheduling is still fairly common for computing clusters due to its simplicity and status as the default for workload managers like SLURM and Sun Grid Engine. However, this scheduling mechanism is poorly suited for machine learning clusters for two main reasons. First, it can be suboptimal from a resource utilization perspective to require users to specify static resource requirements for every experiment. To illustrate, consider a cluster with 10 slots and an HP tuning experiment that can benefit from 10 slots in the exploration phase but will only require an average of 4 slots over the course of the experiment. Second, FIFO scheduling can result in poor sharing of cluster resources among users, as a single large job could saturate the cluster and block all other user jobs, see the figure below for example.", "Determined enables everyone to be productive while maximizing cluster utilization by using a centralized fair-share scheduler. The scheduler adaptively assigns resources to experiments as they are submitted and worked on. This allows experiments to exploit the maximum level of parallelism when the compute resources are available. In the presence of resource contention however, our scheduler shares resources across experiments to allow all users to make progress. This behavior is especially desirable for teams since at any given time the cluster will have to process experiments of different sizes from notebooks requiring a single GPU to HP tuning experiments with thousands of different trials.", "Reproducibility is important for reducing errors and building upon the work of others. Although we have seen rising awareness of the challenges concerning reproducibility in machine learning (e.g., 1, 2), reproducibility is still difficult to achieve due to the multitude of moving parts throughout the course of model development. Moreover, SOTA parallel HP tuning methods add another layer of complexity to reproducibility due to the asynchronous nature of the algorithms.", "Determined stores all the experiment artifacts mentioned above in a managed database for easy access in the future. This enables users to do things like (1) resume HP tuning experiments and continue where they left off, (2) fork an experiment and run with a different configuration, and (3) warmstart training from a checkpoint by specifying a checkpoint ID.", "Our earlier observation that most ML researchers still use simple HP tuning methods like manual, grid, or random search is perhaps unsurprising since using advanced HP tuning methods can introduce significant complexity. In particular, it is difficult to apply these methods in practice because", "In the case of Determined\u2019s adaptive HP tuning algorithm, for (1) we have simplified the user interface by configuring the search algorithm with robust default values that have worked well across a wide range of HP tuning experiments. To use adaptive HP tuning for your experiment in Determined, simply specify the searcher portion of the experiment configuration like below.", "Determined enables reproducible HP tuning by tracking the intermediate performance of all trials for subsequent replay. On the trial level, Determined offers fault-tolerant reproducibility by tracking and saving all stateful objects (including random generators). These capabilities are accompanied by other components that make Determined reproducible by design (see figure below). With automated tracking of the environment, code, and experiment artifacts, Determined allows users to reproduce HP tuning experiments with the click of a button.", "By design, the searcher configuration scheme for adaptive mirrors random search and grid search, where the main inputs correspond to the number of HP settings (i.e., trials) to evaluate, and how long to train each trial. An advanced user can optionally specify an early-stopping mode, with more aggressive early-stopping potentially offering higher speedups using noisier signals to allocate training resources. More fine-grained control over the behavior of adaptive is supported as well along with other HP tuning algorithms.", "For (2), our HP tuning capabilities work seamlessly with our integrated system so you can easily move from training a single model to tuning the hyperparameters of your model across multiple machines. This simply requires specifying the search space with the hyperparameters and associated ranges you want to search across in an experiment configuration. No need to deal with scheduling across multiple machines or modifying your code to work with different HP tuning libraries.", "After an HP tuning experiment is launched, it is important to monitor the progress and make adjustments as needed. As a researcher, I resorted to monitoring the logs to sanity check my results and usually waited until the entire HP tuning experiment completed before analyzing the results. With Determined, users can monitor and manage experiments through our web UI (see figure below). For a given experiment, the interface shows the best validation performance achieved by any trial so far and summarizes all the hyperparameter settings evaluated. Users can also easily manage their experiments, e.g., pausing the experiment if a suitable HP setting has been identified, resuming the experiment if further tuning is required, and forking the experiment to run with a modified search space.", "Let\u2019s tie this all together with benchmarks of Determined\u2019s adaptive searcher against the reference implementation used in the ASHA paper and BOHB, another popular HP tuning method. In particular, we will use two of the benchmarks studied in the ASHA paper for neural architecture search (NAS) over a well-studied search space. The code to reproduce the results for Determined\u2019s adaptive searcher is available here for you to follow along.", "HP tuning methods are evaluated according to search speed and search quality; i.e., how fast can the algorithm find a high-quality HP setting? To evaluate this, we track the validation metric of the best performing HP setting found by the search method through time and compare the resulting learning curves. For the two benchmarks below, we average results over 5 HP tuning experiments for more robust comparisons.", "This search space includes over 15 billion possible architectures corresponding to different recurrent cells for language modeling. We trained and evaluated different architectures on the Penn Treebank Dataset and recorded the best validation perplexity (lower is better) as the adaptive searcher progressed. The chart below shows that Determined\u2019s adaptive searcher slightly outperforms the reference implementation of ASHA and dominates BOHB. In fact, in just two hours, Determined is able to automatically find a model with a perplexity below 76 by considering over 300 configurations; after six hours, Determined has explored ~1k different configurations compared to the ~20 configurations that would have been evaluated by random search. To put this in perspective, it cost $50 for Determined to evaluate 1k configurations with preemptible instances compared to the $7k it would cost for random search to evaluate 1k configurations with on-demand instances.", "This search space includes over a quintillion (or 10\u00b9\u2078) possible architectures corresponding to different convolutional neural networks for computer vision. We trained and evaluated different architectures on CIFAR-10 and recorded the best validation accuracy as the adaptive searcher progressed. The chart below shows that Determined\u2019s adaptive searcher matches the reference implementation of ASHA and is also faster than BOHB at finding a good CNN architecture. The story here is similar to that for the RNN search space: after 20 hours, Determined has explored ~1k different configurations using just $150 on preemptible instances compared to $23k that would be required for random search to evaluate 1k configurations using on-demand instances.", "Let\u2019s continue with the NAS CNN benchmark to demonstrate what end-to-end model development looks like with Determined.", "After performing HP tuning to identify a good CNN architecture, we need to further validate the architecture by training it with multiple random initializations and evaluating it on the test set. This step would usually take nearly 40 hours on a single GPU with a batch size of 96. With Determined, we easily reduced the time by two fold by increasing the batch size to 256 and parallelizing across 2 GPUs. Note you can further increase the degree of parallelism and aim for higher speedups (e.g., this NLP example where we use Determined to speed up training by 44x).", "The best architecture found by adaptive search had a test accuracy of 97.21 on CIFAR-10, which outperforms the ASHA baseline in this paper (cf. Table 5) and matches or exceeds the performance of many complex NAS methods (e.g., ENAS, DARTS, GHN, SNAS).", "When you find the model you are happy with, you can save it to the model registry for model versioning/tracking and easy access in downstream applications:", "In this post, we shared how Determined addresses the key challenges to applying SOTA HP tuning in end-to-end ML workflows. Our benchmark results showed that HP tuning using our integrated system matches the SOTA results for HP tuning from research. To try out our SOTA HP tuning for your problems, take a look at this tutorial to get started!", "The benchmarks above also demonstrated how you can apply Determined to the problem of NAS. Our results confirmed our prior work showing ASHA to be a strong baseline for NAS. You can install a Determined cluster and modify the dataloader in the provided code to try NAS for your ML tasks today!", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fac139a5bf9e3&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-does-no-one-use-advanced-hyperparameter-tuning-ac139a5bf9e3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-does-no-one-use-advanced-hyperparameter-tuning-ac139a5bf9e3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-does-no-one-use-advanced-hyperparameter-tuning-ac139a5bf9e3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-does-no-one-use-advanced-hyperparameter-tuning-ac139a5bf9e3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----ac139a5bf9e3--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ac139a5bf9e3--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@liamcli?source=post_page-----ac139a5bf9e3--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@liamcli?source=post_page-----ac139a5bf9e3--------------------------------", "anchor_text": "Liam Li"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fec8ce5487f11&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-does-no-one-use-advanced-hyperparameter-tuning-ac139a5bf9e3&user=Liam+Li&userId=ec8ce5487f11&source=post_page-ec8ce5487f11----ac139a5bf9e3---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fac139a5bf9e3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-does-no-one-use-advanced-hyperparameter-tuning-ac139a5bf9e3&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fac139a5bf9e3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-does-no-one-use-advanced-hyperparameter-tuning-ac139a5bf9e3&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://en.wikipedia.org/wiki/Hyperparameter_optimization", "anchor_text": "Hyperparameter tuning"}, {"url": "https://en.wikipedia.org/wiki/Hyperparameter_optimization", "anchor_text": "many software implementations of these algorithms can be found online"}, {"url": "http://gael-varoquaux.info/science/survey-of-machine-learning-experimental-methods-at-neurips2019-and-iclr2020.html", "anchor_text": "researchers still primarily use these simple methods"}, {"url": "https://arxiv.org/abs/1603.06560", "anchor_text": "Hyperband HP tuning algorithm"}, {"url": "https://determined.ai/product/", "anchor_text": "Determined\u2019s integrated platform for deep learning"}, {"url": "http://0b12", "anchor_text": "Distributed training for large-scale models"}, {"url": "https://arxiv.org/abs/1603.06560", "anchor_text": "Hyperband for efficient HP tuning via early-stopping"}, {"url": "https://arxiv.org/abs/1810.05934", "anchor_text": "a paper published at MLSys"}, {"url": "https://docs.determined.ai/latest/topic-guides/hp-tuning-det/hp-adaptive-asha.html#topic-guides-hp-tuning-det-adaptive-asha", "anchor_text": "our adaptive HP tuning algorithm"}, {"url": "https://blog.ml.cmu.edu/2018/12/12/massively-parallel-hyperparameter-optimization/", "anchor_text": "this blog post on ASHA for more details"}, {"url": "https://docs.determined.ai/latest/topic-guides/checkpoints.html#checkpoint-garbage-collection", "anchor_text": "checkpoint gc"}, {"url": "https://docs.determined.ai/latest/reference/experiment-config.html", "anchor_text": "checkpoint policy"}, {"url": "https://arxiv.org/pdf/2004.08900.pdf", "anchor_text": "models require thousands of hours to train"}, {"url": "https://determined.ai/assets/images/resources/Recursion-Drug-Discovery-with-Determined-Training-Platform.pdf", "anchor_text": "24x speedup with Determined in a drug discovery application"}, {"url": "https://docs.determined.ai/latest/topic-guides/optimizing-distributed-training.html#advanced-optimizations", "anchor_text": "more advanced optimization options for distributed training"}, {"url": "https://docs.determined.ai/latest/topic-guides/aws.html", "anchor_text": "AWS"}, {"url": "https://docs.determined.ai/latest/how-to/installation/gcp.html", "anchor_text": "GCP"}, {"url": "https://determined.ai/blog/scale-your-model-development-on-a-budget/", "anchor_text": "make their compute budget go even further with spot/preemptible instances"}, {"url": "https://slurm.schedmd.com/overview.html", "anchor_text": "SLURM"}, {"url": "https://en.wikipedia.org/wiki/Oracle_Grid_Engine", "anchor_text": "Sun Grid Engine"}, {"url": "https://determined.ai/blog/reproducibility-in-ml/", "anchor_text": "reproducibility in machine learning"}, {"url": "https://www.wired.com/story/artificial-intelligence-confronts-reproducibility-crisis/", "anchor_text": "1"}, {"url": "https://sites.google.com/view/icml-reproducibility-workshop/home", "anchor_text": "2"}, {"url": "https://determined.ai/blog/warm-starting-deep-learning/", "anchor_text": "warmstart training from a checkpoint"}, {"url": "https://docs.determined.ai/latest/topic-guides/hp-tuning-det/hp-adaptive-asha.html#topic-guides-hp-tuning-det-adaptive-asha", "anchor_text": "fine-grained control over the behavior of adaptive"}, {"url": "https://docs.determined.ai/latest/topic-guides/hp-tuning-det/index.html#other-supported-methods", "anchor_text": "other HP tuning algorithms"}, {"url": "https://docs.determined.ai/latest/reference/experiment-config.html#hyperparameters", "anchor_text": "specifying the search space with the hyperparameters and associated ranges"}, {"url": "https://arxiv.org/pdf/1810.05934.pdf", "anchor_text": "ASHA paper"}, {"url": "https://arxiv.org/abs/1807.01774", "anchor_text": "BOHB"}, {"url": "https://en.wikipedia.org/wiki/Neural_architecture_search", "anchor_text": "neural architecture search"}, {"url": "https://arxiv.org/abs/1806.09055", "anchor_text": "well-studied search space"}, {"url": "https://github.com/determined-ai/determined/tree/master/examples/hp_search_benchmarks", "anchor_text": "here"}, {"url": "https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/rnn_ctxt.pdf", "anchor_text": "Penn Treebank Dataset"}, {"url": "https://en.wikipedia.org/wiki/Perplexity", "anchor_text": "perplexity"}, {"url": "https://www.cs.toronto.edu/~kriz/cifar.html", "anchor_text": "CIFAR-10"}, {"url": "https://determined.ai/blog/faster-nlp-with-deep-learning-distributed-training/", "anchor_text": "NLP example where we use Determined to speed up training by 44x"}, {"url": "https://arxiv.org/pdf/1902.07638.pdf", "anchor_text": "this paper"}, {"url": "https://arxiv.org/pdf/1802.03268", "anchor_text": "ENAS"}, {"url": "https://arxiv.org/pdf/1806.09055", "anchor_text": "DARTS"}, {"url": "https://arxiv.org/pdf/1810.05749", "anchor_text": "GHN"}, {"url": "https://arxiv.org/pdf/1812.09926", "anchor_text": "SNAS"}, {"url": "https://docs.determined.ai/latest/tutorials/model-registry.html#organizing-models", "anchor_text": "model registry"}, {"url": "https://docs.determined.ai/latest/how-to/hyperparameter-tuning.html#hyperparameter-tuning", "anchor_text": "this tutorial"}, {"url": "https://arxiv.org/pdf/1902.07638.pdf", "anchor_text": "ASHA to be a strong baseline for NAS"}, {"url": "https://docs.determined.ai/latest/how-to/install-main.html#install-cluster", "anchor_text": "install a Determined cluster"}, {"url": "https://determined.ai/blog/why-does-no-one-use-advanced-hp-tuning/", "anchor_text": "https://determined.ai"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----ac139a5bf9e3---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----ac139a5bf9e3---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----ac139a5bf9e3---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----ac139a5bf9e3---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/automl?source=post_page-----ac139a5bf9e3---------------automl-----------------", "anchor_text": "Automl"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fac139a5bf9e3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-does-no-one-use-advanced-hyperparameter-tuning-ac139a5bf9e3&user=Liam+Li&userId=ec8ce5487f11&source=-----ac139a5bf9e3---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fac139a5bf9e3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-does-no-one-use-advanced-hyperparameter-tuning-ac139a5bf9e3&user=Liam+Li&userId=ec8ce5487f11&source=-----ac139a5bf9e3---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fac139a5bf9e3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-does-no-one-use-advanced-hyperparameter-tuning-ac139a5bf9e3&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ac139a5bf9e3--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fac139a5bf9e3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-does-no-one-use-advanced-hyperparameter-tuning-ac139a5bf9e3&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----ac139a5bf9e3---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----ac139a5bf9e3--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----ac139a5bf9e3--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----ac139a5bf9e3--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----ac139a5bf9e3--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----ac139a5bf9e3--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----ac139a5bf9e3--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----ac139a5bf9e3--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----ac139a5bf9e3--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@liamcli?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@liamcli?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Liam Li"}, {"url": "https://medium.com/@liamcli/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "12 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fec8ce5487f11&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-does-no-one-use-advanced-hyperparameter-tuning-ac139a5bf9e3&user=Liam+Li&userId=ec8ce5487f11&source=post_page-ec8ce5487f11--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Fec8ce5487f11%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-does-no-one-use-advanced-hyperparameter-tuning-ac139a5bf9e3&user=Liam+Li&userId=ec8ce5487f11&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}