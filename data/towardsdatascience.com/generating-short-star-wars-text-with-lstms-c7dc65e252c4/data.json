{"url": "https://towardsdatascience.com/generating-short-star-wars-text-with-lstms-c7dc65e252c4", "time": 1683014703.247209, "path": "towardsdatascience.com/generating-short-star-wars-text-with-lstms-c7dc65e252c4/", "webpage": {"metadata": {"title": "Generating Short Star Wars Text With LSTM\u2019s | by Pedro Henrique Gomes Venturott | Towards Data Science", "h1": "Generating Short Star Wars Text With LSTM\u2019s", "description": "I am, without a doubt, one of those human beings known as \u201cStar Wars fans\u201d. I remember watching The Phantom Menace on TV and being blown away by it when I was a kid (I know, it is not a great movie)\u2026"}, "outgoing_paragraph_urls": [{"url": "https://chatbotslife.com/tagged/artificial-intelligence", "anchor_text": "NLP", "paragraph_index": 1}, {"url": "https://chatbotslife.com/", "anchor_text": "Natural Language Processing", "paragraph_index": 1}, {"url": "https://starwars.fandom.com/wiki/Main_Page", "anchor_text": "Wookiepedia.com", "paragraph_index": 2}, {"url": "https://www.fandom.com/", "anchor_text": "Fandom.com", "paragraph_index": 2}, {"url": "https://github.com/Pedrohgv/Star-Wars-Text-Generation", "anchor_text": "here", "paragraph_index": 4}, {"url": "https://drive.google.com/drive/folders/1JTzVV8uir74BlqF9HXtYMZdIu_4ZDpBP?usp=sharing", "anchor_text": "here", "paragraph_index": 6}, {"url": "https://starwars.fandom.com/wiki/Main_Page", "anchor_text": "Wookiepedia website", "paragraph_index": 9}, {"url": "https://github.com/Pedrohgv/Star-Wars-Text-Generation/blob/master/wookiescraper.py", "anchor_text": "wookiescraper.py", "paragraph_index": 9}, {"url": "https://github.com/Pedrohgv/Star-Wars-Text-Generation/blob/master/data_processor.py", "anchor_text": "data_processor.py", "paragraph_index": 11}, {"url": "https://en.wikipedia.org/wiki/Long_short-term_memory", "anchor_text": "LSTM", "paragraph_index": 12}, {"url": "https://chatbotslife.com/ultimate-guide-to-leveraging-nlp-machine-learning-for-you-chatbot-531ff2dd870c", "anchor_text": "NLP", "paragraph_index": 12}, {"url": "https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html", "anchor_text": "sequence-to-sequence", "paragraph_index": 12}, {"url": "https://github.com/Pedrohgv/Star-Wars-Text-Generation/blob/master/run.py", "anchor_text": "run.py", "paragraph_index": 12}, {"url": "https://github.com/Pedrohgv/Star-Wars-Text-Generation/blob/master/callbacks.py", "anchor_text": "callbacks.py", "paragraph_index": 19}, {"url": "https://github.com/Pedrohgv/Star-Wars-Text-Generation/tree/master/deploy", "anchor_text": "deploy", "paragraph_index": 22}, {"url": "https://pedrohgv.github.io/", "anchor_text": "https://pedrohgv.github.io/", "paragraph_index": 26}], "all_paragraphs": ["I am, without a doubt, one of those human beings known as \u201cStar Wars fans\u201d. I remember watching The Phantom Menace on TV and being blown away by it when I was a kid (I know, it is not a great movie). Later, I watched Attack of the Clones and Revenge of the Sith (this last on the theater) and went on to watch the Original Trilogy on DVD, back when those were still around. This culminated with me, as an adult, having a tattoo of a Tie Fighter on my right arm.", "This passion for Star Wars was a good answer to the question \u201cwhat NLP project should I do?\u201d; I wanted to develop a complete Natural Language Processing project, practicing several skills while doing it.", "This is a brief documentation of a project experiment on LSTM\u2019s and how to train a language model to generate texts within a specific domain at a character level. Given a seed title, it writes a brief description of it through an API. The model was built using Tensorflow from scratch, without transfer learning. It uses texts from Wookiepedia.com, and the team at Fandom.com (responsible for managing the website) was kind enough to allow me to collect the data using a web scraper and to publish this article.", "A demonstration of the working model can be seen below.", "In order to replicate this model, you need to download the code from here. Then, install all needed dependencies (it is very recommended to create a new virtual environment and install all packages on it) if you are using Linux with:", "If you are on Windows, however, just use:", "After all dependencies are installed, you need the trained model to be able to generate any text. Because of size restrictions on GitHub, the model must be downloaded from here. Just download the entire model folder and put it under the deploy folder. Finally, with the environment you installed all dependencies on activated and with an open terminal on the folder you downloaded the project, type:", "Below is a list of all the important libraries used:", "We will now go through all the main parts of the project.", "The texts used to train the model were mined from the Wookiepedia website (a sort of Star Wars Wikipedia) using web scrapping. All the code used for this task is on the wookiescraper.py file and a class Article was created to structure the texts, with each article containing a title, a description of the subject (this will be the first and brief description on the article's page), and the categories in which they belong. The main libraries for extracting the data were beautifulsoup, requests, and Pandas (to store texts in a dataframe).", "In order to list all possible articles, the function create_complete_database is called. It creates a list of URL's of all Canon articles (in the Star Wars Universe there was a reboot of all stories that were produced in alternative media like books, comics, and video games; the old stories were labeled as \"Legends\" while the stories that remained official and new stories are considered to be Canon). Then, it downloads and creates each article on the list by using the Article class's own functions. A dataframe is then created containing all downloaded articles and saved on a Complete Database.csv file.", "To feed the model, we must also process the acquired data; the file data_processor.py contains all the code used for this task. The function clean_data takes a dataframe of texts and formats it by removing undesired characters and shortening texts (this must be done because since we are creating a model that works on a character level, the model will have a hard time learning patterns and context from longer sentences). The file also contains functions that transform a given text corpus into one-hot vectors and vice-versa, as well as dataset generator functions; instead of loading all data into memory during training, the function build_datasets will build a training_dataset and a validation_dataset, each one being a Tensorflow Dataset object that process and feeds data into the model as chunks during training.", "LSTM is a type of Recurrent Neural Network cell that has a higher capacity of holding information over long sequences of data. Because of this feature, it is very useful when dealing with NLP problems. For this project, a sequence-to-sequence architecture was used to generate the output sentences. In this approach, an input string (article\u2019s title) is given to an encoder that processes the data sequentially character by character and delivers an encoded vector that contains information about the input. Then, a decoder will use this information to generate, again sequentially and character by character, a new embedded vector that will then go into a softmax layer to produce a vector of probabilities, one value for each possible character. Each output character is generated after the previous one, always using also the vector containing the input information generated by the encoder. The code for creating and training the model is in the file run.py.", "After GPU initialization and vocabulary definition, a config dictionary was created in order to make hyperparameter tunning easier:", "A seed will also be used in order to make results reproducible:", "After that, finally, the acquired data is loaded into a dataframe, cleaned, and new configuration options can be set now, like the train/validation split:", "The learning rate is then defined. In this project, an exponentially decaying learning rate was shown to give the best results:", "With the data loaded, both training and validation datasets can now be built:", "With the goal of saving the model after training, a path will be chosen. This folder will be named with a generic name, and then changed to the specific time the model finished training. Also, both vocabulary and model configuration will be saved as json files.", "For monitoring the model, some Callback functions (functions that are called during training, at specified intervals) were used. These functions are contained in the callbacks.py file. A custom class CallbackPlot was created, in order to plot the training error throughout training. Objects of Tensorflow Callback classes ModelCheckpoint and CSVLogger were also instantiated, in order to save the model and training logs during training respectively:", "Finally, the model can be built, compiled, and trained:", "After training, a folder will contain all data relevant to this training session, like loss function plots, error at different time steps, and the model itself.", "In order to serve the model, a simple interface was built using the Flask package, and its code can be found under the deploy folder.", "After training, the model is able to generate sentences given a seed string. Below are some examples with the produced sentence and the seed string used to generate it (I would use the names of my cats as examples, but since they are already called Luke, Han, and Leia, they\u2019re already present in the training dataset):", "As can be seen above, the model learned how to form some words reasonably well, how to size those words, how to end a sentence properly, and how to form some sort of context. However, it seems heavily biased towards always describing humans serving under a faction within the Star Wars Universe. This can be explained by the fact that the model\u2019s architecture wasn\u2019t build using word embeddings (which would allow for more complex context learning) because there are several words that are unique to Star Wars. I nice idea for a future project would be to generate a specific word embedding for the Star Wars Universe and then use that to generate new text.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "B.S. in Electrical Engineering, Data Science student. Link to my portfolio: https://pedrohgv.github.io/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fc7dc65e252c4&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerating-short-star-wars-text-with-lstms-c7dc65e252c4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerating-short-star-wars-text-with-lstms-c7dc65e252c4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerating-short-star-wars-text-with-lstms-c7dc65e252c4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerating-short-star-wars-text-with-lstms-c7dc65e252c4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----c7dc65e252c4--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c7dc65e252c4--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://pedro-venturott.medium.com/?source=post_page-----c7dc65e252c4--------------------------------", "anchor_text": ""}, {"url": "https://pedro-venturott.medium.com/?source=post_page-----c7dc65e252c4--------------------------------", "anchor_text": "Pedro Henrique Gomes Venturott"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fbe562e76b4c2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerating-short-star-wars-text-with-lstms-c7dc65e252c4&user=Pedro+Henrique+Gomes+Venturott&userId=be562e76b4c2&source=post_page-be562e76b4c2----c7dc65e252c4---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc7dc65e252c4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerating-short-star-wars-text-with-lstms-c7dc65e252c4&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc7dc65e252c4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerating-short-star-wars-text-with-lstms-c7dc65e252c4&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://chatbotslife.com/tagged/artificial-intelligence", "anchor_text": "NLP"}, {"url": "https://chatbotslife.com/", "anchor_text": "Natural Language Processing"}, {"url": "https://starwars.fandom.com/wiki/Main_Page", "anchor_text": "Wookiepedia.com"}, {"url": "https://www.fandom.com/", "anchor_text": "Fandom.com"}, {"url": "https://github.com/Pedrohgv/Star-Wars-Text-Generation", "anchor_text": "here"}, {"url": "https://drive.google.com/drive/folders/1JTzVV8uir74BlqF9HXtYMZdIu_4ZDpBP?usp=sharing", "anchor_text": "here"}, {"url": "https://starwars.fandom.com/wiki/Main_Page", "anchor_text": "Wookiepedia website"}, {"url": "https://github.com/Pedrohgv/Star-Wars-Text-Generation/blob/master/wookiescraper.py", "anchor_text": "wookiescraper.py"}, {"url": "https://github.com/Pedrohgv/Star-Wars-Text-Generation/blob/master/data_processor.py", "anchor_text": "data_processor.py"}, {"url": "https://en.wikipedia.org/wiki/Long_short-term_memory", "anchor_text": "LSTM"}, {"url": "https://chatbotslife.com/ultimate-guide-to-leveraging-nlp-machine-learning-for-you-chatbot-531ff2dd870c", "anchor_text": "NLP"}, {"url": "https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html", "anchor_text": "sequence-to-sequence"}, {"url": "https://github.com/Pedrohgv/Star-Wars-Text-Generation/blob/master/run.py", "anchor_text": "run.py"}, {"url": "https://github.com/Pedrohgv/Star-Wars-Text-Generation/blob/master/callbacks.py", "anchor_text": "callbacks.py"}, {"url": "https://github.com/Pedrohgv/Star-Wars-Text-Generation/tree/master/deploy", "anchor_text": "deploy"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----c7dc65e252c4---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/nlp?source=post_page-----c7dc65e252c4---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/lstm?source=post_page-----c7dc65e252c4---------------lstm-----------------", "anchor_text": "Lstm"}, {"url": "https://medium.com/tag/tensorflow?source=post_page-----c7dc65e252c4---------------tensorflow-----------------", "anchor_text": "TensorFlow"}, {"url": "https://medium.com/tag/bots?source=post_page-----c7dc65e252c4---------------bots-----------------", "anchor_text": "Bots"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc7dc65e252c4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerating-short-star-wars-text-with-lstms-c7dc65e252c4&user=Pedro+Henrique+Gomes+Venturott&userId=be562e76b4c2&source=-----c7dc65e252c4---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc7dc65e252c4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerating-short-star-wars-text-with-lstms-c7dc65e252c4&user=Pedro+Henrique+Gomes+Venturott&userId=be562e76b4c2&source=-----c7dc65e252c4---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc7dc65e252c4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerating-short-star-wars-text-with-lstms-c7dc65e252c4&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c7dc65e252c4--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fc7dc65e252c4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerating-short-star-wars-text-with-lstms-c7dc65e252c4&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----c7dc65e252c4---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----c7dc65e252c4--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----c7dc65e252c4--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----c7dc65e252c4--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----c7dc65e252c4--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----c7dc65e252c4--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----c7dc65e252c4--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----c7dc65e252c4--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----c7dc65e252c4--------------------------------", "anchor_text": ""}, {"url": "https://pedro-venturott.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://pedro-venturott.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Pedro Henrique Gomes Venturott"}, {"url": "https://pedro-venturott.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "12 Followers"}, {"url": "https://pedrohgv.github.io/", "anchor_text": "https://pedrohgv.github.io/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fbe562e76b4c2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerating-short-star-wars-text-with-lstms-c7dc65e252c4&user=Pedro+Henrique+Gomes+Venturott&userId=be562e76b4c2&source=post_page-be562e76b4c2--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Fbe562e76b4c2%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerating-short-star-wars-text-with-lstms-c7dc65e252c4&user=Pedro+Henrique+Gomes+Venturott&userId=be562e76b4c2&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}