{"url": "https://towardsdatascience.com/the-curse-of-dimensionality-f07c66128fe1", "time": 1683000147.848036, "path": "towardsdatascience.com/the-curse-of-dimensionality-f07c66128fe1/", "webpage": {"metadata": {"title": "The Curse of Dimensionality. Weird things happen in higher\u2026 | by Aaron Lipeles | Towards Data Science", "h1": "The Curse of Dimensionality", "description": "Most of us have very reasonable intuition that more information is always better. For example, the more I know about a potential borrower, the better I can predict whether that borrower will default\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/ai-ml-practicalities-bca0a47013c9", "anchor_text": "AI/ML Practicalities", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/ai-ml-practicalities-more-data-isnt-always-better-ae1dac9ad28f", "anchor_text": "I\u2019ve written separately", "paragraph_index": 2}, {"url": "https://towardsdatascience.com/if-multi-dimensional-hurts-your-brain-c137c9c572d6", "anchor_text": "this article", "paragraph_index": 4}, {"url": "https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm", "anchor_text": "k-nearest neighbors", "paragraph_index": 22}, {"url": "https://en.wikipedia.org/wiki/Spline_(mathematics)", "anchor_text": "cubic spline", "paragraph_index": 30}, {"url": "http://Quare.ai", "anchor_text": "Quare.ai", "paragraph_index": 47}], "all_paragraphs": ["This article is part of the series AI/ML Practicalities.", "Most of us have very reasonable intuition that more information is always better. For example, the more I know about a potential borrower, the better I can predict whether that borrower will default on a loan. Surprisingly, this intuition is false, sometimes spectacularly false.", "I\u2019ve written separately about how extra information can be worse than useless because fields in a sample of data can cluster or correlate in misleading ways. Such irrelevant or random chance effects can obscure real ones.", "In fact, matters get worse. Extra information can cause problems even if it\u2019s useful. This surprising fact is due to phenomena that arise only in high dimensions and is known as The Curse of Dimensionality.", "(NB: If you\u2019re uncomfortable with concept of higher dimensions, this article may help.)", "The curse is a family of effects that depend on the specifics of a problem. I\u2019ll go into detail about two of the biggest ones below. All, though, relate to the intuition-stretching effects of high-dimensional spaces. So, I want to start with a simple mind-bender.", "Let\u2019s take a look at what happens to the input space as the number of inputs grows. Input space is a fancy term for the set of all possible combinations of inputs, regardless of whether they\u2019re realistic or not.", "Suppose we are trying to predict an outcome using only one input feature. Our input space consists of all possible values of that one feature. Let\u2019s look at one third of the possible values, represented by the blue bar, below:", "To be annoyingly obvious, one third of all possible values for the input are contained in the middle third of that input\u2019s range. And, since there is only one feature, it also contains one third of all combinations of the inputs, i.e. one third of the input space.", "When we add a second feature and similarly divide its possible values into thirds, we can draw a square to represent a range that covers one third of the possible values for each of the two features:", "Here, the input space has two values, one for each feature. Though the box covers a third of each feature\u2019s possible values, it covers only one ninth, or about 11% of all the possible combinations of values.", "Adding a third feature gives us a cube that again covers one third of each feature range, but now only 1/27th or about 4% of the input space:", "We can\u2019t visualize more than three dimensions, but we can do the math. Once we reach ten features, the equivalent box will contain less than one thousandth of the input space. With 20 features, the box will contain less than one millionth of the input space. And, with 50 features, we\u2019re well past one trillionth.", "Yet, these tiny boxes that represent vanishingly small fractions of the input space still each represent a full third of the possible values for each dimension.", "Though the boxes are miniscule, they have very long edges!", "To see why this matters, let\u2019s switch to an effect that is easier to think about.", "Assume that we have a sample of 500 data points and let\u2019s explore what happens when we gain more information about each data point, i.e. as we add dimensions.", "Adding more information about each data point creates new ways for the data points to differ from each other. So, it increases the possible distance between the data points.", "With only one piece of information, our 500 data points appear to the eye to be almost continuous.", "With two pieces of information about each data point, the average distance between points is about double:", "And with three pieces of information, they\u2019re about four times as far apart, on average:", "Though we can\u2019t visualize higher dimensions, the math tells us that the distance between points continues to grow. With 50 dimensions, the average distance between points will be more than 20 times what it was with only one dimension.", "Put in very simple terms, machine learning\u2019s basic strategy is to make inferences by looking at similar data. Some models, like k-nearest neighbors, even do this explicitly.", "In mathematical terms, \u201csimilar\u201d means close. As we learn more about each data point, we find that they are farther and farther apart. Hence, we\u2019re forced to make inferences from increasingly dissimilar data.", "Not all distance matters equally, though. Some features tell us more about what we want to infer than others. The job of machine learning is to reveal these differences and down-weight distances that matter less. But, as the number of dimensions increases and the data grows exponentially more sparse, real differences get obscured by random variation (noise).", "You can make this problem go away by getting more training data. But, past a certain point, that\u2019s pretty much impossible. In our example, to maintain the same density of data would require 500 times as much data for each dimension added. At 50 dimensions, you would need more data points than there are particles in the universe!", "K-nearest neighbors, by the way, runs into another weird problem. If there is random variation in the data, adding many dimensions can make all the points roughly the same distance from each other. Hence, the concept of a \u2018neighborhood\u2019 is lost.", "In statistical analysis, data points that stand out by differing significantly from the rest of the data are known as outliers. These extreme events cause more than a handful of trouble.", "For starters, it\u2019s hard to know whether outliers represent statistical variation in what\u2019s being measured or in how we measure it. What\u2019s more likely, we might ask, that a baby is born weighing 12 pounds? Or that the scale malfunctioned? It is common, even, for statisticians to discard outliers to keep them from muddying up statistical models.", "Moreover, inference near the edge of a dataset never works as well as near the center. Figuring out what happens between two known cases is much easier than imagining what goes on beyond the known cases.", "In the following example, 10 points were sampled from a sine wave (the green line). A technique called \u201ccubic spline\u201d (dotted orange line) does a great job of interpolating the curve between the data points, but completely blows up at the edges.", "This is a contrived situation, but it\u2019s merely an extreme example of what happens to every machine learning model.", "In high dimensions, this is an even bigger problem. With so many aspects to look at, it turns out that nearly every data point is an outlier in some way. Let\u2019s do the same dimension-at-a-time walk-through to see what\u2019s going on.", "Below is the same data we looked at in the sparsity section. The extreme data points are highlighted in purple. For the purposes of the example, we\u2019ll consider a value extreme if it\u2019s in either the lowest or highest 2.5% of all observed values.", "Adding a second dimension, we now highlight data points whose values are extreme in either or both of the two dimension.", "The fraction of data points that are extreme in at least one way almost doubles, from 5% to 9.75%.", "With three dimensions, the percentage of outliers rises to 14.26%:", "Once again, we can\u2019t visualize beyond three dimensions, but we can do the math. With 10 dimensions, 40.13% of data points are extreme in at least one dimension. With 100 dimensions, 99.41% of all data points are extreme in at least one dimension.", "Looking again at the 2-dimensional plot, we can describe outliers in another way. They are the points near the edges of the dataset, forming an \u201couter shell\u201d.", "In high dimensions, almost all of the data is in the outer shell. This means, among other things, that \u201cneighborhoods\u201d typically stretch to the outer edges of multiple dimensions, that machine learning models almost always need to extrapolate, and that few other data points will be \u201csimilar\u201d to any given data point.", "Despite the curse, many of the greatest successes of machine learning come from high-dimensional domains.", "In image recognition, each pixel is typically a dimension. Spam filters and machine translation usually treat each word in the language as a dimension. Yet, these are domains where machine learning produces amazing results. What gives?", "First, where success has been found, data is plentiful. It\u2019s no surprise that Google dominates in machine translation. They\u2019ve cataloged all the documents on the Internet!", "Second, unlike the cubic spline method I used above, the models used in image recognition and translation are designed and continuously improved to address the problems of high dimensionality.", "And third, the problems of high dimensionality are at their worst when variation in the data in the data is high. But many, maybe most, datasets have a high degree of regularity in at least some dimensions. This effect, known as the \u201cBlessing of Uniformity\u201d, takes some of the punch out of the Curse of Dimensionality.", "Note: The dataset in the examples is independently standard normal distributed in all dimensions.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Founder @ Quare.ai \u2014 Focused on AI Explainability"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Ff07c66128fe1&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-curse-of-dimensionality-f07c66128fe1&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-curse-of-dimensionality-f07c66128fe1&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-curse-of-dimensionality-f07c66128fe1&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-curse-of-dimensionality-f07c66128fe1&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----f07c66128fe1--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f07c66128fe1--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@aaron.lipeles?source=post_page-----f07c66128fe1--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@aaron.lipeles?source=post_page-----f07c66128fe1--------------------------------", "anchor_text": "Aaron Lipeles"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe12e2e714e02&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-curse-of-dimensionality-f07c66128fe1&user=Aaron+Lipeles&userId=e12e2e714e02&source=post_page-e12e2e714e02----f07c66128fe1---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff07c66128fe1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-curse-of-dimensionality-f07c66128fe1&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff07c66128fe1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-curse-of-dimensionality-f07c66128fe1&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "http://towardsdatascience.com/tagged/ai-ml-practicalities", "anchor_text": "AI/ML Practicalities"}, {"url": "https://towardsdatascience.com/ai-ml-practicalities-bca0a47013c9", "anchor_text": "AI/ML Practicalities"}, {"url": "https://unsplash.com/@montylov?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "MontyLov"}, {"url": "https://unsplash.com/s/photos/curse?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://towardsdatascience.com/ai-ml-practicalities-more-data-isnt-always-better-ae1dac9ad28f", "anchor_text": "I\u2019ve written separately"}, {"url": "https://towardsdatascience.com/if-multi-dimensional-hurts-your-brain-c137c9c572d6", "anchor_text": "this article"}, {"url": "https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm", "anchor_text": "k-nearest neighbors"}, {"url": "https://en.wikipedia.org/wiki/Spline_(mathematics)", "anchor_text": "cubic spline"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----f07c66128fe1---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/ai?source=post_page-----f07c66128fe1---------------ai-----------------", "anchor_text": "AI"}, {"url": "https://medium.com/tag/data-science?source=post_page-----f07c66128fe1---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/ai-ml-practicalities?source=post_page-----f07c66128fe1---------------ai_ml_practicalities-----------------", "anchor_text": "Ai Ml Practicalities"}, {"url": "https://medium.com/tag/statistics?source=post_page-----f07c66128fe1---------------statistics-----------------", "anchor_text": "Statistics"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff07c66128fe1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-curse-of-dimensionality-f07c66128fe1&user=Aaron+Lipeles&userId=e12e2e714e02&source=-----f07c66128fe1---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff07c66128fe1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-curse-of-dimensionality-f07c66128fe1&user=Aaron+Lipeles&userId=e12e2e714e02&source=-----f07c66128fe1---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff07c66128fe1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-curse-of-dimensionality-f07c66128fe1&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f07c66128fe1--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Ff07c66128fe1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-curse-of-dimensionality-f07c66128fe1&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----f07c66128fe1---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----f07c66128fe1--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----f07c66128fe1--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----f07c66128fe1--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----f07c66128fe1--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----f07c66128fe1--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----f07c66128fe1--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----f07c66128fe1--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----f07c66128fe1--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@aaron.lipeles?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@aaron.lipeles?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Aaron Lipeles"}, {"url": "https://medium.com/@aaron.lipeles/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "76 Followers"}, {"url": "http://Quare.ai", "anchor_text": "Quare.ai"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe12e2e714e02&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-curse-of-dimensionality-f07c66128fe1&user=Aaron+Lipeles&userId=e12e2e714e02&source=post_page-e12e2e714e02--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F469a3bbb0b9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-curse-of-dimensionality-f07c66128fe1&newsletterV3=e12e2e714e02&newsletterV3Id=469a3bbb0b9&user=Aaron+Lipeles&userId=e12e2e714e02&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}