{"url": "https://towardsdatascience.com/document-search-with-fragment-embeddings-7e1d73eb0104", "time": 1683005543.213357, "path": "towardsdatascience.com/document-search-with-fragment-embeddings-7e1d73eb0104/", "webpage": {"metadata": {"title": "Document search with fragment embeddings | by Ajit Rajasekharan | Towards Data Science", "h1": "Document search with fragment embeddings", "description": "Embeddings for sentence fragments harvested from a document can serve as extractive summary facets of that document and potentially accelerate its discovery, particularly when user input is a\u2026"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "BERT", "paragraph_index": 1}, {"url": "https://www.nature.com/articles/d41586-020-00548-w", "anchor_text": "COVID-19", "paragraph_index": 2}, {"url": "https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge", "anchor_text": "COVID-19 data set", "paragraph_index": 2}, {"url": "https://arxiv.org/pdf/1301.3781.pdf", "anchor_text": "Word2vec", "paragraph_index": 15}, {"url": "https://www.quora.com/How-does-word2vec-work-Can-someone-walk-through-a-specific-example/answer/Ajit-Rajasekharan", "anchor_text": "two arrays of vectors", "paragraph_index": 15}, {"url": "https://arxiv.org/pdf/1301.3781.pdf", "anchor_text": "Word2vec", "paragraph_index": 16}, {"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "BERT", "paragraph_index": 16}, {"url": "https://www.logos.ic.i.u-tokyo.ac.jp/~tsuruoka/lapos/", "anchor_text": "Part-of-speech tagger to tag a sentence", "paragraph_index": 19}, {"url": "https://arxiv.org/abs/1301.3781", "anchor_text": "Word2vec", "paragraph_index": 20}, {"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "BERT", "paragraph_index": 21}, {"url": "https://arxiv.org/pdf/1908.10084.pdf", "anchor_text": "fragment embeddings (sentence transformers)", "paragraph_index": 21}, {"url": "https://cloud.google.com/solutions/machine-learning/building-real-time-embeddings-similarity-matching-system", "anchor_text": "open source solutions", "paragraph_index": 25}, {"url": "https://github.com/UKPLab/sentence-transformers", "anchor_text": "sentence-transformers", "paragraph_index": 31}, {"url": "https://github.com/hanxiao/bert-as-service", "anchor_text": "bert-as-service", "paragraph_index": 31}, {"url": "https://qr.ae/pNrXUl", "anchor_text": "Quora", "paragraph_index": 44}], "all_paragraphs": ["Embeddings for sentence fragments harvested from a document can serve as extractive summary facets of that document and potentially accelerate its discovery, particularly when user input is a sentence fragment. These fragment embeddings not only yield better quality results than traditional text matching systems, but also circumvent a problem inherent in modern distributed representation driven search approaches \u2014 the challenge to create effective document embeddings, i.e. a single embedding, at a document level, that captures all facets of a document and enables its discovery through search.", "Examples of sentence fragments are \u201cbats as a source of coronavirus\u201d, \u201ccoronavirus in pangolins\u201d \u2014 short sequences with one or more noun phrases connected by prepositions, adjectives etc. These highlighted connective terms that are largely ignored by traditional search systems, can play a key role not only in capturing user intent (e.g. \u201ccoronavirus in bats\u201d has a search intent distinct from \u201cbats as a source of coronavirus\u201d or \u201ccoronavirus not present in bats\u201d), but sentence fragments that preserve them can also be valuable index candidates serving as extractive summary facets (sub-summaries) of a document. By embedding these sentence fragments into an appropriate embedding space (e.g. BERT) we can use the search input fragment as a probe into that embedding space for discovering relevant documents.", "Finding a comprehensive answer supported by literature evidence to the questions \u201cWhat are the animal sources of COVID-19?\u201d or \u201creceptors coronavirus binds to\u201d is challenging even on a tiny data set like the recently released COVID-19 data set (~500 MB corpus size, ~13k documents, 85+ million words, about a million unique words in normalized text).", "Traditional document search methods are quite effective for the typical use case where the answer is obtainable from a few documents by using one or more noun phrases in search. Traditional document search methods also satisfy the following user experience constraint for words and phrases:", "For instance, when we search for words and phrases (contiguous word sequences like New York, Rio De Janeiro) \u2014 the results typically contains the terms we entered or their synonyms (e.g. COVID-19 search yields results with Sars-COV-2 or novel coronavirus etc.).", "However, the quality of results tends to degrade as the number of words in search input increases, particularly with the use of connective terms between noun phrases. This degradation of results quality is visibly evident even through the highlighted terms in results by these search engines.", "For instance, in the figure below, mostly the nouns in \u201cbats as a source of coronavirus\u201d are selectively highlighted by current search engines in a distributed manner within and across sentences/paragraphs in the displayed extractive summary for a document, at times not even honoring the ordering of those words in the input sequence. Even though document relevance ordering often mitigates this to a large degree, we are still left with the task of examining the extractive summary of each document, often having to navigate into the document only to return back to main results set again because the document didn\u2019t satisfy our search intent.", "The document search approach described in this article may reduce this cognitive overload present in search systems in addition to yielding more relevant results, particularly when searching for sentence fragments. As an illustration, the same query we used in existing search systems above can yield results of the form shown below (the interface is a schematic purely intended to show the search approach). A key point worth noting in the schematic below is that the extractive summary facets are actual matches in documents (the numbers in parenthesis are the number of documents containing a fragment and the cosine distance of a fragment with the input search fragment), as opposed to suggested queries or related search queries displayed in traditional search systems. These summary facets give a panoramic view of the results space reducing futile document navigation and accelerating convergence to documents of interest.", "The input fragments can be a full or partial sentences with no constraint in its composition or style. For instance, they can be interrogative as opposed to the affirmative query above \u2014 we can find the protein receptors coronavirus binds to, by searching for \u201creceptors coronavirus binds to\u201d", "The comparison between search systems above is only meant to illustrate the differences in the underlying approach of document discovery. It would be an unfair comparison otherwise given the orders of magnitude difference in corpus sizes \u2014 we are bound to get more relevant results in a tiny corpus.", "Distributed representations have become integral to any form of search given their advantages over traditional purely symbolic search approaches. Modern search systems are increasingly leveraging off their properties to complement symbolic search methods. If we consider document search broadly as a combination of breadth first and depth first traversal of document space, these two forms of traversals require embeddings that have characteristics specific to those traversals. For instance, we might start off broadly with the search of animals causing coronavirus, then drill down into bats, then broaden again to reptiles etc.", "Distributed representations of document facets \u2014 be it word, phrase, or sentence fragments, drawn from the embedding space of Word2vec and BERT, have unique and complementary attributes that are useful to perform broad and deep searches. Specifically,", "Expanded terms/fragments obtained from word2vec/BERT embeddings for user input are used to exact match documents that were already indexed offline using these terms/fragments. Offline, fragments are harvested from corpus using a combination of part-of-speech tagger and chunker, and embeddings are created for them using both models, word2vec and BERT.", "As mentioned earlier, word2vec embeddings expand the breadth of search for words and phrases. They do not expand the breadth of search for fragments \u2014 the histogram of neighborhood lacks a distinct tail quite often (figure 8 below). This is because fragments, by virtue of their lengths do not have enough neighboring context to learn quality embeddings. This deficiency could in part be addressed by expanding the window size of training and increase the surrounding context by ignoring sentence boundaries, but it still is inadequate in practice, given the low occurrence counts of fragments (figure 8).", "BERT embeddings largely only increase the depth of search particularly for fragments and phrases (depth of search expansion for words using BERT embeddings is not useful in practice). While they do increase the breadth to some degree, for instance, the query \u201ccoronavirus in macaques\u201d broadens to include \u201ccoronavirus in palm civets\u201d within the distribution tail of significant results, the breadth is not much as what word2vec offers in depth for words and phrase. Figure 6 caption below illustrates where it is deficient. Implementation notes have additional examples of lack of breadth in fragment search and ways to circumvent this limitation.", "Word2vec was perhaps the first model that clearly established the power of distributed representations about seven years ago. The embeddings output by this simple model with essentially two arrays of vectors for its \u201carchitecture\u201d, is still of immense value for downstream applications such as the document search method described above.", "Word2vec, in concert with BERT embeddings, offers a solution to document search that potentially improves upon traditional approaches in the quality of results and the time to converge on them (this claim needs to be quantified). The solution circumvents the problem of learning all the important aspects of a document in a single vector representation that could then be used by a search system to not only pick a particular document but also find documents similar to the picked one.", "The circumventing is made possible by the use of embeddings, be it a word, phrase, or sentence fragment, to broaden/deepen search prior to document picking. Word2vec embeddings for word and phrases largely increase breadth of document search. BERT embeddings for sentence fragments largely increase depth of search. BERT embeddings also eliminate out of vocabulary scenario as well as facilitate searchable extractive summarizations of different salient facets of a document which accelerate convergence on to relevant documents.", "1. What are the NLP methods/models used in this approach?", "Part-of-speech tagger to tag a sentence(scalable CRF-based that is at least an order of magnitude faster compared to more recent models with state-of-art F1 scores. However this model\u2019s recall is adequate for this tagging task),", "Word2vec for word and phrase embeddings,", "BERT for fragment embeddings (sentence transformers) and", "2. How is relevance computed for document results?", "The ordering of fragments is based the cosine distance from input fragment. The first document from the set of documents matching each facet fragment is picked and listed in the same order as the input fragment order. The ordering of documents could be based on some other document relevance ordering. The picking of document from each fragment match set could also be based on some other relevance ordering.", "3. Will this search approach scale for real time search?", "The computationally intensive step in realtime search is similarity search in an embedding space (Word2vec or BERT). There are existing open source solutions already present to perform this operation at scale. There are optimizations we can do to reduce time/compute cycles such as searching only in one of the two embedding spaces based on input search length given the strengths/weaknesses of these models relies on it.", "4. Isn\u2019t a fragment nothing but a long phrase? If so, why call it differently?", "A fragment is essentially a long phrase. The distinction from phrases is useful for two reasons", "a) fragments could be a full sentence, not just partial sentences", "b) the strengths of these models rely on length of input as we saw earlier (see Figure 6).Word2vec performs well in the term/short phrase region. BERT performs best in the fragment region (\u2265 5 words)", "5. How do histogram distributions of the neighborhoods look for terms and fragments?", "BERT and Word2vec neighborhoods below for word, phrase (3 words), and fragments (8 words) illustrate the complementary natures of these two models. The tail of the distribution increases as the word length increases for BERT \u2014 fragments have a distinct tail compared to phrases or words. When the term count is low, at times the distribution may have very thick tails \u2014 this is indicative of poor results. Embeddings created by sentence-transformers tend to have a distinct tail in contrast to bert-as-service embeddings despite both using summation of subwords as pooling (both have other pooling approaches too) because of the supervised training step in sentence-transfomers using sentence pairs with labels for entailment, neutral and contradiction.", "Word2vec neighborhoods have a tail for words and phrases. The distribution almost breaks down to a \u201cpathological form\u201d for long phrases even with high occurrence counts, with crowding at the high end and then the rest of the mass concentrating at the low end. The distribution shape varies for long phrases too. However regardless of shape, neighborhood results clearly show this breakdown in the quality.", "6. Results sensitivity to variations in the input fragment. That is how likely are we to converge on the same result using variants of the input.", "While the set of fragments retrieved for different variations of the same question are distinct, there appears to be a lot of intersection in the retrieved fragment set. However, some questions may not yield any fragments involving all the nouns in search due the limitation of breadth for fragments discussed earlier. For instance \u201cpteropus as a source of coronavirus\u201d or \u201ccoronavirus in pteropus\u201d may not yield any fragments having pteropus or bats (pteropus belongs to bat family). When fragments do not contain all nouns, one approach to consider is to find the Word2vec neighbors for that term and reconstruct a query using those terms.", "7. How do the models compare to find needle in the haystack document using terms, phrases and fragments?", "Word2vec embeddings are not directly useful for this case simply because the vector for a single occurrence term/phrase does not have sufficient context to learn a rich representation. BERT embeddings do not suffer from this drawback by the very nature of constructing even a word from subwords \u2014 subwords have sufficient context to learn good representations. However Word2vec may still have a role to play to find the equivalent name for a noun in search which then yields a needle in haystack document. For instance, if the document space has a single reference to coronavirus in fruit bats, a search for coronavirus in pteropus may not yield that document. However, a search for the fragment coronavirus in fruits bats (created using Word2vec) would pick up that single document in BERT\u2019s embedding space. In addition to the presence of a fragment being in a distribution tail (if it indeed is) qualifying it as a candidate, the interpretability inherent in most fragments offers an advantage, which a word or a phrase may not necessarily have.", "8. Additional details on the extraction of information about coronavirus in animals", "About 1000 (998) biological entity mentions were harvested using Word2vec and entity tagging. These were used to harvest 195 fragments with virus mention. A sample of 30 fragments are shown", "A sample of the fragments with evidence of animals that are potential sources of coronavirus", "9. Do we need both forms of embeddings for fragments search?", "We can do with just one \u2014 BERT embeddings. However we need to pair it up with a traditional lexical search system minimally to complement it in the area where it is weak \u2014 single term and short phrase search.", "The flow below shows the use case of both such systems used together to discover documents", "Figures below show examples of document discovery using both the flows shown above.", "This article was manually imported from Quora", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F7e1d73eb0104&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdocument-search-with-fragment-embeddings-7e1d73eb0104&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdocument-search-with-fragment-embeddings-7e1d73eb0104&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdocument-search-with-fragment-embeddings-7e1d73eb0104&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdocument-search-with-fragment-embeddings-7e1d73eb0104&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----7e1d73eb0104--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----7e1d73eb0104--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://ajitrajasekharan.medium.com/?source=post_page-----7e1d73eb0104--------------------------------", "anchor_text": ""}, {"url": "https://ajitrajasekharan.medium.com/?source=post_page-----7e1d73eb0104--------------------------------", "anchor_text": "Ajit Rajasekharan"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffd04a90b4be7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdocument-search-with-fragment-embeddings-7e1d73eb0104&user=Ajit+Rajasekharan&userId=fd04a90b4be7&source=post_page-fd04a90b4be7----7e1d73eb0104---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7e1d73eb0104&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdocument-search-with-fragment-embeddings-7e1d73eb0104&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7e1d73eb0104&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdocument-search-with-fragment-embeddings-7e1d73eb0104&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://github.com/UKPLab/sentence-transformers", "anchor_text": "BERT embeddings"}, {"url": "https://arxiv.org/abs/1301.3781", "anchor_text": "Word2vec"}, {"url": "https://towardsdatascience.com/unsupervised-ner-using-bert-2d7af5f90b8a", "anchor_text": "unsupervised NER with BERT"}, {"url": "https://www.nature.com/articles/d41586-020-00548-w", "anchor_text": "is not confirmed to date."}, {"url": "https://www.cdc.gov/coronavirus/2019-ncov/covid-data/covidview.html", "anchor_text": "CDC"}, {"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "BERT"}, {"url": "https://www.nature.com/articles/d41586-020-00548-w", "anchor_text": "COVID-19"}, {"url": "https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge", "anchor_text": "COVID-19 data set"}, {"url": "https://arxiv.org/abs/1301.3781", "anchor_text": "Word2vec"}, {"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "BERT"}, {"url": "https://www.nature.com/articles/d41586-020-00548-w", "anchor_text": "COVID-19"}, {"url": "https://cloud.google.com/solutions/machine-learning/building-real-time-embeddings-similarity-matching-system", "anchor_text": "efficient hashing approaches"}, {"url": "https://cloud.google.com/solutions/machine-learning/building-real-time-embeddings-similarity-matching-system", "anchor_text": "space search at scale"}, {"url": "https://arxiv.org/pdf/1301.3781.pdf", "anchor_text": "Word2vec"}, {"url": "https://www.quora.com/How-does-word2vec-work-Can-someone-walk-through-a-specific-example/answer/Ajit-Rajasekharan", "anchor_text": "two arrays of vectors"}, {"url": "https://arxiv.org/pdf/1301.3781.pdf", "anchor_text": "Word2vec"}, {"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "BERT"}, {"url": "https://www.sciencedaily.com/releases/2020/03/200317175442.htm", "anchor_text": "COVID-19"}, {"url": "https://www.nature.com/articles/d41586-020-00548-w", "anchor_text": "not confirmed to date"}, {"url": "https://arxiv.org/pdf/1908.10084.pdf", "anchor_text": "Sentence BERT"}, {"url": "https://towardsdatascience.com/unsupervised-ner-using-bert-2d7af5f90b8a", "anchor_text": "Unsupervised NER using BER"}, {"url": "https://www.quora.com/How-does-word2vec-work-Can-someone-walk-through-a-specific-example/answer/Ajit-Rajasekharan", "anchor_text": "An answer explaining how word2vec works"}, {"url": "https://www.logos.ic.i.u-tokyo.ac.jp/~tsuruoka/lapos/", "anchor_text": "Part-of-speech tagger to tag a sentence"}, {"url": "https://arxiv.org/abs/1301.3781", "anchor_text": "Word2vec"}, {"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "BERT"}, {"url": "https://arxiv.org/pdf/1908.10084.pdf", "anchor_text": "fragment embeddings (sentence transformers)"}, {"url": "https://towardsdatascience.com/unsupervised-ner-using-bert-2d7af5f90b8a", "anchor_text": "unsupervised entity tagging"}, {"url": "https://cloud.google.com/solutions/machine-learning/building-real-time-embeddings-similarity-matching-system", "anchor_text": "open source solutions"}, {"url": "https://github.com/UKPLab/sentence-transformers", "anchor_text": "sentence-transformers"}, {"url": "https://github.com/hanxiao/bert-as-service", "anchor_text": "bert-as-service"}, {"url": "https://www.meilisearch.com/", "anchor_text": "meilisearch"}, {"url": "https://qr.ae/pNrXUl", "anchor_text": "Quora"}, {"url": "https://medium.com/tag/naturallanguageprocessing?source=post_page-----7e1d73eb0104---------------naturallanguageprocessing-----------------", "anchor_text": "Naturallanguageprocessing"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----7e1d73eb0104---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/nlp?source=post_page-----7e1d73eb0104---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/ai?source=post_page-----7e1d73eb0104---------------ai-----------------", "anchor_text": "AI"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----7e1d73eb0104---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7e1d73eb0104&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdocument-search-with-fragment-embeddings-7e1d73eb0104&user=Ajit+Rajasekharan&userId=fd04a90b4be7&source=-----7e1d73eb0104---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7e1d73eb0104&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdocument-search-with-fragment-embeddings-7e1d73eb0104&user=Ajit+Rajasekharan&userId=fd04a90b4be7&source=-----7e1d73eb0104---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7e1d73eb0104&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdocument-search-with-fragment-embeddings-7e1d73eb0104&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----7e1d73eb0104--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F7e1d73eb0104&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdocument-search-with-fragment-embeddings-7e1d73eb0104&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----7e1d73eb0104---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----7e1d73eb0104--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----7e1d73eb0104--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----7e1d73eb0104--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----7e1d73eb0104--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----7e1d73eb0104--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----7e1d73eb0104--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----7e1d73eb0104--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----7e1d73eb0104--------------------------------", "anchor_text": ""}, {"url": "https://ajitrajasekharan.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://ajitrajasekharan.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ajit Rajasekharan"}, {"url": "https://ajitrajasekharan.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "779 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffd04a90b4be7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdocument-search-with-fragment-embeddings-7e1d73eb0104&user=Ajit+Rajasekharan&userId=fd04a90b4be7&source=post_page-fd04a90b4be7--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F974aed893170&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdocument-search-with-fragment-embeddings-7e1d73eb0104&newsletterV3=fd04a90b4be7&newsletterV3Id=974aed893170&user=Ajit+Rajasekharan&userId=fd04a90b4be7&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}