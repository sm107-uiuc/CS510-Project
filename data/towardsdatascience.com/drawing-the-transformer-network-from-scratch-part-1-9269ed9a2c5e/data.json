{"url": "https://towardsdatascience.com/drawing-the-transformer-network-from-scratch-part-1-9269ed9a2c5e", "time": 1683016493.092056, "path": "towardsdatascience.com/drawing-the-transformer-network-from-scratch-part-1-9269ed9a2c5e/", "webpage": {"metadata": {"title": "Drawing the Transformer Network from Scratch (Part 1) | by Thomas Kurbiel | Towards Data Science", "h1": "Drawing the Transformer Network from Scratch (Part 1)", "description": "The Transformer Neural Networks \u2014 usually just called \u201cTransformers\u201d \u2014 were introduced by a Google-led team in 2017 in a paper titled \u201cAttention Is All You Need\u201d. They were refined and popularized by\u2026"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/pdf/1706.03762.pdf", "anchor_text": "Original Paper", "paragraph_index": 33}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "The Illustrated Transformer", "paragraph_index": 33}, {"url": "https://towardsdatascience.com/transformers-explained-65454c0f3fa7", "anchor_text": "Transformers Explained", "paragraph_index": 33}, {"url": "https://www.r-craft.org/r-news/get-busy-with-word-embeddings-an-introduction/", "anchor_text": "Get Busy with Word Embeddings", "paragraph_index": 33}], "all_paragraphs": ["The Transformer Neural Networks \u2014 usually just called \u201cTransformers\u201d \u2014 were introduced by a Google-led team in 2017 in a paper titled \u201cAttention Is All You Need\u201d. They were refined and popularized by many people in the following work.", "Like many models invented before it, the Transformer has an encoder-decoder architecture. In this post, we put our focus on the encoder part. We will successively draw all its parts in a Bottom-top fashion. Doing so will hopefully allow the readers to easily develop a \u201cmental model\u201d of the Transformer.", "The animation below shows in fast motion what we will cover in this post:", "A Transformer takes as input a sequence of words, which are presented to the network as vectors. In NLP tasks usually a vocabulary (also called dictionary) is used, in which each word is assigned a unique index. The index can be represented as a so called one-hot vector, which is predominantly made up of zeros, with a single \u201cone\u201d value at the correct location. A simple one-hot word encoding for a small vocabulary of ten words is shown in the diagram below:", "Please note that the one-hot encoded vectors have the same size as the number of words in the vocabulary, which in real-world application is at least 10.000. Furthermore, all one-hot encodings have the same Euclidean distance of \u221a2 to each other.", "Next, we reduce the dimensionality of the one-hot encoded vectors by multiplying them with a so called \u201cembedding matrix\u201d. The resulting vectors are called word embeddings. The size of the word embeddings in the original paper is 512.", "The huge benefit of word embeddings is that words with similar meanings are put close to each other, e.g. the word \u201ccat\u201d and \u201ckitty\u201d end up having similar embedding vectors.", "Please note that the \u201cembedding matrix\u201d is a normal matrix, just with a fancy name.", "All the words are presented to the Transformer simultaneously. This is a huge difference to recurrent neural networks, e.g. LSTMs, where words are fed successively. However, this means that the order in which words occur in the input sequence is lost. To address this, the Transformer adds a vector to each input embedding, thus injecting some information about the relative or absolute position.", "Finally, we multiply the word embeddings by matrices WQ and WK to obtain the \u201cquery vectors\u201d and \u201ckey vectors\u201d, each of size 64.", "All the components, mentioned so far, are drawn in the following animation:", "Please note that the order in which we draw the single elements has nothing to do with the order in which the elements are computed.", "One thing to emphasize before we continue, is the way the Transformer lends itself to parallelization. Please note, that all the word embeddings can be computed in parallel. Once we\u2019ve got the embeddings, we also can simultaneously compute the query vectors and key vectors for all the embeddings. This pattern will continue throughout the whole architecture. Please pay attention to it.", "We calculate the dot products for all possible combinations of \u201cquery vectors\u201d and \u201ckey vectors\u201d. The result of a dot product is a single number, which in a later step will be used as a weight factor. The weights factors tell us, how much two words at different positions of the input sentence depend on each other. This is called self-attention in the original paper. The mechanism of self-attention allows the Transformer to learn difficult dependencies even between distant positions.", "Subsequently, all weight factors are divided by 8 (the square root of the dimension of the key vectors 64). The authors assume that during training the dot products can grow large in magnitude, thus pushing the softmax function into regions where it has extremely small gradients. Dividing by 8 leads to having more stable gradients.", "The scaled factors are put through a softmax function, which normalizes them so they are all positive and sum up to 1.", "In the animation below, we perform the scaling for the weight factors belonging to the first word in our sentence, which is \u201cThe\u201d. Please remember, that the weight factors belonging to the first word are the dot products: q1*k1, q1*k2, q1*k3 and q1*k4.", "Analogously, for the other words \u201ccar\u201d, \u201cis\u201d and \u201cblue\u201d in our input sequence we get:", "This completes the calculation of the weights factors.", "Identical to the computation of the \u201ckey vector\u201d and \u201cquery vectors\u201d we obtain the \u201cvalue vectors\u201d by multiplying the word embeddings by matrix WV. Again the size of the value vectors is 64.", "Now, we multiply each \u201cvalue vector\u201d by its corresponding \u201cweight factor\u201d. As mentioned before, this way we only keep the words we want to focus on, while irrelevant words are suppressed by weighting them by tiny numbers like 0.001", "Now we sum up all the weighted \u201cvalue vectors\u201d belonging to a word. This produces the output of the self-attention layer at this position.", "In the next animation we depict the computation of the \u201cvalue vectors\u201d and their subsequent weighting and summation performed for the first word in the input sequence.", "Analogously for the other words \u201ccar\u201d, \u201cis\u201d, \u201cblue\u201d in our input sequence, we get:", "That concludes the self-attention calculation. The output of the self-attention layer can be considered as a context enriched word embedding. Depending on the context, a word can have different meanings:", "Please note that the embeddings matrix at the bottom is operating on single words only. Hence for both sentences, we would wrongly obtain the same embedding vector. The self-attention layer is taking this into consideration.", "The length of the input sequence is supposed to be fixed in length \u2014 basically it is the length of the longest sentence in training dataset. Hence, a parameter defines the maximum length of a sequence that the Transformer can accept. Sequences that are greater in length are just truncated. Shorter sequences are padded with zeros. However, padded words are not supposed to contribute to the self-attention calculation. This is avoided by masking the corresponding words (setting them to -inf) before the softmax step in the self-attention calculation. This in fact sets their weight factors to zero.", "Instead of performing a single self-attention function, the authors employ multiple self-attention heads, each with different weight matrices. Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. The Transformer in the original paper uses eight parallel attention heads. The outputs of the attention heads are concatenated and once again multiplied by an additional weights matrix WO.", "The multi-head self-attention mechanism, just covered, is the first sub-module of the encoder. It has a residual connection around it, and is followed by a layer-normalization step. Layer-normalization just subtracts the mean of each vector and divides by its standard deviation.", "The outputs of the self-attention layer are fed to a fully connected feed-forward network. This consists of two linear transformations with a ReLU activation in between. The dimensionality of input and output is 512, and the inner-layer has dimensionality 2048. The exact same feed-forward network is independently applied to each position, i.e. for each word in the input sequence.", "Next, we again employ a residual connection around the fully connected feed-forward layer, followed by layer normalization.", "The entire encoding component is a stack of six encoders. The encoders are all identical in structure, yet they do not share weights.", "In the next post, we are going to cover the decoder part of the Transformer. This should be quite straight forward since most of the required concepts were already covered in this post.", "Original PaperThe Illustrated TransformerTransformers ExplainedGet Busy with Word Embeddings", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Advanced Computer Vision & AI Research Engineer at APTIV Germany"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F9269ed9a2c5e&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdrawing-the-transformer-network-from-scratch-part-1-9269ed9a2c5e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdrawing-the-transformer-network-from-scratch-part-1-9269ed9a2c5e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdrawing-the-transformer-network-from-scratch-part-1-9269ed9a2c5e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdrawing-the-transformer-network-from-scratch-part-1-9269ed9a2c5e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----9269ed9a2c5e--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----9269ed9a2c5e--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://tkurbiel.medium.com/?source=post_page-----9269ed9a2c5e--------------------------------", "anchor_text": ""}, {"url": "https://tkurbiel.medium.com/?source=post_page-----9269ed9a2c5e--------------------------------", "anchor_text": "Thomas Kurbiel"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa86bca18debd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdrawing-the-transformer-network-from-scratch-part-1-9269ed9a2c5e&user=Thomas+Kurbiel&userId=a86bca18debd&source=post_page-a86bca18debd----9269ed9a2c5e---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9269ed9a2c5e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdrawing-the-transformer-network-from-scratch-part-1-9269ed9a2c5e&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9269ed9a2c5e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdrawing-the-transformer-network-from-scratch-part-1-9269ed9a2c5e&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://arxiv.org/pdf/1706.03762.pdf", "anchor_text": "Original Paper"}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "The Illustrated Transformer"}, {"url": "https://towardsdatascience.com/transformers-explained-65454c0f3fa7", "anchor_text": "Transformers Explained"}, {"url": "https://www.r-craft.org/r-news/get-busy-with-word-embeddings-an-introduction/", "anchor_text": "Get Busy with Word Embeddings"}, {"url": "https://medium.com/tag/transformers?source=post_page-----9269ed9a2c5e---------------transformers-----------------", "anchor_text": "Transformers"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----9269ed9a2c5e---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----9269ed9a2c5e---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/nlp?source=post_page-----9269ed9a2c5e---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/data-science?source=post_page-----9269ed9a2c5e---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9269ed9a2c5e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdrawing-the-transformer-network-from-scratch-part-1-9269ed9a2c5e&user=Thomas+Kurbiel&userId=a86bca18debd&source=-----9269ed9a2c5e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9269ed9a2c5e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdrawing-the-transformer-network-from-scratch-part-1-9269ed9a2c5e&user=Thomas+Kurbiel&userId=a86bca18debd&source=-----9269ed9a2c5e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9269ed9a2c5e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdrawing-the-transformer-network-from-scratch-part-1-9269ed9a2c5e&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----9269ed9a2c5e--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F9269ed9a2c5e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdrawing-the-transformer-network-from-scratch-part-1-9269ed9a2c5e&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----9269ed9a2c5e---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----9269ed9a2c5e--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----9269ed9a2c5e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----9269ed9a2c5e--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----9269ed9a2c5e--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----9269ed9a2c5e--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----9269ed9a2c5e--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----9269ed9a2c5e--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----9269ed9a2c5e--------------------------------", "anchor_text": ""}, {"url": "https://tkurbiel.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://tkurbiel.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Thomas Kurbiel"}, {"url": "https://tkurbiel.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "287 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa86bca18debd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdrawing-the-transformer-network-from-scratch-part-1-9269ed9a2c5e&user=Thomas+Kurbiel&userId=a86bca18debd&source=post_page-a86bca18debd--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fadf7de277c18&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdrawing-the-transformer-network-from-scratch-part-1-9269ed9a2c5e&newsletterV3=a86bca18debd&newsletterV3Id=adf7de277c18&user=Thomas+Kurbiel&userId=a86bca18debd&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}