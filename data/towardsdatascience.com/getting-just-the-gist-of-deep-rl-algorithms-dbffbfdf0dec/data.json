{"url": "https://towardsdatascience.com/getting-just-the-gist-of-deep-rl-algorithms-dbffbfdf0dec", "time": 1682995502.025074, "path": "towardsdatascience.com/getting-just-the-gist-of-deep-rl-algorithms-dbffbfdf0dec/", "webpage": {"metadata": {"title": "Gists of Recent Deep RL Algorithms | by Nathan Lambert | Towards Data Science", "h1": "Gists of Recent Deep RL Algorithms", "description": "A resource for getting the gist of RL algorithms without needing to surf through piles of documentation \u2014 a resource for students and researchers, without a single formula."}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/Dynamic_programming", "anchor_text": "wikipedia", "paragraph_index": 1}, {"url": "https://en.wikipedia.org/wiki/TD-Gammon", "anchor_text": "Backgammon programs of Tesauro", "paragraph_index": 2}, {"url": "https://link.springer.com/article/10.1007/BF00992696", "anchor_text": "1992", "paragraph_index": 6}, {"url": "https://link.springer.com/article/10.1007/BF00992698", "anchor_text": "1992", "paragraph_index": 8}, {"url": "https://arxiv.org/pdf/1509.06461.pdf", "anchor_text": "2015", "paragraph_index": 9}, {"url": "https://papers.nips.cc/paper/1786-actor-critic-algorithms.pdf", "anchor_text": "2000", "paragraph_index": 11}, {"url": "https://medium.com/@natolambert/deep-rl-case-study-policy-based-vs-model-conditioned-gradients-in-rl-4546434c84b0", "anchor_text": "in another post", "paragraph_index": 13}, {"url": "http://proceedings.mlr.press/v37/schulman15.pdf", "anchor_text": "2015", "paragraph_index": 14}, {"url": "https://arxiv.org/abs/1707.06347", "anchor_text": "2017", "paragraph_index": 15}, {"url": "https://arxiv.org/pdf/1509.02971.pdf", "anchor_text": "2016", "paragraph_index": 16}, {"url": "https://arxiv.org/abs/1710.02298", "anchor_text": "2017", "paragraph_index": 17}, {"url": "https://arxiv.org/abs/1802.09477", "anchor_text": "2018", "paragraph_index": 19}, {"url": "https://arxiv.org/abs/1801.01290", "anchor_text": "2018", "paragraph_index": 20}, {"url": "https://www.ias.informatik.tu-darmstadt.de/uploads/Publications/Deisenroth_ICML_2011.pdf", "anchor_text": "2011", "paragraph_index": 23}, {"url": "https://arxiv.org/abs/1805.12114", "anchor_text": "2018", "paragraph_index": 24}, {"url": "https://arxiv.org/abs/1809.05214", "anchor_text": "2018", "paragraph_index": 25}, {"url": "https://arxiv.org/abs/1802.10592", "anchor_text": "2018", "paragraph_index": 26}, {"url": "https://arxiv.org/abs/1903.00374", "anchor_text": "2019", "paragraph_index": 27}, {"url": "https://arxiv.org/abs/1901.03737", "anchor_text": "quadrotors", "paragraph_index": 28}, {"url": "https://arxiv.org/abs/1708.02596", "anchor_text": "walking robots", "paragraph_index": 28}, {"url": "https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence", "anchor_text": "Kullback-Leibler Divergence", "paragraph_index": 30}, {"url": "http://robotic.substack.com", "anchor_text": "robotic.substack.com", "paragraph_index": 32}, {"url": "http://natolambert.com", "anchor_text": "natolambert.com", "paragraph_index": 32}], "all_paragraphs": ["As a reinforcement learning (RL) researcher I often need to remind myself of the subtle differences between the algorithms. Here I want to create a list of algorithms and a sentence or two for each that distinguishes it from others in its sub area. I pair this with a brief historical introduction to the field.", "Reinforcement learning holds its roots in the history of optimal control. The story began in the 1950s with exact dynamic programming, which broadly speaking is the structured approach of breaking down a confined problem into smaller, solvable sub-problems [wikipedia], credited to Richard Bellman. Good history to know is that Claude Shannon and Richard Bellman revolutionized many computational sciences in the 1950s and 1960s.", "Through the 1980s, some initial work on the link between RL and control emerged, and the first notable result was the Backgammon programs of Tesauro based on temporal-difference models in 1992. Through the 1990s, more analysis of algorithms emerged and leaned towards what we now call RL. A seminal paper is \u201cSimple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning\u201d from Ronald J. Williams, which introduced what is now vanilla policy gradient. Note that in the title he included the term \u2018Connectionist\u2019 to describe RL \u2014 this was his way of specifying his algorithm towards models following the design of human cognition. These are now called neural networks, but just two and a half decades ago was a small subfield of investigation.", "It was not until the mid-2000s, with the advent of big data and the computation revolution that RL turned to be neural network based, with many gradient based convergence algorithms. Modern RL is often separated into two flavors, being model \u201cfree\u201d and model \u201cbased\u201d RL. I will do the same.", "Model free RL directly generates a policy for an actor. I like to think of it as end-to-end learning of how to act, with all the environmental knowledge being embedded in this policy.", "Policy gradient algorithms modify an agent\u2019s policy to track those actions that bring it higher reward. This lends these algorithms to be on-policy, so they can only learn from actions taken within the algorithm.", "Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning (REINFORCE) \u2014 1992: This paper kickstarted the policy gradient idea, suggesting the core idea of systematically increasing the likelihood of actions that yield high rewards.", "Value based algorithms modify an agent\u2019s policy based on the perceived value of a given state.This lends these algorithms to be off-policy because an agent can update its internal value structure of a state by reading the reward function from any policy.", "Q-Learning \u2014 1992: Q-learning is the classic value based method in modern RL, where the agent stores a perceived value for each action, state pair, which then informs the policy action.", "Deep Q-Network (DQN) \u2014 2015: Deep Q-Learning simply applies a neural network to approximate the Q function for each action and state, which can save vast amounts of computational resources, and potentially expand to continuous time action spaces.", "Actor-critic algorithms take policy based and value based methods together \u2014 by having separate network approximations for the value (critic) and actions (actor). These two networks work together to regularize each other and create, hopefully, more stable results.", "Actor Critic Algorithms \u2014 2000: This paper introduced the idea of having two separate, but intertwined models for generating a control policy.", "A decade later, we find ourselves in an explosion of deep RL algorithms. Note that in all the press you read, deep at the core is referring to methods using neural network approximations.", "Policy gradient algorithms regularly suffer from noisy gradients. I talked about one change in the gradient calculations recently proposed in another post, and a bunch of the most recent \u2018State of the Art\u2019 algorithms at their time looked to address this, including TRPO and PPO.", "Trust Region Policy Optimization (TRPO) \u2014 2015: Building on the actor critic approach, the authors of TRPO looked to regularize the change in policies at each training iteration, and they introduce a hard constraint on the KL divergence(***), or the information change in the new policy distribution. The use of a constraint, rather than a penalty, allows bigger training steps and faster convergence in practice.", "Proximal Policy Optimization (PPO) \u2014 2017: PPO builds on a similar idea as TRPO with the KL Divergence, and addresses the difficulty of implementing TRPO (which involves conjugate gradients to estimate the Fisher Information matrix), by using a surrogate loss function taking into account the KL divergence. PPO uses clipping to make this surrogate loss and assist convergence.", "Deep Deterministic Policy Gradient (DDPG) \u2014 2016: DDPG combines improvements in Q learning with a policy gradient update rule, which allowed application of Q learning to many continuous control environments.", "Combining Improvements in Deep RL (Rainbow) \u2014 2017: Rainbow combines and compares many innovations in improving deep Q learning (DQN). There are many papers referenced here, so it can be a great place to learn about progress on DQN:", "The next two incorporate similar changes to the actor critic algorithms. Note that SAC is not a successor to TD3 as they were released nearly concurrently, but SAC uses a few of the tricks also used in TD3.", "Twin Delayed Deep Deterministic Policy Gradient (TD3) \u2014 2018: TD3 builds on DDPG with 3 key changes: 1) \u201cTwin\u201d: learns two Q functions simultaneously, taking the lower value for the Bellman estimate to reduce variance, 2) \u201cDelayed\u201d: updates the policy less frequently than the Q function, 3) adds noise to the to the target action to lower exploitative policies.", "Soft Actor Critic (SAC) \u2014 2018: To use model-free RL in robotic experiment the authors looked to improve sample efficiency, the breadth of data collection, and safety of exploration. Using entropy based RL they control exploration along with DDPG style Q function approximation for continuous control. Note: SAC also implemented clipping like TD3, and using a stochastic policy it benefits from regularizing action choice, which is similar to smoothing.", "Many people are very excited about the applications of model-free RL as sample complexity falls and results rise. Recent research has brought an increasing portion of these methods to physical experiments, which is bringing the prospects of widely available robots one step closer.", "Model based RL(MBRL) attempts to build knowledge of the environment, and leverages said knowledge to take an informed action. The goal of these methods is often to reduce sample complexity on the model-free variants that are closer to end-to-end learning.", "Probabilistic Inference for Learning Control (PILCO) \u2014 2011: This paper is one of the first in model-based RL, and it proposed a policy search method (essentially policy iteration) on top of a Gaussian Process (GP) dynamics model (built in uncertainty estimates). There have been many applications of learning with GPs, but not as many core algorithms to date.", "Probabilistic Ensembles with Trajectory Sampling (PETS) \u2014 2018: PETS combines three parts into one functional algorithm: 1) a dynamics model consisting of multiple randomly initialized neural networks (ensemble of models), 2) a particle based propagation algorithm, and 3) and simple model predictive controller. These three parts leverage deep learning of a dynamics model in a potentially generalizable fashion.", "Model-Based Meta-Policy-Optimization (MB-MPO) \u2014 2018: This paper uses meta-learning to choose which dynamics model in an ensemble best optimizes a policy and mitigate model bias. This meta-optimization allows MBRL to come closer to asymptotic model-free performance in substantially lower samples.", "Model-Ensemble Trust Region Policy Optimization (ME-TRPO) \u2014 2018: ME-TRPO is the application of TRPO on an ensemble of models assumed to be the ground truth of an environment. A subtle addition to the model-free version is a stop condition on policy training only when a user defined proportion of models in an ensemble no longer sees improvement when the policy is iterated.", "Model-Based Reinforcement Learning for Atari (SimPLe) \u2014 2019: SimPLe combines many tricks in the model-based RL area with a variational auto-encoder modeling dynamics from pixels. This shows the current state of the art for MBRL in Atari games (personally I think this is a very cool piece to read, and expect people to build on it soon).", "The hype behind model-based RL has been increasing in recent years. It has often been given a short look because it lacks the asymptotic performance of its model-free counterparts. I am particularly interested in it because it has enabled many experiment only, exciting applications including: quadrotors and walking robots.", "Thanks for reading! I\u2019ll likely update this page whenever I see fit \u2014 I hope to keep it for a resource of reminders for myself! Cheers.", "(***) KL Divergence, more rarely referred to as Kullback-Leibler Divergence is a measure of difference between two probability distributions. I best connect with thinking about it as the difference between the cross entropy of two distributions p (original) and q (new) H(p,q) and the entropy of the original distribution p, H(p). It is represented by KL(P | Q), and is a measure of the information gain.", "More? Subscribe to my newsletter on robotics, artificial intelligence, and society!", "Trying to think freely and create equitable & impactful automation @ UCBerkeley EECS. Subscribe directly at robotic.substack.com. More at natolambert.com"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fdbffbfdf0dec&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgetting-just-the-gist-of-deep-rl-algorithms-dbffbfdf0dec&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgetting-just-the-gist-of-deep-rl-algorithms-dbffbfdf0dec&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgetting-just-the-gist-of-deep-rl-algorithms-dbffbfdf0dec&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgetting-just-the-gist-of-deep-rl-algorithms-dbffbfdf0dec&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://natolambert.medium.com/?source=post_page-----dbffbfdf0dec--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----dbffbfdf0dec--------------------------------", "anchor_text": ""}, {"url": "https://natolambert.medium.com/?source=post_page-----dbffbfdf0dec--------------------------------", "anchor_text": "Nathan Lambert"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F890b1765e6d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgetting-just-the-gist-of-deep-rl-algorithms-dbffbfdf0dec&user=Nathan+Lambert&userId=890b1765e6d&source=post_page-890b1765e6d----dbffbfdf0dec---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----dbffbfdf0dec--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fdbffbfdf0dec&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgetting-just-the-gist-of-deep-rl-algorithms-dbffbfdf0dec&user=Nathan+Lambert&userId=890b1765e6d&source=-----dbffbfdf0dec---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdbffbfdf0dec&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgetting-just-the-gist-of-deep-rl-algorithms-dbffbfdf0dec&source=-----dbffbfdf0dec---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://deepmind.com/blog/alphazero-shedding-new-light-grand-games-chess-shogi-and-go/", "anchor_text": "Shedding Light on AlphaZero"}, {"url": "https://en.wikipedia.org/wiki/Dynamic_programming", "anchor_text": "wikipedia"}, {"url": "https://en.wikipedia.org/wiki/Richard_E._Bellman", "anchor_text": "wikipedia"}, {"url": "https://en.wikipedia.org/wiki/TD-Gammon", "anchor_text": "Backgammon programs of Tesauro"}, {"url": "https://link.springer.com/article/10.1007/BF00992696", "anchor_text": "1992"}, {"url": "https://link.springer.com/article/10.1007/BF00992698", "anchor_text": "1992"}, {"url": "https://arxiv.org/pdf/1509.06461.pdf", "anchor_text": "2015"}, {"url": "https://arxiv.org/pdf/1708.05866.pdf", "anchor_text": "source"}, {"url": "https://papers.nips.cc/paper/1786-actor-critic-algorithms.pdf", "anchor_text": "2000"}, {"url": "https://medium.com/@natolambert/deep-rl-case-study-policy-based-vs-model-conditioned-gradients-in-rl-4546434c84b0", "anchor_text": "in another post"}, {"url": "https://drive.google.com/file/d/0BxXI_RttTZAhMVhsNk5VSXU0U3c/view", "anchor_text": "Source"}, {"url": "https://medium.com/@jonathan_hui/rl-trust-region-policy-optimization-trpo-explained-a6ee04eeeee9", "anchor_text": "More on TRPO here."}, {"url": "http://proceedings.mlr.press/v37/schulman15.pdf", "anchor_text": "2015"}, {"url": "https://arxiv.org/abs/1707.06347", "anchor_text": "2017"}, {"url": "https://arxiv.org/pdf/1509.02971.pdf", "anchor_text": "2016"}, {"url": "https://arxiv.org/abs/1710.02298", "anchor_text": "2017"}, {"url": "https://arxiv.org/abs/1802.09477", "anchor_text": "2018"}, {"url": "https://arxiv.org/abs/1801.01290", "anchor_text": "2018"}, {"url": "https://www.ias.informatik.tu-darmstadt.de/uploads/Publications/Deisenroth_ICML_2011.pdf", "anchor_text": "2011"}, {"url": "https://www.ias.informatik.tu-darmstadt.de/uploads/Publications/Deisenroth_ICML_2011.pdf", "anchor_text": "Source"}, {"url": "https://arxiv.org/abs/1805.12114", "anchor_text": "2018"}, {"url": "https://arxiv.org/abs/1805.12114", "anchor_text": "Source"}, {"url": "https://arxiv.org/abs/1809.05214", "anchor_text": "2018"}, {"url": "https://arxiv.org/abs/1802.10592", "anchor_text": "2018"}, {"url": "https://arxiv.org/abs/1903.00374", "anchor_text": "2019"}, {"url": "https://arxiv.org/abs/1903.00374", "anchor_text": "source"}, {"url": "https://arxiv.org/abs/1901.03737", "anchor_text": "quadrotors"}, {"url": "https://arxiv.org/abs/1708.02596", "anchor_text": "walking robots"}, {"url": "https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence", "anchor_text": "Kullback-Leibler Divergence"}, {"url": "https://arxiv.org/pdf/1708.05866.pdf", "anchor_text": "https://arxiv.org/pdf/1708.05866.pdf"}, {"url": "http://rail.eecs.berkeley.edu/deeprlcourse/", "anchor_text": "CS 294-112The lectures will be streamed and recorded. The course is not being offered as an online course, and the videos are\u2026rail.eecs.berkeley.edu"}, {"url": "https://blog.openai.com/spinning-up-in-deep-rl/", "anchor_text": "Spinning Up in Deep RLWe're releasing Spinning Up in Deep RL, an educational resource designed to let anyone learn to become a skilled\u2026blog.openai.com"}, {"url": "https://robotic.substack.com/", "anchor_text": "Democratizing AutomationA blog about robots & artificial intelligence, making them beneficial for everyone, and the coming automation wave\u2026robotic.substack.com"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----dbffbfdf0dec---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----dbffbfdf0dec---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/research?source=post_page-----dbffbfdf0dec---------------research-----------------", "anchor_text": "Research"}, {"url": "https://medium.com/tag/mathematics?source=post_page-----dbffbfdf0dec---------------mathematics-----------------", "anchor_text": "Mathematics"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fdbffbfdf0dec&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgetting-just-the-gist-of-deep-rl-algorithms-dbffbfdf0dec&user=Nathan+Lambert&userId=890b1765e6d&source=-----dbffbfdf0dec---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fdbffbfdf0dec&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgetting-just-the-gist-of-deep-rl-algorithms-dbffbfdf0dec&user=Nathan+Lambert&userId=890b1765e6d&source=-----dbffbfdf0dec---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdbffbfdf0dec&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgetting-just-the-gist-of-deep-rl-algorithms-dbffbfdf0dec&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://natolambert.medium.com/?source=post_page-----dbffbfdf0dec--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----dbffbfdf0dec--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F890b1765e6d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgetting-just-the-gist-of-deep-rl-algorithms-dbffbfdf0dec&user=Nathan+Lambert&userId=890b1765e6d&source=post_page-890b1765e6d----dbffbfdf0dec---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F15278b7ad062&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgetting-just-the-gist-of-deep-rl-algorithms-dbffbfdf0dec&newsletterV3=890b1765e6d&newsletterV3Id=15278b7ad062&user=Nathan+Lambert&userId=890b1765e6d&source=-----dbffbfdf0dec---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://natolambert.medium.com/?source=post_page-----dbffbfdf0dec--------------------------------", "anchor_text": "Written by Nathan Lambert"}, {"url": "https://natolambert.medium.com/followers?source=post_page-----dbffbfdf0dec--------------------------------", "anchor_text": "653 Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----dbffbfdf0dec--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "http://robotic.substack.com", "anchor_text": "robotic.substack.com"}, {"url": "http://natolambert.com", "anchor_text": "natolambert.com"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F890b1765e6d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgetting-just-the-gist-of-deep-rl-algorithms-dbffbfdf0dec&user=Nathan+Lambert&userId=890b1765e6d&source=post_page-890b1765e6d----dbffbfdf0dec---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F15278b7ad062&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgetting-just-the-gist-of-deep-rl-algorithms-dbffbfdf0dec&newsletterV3=890b1765e6d&newsletterV3Id=15278b7ad062&user=Nathan+Lambert&userId=890b1765e6d&source=-----dbffbfdf0dec---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://natolambert.medium.com/cutting-weight-101-for-athletes-cc2cca0ecd9b?source=author_recirc-----dbffbfdf0dec----0---------------------37ffcfff_be88_43de_a831_cca1bc097520-------", "anchor_text": ""}, {"url": "https://natolambert.medium.com/?source=author_recirc-----dbffbfdf0dec----0---------------------37ffcfff_be88_43de_a831_cca1bc097520-------", "anchor_text": ""}, {"url": "https://natolambert.medium.com/?source=author_recirc-----dbffbfdf0dec----0---------------------37ffcfff_be88_43de_a831_cca1bc097520-------", "anchor_text": "Nathan Lambert"}, {"url": "https://natolambert.medium.com/cutting-weight-101-for-athletes-cc2cca0ecd9b?source=author_recirc-----dbffbfdf0dec----0---------------------37ffcfff_be88_43de_a831_cca1bc097520-------", "anchor_text": "Cutting Weight 101 (for Athletes)You\u2019re an athlete that needs to get down XXX pounds in YYY days? Can I do it and am I risking my own health?"}, {"url": "https://natolambert.medium.com/cutting-weight-101-for-athletes-cc2cca0ecd9b?source=author_recirc-----dbffbfdf0dec----0---------------------37ffcfff_be88_43de_a831_cca1bc097520-------", "anchor_text": "9 min read\u00b7Feb 10, 2019"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2Fcc2cca0ecd9b&operation=register&redirect=https%3A%2F%2Fnatolambert.medium.com%2Fcutting-weight-101-for-athletes-cc2cca0ecd9b&user=Nathan+Lambert&userId=890b1765e6d&source=-----cc2cca0ecd9b----0-----------------clap_footer----37ffcfff_be88_43de_a831_cca1bc097520-------", "anchor_text": ""}, {"url": "https://natolambert.medium.com/cutting-weight-101-for-athletes-cc2cca0ecd9b?source=author_recirc-----dbffbfdf0dec----0---------------------37ffcfff_be88_43de_a831_cca1bc097520-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcc2cca0ecd9b&operation=register&redirect=https%3A%2F%2Fnatolambert.medium.com%2Fcutting-weight-101-for-athletes-cc2cca0ecd9b&source=-----dbffbfdf0dec----0-----------------bookmark_preview----37ffcfff_be88_43de_a831_cca1bc097520-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----dbffbfdf0dec----1---------------------37ffcfff_be88_43de_a831_cca1bc097520-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----dbffbfdf0dec----1---------------------37ffcfff_be88_43de_a831_cca1bc097520-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----dbffbfdf0dec----1---------------------37ffcfff_be88_43de_a831_cca1bc097520-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----dbffbfdf0dec----1---------------------37ffcfff_be88_43de_a831_cca1bc097520-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----dbffbfdf0dec----1---------------------37ffcfff_be88_43de_a831_cca1bc097520-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----dbffbfdf0dec----1---------------------37ffcfff_be88_43de_a831_cca1bc097520-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----37ffcfff_be88_43de_a831_cca1bc097520-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----dbffbfdf0dec----1---------------------37ffcfff_be88_43de_a831_cca1bc097520-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----dbffbfdf0dec----1-----------------bookmark_preview----37ffcfff_be88_43de_a831_cca1bc097520-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----dbffbfdf0dec----2---------------------37ffcfff_be88_43de_a831_cca1bc097520-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----dbffbfdf0dec----2---------------------37ffcfff_be88_43de_a831_cca1bc097520-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----dbffbfdf0dec----2---------------------37ffcfff_be88_43de_a831_cca1bc097520-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----dbffbfdf0dec----2---------------------37ffcfff_be88_43de_a831_cca1bc097520-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----dbffbfdf0dec----2---------------------37ffcfff_be88_43de_a831_cca1bc097520-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----dbffbfdf0dec----2---------------------37ffcfff_be88_43de_a831_cca1bc097520-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----2-----------------clap_footer----37ffcfff_be88_43de_a831_cca1bc097520-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----dbffbfdf0dec----2---------------------37ffcfff_be88_43de_a831_cca1bc097520-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----dbffbfdf0dec----2-----------------bookmark_preview----37ffcfff_be88_43de_a831_cca1bc097520-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/convergence-of-reinforcement-learning-algorithms-3d917f66b3b7?source=author_recirc-----dbffbfdf0dec----3---------------------37ffcfff_be88_43de_a831_cca1bc097520-------", "anchor_text": ""}, {"url": "https://natolambert.medium.com/?source=author_recirc-----dbffbfdf0dec----3---------------------37ffcfff_be88_43de_a831_cca1bc097520-------", "anchor_text": ""}, {"url": "https://natolambert.medium.com/?source=author_recirc-----dbffbfdf0dec----3---------------------37ffcfff_be88_43de_a831_cca1bc097520-------", "anchor_text": "Nathan Lambert"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----dbffbfdf0dec----3---------------------37ffcfff_be88_43de_a831_cca1bc097520-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/convergence-of-reinforcement-learning-algorithms-3d917f66b3b7?source=author_recirc-----dbffbfdf0dec----3---------------------37ffcfff_be88_43de_a831_cca1bc097520-------", "anchor_text": "Convergence of Reinforcement Learning AlgorithmsAre there any simple bounds one can put on convergence?"}, {"url": "https://towardsdatascience.com/convergence-of-reinforcement-learning-algorithms-3d917f66b3b7?source=author_recirc-----dbffbfdf0dec----3---------------------37ffcfff_be88_43de_a831_cca1bc097520-------", "anchor_text": "9 min read\u00b7Apr 9, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3d917f66b3b7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconvergence-of-reinforcement-learning-algorithms-3d917f66b3b7&user=Nathan+Lambert&userId=890b1765e6d&source=-----3d917f66b3b7----3-----------------clap_footer----37ffcfff_be88_43de_a831_cca1bc097520-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/convergence-of-reinforcement-learning-algorithms-3d917f66b3b7?source=author_recirc-----dbffbfdf0dec----3---------------------37ffcfff_be88_43de_a831_cca1bc097520-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3d917f66b3b7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconvergence-of-reinforcement-learning-algorithms-3d917f66b3b7&source=-----dbffbfdf0dec----3-----------------bookmark_preview----37ffcfff_be88_43de_a831_cca1bc097520-------", "anchor_text": ""}, {"url": "https://natolambert.medium.com/?source=post_page-----dbffbfdf0dec--------------------------------", "anchor_text": "See all from Nathan Lambert"}, {"url": "https://towardsdatascience.com/?source=post_page-----dbffbfdf0dec--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----dbffbfdf0dec----0---------------------bbcc8a5f_0454_4be2_a870_86834c9d05fb-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData?source=read_next_recirc-----dbffbfdf0dec----0---------------------bbcc8a5f_0454_4be2_a870_86834c9d05fb-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData?source=read_next_recirc-----dbffbfdf0dec----0---------------------bbcc8a5f_0454_4be2_a870_86834c9d05fb-------", "anchor_text": "Andrew Austin"}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----dbffbfdf0dec----0---------------------bbcc8a5f_0454_4be2_a870_86834c9d05fb-------", "anchor_text": "AI Anyone Can Understand Part 1: Reinforcement LearningReinforcement learning is a way for machines to learn by trying different things and seeing what works best. For example, a robot could\u2026"}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----dbffbfdf0dec----0---------------------bbcc8a5f_0454_4be2_a870_86834c9d05fb-------", "anchor_text": "\u00b74 min read\u00b7Dec 11, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F6c3b3d623a2d&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40MoneyAndData%2Fai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d&user=Andrew+Austin&userId=42d388912d13&source=-----6c3b3d623a2d----0-----------------clap_footer----bbcc8a5f_0454_4be2_a870_86834c9d05fb-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----dbffbfdf0dec----0---------------------bbcc8a5f_0454_4be2_a870_86834c9d05fb-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6c3b3d623a2d&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40MoneyAndData%2Fai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d&source=-----dbffbfdf0dec----0-----------------bookmark_preview----bbcc8a5f_0454_4be2_a870_86834c9d05fb-------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----dbffbfdf0dec----1---------------------bbcc8a5f_0454_4be2_a870_86834c9d05fb-------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk?source=read_next_recirc-----dbffbfdf0dec----1---------------------bbcc8a5f_0454_4be2_a870_86834c9d05fb-------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk?source=read_next_recirc-----dbffbfdf0dec----1---------------------bbcc8a5f_0454_4be2_a870_86834c9d05fb-------", "anchor_text": "Steve Roberts"}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----dbffbfdf0dec----1---------------------bbcc8a5f_0454_4be2_a870_86834c9d05fb-------", "anchor_text": "State Values and Policy Evaluation in 5 minutesAn Introduction to Reinforcement Learning"}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----dbffbfdf0dec----1---------------------bbcc8a5f_0454_4be2_a870_86834c9d05fb-------", "anchor_text": "\u00b75 min read\u00b7Jan 11"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2Ff3e00f3c1a50&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40tinkertytonk%2Fstate-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50&user=Steve+Roberts&userId=6b6735266652&source=-----f3e00f3c1a50----1-----------------clap_footer----bbcc8a5f_0454_4be2_a870_86834c9d05fb-------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----dbffbfdf0dec----1---------------------bbcc8a5f_0454_4be2_a870_86834c9d05fb-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff3e00f3c1a50&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40tinkertytonk%2Fstate-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50&source=-----dbffbfdf0dec----1-----------------bookmark_preview----bbcc8a5f_0454_4be2_a870_86834c9d05fb-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----dbffbfdf0dec----0---------------------bbcc8a5f_0454_4be2_a870_86834c9d05fb-------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=read_next_recirc-----dbffbfdf0dec----0---------------------bbcc8a5f_0454_4be2_a870_86834c9d05fb-------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=read_next_recirc-----dbffbfdf0dec----0---------------------bbcc8a5f_0454_4be2_a870_86834c9d05fb-------", "anchor_text": "Wouter van Heeswijk, PhD"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----dbffbfdf0dec----0---------------------bbcc8a5f_0454_4be2_a870_86834c9d05fb-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----dbffbfdf0dec----0---------------------bbcc8a5f_0454_4be2_a870_86834c9d05fb-------", "anchor_text": "Proximal Policy Optimization (PPO) ExplainedThe journey from REINFORCE to the go-to algorithm in continuous control"}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----dbffbfdf0dec----0---------------------bbcc8a5f_0454_4be2_a870_86834c9d05fb-------", "anchor_text": "\u00b713 min read\u00b7Nov 29, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fabed1952457b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fproximal-policy-optimization-ppo-explained-abed1952457b&user=Wouter+van+Heeswijk%2C+PhD&userId=33f45c9ab481&source=-----abed1952457b----0-----------------clap_footer----bbcc8a5f_0454_4be2_a870_86834c9d05fb-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----dbffbfdf0dec----0---------------------bbcc8a5f_0454_4be2_a870_86834c9d05fb-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fabed1952457b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fproximal-policy-optimization-ppo-explained-abed1952457b&source=-----dbffbfdf0dec----0-----------------bookmark_preview----bbcc8a5f_0454_4be2_a870_86834c9d05fb-------", "anchor_text": ""}, {"url": "https://medium.com/@anandmishra.kanha/deep-reinforcement-learning-current-state-of-art-383190b14464?source=read_next_recirc-----dbffbfdf0dec----1---------------------bbcc8a5f_0454_4be2_a870_86834c9d05fb-------", "anchor_text": ""}, {"url": "https://medium.com/@anandmishra.kanha?source=read_next_recirc-----dbffbfdf0dec----1---------------------bbcc8a5f_0454_4be2_a870_86834c9d05fb-------", "anchor_text": ""}, {"url": "https://medium.com/@anandmishra.kanha?source=read_next_recirc-----dbffbfdf0dec----1---------------------bbcc8a5f_0454_4be2_a870_86834c9d05fb-------", "anchor_text": "Anand Mishra"}, {"url": "https://medium.com/@anandmishra.kanha/deep-reinforcement-learning-current-state-of-art-383190b14464?source=read_next_recirc-----dbffbfdf0dec----1---------------------bbcc8a5f_0454_4be2_a870_86834c9d05fb-------", "anchor_text": "Deep reinforcement learning \u2014 current state of artCurrent"}, {"url": "https://medium.com/@anandmishra.kanha/deep-reinforcement-learning-current-state-of-art-383190b14464?source=read_next_recirc-----dbffbfdf0dec----1---------------------bbcc8a5f_0454_4be2_a870_86834c9d05fb-------", "anchor_text": "5 min read\u00b7Dec 15, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F383190b14464&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40anandmishra.kanha%2Fdeep-reinforcement-learning-current-state-of-art-383190b14464&user=Anand+Mishra&userId=86f86a9a5573&source=-----383190b14464----1-----------------clap_footer----bbcc8a5f_0454_4be2_a870_86834c9d05fb-------", "anchor_text": ""}, {"url": "https://medium.com/@anandmishra.kanha/deep-reinforcement-learning-current-state-of-art-383190b14464?source=read_next_recirc-----dbffbfdf0dec----1---------------------bbcc8a5f_0454_4be2_a870_86834c9d05fb-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F383190b14464&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40anandmishra.kanha%2Fdeep-reinforcement-learning-current-state-of-art-383190b14464&source=-----dbffbfdf0dec----1-----------------bookmark_preview----bbcc8a5f_0454_4be2_a870_86834c9d05fb-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/trust-region-policy-optimization-trpo-explained-4b56bd206fc2?source=read_next_recirc-----dbffbfdf0dec----2---------------------bbcc8a5f_0454_4be2_a870_86834c9d05fb-------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=read_next_recirc-----dbffbfdf0dec----2---------------------bbcc8a5f_0454_4be2_a870_86834c9d05fb-------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=read_next_recirc-----dbffbfdf0dec----2---------------------bbcc8a5f_0454_4be2_a870_86834c9d05fb-------", "anchor_text": "Wouter van Heeswijk, PhD"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----dbffbfdf0dec----2---------------------bbcc8a5f_0454_4be2_a870_86834c9d05fb-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/trust-region-policy-optimization-trpo-explained-4b56bd206fc2?source=read_next_recirc-----dbffbfdf0dec----2---------------------bbcc8a5f_0454_4be2_a870_86834c9d05fb-------", "anchor_text": "Trust Region Policy Optimization (TRPO) ExplainedThe Reinforcement Learning algorithm TRPO builds upon natural policy gradient algorithms, ensuring updates remain within \u2018trustworthy\u2019\u2026"}, {"url": "https://towardsdatascience.com/trust-region-policy-optimization-trpo-explained-4b56bd206fc2?source=read_next_recirc-----dbffbfdf0dec----2---------------------bbcc8a5f_0454_4be2_a870_86834c9d05fb-------", "anchor_text": "\u00b712 min read\u00b7Oct 12, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4b56bd206fc2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrust-region-policy-optimization-trpo-explained-4b56bd206fc2&user=Wouter+van+Heeswijk%2C+PhD&userId=33f45c9ab481&source=-----4b56bd206fc2----2-----------------clap_footer----bbcc8a5f_0454_4be2_a870_86834c9d05fb-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/trust-region-policy-optimization-trpo-explained-4b56bd206fc2?source=read_next_recirc-----dbffbfdf0dec----2---------------------bbcc8a5f_0454_4be2_a870_86834c9d05fb-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4b56bd206fc2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrust-region-policy-optimization-trpo-explained-4b56bd206fc2&source=-----dbffbfdf0dec----2-----------------bookmark_preview----bbcc8a5f_0454_4be2_a870_86834c9d05fb-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----dbffbfdf0dec----3---------------------bbcc8a5f_0454_4be2_a870_86834c9d05fb-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----dbffbfdf0dec----3---------------------bbcc8a5f_0454_4be2_a870_86834c9d05fb-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----dbffbfdf0dec----3---------------------bbcc8a5f_0454_4be2_a870_86834c9d05fb-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----dbffbfdf0dec----3---------------------bbcc8a5f_0454_4be2_a870_86834c9d05fb-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----dbffbfdf0dec----3---------------------bbcc8a5f_0454_4be2_a870_86834c9d05fb-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----dbffbfdf0dec----3---------------------bbcc8a5f_0454_4be2_a870_86834c9d05fb-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----3-----------------clap_footer----bbcc8a5f_0454_4be2_a870_86834c9d05fb-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----dbffbfdf0dec----3---------------------bbcc8a5f_0454_4be2_a870_86834c9d05fb-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----dbffbfdf0dec----3-----------------bookmark_preview----bbcc8a5f_0454_4be2_a870_86834c9d05fb-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----dbffbfdf0dec--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----dbffbfdf0dec--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----dbffbfdf0dec--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----dbffbfdf0dec--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----dbffbfdf0dec--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----dbffbfdf0dec--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----dbffbfdf0dec--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----dbffbfdf0dec--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----dbffbfdf0dec--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----dbffbfdf0dec--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}