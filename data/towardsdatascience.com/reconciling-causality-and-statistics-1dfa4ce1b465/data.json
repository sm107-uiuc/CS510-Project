{"url": "https://towardsdatascience.com/reconciling-causality-and-statistics-1dfa4ce1b465", "time": 1683010664.6794639, "path": "towardsdatascience.com/reconciling-causality-and-statistics-1dfa4ce1b465/", "webpage": {"metadata": {"title": "Reconciling Causality and Statistics | by Pirmin Lemberger | Towards Data Science", "h1": "Reconciling Causality and Statistics", "description": "Consider the following three pairs of concomitant events: \u201cthe sun rises shortly after the first crow of the rooster\u201d, \u201cthe number of visits to some website tends to increase when an ad banner is\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.linkedin.com/in/pirminlemberger/", "anchor_text": "Pirmin Lemberger", "paragraph_index": 0}, {"url": "https://www.linkedin.com/in/denisoblin/", "anchor_text": "Denis Oblin", "paragraph_index": 0}, {"url": "https://arxiv.org/abs/2007.03940", "anchor_text": "PDF version with LaTeX typesetting on arXiv", "paragraph_index": 96}], "all_paragraphs": ["This is joint work by Pirmin Lemberger and Denis Oblin.", "Consider the following three pairs of concomitant events: \u201cthe sun rises shortly after the first crow of the rooster\u201d, \u201cthe number of visits to some website tends to increase when an ad banner is shown in some appropriate location\u2019\u2019 and \u201cthe rate of skin cancer parallels the sale of ice creams on the French Riviera\u2019\u2019.", "While common sense dictates that there is little chance to reduce cancer by limiting ice cream sales or to speed up sunrise by tickling the rooster so it crows earlier, it is not unlikely though that trying to optimize the position of an ad banner might indeed be worth the effort. So it seems our intuition about how the world works tells us something about these concomitant events which goes beyond what mere statistics can describe. Statistics indeed only deals with correlation, meaning it quantifies how likely it is that an event B will occur assuming that we have observed another event A. It tells us nothing however about what happens if we act on a system and it even provides no means to describe what an intervention on a system is. The conclusion is thus straightforward: in general, we need more than just probability and statistics to express and answer causality questions. The method of Randomized Controlled Trials (RCT) which entirely rests on statistic analysis and is widely recognized as the gold standard for clinical trials is one noteworthy exception that we shall have more to say about later on.", "The innate inability of plain statistics to properly deal with causality in all its generality has only been recognized at the turn of this century [1]. Before that, a controversy had been going on for nearly one century. We won\u2019t enter these lengthy debates because fortunately nowadays there is no reason for controversy anymore! The pioneering work of Judea Pearl [1,2,3], a philosopher and statistician, has put a definitive end to these quarrels by providing a mathematically sound framework that allows both to formulate causality questions in an unambiguous way and to answer them systematically whenever this is possible [3]. Judea Pearl is largely responsible for a complete reshaping of the causality debate, which has since then been termed the Causal Revolution. For this and his other outstanding achievements on causal reasoning in AI, he was awarded the Turing prize in 2011.", "Before we dive into the subtleties of Judea Pearl\u2019s conceptions of causality, let\u2019s acknowledge that many data scientists who use supervised machine learning as their favorite tool never bother about causality issues. As a matter of fact, they could run into serious trouble because of this but it is nevertheless a legitimate attitude in two common situations that we briefly mention.", "Where Judea Pearl\u2019s approach really shines is in less obvious situations where we would like to infer the effect of setting X=x on the value of Y in the presence of a whole network of causal relationships that involves other variables Z. These other variables are often called concomitants [4] and may be observed or not. In its simplest version, the root of the difficulty in answering causal questions is depicted in figure 1 which illustrates the notion of a confounder Z.", "In intuitive terms, a confounder Z is a variable that could potentially hinder the identification of a causal relationship between two other variables X and Y by being a cause of both. An observed change in the value of Y that results from a change of X could just as well be attributed to a change of Z. Yet, if had access to the values of the confounder Z, we could perhaps adjust for its effect and thus extract the desired effect of X on Y. The procedure for doing this, when it is possible, is well known to statisticians and goes under the name \u201ccontrolling for Z\u2019\u2019. In a nutshell, what Pearl\u2019s machinery achieves is a vast generalization of this controlling procedure:", "Pearl\u2019s graphic and algebraic tools allow the identification of situations in which we may possibly disambiguate a causal relationship between two variables X and Y. Wherever this is possible these tools provide explicit formulas for computing the effect on a variable Y of an intervention that prescribes the value x of another variable X. These formulas merge two kinds of information. First, they uses the values of some concomitants Z which are just observed. Second, they leverage a causal graph G which encapsulates our knowledge of the causal relationships between the variables within a system of interest.", "Later we shall consider still another way to interpret Pearl\u2019s work namely as a vast generalization of RCT\u2019s.", "Pearl\u2019s conceptual and computational tools have several important merits:", "The sequel of this article is organized as follows. Section 2 introduces the main concepts and tools defined by Pearl and coworkers. After a brief review of Bayesian networks which allows us to introduce the notion of d\u2013separation, we define what a functional causal model is and describe the extent to which it can be inferred from data. We define the notions of intervention and of identifiability. We explain the graphical back-door and front-door criteria that allow to identify causal effects in some specific circumstances. At last, we introduce the rules of the do Calculus that are even more flexible. Section 3 discusses various business examples as illustrations. The last section offers some concluding remarks and tries to explain why in our opinion these tools still remain little known within the data scientist community at large.", "In this section, we will present the main definitions and mathematical results underpinning the Causal Revolution. We define what causal relationships are, how and when we can identify them from data. Then we define what an intervention is and describe the graphical and algebraic tools which allow to identify situations where we can compute the consequences of an intervention using data collected from pure observation. For proofs, we refer to Pearl\u2019s book on Causality [2] and to the original papers [3].", "The tools we shall present in the following subsections rely heavily on a graphical method called the d-separation criterion that allows answering general independence questions among groups of random variables (r.v). These tools were pioneered by Pearl who developed the concept of a Bayesian Network. There are many good textbooks on this topic [9] but we provide a brief introduction both to introduce notations and to make the presentation reasonably self-contained. Readers familiar with this material are invited to jump to subsection 2.2.", "We use upper case letters like X, Y or Z to designate r.v. and boldface upper case to denote sequences (or sets) of r.v X=(X_1, X_2,\u2026, X_m), with lower indices to denote the components of X. For simplicity we shall assume that each component takes only a finite number of values. The corresponding realizations of these r.v. will be denoted with corresponding lower case letters x=(x_1, x_2,\u2026, x_m).", "The first step is to associate a graph G to a probability distribution p(x)=p(x_1, x_2,\u2026, x_m). Iterating the product rule for conditional probabilities we have:", "where the conditioning set for j=1 is empty. Now if X_j is only conditioned on a subset of X_1,\u2026,X_j-1, let\u2019s denote the minimal such subset by PA_j, the parents of X_j, so that we can write:", "A natural representation of (2) is to use a Bayesian network which is a directed acyclic graph (DAG) where each node of the graph corresponds to a variable X_j and a directed link points from each member of PA_j towards X_j as figure 2 shows.", "We say that the probability distribution p and the graph G are compatible. Note also that according to (1) the DAG associated to a generic distribution p with no specific factorization properties is a fully connected graph. Hence all information in a DAG is really contained it the missing links.", "For a given order of the r.v. this graph G is unique. However, if we had chosen another order for the r.v. in p we would have obtained another graph G\u2019. Therefore, the natural question that arises is to determine when two DAG G and G\u2019 observationally equivalent. More precisely we want to ask whether any probability distribution p which is compatible with G is also compatible with another graph G\u2019. The following can be shown [2]:", "Theorem 1(observational equivalence): Two DAG\u2019s are observationally equivalent if two conditions are met. First the graphs that result from stripping of the arrows on the edges of G and G\u2019 (their skeletons) should be the same. Second, G and G\u2019 should have the same v-structures, which are nodes with converging arrows whose tails are not connected by an arrow.", "Let X, Y and Z be three groups of r.v. whose joint distribution is p(x,y,z). Of particular interest is the question whether X is independent of Y conditionally on observing Z, that is whether", "When the answer is yes, we denote this by (X \u2018indep of\u2019 Y | Z). The d-separation theorem below allows us to answer this question graphically by looking at the graph G associated to p. For this we need the following graphical concept:", "Definition 1: A path \u03b3 (a sequence of consecutive edges oriented in any direction) in a DAG G is said to be blocked by a set of nodes Z if and only if one of the following two conditions is met:", "A set Z is said to d-separate X from Y whenever Z blocks every path from a node in X to a node in Y.", "We can now formulate the d-separation criterion which relate the factorization properties of p described by a compatible DAG G and the independence relationships they imply:", "Theorem 2 (d-separation theorem): Let X,Y,Z be three disjoint sets of nodes in a DAG G compatible with a distribution p. Then the corresponding r.v. distributed according to p satisfy X \u2018indep of\u2019 Y | Z whenever X and Y are d-separated by Z.", "The converse is true as well. An important consequence from theorem 2 is that the set of conditional independence among r.v. is determined by the topology of G only, while the ordering of the r.v. plays no role. When we consider the d-separation criterion as a graphical test on the nodes of a DAG G we shall write (X \u2018indep of\u2019 Y | Z)_G. Using (4) we can then rewrite theorem 2 compactly as", "We insist that that so far there has been no discussion of causality, only independence questions were examined. There is however an easy mnemonic for the d-separation criterion if we momentarily allow ourselves to associate a causal meaning to the arrows in G. Indeed, if we fix the value of the middle node m in a chain or a fork we block the information flow between the two end nodes i and j which thus become independent. If, on the contrary, we observe the value of a collider node m, or of some of its descendants, we actually observe a common consequence of i and j. Knowing both i and m for example will thus tell us something about j, which thus opens the information flow. This independence analysis based on d-separation prefigures the genuine causal analysis we develop in the next subsections.", "A conditional probability p(y|x) tells us how the probability of seeing Y=y is affected by having observed that X=x. It tells us nothing however about what happens when we prescribe the value of X. To answer that question we need more information than p contains, namely information about how the probabilistic model p will change under an external intervention. This is precisely what a functional causal model M does. It is defined as a set of functional relationships which describe how each r.v. X_j is determined as a function of the others and of some noisy disturbances. It thus explains how the data is generated. Using the notation PA_j to denote the set of r.v. that directly influence X_j, a causal model is then defined as a set of structural equations:", "where the f_j\u2019s are deterministic functions of both the parents variables PA_j and some noisy disturbance \u03f5_j. An important point is that the \u03f5 should be independent. In other words each relation is assumed to be disturbed by a single random perturbation. If a perturbation were to affect several r.v. simultaneously, we should promote it to an unobserved X_j with a structural equation describing its influence on other variables. A causal graph G can be associated to a causal model M as shown in figure 4 which also defines the notion of a confounding arc.", "The probability distribution p on a set of r.v. X is completely specified by the model M which comprises the set of deterministic functions f_j and the distributions of the disturbances \u03f5. In practice, these will, however, remain unspecified. Beware that the links in figure 2 represent conditional dependencies p(x_j|pa_j) while they represent deterministic dependencies x_j = f_j(pa_j, \u03f5_j) in figure 4.", "Assume that we are given a causal graph G associated with a causal model M which generates a probability distribution p. What independence relationships hold for this p? The answer is as simple as we could have dreamed: they are precisely those determined by the d-separation criterion applied to G. This follows from the fact that p is compatible with G in the Bayesian network sense and from the d-separation theorem.", "An intervention on a system described by a causal model is an alteration of the structural equations (6) where some relationships are replaced by others. In the simplest case where we fix the value of, say, X_i to a this amounts to replacing the equation X_i=f_i(PA_i, \u03f5_i) with X_i=a. The corresponding causal graph of the modified model is thus obtained from G by removing all links connecting the nodes to X_i to their parents PA_i as figure 5 shows.", "After the intervention, the probability distribution p(x) is replaced by a new one which we denote by p(x|do(x_i=a)). This defines the do operator. Using this definition and the factorization (1) for the distribution p we can also express the post intervention distribution as a truncated factorization:", "In general p(x|do(x_i=a)) is of course different from the classical conditional p(x|x_i=a). One trivial case where both expressions are indeed identical is when X_i has no parents that is PA_i is the empty set. The general question of when an action on a variable X has the same consequence on another variable Y as an observation will be answered precisely when we present the do-calculus in section 2.4.", "To get more intuition about the do-operator let\u2019s compute the effect of an intervention do(x_i=a) on a single variable Y which is different from both X_i and its parents PA_i. Plugging", "into (7) for x_i=a and marginalizing over all variables except on y and on x_i we obtain", "This conditioning and weighting operation (9) on the variables PA_i to compute p(y|do(x_i=a)) is known as adjusting for the direct causes of X_i.", "Equation (9) shows that when the parents PA_i of a manipulated variable X_i are measurable, then the effect on Y of an intervention on X_i can be computed from passive observations only. Indeed these determine both p(x) and p(x_i|pa_i). In practice we could use machine learning for this purpose.", "The interesting question then is whether we can compute the effect of an intervention when this is not the case, that is when some of the r.v. in PA_i are not measurable? More generally, the question that naturally arises is then the following. Given a causal graph G and a subset Z of observable concomitants can we express p(y|do(x=a)) in terms of the distribution on the observed variables Z, X and Y only? When this is the case we shall say that the causal effect of X on Y is identifiable. Let\u2019s compactly summarize this concept of identifiability [11].", "Passive observation of the variables X and Y and a group Z of observable concomitants informs us on their joint distribution p(x,y,z), as long as we have enough data of course. However, the effect p(y|do(x)) on Y of an intervention on X is only encoded in the causal model M which generates this p, but not in p itself! The problem we thus face is that two different causal models M_1 and M_2 could generate the same p. To predict the effect of an intervention we thus need more information than is contained in p. The causal graph G associated to the causal model M can sometimes provide this missing information, without the need for a complete knowledge of the causal model M. When this is the case, we say that the intervention p(y|do(x)) is identifiable from G.", "In subsections 2.3 and 2.4 we shall describe graphical and symbolic tools to answer these questions.", "Interpreting Randomized Control Trials with the do Operator", "Lets quickly make contact between the definition (7) of the do operator and the Randomized Control Trial (RCT) procedure. Consider a situation where we would like to assess the effectiveness of a treatment on a group of patients. Say that X=1 means we apply the treatment and X=0 we don\u2019t and that Y represents the resulting impact of their health (\u201cimproved\u201d\u2019, \u201cstable\u201d, \u201cdeteriorated\u201d). Patients are characterized by features like Z_1=age, Z_2=gender and Z_3=is diabetic which could also influence their health state.", "The corresponding causal diagram is shown in figure 6 (a). For this diagram we would like to compute p(y|do(x)). If the experimenter chooses which patient she will administer the treatment to, she could inadvertently transform the features Z into confounders that will simultaneously impact the decision X to administer the treatment and the health Y of the patient. This is represented in figure 6 (b). The RCT procedure subordinates the decision to administer the treatment to the result of flipping a perfect coin C whose outcome is neither influenced by Z nor has any direct influence on Y. It thus cuts the same incoming links to the instrumented variable X as those which had to be removed to define p(y|do(x)) in (7). In such a circumstance p(y|x,c)=p(y|x), because X blocks the path between C and Y. Therefore (9) implies that p(y|do(x))=p(y|x). In other words, when using the RCT procedure observing X has the same effect on predicting Y as prescribing the value of X.", "Can we Infer a Causal Graph from Raw Data?", "In sections 2.3 and 2.4 we shall describe tools to answer the identifiability question above assuming that we are given a causal graph G and that some of its variables Z can be measured. But what if we have no causal graph G available in the first place? Can we infer it directly from data? This is a secular question which has sparked a lot of controversy. In everyday life we all use some clues to identify a causal relationship, the most important one being the assumption that a cause always precedes its effects. Analyzing these temporal aspects of causality would however quickly drive us into discussing the origin of the asymmetry between the past and the future and other foundational issues in physics [8]. As interesting as they are, we will not address these issues here, because this would take us too far away from our subject of analyzing the patterns of data which can identity functional causal relations as defined by structural equations (6). We refer to [1] for a broader panorama of causality.", "The common wisdom among statisticians has long been that this identifying a causal model from data is plain impossible. And, indeed, in general, it is impossible as we shall confirm. The interesting point though is that contrary to this common belief there are practically relevant situations where this identification of G is possible, at least in part. Again, a thorough discussion of these matters would be out of the scope for this short paper. We, therefore, limit ourselves to summarizing one significant result in [2] which shows that, assuming a specific interpretation of Occam\u2019s razor, an equivalence class of causal graphs G can be inferred when all variables X in G are measurable.", "So let's assume that the variables X are all measurable and that their probability distribution is p(x). With no further assumptions, there could exist many different causal graphs compatible with p. Therefore we need to put further restrictions on the causal models we are trying to infer. We shall indeed put two such restrictions. The first, minimality, conforms with standard scientific practice and restricts our search to the simplest possible causal graphs G only, the second, stability, will be a condition on the causal model M itself. To formulate them precisely we assume that the causal model M defined by (6), depends on parameters \u03b8 that define both the deterministic dependencies f_j and the distributions of the noise variables \u03f5_j. Such a parameterized causal model M therefore really defines a set of distributions p that could be obtained by varying the parameters \u03b8 of M.", "Theorem 1 on observational equivalence then implies the following:", "Theorem 3: If a causal model M generates a stable distribution p then, up to observational equivalence, there is a unique minimal causal graph G compatible with p.", "A class of observationally equivalent graphs can be represented graphically by a representation called a pattern [2]. A pattern is a partially directed DAG where some edges are directed while others are not. The directed ones correspond to edges that are common to all members of the pattern, while the undirected one could be oriented in either direction. Pearl and Verma [6] developed an algorithm, the IC algorithm (for Inferred Causality) which constructs the pattern associated to any stable distribution p on a set of observable variables. We refer to [2] for a detailed description of the IC algorithm and for various extensions of IC to causal models with latent variables. Other interesting results obtained by Pearl pertain to the falsifiability of some assumptions in a causal graph [12,13].", "As we have seen earlier, the causal effect p(y|do(x) is identifiable as soon as the parents PA(X) of the cause X are measurable. Formula (9) answers the question by showing how we should adjust the conditionals p(y|x, pa) according to the prior probabilities p(pa) of the direct causes. In this subsection we describe two simple graphical criteria which answer the question in more general cases.", "Theorem 4: The effect p(y|do(x)) on Y of an intervention on X is identifiable from the causal graph G when a group Z of observable r.v. satisfy the following two conditions:", "When both conditions are verified we have the reduction:", "The above reduction formula can be considered as a generalization of the basic adjustment formula (9). Both conditions in theorem 4 can be easily tested systematically. Various sets Z can be tried to optimize the cost of their measurement.", "Figure 7 gives an example of a group Z=(Z_3,Z_4) which makes the causal influence of X on Y identifiable while Z\u2019=(Z_4) does not.", "Condition 1 in theorem 4 requires that the cause X should not affect the observable variables Z. Fortunately, another criterion, namely the Front-Door criterion allows us to prove that the effect of X on Y is nevertheless identifiable in other some cases. Consider the situation depicted in figure 8 obtained by amalgamating Z_1 to Z_5 in figure 7 into a group U which we assume to be unobserved then renaming Z_6 as Z and denoting Z=(Z). This variable Z is under direct influence of X and does not block the path X \u2190 U \u2192 Y. Therefore Z meet neither of the two back-door criteria.", "Note that the following three conditions hold in the example of figure 8:", "When these three conditions are met we say that the variables Z satisfie the frond-door criterion relative to the causal effect of X on Y. The following then holds:", "Theorem 5: When the variables Z satisfy the front-door criterion for the effect of a variable X on a variable Y, then this effect is identifiable and is given by", "Notice that the variables u do not appear in the above reduction formula as should be the case for unmeasured variables. This second graphical tool thus complements our arsenal for discovering identifiable causal graphs. We will prove it in subsection 2.4 as an example application of the do Calculus.", "Graphical tools are nice because they allow us to identify causal relationships without having to do any math. Unfortunately, this is not always possible. In this subsection we present three simple algebraic rules [3] that can be combined to transform expressions containing both ordinary conditioning .|x and interventions .|do(x) into equivalent expressions. These rules are the rule of the do Calculus. Our goal, of course, is to find an appropriate sequence of these transformations to eventually get rid of all .|do(x) in order to prove that an intervention is identifiable.", "Assume we are given four disjoint subsets of variables X, Y, Z and W in a causal graph G. Let us denote by", "the graphs obtained from G by respectively deleting the links incoming into X and outgoing from X. To represent simultaneous removal of incoming and outgoing links we use a notation like", "At last, Z(W) will denote the set of Z-nodes which are not ancestors of any W-nodes in", "In the 3 rules below, X always enters as a do(X) intervention, while W occurs as a passive observation and Y as the consequences we would like to predict. We also define a mixed conditional probability on Y given an intervention on X and an observation of W:", "Theorem 6 (do Calculus Rules): Assume that the distribution p is generated by a causal model associated with the graph G. Then the following holds:", "These rules generalize the basic d-separation rule (5) which recall here for comparison:", "In fact (13) is a special case of rule 1 when X is empty. In the other way around, we can easily understand rule 1 as a consequence of (13) when we remember that an intervention do(x) simply removes from G all links entering into X. What results is precisely the graph used in rule 1. Rules 2 and 3 use sub-graphs of that graph. Rule 3 answers the important question of whether an intervention on the variable Z has the same effect as its passive observation. The simplest case where rule 3 applies is when G is the two-node graph Z \u2192 Y, describing a direct causal effect, for which", "reduces to two disconnected nodes thus trivially ensuring that Z and Y are d-separated with that graph.", "An Example \u2014 Deriving the Front-Door formula", "As an example of an application of the do Calculus let\u2019s see how we can use them to derive the front door rule (11), referring to figure 9 for the various subgraphs that we shall need to examine.", "Lets go through this step by step following [3].", "First, using the sum rule and definition (12), the desired quantity can be written as:", "Now we can apply rule 3 to remove the do operator on x in the second factor of (14) to write", "Lets now focus on the factor p(y|z, do(x)) in (14). Using rule 3 again we can replace z by do(z)", "Rule 2 now allows deleting the do(x) action from the right member of (16), therefore", "Lets thus compute p(y|do(z)). Apply definition (12) on mixed conditioning and marginalizing over x", "We can apply rule 2 which implies p(x|do(z))=p(x). We can also apply rule 3 which implies p(y|x, do(z))=p(y|x, z). Altogether (18) and these last two results imply", "which is the same as the front door rule (11).", "As this example shows, the do Calculus allows for elegant derivations of identifiability cases but by no means is it obvious! On the contrary, it requires a fair amount of intuition and practice.", "Strengths and Limitations of the Do Calculus", "The rules of the do Calculus have been shown to be complete [10] in the sense that, if an intervention is identifiable at all, then there exists indeed a sequence of these 3 rules that will produce an expression free of any do operator. Unfortunately, there is no general rule for deciding whether an expression is identifiable or not [2]. In fact one the main difficulty in applying the do Calculus is that there is no general guiding principle that tells us which rule to apply at each step. Luckily, courageous people have been compiling both identifiable and non identifiable causal diagrams that we can reuse. Figure 10 shows examples which are identifiable.", "One important remark is that we can generate many more identifiable diagrams from these as soon as we realize the following two facts:", "All diagrams shown in figure 10 are maximal in the sense that adding a single edge to any of them will destroy their identifiability. They are thus the most useful graphs to list. Figure 11 shows examples which are not identifiable.", "Examples of causal analysis are often drawn from fields like agronomy, social sciences or economics which rely on complex causal models. We rather chose to illustrate how causal analysis naturally occurs in even the most common business examples.", "The level of satisfaction of customers with a service can often be judged from their observable behavior such as the number of visits to a website or the time spent for using that service. This information can help the service provider to identify early signs that some customers intend to leave the service. Marketing can then decide to launch a loyalty campaign that aims at retaining the customers whose faith wavers. These campaigns of course have a cost and its therefore important to be able to assess their effectiveness. Does a loyalty campaign of a certain type really have an impact on the attrition rate?", "To answer this question let\u2019s formalize the problem. First, let U be a binary variable that corresponds to the intention of the customer to leave the service or not, before being targeted by any loyalty campaign. This, of course, will be an unmeasured variable. This initial intention U certainly has an impact on the behavior of the customer that can be measured, let\u2019s call this Z. This behavior Z will determine in turn the decision by the service provider whether this customer should be targeted by a loyalty campaign. Let\u2019s call this decision X. Eventually, the final decision of the customer to cancel the service is again a binary variable, call it Y. This final decision of the customer is certainly dependent both on his initial intention U and whether he has been targeted or not by the campaign (X). So, formally, our question translates as : can we identify the causal impact of being target X on the final decision Y when we measure Z ? Figure 12 shows the corresponding causal diagram.", "This diagram is nothing but the diagram (c) from the list of identifiable diagrams in figure 10 with the direct link from Z to Y removed. Therefore it is identifiable. On the other hand Z satisfies the two condition of the back-door criterion in theorem 4. It is not a descendant of X and it blocks the back-door path between X and Y because it is an observed chain. Thus (10) gives the explicit reduction of the causal impact of X on Y into observable quantities.", "When a claim is reported by an insured, an insurer usually sets aside a certain amount. This is especially true when personal injury is reported. The final compensation will obviously depend on the extent of the damage. But, as insurers have become aware, this compensation will also depend on the initial provisioning. The aim of causal analysis here is to determine the impact of this provisioning on the final compensation granted to the insured.", "Let U be the complete claim characterization. Its value is unknown at the time of reporting but it will determine both the declaration Z by the insured and the final compensation Y granted after investigations. The initial evaluation Z is what directly determines the expert\u2019s provisioning X which in turns impacts the final compensation Y. The causal graph is thus exactly the same as in figure 12 and, as a consequence, the same back-door reduction formula (10) applies here as well.", "To improve the efficiency of its sales teams, a company can invest in their training. This has an easily measurable cost X which it is hoped will help to increase the turnover Y by increasing sales. As such, the causal relationship would be direct and thus trivially identifiable. However there is at least one unmeasured confounding variable which spoils this identifiability, namely the competitive pressure U. Indeed a strong competitive pressure U will encourage investment X in a training effort but, simultaneously, it will also impact the turnover Y. Fortunately though, we can restore identifiability if we can find a measurable \u201cdeconfounding\u201d variable Z within the causal chain from X to Y. The skills and motivation of salespeople induced by these training and measured by surveys or polls could play this role. The causal graph is depicted in figure 13, it is the same as in figure 8 which was the basic graph for which the front-door rule applies. We can thus use the front-door reduction formula (11).", "The turnover Y for a company on a product is a function of the price X of the item and its sales volume Z. For instance Y could be proportional to X * Z. Now the price X impacts both the turnover Y but also the sales volume Z, because lower prices generally increasing the volume. As such, the corresponding causal graph would be identifiable because it corresponds to graph (b) from in the list of identifiable graphs from figure 10 with the confounding link between Y and Z removed. Unfortunately, the competition pressure U which is an unknown variables is a confounder which impacts both the price X and the sales volume Z rendering the graph in figure 14 unidentifiable as it is the same as the graph (c) in the list of unidentifiable graphs in figure 11.", "Once one fully grasps the importance of the paradigm shift implied by the Causal Revolution, a question will occur to many newcomers. Why hasn\u2019t anyone ever told me about this while I as learning statistics, probability or machine learning? How come as a data scientist I\u2019ve never heard of the do Calculus? Is it really useful in practice? In what area? There are probably several answers to this. We list some of them below.", "The founding fathers of statistics were certainly right when they insisted that, in general, we cannot infer causal relations from mere data. But in the enthusiasm of their nascent discipline, many of their disciples and epigones have somehow become overzealous. They neglected that in most cases we have more information available than just data! We have some prior knowledge of how the world works! Perhaps not a full-fledged causal model, but at least a partial causal graph. What Pearl tells us is that in some favorable conditions (that we discussed in section 2), we can merge those two pieces of knowledge in order to predict the effect of an intervention on a system. If it turns out that these predictions do not match observations, we just have to work harder and think of some better model. But, in any case, we have to give up the hope of automating the discovery of how the world works from observation only. Nature won\u2019t reveal its charms in front of some dumb algorithm. She is a demanding mistress and expects us to engage in a thoughtful conversation with her, as curious and creative minds. So, let\u2019s rejoice, because this is why science is so much fun!", "PDF version with LaTeX typesetting on arXiv", "I am currently scientific director at onepoint. My main interests are in Deep Learning, NLP and general Data Science. I have a PhD in theoretical physics."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F1dfa4ce1b465&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freconciling-causality-and-statistics-1dfa4ce1b465&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freconciling-causality-and-statistics-1dfa4ce1b465&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freconciling-causality-and-statistics-1dfa4ce1b465&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freconciling-causality-and-statistics-1dfa4ce1b465&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/@pirminlemberger?source=post_page-----1dfa4ce1b465--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1dfa4ce1b465--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@pirminlemberger?source=post_page-----1dfa4ce1b465--------------------------------", "anchor_text": "Pirmin Lemberger"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Faa1fd8f69eaf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freconciling-causality-and-statistics-1dfa4ce1b465&user=Pirmin+Lemberger&userId=aa1fd8f69eaf&source=post_page-aa1fd8f69eaf----1dfa4ce1b465---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----1dfa4ce1b465--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1dfa4ce1b465&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freconciling-causality-and-statistics-1dfa4ce1b465&user=Pirmin+Lemberger&userId=aa1fd8f69eaf&source=-----1dfa4ce1b465---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1dfa4ce1b465&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freconciling-causality-and-statistics-1dfa4ce1b465&source=-----1dfa4ce1b465---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://photos.app.goo.gl/a1BjXQkFTSeRV5ac7", "anchor_text": "source"}, {"url": "https://www.linkedin.com/in/pirminlemberger/", "anchor_text": "Pirmin Lemberger"}, {"url": "https://www.linkedin.com/in/denisoblin/", "anchor_text": "Denis Oblin"}, {"url": "https://arxiv.org/abs/2007.03940", "anchor_text": "PDF version with LaTeX typesetting on arXiv"}, {"url": "https://medium.com/tag/causality?source=post_page-----1dfa4ce1b465---------------causality-----------------", "anchor_text": "Causality"}, {"url": "https://medium.com/tag/statistics?source=post_page-----1dfa4ce1b465---------------statistics-----------------", "anchor_text": "Statistics"}, {"url": "https://medium.com/tag/probability?source=post_page-----1dfa4ce1b465---------------probability-----------------", "anchor_text": "Probability"}, {"url": "https://medium.com/tag/editors-pick?source=post_page-----1dfa4ce1b465---------------editors_pick-----------------", "anchor_text": "Editors Pick"}, {"url": "https://medium.com/tag/mathematics?source=post_page-----1dfa4ce1b465---------------mathematics-----------------", "anchor_text": "Mathematics"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1dfa4ce1b465&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freconciling-causality-and-statistics-1dfa4ce1b465&user=Pirmin+Lemberger&userId=aa1fd8f69eaf&source=-----1dfa4ce1b465---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1dfa4ce1b465&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freconciling-causality-and-statistics-1dfa4ce1b465&user=Pirmin+Lemberger&userId=aa1fd8f69eaf&source=-----1dfa4ce1b465---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1dfa4ce1b465&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freconciling-causality-and-statistics-1dfa4ce1b465&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/@pirminlemberger?source=post_page-----1dfa4ce1b465--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1dfa4ce1b465--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Faa1fd8f69eaf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freconciling-causality-and-statistics-1dfa4ce1b465&user=Pirmin+Lemberger&userId=aa1fd8f69eaf&source=post_page-aa1fd8f69eaf----1dfa4ce1b465---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F4e1867e29c1b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freconciling-causality-and-statistics-1dfa4ce1b465&newsletterV3=aa1fd8f69eaf&newsletterV3Id=4e1867e29c1b&user=Pirmin+Lemberger&userId=aa1fd8f69eaf&source=-----1dfa4ce1b465---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/@pirminlemberger?source=post_page-----1dfa4ce1b465--------------------------------", "anchor_text": "Written by Pirmin Lemberger"}, {"url": "https://medium.com/@pirminlemberger/followers?source=post_page-----1dfa4ce1b465--------------------------------", "anchor_text": "86 Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----1dfa4ce1b465--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Faa1fd8f69eaf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freconciling-causality-and-statistics-1dfa4ce1b465&user=Pirmin+Lemberger&userId=aa1fd8f69eaf&source=post_page-aa1fd8f69eaf----1dfa4ce1b465---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F4e1867e29c1b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freconciling-causality-and-statistics-1dfa4ce1b465&newsletterV3=aa1fd8f69eaf&newsletterV3Id=4e1867e29c1b&user=Pirmin+Lemberger&userId=aa1fd8f69eaf&source=-----1dfa4ce1b465---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/perplexity-of-language-models-revisited-6b9b4cf46792?source=author_recirc-----1dfa4ce1b465----0---------------------8fe35ca0_997e_4eea_a3ba_e91f12b80477-------", "anchor_text": ""}, {"url": "https://medium.com/@pirminlemberger?source=author_recirc-----1dfa4ce1b465----0---------------------8fe35ca0_997e_4eea_a3ba_e91f12b80477-------", "anchor_text": ""}, {"url": "https://medium.com/@pirminlemberger?source=author_recirc-----1dfa4ce1b465----0---------------------8fe35ca0_997e_4eea_a3ba_e91f12b80477-------", "anchor_text": "Pirmin Lemberger"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----1dfa4ce1b465----0---------------------8fe35ca0_997e_4eea_a3ba_e91f12b80477-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/perplexity-of-language-models-revisited-6b9b4cf46792?source=author_recirc-----1dfa4ce1b465----0---------------------8fe35ca0_997e_4eea_a3ba_e91f12b80477-------", "anchor_text": "Perplexity of language models revisitedFrom basic information theory to practical computation"}, {"url": "https://towardsdatascience.com/perplexity-of-language-models-revisited-6b9b4cf46792?source=author_recirc-----1dfa4ce1b465----0---------------------8fe35ca0_997e_4eea_a3ba_e91f12b80477-------", "anchor_text": "12 min read\u00b7Jun 28, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6b9b4cf46792&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fperplexity-of-language-models-revisited-6b9b4cf46792&user=Pirmin+Lemberger&userId=aa1fd8f69eaf&source=-----6b9b4cf46792----0-----------------clap_footer----8fe35ca0_997e_4eea_a3ba_e91f12b80477-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/perplexity-of-language-models-revisited-6b9b4cf46792?source=author_recirc-----1dfa4ce1b465----0---------------------8fe35ca0_997e_4eea_a3ba_e91f12b80477-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6b9b4cf46792&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fperplexity-of-language-models-revisited-6b9b4cf46792&source=-----1dfa4ce1b465----0-----------------bookmark_preview----8fe35ca0_997e_4eea_a3ba_e91f12b80477-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----1dfa4ce1b465----1---------------------8fe35ca0_997e_4eea_a3ba_e91f12b80477-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----1dfa4ce1b465----1---------------------8fe35ca0_997e_4eea_a3ba_e91f12b80477-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----1dfa4ce1b465----1---------------------8fe35ca0_997e_4eea_a3ba_e91f12b80477-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----1dfa4ce1b465----1---------------------8fe35ca0_997e_4eea_a3ba_e91f12b80477-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----1dfa4ce1b465----1---------------------8fe35ca0_997e_4eea_a3ba_e91f12b80477-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----1dfa4ce1b465----1---------------------8fe35ca0_997e_4eea_a3ba_e91f12b80477-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----8fe35ca0_997e_4eea_a3ba_e91f12b80477-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----1dfa4ce1b465----1---------------------8fe35ca0_997e_4eea_a3ba_e91f12b80477-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----1dfa4ce1b465----1-----------------bookmark_preview----8fe35ca0_997e_4eea_a3ba_e91f12b80477-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----1dfa4ce1b465----2---------------------8fe35ca0_997e_4eea_a3ba_e91f12b80477-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----1dfa4ce1b465----2---------------------8fe35ca0_997e_4eea_a3ba_e91f12b80477-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----1dfa4ce1b465----2---------------------8fe35ca0_997e_4eea_a3ba_e91f12b80477-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----1dfa4ce1b465----2---------------------8fe35ca0_997e_4eea_a3ba_e91f12b80477-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----1dfa4ce1b465----2---------------------8fe35ca0_997e_4eea_a3ba_e91f12b80477-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----1dfa4ce1b465----2---------------------8fe35ca0_997e_4eea_a3ba_e91f12b80477-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----2-----------------clap_footer----8fe35ca0_997e_4eea_a3ba_e91f12b80477-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----1dfa4ce1b465----2---------------------8fe35ca0_997e_4eea_a3ba_e91f12b80477-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----1dfa4ce1b465----2-----------------bookmark_preview----8fe35ca0_997e_4eea_a3ba_e91f12b80477-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/deep-learning-models-for-automatic-summarization-4c2b89f2a9ea?source=author_recirc-----1dfa4ce1b465----3---------------------8fe35ca0_997e_4eea_a3ba_e91f12b80477-------", "anchor_text": ""}, {"url": "https://medium.com/@pirminlemberger?source=author_recirc-----1dfa4ce1b465----3---------------------8fe35ca0_997e_4eea_a3ba_e91f12b80477-------", "anchor_text": ""}, {"url": "https://medium.com/@pirminlemberger?source=author_recirc-----1dfa4ce1b465----3---------------------8fe35ca0_997e_4eea_a3ba_e91f12b80477-------", "anchor_text": "Pirmin Lemberger"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----1dfa4ce1b465----3---------------------8fe35ca0_997e_4eea_a3ba_e91f12b80477-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/deep-learning-models-for-automatic-summarization-4c2b89f2a9ea?source=author_recirc-----1dfa4ce1b465----3---------------------8fe35ca0_997e_4eea_a3ba_e91f12b80477-------", "anchor_text": "Deep Learning Models for Automatic SummarizationThe Next Big Thing in NLP?"}, {"url": "https://towardsdatascience.com/deep-learning-models-for-automatic-summarization-4c2b89f2a9ea?source=author_recirc-----1dfa4ce1b465----3---------------------8fe35ca0_997e_4eea_a3ba_e91f12b80477-------", "anchor_text": "\u00b716 min read\u00b7May 13, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4c2b89f2a9ea&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-models-for-automatic-summarization-4c2b89f2a9ea&user=Pirmin+Lemberger&userId=aa1fd8f69eaf&source=-----4c2b89f2a9ea----3-----------------clap_footer----8fe35ca0_997e_4eea_a3ba_e91f12b80477-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/deep-learning-models-for-automatic-summarization-4c2b89f2a9ea?source=author_recirc-----1dfa4ce1b465----3---------------------8fe35ca0_997e_4eea_a3ba_e91f12b80477-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4c2b89f2a9ea&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-models-for-automatic-summarization-4c2b89f2a9ea&source=-----1dfa4ce1b465----3-----------------bookmark_preview----8fe35ca0_997e_4eea_a3ba_e91f12b80477-------", "anchor_text": ""}, {"url": "https://medium.com/@pirminlemberger?source=post_page-----1dfa4ce1b465--------------------------------", "anchor_text": "See all from Pirmin Lemberger"}, {"url": "https://towardsdatascience.com/?source=post_page-----1dfa4ce1b465--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://james-thorn.medium.com/probability-vs-statistics-28fbc1ad9696?source=read_next_recirc-----1dfa4ce1b465----0---------------------9c58fa38_f0fd_412b_8ec3_846e0df88096-------", "anchor_text": ""}, {"url": "https://james-thorn.medium.com/?source=read_next_recirc-----1dfa4ce1b465----0---------------------9c58fa38_f0fd_412b_8ec3_846e0df88096-------", "anchor_text": ""}, {"url": "https://james-thorn.medium.com/?source=read_next_recirc-----1dfa4ce1b465----0---------------------9c58fa38_f0fd_412b_8ec3_846e0df88096-------", "anchor_text": "James Thorn"}, {"url": "https://james-thorn.medium.com/probability-vs-statistics-28fbc1ad9696?source=read_next_recirc-----1dfa4ce1b465----0---------------------9c58fa38_f0fd_412b_8ec3_846e0df88096-------", "anchor_text": "Probability vs StatisticsProbability, statistics, all familiar names to the Data Scientist. But if you had to explain what each of them is, would you be able to do\u2026"}, {"url": "https://james-thorn.medium.com/probability-vs-statistics-28fbc1ad9696?source=read_next_recirc-----1dfa4ce1b465----0---------------------9c58fa38_f0fd_412b_8ec3_846e0df88096-------", "anchor_text": "\u00b75 min read\u00b7Dec 17, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F28fbc1ad9696&operation=register&redirect=https%3A%2F%2Fjames-thorn.medium.com%2Fprobability-vs-statistics-28fbc1ad9696&user=James+Thorn&userId=1fd70d25ff14&source=-----28fbc1ad9696----0-----------------clap_footer----9c58fa38_f0fd_412b_8ec3_846e0df88096-------", "anchor_text": ""}, {"url": "https://james-thorn.medium.com/probability-vs-statistics-28fbc1ad9696?source=read_next_recirc-----1dfa4ce1b465----0---------------------9c58fa38_f0fd_412b_8ec3_846e0df88096-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F28fbc1ad9696&operation=register&redirect=https%3A%2F%2Fjames-thorn.medium.com%2Fprobability-vs-statistics-28fbc1ad9696&source=-----1dfa4ce1b465----0-----------------bookmark_preview----9c58fa38_f0fd_412b_8ec3_846e0df88096-------", "anchor_text": ""}, {"url": "https://leihua-ye.medium.com/why-data-scientists-should-learn-causal-inference-a70c4ffb4809?source=read_next_recirc-----1dfa4ce1b465----1---------------------9c58fa38_f0fd_412b_8ec3_846e0df88096-------", "anchor_text": ""}, {"url": "https://leihua-ye.medium.com/?source=read_next_recirc-----1dfa4ce1b465----1---------------------9c58fa38_f0fd_412b_8ec3_846e0df88096-------", "anchor_text": ""}, {"url": "https://leihua-ye.medium.com/?source=read_next_recirc-----1dfa4ce1b465----1---------------------9c58fa38_f0fd_412b_8ec3_846e0df88096-------", "anchor_text": "Leihua Ye, PhD"}, {"url": "https://leihua-ye.medium.com/why-data-scientists-should-learn-causal-inference-a70c4ffb4809?source=read_next_recirc-----1dfa4ce1b465----1---------------------9c58fa38_f0fd_412b_8ec3_846e0df88096-------", "anchor_text": "Why Data Scientists Should Learn Causal InferenceClimb up the ladder of causation"}, {"url": "https://leihua-ye.medium.com/why-data-scientists-should-learn-causal-inference-a70c4ffb4809?source=read_next_recirc-----1dfa4ce1b465----1---------------------9c58fa38_f0fd_412b_8ec3_846e0df88096-------", "anchor_text": "\u00b77 min read\u00b7Jul 5, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2Fa70c4ffb4809&operation=register&redirect=https%3A%2F%2Fleihua-ye.medium.com%2Fwhy-data-scientists-should-learn-causal-inference-a70c4ffb4809&user=Leihua+Ye%2C+PhD&userId=4e1d06dd743&source=-----a70c4ffb4809----1-----------------clap_footer----9c58fa38_f0fd_412b_8ec3_846e0df88096-------", "anchor_text": ""}, {"url": "https://leihua-ye.medium.com/why-data-scientists-should-learn-causal-inference-a70c4ffb4809?source=read_next_recirc-----1dfa4ce1b465----1---------------------9c58fa38_f0fd_412b_8ec3_846e0df88096-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "9"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa70c4ffb4809&operation=register&redirect=https%3A%2F%2Fleihua-ye.medium.com%2Fwhy-data-scientists-should-learn-causal-inference-a70c4ffb4809&source=-----1dfa4ce1b465----1-----------------bookmark_preview----9c58fa38_f0fd_412b_8ec3_846e0df88096-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/from-data-to-clusters-when-is-your-clustering-good-enough-5895440a978a?source=read_next_recirc-----1dfa4ce1b465----0---------------------9c58fa38_f0fd_412b_8ec3_846e0df88096-------", "anchor_text": ""}, {"url": "https://erdogant.medium.com/?source=read_next_recirc-----1dfa4ce1b465----0---------------------9c58fa38_f0fd_412b_8ec3_846e0df88096-------", "anchor_text": ""}, {"url": "https://erdogant.medium.com/?source=read_next_recirc-----1dfa4ce1b465----0---------------------9c58fa38_f0fd_412b_8ec3_846e0df88096-------", "anchor_text": "Erdogan Taskesen"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----1dfa4ce1b465----0---------------------9c58fa38_f0fd_412b_8ec3_846e0df88096-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/from-data-to-clusters-when-is-your-clustering-good-enough-5895440a978a?source=read_next_recirc-----1dfa4ce1b465----0---------------------9c58fa38_f0fd_412b_8ec3_846e0df88096-------", "anchor_text": "From Data to Clusters; When is Your Clustering Good Enough?Sensible clusters and hidden gems can be found using clustering approaches but you need the right cluster evaluation method!"}, {"url": "https://towardsdatascience.com/from-data-to-clusters-when-is-your-clustering-good-enough-5895440a978a?source=read_next_recirc-----1dfa4ce1b465----0---------------------9c58fa38_f0fd_412b_8ec3_846e0df88096-------", "anchor_text": "\u00b717 min read\u00b76 days ago"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5895440a978a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-data-to-clusters-when-is-your-clustering-good-enough-5895440a978a&user=Erdogan+Taskesen&userId=4e636e2ef813&source=-----5895440a978a----0-----------------clap_footer----9c58fa38_f0fd_412b_8ec3_846e0df88096-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/from-data-to-clusters-when-is-your-clustering-good-enough-5895440a978a?source=read_next_recirc-----1dfa4ce1b465----0---------------------9c58fa38_f0fd_412b_8ec3_846e0df88096-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5895440a978a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-data-to-clusters-when-is-your-clustering-good-enough-5895440a978a&source=-----1dfa4ce1b465----0-----------------bookmark_preview----9c58fa38_f0fd_412b_8ec3_846e0df88096-------", "anchor_text": ""}, {"url": "https://shawhin.medium.com/understanding-causal-effects-37a054b2ec3b?source=read_next_recirc-----1dfa4ce1b465----1---------------------9c58fa38_f0fd_412b_8ec3_846e0df88096-------", "anchor_text": ""}, {"url": "https://shawhin.medium.com/?source=read_next_recirc-----1dfa4ce1b465----1---------------------9c58fa38_f0fd_412b_8ec3_846e0df88096-------", "anchor_text": ""}, {"url": "https://shawhin.medium.com/?source=read_next_recirc-----1dfa4ce1b465----1---------------------9c58fa38_f0fd_412b_8ec3_846e0df88096-------", "anchor_text": "Shawhin Talebi"}, {"url": "https://shawhin.medium.com/understanding-causal-effects-37a054b2ec3b?source=read_next_recirc-----1dfa4ce1b465----1---------------------9c58fa38_f0fd_412b_8ec3_846e0df88096-------", "anchor_text": "Understanding Causal EffectsReview and Summary of a 5-part Series"}, {"url": "https://shawhin.medium.com/understanding-causal-effects-37a054b2ec3b?source=read_next_recirc-----1dfa4ce1b465----1---------------------9c58fa38_f0fd_412b_8ec3_846e0df88096-------", "anchor_text": "\u00b75 min read\u00b7Jan 21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F37a054b2ec3b&operation=register&redirect=https%3A%2F%2Fshawhin.medium.com%2Funderstanding-causal-effects-37a054b2ec3b&user=Shawhin+Talebi&userId=f3998e1cd186&source=-----37a054b2ec3b----1-----------------clap_footer----9c58fa38_f0fd_412b_8ec3_846e0df88096-------", "anchor_text": ""}, {"url": "https://shawhin.medium.com/understanding-causal-effects-37a054b2ec3b?source=read_next_recirc-----1dfa4ce1b465----1---------------------9c58fa38_f0fd_412b_8ec3_846e0df88096-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F37a054b2ec3b&operation=register&redirect=https%3A%2F%2Fshawhin.medium.com%2Funderstanding-causal-effects-37a054b2ec3b&source=-----1dfa4ce1b465----1-----------------bookmark_preview----9c58fa38_f0fd_412b_8ec3_846e0df88096-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/frequentist-vs-bayesian-statistics-with-tensorflow-fbba2c6c9ae5?source=read_next_recirc-----1dfa4ce1b465----2---------------------9c58fa38_f0fd_412b_8ec3_846e0df88096-------", "anchor_text": ""}, {"url": "https://medium.com/@luisroque?source=read_next_recirc-----1dfa4ce1b465----2---------------------9c58fa38_f0fd_412b_8ec3_846e0df88096-------", "anchor_text": ""}, {"url": "https://medium.com/@luisroque?source=read_next_recirc-----1dfa4ce1b465----2---------------------9c58fa38_f0fd_412b_8ec3_846e0df88096-------", "anchor_text": "Lu\u00eds Roque"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----1dfa4ce1b465----2---------------------9c58fa38_f0fd_412b_8ec3_846e0df88096-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/frequentist-vs-bayesian-statistics-with-tensorflow-fbba2c6c9ae5?source=read_next_recirc-----1dfa4ce1b465----2---------------------9c58fa38_f0fd_412b_8ec3_846e0df88096-------", "anchor_text": "Frequentist vs. Bayesian Statistics with TensorflowProbabilistic deep learning"}, {"url": "https://towardsdatascience.com/frequentist-vs-bayesian-statistics-with-tensorflow-fbba2c6c9ae5?source=read_next_recirc-----1dfa4ce1b465----2---------------------9c58fa38_f0fd_412b_8ec3_846e0df88096-------", "anchor_text": "\u00b710 min read\u00b7Jan 5"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffbba2c6c9ae5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrequentist-vs-bayesian-statistics-with-tensorflow-fbba2c6c9ae5&user=Lu%C3%ADs+Roque&userId=2195f049db86&source=-----fbba2c6c9ae5----2-----------------clap_footer----9c58fa38_f0fd_412b_8ec3_846e0df88096-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/frequentist-vs-bayesian-statistics-with-tensorflow-fbba2c6c9ae5?source=read_next_recirc-----1dfa4ce1b465----2---------------------9c58fa38_f0fd_412b_8ec3_846e0df88096-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffbba2c6c9ae5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrequentist-vs-bayesian-statistics-with-tensorflow-fbba2c6c9ae5&source=-----1dfa4ce1b465----2-----------------bookmark_preview----9c58fa38_f0fd_412b_8ec3_846e0df88096-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-type-i-error-confidence-intervals-type-ii-error-and-power-are-all-related-579a16e2c0e3?source=read_next_recirc-----1dfa4ce1b465----3---------------------9c58fa38_f0fd_412b_8ec3_846e0df88096-------", "anchor_text": ""}, {"url": "https://medium.com/@andreasmartinson?source=read_next_recirc-----1dfa4ce1b465----3---------------------9c58fa38_f0fd_412b_8ec3_846e0df88096-------", "anchor_text": ""}, {"url": "https://medium.com/@andreasmartinson?source=read_next_recirc-----1dfa4ce1b465----3---------------------9c58fa38_f0fd_412b_8ec3_846e0df88096-------", "anchor_text": "Andreas Martinson"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----1dfa4ce1b465----3---------------------9c58fa38_f0fd_412b_8ec3_846e0df88096-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/how-type-i-error-confidence-intervals-type-ii-error-and-power-are-all-related-579a16e2c0e3?source=read_next_recirc-----1dfa4ce1b465----3---------------------9c58fa38_f0fd_412b_8ec3_846e0df88096-------", "anchor_text": "How Type I Error, Confidence Intervals, Type II Error, and Power Are All RelatedComparing normal distributions and their metrics"}, {"url": "https://towardsdatascience.com/how-type-i-error-confidence-intervals-type-ii-error-and-power-are-all-related-579a16e2c0e3?source=read_next_recirc-----1dfa4ce1b465----3---------------------9c58fa38_f0fd_412b_8ec3_846e0df88096-------", "anchor_text": "\u00b710 min read\u00b7Nov 30, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F579a16e2c0e3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-type-i-error-confidence-intervals-type-ii-error-and-power-are-all-related-579a16e2c0e3&user=Andreas+Martinson&userId=f6366993e3b5&source=-----579a16e2c0e3----3-----------------clap_footer----9c58fa38_f0fd_412b_8ec3_846e0df88096-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-type-i-error-confidence-intervals-type-ii-error-and-power-are-all-related-579a16e2c0e3?source=read_next_recirc-----1dfa4ce1b465----3---------------------9c58fa38_f0fd_412b_8ec3_846e0df88096-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F579a16e2c0e3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-type-i-error-confidence-intervals-type-ii-error-and-power-are-all-related-579a16e2c0e3&source=-----1dfa4ce1b465----3-----------------bookmark_preview----9c58fa38_f0fd_412b_8ec3_846e0df88096-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----1dfa4ce1b465--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----1dfa4ce1b465--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----1dfa4ce1b465--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----1dfa4ce1b465--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----1dfa4ce1b465--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----1dfa4ce1b465--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----1dfa4ce1b465--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----1dfa4ce1b465--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----1dfa4ce1b465--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----1dfa4ce1b465--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}