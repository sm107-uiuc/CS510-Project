{"url": "https://towardsdatascience.com/the-mathematics-behind-deep-learning-f6c35a0fe077", "time": 1683013717.6871119, "path": "towardsdatascience.com/the-mathematics-behind-deep-learning-f6c35a0fe077/", "webpage": {"metadata": {"title": "The Mathematics Behind Deep Learning | by Trist'n Joseph | Towards Data Science", "h1": "The Mathematics Behind Deep Learning", "description": "Deep neural networks (DNNs) are essentially formed by having multiple connected perceptrons, where a perceptron is a single neuron. Think of an artificial neural network (ANN) as a system which\u2026"}, "outgoing_paragraph_urls": [], "all_paragraphs": ["Deep neural networks (DNNs) are essentially formed by having multiple connected perceptrons, where a perceptron is a single neuron. Think of an artificial neural network (ANN) as a system which contains a set of inputs that are fed along weighted paths. These inputs are then processed, and an output is produced to perform some task. Over time, the ANN \u2018learns\u2019, and different paths are developed. Various paths can have different weightings, and paths that are found to be more important (or produce more desirable results) are assigned higher weightings within the model than those which produce fewer desirable results.", "Within a DNN, if all the inputs are densely connected to all the outputs, then these layers are referred to as dense layers. Additionally, DNNs can contain multiple hidden layers. A hidden layer is basically the point between the input and output of the neural network, where the activation function does a transformation on the information being fed in. It is referred to as a hidden layer because it is not directly observable from the system\u2019s inputs and outputs. The deeper the neural network, the more the network can recognize from data.", "However, although learning as much as possible from the data is the goal, deep learning models can suffer from overfitting. This occurs when a model learns too much from the training data, including random noise. Models are then able to determine very intricate patterns within the data, but this negatively affects the performance on new data. The noise picked up in the training data does not apply to new or unseen data, and the model is unable to generalize the patterns found. Non-linearity is also of high importance in deep learning models. Although the model will learn a lot from having multiple hidden layers, applying linear forms to non-linear problems will result in poor performance.", "The question now comes, \u201chow do these layers learn things?\u201d Well, let us apply an ANN to a real scenario to solve a problem and understand how the model would be trained to accomplish its goal. With the current pandemic, many schools have transitioned to virtual learning, and this has caused some students to be concerned about their chances of passing their courses. The \u2018will I pass this class\u2019 problem is one that any artificial intelligence system should be able to solve.", "For simplicity, let us consider that this model only has 3 inputs: the number of lectures the student attended, the amount of time spent on assignments, and the number of times that internet connection was lost throughout lectures. The output of this model will be a binary classification; either the student passes the course or they do not. It is now the end of the semester and student A has attended 21 lectures, spent 90 hours on assignments, and lost internet connection 7 times over the semester. These inputs are fed into the model, and the output predicts that the student has a 5% chance of passing the course. A week later, final grades are released, and student A passed the course. So, what went wrong with the model\u2019s prediction?", "Technically, nothing went wrong. The model would have worked as it was currently developed to work. The issue is that the model has no idea what is going on. We would have just initialized some weights on the pathways, but the model currently does not know what is right from wrong; and thus, the weights are incorrect. This is where the learning comes in. The idea is that the model needs to understand when it is wrong, and we do this by calculating some form of \u2018loss\u2019. The loss being calculated is dependent on the problem at hand, but it typically involves minimizing the discrepancy between the predicted output and the actual output.", "In the scenario presented above, there is only one student and one point of error to minimize. However, this is typically not the case. Now, consider that there are multiple students and multiple discrepancies to minimize. The overall loss, then, would typically be calculated as the average of the differences between all predictions and actual observations.", "Recall that the loss being calculated is dependent on the problem at hand. Therefore, since our current problem is a binary classification, an appropriate loss calculation would be a cross-entropy loss. The idea behind this function is that it compares the predicted distribution of whether a student will pass the course to the actual distribution, and attempts to minimize the differences between these distributions.", "Suppose instead that we no longer want to predict whether the student will pass the class, but we now want to predict the grade that they will get for the class. The cross-entropy loss would no longer be an appropriate method. Rather, the mean squared error loss would be more appropriate. This method is suitable for a regression problem, and the idea is that it will try to minimize the squared difference between the actual value and a predicted value.", "Now that we understand some loss functions, we can get into loss optimization and model training. A key factor in having good DNNs is having appropriate weights. The loss optimization should attempt to find a set of weights, W, that will minimize the calculated loss. If there is only one weight component, then it is possible to plot the weight and the loss on a 2-D graph and then choose the weight which minimizes the loss. However, most DNNs have multiple weight components, and visualizing an n-dimensional graph is quite hard.", "Instead, the derivate of the loss function with respect to all the weights is calculated to determine the direction of maximum ascent. Now that the model understands which way is up and down, it travels downwards until it reaches a point of convergence at a local minimum. Once this decent is completed, a set of optimal weights will be returned, and this is what should be used for the DNN (assuming that the model was well developed).", "The process of calculating this derivative is known as back propagation, and it essentially the chain rule from calculus. Consider the neural network shown above, how does a small change in the first set of weights affect the final loss? This is what the derivative, or gradient, seeks to explain. But, the first set of weights are fed into a hidden layer, which then has another set of weights leading to the predicted output and the loss. So, the effect that a change in weights has on the hidden layer should also be considered. Now, these were the only two parts within the network. But, if there are more weights to consider, this process would be continued by applying the chain rule from output to input.", "Another important factor to consider when training a DNN is the learning rate. As the model travels to find an optimal set of weights, it needs to update its the weights by some factor. Although this could seem trivial, determining factor by which the model should move is quite difficult. If the factor is too small, then the model can either run for an exponentially long period of time or get trapped somewhere that\u2019s not the global minimum. If the factor is too large, then the model might completely miss the target point and then diverge.", "Although a fixed rate can be ideal, an adaptive learning rate reduces the chances of the problems previously mentioned. That is, the factor will change depending on the current gradient, the size of the current weights, or some other thing that can affect where the model should go next to find the optimal weights.", "As can be seen, DNNs are built on calculus and some statistics. Evaluating the mathematics behind these processes is useful because it can help one understand what is truly happening within the model, and this can lead to developing better models overall. But, even if the concepts are not easily understood, most programs come with tools such as automatic differentiation, so no worries. Happy coding!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Scientist? Yes. Researcher? Somewhat. Content creator? Sure, why not."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Ff6c35a0fe077&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-mathematics-behind-deep-learning-f6c35a0fe077&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-mathematics-behind-deep-learning-f6c35a0fe077&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-mathematics-behind-deep-learning-f6c35a0fe077&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-mathematics-behind-deep-learning-f6c35a0fe077&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----f6c35a0fe077--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f6c35a0fe077--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://trisxcjoseph.medium.com/?source=post_page-----f6c35a0fe077--------------------------------", "anchor_text": ""}, {"url": "https://trisxcjoseph.medium.com/?source=post_page-----f6c35a0fe077--------------------------------", "anchor_text": "Trist'n Joseph"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F32920ce9f4b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-mathematics-behind-deep-learning-f6c35a0fe077&user=Trist%27n+Joseph&userId=32920ce9f4b&source=post_page-32920ce9f4b----f6c35a0fe077---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff6c35a0fe077&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-mathematics-behind-deep-learning-f6c35a0fe077&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff6c35a0fe077&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-mathematics-behind-deep-learning-f6c35a0fe077&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.digitaltrends.com/cool-tech/what-is-an-artificial-neural-network/", "anchor_text": "digitaltrends.com/cool-tech/what-is-an-artificial-neural-network/"}, {"url": "https://deepai.org/machine-learning-glossary-and-terms/hidden-layer-machine-learning#:~:text=In%20neural%20networks%2C%20a%20hidden,inputs%20entered%20into%20the%20network.", "anchor_text": "deepai.org/machine-learning-glossary-and-terms/hidden-layer-machine-learning#:~:text=In%20neural%20networks%2C%20a%20hidden,inputs%20entered%20into%20the%20network."}, {"url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4960264/", "anchor_text": "ncbi.nlm.nih.gov/pmc/articles/PMC4960264/"}, {"url": "https://towardsdatascience.com/introduction-to-artificial-neural-networks-ann-1aea15775ef9", "anchor_text": "towardsdatascience.com/introduction-to-artificial-neural-networks-ann-1aea15775ef9"}, {"url": "https://www.explainthatstuff.com/introduction-to-neural-networks.html", "anchor_text": "explainthatstuff.com/introduction-to-neural-networks.html"}, {"url": "http://neuralnetworksanddeeplearning.com/", "anchor_text": "neuralnetworksanddeeplearning.com/"}, {"url": "https://www.mathsisfun.com/calculus/derivatives-rules.html", "anchor_text": "mathsisfun.com/calculus/derivatives-rules.html"}, {"url": "https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/multivariable-calculus.html", "anchor_text": "d2l.ai/chapter_appendix-mathematics-for-deep-learning/multivariable-calculus.html"}, {"url": "https://deeplearning.mit.edu/", "anchor_text": "deeplearning.mit.edu/"}, {"url": "https://www.math.ucdavis.edu/~kouba/CalcOneDIRECTORY/chainruledirectory/ChainRule.html", "anchor_text": "math.ucdavis.edu/~kouba/CalcOneDIRECTORY/chainruledirectory/ChainRule.html"}, {"url": "https://www.youtube.com/watch?v=tGVnBAHLApA", "anchor_text": "youtube.com/watch?v=tGVnBAHLApA"}, {"url": "https://www.inertia7.com/tristn", "anchor_text": "https://www.inertia7.com/tristn"}, {"url": "https://www.youtube.com/watch?v=aircAruvnKk", "anchor_text": "youtube.com/watch?v=aircAruvnKk"}, {"url": "https://www.youtube.com/watch?v=bfmFfD2RIcg", "anchor_text": "youtube.com/watch?v=bfmFfD2RIcg"}, {"url": "https://towardsdatascience.com/what-is-deep-learning-adf5d4de9afc", "anchor_text": "https://towardsdatascience.com/what-is-deep-learning-adf5d4de9afc"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----f6c35a0fe077---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----f6c35a0fe077---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/mathematics?source=post_page-----f6c35a0fe077---------------mathematics-----------------", "anchor_text": "Mathematics"}, {"url": "https://medium.com/tag/data-science?source=post_page-----f6c35a0fe077---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----f6c35a0fe077---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff6c35a0fe077&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-mathematics-behind-deep-learning-f6c35a0fe077&user=Trist%27n+Joseph&userId=32920ce9f4b&source=-----f6c35a0fe077---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff6c35a0fe077&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-mathematics-behind-deep-learning-f6c35a0fe077&user=Trist%27n+Joseph&userId=32920ce9f4b&source=-----f6c35a0fe077---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff6c35a0fe077&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-mathematics-behind-deep-learning-f6c35a0fe077&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f6c35a0fe077--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Ff6c35a0fe077&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-mathematics-behind-deep-learning-f6c35a0fe077&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----f6c35a0fe077---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----f6c35a0fe077--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----f6c35a0fe077--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----f6c35a0fe077--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----f6c35a0fe077--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----f6c35a0fe077--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----f6c35a0fe077--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----f6c35a0fe077--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----f6c35a0fe077--------------------------------", "anchor_text": ""}, {"url": "https://trisxcjoseph.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://trisxcjoseph.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Trist'n Joseph"}, {"url": "https://trisxcjoseph.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "385 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F32920ce9f4b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-mathematics-behind-deep-learning-f6c35a0fe077&user=Trist%27n+Joseph&userId=32920ce9f4b&source=post_page-32920ce9f4b--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fcfd73cacf13a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-mathematics-behind-deep-learning-f6c35a0fe077&newsletterV3=32920ce9f4b&newsletterV3Id=cfd73cacf13a&user=Trist%27n+Joseph&userId=32920ce9f4b&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}