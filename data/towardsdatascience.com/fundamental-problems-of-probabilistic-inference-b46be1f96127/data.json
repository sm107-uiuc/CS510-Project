{"url": "https://towardsdatascience.com/fundamental-problems-of-probabilistic-inference-b46be1f96127", "time": 1683013128.373848, "path": "towardsdatascience.com/fundamental-problems-of-probabilistic-inference-b46be1f96127/", "webpage": {"metadata": {"title": "Fundamental Problems of Probabilistic Inference | by Marin Vlastelica | Towards Data Science", "h1": "Fundamental Problems of Probabilistic Inference", "description": "By now, I know many people that do research in ML or play around with machine learning algorithms. Yet, most of them somehow don\u2019t appreciate the fundamental problems that machine learning is built\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/what-is-the-information-in-information-theory-d916250e4899", "anchor_text": "What is the \u201cInformation\u201d in Information Theory", "paragraph_index": 17}, {"url": "https://jimimvp.github.io/", "anchor_text": "https://jimimvp.github.io/", "paragraph_index": 19}, {"url": "https://www.linkedin.com/in/mvlastelica/", "anchor_text": "https://www.linkedin.com/in/mvlastelica/", "paragraph_index": 19}], "all_paragraphs": ["By now, I know many people that do research in ML or play around with machine learning algorithms. Yet, most of them somehow don\u2019t appreciate the fundamental problems that machine learning is built upon, the problems of probabilistic inference. The point of this article is to maybe turn your attention to questions that you might not have considered when coding machine learning algorithms.", "Why do we talk about probability in the first place? Where does the randomness come from? Is there such a thing as random variables really? In the end, we want to predict something relatively concrete, a class label given an image, an optimal action given some kind of state description in a Markov Decision Process. Arguably, there is nothing random about these things. An object is not really with some probability assigned a class label, in a random sense. A cow is not maybe a cow, it is certainly a cow.", "On the other hand, we have problems such as different flavors of unsupervised learning, where we might want to reduce dimensionality of the data, cluster the data, learn a generative model that reflects the probability distribution of the data. All of these flavors can be expressed in terms of probability. But again, assigning a latent low-dimensional representation to a data point is not truly random. We want to map directly an input data point to a latent representation (a cluster, a latent variable of lower dimension).", "So, why do we use randomness and probabilities in the end? The main argument is that there is no randomness. What we actually talk about when we talk about probability distributions, densities in the machine learning world. We actually talk about information or uncertainty. A probability measure reflects how much information do we have about a given event. Let\u2019s look at a supervised example, an image containing a cow. When I say, the probability of the image containing a cow is 0.9. This doesn\u2019t mean that the cow is there sometimes, it means that I am 90% certain that a cow is in this image. Perhaps because the image doesn\u2019t have enough information to determine fully that it contains a cow, or perhaps my model is wrong\u2026 Who knows.", "Where does the randomness come in? The punchline is that there is no randomness. Let\u2019s talk about prior distributions for a moment. What does it mean that I have a prior distribution over my model parameters, or in other words hypothesis space? That means that I am more certain about particular configurations, particular hypotheses than others. It doesn\u2019t mean that the model is random, that it can change based on an underlying stochastic process.", "Our goal when we do machine learning is mostly (in different shapes and forms) the following, stated in the most general sense. Given some kind of data, we want to infer some parameters of the model that best describes the data. If you want to be Bayesian, then you will adhere to these concepts of uncertainty. The following is the Bayes rule given data D to infer the model parameters \u03b8.", "p(\u03b8) we name the prior, p(D|\u03b8) the likelihood, and the LHS (left-hand side) is the posterior. The posterior simply expresses how certain we are about the parameters. In the denominator is the marginal over D. Note that we don\u2019t magically have access to this marginal, we need to somehow evaluate it. To come to an integral formulation of the denominator we can marginalize out the \u03b8 from the joint distribution of \u03b8 and D. In that case, we have the following formulation of the Bayes rule:", "Equivalently, we can separate the term in the denominator in the integral such that it contains the likelihood and the prior over \u03b8.", "Why is this a hard problem? ML practitioners say that evaluating this integral is hard, but it may be sometimes tricky to understand why it is hard. First of all, the \u03b8 in the denominator is not the same one as in the numerator. The integral basically means that we sum the joint distribution over all possible parameters \u03b8. Now, take that this \u03b8 are the parameters of a neural network, and they are real numbers. There exist infinite configurations of parameters to evaluate in the denominator. This is clearly intractable. Even if we consider the real numbers to be limited to 32-bit precision, the number of possible configurations is exponential in the number of bits used for storing the model. Furthermore, the process of evaluating the integral involves sampling, clearly, the better our sampling, the better we can expect our estimate to be.", "As you read on, it is useful to keep the following in mind:", "Computational complexity is important in the case of estimation and sampling.", "Nevertheless, the situation is not hopeless. Although some integrals cannot be evaluated, we can still estimate them. There are different ways of estimating an integral, one of which is (the most basic one) Monte Carlo Sampling (notice the Sampling part of the name). So what we can do is to draw a certain number of parameters \u03b8 and evaluate the function within the integral (note that we assume that we have access to the function to evaluate it), which looks like this:", "This is a relatively simple estimator and it is unbiased, meaning that it is correct and yields the exact correct result with an infinite number of samples. However, it does come with a drawback.", "It can be shown that for the Monte-Carlo estimator the error of the estimate drops asymptotically with the square root of the number of samples.", "This means that, if we want a good ball-park estimate of the target value, it might be good enough. But if we want a high precision estimate, there exist perhaps better options with faster convergence properties. Additionally, we might not like sampling from the distribution of the parameters that often, since sampling may be expensive.", "Till now, we have talked about the problem of evaluating the denominator in the Bayes rule to calculate the posterior, . In the Monte-Carlo estimate, we need to draw samples of \u03b8 in order to estimate the integral. But how does the distribution over \u03b8 exactly look like? Have you ever wondered how does sampling actually work, how do we arrive at a \u201crandom\u201d number when using a Python or R library? Even if I give you a sampler capable of sampling from a uniform distribution, how would you use it in order to sample from more complicated distributions? As it turns out, the problem of sampling is a fascinating problem in itself, both computationally and mathematically.", "The main question that we want to ask ourselves when sampling is the following. If we are given the possibility of efficiently sampling from a simpler probability distribution, how can we sample from a more complicated probability distribution? As it turns out, sampling from complicated distributions is a non-trivial problem, and forms the basis of many approaches in ML. To name a few: offline reinforcement learning, normalizing flows, variational inference \u2026 But this discussion I leave for a later article.", "What is the \u201cInformation\u201d in Information Theory \u2014 To understand more how uncertainty and the concept of information relate.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "PhD @ Max Planck Institute for Intelligent Systems | All things ML/AI | Gutar | https://jimimvp.github.io/ | https://www.linkedin.com/in/mvlastelica/ |"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fb46be1f96127&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffundamental-problems-of-probabilistic-inference-b46be1f96127&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffundamental-problems-of-probabilistic-inference-b46be1f96127&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffundamental-problems-of-probabilistic-inference-b46be1f96127&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffundamental-problems-of-probabilistic-inference-b46be1f96127&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----b46be1f96127--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b46be1f96127--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://marinvp.medium.com/?source=post_page-----b46be1f96127--------------------------------", "anchor_text": ""}, {"url": "https://marinvp.medium.com/?source=post_page-----b46be1f96127--------------------------------", "anchor_text": "Marin Vlastelica"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8f3ed874afef&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffundamental-problems-of-probabilistic-inference-b46be1f96127&user=Marin+Vlastelica&userId=8f3ed874afef&source=post_page-8f3ed874afef----b46be1f96127---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb46be1f96127&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffundamental-problems-of-probabilistic-inference-b46be1f96127&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb46be1f96127&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffundamental-problems-of-probabilistic-inference-b46be1f96127&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@seanwsinclair?utm_source=medium&utm_medium=referral", "anchor_text": "Sean Sinclair"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://unsplash.com/@jeancarloemer?utm_source=medium&utm_medium=referral", "anchor_text": "Jean Carlo Emer"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://unsplash.com/@aj40?utm_source=medium&utm_medium=referral", "anchor_text": "TAHA AJMI"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://towardsdatascience.com/what-is-the-information-in-information-theory-d916250e4899", "anchor_text": "What is the \u201cInformation\u201d in Information Theory"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----b46be1f96127---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/probability?source=post_page-----b46be1f96127---------------probability-----------------", "anchor_text": "Probability"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----b46be1f96127---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----b46be1f96127---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/data-science?source=post_page-----b46be1f96127---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb46be1f96127&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffundamental-problems-of-probabilistic-inference-b46be1f96127&user=Marin+Vlastelica&userId=8f3ed874afef&source=-----b46be1f96127---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb46be1f96127&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffundamental-problems-of-probabilistic-inference-b46be1f96127&user=Marin+Vlastelica&userId=8f3ed874afef&source=-----b46be1f96127---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb46be1f96127&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffundamental-problems-of-probabilistic-inference-b46be1f96127&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b46be1f96127--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fb46be1f96127&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffundamental-problems-of-probabilistic-inference-b46be1f96127&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----b46be1f96127---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----b46be1f96127--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----b46be1f96127--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----b46be1f96127--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----b46be1f96127--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----b46be1f96127--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----b46be1f96127--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----b46be1f96127--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----b46be1f96127--------------------------------", "anchor_text": ""}, {"url": "https://marinvp.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://marinvp.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Marin Vlastelica"}, {"url": "https://marinvp.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1K Followers"}, {"url": "https://jimimvp.github.io/", "anchor_text": "https://jimimvp.github.io/"}, {"url": "https://www.linkedin.com/in/mvlastelica/", "anchor_text": "https://www.linkedin.com/in/mvlastelica/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8f3ed874afef&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffundamental-problems-of-probabilistic-inference-b46be1f96127&user=Marin+Vlastelica&userId=8f3ed874afef&source=post_page-8f3ed874afef--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F9131bca7a450&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffundamental-problems-of-probabilistic-inference-b46be1f96127&newsletterV3=8f3ed874afef&newsletterV3Id=9131bca7a450&user=Marin+Vlastelica&userId=8f3ed874afef&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}