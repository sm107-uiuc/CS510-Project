{"url": "https://towardsdatascience.com/difference-between-local-response-normalization-and-batch-normalization-272308c034ac", "time": 1682996731.1497548, "path": "towardsdatascience.com/difference-between-local-response-normalization-and-batch-normalization-272308c034ac/", "webpage": {"metadata": {"title": "Difference between Local Response Normalization and Batch Normalization | by Aqeel Anwar | Towards Data Science", "h1": "Difference between Local Response Normalization and Batch Normalization", "description": "Normalization has become important for deep neural networks that compensate for the unbounded nature of certain activation functions such as ReLU, ELU, etc. With these activation functions, the\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.coursera.org/lecture/deep-neural-network/why-does-batch-norm-work-81oTm", "anchor_text": "here", "paragraph_index": 15}, {"url": "https://medium.com/u/a7cc4f201fb5?source=post_page-----272308c034ac--------------------------------", "anchor_text": "Aqeel Anwar", "paragraph_index": 19}, {"url": "https://www.linkedin.com/in/aqeelanwarmalik/", "anchor_text": "LinkedIn", "paragraph_index": 19}, {"url": "http://www.aqeel-anwar.com", "anchor_text": "www.aqeel-anwar.com", "paragraph_index": 21}, {"url": "https://twitter.com/_aqeelanwar", "anchor_text": "https://twitter.com/_aqeelanwar", "paragraph_index": 21}], "all_paragraphs": ["Normalization has become important for deep neural networks that compensate for the unbounded nature of certain activation functions such as ReLU, ELU, etc. With these activation functions, the output layers are not constrained within a bounded range (such as [-1,1] for tanh), rather they can grow as high as the training allows it. To limit the unbounded activation from increasing the output layer values, normalization is used just before the activation function. There are two common normalization techniques used in deep neural networks and are often misunderstood by beginners. In this tutorial, a detailed explanation of both the normalization techniques will be discussed highlighting their key differences.", "Local Response Normalization (LRN) was first introduced in AlexNet architecture where the activation function used was ReLU as opposed to the more common tanh and sigmoid at that time. Apart from the reason mentioned above, the reason for using LRN was to encourage lateral inhibition. It is a concept in Neurobiology that refers to the capacity of a neuron to reduce the activity of its neighbors [1]. In DNNs, the purpose of this lateral inhibition is to carry out local contrast enhancement so that locally maximum pixel values are used as excitation for the next layers.", "LRN is a non-trainable layer that square-normalizes the pixel values in a feature map within a local neighborhood. There are two types of LRN based on the neighborhood defined and can be seen in the figure below.", "Inter-Channel LRN: This is originally what the AlexNet paper used. The neighborhood defined is across the channel. For each (x,y) position, the normalization is carried out in the depth dimension and is given by the following formula", "where i indicates the output of filter i, a(x,y), b(x,y) the pixel values at (x,y) position before and after normalization respectively, and N is the total number of channels. The constants (k,\u03b1,\u03b2,n) are hyper-parameters. k is used to avoid any singularities (division by zero), \u03b1 is used as a normalization constant, while \u03b2 is a contrasting constant. The constant n is used to define the neighborhood length i.e. how many consecutive pixel values need to be considered while carrying out the normalization. The case of (k,\u03b1, \u03b2, n)=(0,1,1,N) is the standard normalization). In the figure above n is taken to be to 2 while N=4.", "Let\u2019s have a look at an example of Inter-channel LRN. Consider the following figure", "Intra-Channel LRN: In Intra-channel LRN, the neighborhood is extended within the same channel only as can be seen in the figure above. The formula is given by", "where (W,H) are the width and height of the feature map (for example in the figure above (W,H) = (8,8)). The only difference between Inter and Intra Channel LRN is the neighborhood for normalization. In Intra-channel LRN, a 2D neighborhood is defined (as opposed to the 1D neighborhood in Inter-Channel) around the pixel under-consideration. As an example, the figure below shows the Intra-Channel normalization on a 5x5 feature map with n=2 (i.e. 2D neighborhood of size (n+1)x(n+1) centered at (x,y)).", "Batch Normalization (BN) is a trainable layer normally used for addressing the issues of Internal Covariate Shift (ICF) [1]. ICF arises due to the changing distribution of the hidden neurons/activation. Consider the following example of binary classification where we need to classify roses and no-roses", "Say we have trained a neural network, and now we select two significantly different looking batches from the dataset for inference (as shown above). If we do a forward pass with these two batches and plot the feature space of a hidden layer (deep in the network) we will see a significant shift in the distribution as seen on the right-hand side of the figure above. This is called the Covariate shift of the input neurons. What impact does this have during training? During training, if we select batches that belong to different distributions then it slows down the training since for a given batch it tries to learn a certain distribution, which is different for the next batch. Hence it keeps on bouncing back and forth between distributions until it converges. This Covariate Shift can be mitigated by making sure that the members within a batch do not belong to the same/similar distribution. This can be done by randomly selecting images for batches. Similar Covariate Shift exists for hidden neurons. Even if the batches are randomly selected, the hidden neuron can end up having a certain distribution which slows down the training. This Covariate shift for hidden layers is called Internal Covariate Shift. The problem is that we can\u2019t directly control the distribution of the hidden neurons, as we did for input neurons, because it keeps on changing as training updates the training parameters. Batch Normalization helps mitigate this issue.", "In batch normalization, the output of hidden neurons is processed in the following manner before being fed to the activation function.", "2. Introduce two trainable parameters (Gamma: scale_variable and Beta: shift_variable) to scale and shift the normalized mini-batch output", "3. Feed this scaled and shifted normalized mini-batch to the activation function.", "The BN algorithm can be seen in the figure below.", "The normalization is carried out for each pixel across all the activations in a batch. Consider the figure below. Let us assume we have a mini-batch of size 3. A hidden layer produces an activation of size (C,H,W) = (4,4,4). Since the batch size is 3, we will have 3 of such activations. Now for each pixel in the activation (i.e. for each 4x4x4=64 pixel), we will normalize it by finding the mean and variance of this pixel position in all the activations as shown in the left part of the figure below. Once the mean and variance are found, we will subtract the mean from each of the activations and divide it with the variance. The right part of the figure below depicts this. The subtraction and division are carried out point-wise. (if you are used to MATLAB, the division is dot-division ./ ).", "The reason for step 2 i.e. scaling and shifting is to let the training decide whether we even need the normalization or not. There are some cases when not having normalization may yield better results. So instead of selecting beforehand whether to include a normalization layer or not, BN lets the training decide it. When Gamma = sigma_B and Beta = u_B, no normalization is carried out, and original activations are restored. A really good video tutorial on BN by Andrew Ng can be found here", "LRN has multiple directions to perform normalization across (Inter or Intra Channel), on the other hand, BN has only one way of being carried out (for each pixel position across all the activations). The table below compares the two normalization techniques.", "[2] Ioffe, Sergey, and Christian Szegedy. \u201cBatch normalization: Accelerating deep network training by reducing internal covariate shift.\u201d arXiv preprint arXiv:1502.03167 (2015).", "Compact cheat sheets for this topic and many other important topics in Machine Learning can be found in the link below", "If this article was helpful to you, feel free to clap, share and respond to it. If want to learn more about Machine Learning and Data Science, follow me @Aqeel Anwar or connect with me on LinkedIn.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Senior ML Engineer @NVIDIA | ex-Samsung | GeorgiaTech | Writer | Researcher | Traveler | www.aqeel-anwar.com | https://twitter.com/_aqeelanwar"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F272308c034ac&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdifference-between-local-response-normalization-and-batch-normalization-272308c034ac&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdifference-between-local-response-normalization-and-batch-normalization-272308c034ac&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdifference-between-local-response-normalization-and-batch-normalization-272308c034ac&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdifference-between-local-response-normalization-and-batch-normalization-272308c034ac&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----272308c034ac--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----272308c034ac--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://aqeel-anwar.medium.com/?source=post_page-----272308c034ac--------------------------------", "anchor_text": ""}, {"url": "https://aqeel-anwar.medium.com/?source=post_page-----272308c034ac--------------------------------", "anchor_text": "Aqeel Anwar"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa7cc4f201fb5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdifference-between-local-response-normalization-and-batch-normalization-272308c034ac&user=Aqeel+Anwar&userId=a7cc4f201fb5&source=post_page-a7cc4f201fb5----272308c034ac---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F272308c034ac&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdifference-between-local-response-normalization-and-batch-normalization-272308c034ac&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F272308c034ac&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdifference-between-local-response-normalization-and-batch-normalization-272308c034ac&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.coursera.org/lecture/deep-neural-network/why-does-batch-norm-work-81oTm", "anchor_text": "here"}, {"url": "https://www.learnopencv.com/batch-normalization-in-deep-networks/", "anchor_text": "https://www.learnopencv.com/batch-normalization-in-deep-networks/"}, {"url": "https://medium.com/swlh/cheat-sheets-for-machine-learning-interview-topics-51c2bc2bab4f", "anchor_text": "Cheat Sheets for Machine Learning Interview TopicsA visual cheatsheet for ML interviews (www.cheatsheets.aqeel-anwar.com)medium.com"}, {"url": "https://medium.com/u/a7cc4f201fb5?source=post_page-----272308c034ac--------------------------------", "anchor_text": "Aqeel Anwar"}, {"url": "https://www.linkedin.com/in/aqeelanwarmalik/", "anchor_text": "LinkedIn"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----272308c034ac---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----272308c034ac---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/deep-neural-networks?source=post_page-----272308c034ac---------------deep_neural_networks-----------------", "anchor_text": "Deep Neural Networks"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----272308c034ac---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F272308c034ac&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdifference-between-local-response-normalization-and-batch-normalization-272308c034ac&user=Aqeel+Anwar&userId=a7cc4f201fb5&source=-----272308c034ac---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F272308c034ac&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdifference-between-local-response-normalization-and-batch-normalization-272308c034ac&user=Aqeel+Anwar&userId=a7cc4f201fb5&source=-----272308c034ac---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F272308c034ac&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdifference-between-local-response-normalization-and-batch-normalization-272308c034ac&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----272308c034ac--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F272308c034ac&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdifference-between-local-response-normalization-and-batch-normalization-272308c034ac&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----272308c034ac---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----272308c034ac--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----272308c034ac--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----272308c034ac--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----272308c034ac--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----272308c034ac--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----272308c034ac--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----272308c034ac--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----272308c034ac--------------------------------", "anchor_text": ""}, {"url": "https://aqeel-anwar.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://aqeel-anwar.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Aqeel Anwar"}, {"url": "https://aqeel-anwar.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "3K Followers"}, {"url": "http://www.aqeel-anwar.com", "anchor_text": "www.aqeel-anwar.com"}, {"url": "https://twitter.com/_aqeelanwar", "anchor_text": "https://twitter.com/_aqeelanwar"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa7cc4f201fb5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdifference-between-local-response-normalization-and-batch-normalization-272308c034ac&user=Aqeel+Anwar&userId=a7cc4f201fb5&source=post_page-a7cc4f201fb5--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fb2e154cad83e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdifference-between-local-response-normalization-and-batch-normalization-272308c034ac&newsletterV3=a7cc4f201fb5&newsletterV3Id=b2e154cad83e&user=Aqeel+Anwar&userId=a7cc4f201fb5&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}