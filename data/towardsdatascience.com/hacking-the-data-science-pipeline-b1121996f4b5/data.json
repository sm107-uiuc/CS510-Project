{"url": "https://towardsdatascience.com/hacking-the-data-science-pipeline-b1121996f4b5", "time": 1683006132.517568, "path": "towardsdatascience.com/hacking-the-data-science-pipeline-b1121996f4b5/", "webpage": {"metadata": {"title": "Hacking The Data Science Pipeline! | by Aisha Javed | Towards Data Science", "h1": "Hacking The Data Science Pipeline!", "description": "We have state-of-the-art advanced algorithms to generate utterly fascinating visualizations, reveal billion dollar consumer purchasing patterns, and analyse revenue streams with big data coupled with\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.kaggle.com/c/widsdatathon2020?rvi=1", "anchor_text": "WiDS Datathon 2020", "paragraph_index": 5}, {"url": "https://www.kaggle.com/c/widsdatathon2020/overview", "anchor_text": "Source", "paragraph_index": 7}, {"url": "https://www.kaggle.com/c/widsdatathon2020", "anchor_text": "here", "paragraph_index": 8}, {"url": "https://whatsthebigdata.com/2016/05/01/data-scientists-spend-most-of-their-time-cleaning-data/", "anchor_text": "here is a well explained read to understand importance of feature engineering", "paragraph_index": 18}, {"url": "https://github.com/aishajv/Feature-Engineering-Series", "anchor_text": "here", "paragraph_index": 21}, {"url": "https://github.com/aishajv/Feature-Engineering-Series", "anchor_text": "Github Repo Jupyter Notebooks", "paragraph_index": 24}, {"url": "https://github.com/aishajv/Feature-Engineering-Series", "anchor_text": "Github Repo", "paragraph_index": 71}, {"url": "https://www.linkedin.com/in/aisha-javed/", "anchor_text": "LinkedIn", "paragraph_index": 74}], "all_paragraphs": ["We have state-of-the-art advanced algorithms to generate utterly fascinating visualizations, reveal billion dollar consumer purchasing patterns, and analyse revenue streams with big data coupled with jaw-dropping scalable analytics engines. But being a data scientist, it all all boils to a single point of entry especially when stepping out into industry\u2014 \u201cWhere Do I Start a Data Science Project!!!\u201d", "Unravel the complete data science project pipeline\u2026", "There are tons of online courses providing complete pathway of training generalized machine machine learning models ranging from basic machine learning algorithms to highly sophisticated deep neural networks and reinforcement learning. But, hacking the data science pipeline of modelling features out of raw, messy and incomplete real world datasets is one of the most indispensable puzzle without which the dream of training cutting edge machine learning models for real world remains unfulfilled!", "You should have sound expertise in the following areas:", "If you have above in your toolkit and you are further looking for hands on experience to unfold the feature engineering process from the point of data acquisition to building a feature set to boost performance gain of your ML model, this blog post is for you!", "We will walk though the early stages of the feature engineering pipeline through hands-on experience of a real world dataset of a recently held hackathon on WiDS Datathon 2020 .", "There are couple of reasons for choosing this dataset as listed below:", "4. The challenge is to create a model that uses data from the first 24 hours of intensive care to predict patient survival (label:\u201chospital_death\u201d). MIT\u2019s GOSSIS community initiative, with privacy certification from the Harvard Privacy Lab, has provided a dataset of more than 130,000 hospital Intensive Care Unit (ICU) visits from patients, spanning a one-year timeframe. [Source]", "5. Detailed description about challenge can be found here", "I would suggest read along and understand the general thought process and then tune the thought process defined here on a real world dataset of your own choice.", "Milestone # 1 : Feature Engineering & Its Indispensable Value!", "And Then : Complete Project Walk-through of Early Stages of Feature Engineering Process", "Milestone # 2 : Feature Understanding", "Milestone # 3 : Feature Improvement", "Milestone # 4 : Training Baseline Model", "What is Feature Engineering?The process of mining/modelling/extracting features from raw data.", "Objective of Feature Engineering?Features are those attributes of your dataset that embed the underlying data patterns and hence extracting/modelling features from your raw data attributes implies high performance gains of your model!", "Data Science Project Fun Facts: Typically, a data scientist spends 80% of the project time in feature engineering and only 20% in training cutting edge models. In fact, feature engineering is most valuable but most under-valued part of any data science project. Effective feature engineering pays of the hard work and you will soon start realizing its worthy importance once you start diving into real world data sets!", "Infact, here is a well explained read to understand importance of feature engineering", "Project Walk-through of Early Stages of Feature Engineering Process", "Feature understanding focuses on \u201cUnderstanding Your Data\u201d and by this I mean scrutinizing your data set as per the following roadmap (this is not a hard coded road-map for every data science project!) :", "You can find complete project code here", "1. Check whether your data is structured or unstructured. For this blog post, we would be dealing with structured data only.", "2. Check out for rows and columns of your dataset.", "4. Determine data types in your given dataset and determine at what level of the four levels of data every attribute lies at Map your data types to categorical and numerical datatypes. Furthermore, lookout for ordinal and nominal in categorical and interval and ratio level for numeric data types. This will provide you the basis for your EDA! You will find more comprehensive EDA in the Github Repo Jupyter Notebooks (Lets keep it a bit short for this blog post). Also, EDA continues throughout the iterations of any Data Science Project as described later in this blog post.", "Feature improvement deals with data cleaning and it is only on the basis of understanding your data that you would be able to further move towards cleaning your dataset in a sensible way.", "More precisely, data cleaning deals with:", "Identifying missing values on the basis of data types and and then defining strategy to deal with them. For example, string data types may be encoded as empty strings as missing data where as numerical data may be encoded as NaN or simply \u201c0\"s to imply missing values. Furthermore, dropping the missing values wouldn't be a wise move if by dropping missing rows, you are left with only half of your dataset or a significant number of rows was dropped. On the other hand, imputation of missing values depends on what particular imputation strategy could best fill the missing values.", "Some of the missing values imputation methods are:", "Moving on in a data science project without identifying missing values is an immediate attempt to blow your results especially when you will be using aggregate functions (more precisely our all time favorites the MEAN and COUNT) and there are plenty of cases apart from this one as well. Trends will go completely opposite and in the end, you will backtrack incorrect mean being computed by considering \u201c0\u201d as a legit variable value instead of a missing value. Same goes for string values! Empty string values would still show up in counts functions. To avoid this, you would encode numeric missing values as proper \u201cNaN\u201ds and strings to some other missing identifiers.", "Take Away : To identify missing values, first ensure that they are correctly encoded as missing values! Then, Whether to drop missing values or impute them and furthermore selecting an imputation criteria all depends upon what type of data problem you have in your hand as well as what project constraints in terms of hardware resources and time you have to encounter.", "Enough of Theory \u2014 Mapping above to our problem statement", "Lets check distribution of missing values against each column in our dataset", "The Graph provides a clear insight of varying distribution of percentage of missing values from almost 0% to 90%. Wow! For this project, this seems to be one of the most biggest data issue and something most rewarding as well if fixed properly.", "Plus, a complete data dictionary has been provided by dataset distributors which needs to be studied in detail in order to explore further dimensions of the data most importantly if missing Values could be encoded in some other way as well apart from 0\u2019s and empty strings!", "Even if documentation wasn't provided, we would still need to dig deeper into attributes for insights regarding missing vales. For example, attributes such as \u201cheight\u201d and \u201cbmi\u201d can never be zero! If for such attributes, zero is present, that implies missing values in our data!", "Lets check how many rows have atleast 1 missing value in our dataset", "Only 25 rows out of entire dataset of 91688 have complete values! So clearly dropping them is not wise move at all. It seems that almost every other row has missing values. Lets dig deeper into which columns have complete row values.", "As we cannot drop rows with missing values, instead, lets focus on above columns for time being as our objective is to train a a baseline model. Plus, also notice that there are 4 id columns that have no link with prediction so we would be dropping those. This leaves us with 6 subset columns(excluding label column) out of total 185 attributes.", "At this point, we have a subset of 6 features without many missing values. The feature set consists of following data attributes:", "2. Distribution of data types as read from the raw csv file", "A simple EDA of above shows that quite a few of binary/bool data types have ended up as \u201cnumeric\u201d data types in the file being read. Being a savvy data scientist, its immensely important to have a vigilant eye over these minute details!", "Let me couple this with a real life scenario. It doesnot seem any harm to encode binary/bool variables as numeric but this would add up in the memory as binary/boolean variables that take up less bytes as compared to float objects! And if you are already short of memory, this tiny boolean bytes casted as floats will collectively add up as big bytes and blow off your memory! Again \u2014 understand your data as much as you can!", "#savvy \u2014 It is always worth reading the documentation sources that come along with your data. They will provide you a keen understanding of data. For example, it was through the documentation I read that it was relatively easier for me to ensure that provided data types and encoded data types are of the same type through documentation.", "Additionaly, for numeric columns, you can decode through documentation that a missing value might be encoded as \u201c0\u201d. Its easy to decode \u201c0\u201d in variables such as age, height and bmi (because we already have domain knowledge) but documentation opens up new ways of exploration when our own domain knowledge is scarce.", "For example, there might be couple of other attributes in given dataset that could be having tons of missing values and for me as an individual with non-medical background, documentation and collaboration discussions with domain experts will provide a deeper level of data understanding.", "Infact, a close collaboration with domain experts is also one of the indispensable practice of a savvy data scientist to analyze data from different unseen dimensions and hence boost performance gains!", "Continuing with EDA..Note : Below graphs correspond to data types/columns which have no missing values at at all i.e for columns listed at your left", "3. Data types distribution of file read data types after dropping missing values and selecting subset of 6 features reveals", "4. Further digging deeper into above visualization \u2014 Data types distribution after further mapping file read data types to dict provided data types", "So 4 of the 8 integer variables decomposed to boolean data types.", "Now that we have all fixed data types fixed and dropping ID based columns from above 11 columns leaves 7 columns (including label colum). Apart from pre_icu_los_days, all attributes are categorical in nature.", "By default, pd.DataFrame.describe() reveals stats of continuous/numeric variables only. Also,", "We need to check for duplicates in original dataset and selected subset of dataset against 6 columns of feature set as well.", "So we originally had 91713 rows out of which unique rows against the selected 6 features make up 19438 only! There are two immensely important points that I would like to stress over here:", "Using keep=first would have introduced noise! Why? Because considering the data frame based on 6 features, you would have different labels against same rows. So I opted to drop all those misleading rows (they are not actually misleading but on the basis of 6 features, yes they are). If you call unique over the entire dataframe of these 6 features and label, you would get 91713 rows but that is utterly wrong if we intend to feed feature set based on 6 columns only (which is the case for baseline model at the moment)", "#Savvy \u2014 Again, it will always be these seemingly minute points that will have gigantic effect on your model- Beware!", "Also note that we are learning form ~20% of dataset only at the moment for our baseline model! Yes, we are good to go for the baseline model but at the same time, this also reveals:", "Concerning above point, we are not at all blind spot here for room for improvement in later iterations. Insight to validity of above can be provided through correlation of attributes/columns with label & check if significant columns of higher correlations are included in the 6 attributes or not. Also if there is a significant change in distribution of labels by considering the subset of 6 attributes only, the shape of data must have changed and hence this would also account for low performance (data imbalance to be more precisely).", "Room for improvement in later iterations exist when we will be adding more columns and hence more rows (unique rows).", "#savvy \u2014 Another insight! The column readmission_status doesnot show up in correlation matrix! Digging deeper into its unique value shows:", "Its all false for the subset of columns and rows selected! Hence, we would be dropping this column too as it would play no role in extracting patterns or generating predictions!", "#savvy \u2014 A simple EDA and such a valuable insight. Again a minute observation but totally worth it!", "By now we have, 5 columns and we are good to go for training a baseline model. Notice that, we have two categorical \u201cstring\u201d variables/attributes and we would need some numeric transformation before feeding the data into training model. For that, we would be using dummy encoding in our data preprocessing pipeline.", "We will be using KNN as classifier. Splitting of dataset, dummifying the categorical variables, scaling (Z-score standardization)and grid search for\u201c k\u201d, have all been added to data preprocessing pipeline (sklearn\u2019s Pipeline components to be more precise).", "Fitting & Transforming data through pipeline and calling fit reveals:", "Great! lets predict for kaggle test set", "Its worth checking that how many values of test set actually exist and how many unique values of test set exist considering the feature subset of 5 columns that we have right now", "#savvy \u2014 Insight! So considering only 5 columns for tests set, we are actually predicting only 9768 test set rows out of 39308 rows which is only 24%! At this moment we are considering most of test set rows as same which is not the case but this also implies that there is a lot of room for potential gain in model\u2019s performance (and hence Kaggle submission).", "And Lastly, here is a flow chart of the entire process that we have done up till now", "So far, we have a good baseline model with us. A baseline model serves as a benchmark for performance gains. We continue adding components into our feature engineering pipeline and compare the newly acquired performance against each added component with baseline model to evaluate how well we have climbed the ladder of performance gain.", "Although, I have not included code snippets in this blog post but you can find the complete project code in my Github Repo. For real world Data Science Projects, we strong follow OOP principles and follow the pipeline approach. Adding new components to Pipeline approach is fairly easy. Your code would be easier to reuse, maintain, debug and make it salable as more things come in. This also implies that in the later iterations, you get more time to spend over the problem itself.", "If you dont trust me, take source code of any python data science framework and you will be able to analyze all the software engineering principles being put into work!", "Although there are standard steps to follow in a feature engineering pipeline for any Data Science project but there is infinite creativity that comes along with every dataset! The more you dive deeper into feature engineering, the more you are forced to think out of the box, analyze problems with your data, explore possible solutions not only for feature engineering but also for ML, and continue to upscale you data science expertise and the FUN never ends!", "If you have any thoughts, comments, or questions, feel free to comment below or connect \ud83d\udcde with me on LinkedIn", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Backend Software Engineering | Big Data | Data Streaming | DevOps | User Centric Data Driven Product Development"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fb1121996f4b5&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhacking-the-data-science-pipeline-b1121996f4b5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhacking-the-data-science-pipeline-b1121996f4b5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhacking-the-data-science-pipeline-b1121996f4b5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhacking-the-data-science-pipeline-b1121996f4b5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----b1121996f4b5--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b1121996f4b5--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@aisha.jv70?source=post_page-----b1121996f4b5--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@aisha.jv70?source=post_page-----b1121996f4b5--------------------------------", "anchor_text": "Aisha Javed"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa0dd5d72e422&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhacking-the-data-science-pipeline-b1121996f4b5&user=Aisha+Javed&userId=a0dd5d72e422&source=post_page-a0dd5d72e422----b1121996f4b5---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb1121996f4b5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhacking-the-data-science-pipeline-b1121996f4b5&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb1121996f4b5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhacking-the-data-science-pipeline-b1121996f4b5&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@quinten149?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Quinten de Graaf"}, {"url": "https://unsplash.com/s/photos/pipelines?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://unsplash.com/@frostroomhead?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Rodion Kutsaev"}, {"url": "https://unsplash.com/s/photos/pipelines?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://www.kaggle.com/c/widsdatathon2020?rvi=1", "anchor_text": "WiDS Datathon 2020"}, {"url": "https://www.kaggle.com/c/widsdatathon2020/overview", "anchor_text": "Source"}, {"url": "https://www.kaggle.com/c/widsdatathon2020", "anchor_text": "here"}, {"url": "https://unsplash.com/@mael_bld?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Mael BALLAND"}, {"url": "https://unsplash.com/s/photos/diver?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://whatsthebigdata.com/2016/05/01/data-scientists-spend-most-of-their-time-cleaning-data/", "anchor_text": "here is a well explained read to understand importance of feature engineering"}, {"url": "https://whatsthebigdata.com/2016/05/01/data-scientists-spend-most-of-their-time-cleaning-data/", "anchor_text": "https://whatsthebigdata.com/2016/05/01/data-scientists-spend-most-of-their-time-cleaning-data/"}, {"url": "https://github.com/aishajv/Feature-Engineering-Series", "anchor_text": "here"}, {"url": "https://github.com/aishajv/Feature-Engineering-Series", "anchor_text": "Github Repo Jupyter Notebooks"}, {"url": "https://github.com/aishajv/Feature-Engineering-Series", "anchor_text": "Github Repo"}, {"url": "https://www.linkedin.com/in/aisha-javed/", "anchor_text": "LinkedIn"}, {"url": "https://unsplash.com/@cannonmatt?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Matt Cannon"}, {"url": "https://unsplash.com/s/photos/victory?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://medium.com/tag/feature-engineering?source=post_page-----b1121996f4b5---------------feature_engineering-----------------", "anchor_text": "Feature Engineering"}, {"url": "https://medium.com/tag/data-scientist?source=post_page-----b1121996f4b5---------------data_scientist-----------------", "anchor_text": "Data Scientist"}, {"url": "https://medium.com/tag/data-cleaning?source=post_page-----b1121996f4b5---------------data_cleaning-----------------", "anchor_text": "Data Cleaning"}, {"url": "https://medium.com/tag/missing-data?source=post_page-----b1121996f4b5---------------missing_data-----------------", "anchor_text": "Missing Data"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----b1121996f4b5---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb1121996f4b5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhacking-the-data-science-pipeline-b1121996f4b5&user=Aisha+Javed&userId=a0dd5d72e422&source=-----b1121996f4b5---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb1121996f4b5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhacking-the-data-science-pipeline-b1121996f4b5&user=Aisha+Javed&userId=a0dd5d72e422&source=-----b1121996f4b5---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb1121996f4b5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhacking-the-data-science-pipeline-b1121996f4b5&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b1121996f4b5--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fb1121996f4b5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhacking-the-data-science-pipeline-b1121996f4b5&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----b1121996f4b5---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----b1121996f4b5--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----b1121996f4b5--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----b1121996f4b5--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----b1121996f4b5--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----b1121996f4b5--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----b1121996f4b5--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----b1121996f4b5--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----b1121996f4b5--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@aisha.jv70?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@aisha.jv70?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Aisha Javed"}, {"url": "https://medium.com/@aisha.jv70/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "429 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa0dd5d72e422&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhacking-the-data-science-pipeline-b1121996f4b5&user=Aisha+Javed&userId=a0dd5d72e422&source=post_page-a0dd5d72e422--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F1afc3fc19ff3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhacking-the-data-science-pipeline-b1121996f4b5&newsletterV3=a0dd5d72e422&newsletterV3Id=1afc3fc19ff3&user=Aisha+Javed&userId=a0dd5d72e422&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}