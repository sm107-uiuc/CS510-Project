{"url": "https://towardsdatascience.com/intuitive-understanding-and-step-by-step-implementation-of-sequence-to-sequence-model-with-86be00ebe0fd", "time": 1683007876.48499, "path": "towardsdatascience.com/intuitive-understanding-and-step-by-step-implementation-of-sequence-to-sequence-model-with-86be00ebe0fd/", "webpage": {"metadata": {"title": "Intuitive understanding and step by step Implementation of Sequence to Sequence model with Attention mechanism | by Ayush Jain | Towards Data Science", "h1": "Intuitive understanding and step by step Implementation of Sequence to Sequence model with Attention mechanism", "description": "If you have ever used Google Translate or have ever interacted with Siri, Alexa, or Google Assistant, you have witnessed the direct impact of Sequence to Sequence (seq2seq) neural architecture\u2026"}, "outgoing_paragraph_urls": [{"url": "https://github.com/ayushjain19/NMT-Sequence-to-sequence-model-with-Attention-mechanism-for-English-to-Hindi-Translation", "anchor_text": "here", "paragraph_index": 1}, {"url": "https://www.kaggle.com/aiswaryaramachandran/hindienglish-corpora", "anchor_text": "here", "paragraph_index": 2}, {"url": "https://arxiv.org/abs/1409.3215", "anchor_text": "original paper", "paragraph_index": 6}, {"url": "https://arxiv.org/abs/1409.0473", "anchor_text": "original paper", "paragraph_index": 14}, {"url": "https://medium.com/voice-tech-podcast/visualising-beam-search-and-other-decoding-algorithms-for-natural-language-generation-fbba7cba2c5b", "anchor_text": "here", "paragraph_index": 29}, {"url": "https://www.linkedin.com/in/ayushjain19/", "anchor_text": "LinkedIn", "paragraph_index": 30}], "all_paragraphs": ["If you have ever used Google Translate or have ever interacted with Siri, Alexa, or Google Assistant, you have witnessed the direct impact of Sequence to Sequence (seq2seq) neural architecture. Seq2seq models form the very basis of Machine Translation, Image and Video Captioning, text summarization, chatbots, and any task which you might imagine includes conversion from one sequence of data to another.", "Our focus here will be on Machine Translation which is basically the translation of a sentence x from one language to sentence y in another language. Machine Translation is a major use-case of seq2seq model and is improved by the Attention mechanism. Articles on these kinds of topics usually involve large code segments for implementation and a lot of API calls from multiple libraries, without an intuitive understanding of the concept itself. Here, we will be doing full justice to theory as well as implementation. Alongside implementation, we will understand what each component of seq2seq architecture and Attention represents in detail. The code used in this article can be found here.", "First, we import all of our libraries we will be needing. English to Hindi corpus used in this implementation can be found at Kaggle here. A file named \u201cHindi_English_Truncated_Corpus.csv\u201d will be downloaded. Make sure to put the right file path, corresponding to the one in your file system, in pd.read_csv() function.", "Let\u2019s have a quick look at the kind of dataset we are dealing with. It\u2019s fairly simple.", "Before we move on to our Encoder, Decoder, and Attention implementation, we need to preprocess our data in such a way that it can be interpreted mathematically. Note that preprocessing steps also depend on the kind of data we are dealing with. For example, in the dataset considered here, there were sentences with empty strings as well. We need to handle these kinds of cases accordingly. If you use some other dataset, there might be some additional or fewer steps as well. The steps for preprocessing include the following:", "Above three tasks for each sentence are implemented using preprocess_sentence() function. We have also initialized all of our hyperparameters and global variables in the start. Do read these hyperparameters and global variables below. We will be using them as and when required.", "Seq2seq architecture involves two Long Short Term Memory (LSTM) in the original paper. One for the encoder and the other for the decoder. Note that we will be using GRU (Gated Recurrent Units) in place of LSTM in both Encoder and Decoder, as GRU takes less compute power and gives almost similar results as LSTM. Steps involved in Encoder:", "Note: In this section, we will be understanding decoder for the case when Attention is not involved. This is important to understand the role of Attention later alongside decoder, which is explained in the sections ahead.", "The decoder GRU network is a language model that generates the target sentence. The final encoder hidden state is used as the initial hidden state for decoder GRU. The first word given to the decoder GRU cell to predict the next is a start token like \u2018sentencestart\u2019. This token is used to predict the probability of occurrence of all of the num_words number of words. Loss is calculated using the predicted probability tensor and the one-hot encoding of the actual word while training. This loss is backpropagated to optimize parameters in encoder & decoder. Meanwhile, the word with max probability becomes the input to the next GRU cell. The above step is repeated until the occurrence of end token like \u2018sentenceend\u2019.", "Solution: We solve the above issue by not relying on just encoder final state for the information of the source sentence but also using a weighted sum of all the outputs from the encoder. So, which encoder output is weighted more than the other you ask? Attention is here to the rescue and we will discuss this in coming sections.", "Attention not only provides the solution to the bottleneck problem but also gives weightage to each word in the sentence(Quite literally). You see, the source sequence has its information in encoder outputs and the word being predicted in decoder has its information in the corresponding decoder hidden state. We need to know which encoder output holds the similar information as that in the decoder hidden state at hand. So, these encoder outputs and the decoder hidden state are used as inputs to a mathematical function to result in a vector of Attention scores. This Attention scores vector is calculated at each step when a word is being predicted (at each GRU cell in the decoder). This vector determines the weightage of each encoder output to find the weighted sum.", "General definition of Attention: Given a set of vectors \u201cvalues\u201d, and a vector \u201cquery\u201d, attention is a technique to compute a weighted sum of values dependent on the query.", "In the context of our seq2seq architecture, each decoder hidden state (query) attends to all of the encoder outputs (values) to get a weighted sum of the encoder output (values) dependent on the decoder hidden state (query).", "The weighted sum is a selective summary of the information contained in the values, where the query determines which values to focus on. This process is like projecting query into values space to find the context of query (score) in values space. The high score represents that the corresponding value is more similar to the query.", "According to the original paper with Attention, the decoder decides parts of the source sentence to pay attention to. By letting the decoder have an attention mechanism, we relieve the encoder from the burden of having to encode all information in the source sentence into a fixed-length vector. With this new approach the information can be spread throughout the sequence of annotations, which can be selectively retrieved by the decoder accordingly.", "Remember the mathematical function we just talked about? Well, there are several ways to find attention scores (similarity). Major ones are mentioned below:", "We will not be going into depth of each one of these here. A simple Google search will be enough to dive into them. We will be considering Basic Dot Product Attention for our implementation, as it is easiest to grasp. You have already guessed what this category of attention does. Judging by the name, it is the dot product of the input matrices.", "Note that Basic Dot Product Attention has one assumption though. It assumes that the dimensions of both the input matrices on the axis where dot product is to be taken need to be the same, for dot product to happen. This dimension in our implementation is given by the hyperparameter hidden_units and is same for both Encoder & Decoder.", "Too much theory. Let\u2019s get back to the code now! We will define our Attention class.", "The following steps are taken in our decoder class.", "We define our loss function and optimizer. Sparse Categorical Crossentropy loss and Adam Optimizer are chosen. Steps involved in each training step:", "In each epoch, above training step is called for every batch, and loss corresponding to each epoch is stored and plotted in the end.", "A side note: In step 1, why are we still using encoder\u2019s final hidden state as our decoder\u2019s first hidden state?", "That\u2019s because, if we do this, seq2seq model will be optimized as a single system. Backpropagation operates end to end. We do not want to optimize encoder and decoder separately. And, there is no need to get source sequence information through this hidden state because we have our attention now :)", "To test how our model performs after being trained, we define a function that takes in an English sentence and returns a Hindi sentence as predicted by our model. Let\u2019s implement this function, and we will see how good or bad the results are in the next section.", "Let\u2019s talk about results and findings. I ran the code on Kaggle with NVidia K80 GPU with the hyperparameters as given in the code above. For 100 epochs, it took 70 minutes to train. The loss vs epoch plot is shown below.", "After training for 35 epochs, I tried throwing random English sentences to our translate_sentence() function, and the results were somewhat satisfying, yet questionable to some extent. Clearly, hyperparameters can be optimized more.", "But hyperparameters are not the only ones to blame here for a few deviations from the actual translations. Let\u2019s have a small discussion on some more points which can be implemented to make our model perform even better.", "We have seen a very basic understanding of encoder, decoder, and attention mechanism while implementing our model. Depending upon the time and compute power available, following are some of the points which can be tried and tested to know if they work out fine when implemented:", "The decoding we saw was greedy decoding. We assumed that the word with highest probability is the final predicted word and input to the next decoder state. The problem with this approach is that there is no way to undo this decision. Beam Search Decoding, on the other hand considers top k number of possible words from the word probability distribution, and checks all possibilities. You can read more about Beam Search Decoding and some other possible decodings here.", "I hope the information provided here increased the horizons of your understanding in NLP and seq2seq architecture. Follow to promote more content like this. You can also connect with me on LinkedIn.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Science at ShareChat | BITS Pilani"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F86be00ebe0fd&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuitive-understanding-and-step-by-step-implementation-of-sequence-to-sequence-model-with-86be00ebe0fd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuitive-understanding-and-step-by-step-implementation-of-sequence-to-sequence-model-with-86be00ebe0fd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuitive-understanding-and-step-by-step-implementation-of-sequence-to-sequence-model-with-86be00ebe0fd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuitive-understanding-and-step-by-step-implementation-of-sequence-to-sequence-model-with-86be00ebe0fd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----86be00ebe0fd--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----86be00ebe0fd--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@ayushjain19?source=post_page-----86be00ebe0fd--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ayushjain19?source=post_page-----86be00ebe0fd--------------------------------", "anchor_text": "Ayush Jain"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F107ed3fc80a9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuitive-understanding-and-step-by-step-implementation-of-sequence-to-sequence-model-with-86be00ebe0fd&user=Ayush+Jain&userId=107ed3fc80a9&source=post_page-107ed3fc80a9----86be00ebe0fd---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F86be00ebe0fd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuitive-understanding-and-step-by-step-implementation-of-sequence-to-sequence-model-with-86be00ebe0fd&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F86be00ebe0fd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuitive-understanding-and-step-by-step-implementation-of-sequence-to-sequence-model-with-86be00ebe0fd&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://github.com/ayushjain19/NMT-Sequence-to-sequence-model-with-Attention-mechanism-for-English-to-Hindi-Translation", "anchor_text": "here"}, {"url": "https://www.kaggle.com/aiswaryaramachandran/hindienglish-corpora", "anchor_text": "here"}, {"url": "https://arxiv.org/abs/1409.3215", "anchor_text": "original paper"}, {"url": "https://arxiv.org/abs/1409.0473", "anchor_text": "original paper"}, {"url": "https://medium.com/voice-tech-podcast/visualising-beam-search-and-other-decoding-algorithms-for-natural-language-generation-fbba7cba2c5b", "anchor_text": "here"}, {"url": "https://www.linkedin.com/in/ayushjain19/", "anchor_text": "LinkedIn"}, {"url": "https://arxiv.org/abs/1409.3215", "anchor_text": "https://arxiv.org/abs/1409.3215"}, {"url": "https://arxiv.org/abs/1409.0473", "anchor_text": "https://arxiv.org/abs/1409.0473"}, {"url": "https://www.kaggle.com/aiswaryaramachandran/hindienglish-corpora", "anchor_text": "https://www.kaggle.com/aiswaryaramachandran/hindienglish-corpora"}, {"url": "https://www.youtube.com/watch?v=XXtpJxZBa2c&list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&index=8", "anchor_text": "Stanford CS224N: NLP with Deep Learning | Winter 2019 | Lecture 8 \u2014 Translation, Seq2Seq, Attention"}, {"url": "https://www.tensorflow.org/tutorials/text/nmt_with_attention", "anchor_text": "https://www.tensorflow.org/tutorials/text/nmt_with_attention"}, {"url": "https://medium.com/tag/nlp?source=post_page-----86be00ebe0fd---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/sequence-to-sequence?source=post_page-----86be00ebe0fd---------------sequence_to_sequence-----------------", "anchor_text": "Sequence To Sequence"}, {"url": "https://medium.com/tag/attention?source=post_page-----86be00ebe0fd---------------attention-----------------", "anchor_text": "Attention"}, {"url": "https://medium.com/tag/encoder-decoder?source=post_page-----86be00ebe0fd---------------encoder_decoder-----------------", "anchor_text": "Encoder Decoder"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----86be00ebe0fd---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F86be00ebe0fd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuitive-understanding-and-step-by-step-implementation-of-sequence-to-sequence-model-with-86be00ebe0fd&user=Ayush+Jain&userId=107ed3fc80a9&source=-----86be00ebe0fd---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F86be00ebe0fd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuitive-understanding-and-step-by-step-implementation-of-sequence-to-sequence-model-with-86be00ebe0fd&user=Ayush+Jain&userId=107ed3fc80a9&source=-----86be00ebe0fd---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F86be00ebe0fd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuitive-understanding-and-step-by-step-implementation-of-sequence-to-sequence-model-with-86be00ebe0fd&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----86be00ebe0fd--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F86be00ebe0fd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuitive-understanding-and-step-by-step-implementation-of-sequence-to-sequence-model-with-86be00ebe0fd&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----86be00ebe0fd---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----86be00ebe0fd--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----86be00ebe0fd--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----86be00ebe0fd--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----86be00ebe0fd--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----86be00ebe0fd--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----86be00ebe0fd--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----86be00ebe0fd--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----86be00ebe0fd--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ayushjain19?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ayushjain19?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ayush Jain"}, {"url": "https://medium.com/@ayushjain19/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "58 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F107ed3fc80a9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuitive-understanding-and-step-by-step-implementation-of-sequence-to-sequence-model-with-86be00ebe0fd&user=Ayush+Jain&userId=107ed3fc80a9&source=post_page-107ed3fc80a9--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F107ed3fc80a9%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuitive-understanding-and-step-by-step-implementation-of-sequence-to-sequence-model-with-86be00ebe0fd&user=Ayush+Jain&userId=107ed3fc80a9&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}