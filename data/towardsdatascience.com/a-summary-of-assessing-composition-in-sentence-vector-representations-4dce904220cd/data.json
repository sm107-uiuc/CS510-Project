{"url": "https://towardsdatascience.com/a-summary-of-assessing-composition-in-sentence-vector-representations-4dce904220cd", "time": 1683009860.882464, "path": "towardsdatascience.com/a-summary-of-assessing-composition-in-sentence-vector-representations-4dce904220cd/", "webpage": {"metadata": {"title": "A Summary of Assessing Composition in Sentence Vector Representations | by Chaithanya Pramodh Kasula | Towards Data Science", "h1": "A Summary of Assessing Composition in Sentence Vector Representations", "description": "Background: In order to understand what sentence embeddings are, it is necessary to understand word embeddings. Word embeddings have become very famous because of their ability to represent a word in\u2026"}, "outgoing_paragraph_urls": [], "all_paragraphs": ["Background: In order to understand what sentence embeddings are, it is necessary to understand word embeddings. Word embeddings have become very famous because of their ability to represent a word in the form of a vector that can be used universally. Famous embeddings include word2vec, Glove, etc. The same concept of word embeddings can be extended to sentences such that each sentence is represented with a vector. Tasks such as \u2018finding whether two stack-overflow questions are duplicates or not\u2019 create the necessity for the utilization of sentence embeddings.", "Introduction: In order to understand the language, it is essential to understand the meaning and the composition of a sentence. Most of today\u2019s Neural Network models are black box in nature. Hence, it is difficult to understand what sentence embeddings are capturing as a part of their training which ultimately generates them.", "The paper discusses specific tasks that are designed solely to test whether the sentence embeddings capture the composition and meaning of the sentence. The current paper builds on another paper (Adi et. al, 2016), in which, the authors of (Adi et. al, 2016) perform similar experiments with a BOW model and an auto-encoder. However, their results are quite questionable due to the unintended bias in the dataset. For example, they show that in word order tasks, a BOW model achieves a performance of 70% even when it is logically not possible for such a model to preserve the information related to the order of the words. So, the authors of the current paper believe that such performance of the BOW model was a chance event due to the bias present in the dataset.", "The current paper proposes to remove/reduce such bias and test other deep learning models. They also introduce extra tasks that are discussed below in detail. The contributions of the paper include a sentence generation set, a proven model to test whether the composition and the meaning are indeed embedded by the embeddings, the generation system and the datasets used for classification are provided as an open-source to others for further exploration and analysis.", "Research Questions: The research questions that the paper tries to answer are:", "1. How well do the current neural sentence embedding modules capture the meaning and the composition of a sentence in their sentence embeddings?", "2. Can we propose a method and a framework to evaluate sentence embeddings and their models in terms of how well they capture the meaning and the composition of a sentence?", "Meaning and Composition: The elements that make up a sentence deliver meaning to the reader. It generally contains an agent, a patient and an event. The composition is a way the elements are arranged which helps in conveying the meaning. Different parts of a sentence can be combined to arrive at its meaning.", "Dataset preparation: The authors propose a new sentence generation system that reduces the bias in the data set. For example, it generates diverse and completely annotated sentences following the syntactic, semantic and lexical rules of the English language. It consists of three parts.", "Event/Sentence Representations: These are partial representations of a sentence that take in arguments such as agent, patient, transitive, intransitive verbs and the presence or absence of relative clause. These representations are provided as input to the Event generation system. For example, consider a sentence as shown below.", "Event population: The system takes input from the Sentence Representation system and populates the event with the given information. It uses adverbs and 17 words in its vocabulary. The number of words is restricted, in order to maintain control over generated sentences. It loops through the adverbs and nouns to generate the sentences.", "Syntactic Realizations: It uses a rule-based technique to make inflections to words the generated sentence to follow morphology. They use the NLTK framework as mentioned in (Bird et al., 2009) and the inflections are extracted from the database of XTAG morphology (Doran et al., 1994).", "Classification tasks: Different tasks are designed to test whether the sentence embeddings preserve the composition and the meaning of a sentence. They can be classified broadly into two types, SemRole and Negation. The SemRole task is designed to test whether the sentence embeddings capture the meaning. The question to be answered is, given a noun (n), verb (v) and a sentence embedding (s), is n the agent of v in s?", "The negation task is a test to know whether the sentence embeddings capture the negation of the verb or not. That is, given a verb (v) and a sentence embedding (s), is v negated in s or not? The generated sentences have adverbs filled between the verb and the negation so that it is not obvious that the verb comes after the negation and that pattern is easy for the model to learn and detect.", "The three other tasks are related to the word content and order. The first task, Content1Probe is framed to test whether the sentence embeddings contain the verb given the verb is present int the input sentence. Content2Probe is similar to Content1Probe and its tests whether the sentence embeddings contain both the noun and the verb, given both are presented as input before the generation of sentence embeddings.", "The Order task is designed to test whether the resultant sentence embeddings capture the information of the order of words in a sentence. Does then the noun (n) occur before the verb (v), given a sentence embedding (s) which contains both a verb (v) and noun (n).", "Classification Experiments: The authors build a Neural Network/Multi-layer perceptron model whose input size equals the size of the sentence embeddings. ReLU is used as the activation function for the neurons. The above classification tasks are binary in nature. Hence, for the generated sentences, the label for each task (yes or no) is assigned. 5000 such sentences are generated from which 4000 sentences, with their appropriate labels, are used as the training set and the rest 1000 sentences (with their appropriate labels) are used as the test set. The hyperparameter tuning is not needed as the specifications of the network are mentioned in (Adi et. al, 2016). The authors do not perform any training to generate sentence embeddings. They do not design a sentence embedding algorithm too. They use the embedding algorithms that are already available as pre-trained models on their own corpora. These models are used to produce sentence embeddings for the generated 5000 sentences. The embeddings of a sentence and their respective labels for a task form the dataset for the neural network model. The model is trained with the train set and tested using the test set.", "Sentence Embedding Models: The different sentence embedding models used are described below:", "BOW: The BOW model is a simple model that uses a vector to represent each word in a sentence. It averages these embeddings and uses it as a sentence embedding.", "Sequential Denoising Autoencoder: It is an unsupervised learning technique that uses an LSTM based autoencoder.", "Skip Thought Embeddings: They utilize GRUs to generate sentence embeddings. There are two available variants, uni-skip (ST-UNI) and bi-skip (ST-BI). Uni-skip encoders reply on forward pass whereas bi-skip uses both forward and backward passes in the neural network.", "InferSent: InferSent is the most advanced model which uses a multi-layer Bidirectional LSTM to generate sentence embeddings.", "The results of the models on various tasks are depicted in the above table. It is extracted from Ettinger et al. (2018).", "The Bag of Words (BOW) model performs well on the tasks based on content. This states that the BOW model encodes the meaning of a word perfectly. It is expected to perform badly over the Order, SemRole and the Negation tasks and it did so. This serves as a check for the dataset and it satisfies that criteria. It can also be due to the very limited vocabulary set of 17 words used in the data generation module.", "For the negation task, all the models except ST-BI performed well. This is probably because during the forward pass it encounters the negation first followed by the verb but in the backward pass, this sequence is reversed. Hence, the information might not have been captured well. The other models did well even when it is difficult to capture the relationship even when it is difficult to do so with the presence of adverbs between the negation and the verb. It can also be due to the reduced dimensionality of the embeddings from 1200 to 300.", "For the semantic role (SemRole) task, InferSent performs at chance and the other models also do not perform well. As stated by the authors of the current paper (Ettinger et al., 2018), the sentence embedding models do not provide strong evidence showing that they capture the semantic role substantially.", "It can also be inferred that different sentence embedding models, which have a different design architecture and objective, have failed in having any significant impact on the meaning and the composition of a sentence when evaluated through their embeddings. All the models capture almost the same level of meaning and composition in their embeddings.", "The result also proves that the proposed approach is reliable to detect how much/whether the meaning and the composition are captured by the sentence embeddings. More tasks can be defined to gain a better understanding of the information captured in the sentence embeddings.", "The authors plan to test other models by explicitly embedding the syntactic structure as mentioned in e (Bowman et al., 2016; Dyer et al., 2016; Socher et al., 2013) in their future work.", "Yossi Adi, Einat Kermany, Yonatan Belinkov, Ofer Lavi, and Yoav Goldberg. 2016. Fine-grained analysis of sentence embeddings using auxiliary prediction tasks. arXiv preprint arXiv:1608.04207.", "Steven Bird, Ewan Klein, and Edward Loper. 2009. Natural language processing with Python: analyzing text with the natural language toolkit. O\u2019Reilly Media, Inc.", "Christy Doran, Dania Egedi, Beth Ann Hockey, Bangalore Srinivas, and Martin Zaidel. 1994. XTAG system: a wide coverage grammar for English. In Proceedings of the 15th conference on Computational linguistics Volume 2, pages 922\u2013928. Association for Computational Linguistics.", "Samuel R Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning. 2015. A large annotated corpus for learning natural language inference. In EMNLP.", "Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A Smith. 2016. Recurrent neural network grammars. NAACL.", "Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1631\u2013 1642.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Graduate Research Assistant at George Mason University"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F4dce904220cd&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-summary-of-assessing-composition-in-sentence-vector-representations-4dce904220cd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-summary-of-assessing-composition-in-sentence-vector-representations-4dce904220cd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-summary-of-assessing-composition-in-sentence-vector-representations-4dce904220cd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-summary-of-assessing-composition-in-sentence-vector-representations-4dce904220cd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----4dce904220cd--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4dce904220cd--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@chaitupramod?source=post_page-----4dce904220cd--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@chaitupramod?source=post_page-----4dce904220cd--------------------------------", "anchor_text": "Chaithanya Pramodh Kasula"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa855a9e586cf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-summary-of-assessing-composition-in-sentence-vector-representations-4dce904220cd&user=Chaithanya+Pramodh+Kasula&userId=a855a9e586cf&source=post_page-a855a9e586cf----4dce904220cd---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4dce904220cd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-summary-of-assessing-composition-in-sentence-vector-representations-4dce904220cd&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4dce904220cd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-summary-of-assessing-composition-in-sentence-vector-representations-4dce904220cd&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@impatrickt", "anchor_text": "Patrick"}, {"url": "https://unsplash.com/photos/Oaqk7qqNh_c", "anchor_text": "Unsplash"}, {"url": "https://www.aclweb.org/anthology/C18-1152.pdf", "anchor_text": "https://www.aclweb.org/anthology/C18-1152.pdf"}, {"url": "https://www.aclweb.org/anthology/C18-1152.pdf", "anchor_text": "source"}, {"url": "https://medium.com/tag/sentence-embedding?source=post_page-----4dce904220cd---------------sentence_embedding-----------------", "anchor_text": "Sentence Embedding"}, {"url": "https://medium.com/tag/nlp?source=post_page-----4dce904220cd---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----4dce904220cd---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----4dce904220cd---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/classification?source=post_page-----4dce904220cd---------------classification-----------------", "anchor_text": "Classification"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4dce904220cd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-summary-of-assessing-composition-in-sentence-vector-representations-4dce904220cd&user=Chaithanya+Pramodh+Kasula&userId=a855a9e586cf&source=-----4dce904220cd---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4dce904220cd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-summary-of-assessing-composition-in-sentence-vector-representations-4dce904220cd&user=Chaithanya+Pramodh+Kasula&userId=a855a9e586cf&source=-----4dce904220cd---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4dce904220cd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-summary-of-assessing-composition-in-sentence-vector-representations-4dce904220cd&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4dce904220cd--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F4dce904220cd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-summary-of-assessing-composition-in-sentence-vector-representations-4dce904220cd&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----4dce904220cd---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----4dce904220cd--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----4dce904220cd--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----4dce904220cd--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----4dce904220cd--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----4dce904220cd--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----4dce904220cd--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----4dce904220cd--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----4dce904220cd--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@chaitupramod?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@chaitupramod?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Chaithanya Pramodh Kasula"}, {"url": "https://medium.com/@chaitupramod/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "49 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa855a9e586cf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-summary-of-assessing-composition-in-sentence-vector-representations-4dce904220cd&user=Chaithanya+Pramodh+Kasula&userId=a855a9e586cf&source=post_page-a855a9e586cf--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fa96513dd17d2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-summary-of-assessing-composition-in-sentence-vector-representations-4dce904220cd&newsletterV3=a855a9e586cf&newsletterV3Id=a96513dd17d2&user=Chaithanya+Pramodh+Kasula&userId=a855a9e586cf&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}