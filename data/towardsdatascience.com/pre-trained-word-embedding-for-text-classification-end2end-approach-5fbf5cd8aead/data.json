{"url": "https://towardsdatascience.com/pre-trained-word-embedding-for-text-classification-end2end-approach-5fbf5cd8aead", "time": 1683016390.154165, "path": "towardsdatascience.com/pre-trained-word-embedding-for-text-classification-end2end-approach-5fbf5cd8aead/", "webpage": {"metadata": {"title": "Use Pre-trained Word Embedding to detect real disaster tweets | by Zeineb Ghrib | Towards Data Science", "h1": "Use Pre-trained Word Embedding to detect real disaster tweets", "description": "In this post we will go through the overall text classification pipeline, and especially the data pre-processing steps, we will be using a Glove pre-trained word embedding. Textual features\u2026"}, "outgoing_paragraph_urls": [{"url": "https://nlp.stanford.edu/projects/glove/", "anchor_text": "Glove", "paragraph_index": 0}, {"url": "https://en.wikipedia.org/wiki/Natural_language_processing", "anchor_text": "Natural Language Processing", "paragraph_index": 0}, {"url": "https://www.kaggle.com/c/nlp-getting-started/data", "anchor_text": "Real or Not? NLP with Disaster Tweets", "paragraph_index": 1}, {"url": "https://medium.com/prevision-io/automated-nlp-with-prevision-io-part1-naive-bayes-classifier-475fa8bd73de", "anchor_text": "post", "paragraph_index": 1}, {"url": "https://www.kaggle.com/schopenhacker75/eda-text-cleaning-glove?scriptVersionId=46794932", "anchor_text": "kaggle notebook", "paragraph_index": 2}, {"url": "https://nlp.stanford.edu/projects/glove/", "anchor_text": "Glove pre-trained embedding", "paragraph_index": 17}, {"url": "https://github.com/keras-team/keras/wiki/Keras-2.0-release-notes", "anchor_text": "2.0 version", "paragraph_index": 20}, {"url": "https://cloud.prevision.io/", "anchor_text": "prevision", "paragraph_index": 23}, {"url": "https://huggingface.co/transformers/model_doc/bert.html", "anchor_text": "BERT", "paragraph_index": 23}, {"url": "https://www.kaggle.com/schopenhacker75/eda-text-cleaning-glove?scriptVersionId=46794932", "anchor_text": "kaggle notebook", "paragraph_index": 24}, {"url": "https://medium.com/prevision-io/automated-nlp-with-prevision-io-part1-naive-bayes-classifier-475fa8bd73de", "anchor_text": "my last post", "paragraph_index": 24}, {"url": "https://cloud.prevision.io/", "anchor_text": "prevision cloud instance", "paragraph_index": 26}], "all_paragraphs": ["In this post we will go through the overall text classification pipeline, and especially the data pre-processing steps, we will be using a Glove pre-trained word embedding. Textual features processing is a little bit more tricky than linear or categorical features. In fact, machine learning algorithms are more about scalars and vectors rather than characters or words. So we have to convert the text input into scalars, and the keystone \ud83d\udddd element consists in how to find out the best representation of the input words. This is the main idea behind Natural Language Processing", "We will use a dataset from a Kaggle competition called Real or Not? NLP with Disaster Tweets. The task consists in predicting whether or not a given tweet is about a real disaster. To address this text classification task we will use word embedding transformation followed by a recurrent deep learning model. Other less sophisticated solutions, but still efficient, are also possible such as combining tf-idf encoding and a naive Bayes classifier (check out my last post).", "Also I will include some handy Python code that can be reproduced in other NLP tasks. The overall source code is accessible in this kaggle notebook.", "Models such as LSTM or CNN are more efficient to capture the words order and the semantic relationship between them, which usually is critical to the text\u2019s meaning : a sample from our dataset that is labelled as a real disaster:", "'#RockyFire Update => California Hwy. 20 closed in both directions due to Lake County fire - #CAfire #wildfires'", "It is obvious that the words order was important in the example above.", "In the other hand, we need to convert input text to a machine readable format. It exists many technics such as", "An embedding is a dense vector that represents a word (or a symbol). By default, the embedding vectors are randomly initialized, then will gradually be improved during the training phase, with the gradient descent algorithm at each back-propagation step, so that similar words or words in the same lexical field or with common stem \u2026 will end up close in terms of distance in the new vector space; (see figure below):", "Pre-trained word embedding is an example of Transfer Learning. The main idea behind it is to use public embeddings that are already trained on large datasets. Specifically, instead of initializing our neural network weights randomly, we will set these pre trained embeddings as initialization weights. This trick helps to accelerate training and boost the performance of NLP models.", "Before all, let\u2019s import the required libraries and tools that will help us perform the NLP processing and the", "Regardless of the EDA step that can bring out the uncleaned elements and help us to customize the cleaning code, we can apply some basic data cleaning that are recurrent in tweeters such as removing punctuation, html tags urls and emojis, spelling correction,..", "Below a python code that can be be reproduced in other similar use cases \ud83d\ude09", "Then we will split the datset into:", "As mentioned before, machine learning algorithms take numbers as inputs, not text, which means that we need to convert the texts into numerical vectors.We proceed as follows:", "It consists in dividing the texts into words or smaller sub-texts, allowing us to determine the \u201cvocabulary\u201d of the dataset (set of unique tokens present in the data). Usually we use word-level representation. For our exemple we will use NLTK Tokenizer()", "Construct a vocablary_index mapper based on word frequency: the index would be inversely proportional to the word occurrence frequency in the overall dataset. the most frequent world would have index=1.. And every single word would get a unique index.", "These two steps are factorized as follows:", "First of we will download Glove pre-trained embedding from the official site, (because of some technical constraints I had to download it via a code :", "Then we will create an embedding matrix that maps each word index to its corresponding embedding vector:", "We will create a recurrent neural network using a Sequential keras model that will contain:", "If we want to compute, in addition to the accuracy, the precision, recall and F1-score for our binary Keras Classifier model, we have to calculate them manually, because these metrics are not supported by keras since 2.0 version.", "Now compile and train the model:", "To get the validation performances results, use the evaluate() method:", "These results seems to be pretty good but of course it can be enhanced by fine-tuning the neural network hyper-parameters, or by using auto-ml tools such as prevision, which apply many other transformations, in addition to the wor2vec, such as ngram tokenization, tf-idf or more advanced technics such as BERT transformers.", "In this post I showed you, step by step, how to apply wor2vec transformation from Glove pre-trained word embedding, and how to use it to train a recurrent neural network. Please note that the approach and the code can be reused in other similar use cases. The overall source code can be found in this kaggle notebook.I also applied on the same dataset a complete different approach : I used tf-idf naive Bayes classifiers, if you want to get more information visit my last post.", "I am intending to write a post about how to use a breakthrough algorithm called Bert and compare it with other NLP algorithms", "Thanks for reading my post \ud83e\udd17!! If you have any question you can find me at the chat session in prevision cloud instance or send me an email to : zeineb.ghrib@prevision.io", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F5fbf5cd8aead&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpre-trained-word-embedding-for-text-classification-end2end-approach-5fbf5cd8aead&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpre-trained-word-embedding-for-text-classification-end2end-approach-5fbf5cd8aead&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpre-trained-word-embedding-for-text-classification-end2end-approach-5fbf5cd8aead&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpre-trained-word-embedding-for-text-classification-end2end-approach-5fbf5cd8aead&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----5fbf5cd8aead--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5fbf5cd8aead--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://zghrib.medium.com/?source=post_page-----5fbf5cd8aead--------------------------------", "anchor_text": ""}, {"url": "https://zghrib.medium.com/?source=post_page-----5fbf5cd8aead--------------------------------", "anchor_text": "Zeineb Ghrib"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F16388127d873&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpre-trained-word-embedding-for-text-classification-end2end-approach-5fbf5cd8aead&user=Zeineb+Ghrib&userId=16388127d873&source=post_page-16388127d873----5fbf5cd8aead---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5fbf5cd8aead&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpre-trained-word-embedding-for-text-classification-end2end-approach-5fbf5cd8aead&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5fbf5cd8aead&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpre-trained-word-embedding-for-text-classification-end2end-approach-5fbf5cd8aead&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/", "anchor_text": "https://unsplash.com/"}, {"url": "https://nlp.stanford.edu/projects/glove/", "anchor_text": "Glove"}, {"url": "https://en.wikipedia.org/wiki/Natural_language_processing", "anchor_text": "Natural Language Processing"}, {"url": "https://www.kaggle.com/c/nlp-getting-started/data", "anchor_text": "Real or Not? NLP with Disaster Tweets"}, {"url": "https://medium.com/prevision-io/automated-nlp-with-prevision-io-part1-naive-bayes-classifier-475fa8bd73de", "anchor_text": "post"}, {"url": "https://www.kaggle.com/schopenhacker75/eda-text-cleaning-glove?scriptVersionId=46794932", "anchor_text": "kaggle notebook"}, {"url": "https://developers.google.com/machine-learning/crash-course/images/linear-relationships.svg", "anchor_text": "https://developers.google.com/machine-learning/crash-course/images/linear-relationships.svg"}, {"url": "https://nlp.stanford.edu/projects/glove/", "anchor_text": "Glove pre-trained embedding"}, {"url": "https://developers.google.com/machine-learning/guides/text-classification/images/EmbeddingLayer.png", "anchor_text": "https://developers.google.com/machine-learning/guides/text-classification/images/EmbeddingLayer.png"}, {"url": "https://emojipedia.org/whatsapp/2.20.198.15/robot/", "anchor_text": "whatsapp robot"}, {"url": "https://machinelearningmastery.com/use-dropout-lstm-networks-time-series-forecasting/#:~:text=Dropout%20is%20a%20regularization%20method,overfitting%20and%20improving%20model%20performance.", "anchor_text": "post"}, {"url": "https://github.com/keras-team/keras/wiki/Keras-2.0-release-notes", "anchor_text": "2.0 version"}, {"url": "https://datascience.stackexchange.com/questions/45165/how-to-get-accuracy-f1-precision-and-recall-for-a-keras-model", "anchor_text": "here"}, {"url": "https://www.kaggle.com/schopenhacker75/eda-text-cleaning-glove?scriptVersionId=46794932", "anchor_text": "here"}, {"url": "https://cloud.prevision.io/", "anchor_text": "prevision"}, {"url": "https://huggingface.co/transformers/model_doc/bert.html", "anchor_text": "BERT"}, {"url": "https://www.kaggle.com/schopenhacker75/eda-text-cleaning-glove?scriptVersionId=46794932", "anchor_text": "kaggle notebook"}, {"url": "https://medium.com/prevision-io/automated-nlp-with-prevision-io-part1-naive-bayes-classifier-475fa8bd73de", "anchor_text": "my last post"}, {"url": "https://cloud.prevision.io/", "anchor_text": "prevision cloud instance"}, {"url": "https://medium.com/tag/nlp?source=post_page-----5fbf5cd8aead---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/kears?source=post_page-----5fbf5cd8aead---------------kears-----------------", "anchor_text": "Kears"}, {"url": "https://medium.com/tag/kaggle?source=post_page-----5fbf5cd8aead---------------kaggle-----------------", "anchor_text": "Kaggle"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----5fbf5cd8aead---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/word-embeddings?source=post_page-----5fbf5cd8aead---------------word_embeddings-----------------", "anchor_text": "Word Embeddings"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5fbf5cd8aead&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpre-trained-word-embedding-for-text-classification-end2end-approach-5fbf5cd8aead&user=Zeineb+Ghrib&userId=16388127d873&source=-----5fbf5cd8aead---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5fbf5cd8aead&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpre-trained-word-embedding-for-text-classification-end2end-approach-5fbf5cd8aead&user=Zeineb+Ghrib&userId=16388127d873&source=-----5fbf5cd8aead---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5fbf5cd8aead&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpre-trained-word-embedding-for-text-classification-end2end-approach-5fbf5cd8aead&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5fbf5cd8aead--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F5fbf5cd8aead&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpre-trained-word-embedding-for-text-classification-end2end-approach-5fbf5cd8aead&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----5fbf5cd8aead---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----5fbf5cd8aead--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----5fbf5cd8aead--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----5fbf5cd8aead--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----5fbf5cd8aead--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----5fbf5cd8aead--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----5fbf5cd8aead--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----5fbf5cd8aead--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----5fbf5cd8aead--------------------------------", "anchor_text": ""}, {"url": "https://zghrib.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://zghrib.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Zeineb Ghrib"}, {"url": "https://zghrib.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "114 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F16388127d873&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpre-trained-word-embedding-for-text-classification-end2end-approach-5fbf5cd8aead&user=Zeineb+Ghrib&userId=16388127d873&source=post_page-16388127d873--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F8f09b43f6a7e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpre-trained-word-embedding-for-text-classification-end2end-approach-5fbf5cd8aead&newsletterV3=16388127d873&newsletterV3Id=8f09b43f6a7e&user=Zeineb+Ghrib&userId=16388127d873&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}