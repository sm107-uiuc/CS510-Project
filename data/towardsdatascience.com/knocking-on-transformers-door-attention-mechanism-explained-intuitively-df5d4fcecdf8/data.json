{"url": "https://towardsdatascience.com/knocking-on-transformers-door-attention-mechanism-explained-intuitively-df5d4fcecdf8", "time": 1683011499.959264, "path": "towardsdatascience.com/knocking-on-transformers-door-attention-mechanism-explained-intuitively-df5d4fcecdf8/", "webpage": {"metadata": {"title": "The Transformer Isn\u2019t As Hard To Understand As You Might Think | by jiawei hu | Towards Data Science", "h1": "The Transformer Isn\u2019t As Hard To Understand As You Might Think", "description": "Transformer\u2019s architecture has been the cornerstone for the development of many of the latest SOTA NLP models. It mainly relies on a mechanism called attention. Unlike other successful models that\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.youtube.com/channel/UCupQLyNchb9-2Z5lmUOIijw", "anchor_text": "free course", "paragraph_index": 33}], "all_paragraphs": ["Transformer\u2019s architecture has been the cornerstone for the development of many of the latest SOTA NLP models. It mainly relies on a mechanism called attention. Unlike other successful models that came before, it has no involvement with convolutional or recurrent layers what so ever.", "If you\u2019re new to this model, chances are you won\u2019t find this architecture to be easiest to understand. If that\u2019s the case, I hope this article can help.", "We\u2019ll start the explanation with how a regular encoder-decoder network works and what difficulties it may encounter, what is an attention mechanism used for in a regular encoder-decoder architecture, and finally, how it\u2019s used in Transformers.", "The image on the left shows an Encoder-Decoder architecture, with both components composed of recurrent layers.", "On the left, the encoder takes in the input sentences with each word represented by their embeddings and is expected to output a good summary for the input sentences. This summary is known as the context vector(the arrow connecting the two) and is fed to the decoder as its initial state.", "The decoder on the right is in charge of outputting translations, one word per step. During training, it takes as inputs the target sentences. When making a prediction, it feeds itself with its output from the last step(as shown here).", "This architecture is preferred over a simpler sequence-to-sequence RNN because the context vector from the encoder provides more direct access to the input sentence as a whole. Consequently, the decoder gets to look at the entire sentence via the context vector before outputting a single translation, while a regular sequence-to-sequence RNN only has access to words located before the current time step.", "In all recurrent layers, information is passed via hidden states from time step to time step, and they are gradually \u2018forgotten\u2019 as more time steps are performed. When encoding a long sequence, the output context vector is likely to have forgotten much of the information about the first components of the sentence. The same applies to the decoder, the information contained in the context vector won\u2019t pass down to the last few time steps if the sequence is too long.", "To tackle this memory issue and improve model performance, Attention Mechanism, bidirectional recurrent layers, along with some modifications on the model structure are introduced.", "The encoder is now composed of bidirectional LSTM/GRU so the input sentence is read in both directions. Aside from the context vector, the encoder now also outputs a sequence composed of encodings of each input word from each encoding step. The encoded sequence is passed entirely to the decoder at every time step. Since the decoder outputs one word at the time, the attention mechanism is used to select the right parts of the encoded sequence to process, together with the encoder\u2019s own inputs.", "The image below illustrates that, though the same encoder output is used, different time steps pay different levels of attention to different parts of the encoder sequence.", "This selection is done by performing a weighted sum of the encoder sequence with a set of attention weights, composed of floating-points that sum up to 1. For a given time step, there are only a few parts of the encoded sequence worth paying attention to, and a different set of weights is used for each step.", "These attention weights are learnable parameters and are determined by measuring the compatibility of each element in the encoder sequence S, with the decoder\u2019s last hidden state h. The more similar a vector from the encoder sequence is to this hidden state, the bigger its corresponding attention weight will be.", "The dot product between h and S measures how similar each vector in S is to h, and by applying softmax we turn the results into a distribution. The whole process can be seen as if the model is using the hidden state h as a query to find the most similar elements in S. To retrieve them, multiply S by the obtained attention weights.", "So far attentions are used by the decoder to map its hidden states and the encoder outputs to find relevant parts to attend to. In a Transformer, this mechanism is also used by its encoder to find relevant information from a sentence concerning the sentence itself. In other words, for a given word in a sentence, more context is taken into account when producing its encodings. This is called self-attention.", "This image illustrates self-attention in a figurative way. The relevance of each word to the word \u2018it\u2019 is represented by the strength of the line connecting them.", "In this case, not all words have the same relevance to the word \u2018it\u2019, different words from the sentence have a different level of importance to this word, including the very \u2018it\u2019.", "This is what self-attention does, the encoding of each word contains its relationship and other information concerning all the words in a sentence. This can provide the encoder with the ability to better understand the underlying syntactic and semantic meanings of words.", "The equation used to perform self-attention is the same as the one shown above, except now instead of using hidden state h, it uses the sentence itself as a query, and the value inside softmax is scaled to avoid softmax from having negligible gradients.", "Under the hood, attention is simply a tool for the model to perform lookups, mapping a Query with a set of Key-Value pair to an output. A component from Value is retrieved only if its corresponding key matches with the query.", "In the early case, Q was the hidden state h and both K and V were the encoder output S. S and h are used to find attention weights \u237a, and they are then used to retrieve the matching components from S by performing \u237a \u22c5 S.", "Due to the sequential nature of the recurrent layers, we cannot pack all the hidden states in a matrix and find all the attention weights at one shot. Attention weights were found at each time step.", "To perform self-attention, all the values Q, K, and V are the same list of words from the input sentence so the comparison is made for words with themselves. Unlike the previous case, all attention weights for all words can now be processed in a single shot, given the fact that it\u2019s not restricted by the sequential nature of the recurrent layers.", "Now, let\u2019s take look at the Transformer architecture, there\u2019s much more to be dug.", "The model is composed, again, of an encoder-decoder structure and each is in turn composed of N(=6) encoder/decoder blocks stacked on one another.", "Each encoder/decoder block passes its output to the next one and the output from the last encoding block is passed to each of the N decoding blocks.", "There are different sub-layers in each of the encoder/decoder blocks: Skip connections with layer normalization layers, a \u2018Feed Forward\u2019 module composed of two dense layers, and lastly, a Multi-Head Attention module. A Masked Multi-Head Attention is added at the bottom of each decoder block, with slightly different behavior. The main goal here is then to understand how the Multi-Head Attention operates and how attentions are performed within them.", "As you can see, there are no convolutional or recurrent layers in this model, the entire architecture relies solely on attention mechanisms.", "In essence, a Multi-Head Attention module is simply a number of scaled dot-product attentions working in parallel. For each attention, inputs Q, K, and V are first projected to different subspaces using different weight matrices Wq, Wk, and Wv. In this way, each attention gets to process a different aspect of the inputs. Their outputs are then combined linearly.", "Its behavior in the encoder is quite simple, it performs self-attention with Q, K, and V all being the same list of words in the input sentence. Each encoder block passes its output to the next one, and the last output is fed to all decoder blocks.", "For the decoder, the Masked Multi-Head Attention applies self-attention to the target sentences(during training), with future words being masked out, preventing any word to compare itself to the ones located after it. At inference time, the decoder only has access to its last output, so no masking is needed. The outputs from this module are passed as Q to the upper Multi-Head Attention, which receives encodings from the encoder and uses them as K and V. Its output is recombined with the output from the Masked attention module and is passed down for further processing.", "Finally, the decoder outputs are passed to a dense layer, and the softmax at the end outputs its translation for the current step.", "That\u2019s it for this article. If you like the content, check out some of my other pieces. If there\u2019s any errors, it\u2019d be great to see them in the comments.", "PS: Alfredo has a free course on Deep Learning. Go check it out if you\u2019re interested and tell me how bad it is!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Software Engineer @ Qbeast. I write to clarify and remember."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fdf5d4fcecdf8&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknocking-on-transformers-door-attention-mechanism-explained-intuitively-df5d4fcecdf8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknocking-on-transformers-door-attention-mechanism-explained-intuitively-df5d4fcecdf8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknocking-on-transformers-door-attention-mechanism-explained-intuitively-df5d4fcecdf8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknocking-on-transformers-door-attention-mechanism-explained-intuitively-df5d4fcecdf8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----df5d4fcecdf8--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----df5d4fcecdf8--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@jiaweihu08?source=post_page-----df5d4fcecdf8--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@jiaweihu08?source=post_page-----df5d4fcecdf8--------------------------------", "anchor_text": "jiawei hu"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc71c1fba8a3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknocking-on-transformers-door-attention-mechanism-explained-intuitively-df5d4fcecdf8&user=jiawei+hu&userId=c71c1fba8a3&source=post_page-c71c1fba8a3----df5d4fcecdf8---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdf5d4fcecdf8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknocking-on-transformers-door-attention-mechanism-explained-intuitively-df5d4fcecdf8&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdf5d4fcecdf8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknocking-on-transformers-door-attention-mechanism-explained-intuitively-df5d4fcecdf8&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.pexels.com/@maumascaro?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels", "anchor_text": "Maur\u00edcio Mascaro"}, {"url": "https://www.pexels.com/photo/person-holding-magnifying-glass-712786/?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels", "anchor_text": "Pexels"}, {"url": "https://6chaoran.wordpress.com/2019/01/15/build-a-machine-translator-using-keras-part-1-seq2seq-with-lstm/", "anchor_text": "source"}, {"url": "https://blog.floydhub.com/attention-mechanism/", "anchor_text": "source"}, {"url": "https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html", "anchor_text": "source"}, {"url": "http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf", "anchor_text": "original paper"}, {"url": "http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf", "anchor_text": "original paper"}, {"url": "https://www.youtube.com/channel/UCupQLyNchb9-2Z5lmUOIijw", "anchor_text": "free course"}, {"url": "https://towardsdatascience.com/an-overview-for-text-representations-in-nlp-311253730af1", "anchor_text": "An Overview for Text Representations in NLPA discussion of three of the most used input types in NLP.towardsdatascience.com"}, {"url": "https://towardsdatascience.com/ensemble-learning-from-scratch-20672123e6ca", "anchor_text": "Ensemble Learning from ScratchIntroducing Ensemble Learning, a powerful tool to improve performance by combining trained models.towardsdatascience.com"}, {"url": "https://medium.com/tag/nlp?source=post_page-----df5d4fcecdf8---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----df5d4fcecdf8---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/ai?source=post_page-----df5d4fcecdf8---------------ai-----------------", "anchor_text": "AI"}, {"url": "https://medium.com/tag/data-science?source=post_page-----df5d4fcecdf8---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fdf5d4fcecdf8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknocking-on-transformers-door-attention-mechanism-explained-intuitively-df5d4fcecdf8&user=jiawei+hu&userId=c71c1fba8a3&source=-----df5d4fcecdf8---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fdf5d4fcecdf8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknocking-on-transformers-door-attention-mechanism-explained-intuitively-df5d4fcecdf8&user=jiawei+hu&userId=c71c1fba8a3&source=-----df5d4fcecdf8---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdf5d4fcecdf8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknocking-on-transformers-door-attention-mechanism-explained-intuitively-df5d4fcecdf8&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----df5d4fcecdf8--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fdf5d4fcecdf8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknocking-on-transformers-door-attention-mechanism-explained-intuitively-df5d4fcecdf8&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----df5d4fcecdf8---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----df5d4fcecdf8--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----df5d4fcecdf8--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----df5d4fcecdf8--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----df5d4fcecdf8--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----df5d4fcecdf8--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----df5d4fcecdf8--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----df5d4fcecdf8--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----df5d4fcecdf8--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@jiaweihu08?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@jiaweihu08?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "jiawei hu"}, {"url": "https://medium.com/@jiaweihu08/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "59 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc71c1fba8a3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknocking-on-transformers-door-attention-mechanism-explained-intuitively-df5d4fcecdf8&user=jiawei+hu&userId=c71c1fba8a3&source=post_page-c71c1fba8a3--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F5b5f5226e02b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknocking-on-transformers-door-attention-mechanism-explained-intuitively-df5d4fcecdf8&newsletterV3=c71c1fba8a3&newsletterV3Id=5b5f5226e02b&user=jiawei+hu&userId=c71c1fba8a3&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}