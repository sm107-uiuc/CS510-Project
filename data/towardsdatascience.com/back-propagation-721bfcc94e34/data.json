{"url": "https://towardsdatascience.com/back-propagation-721bfcc94e34", "time": 1683004747.410832, "path": "towardsdatascience.com/back-propagation-721bfcc94e34/", "webpage": {"metadata": {"title": "Backpropagation Algorithm: Step by Step mathematical guide | By Ria K. | Towards Data Science", "h1": "Yes, you should listen to Andrej Karpathy, and understand Backpropagation", "description": "Backprop mechanism helps us propagate loss/error in the reverse direction, from output to input, using gradient descent for training models in Machine Learning."}, "outgoing_paragraph_urls": [{"url": "https://www.youtube.com/watch?v=d14TUNcbn1k&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&index=4", "anchor_text": "computational graph", "paragraph_index": 3}, {"url": "https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant", "anchor_text": "Jacobian matrices", "paragraph_index": 6}, {"url": "https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b", "anchor_text": "article", "paragraph_index": 13}, {"url": "https://medium.com/@ria.kulshrestha16", "anchor_text": "here", "paragraph_index": 17}, {"url": "https://twitter.com/ree_____ree", "anchor_text": "Twitter", "paragraph_index": 18}], "all_paragraphs": ["If you ask anyone around, they\u2019ll tell you that AI is the hottest and the fastest developing field out there. We have made tremendous progress in supervised learning \u2014 both Natural Language Processing and Computer Vision. These two fields solve very different problems and do not share a lot of common literature. Models that excel in one do not perform at par in the other. But deep down they do share something common, that is backpropagation\u2014 the mechanism that helps us propagate loss in a reverse direction, from output to input \u2014 plays a very crucial and critical role in the training phase of a model.", "But first let us see what every supervised learning model will have:", "So where does back propagation fit in the picture here? It connects the loss function and the optimizer. The loss is calculated at the output layer, then we use back propagation to propagate the loss back \u2014 in form of its gradients \u2014 all the way back to the inputs in the input layer(hence the name back propagation). Once back prop is done, we know the gradient of the loss at each node, then we use the optimizer to update the weights in an attempt to minimize the loss in the next iteration. Back prop is a local", "But how to do we calculate it using the computational graph above?", "The only thing that changes here is the calculation happening at each node. Rather than a simple multiplication operation, each nodes have an activation function, Y =H(X1, X2,\u2026,Xi). In the forward pass itself \u2014 when we calculate Y \u2014 we will also calculate and store the local gradients \u2202Y/\u2202X1, \u2202Y/\u2202X2, \u2202Y/\u2202X? and \u2202Y/\u2202Xi.", "When we get the upstream gradient in the back propagation, we can simply multiply it with the local gradient corresponding to each input and pass it back. In the above example we get the upstream gradient from 2 nodes, so the total gradient received by the green node is simply the addition of all the upstream gradients \u2014 in this case two upstream gradients. Then we multiply each local gradient with the total upstream gradient and pass it backward to the corresponding input.", "In Neural Networks, the inputs X and output of a node are vectors. The function H is a matrix multiplication operation. Y =H(X) = W*X, where W is our weight matrix. The local gradients are Jacobian matrices \u2014 differential of each element of Y wrt each element of X).", "In practice, an input vector Xi only affects Fi in the output vector.", "Things get a little tricky in RNNs because unlike NNs, where the output and inputs of a node are independent of each other, the output of the current step is fed as an input to the same node in the next step. Theoretically, it doesn\u2019t make any difference in our back propagation flow, but in practice, it leads to the infamous vanishing gradient and exploding gradient problem.", "Consider the vanilla RNN above, in the forward pass for each time step, F(xi) = W*xi where xi is the input at ith time step and W is the weight matrix, W remains the same for each step. During the back propagation, the local gradient for each node is say h(xi). The gradient reaching our input node will be a L*h(xt)*h(xt-1)*..*h(x1). If all h(xi) greater than k then gradient at input is greater than L*k^t. For deep RNNs t can be large and in such cases if k>1, the gradients keep on increasing exponentially causing the exploding gradient problem where the gradients become too large.Similarly in the cases where all h(xi) < k, gradient at input node will be less then L*K^t and if k<1, the gradients will almost become 0 by the time they reach the input layer \u2014 the vanishing gradient problem where gradients become too small to cause any meaningful updates.", "Since F(xi) is essentially a matrix multiplication, h(xi) = F\u2019(xi) which is~ W. For an RNN having t time-steps, during back propagation, local gradient would be h(xi ) i.e the weight matrix \u2014 or a part of it \u2014 and at each step, when we calculate localGradient*upstreamGradient it will get multiplied with itself again and again as upstream gradients is a recursive product of some local gradient \u2014 which is essentially W or a part of it \u2014 and its corresponding upstream gradient at each previous time step. Depending on the values of elements in W, we will face the either of the gradient problems, the elements greater than one would explode and smaller elements would vanish.", "Things are a bit different in CNNs that the rest of the cases but the basic concept remains the same. We will still calculate the gradient by multiplying upstream and local gradients, but things are a bit convoluted in this case. In the forwards pass, the inputs here don\u2019t contribute to all the outputs like in NN, at the same time, they don\u2019t just contribute to a single output as in an RNN. In the forward pass, as shown in the following figure, each outputs always takes contribution from four different inputs, while the number of outputs an input can contribute to depends on its position.", "In practice, we can calculate the gradients by rotating the kernel by 180 degrees and performing a full convolution on the output matrix using the rotated kernel.", "In his article about back propagation, Andrej Karpathy described it as follows:", "Backpropagation is a leaky abstraction; it is a credit assignment scheme with non-trivial consequences. If you try to ignore how it works under the hood because \u201cTensorFlow automagically makes my networks learn\u201d, you will not be ready to wrestle with the dangers it presents, and you will be much less effective at building and debugging neural networks.", "While it is probable that you can train a ML model and achieve good results without knowing about back propagation, but understanding it will give you an upper-hand in training neural networks that will enable you to get a better understanding of working and hence improving its performance.", "I\u2018m glad you made it till the end of this article. \ud83d\udc4fI hope your reading experience was as enriching as the one I had writing this.\ud83d\udc96", "Do check out my other articles here.", "If you want to reach out to me, my medium of choice would be Twitter.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F721bfcc94e34&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fback-propagation-721bfcc94e34&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fback-propagation-721bfcc94e34&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fback-propagation-721bfcc94e34&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fback-propagation-721bfcc94e34&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----721bfcc94e34--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----721bfcc94e34--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://ria-kulshrestha.medium.com/?source=post_page-----721bfcc94e34--------------------------------", "anchor_text": ""}, {"url": "https://ria-kulshrestha.medium.com/?source=post_page-----721bfcc94e34--------------------------------", "anchor_text": "Ria Kulshrestha"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F406aa3cbd38e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fback-propagation-721bfcc94e34&user=Ria+Kulshrestha&userId=406aa3cbd38e&source=post_page-406aa3cbd38e----721bfcc94e34---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F721bfcc94e34&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fback-propagation-721bfcc94e34&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F721bfcc94e34&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fback-propagation-721bfcc94e34&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@roman_lazygeek?utm_source=medium&utm_medium=referral", "anchor_text": "Roman Mager"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://www.youtube.com/watch?v=d14TUNcbn1k&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&index=4", "anchor_text": "computational graph"}, {"url": "https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant", "anchor_text": "Source"}, {"url": "https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant", "anchor_text": "Jacobian matrices"}, {"url": "https://medium.com/@2017csm1006/forward-and-backpropagation-in-convolutional-neural-network-4dfa96d7b37e", "anchor_text": "Reference"}, {"url": "https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b", "anchor_text": "article"}, {"url": "https://www.jefkine.com/general/2016/09/05/backpropagation-in-convolutional-neural-networks/", "anchor_text": "DeepGrid"}, {"url": "https://medium.com/@2017csm1006/forward-and-backpropagation-in-convolutional-neural-network-4dfa96d7b37e", "anchor_text": "https://medium.com/@2017csm1006/forward-and-backpropagation-in-convolutional-neural-network-4dfa96d7b37e"}, {"url": "http://cs231n.github.io/optimization-2/", "anchor_text": "Stanford notes"}, {"url": "https://www.youtube.com/watch?v=d14TUNcbn1k&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&index=4", "anchor_text": "here"}, {"url": "https://medium.com/@ria.kulshrestha16", "anchor_text": "here"}, {"url": "https://twitter.com/ree_____ree", "anchor_text": "Twitter"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----721bfcc94e34---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/backpropagation?source=post_page-----721bfcc94e34---------------backpropagation-----------------", "anchor_text": "Backpropagation"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----721bfcc94e34---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/ai?source=post_page-----721bfcc94e34---------------ai-----------------", "anchor_text": "AI"}, {"url": "https://medium.com/tag/nlp?source=post_page-----721bfcc94e34---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F721bfcc94e34&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fback-propagation-721bfcc94e34&user=Ria+Kulshrestha&userId=406aa3cbd38e&source=-----721bfcc94e34---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F721bfcc94e34&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fback-propagation-721bfcc94e34&user=Ria+Kulshrestha&userId=406aa3cbd38e&source=-----721bfcc94e34---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F721bfcc94e34&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fback-propagation-721bfcc94e34&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----721bfcc94e34--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F721bfcc94e34&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fback-propagation-721bfcc94e34&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----721bfcc94e34---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----721bfcc94e34--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----721bfcc94e34--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----721bfcc94e34--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----721bfcc94e34--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----721bfcc94e34--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----721bfcc94e34--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----721bfcc94e34--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----721bfcc94e34--------------------------------", "anchor_text": ""}, {"url": "https://ria-kulshrestha.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://ria-kulshrestha.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ria Kulshrestha"}, {"url": "https://ria-kulshrestha.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "768 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F406aa3cbd38e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fback-propagation-721bfcc94e34&user=Ria+Kulshrestha&userId=406aa3cbd38e&source=post_page-406aa3cbd38e--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fccdcb0ec19a4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fback-propagation-721bfcc94e34&newsletterV3=406aa3cbd38e&newsletterV3Id=ccdcb0ec19a4&user=Ria+Kulshrestha&userId=406aa3cbd38e&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}