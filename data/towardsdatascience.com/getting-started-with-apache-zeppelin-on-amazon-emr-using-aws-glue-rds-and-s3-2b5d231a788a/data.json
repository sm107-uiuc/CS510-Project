{"url": "https://towardsdatascience.com/getting-started-with-apache-zeppelin-on-amazon-emr-using-aws-glue-rds-and-s3-2b5d231a788a", "time": 1683002148.97823, "path": "towardsdatascience.com/getting-started-with-apache-zeppelin-on-amazon-emr-using-aws-glue-rds-and-s3-2b5d231a788a/", "webpage": {"metadata": {"title": "Getting Started with Apache Zeppelin on Amazon EMR, using AWS Glue, RDS, and S3 | by Gary A. Stafford | Towards Data Science", "h1": "Getting Started with Apache Zeppelin on Amazon EMR, using AWS Glue, RDS, and S3", "description": "Explore the use of Apache Zeppelin on Amazon EMR for data analytics and data science, using a series of pre-made Zeppelin notebooks."}, "outgoing_paragraph_urls": [{"url": "https://searchbusinessanalytics.techtarget.com/definition/big-data-analytics", "anchor_text": "big data analytics", "paragraph_index": 0}, {"url": "https://en.wikipedia.org/wiki/Data_science", "anchor_text": "data science", "paragraph_index": 0}, {"url": "https://en.wikipedia.org/wiki/Artificial_intelligence", "anchor_text": "artificial intelligence", "paragraph_index": 0}, {"url": "https://en.wikipedia.org/wiki/Machine_learning", "anchor_text": "machine learning", "paragraph_index": 0}, {"url": "https://aws.amazon.com/emr/", "anchor_text": "Amazon Elastic MapReduce", "paragraph_index": 1}, {"url": "https://hadoop.apache.org/", "anchor_text": "Apache Hadoop", "paragraph_index": 1}, {"url": "https://spark.apache.org/", "anchor_text": "Apache Spark", "paragraph_index": 1}, {"url": "https://hadoop.apache.org/", "anchor_text": "Apache Hadoop", "paragraph_index": 1}, {"url": "https://aws.amazon.com/sagemaker/", "anchor_text": "SageMaker", "paragraph_index": 1}, {"url": "https://zeppelin.apache.org/", "anchor_text": "Apache Zeppelin", "paragraph_index": 2}, {"url": "http://jupyter.org/", "anchor_text": "Project Jupyter", "paragraph_index": 2}, {"url": "https://medium.com/netflix-techblog/open-sourcing-polynote-an-ide-inspired-polyglot-notebook-7f929d3f447", "anchor_text": "Netflix\u2019s Polynote", "paragraph_index": 2}, {"url": "https://zeppelin.apache.org/docs/0.8.2/usage/interpreter/overview.html", "anchor_text": "interpreters", "paragraph_index": 2}, {"url": "https://aws.amazon.com/glue/", "anchor_text": "AWS Glue", "paragraph_index": 3}, {"url": "https://aws.amazon.com/rds/", "anchor_text": "Amazon Relational Database Service", "paragraph_index": 3}, {"url": "https://aws.amazon.com/s3/", "anchor_text": "Amazon Simple Cloud Storage Service", "paragraph_index": 3}, {"url": "https://aws.amazon.com/big-data/datalakes-and-analytics/what-is-a-data-lake/", "anchor_text": "Data Lake", "paragraph_index": 3}, {"url": "https://zeppelin.apache.org/supported_interpreters.html", "anchor_text": "interpreters", "paragraph_index": 3}, {"url": "https://spark.apache.org/", "anchor_text": "Apache Spark", "paragraph_index": 3}, {"url": "https://spark.apache.org/sql/", "anchor_text": "Spark SQL", "paragraph_index": 3}, {"url": "https://spark.apache.org/docs/2.4.4/api/python/index.html", "anchor_text": "PySpark", "paragraph_index": 3}, {"url": "https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-overview-arch.html#emr-arch-storage", "anchor_text": "Hadoop Distributed File System", "paragraph_index": 4}, {"url": "https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-overview-arch.html", "anchor_text": "EMR architecture", "paragraph_index": 4}, {"url": "https://www.zepl.com/explore", "anchor_text": "Notebook Explorer", "paragraph_index": 5}, {"url": "https://www.zepl.com/", "anchor_text": "Zepl", "paragraph_index": 5}, {"url": "https://www.linkedin.com/in/moonsoo-lee-4982a511/", "anchor_text": "Moonsoo Lee", "paragraph_index": 5}, {"url": "https://www.zepl.com/viewer/github/garystafford/zeppelin-emr-demo/blob/master/2ERVVKTCG/note.json", "anchor_text": "first notebook", "paragraph_index": 6}, {"url": "https://www.kaggle.com/", "anchor_text": "kaggle", "paragraph_index": 6}, {"url": "https://www.kaggle.com/sulmansarwar/transactions-from-a-bakery", "anchor_text": "Transactions from a Bakery", "paragraph_index": 6}, {"url": "https://zeppelin.apache.org/docs/0.8.2/development/helium/overview.html", "anchor_text": "Helium", "paragraph_index": 6}, {"url": "https://parquet.apache.org/", "anchor_text": "Apache Parquet", "paragraph_index": 6}, {"url": "https://www.zepl.com/viewer/github/garystafford/zeppelin-emr-demo/blob/master/2EUZKQXX7/note.json", "anchor_text": "second notebook", "paragraph_index": 7}, {"url": "https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-overview.html#emr-overview-clusters", "anchor_text": "single-node", "paragraph_index": 7}, {"url": "https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-master-core-task-nodes.html", "anchor_text": "multi-node", "paragraph_index": 7}, {"url": "https://grouplens.org/datasets/movielens/", "anchor_text": "MovieLens", "paragraph_index": 7}, {"url": "https://aws.amazon.com/ec2/instance-types/", "anchor_text": "Amazon EC2 Instance Types", "paragraph_index": 7}, {"url": "https://www.zepl.com/viewer/github/garystafford/zeppelin-emr-demo/blob/master/2ERYY923A/note.json", "anchor_text": "third notebook", "paragraph_index": 8}, {"url": "https://aws.amazon.com/emr/", "anchor_text": "Amazon EMR", "paragraph_index": 8}, {"url": "https://aws.amazon.com/glue/", "anchor_text": "AWS Glue", "paragraph_index": 8}, {"url": "https://docs.aws.amazon.com/glue/latest/dg/components-overview.html#data-catalog-intro", "anchor_text": "Data Catalog", "paragraph_index": 8}, {"url": "https://hive.apache.org/", "anchor_text": "Apache Hive", "paragraph_index": 8}, {"url": "https://cwiki.apache.org/confluence/display/Hive/Design#Design-Metastore", "anchor_text": "metastore", "paragraph_index": 8}, {"url": "https://spark.apache.org/sql/", "anchor_text": "Spark SQL", "paragraph_index": 8}, {"url": "https://aws.amazon.com/big-data/datalakes-and-analytics/what-is-a-data-lake/", "anchor_text": "Data Lake", "paragraph_index": 8}, {"url": "https://docs.aws.amazon.com/glue/latest/dg/add-crawler.html", "anchor_text": "AWS Glue Crawlers", "paragraph_index": 8}, {"url": "https://www.zepl.com/viewer/github/garystafford/zeppelin-emr-demo/blob/master/2ESH8DGFS/note.json", "anchor_text": "fourth notebook", "paragraph_index": 9}, {"url": "https://zeppelin.apache.org/docs/0.8.2/interpreter/jdbc.html#overview", "anchor_text": "external data source", "paragraph_index": 9}, {"url": "https://aws.amazon.com/rds/postgresql/", "anchor_text": "Amazon RDS PostgreSQL", "paragraph_index": 9}, {"url": "http://initd.org/psycopg/", "anchor_text": "Psycopg 2", "paragraph_index": 9}, {"url": "https://zeppelin.apache.org/docs/0.8.2/interpreter/jdbc.html", "anchor_text": "JDBC Interpreter", "paragraph_index": 9}, {"url": "https://github.com/garystafford/zeppelin-emr-demo", "anchor_text": "zeppelin-emr-demo", "paragraph_index": 11}, {"url": "https://zeppelin.apache.org/docs/0.8.2/setup/storage/storage.html#notebook-storage-options-for-apache-zeppelin", "anchor_text": "pluggable notebook storage mechanisms", "paragraph_index": 11}, {"url": "https://github.com/settings/tokens", "anchor_text": "GitHub personal access token", "paragraph_index": 14}, {"url": "https://github.com/garystafford/zeppelin-emr-config", "anchor_text": "zeppelin-emr-config", "paragraph_index": 15}, {"url": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html", "anchor_text": "EC2 key pair", "paragraph_index": 17}, {"url": "https://aws.amazon.com/cli/", "anchor_text": "AWS CLI", "paragraph_index": 17}, {"url": "https://github.com/garystafford/zeppelin-emr-config/blob/master/bootstrap/bootstrap.sh", "anchor_text": "bootstrap.sh", "paragraph_index": 18}, {"url": "https://github.com/garystafford/zeppelin-emr-config/blob/master/bootstrap/helium.json", "anchor_text": "helium.json", "paragraph_index": 18}, {"url": "https://github.com/garystafford/zeppelin-emr-config/blob/master/sql/ratings.sql", "anchor_text": "ratings.sql", "paragraph_index": 18}, {"url": "https://aws.amazon.com/cloudformation/", "anchor_text": "AWS CloudFormation", "paragraph_index": 20}, {"url": "https://aws.amazon.com/emr/faqs/?nc=sn&loc=7#Billing", "anchor_text": "compute costs", "paragraph_index": 20}, {"url": "https://github.com/garystafford/zeppelin-emr-config/blob/master/cloudformation/emr_single_node.yml", "anchor_text": "emr_single_node.yml", "paragraph_index": 26}, {"url": "https://github.com/garystafford/zeppelin-emr-config/blob/master/bootstrap/bootstrap.sh", "anchor_text": "bootstrap.sh", "paragraph_index": 26}, {"url": "https://jdbc.postgresql.org/", "anchor_text": "PostgreSQL driver JAR", "paragraph_index": 26}, {"url": "https://github.com/garystafford/zeppelin-emr-demo", "anchor_text": "zeppelin-emr-demo", "paragraph_index": 26}, {"url": "https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-release-5x.html", "anchor_text": "5.28.0", "paragraph_index": 28}, {"url": "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_PostgreSQL.html", "anchor_text": "Amazon RDS PostgreSQL", "paragraph_index": 31}, {"url": "https://github.com/garystafford/zeppelin-emr-config/blob/master/cloudformation/rds_postgres.yml", "anchor_text": "rds_postgres.yml", "paragraph_index": 31}, {"url": "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.DBInstanceClass.html", "anchor_text": "RDS-supported instance type", "paragraph_index": 31}, {"url": "https://github.com/garystafford/zeppelin-emr-config/blob/master/cloudformation/crawler.yml", "anchor_text": "crawler.yml", "paragraph_index": 34}, {"url": "https://github.com/garystafford/zeppelin-emr-config/blob/master/bootstrap/bootstrap.sh#L19-L20", "anchor_text": "bootstrap.sh", "paragraph_index": 42}, {"url": "https://zeppelin.apache.org/docs/0.8.0/development/helium/writing_visualization_basic.html", "anchor_text": "Zeppelin website", "paragraph_index": 45}, {"url": "https://docs.npmjs.com/about-packages-and-modules", "anchor_text": "npm package", "paragraph_index": 45}, {"url": "https://www.npmjs.com/package/ultimate-pie-chart", "anchor_text": "ultimate-pie-chart", "paragraph_index": 45}, {"url": "https://github.com/garystafford/zeppelin-emr-config/blob/master/bootstrap/helium.json", "anchor_text": "helium.json", "paragraph_index": 46}, {"url": "https://zeppelin.apache.org/docs/0.8.2/interpreter/jdbc.html", "anchor_text": "Zeppelin JDBC Interpreter", "paragraph_index": 48}, {"url": "https://en.wikipedia.org/wiki/Java_Database_Connectivity", "anchor_text": "Java Database Connectivity", "paragraph_index": 49}, {"url": "https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-web-interfaces.html", "anchor_text": "setup SSH tunneling", "paragraph_index": 53}, {"url": "https://getfoxyproxy.org/", "anchor_text": "FoxyProxy", "paragraph_index": 54}, {"url": "https://zeppelin.apache.org/docs/0.8.2/setup/security/shiro_authentication.html", "anchor_text": "Shiro Authentication", "paragraph_index": 55}, {"url": "https://github.com/garystafford/zeppelin-emr-config/blob/master/cloudformation/emr_single_node.yml#L67", "anchor_text": "emr_single_node.yml", "paragraph_index": 56}, {"url": "https://github.com/garystafford/zeppelin-emr-config/blob/master/bootstrap/helium.json", "anchor_text": "helium.json", "paragraph_index": 57}, {"url": "https://github.com/garystafford/zeppelin-emr-config/blob/master/bootstrap/bootstrap.sh#L27", "anchor_text": "bootstrap.sh", "paragraph_index": 61}, {"url": "https://www.zepl.com/viewer/github/garystafford/zeppelin-emr-demo/blob/master/2ERVVKTCG/note.json", "anchor_text": "first notebook", "paragraph_index": 65}, {"url": "https://www.kaggle.com/", "anchor_text": "kaggle", "paragraph_index": 65}, {"url": "https://www.kaggle.com/sulmansarwar/transactions-from-a-bakery", "anchor_text": "Transactions from a Bakery", "paragraph_index": 65}, {"url": "https://zeppelin.apache.org/docs/0.8.2/development/helium/overview.html", "anchor_text": "Helium", "paragraph_index": 65}, {"url": "https://parquet.apache.org/", "anchor_text": "Apache Parquet", "paragraph_index": 65}, {"url": "https://www.npmjs.com/package/ultimate-pie-chart", "anchor_text": "Ultimate Pie Chart", "paragraph_index": 68}, {"url": "http://spark.apache.org/sql/", "anchor_text": "Spark SQL", "paragraph_index": 69}, {"url": "https://spark.apache.org/docs/2.4.4/sql-programming-guide.html", "anchor_text": "Spark DataFrame", "paragraph_index": 69}, {"url": "https://parquet.apache.org/", "anchor_text": "Apache Parquet", "paragraph_index": 70}, {"url": "https://docs.aws.amazon.com/AmazonS3/latest/dev/selecting-content-from-objects.html", "anchor_text": "Amazon S3 Select", "paragraph_index": 72}, {"url": "https://aws.amazon.com/s3/features/#s3-select", "anchor_text": "query in place", "paragraph_index": 72}, {"url": "https://www.zepl.com/viewer/github/garystafford/zeppelin-emr-demo/blob/master/2EUZKQXX7/note.json", "anchor_text": "second notebook", "paragraph_index": 75}, {"url": "https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-overview.html#emr-overview-clusters", "anchor_text": "single-node", "paragraph_index": 75}, {"url": "https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-master-core-task-nodes.html", "anchor_text": "multi-node", "paragraph_index": 75}, {"url": "https://grouplens.org/datasets/movielens/", "anchor_text": "MovieLens", "paragraph_index": 75}, {"url": "https://aws.amazon.com/ec2/instance-types/", "anchor_text": "Amazon EC2 Instance Types", "paragraph_index": 75}, {"url": "https://aws.amazon.com/emr/faqs/?nc=sn&loc=7#Billing", "anchor_text": "compute costs", "paragraph_index": 76}, {"url": "https://aws.amazon.com/emr/faqs/?nc=sn&loc=7", "anchor_text": "normalization factor", "paragraph_index": 78}, {"url": "http://ganglia.sourceforge.net/", "anchor_text": "Ganglia", "paragraph_index": 82}, {"url": "https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hadoop-task-config.html#emr-hadoop-task-config-m5", "anchor_text": "default", "paragraph_index": 84}, {"url": "https://spark.apache.org/docs/latest/tuning.html", "anchor_text": "Spark", "paragraph_index": 85}, {"url": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/memory-optimized-instances.html", "anchor_text": "memory-optimized instances", "paragraph_index": 85}, {"url": "https://grouplens.org/datasets/movielens/", "anchor_text": "MovieLens", "paragraph_index": 86}, {"url": "https://www.zepl.com/viewer/github/garystafford/zeppelin-emr-demo/blob/master/2ERYY923A/note.json", "anchor_text": "third notebook", "paragraph_index": 88}, {"url": "https://aws.amazon.com/emr/", "anchor_text": "Amazon EMR", "paragraph_index": 88}, {"url": "https://aws.amazon.com/glue/", "anchor_text": "AWS Glue", "paragraph_index": 88}, {"url": "https://docs.aws.amazon.com/glue/latest/dg/components-overview.html#data-catalog-intro", "anchor_text": "Data Catalog", "paragraph_index": 88}, {"url": "https://hive.apache.org/", "anchor_text": "Apache Hive", "paragraph_index": 88}, {"url": "https://cwiki.apache.org/confluence/display/Hive/Design#Design-Metastore", "anchor_text": "metastore", "paragraph_index": 88}, {"url": "https://spark.apache.org/sql/", "anchor_text": "Spark SQL", "paragraph_index": 88}, {"url": "https://aws.amazon.com/big-data/datalakes-and-analytics/what-is-a-data-lake/", "anchor_text": "Data Lake", "paragraph_index": 88}, {"url": "https://docs.aws.amazon.com/glue/latest/dg/add-crawler.html", "anchor_text": "AWS Glue Crawlers", "paragraph_index": 88}, {"url": "https://docs.aws.amazon.com/athena/latest/ug/understanding-tables-databases-and-the-data-catalog.html", "anchor_text": "Amazon", "paragraph_index": 93}, {"url": "https://www.zepl.com/viewer/github/garystafford/zeppelin-emr-demo/blob/master/2ESH8DGFS/note.json", "anchor_text": "fourth notebook", "paragraph_index": 94}, {"url": "https://zeppelin.apache.org/docs/0.8.2/interpreter/jdbc.html#overview", "anchor_text": "external data source", "paragraph_index": 94}, {"url": "https://aws.amazon.com/rds/postgresql/", "anchor_text": "Amazon RDS PostgreSQL", "paragraph_index": 94}, {"url": "http://initd.org/psycopg/", "anchor_text": "Psycopg 2", "paragraph_index": 94}, {"url": "https://zeppelin.apache.org/docs/0.8.2/interpreter/jdbc.html", "anchor_text": "JDBC Interpreter", "paragraph_index": 94}, {"url": "https://grouplens.org/datasets/movielens/", "anchor_text": "MovieLens", "paragraph_index": 96}, {"url": "http://initd.org/psycopg/", "anchor_text": "Psycopg", "paragraph_index": 97}, {"url": "https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html", "anchor_text": "Spark documentation", "paragraph_index": 98}, {"url": "https://jdbc.postgresql.org/", "anchor_text": "PostgreSQL JDBC Driver", "paragraph_index": 98}, {"url": "https://jdbc.postgresql.org/", "anchor_text": "PostgreSQL JDBC Driver", "paragraph_index": 99}, {"url": "https://zeppelin.apache.org/docs/0.8.2/usage/dynamic_form/intro.html", "anchor_text": "Dynamic Forms", "paragraph_index": 101}], "all_paragraphs": ["There is little question big data analytics, data science, artificial intelligence (AI), and machine learning (ML), a subcategory of AI, have all experienced a tremendous surge in popularity over the last 3\u20135 years. Behind the hype cycles and marketing buzz, these technologies are having a significant influence on all aspects of our modern lives. Due to their popularity, commercial enterprises, academic institutions, and the public sector have all rushed to develop hardware and software solutions to decrease the barrier to entry and increase the velocity of ML and Data Scientists and Engineers.", "All three major cloud providers, Amazon Web Services (AWS), Microsoft Azure, and Google Cloud, have rapidly maturing big data analytics, data science, and AI and ML services. For example, AWS introduced Amazon Elastic MapReduce (EMR) in 2009, primarily as an Apache Hadoop-based big data processing service. Since then, according to Amazon, EMR has evolved into a service that uses Apache Spark, Apache Hadoop, and several other leading open-source frameworks to quickly and cost-effectively process and analyze vast amounts of data. More recently, in late 2017, Amazon released SageMaker, a service that provides the ability to build, train, and deploy machine learning models quickly and securely.", "Simultaneously, organizations are building solutions that integrate and enhance these Cloud-based big data analytics, data science, AI, and ML services. One such example is Apache Zeppelin. Similar to the immensely popular Project Jupyter and the newly open-sourced Netflix\u2019s Polynote, Apache Zeppelin is a web-based, polyglot, computational notebook. Zeppelin enables data-driven, interactive data analytics and document collaboration using a number of interpreters such as Scala, Python, Spark SQL, JDBC, Markdown, and Shell. Zeppelin is one of the core applications natively supported by Amazon EMR.", "In the following post, we will explore the use of Apache Zeppelin on EMR for data analytics and data science, using a series of Zeppelin notebooks. The notebooks feature the use of AWS Glue, the fully managed extract, transform, and load (ETL) service that makes it easy to prepare and load data for analytics. The notebooks also feature the use of Amazon Relational Database Service (RDS) for PostgreSQL and Amazon Simple Cloud Storage Service (S3). Amazon S3 will serve as a Data Lake to store our unstructured data. Given the current choice of Zeppelin\u2019s more than twenty different interpreters, we will use Python3 and Apache Spark, specifically Spark SQL and PySpark, for all notebooks.", "We will build an economical single-node EMR cluster for data exploration, as well as a larger multi-node EMR cluster for efficiently analyzing large data sets. Amazon S3 will be used to store input and output data, while intermediate results are stored in the Hadoop Distributed File System (HDFS) on the EMR cluster. Amazon provides a good overview of EMR architecture. Below is a high-level architectural diagram of the infrastructure we will construct for this demonstration.", "Below is a brief overview of each Zeppelin notebook with a link to view, using Zepl\u2019s free Notebook Explorer. Zepl was founded by the same engineers that developed Apache Zeppelin, including Moonsoo Lee, Zepl CTO and creator for Apache Zeppelin. Zepl\u2019s enterprise collaboration platform, built on Apache Zeppelin, enables both Data Science and AI/ML teams to collaborate around data.", "The first notebook uses a small 21k row kaggle dataset, Transactions from a Bakery. The notebook demonstrates Zeppelin\u2019s integration capabilities with the Helium plugin system for adding new chart types, the use of Amazon S3 for data storage and retrieval, and the use of Apache Parquet, a compressed and efficient columnar data storage format, and Zeppelin\u2019s storage integration with GitHub for notebook version control.", "The second notebook demonstrates the use of a single-node and multi-node Amazon EMR cluster for the exploration and analysis of public datasets ranging from approximately 100k rows up to 27MM rows, using Zeppelin. We will use the latest GroupLens MovieLens rating datasets to examine the performance characteristics of Zeppelin, using Spark, on single- verses multi-node EMR clusters for analyzing big data using a variety of Amazon EC2 Instance Types.", "The third notebook demonstrates Amazon EMR and Zeppelin\u2019s integration capabilities with an AWS Glue Data Catalog as an Apache Hive-compatible metastore for Spark SQL. We will create an Amazon S3-based Data Lake using the AWS Glue Data Catalog and a set of AWS Glue Crawlers.", "The fourth notebook demonstrates Zeppelin\u2019s ability to integrate with an external data source. In this case, we will interact with data in an Amazon RDS PostgreSQL relational database using three methods, including the Psycopg 2 PostgreSQL adapter for Python, Spark\u2019s native JDBC capability, and Zeppelin\u2019s JDBC Interpreter.", "First, as a DataOps Engineer, we will create and configure the AWS resources required to demonstrate the use of Apache Zeppelin on EMR, using an AWS Glue Data Catalog, Amazon RDS PostgreSQL database, and an S3-based data lake. Following the setup, as a Data Analyst, we will explore Apache Zeppelin\u2019s features and integration capabilities with a variety of AWS services using the pre-built Zeppelin notebooks.", "The demonstration\u2019s source code is contained in two public GitHub repositories. The first repository, zeppelin-emr-demo, includes the four pre-built Zeppelin notebooks, organized according to the conventions of Zeppelin\u2019s pluggable notebook storage mechanisms.", "During the demonstration, changes made to your copy of the Zeppelin notebooks running on EMR will be automatically pushed back to GitHub when a commit occurs. To accomplish this, instead of just cloning a local copy of the zeppelin-emr-demo project repository, you will want your own copy, within your personal GitHub account. You could fork the zeppelin-emr-demo GitHub repository or copy a clone into your own GitHub repository.", "To make a copy of the project in your own GitHub account, first, create a new empty repository on GitHub, for example, \u2018my-zeppelin-emr-demo-copy\u2019. Then, execute the following commands from your terminal, to clone the original project repository to your local environment, and finally, push it to your GitHub account.", "To automatically push changes to your GitHub repository when a commit occurs, Zeppelin will need a GitHub personal access token. Create a personal access token with the scope shown below. Be sure to keep the token secret. Make sure you do not accidentally check your token value into your source code on GitHub. To minimize the risk, immediately change or delete the token after completing the demo.", "The second repository, zeppelin-emr-config, contains the necessary bootstrap files, CloudFormation templates, and PostgreSQL DDL (Data Definition Language) SQL script.", "Use the following AWS CLI command to clone the GitHub repository to your local environment.", "To follow along with the demonstration, you will need an AWS Account, an existing Amazon S3 bucket to store EMR configuration and data, and an EC2 key pair. You will also need a current version of the AWS CLI installed in your work environment. Due to the particular EMR features we will be using, I recommend using the us-east-1 AWS Region to create the demonstration\u2019s resources.", "To start, copy three configuration files, bootstrap.sh, helium.json, and ratings.sql, from the zeppelin-emr-demo-setup project directory to our S3 bucket. Change the ZEPPELIN_DEMO_BUCKET variable value, then run the following s3 cp API commands, using the AWS CLI. The three files will be copied to a bootstrap directory within your S3 bucket.", "Below, sample output from copying local files to S3.", "We will start by creating most of the required AWS resources for the demonstration using three AWS CloudFormation templates. We will create a single-node Amazon EMR cluster, an Amazon RDS PostgresSQL database, an AWS Glue Data Catalog database, two AWS Glue Crawlers, and a Glue IAM Role. We will wait to create the multi-node EMR cluster due to the compute costs of running large EC2 instances in the cluster. You should understand the cost of these resources before proceeding, and that you ensure they are destroyed immediately upon completion of the demonstration to minimize your expenses.", "We will start by creating the single-node Amazon EMR cluster, consisting of just one master node with no core or task nodes (a cluster of one). All operations will take place on the master node.", "The following EMR instructions assume you have already created at least one EMR cluster in the past, in your current AWS Region, using the EMR web interface with the \u2018Create Cluster \u2014 Quick Options\u2019 option. Creating a cluster this way creates several additional AWS resources, such as the EMR_EC2_DefaultRole EC2 instance profile, the default EMR_DefaultRole EMR IAM Role, and the default EMR S3 log bucket.", "If you have not created any EMR clusters using the EMR \u2018Create Cluster \u2014 Quick Options\u2019 feature in the past, don\u2019t worry. You can also create the required resources with a few quick AWS CLI commands. Change the following LOG_BUCKET variable value, then run the aws emr and aws s3api API commands, using the AWS CLI. The LOG_BUCKET variable value follows the convention of aws-logs-awsaccount-region. For example, aws-logs-012345678901-us-east-1.", "The new EMR IAM Roles can be viewed in the IAM Roles web interface.", "Often, I see tutorials that reference these default EMR resources from the AWS CLI or CloudFormation, without any understanding or explanation of how they are created.", "As part of creating our EMR cluster, the CloudFormation template, emr_single_node.yml, will call the bootstrap script we copied earlier to S3, bootstrap.sh. The bootstrap script pre-installs required Python and Linux software packages, and the PostgreSQL driver JAR. The bootstrap script also clones your copy of the zeppelin-emr-demo GitHub repository.", "The EMR CloudFormation template will also modify the EMR cluster\u2019s Spark and Zeppelin application configurations. Amongst other configuration properties, the template sets the default Python version to Python3, instructs Zeppelin to use the cloned GitHub notebook directory path, and adds the PostgreSQL Driver JAR to the JVM Classpath. Below we can see the configuration properties applied to an existing EMR cluster.", "As of the date of this post (December, 2019), EMR is at version 5.28.0. Below, as shown in the EMR web interface, are the current (21) applications and frameworks available for installation on EMR.", "Change the following (7) variable values, then run the emr cloudformation create-stack API command, using the AWS CLI.", "You can use the Amazon EMR web interface to confirm the results of the CloudFormation stack. The cluster should be in the \u2018Waiting\u2019 state.", "Next, create a simple, single-AZ, single-master, non-replicated Amazon RDS PostgreSQL database, using the included CloudFormation template, rds_postgres.yml. We will use this database in Notebook 4. For the demo, I have selected the current-generation general purpose db.m4.large EC2 instance type to run PostgreSQL. You can easily change the instance type to another RDS-supported instance type to suit your particular requirements.", "Change the following (3) variable values and then run the cloudformation create-stack API command using the AWS CLI.", "You can use the Amazon RDS web interface to confirm the results of the CloudFormation stack.", "Next, create the AWS Glue Data Catalog database, the Apache Hive-compatible metastore for Spark SQL, two AWS Glue Crawlers, and a Glue IAM Role (ZeppelinDemoCrawlerRole), using the included CloudFormation template, crawler.yml. The AWS Glue Data Catalog database will be used in Notebook 3.", "Change the following variable value, then run the cloudformation create-stack API command, using the AWS CLI.", "You can use the AWS Glue web interface to confirm the results of the CloudFormation stack. Note the Data Catalog database and the two Glue Crawlers. We will not run the two crawlers later in the post. Thus, no tables will exist in the Data Catalog database, yet.", "At this point in the demonstration, you should have successfully created a single-node Amazon EMR cluster, an Amazon RDS PostgresSQL database, and several AWS Glue resources, all using CloudFormation templates.", "For the new EMR cluster to communicate with the RDS PostgreSQL database, we need to ensure that port 5432 is open from the RDS database\u2019s VPC security group, which is the default VPC security group, to the security groups of the EMR nodes. Obtain the Group ID of the ElasticMapReduce-master and ElasticMapReduce-slave Security Groups from the EMR web interface or using the AWS CLI.", "Access the Security Group for the RDS database using the RDS web interface. Change the inbound rule for port 5432 to include both Security Group IDs.", "In addition to the bootstrap script and configurations, we have already applied to the EMR cluster, we need to make several post-EMR creation configuration changes to the EMR cluster for our demonstration. These changes will require connecting to the EMR cluster using SSH. Using the master node\u2019s public DNS address and SSH command provided in the EMR web console, SSH into the master node.", "If you cannot access the node using SSH, check that port 22 is open on the associated EMR master node IAM Security Group (ElasticMapReduce-master) to your IP address or address range.", "We need to change permissions on the git repository we installed during the EMR bootstrapping phase. Typically, with an EC2 instance, you perform operations as the ec2-user user. With Amazon EMR, you often perform actions as the hadoop user. With Zeppelin on EMR, the notebooks perform operations, including interacting with the git repository as the zeppelin user. As a result of the bootstrap.sh script, the contents of the git repository directory, /tmp/zeppelin-emr-demo/, are owned by the hadoop user and group by default.", "We will change their owner to the zeppelin user and group. We could not perform this step as part of the bootstrap script since the the zeppelin user and group did not exist at the time the script was executed.", "The results should look similar to the following output.", "Next, we will pre-install several Apache Zeppelin Visualization packages. According to the Zeppelin website, an Apache Zeppelin Visualization is a pluggable package that can be loaded and unloaded at runtime through the Helium framework in Zeppelin. We can use them just like any other built-in visualization in the notebook. A Visualization is a javascript npm package. For example, here is a link to the ultimate-pie-chart on the public npm registry.", "We can pre-load plugins by replacing the /usr/lib/zeppelin/conf/helium.json file with the version of helium.json we copied to S3, earlier, and restarting Zeppelin. If you have a lot of Visualizations or package types or use any DataOps automation to create EMR clusters, this approach is more efficient and repeatable than manually loading plugins using the Zeppelin UI, each time you create a new EMR cluster. Below, the helium.json file, which pre-loads (8) Visualization packages.", "Run the following commands to load the plugins and adjust the permissions on the file.", "Lastly, we need to create a new Zeppelin JDBC Interpreter to connect to our RDS database. By default, Zeppelin has several interpreters installed. You can review a list of available interpreters using the following command.", "The new JDBC interpreter will allow us to connect to our RDS PostgreSQL database, using Java Database Connectivity (JDBC). First, ensure all available interpreters are installed, including the current Zeppelin JDBC driver (org.apache.zeppelin:zeppelin-jdbc:0.8.0) to /usr/lib/zeppelin/interpreter/jdbc.", "Creating a new interpreter is a two-part process. In this stage, we install the required interpreter files on the master node using the following command. Then later, in the Zeppelin web interface, we will configure the new PostgreSQL JDBC interpreter. Note we must provide a unique name for the interpreter (i.e. \u2018postgres\u2019), which we will refer to in part two of the interpreter creation process.", "To complete the post-EMR creation configuration on the master node, we must restart Zeppelin for our changes to take effect.", "In my experience, it could take 2\u20133 minutes for the Zeppelin UI to become fully responsive after a restart.", "With all the EMR application configuration complete, we will access the Zeppelin web interface running on the master node. Use the Zeppelin connection information provided in the EMR web interface to setup SSH tunneling to the Zeppelin web interface, running on the master node. Using SSH tunneling, we can also access the Spark History Server, Ganglia, and Hadoop Resource Manager web interfaces. All links are provided from the EMR web console.", "To set up a web connection to the applications installed on the EMR cluster, I am using FoxyProxy as a proxy management tool with Google Chrome.", "If everything is working so far, you should see the Zeppelin web interface with all four Zeppelin notebooks available from the cloned GitHub repository. You will be logged in as the anonymous user. Zeppelin offers authentication for accessing notebooks on the EMR cluster. For brevity, we will not cover setting up authentication in Zeppelin, using Shiro Authentication.", "To confirm the path to the local, cloned copy of the GitHub notebook repository, is correct, check the Notebook Repos interface, accessible under the Settings dropdown (anonymous user) in the upper right of the screen. The value should match the ZEPPELIN_NOTEBOOK_DIR configuration property value in the emr_single_node.yml CloudFormation template we executed earlier.", "To confirm the Helium Visualizations were pre-installed correctly, using the helium.json file, open the Helium interface, accessible under the Settings dropdown (anonymous user) in the upper right of the screen.", "Note the enabled visualizations. And, it is easy to enable additional plugins through the web interface.", "If you recall, earlier, we install the required interpreter files on the master node using the following command using the bootstrap script. We will now complete the process of configuring the new PostgreSQL JDBC interpreter. Open the Interpreter interface, accessible under the Settings dropdown (anonymous user) in the upper right of the screen.", "The title of the new interpreter must match the name we used to install the interpreter files, \u2018postgres\u2019. The interpreter group will be \u2018jdbc\u2019. There are minimally, three properties we need to configure for your specific RDS database instance, including default.url, default.user, and default.password. These should match the values you used to create your RDS instance, earlier. Make sure to includes the database name in the default.url. An example is shown below.", "We also need to provide a path to the PostgreSQL driver JAR dependency. This path is the location where we placed the JAR, earlier, using the bootstrap.sh script, /home/hadoop/extrajars/postgresql-42.2.8.jar. Save the new interpreter and make sure it starts successfully (shows a green icon).", "The last thing we need to do is change the Spark and Python interpreters to use Python 3 instead of the default Python 2. On the same screen you used to create a new interpreter, modify the Spark and Python interpreters. First, for the Python interpreter, change the zeppelin.python property to python3.", "Lastly, for the Spark interpreter, change the zeppelin.pyspark.python property to python3.", "Congratulations, with the demonstration setup complete, we are ready to start exploring Apache Zeppelin using each of our four notebooks.", "The first notebook uses a small 21k row kaggle dataset, Transactions from a Bakery. The notebook demonstrates Zeppelin\u2019s integration capabilities with the Helium plugin system for adding new chart types, the use of Amazon S3 for data storage and retrieval, and the use of Apache Parquet, a compressed and efficient columnar data storage format, and Zeppelin\u2019s storage integration with GitHub for notebook version control.", "When you open a notebook for the first time, you are given the choice of interpreters to bind and unbind to the notebook. The last interpreter in the list shown below, postgres, is the new PostgreSQL JDBC Zeppelin interpreter we created earlier in the post. We will use this interpreter in Notebook 3.", "The first two paragraphs of the notebook are used to confirm the version of Spark, Scala, OpenJDK, and Python we are using. Recall we updated the Spark and Python interpreters to use Python 3.", "If you recall from earlier in this post, we pre-installed several additional Helium Visualizations, including the Ultimate Pie Chart. Below, we see the use of the Spark SQL (%sql) interpreter to query a Spark DataFrame, return results, and visualize the data using the Ultimate Pie Chart. In addition to the pie chart, we see the other pre-installed Helium visualizations proceeding the five default visualizations, in the menu bar.", "With Zeppelin, all we have to do is write Spark SQL queries against the Spark DataFrame created earlier in the notebook, and Zeppelin will handle the visualization. You have some basic controls over charts using the \u2018settings\u2019 dropdown option.", "Notebook 1 demonstrates how to read and write data to S3. We read and write the Bakery dataset to both CSV-format and Apache Parquet-format, using Spark (PySpark). We also write the results of Spark SQL queries, like the one above, in Parquet, to S3.", "With Parquet, data may be split into multiple files, as shown in the S3 bucket directory below. Parquet is much faster to read into a Spark DataFrame than CSV. Spark provides support for both reading and writing Parquet files. We will write all of our data to Parquet in S3, making future re-use of the data much more efficient than downloading data from the Internet, like GroupLens or kaggle, or consuming CSV from S3.", "In addition to using the Zeppelin notebook, we can preview data right in the S3 bucket web interface using the Amazon S3 Select feature. This query in place feature is helpful to quickly understand the structure and content of new data files with which you want to interact within Zeppelin.", "Earlier, we configured Zeppelin to read and write the notebooks from your own copy of the GitHub notebook repository. Using the \u2018version control\u2019 menu item, changes made to the notebooks can be committed directly to GitHub.", "In GitHub, note the committer is the zeppelin user.", "The second notebook demonstrates the use of a single-node and multi-node Amazon EMR cluster for the exploration and analysis of public datasets ranging from approximately 100k rows up to 27MM rows, using Zeppelin. We will use the latest GroupLens MovieLens rating datasets to examine the performance characteristics of Zeppelin, using Spark, on single- verses multi-node EMR clusters for analyzing big data using a variety of Amazon EC2 Instance Types.", "If you recall, we waited to create the multi-node cluster due to the compute costs of running the cluster\u2019s large EC2 instances. You should understand the cost of these resources before proceeding, and that you ensure they are destroyed immediately upon completion of the demonstration to minimize your expenses.", "Understanding the costs of EMR requires understanding the concept of normalized instance hours. Clusters displayed in the EMR AWS Console contains two columns, \u2018Elapsed time\u2019 and \u2018Normalized instance hours\u2019. The \u2018Elapsed time\u2019 column reflects the actual wall-clock time the cluster was used. The \u2018Normalized instance hours\u2019 column indicates the approximate number of compute hours the cluster has used, rounded up to the nearest hour.", "Normalized instance hours calculations are based on a normalization factor. The normalization factor ranges from 1 for a small instance, up to 64 for an 8xlarge. Based on the type and quantity of instances in our multi-node cluster, we would use approximately 56 compute hours (aka normalized instance hours) for every one hour of wall-clock time. Note the multi-node cluster used in our demo, highlighted in yellow above. The cluster ran for two hours, which equated to 112 normalized instance hours.", "Create the multi-node EMR cluster using CloudFormation. Change the following nine variable values, then run the emr cloudformation create-stack API command, using the AWS CLI.", "Use the Amazon EMR web interface to confirm the success of the CloudFormation stack. The fully-provisioned cluster should be in the \u2018Waiting\u2019 state when ready.", "Refer to the earlier single-node cluster instructions, for the configuration steps necessary to prepare the EMR cluster and Zeppelin before continuing. Repeat all the steps used for the single-node cluster.", "Earlier, we installed Ganglia as part of creating the EMR cluster. Ganglia, according to its website, is a scalable distributed monitoring system for high-performance computing systems such as clusters and grids. Ganglia can be used to evaluate the performance of the single-node and multi-node EMR clusters. With Ganglia, we can easily view cluster and individual instance CPU, memory, and network I/O performance.", "The YARN Resource Manager Web UI is also available on our EMR cluster. Using the Resource Manager, we can view the compute resource load on the cluster, as well as the individual EMR Core nodes. Below, we see that the multi-node cluster has 24 vCPUs and 72 GiB of memory available, split evenly across the three Core cluster nodes.", "You might recall, the m5.2xlarge EC2 instance type, used for the three Core nodes, each contains 8 vCPUs and 32 GiB of memory. However, by default, although all 8 vCPUs are available for computation per node, only 24 GiB of the node\u2019s 32 GiB of memory are available for computation. EMR ensures a portion of the memory on each node is reserved for other system processes. The maximum available memory is controlled by the YARN memory configuration option, yarn.scheduler.maximum-allocation-mb.", "The YARN Resource Manager preview above shows the load on the Code nodes as Notebook 2 is executing the Spark SQL queries on the large MovieLens DataFrame with 27MM ratings. Note that only 4 of the 24 vCPUs (16.6%) are in use, but that 70.25 of the 72 GiB (97.6%) of available memory is being used. According to Spark, because of the in-memory nature of most Spark computations, Spark programs can be bottlenecked by any resource in the cluster: CPU, network bandwidth, or memory. Most often, if the data fits in memory, the bottleneck is network bandwidth. In this case, memory appears to be the most constrained resource. Using memory-optimized instances, such as r4 or r5 instance types, might be more effective for the core nodes than the m5 instance types.", "By changing one variable in the notebook, we can work with the latest, smaller GroupLens MovieLens dataset containing approximately 100k rows (ml-latest-small) or the larger dataset, containing approximately 27M rows (ml-latest). For this demo, try both datasets on both the single-node and multi-node clusters. Compare the Spark SQL paragraph execution times for each of the four variations, including 1) single-node with the small dataset, 2) single-node with the large dataset, 3) multi-node with the small dataset, and 4) multi-node with the large dataset. Observe how fast the SQL queries are executed on the single-node versus multi-node cluster. Try switching to a different Core node instance type, such as r5.2xlarge. Try creating a cluster with additional Core nodes. How is the compute time effected?", "Terminate the multi-node EMR cluster to save yourself the expense before continuing to Notebook 3.", "The third notebook demonstrates Amazon EMR and Zeppelin\u2019s integration capabilities with an AWS Glue Data Catalog as an Apache Hive-compatible metastore for Spark SQL. We will create an Amazon S3-based Data Lake using the AWS Glue Data Catalog and a set of AWS Glue Crawlers.", "Before continuing with Notebook 3, start the two Glue Crawlers using the AWS CLI.", "The two Crawlers should create a total of seven tables in the Glue Data Catalog database.", "If we examine the Glue Data Catalog database, we should now observe several tables, one for each dataset found in the S3 bucket. The location of each dataset is shown in the \u2018Location\u2019 column of the tables view.", "From the Zeppelin notebook, we can even use Spark SQL to query the AWS Glue Data Catalog, itself, for its databases and the tables within them.", "According to Amazon, the Glue Data Catalog tables and databases are containers for the metadata definitions that define a schema for underlying source data. Using Zeppelin\u2019s SQL interpreter, we can query the Data Catalog\u2019s metadata and return the underlying source data. The SQL query example, below, demonstrates how we can perform a join across two tables in the data catalog database, representing two different data sources, and return results.", "The fourth notebook demonstrates Zeppelin\u2019s ability to integrate with an external data source. In this case, we will interact with data in an Amazon RDS PostgreSQL relational database using three methods, including the Psycopg 2 PostgreSQL adapter for Python, Spark\u2019s native JDBC capability, and Zeppelin\u2019s JDBC Interpreter.", "First, create a new database schema and four related tables for the RDS PostgreSQL movie ratings database using the Psycopg 2 PostgreSQL adapter for Python and the SQL file, which we copied earlier to S3.", "The RDS database\u2019s schema, shown below, approximates the schema of the four CSV files from the GroupLens MovieLens rating dataset we used in Notebook 2.", "Since the schema of the PostgreSQL database matches the MovieLens dataset files, we can import the data from the CVS files, downloaded from GroupLens, directly into the RDS database, again using the Psycopg PostgreSQL adapter for Python.", "According to the Spark documentation, Spark SQL also includes a data source that can read data from other databases using JDBC. Using Spark\u2019s JDBC capability and the PostgreSQL JDBC Driver we installed earlier, during setup, we can perform Spark SQL queries against the RDS database using PySpark (%spark.pyspark). Below, we see an example of reading the RDS database\u2019s movies table, using Spark.", "As a third method of querying the RDS database, we can use the custom Zeppelin PostgreSQL JDBC interpreter (%postgres) we created earlier. Although the default driver of the JDBC interpreter is set as PostgreSQL, and the associated JAR is included with Zeppelin, we overrode that older JAR, with the latest PostgreSQL JDBC Driver JAR.", "Using the %postgres interpreter, we query the RDS database's public schema, and return the four database tables we created earlier in the notebook.", "Using the %postgres interpreter in the notebook\u2019s paragraph, we query the RDS database and return data, which we then visualize using Zeppelin\u2019s bar chart. Finally, note the use of Zeppelin Dynamic Forms in this example. Dynamic Forms allows Zeppelin to dynamically creates input forms, whose input values are then available to use programmatically. The notebook uses two form input values to control the data returned from our query and the resulting visualization.", "In this post, we learned how effectively Apache Zeppelin integrates with Amazon EMR. We also learned how to extend Zeppelin\u2019s capabilities, using AWS Glue, Amazon RDS, and Amazon S3 as a Data Lake. Beyond what was covered in this post, there are dozens of more Zeppelin and EMR features, as well as dozens of more AWS services that integrate with Zeppelin and EMR, for you to discover.", "All opinions expressed in this post are my own and not necessarily the views of my current or past employers or their clients.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Principal Solutions Architect @ AWS | 10x AWS Certified Pro | Polyglot Developer | DataOps | DevOps | Technology consultant, writer, and speaker"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F2b5d231a788a&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgetting-started-with-apache-zeppelin-on-amazon-emr-using-aws-glue-rds-and-s3-2b5d231a788a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgetting-started-with-apache-zeppelin-on-amazon-emr-using-aws-glue-rds-and-s3-2b5d231a788a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgetting-started-with-apache-zeppelin-on-amazon-emr-using-aws-glue-rds-and-s3-2b5d231a788a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgetting-started-with-apache-zeppelin-on-amazon-emr-using-aws-glue-rds-and-s3-2b5d231a788a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----2b5d231a788a--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2b5d231a788a--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://garystafford.medium.com/?source=post_page-----2b5d231a788a--------------------------------", "anchor_text": ""}, {"url": "https://garystafford.medium.com/?source=post_page-----2b5d231a788a--------------------------------", "anchor_text": "Gary A. Stafford"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1c57fc47fc23&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgetting-started-with-apache-zeppelin-on-amazon-emr-using-aws-glue-rds-and-s3-2b5d231a788a&user=Gary+A.+Stafford&userId=1c57fc47fc23&source=post_page-1c57fc47fc23----2b5d231a788a---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2b5d231a788a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgetting-started-with-apache-zeppelin-on-amazon-emr-using-aws-glue-rds-and-s3-2b5d231a788a&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2b5d231a788a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgetting-started-with-apache-zeppelin-on-amazon-emr-using-aws-glue-rds-and-s3-2b5d231a788a&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://searchbusinessanalytics.techtarget.com/definition/big-data-analytics", "anchor_text": "big data analytics"}, {"url": "https://en.wikipedia.org/wiki/Data_science", "anchor_text": "data science"}, {"url": "https://en.wikipedia.org/wiki/Artificial_intelligence", "anchor_text": "artificial intelligence"}, {"url": "https://en.wikipedia.org/wiki/Machine_learning", "anchor_text": "machine learning"}, {"url": "https://trends.google.com/trends/explore?date=today%205-y&geo=US&q=%2Fm%2F01hyh_,%2Fm%2F0jt3_q3", "anchor_text": "Google Trends"}, {"url": "https://plot.ly/~garystafford/64/", "anchor_text": "Plotly"}, {"url": "https://aws.amazon.com/emr/", "anchor_text": "Amazon Elastic MapReduce"}, {"url": "https://hadoop.apache.org/", "anchor_text": "Apache Hadoop"}, {"url": "https://spark.apache.org/", "anchor_text": "Apache Spark"}, {"url": "https://hadoop.apache.org/", "anchor_text": "Apache Hadoop"}, {"url": "https://aws.amazon.com/sagemaker/", "anchor_text": "SageMaker"}, {"url": "https://zeppelin.apache.org/", "anchor_text": "Apache Zeppelin"}, {"url": "http://jupyter.org/", "anchor_text": "Project Jupyter"}, {"url": "https://medium.com/netflix-techblog/open-sourcing-polynote-an-ide-inspired-polyglot-notebook-7f929d3f447", "anchor_text": "Netflix\u2019s Polynote"}, {"url": "https://zeppelin.apache.org/docs/0.8.2/usage/interpreter/overview.html", "anchor_text": "interpreters"}, {"url": "https://aws.amazon.com/glue/", "anchor_text": "AWS Glue"}, {"url": "https://aws.amazon.com/rds/", "anchor_text": "Amazon Relational Database Service"}, {"url": "https://aws.amazon.com/s3/", "anchor_text": "Amazon Simple Cloud Storage Service"}, {"url": "https://aws.amazon.com/big-data/datalakes-and-analytics/what-is-a-data-lake/", "anchor_text": "Data Lake"}, {"url": "https://zeppelin.apache.org/supported_interpreters.html", "anchor_text": "interpreters"}, {"url": "https://spark.apache.org/", "anchor_text": "Apache Spark"}, {"url": "https://spark.apache.org/sql/", "anchor_text": "Spark SQL"}, {"url": "https://spark.apache.org/docs/2.4.4/api/python/index.html", "anchor_text": "PySpark"}, {"url": "https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-overview-arch.html#emr-arch-storage", "anchor_text": "Hadoop Distributed File System"}, {"url": "https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-overview-arch.html", "anchor_text": "EMR architecture"}, {"url": "https://www.zepl.com/explore", "anchor_text": "Notebook Explorer"}, {"url": "https://www.zepl.com/", "anchor_text": "Zepl"}, {"url": "https://www.linkedin.com/in/moonsoo-lee-4982a511/", "anchor_text": "Moonsoo Lee"}, {"url": "https://www.zepl.com/viewer/github/garystafford/zeppelin-emr-demo/blob/master/2ERVVKTCG/note.json", "anchor_text": "first notebook"}, {"url": "https://www.kaggle.com/", "anchor_text": "kaggle"}, {"url": "https://www.kaggle.com/sulmansarwar/transactions-from-a-bakery", "anchor_text": "Transactions from a Bakery"}, {"url": "https://zeppelin.apache.org/docs/0.8.2/development/helium/overview.html", "anchor_text": "Helium"}, {"url": "https://parquet.apache.org/", "anchor_text": "Apache Parquet"}, {"url": "https://www.zepl.com/viewer/github/garystafford/zeppelin-emr-demo/blob/master/2EUZKQXX7/note.json", "anchor_text": "second notebook"}, {"url": "https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-overview.html#emr-overview-clusters", "anchor_text": "single-node"}, {"url": "https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-master-core-task-nodes.html", "anchor_text": "multi-node"}, {"url": "https://grouplens.org/datasets/movielens/", "anchor_text": "MovieLens"}, {"url": "https://aws.amazon.com/ec2/instance-types/", "anchor_text": "Amazon EC2 Instance Types"}, {"url": "https://www.zepl.com/viewer/github/garystafford/zeppelin-emr-demo/blob/master/2ERYY923A/note.json", "anchor_text": "third notebook"}, {"url": "https://aws.amazon.com/emr/", "anchor_text": "Amazon EMR"}, {"url": "https://aws.amazon.com/glue/", "anchor_text": "AWS Glue"}, {"url": "https://docs.aws.amazon.com/glue/latest/dg/components-overview.html#data-catalog-intro", "anchor_text": "Data Catalog"}, {"url": "https://hive.apache.org/", "anchor_text": "Apache Hive"}, {"url": "https://cwiki.apache.org/confluence/display/Hive/Design#Design-Metastore", "anchor_text": "metastore"}, {"url": "https://spark.apache.org/sql/", "anchor_text": "Spark SQL"}, {"url": "https://aws.amazon.com/big-data/datalakes-and-analytics/what-is-a-data-lake/", "anchor_text": "Data Lake"}, {"url": "https://docs.aws.amazon.com/glue/latest/dg/add-crawler.html", "anchor_text": "AWS Glue Crawlers"}, {"url": "https://www.zepl.com/viewer/github/garystafford/zeppelin-emr-demo/blob/master/2ESH8DGFS/note.json", "anchor_text": "fourth notebook"}, {"url": "https://zeppelin.apache.org/docs/0.8.2/interpreter/jdbc.html#overview", "anchor_text": "external data source"}, {"url": "https://aws.amazon.com/rds/postgresql/", "anchor_text": "Amazon RDS PostgreSQL"}, {"url": "http://initd.org/psycopg/", "anchor_text": "Psycopg 2"}, {"url": "https://zeppelin.apache.org/docs/0.8.2/interpreter/jdbc.html", "anchor_text": "JDBC Interpreter"}, {"url": "https://github.com/garystafford/zeppelin-emr-demo", "anchor_text": "zeppelin-emr-demo"}, {"url": "https://zeppelin.apache.org/docs/0.8.2/setup/storage/storage.html#notebook-storage-options-for-apache-zeppelin", "anchor_text": "pluggable notebook storage mechanisms"}, {"url": "https://github.com/settings/tokens", "anchor_text": "GitHub personal access token"}, {"url": "https://github.com/garystafford/zeppelin-emr-config", "anchor_text": "zeppelin-emr-config"}, {"url": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html", "anchor_text": "EC2 key pair"}, {"url": "https://aws.amazon.com/cli/", "anchor_text": "AWS CLI"}, {"url": "https://github.com/garystafford/zeppelin-emr-config/blob/master/bootstrap/bootstrap.sh", "anchor_text": "bootstrap.sh"}, {"url": "https://github.com/garystafford/zeppelin-emr-config/blob/master/bootstrap/helium.json", "anchor_text": "helium.json"}, {"url": "https://github.com/garystafford/zeppelin-emr-config/blob/master/sql/ratings.sql", "anchor_text": "ratings.sql"}, {"url": "https://aws.amazon.com/cloudformation/", "anchor_text": "AWS CloudFormation"}, {"url": "https://aws.amazon.com/emr/faqs/?nc=sn&loc=7#Billing", "anchor_text": "compute costs"}, {"url": "https://github.com/garystafford/zeppelin-emr-config/blob/master/cloudformation/emr_single_node.yml", "anchor_text": "emr_single_node.yml"}, {"url": "https://github.com/garystafford/zeppelin-emr-config/blob/master/bootstrap/bootstrap.sh", "anchor_text": "bootstrap.sh"}, {"url": "https://jdbc.postgresql.org/", "anchor_text": "PostgreSQL driver JAR"}, {"url": "https://github.com/garystafford/zeppelin-emr-demo", "anchor_text": "zeppelin-emr-demo"}, {"url": "https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-release-5x.html", "anchor_text": "5.28.0"}, {"url": "http://ganglia.info/", "anchor_text": "Ganglia"}, {"url": "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_PostgreSQL.html", "anchor_text": "Amazon RDS PostgreSQL"}, {"url": "https://github.com/garystafford/zeppelin-emr-config/blob/master/cloudformation/rds_postgres.yml", "anchor_text": "rds_postgres.yml"}, {"url": "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.DBInstanceClass.html", "anchor_text": "RDS-supported instance type"}, {"url": "https://github.com/garystafford/zeppelin-emr-config/blob/master/cloudformation/crawler.yml", "anchor_text": "crawler.yml"}, {"url": "https://github.com/garystafford/zeppelin-emr-config/blob/master/bootstrap/bootstrap.sh#L19-L20", "anchor_text": "bootstrap.sh"}, {"url": "https://zeppelin.apache.org/docs/0.8.0/development/helium/writing_visualization_basic.html", "anchor_text": "Zeppelin website"}, {"url": "https://docs.npmjs.com/about-packages-and-modules", "anchor_text": "npm package"}, {"url": "https://www.npmjs.com/package/ultimate-pie-chart", "anchor_text": "ultimate-pie-chart"}, {"url": "https://github.com/garystafford/zeppelin-emr-config/blob/master/bootstrap/helium.json", "anchor_text": "helium.json"}, {"url": "https://zeppelin.apache.org/docs/0.8.2/interpreter/jdbc.html", "anchor_text": "Zeppelin JDBC Interpreter"}, {"url": "https://en.wikipedia.org/wiki/Java_Database_Connectivity", "anchor_text": "Java Database Connectivity"}, {"url": "https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-web-interfaces.html", "anchor_text": "setup SSH tunneling"}, {"url": "https://getfoxyproxy.org/", "anchor_text": "FoxyProxy"}, {"url": "https://zeppelin.apache.org/docs/0.8.2/setup/security/shiro_authentication.html", "anchor_text": "Shiro Authentication"}, {"url": "https://github.com/garystafford/zeppelin-emr-config/blob/master/cloudformation/emr_single_node.yml#L67", "anchor_text": "emr_single_node.yml"}, {"url": "https://github.com/garystafford/zeppelin-emr-config/blob/master/bootstrap/helium.json", "anchor_text": "helium.json"}, {"url": "https://github.com/garystafford/zeppelin-emr-config/blob/master/bootstrap/bootstrap.sh#L27", "anchor_text": "bootstrap.sh"}, {"url": "https://www.zepl.com/viewer/github/garystafford/zeppelin-emr-demo/blob/master/2ERVVKTCG/note.json", "anchor_text": "first notebook"}, {"url": "https://www.kaggle.com/", "anchor_text": "kaggle"}, {"url": "https://www.kaggle.com/sulmansarwar/transactions-from-a-bakery", "anchor_text": "Transactions from a Bakery"}, {"url": "https://zeppelin.apache.org/docs/0.8.2/development/helium/overview.html", "anchor_text": "Helium"}, {"url": "https://parquet.apache.org/", "anchor_text": "Apache Parquet"}, {"url": "https://www.npmjs.com/package/ultimate-pie-chart", "anchor_text": "Ultimate Pie Chart"}, {"url": "http://spark.apache.org/sql/", "anchor_text": "Spark SQL"}, {"url": "https://spark.apache.org/docs/2.4.4/sql-programming-guide.html", "anchor_text": "Spark DataFrame"}, {"url": "https://parquet.apache.org/", "anchor_text": "Apache Parquet"}, {"url": "https://docs.aws.amazon.com/AmazonS3/latest/dev/selecting-content-from-objects.html", "anchor_text": "Amazon S3 Select"}, {"url": "https://aws.amazon.com/s3/features/#s3-select", "anchor_text": "query in place"}, {"url": "https://www.zepl.com/viewer/github/garystafford/zeppelin-emr-demo/blob/master/2EUZKQXX7/note.json", "anchor_text": "second notebook"}, {"url": "https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-overview.html#emr-overview-clusters", "anchor_text": "single-node"}, {"url": "https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-master-core-task-nodes.html", "anchor_text": "multi-node"}, {"url": "https://grouplens.org/datasets/movielens/", "anchor_text": "MovieLens"}, {"url": "https://aws.amazon.com/ec2/instance-types/", "anchor_text": "Amazon EC2 Instance Types"}, {"url": "https://aws.amazon.com/emr/faqs/?nc=sn&loc=7#Billing", "anchor_text": "compute costs"}, {"url": "https://aws.amazon.com/emr/faqs/?nc=sn&loc=7", "anchor_text": "normalization factor"}, {"url": "http://ganglia.sourceforge.net/", "anchor_text": "Ganglia"}, {"url": "https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hadoop-task-config.html#emr-hadoop-task-config-m5", "anchor_text": "default"}, {"url": "https://spark.apache.org/docs/latest/tuning.html", "anchor_text": "Spark"}, {"url": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/memory-optimized-instances.html", "anchor_text": "memory-optimized instances"}, {"url": "https://grouplens.org/datasets/movielens/", "anchor_text": "MovieLens"}, {"url": "https://www.zepl.com/viewer/github/garystafford/zeppelin-emr-demo/blob/master/2ERYY923A/note.json", "anchor_text": "third notebook"}, {"url": "https://aws.amazon.com/emr/", "anchor_text": "Amazon EMR"}, {"url": "https://aws.amazon.com/glue/", "anchor_text": "AWS Glue"}, {"url": "https://docs.aws.amazon.com/glue/latest/dg/components-overview.html#data-catalog-intro", "anchor_text": "Data Catalog"}, {"url": "https://hive.apache.org/", "anchor_text": "Apache Hive"}, {"url": "https://cwiki.apache.org/confluence/display/Hive/Design#Design-Metastore", "anchor_text": "metastore"}, {"url": "https://spark.apache.org/sql/", "anchor_text": "Spark SQL"}, {"url": "https://aws.amazon.com/big-data/datalakes-and-analytics/what-is-a-data-lake/", "anchor_text": "Data Lake"}, {"url": "https://docs.aws.amazon.com/glue/latest/dg/add-crawler.html", "anchor_text": "AWS Glue Crawlers"}, {"url": "https://docs.aws.amazon.com/athena/latest/ug/understanding-tables-databases-and-the-data-catalog.html", "anchor_text": "Amazon"}, {"url": "https://www.zepl.com/viewer/github/garystafford/zeppelin-emr-demo/blob/master/2ESH8DGFS/note.json", "anchor_text": "fourth notebook"}, {"url": "https://zeppelin.apache.org/docs/0.8.2/interpreter/jdbc.html#overview", "anchor_text": "external data source"}, {"url": "https://aws.amazon.com/rds/postgresql/", "anchor_text": "Amazon RDS PostgreSQL"}, {"url": "http://initd.org/psycopg/", "anchor_text": "Psycopg 2"}, {"url": "https://zeppelin.apache.org/docs/0.8.2/interpreter/jdbc.html", "anchor_text": "JDBC Interpreter"}, {"url": "https://grouplens.org/datasets/movielens/", "anchor_text": "MovieLens"}, {"url": "http://initd.org/psycopg/", "anchor_text": "Psycopg"}, {"url": "https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html", "anchor_text": "Spark documentation"}, {"url": "https://jdbc.postgresql.org/", "anchor_text": "PostgreSQL JDBC Driver"}, {"url": "https://jdbc.postgresql.org/", "anchor_text": "PostgreSQL JDBC Driver"}, {"url": "https://zeppelin.apache.org/docs/0.8.2/usage/dynamic_form/intro.html", "anchor_text": "Dynamic Forms"}, {"url": "https://medium.com/tag/apache-zeppelin?source=post_page-----2b5d231a788a---------------apache_zeppelin-----------------", "anchor_text": "Apache Zeppelin"}, {"url": "https://medium.com/tag/apache-spark?source=post_page-----2b5d231a788a---------------apache_spark-----------------", "anchor_text": "Apache Spark"}, {"url": "https://medium.com/tag/aws-glue?source=post_page-----2b5d231a788a---------------aws_glue-----------------", "anchor_text": "Aws Glue"}, {"url": "https://medium.com/tag/amazon-emr?source=post_page-----2b5d231a788a---------------amazon_emr-----------------", "anchor_text": "Amazon Emr"}, {"url": "https://medium.com/tag/data-analysis?source=post_page-----2b5d231a788a---------------data_analysis-----------------", "anchor_text": "Data Analysis"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2b5d231a788a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgetting-started-with-apache-zeppelin-on-amazon-emr-using-aws-glue-rds-and-s3-2b5d231a788a&user=Gary+A.+Stafford&userId=1c57fc47fc23&source=-----2b5d231a788a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2b5d231a788a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgetting-started-with-apache-zeppelin-on-amazon-emr-using-aws-glue-rds-and-s3-2b5d231a788a&user=Gary+A.+Stafford&userId=1c57fc47fc23&source=-----2b5d231a788a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2b5d231a788a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgetting-started-with-apache-zeppelin-on-amazon-emr-using-aws-glue-rds-and-s3-2b5d231a788a&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2b5d231a788a--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F2b5d231a788a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgetting-started-with-apache-zeppelin-on-amazon-emr-using-aws-glue-rds-and-s3-2b5d231a788a&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----2b5d231a788a---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----2b5d231a788a--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----2b5d231a788a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----2b5d231a788a--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----2b5d231a788a--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----2b5d231a788a--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----2b5d231a788a--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----2b5d231a788a--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----2b5d231a788a--------------------------------", "anchor_text": ""}, {"url": "https://garystafford.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://garystafford.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Gary A. Stafford"}, {"url": "https://garystafford.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "3.7K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1c57fc47fc23&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgetting-started-with-apache-zeppelin-on-amazon-emr-using-aws-glue-rds-and-s3-2b5d231a788a&user=Gary+A.+Stafford&userId=1c57fc47fc23&source=post_page-1c57fc47fc23--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F558bb209b64a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgetting-started-with-apache-zeppelin-on-amazon-emr-using-aws-glue-rds-and-s3-2b5d231a788a&newsletterV3=1c57fc47fc23&newsletterV3Id=558bb209b64a&user=Gary+A.+Stafford&userId=1c57fc47fc23&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}