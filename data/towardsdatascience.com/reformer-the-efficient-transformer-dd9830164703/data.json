{"url": "https://towardsdatascience.com/reformer-the-efficient-transformer-dd9830164703", "time": 1683012652.535282, "path": "towardsdatascience.com/reformer-the-efficient-transformer-dd9830164703/", "webpage": {"metadata": {"title": "Reformer: The Efficient Transformer | by Rohan Jagtap | Towards Data Science", "h1": "Reformer: The Efficient Transformer", "description": "Transformer (Vaswani et. al.) is great, it attends to a longer context, it offers parallelization in computation which RNNs don\u2019t, and most importantly, they have the state of the art results. In\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/transformers-explained-65454c0f3fa7", "anchor_text": "Transformer", "paragraph_index": 0}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "Vaswani et. al.", "paragraph_index": 0}, {"url": "https://arxiv.org/abs/2001.04451", "anchor_text": "Reformer: The Efficient Transformer", "paragraph_index": 1}, {"url": "https://en.wikipedia.org/wiki/Locality-sensitive_hashing", "anchor_text": "Locality Sensitive Hashing (LSH)", "paragraph_index": 1}, {"url": "https://arxiv.org/abs/1707.04585", "anchor_text": "Reversible Layers", "paragraph_index": 1}, {"url": "https://medium.com/swlh/bert-pre-training-of-transformers-for-language-understanding-5214fba4a9af", "anchor_text": "BERT", "paragraph_index": 2}, {"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "Devlin et. al.", "paragraph_index": 2}, {"url": "https://towardsdatascience.com/transformers-explained-65454c0f3fa7", "anchor_text": "read my blog on the Transformers", "paragraph_index": 6}, {"url": "https://arxiv.org/abs/1707.04585", "anchor_text": "Gomez et. al.", "paragraph_index": 20}, {"url": "https://github.com/google/trax/tree/master/trax/models/reformer", "anchor_text": "Here is the link to the Github Repository", "paragraph_index": 35}, {"url": "https://github.com/google/trax", "anchor_text": "Google trax", "paragraph_index": 35}, {"url": "https://colab.research.google.com/github/google/trax/blob/master/trax/models/reformer/image_generation.ipynb", "anchor_text": "image generation demo", "paragraph_index": 36}, {"url": "https://colab.research.google.com/github/google/trax/blob/master/trax/models/reformer/text_generation.ipynb", "anchor_text": "text generation demo", "paragraph_index": 36}, {"url": "https://huggingface.co/transformers/model_doc/reformer.html", "anchor_text": "Here\u2019s the link to huggingface\u2019s API docs", "paragraph_index": 37}], "all_paragraphs": ["Transformer (Vaswani et. al.) is great, it attends to a longer context, it offers parallelization in computation which RNNs don\u2019t, and most importantly, they have the state of the art results.", "In this article, we\u2019ll be covering the Reformer Model, which was proposed in the paper Reformer: The Efficient Transformer by Google Research. This model essentially addresses some efficiency constraints of the Transformer model and proposes an improved version of the Transformer that implements Locality Sensitive Hashing (LSH) and Reversible Layers to make the model much more efficient. We\u2019ll be discussing these in greater detail in the coming sections of this post.", "Despite being the state of the art, the Transformer is very expensive (w.r.t. memory). One of the most well-known Transformer is BERT (Devlin et. al.), that too, is trained for a maximum allowed sequence length of 512. To further delineate on this, we\u2019ll take the same example as in the paper:", "The largest reported configuration of the Transformer has 64 layers and 0.5B parameters per layer. Say we want to train a Transformer for a sequence of length as long as 64K. Here, the 0.5B parameters account for 2GB memory. Moreover, 1024-dimensional embedding weights for a batch size 8 accounts for 64K x 1K x 8 = 0.5B floats which is again 2GB of memory. Now if we were to train this model for a single layer, we could easily train it on a single GPU, however, there are 63 more layers to be appended. Moreover, the corpus on which BERT is trained requires 17GB to store.", "In light of the above, the following are the problems addressed in the original Transformer:", "In the coming sections, we\u2019ll see how Reformer overcomes these.", "This is the Scaled-Dot Product Attention from the original Transformer Model. Here the actual embedding is first activated into 3 different vectors namely query (Q), key (K), and value (V) (for each token). Then the vector dot-product of the queries and the keys is obtained that tells how much of each vector contributes to obtaining a given vector (attention, basically). For more on self-attention, you can read my blog on the Transformers.", "The main issue lies with the QK\u1d40 term. Let the shape of the queries and the keys be (batch_size, seq_length, d_model) each. Now, QK\u1d40 will result in a shape of (batch_size, seq_length, seq_length). So even for a batch size of 1, a sequence of 64K length will have the QK\u1d40 term to have a 64K x 64K sized matrix (16GB memory).", "Well, the workaround on this is, instead of taking the whole Q term, one can compute attention for each query q_i separately, i.e.", "Now, this sounds inefficient as this in a way takes away the parallel processing ability of the Transformers. However, you\u2019ll be surprised to know that the LSH attention (which we are about to see) compensates for this part.", "In the original implementation of the Transformer, Q, K and V are activated using 3 different sets of weights (linear layers). On the contrary, the authors suggest using the same weights for both, query and the key. It is called a shared-QK model.", "It turns out that sharing QK does not affect the performance of Transformer", "So we\u2019ve discussed computing the attention for each query separately, which seems quite inefficient as it isn\u2019t parallel. But what if we take say just 32 or 64 keys that are closest to the given query from the complete sequence of length 64K and compute the attention for just them? This is exactly what LSH Attention does. Let\u2019s see how LSH works:", "To get a clear idea of this, we\u2019ll quickly discuss the role of a vector space first. So, at the first layer of the Transformer, we essentially map each of the tokens in a given sequence to a \u2018vector\u2019. This suggests that we are mapping all our tokens to a common vector space where the vector representations for all the tokens in the vocabulary co-exist. Now, these mappings are trained on a language such that the vector representation of similar or related tokens are closer to each other, and the ones that are unrelated lie far away from each other (FYI, we can measure the relatedness of these vectors using distance metrics; for eg. cosine distance).", "Consider any 2 vectors from the vector space we discussed earlier as two points x and y. We consider this imaginary circle (sphere actually) to contain these points (the circle is the vector space, say). Then, we divide the circle into 4 parts (4 hashing buckets) and rotate this partition randomly i.e. rotate the circle randomly. We have 2 observations here (refer the figure):", "A hashing scheme that assigns each vector x to a hash h(x) is called locality-sensitive if nearby vectors get the same hash with high probability and distant ones do not.", "In the Reformer, this is achieved by taking a random matrix R of size (d_k, b/2) where d_k is the size of the key vector and b is the number of buckets. And the hash function h for a vector x is given as:", "where [ a ; b ] is concatenation. So by having xR and -xR, we are essentially taking random projections on x. Intuitively speaking, we are taking b vectors (buckets) of size d_k and evaluating the bucket to which x belongs.", "The figure above depicts the flow of LSH Attention implemented in the Reformer.", "We solved the memory issue in attention computation using LSH Attention which is discussed previously. However, there is another major point of concern when it comes to memory consumed by the Transformers. In feed-forward layers, it is normal to have a transformer with the d_ff value to be ~4K and the number of layers ~16. Moreover, the inputs to each layer need to be stored for backpropagation. In such a setting, with a sequence length of 64K, we would still end up with the impractical 16GB memory range.", "So to solve this, Reformer borrows the idea from RevNets (Reversible Residual Neural Networks), proposed in Gomez et. al. To understand this architecture, consider the figure above:", "(a) A regular skip connection in ResNet: We take the input, compute the layer value (a function actually), and add it to the original layer inputs. In the figure, the value of x and y need to be stored in memory for backpropagation. So:", "(b) A forward-pass in RevNet: The idea is to allow the inputs at any layer to be recovered from the inputs of its following layer; i.e.", "(c) A backward-pass in RevNet: Now we can easily reconstruct the inputs for the layers using the following:", "So clearly, we may rather reconstruct the input values than store them for the backpropagation and save memory.", "Finally, the Reversible Transformer implementation of this is as follows:", "Finally, we address the intermediate feed-forward layer which has d_ff output units, where d_ff is pretty large as compared to d_model (generally ~4K).", "Considering a sequence with 64K tokens, in a standard transformer, all outputs are calculated in parallel and hence the weights take more memory. Although the feed-forward output is computed in parallel for the whole sequence, it need not be as the outputs for any given token\u2019s vector is independent of other vectors.", "Hence, the Reformer suggests processing this layer in \u2018c\u2019 chunks:", "With chunking + reversible layers, the layer-input memory of a Reformer model is independent of the number of layers.", "b => batch size, l => sequence length, d_ff => intermediate feed-forward\u2019s dimension, n_h =>number of heads, n_l => number of layers, d_model is the hidden state dimension, n_r => number of hashing rounds, c => chunk size.", "The Reformer can be used to generate a complete image from a partial image:", "Fun Fact: The Reformer is so efficient that it can process text sequences of lengths up to 1M words on a single GPU with just 16GB memory.", "Fun Fact: Reformer can process entire novels, all at once and on a single device.", "In this (really long) article we covered the Reformer model in depth. We saw what efficiency defects it addresses in the Transformer and how it overcomes them.", "Here is the link to the Github Repository of the Reformer code by Google trax.", "Here\u2019s a Colab Notebook for image generation demo by Google, and another one for text generation demo.", "Here\u2019s the link to huggingface\u2019s API docs for Reformer implementation and pre-trained weights.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Immensely interested in AI Research | I read papers and post my notes on Medium"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fdd9830164703&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freformer-the-efficient-transformer-dd9830164703&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freformer-the-efficient-transformer-dd9830164703&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freformer-the-efficient-transformer-dd9830164703&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freformer-the-efficient-transformer-dd9830164703&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----dd9830164703--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----dd9830164703--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://rojagtap.medium.com/?source=post_page-----dd9830164703--------------------------------", "anchor_text": ""}, {"url": "https://rojagtap.medium.com/?source=post_page-----dd9830164703--------------------------------", "anchor_text": "Rohan Jagtap"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F39646f947a4c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freformer-the-efficient-transformer-dd9830164703&user=Rohan+Jagtap&userId=39646f947a4c&source=post_page-39646f947a4c----dd9830164703---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdd9830164703&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freformer-the-efficient-transformer-dd9830164703&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdd9830164703&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freformer-the-efficient-transformer-dd9830164703&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@dnevozhai?utm_source=medium&utm_medium=referral", "anchor_text": "Denys Nevozhai"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://towardsdatascience.com/transformers-explained-65454c0f3fa7", "anchor_text": "Transformer"}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "Vaswani et. al."}, {"url": "https://arxiv.org/abs/2001.04451", "anchor_text": "Reformer: The Efficient Transformer"}, {"url": "https://en.wikipedia.org/wiki/Locality-sensitive_hashing", "anchor_text": "Locality Sensitive Hashing (LSH)"}, {"url": "https://arxiv.org/abs/1707.04585", "anchor_text": "Reversible Layers"}, {"url": "https://medium.com/swlh/bert-pre-training-of-transformers-for-language-understanding-5214fba4a9af", "anchor_text": "BERT"}, {"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "Devlin et. al."}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "Vaswani et. al."}, {"url": "https://towardsdatascience.com/transformers-explained-65454c0f3fa7", "anchor_text": "read my blog on the Transformers"}, {"url": "https://arxiv.org/abs/2001.04451", "anchor_text": "Reformer Paper"}, {"url": "https://arxiv.org/abs/2001.04451", "anchor_text": "Reformer Paper"}, {"url": "https://arxiv.org/abs/2001.04451", "anchor_text": "Reformer Paper"}, {"url": "https://arxiv.org/abs/2001.04451", "anchor_text": "Reformer Paper"}, {"url": "https://arxiv.org/abs/2001.04451", "anchor_text": "Reformer Paper"}, {"url": "https://arxiv.org/abs/2001.04451", "anchor_text": "Reformer Paper"}, {"url": "https://arxiv.org/abs/1707.04585", "anchor_text": "Gomez et. al."}, {"url": "https://ai.googleblog.com/2020/01/reformer-efficient-transformer.html", "anchor_text": "Google AI Blog"}, {"url": "https://arxiv.org/abs/1707.04585", "anchor_text": "Gomez et. al."}, {"url": "https://arxiv.org/abs/2001.04451", "anchor_text": "Reformer Paper"}, {"url": "https://arxiv.org/abs/2001.04451", "anchor_text": "Reformer Paper"}, {"url": "https://arxiv.org/abs/1707.08819", "anchor_text": "Imagenet64 Dataset"}, {"url": "https://ai.googleblog.com/2020/01/reformer-efficient-transformer.html", "anchor_text": "Google AI Blog"}, {"url": "https://github.com/google/trax/tree/master/trax/models/reformer", "anchor_text": "Here is the link to the Github Repository"}, {"url": "https://github.com/google/trax", "anchor_text": "Google trax"}, {"url": "https://colab.research.google.com/github/google/trax/blob/master/trax/models/reformer/image_generation.ipynb", "anchor_text": "image generation demo"}, {"url": "https://colab.research.google.com/github/google/trax/blob/master/trax/models/reformer/text_generation.ipynb", "anchor_text": "text generation demo"}, {"url": "https://huggingface.co/transformers/model_doc/reformer.html", "anchor_text": "Here\u2019s the link to huggingface\u2019s API docs"}, {"url": "https://arxiv.org/abs/2001.04451", "anchor_text": "https://arxiv.org/abs/2001.04451"}, {"url": "https://arxiv.org/abs/1707.04585", "anchor_text": "https://arxiv.org/abs/1707.04585"}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "https://arxiv.org/abs/1706.03762"}, {"url": "https://ai.googleblog.com/2020/01/reformer-efficient-transformer.html", "anchor_text": "Reformer: The Efficient TransformerUnderstanding sequential data - such as language, music or videos - is a challenging task, especially when there is\u2026ai.googleblog.com"}, {"url": "https://towardsdatascience.com/transformers-explained-65454c0f3fa7", "anchor_text": "Transformers ExplainedAn exhaustive explanation of Google\u2019s Transformer model; from theory to implementationtowardsdatascience.com"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----dd9830164703---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----dd9830164703---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----dd9830164703---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----dd9830164703---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----dd9830164703---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fdd9830164703&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freformer-the-efficient-transformer-dd9830164703&user=Rohan+Jagtap&userId=39646f947a4c&source=-----dd9830164703---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fdd9830164703&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freformer-the-efficient-transformer-dd9830164703&user=Rohan+Jagtap&userId=39646f947a4c&source=-----dd9830164703---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdd9830164703&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freformer-the-efficient-transformer-dd9830164703&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----dd9830164703--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fdd9830164703&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freformer-the-efficient-transformer-dd9830164703&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----dd9830164703---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----dd9830164703--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----dd9830164703--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----dd9830164703--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----dd9830164703--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----dd9830164703--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----dd9830164703--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----dd9830164703--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----dd9830164703--------------------------------", "anchor_text": ""}, {"url": "https://rojagtap.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://rojagtap.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Rohan Jagtap"}, {"url": "https://rojagtap.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "465 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F39646f947a4c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freformer-the-efficient-transformer-dd9830164703&user=Rohan+Jagtap&userId=39646f947a4c&source=post_page-39646f947a4c--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fe51e2b6202c5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freformer-the-efficient-transformer-dd9830164703&newsletterV3=39646f947a4c&newsletterV3Id=e51e2b6202c5&user=Rohan+Jagtap&userId=39646f947a4c&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}