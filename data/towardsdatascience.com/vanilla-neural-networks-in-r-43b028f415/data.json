{"url": "https://towardsdatascience.com/vanilla-neural-networks-in-r-43b028f415", "time": 1683016091.0121448, "path": "towardsdatascience.com/vanilla-neural-networks-in-r-43b028f415/", "webpage": {"metadata": {"title": "Vanilla Neural Networks in R. Take a Look Under The Hood of Neural\u2026 | by Chris Mahoney | Towards Data Science", "h1": "Vanilla Neural Networks in R", "description": "1. Introduction 2. Background 3. Semantics 4. Set Up 5. Get Data 6. Check Data 7. Prepare the Data 8. Instantiate the Network 9. Initialise the Network 10. Forward Propagation 11. Calculate the Cost\u2026"}, "outgoing_paragraph_urls": [{"url": "http://266e", "anchor_text": "Prepare the Data", "paragraph_index": 0}, {"url": "https://www.r-project.org/", "anchor_text": "R", "paragraph_index": 1}, {"url": "https://www.tidyverse.org/", "anchor_text": "tidyverse", "paragraph_index": 1}, {"url": "http://caret.r-forge.r-project.org/", "anchor_text": "caret", "paragraph_index": 1}, {"url": "http://127.0.0.1:17287/rmd_output/0/", "anchor_text": "Python", "paragraph_index": 1}, {"url": "https://numpy.org/", "anchor_text": "numpy", "paragraph_index": 1}, {"url": "https://pandas.pydata.org/", "anchor_text": "pandas", "paragraph_index": 1}, {"url": "https://scikit-learn.org/", "anchor_text": "sci-kit learn", "paragraph_index": 1}, {"url": "https://keras.io/", "anchor_text": "keras", "paragraph_index": 1}, {"url": "https://pytorch.org/", "anchor_text": "pytorch", "paragraph_index": 1}, {"url": "https://www.tensorflow.org/", "anchor_text": "tensorflow", "paragraph_index": 1}, {"url": "https://en.wikipedia.org/wiki/Convolutional_neural_network", "anchor_text": "convolution", "paragraph_index": 2}, {"url": "https://en.wikipedia.org/wiki/Recurrent_neural_network", "anchor_text": "recursion", "paragraph_index": 2}, {"url": "https://machinelearningmastery.com/about/", "anchor_text": "Jason Brownlee", "paragraph_index": 4}, {"url": "https://machinelearningmastery.com/implement-backpropagation-algorithm-scratch-python/", "anchor_text": "How to Code a Neural Network with Backpropagation In Python (from scratch)", "paragraph_index": 4}, {"url": "https://notebooks.azure.com/goldengrape/projects/deeplearning-ai", "anchor_text": "DeepLearning.ai", "paragraph_index": 4}, {"url": "https://notebooks.azure.com/goldengrape/projects/deeplearning-ai/html/COURSE%201%20Neural%20Networks%20and%20Deep%20Learning/Week%204/Deep%20Neural%20Network%20Application_%20Image%20Classification/dnn_app_utils_v2.py", "anchor_text": "dnn_app_utils_v2.py", "paragraph_index": 4}, {"url": "https://notebooks.azure.com/", "anchor_text": "Microsoft Azure Notebooks", "paragraph_index": 4}, {"url": "https://www.tensorflow.org/tutorials/images/cnn", "anchor_text": "Convolution", "paragraph_index": 7}, {"url": "https://www.tensorflow.org/guide/keras/rnn", "anchor_text": "Recursion", "paragraph_index": 7}, {"url": "https://www.youtube.com/watch?v=RiqWATOoos8", "anchor_text": "The Math Behind Neural Networks", "paragraph_index": 8}, {"url": "https://medium.com/@purnasaigudikandula/a-beginner-intro-to-neural-networks-543267bda3c8", "anchor_text": "A Beginner Intro to Neural Networks", "paragraph_index": 9}, {"url": "https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi", "anchor_text": "Neural Networks series", "paragraph_index": 9}, {"url": "https://www.rdocumentation.org/packages/tensorflow", "anchor_text": "tensorflow", "paragraph_index": 10}, {"url": "https://www.rdocumentation.org/packages/nnet", "anchor_text": "nnet", "paragraph_index": 10}, {"url": "https://github.com/chrimaho/VanillaNeuralNetworksInR/blob/master/Functions/functions.R", "anchor_text": "here", "paragraph_index": 13}, {"url": "https://www.rdocumentation.org/packages/dplyr", "anchor_text": "dplyr", "paragraph_index": 14}, {"url": "https://www.rdocumentation.org/packages/base", "anchor_text": "base", "paragraph_index": 14}, {"url": "https://www.cs.toronto.edu/~kriz/cifar.html", "anchor_text": "CIFAR-10", "paragraph_index": 18}, {"url": "https://github.com/EN10/CIFAR#classes", "anchor_text": "GitHub > EN10 > CIFAR", "paragraph_index": 20}, {"url": "https://github.com/chrimaho/VanillaNeuralNetworksInR/blob/master/Functions/functions.R", "anchor_text": "here", "paragraph_index": 21}, {"url": "https://en.wikipedia.org/wiki/Cross-validation_%28statistics%29", "anchor_text": "Wikipedia\u2019s Cross-Validation", "paragraph_index": 29}, {"url": "https://developers.google.com/machine-learning/crash-course/training-and-test-sets/splitting-data", "anchor_text": "Google\u2019s Machine Learning Crash Course: Training and Test Sets Splitting Data", "paragraph_index": 29}, {"url": "https://eli.thegreenplace.net/2015/memory-layout-of-multi-dimensional-arrays", "anchor_text": "Memory Layout of Multi-Dimensional Arrays", "paragraph_index": 34}, {"url": "https://www.tensorflow.org/api_docs/python/tf/keras/datasets/cifar10/load_data", "anchor_text": "TensorFlow", "paragraph_index": 39}, {"url": "https://www.cs.toronto.edu/~kriz/cifar.html", "anchor_text": "CIFAR10", "paragraph_index": 39}, {"url": "https://en.wikipedia.org/wiki/RGB_color_model", "anchor_text": "Wikipedia", "paragraph_index": 39}, {"url": "http://alexlenail.me/NN-SVG/AlexNet.html", "anchor_text": "http://alexlenail.me/NN-SVG/AlexNet.html", "paragraph_index": 45}, {"url": "https://prateekvjoshi.com/2016/03/29/understanding-xavier-initialization-in-deep-neural-networks/", "anchor_text": "Xavier", "paragraph_index": 49}, {"url": "https://medium.com/@prateekvishnu/xavier-and-he-normal-he-et-al-initialization-8e3d7a087528", "anchor_text": "He", "paragraph_index": 49}, {"url": "https://github.com/chrimaho/VanillaNeuralNetworksInR/blob/master/Functions/functions.R", "anchor_text": "here", "paragraph_index": 58}, {"url": "https://github.com/chrimaho/VanillaNeuralNetworksInR/blob/master/Functions/functions.R", "anchor_text": "here", "paragraph_index": 60}, {"url": "http://matrixmultiplication.xyz/", "anchor_text": "Matrix Multiplication", "paragraph_index": 62}, {"url": "https://www.desmos.com/", "anchor_text": "Desmos", "paragraph_index": 72}, {"url": "https://www.desmos.com/calculator/rhx5tl8ygi", "anchor_text": "Activation Functions", "paragraph_index": 72}, {"url": "https://kite.com/python/answers/how-to-calculate-a-logistic-sigmoid-function-in-python", "anchor_text": "How to calculate a logistic sigmoid function in Python", "paragraph_index": 74}, {"url": "https://www.geeksforgeeks.org/implement-sigmoid-function-using-numpy/", "anchor_text": "Implement sigmoid function using Numpy", "paragraph_index": 74}, {"url": "https://medium.com/ai%C2%B3-theory-practice-business/a-beginners-guide-to-numpy-with-sigmoid-relu-and-softmax-activation-functions-25b840a9a272", "anchor_text": "A beginner\u2019s guide to NumPy with Sigmoid, ReLu and Softmax activation functions", "paragraph_index": 75}, {"url": "https://arxiv.org/pdf/1710.05941.pdf", "anchor_text": "Searching for Activation Functions", "paragraph_index": 76}, {"url": "https://www.bignerdranch.com/blog/implementing-swish-activation-function-in-keras/", "anchor_text": "Implementing Swish Activation Function in Keras", "paragraph_index": 76}, {"url": "https://github.com/chrimaho/VanillaNeuralNetworksInR/blob/master/Functions/functions.R", "anchor_text": "here", "paragraph_index": 82}, {"url": "https://en.wikipedia.org/wiki/Stochastic_gradient_descent", "anchor_text": "Stochastic Gradient Descent", "paragraph_index": 100}, {"url": "https://leon.bottou.org/publications/pdf/nimes-1991.pdf", "anchor_text": "Stochastic Gradient Learning in Neural Networks", "paragraph_index": 100}, {"url": "https://www.sciencedirect.com/science/article/abs/pii/092523129390006O", "anchor_text": "Backpropagation and Stochastic Gradient Descent Method", "paragraph_index": 100}, {"url": "https://towardsdatascience.com/understanding-rmsprop-faster-neural-network-learning-62e116fcf29a", "anchor_text": "RMSProp", "paragraph_index": 101}, {"url": "https://towardsdatascience.com/adam-latest-trends-in-deep-learning-optimization-6be9a291375c", "anchor_text": "Adam", "paragraph_index": 101}, {"url": "https://heartbeat.fritz.ai/an-empirical-comparison-of-optimizers-for-machine-learning-models-b86f29957050", "anchor_text": "An Empirical Comparison of Optimizers for Machine Learning Models", "paragraph_index": 101}, {"url": "https://medium.com/datadriveninvestor/overview-of-different-optimizers-for-neural-networks-e0ed119440c3", "anchor_text": "Overview of different Optimizers for Neural Networks", "paragraph_index": 101}, {"url": "https://github.com/chrimaho/VanillaNeuralNetworksInR/blob/master/Functions/functions.R", "anchor_text": "here", "paragraph_index": 109}, {"url": "https://www.datacamp.com/community/tutorials/parameter-optimization-machine-learning-models", "anchor_text": "Hyperparameter Optimization in Machine Learning Models", "paragraph_index": 117}, {"url": "https://en.wikipedia.org/wiki/Confusion_matrix", "anchor_text": "Confusion Matrix", "paragraph_index": 126}, {"url": "https://en.wikipedia.org/wiki/Receiver_operating_characteristic", "anchor_text": "Receiver Operating Characteristic", "paragraph_index": 128}, {"url": "https://en.wikipedia.org/wiki/Matrix_multiplication", "anchor_text": "matrix multiplication", "paragraph_index": 132}, {"url": "https://en.wikipedia.org/wiki/Differential_calculus", "anchor_text": "differential calculus", "paragraph_index": 132}, {"url": "https://hackernoon.com/building-a-feedforward-neural-network-from-scratch-in-python-d3526457156b", "anchor_text": "Building a Feedforward Neural Network from Scratch in Python", "paragraph_index": 133}, {"url": "https://www.tooploox.com/blog/deep-neural-networks-in-swift-lessons-learned", "anchor_text": "Deep Neural Networks in Swift, lessons learned", "paragraph_index": 133}, {"url": "https://www.geeksforgeeks.org/ml-neural-network-implementation-in-c-from-scratch/", "anchor_text": "Neural Network Implementation in C++ From Scratch", "paragraph_index": 133}, {"url": "https://medium.com/coinmonks/implementing-an-artificial-neural-network-in-pure-java-no-external-dependencies-975749a38114", "anchor_text": "Implementing an Artificial Neural Network in Pure Java (No external dependencies)", "paragraph_index": 133}, {"url": "https://www.linkedin.com/in/alexjscriven/", "anchor_text": "Alex Scriven", "paragraph_index": 134}, {"url": "http://chrimaho.com", "anchor_text": "chrimaho.com", "paragraph_index": 137}], "all_paragraphs": ["1. Introduction2. Background3. Semantics4. Set Up5. Get Data6. Check Data7. Prepare the Data8. Instantiate the Network9. Initialise the Network10. Forward Propagation11. Calculate the Cost12. Backward Propagation13. Update Model Parameters14. Run the Model End-to-End15. Create Prediction16. Conclusion17. Post Script", "Modern-day Data Science techniques frequently use robust frameworks for designing and building machine learning solutions. In the R community, packages such as the tidyverse and the caret packages are frequently referenced; and within Python, packages such as numpy, pandas, sci-kit learn are frequently referenced. There are even some packages that have been built to be used in either language, such as keras, pytorch, tensorflow. However, the limitation of using these packages is the \u2018blackbox\u2019 phenomenon, where users do not understand what is happening behind-the-scenes (or \u2018under the hood\u2019, if you will). Users know how to use the functions, and can interpret the results, but don\u2019t necessarily know how the packages were able to achieve the results.", "The purpose of this paper is to create a \u2018back to basics\u2019 approach to designing Deep Learning solutions. The intention is not to create the most predictive model, nor is it to use the latest and greatest techniques (such as convolution or recursion); but the intention is to create a basic neural network, from scratch, using no frameworks, and to walk through the methodology.", "Note: The word \u2018Vanilla\u2019 in \u2018Vanilla Neural Networks\u2019 simply refers to the fact it is built from scratch, and does not use any pre-existing frameworks in its construction.", "There are already many websites and blogs which explain how this process is done. Such as Jason Brownlee\u2019s article How to Code a Neural Network with Backpropagation In Python (from scratch), and DeepLearning.ai\u2019s notebook dnn_app_utils_v2.py (on the Microsoft Azure Notebooks network). However, these sources are all written in Python. Which is fine, if that\u2019s what is needed, and there are some very legitimate reasons to use Python over other languages. But this paper will be written in R.", "The R language was chosen for two reasons:", "Therefore, let\u2019s see how to architect and construct a Vanilla Neural Network in R.", "This article does not cover the latest and greatest Deep Learning architectures (like Convolution or Recursion). As such the final performance may not be as good as it could be, if these other architectures were used.", "This article does not teach readers about the theoretical mathematical concepts behind how Neural Networks operate. There are plenty of other lectures which teach this information (eg. The Math Behind Neural Networks). In fact, this article assumes a lot of knowledge from the Reader about programming, about calculus, and about the basics behind what a Neural Network conceptually is.", "This article does not cover why Neural Networks work the way they do, and the conceptual understanding behind a Feed-Forward architecture. There are plenty of other blogs (eg. A Beginner Intro to Neural Networks) and videos (eg. Neural Networks series) which cover such information.", "This article doesn\u2019t point the reader to other packages and applications which may already have this information set up and working. Packages like tensorflow and nnet already have this covered.", "What this article actually is, is a functional walk-through, how-to piece, for creating a Vanilla Neural Network (a Feed-Forward Network), from scratch, step-by-step, in the R programming language. It contains lots of code, and lots of technical details.", "This article is laid our in such a way to describe how a Neural Network is built from the ground-up. It will walk through the steps to:", "In the interest of brevity, the functions defined here will not include all the commentary and validations that should be included in a typical function. They will only include basic steps and prompts. However, the source code for this article (located here) does contain all the appropriate function docstrings and assertions.", "For the most part, the syntax in this article is kept to the dplyr \u2018pipe\u2019 method (which uses the %>% symbol). However, in certain sections the R base syntax is used (for example, in function declaration lines).", "Throughout the article, many custom functions are written. Each of these are prefixed with the words get, let and set. The definitions of each are given below.", "The first step is to import the relevant packages. This list includes the the main packages used throughout this process; and the main purpose of which is also listed.", "Note what is listed above about not using existing Deep Learning packages, and yet the tensorflow package is included. Why? Well, this is used for only accessing the data, which will be covered in the next section. The tensorflow package is not used for building and training any networks.", "The dataset to be used is the CIFAR-10 dataset. It\u2019s chosen for a number of reasons, including:", "The following code chunk has the following process steps:", "One of the challenges behind accessing this data from the TensorFlow package is that the classes are only numeric values (0 to 9) for each type of image. The definitions for these images can be found on GitHub (GitHub > EN10 > CIFAR). These classes are defined in the following code chunk.", "It is important to check the data, to ensure that it has been generated correctly, and all the information looks okay. For this, a custom-function is written (get_ObjectAttributes()), the source code for which can be found here. As seen by the following code chunk, the images object is a 4-Dimensional numeric array, with 10,000 images, each 32x32 pixels, and 3 colour chanels. The entire object is over 117 Mb large.", "When checking the classes object, it is a 2-Dimensional numeric array (with only 1 column), but with the same number of images as the images object (which is to be expected), with the frequency of each class label having exactly 1000 images each. The total size is less than 40 Kb.", "After having gained an appreciation of the size of the objects in memory, it is then worth while to check the actual images themselves. As humans, we understand the actual images and the colours, better than we understand the numbers.", "In order to visualise the images, two custom functions are written, as shown in the following code chunk. These functions take in the data (as a 4-Dimensional array), and visualise the images as a plot.", "When the function is run on the top 16 images, the following is displayed. As shown, the images are extremely pixelated (which is expected, as they are only 32x32 pixels each), and you can see how each of the images are categorised.", "There are four steps to preparing the data:", "For the purposes of this paper, let\u2019s assume that we are trying to predict whether or not the picture is a car or not. This will require transforming the data in to a binary classification problem, where the Neural Network will be looking to predict a 1 or a 0 from the data. This will mean that the model output will be a probability distribution of scores, which can be easily classified by changing the cutoff variable.", "First step is to reclassify the data so that all the cars have the value 1, and everything else is 0. We know from the classes we defined before that the car\u2019s already have the value 1, which means the transformation will only need to occur for all the other classes.", "The next task is to split the data in to a training and testing set. The reason for doing this is covered elsewhere (such sites such as Wikipedia\u2019s Cross-Validation, and Google\u2019s Machine Learning Crash Course: Training and Test Sets Splitting Data).", "Firstly, to get an appreciation of the current split of the data, the following code chunk visualises this data using the ggplot2 package. As show, the data is currently distributed with 90% with the class 0, and the remaining 10% with the class 1.", "To implement this data splitting, we are using the caret::createDataPartition() function. This creates a partition object, which is then used to segregate the cifar data accordingly. The proportion of splitting was arbitrarily chosen at 70% for training, and the remaining for testing. However, it could be justified for this to be 80%; this a is hyperparameter which can be tweaked at a later stage.", "After splitting, the data is re-plotted, and the train/test split is easily seen to have achieved an even 70% distribution over the two classes.", "Another method for checking that the data has been split properly, is to again run the get_ObjectAttributes() function, as shown in the following code chunk. The information shown here is consistent with the plot above. It is also interesting to note that the Training Image array is 82 Mb large, which will be important to know for later checking the performance of forward propagation.", "For the first input layer of our Neural Network, we want to be a single dimension of nodes. Therefore, it is necessary to reshape the data from a 4-Dimensional array in to a 2-Dimensional array. This process is called Flattening, and more information can be found here: Memory Layout of Multi-Dimensional Arrays.", "The implementation of this method can be done quite easily with the array() function, as it has the dim= argument which can be used to specify the dimensions required.", "The desired matrix size should have each image in a new row, and each pixel on a different column. Because each pixel is comprised of the 2nd, 3rd and 4th dimensions, we need to take the product of these three numbers, and use that to specify the desired number of columns. Effectively, we are running the equation: 32\u00d732\u00d73, which equates to having 3072 columns. This equation is implemented programmatically in-line, in the next code chunk.", "When checking the object attributes once again, you will see that the image data has been manipulated correctly to have the number of rows as the number of images, and the number of columns as the number of pixels.", "The final step to preparing the data is to standardise the data so that all the elements are between 0 and 1. The reason for this is to prevent exploding and vanishing gradients in later steps, as the Neural Network will try to fit to all the peaks and troughs which is caused from having the data in a 0 to 255 value range.", "As documented on the TensorFlow website, the CIFAR10 data set consists of RGB Image data. And, as documented on Wikipedia, RGB data are all values between 0 and 255.", "Therefore, what is necessary to do is to divide all the elements by 255, and they will inevitably result in a value between 0 and 1. As the image data is currently in an array, the function in the below code chunk will run as a vectorised function over the entire array, dividing all elements by 255 accordingly.", "The data is now ready for the network. Next step is to build the network.", "Some quick notes on what the Network will actually be:", "For the below code chunk, the function set_InstantiateNetwork() is defined. It only has three input arguments, which specify the number of nodes to be used in each layer. Based on this information, the model will be instantiated and returned, ready for initialisation, in the next step.", "The below code chunk instantiates the network. The model will be set up with the following number of nodes in each layer:", "There is a very good website available which allows Neural Networks to be visualised: http://alexlenail.me/NN-SVG/AlexNet.html. The below image is a representation of the network that has just been created.", "Further visualisation can be conducted once the network is fully initialised and forward propagated. See section Check Model Shapes for more details.", "There are four steps to initialising the network:", "At it\u2019s core, weight initialisation is simply generating a random normal number (with \u03bc=0 and \u03c3=1). However, by only using this randomly generated number, the model gradients are found to be exploding or vanishing when attempting to train deeper neural networks. Therefore, these weights need to be scaled after they are initialised, in order to be robust enough to continue to be trained at deeper layers.", "There are many algorithms that can be used for weight initialisation. Two of the more common ones are the Xavier algorithm, and the He algorithm. Some good resources for understanding the details behind these algorithms include:", "The equation for the Xavier initialisation is:", "The equation for the He initialisation is:", "For programmatic purposes, these functions are written with the order value as part of the function arguments. This means that the order of magnitude of the equation can be altered at a later stage and used as a hyperparameter.", "The next step is to construct a function that will initialise all the relevant aspects of an individual layer. This step is important because it is where the weight matrices are created, and these must be constructed in a certain manner so as to ensure that the dimensions allow successful forward propagation.", "The steps for the layer construction are as follows:", "In order to achieve this, the function arguments used are for the function include:", "The purpose of the set_InitialiseModel() function is to loop through each layer in the network_model object, to initialise them in accordance with the parameters set within the model itself. This function will take the number of nodes (set by the set_InstantiateNetwork() function).", "The function will take only three input parameters:", "Notice, this function uses the user-defined function get_CountOfElementsWithCondition(). This function allows for the counting of the number of hidden layers in the model. The source code for the function can be found here.", "The below code chunk initialises the network using the function defined above. The method utilised is %<>%, which is defined in the magrittr package, which states that it is used to update a value on the left-hand side by first piping it into the first argument position on the right-hand side, and then assigning the result back to the object on the left hand side.", "For a quick sanity-check, the below code chunk checks for the number of parameters for the defined model. This again uses a custom function get_ModelParametersCount(), the definition for which can be found in the source code for this article (located here).", "There are over 320,000 parameters in this neural network. Every single one of these parameters will need to be trained, and each of them will have an influence on the final outcome. We will continue to walk through how this can be achieved in the following sections.", "There is a very good website which outlines what actually happens during a matrix multiplication: Matrix Multiplication.", "The theory for the forward propagation method is as follows:", "To illustrate, the following matrices shows the first step of the forward propagation, using fictitious numbers.", "The below code shows how this process is implemented programmatically.", "The next diagram shows how the bias matrix is applied. As shown, each element in the bias matrix is applied horizontally to each element of the initial matrix. This diagram shows how it works.", "The below code shows how this process is implemented programmatically.", "The activation function is one which is defined at the time of running the function. The algorithm is applied to each element of the initial matrix. In this instance, a simple multiplication of the initial matrix is used.", "The below code shows how this process is implemented programmatically.", "When combined together, the linear algebraic function is implemented in just three lines of code, as shown in the below code chunk in the set_LinearForward() function.", "The real power of Neural Network comes from their Activation function. It is now the network is able to capture the non-linear aspects, and this is what emphasises their predictive power.", "The Activation function can be one of many different algorithms, depending on the purpose of the network. The choice of Activation can be a hyperparameter which is chosen at a later stage. The Desmos website provides an excellent interactive way of reviewing the different types of Activations: Activation Functions.", "Each of the Activation functions are defined individually, and each only take one argument for the function, which is a matrix for which the activation will be applied. For the purposes of brevity, four of the more popular activations are provided here; but there are many, many others which can be used. The resources and equations for how to compute these functions are also provided:", "Sources:1. How to calculate a logistic sigmoid function in Python2. Implement sigmoid function using Numpy", "Sources:1. A beginner\u2019s guide to NumPy with Sigmoid, ReLu and Softmax activation functions", "Sources:1. Searching for Activation Functions2. Implementing Swish Activation Function in Keras", "These Activation Functions are defined programmatically as follows:", "The set_ForwardProp() function pulls together all of the components mentioned above. It implements the following steps:", "In order to implement this process, the function takes only four arguments:", "The last step of the Forward Propagation process is to actually run the function. In the below code chunk, the tic() and toc() functions are implemented to time how long a process takes to run.", "As mentioned above in the Split Data section, the trn_img object is over 82 Mb large, and the model has over 320,000 parameters. The entire end-to-end process for forward propagation took only 7 seconds to run; which is quite impressive, and is a testament to the power of mathematics.", "The Network can now be printed using the custom function get_PrintNetwork(). This function returns each layer in an individual box, containing key information like the relevant shapes of the matrices and the activation functions used. The source code for the function can be found here.", "Once the forward-propagation part has been completed, it\u2019s then necessary to get a measure for how wrong the model is. This will then be used to update the model parameters during the backward propagation steps.", "The first step is to write the functions to be used for getting the cost for the model. Ultimately, it does not matter how bad the results are for the first round of training; remember, the model was initialised with random numbers. What\u2019s important is that the cost functions should be determining a single value for the cost of the network, and this single function will be what is used for the derivative functions to be used in the backwards propagation steps.", "Note here, that there is a very small epsilon value used (epsi), which effectively adjusts for a perfect prediction made by the model. The reason for this is not that we don\u2019t want the model to predict a perfect value, it\u2019s that we want the model to predict the probability of a perfect value. Furthermore, it\u2019s impossible to take a logarithmic value of a 0, so therefore it\u2019s necessary to adjust this to be just a little bit off-zero.", "Then, the cost must be applied back on to the network. For this, the exact same value is applied to each layer of the network.", "The below code chunk runs the cost functions, utilising the functions defined above.", "The function of back-propagation is intended to take the cost of the model, and then differentiate each of the Weights and Biases matrices in the network to determine to what extent each and every single parameter in the network contributed to that final cost value. To do this, the process works backwards from the end to the start, following the following steps:", "Each step in the back-propagation process requires calculus to derive the values, and the final function is implemented here. As this paper is not intended to show how to derive the equations, but more how to run the functions, the necessary algebraic steps are not included here.", "As similar to the functions used to get the cost, the cost differential values are first calculated, then applied to each layer of the network.", "As each of the activations have their own function, there is also a derivation of that function which can be calculated using calculus.", "Having had defined all this so far, the next step is to combine in to one single function, which can be used once per layer, which runs the necessary back-propagation differentiation functions.", "Notice here that the output is actually a list of three elements. This a key difference between R and python. In R, functions can only output a single element; as compared to python which is able to return multiple elements from each function.", "After having defined the differentiation functions, next is necessary to combine these individual functions in to a single, combined, which can be run once per layer.", "First, we will set up the function, and secondly we will run it.", "The backward propagation function to be defined (set_BackwardProp()) must be designed to run through the following steps:", "Having defined this function, the next step is to run the function. The below code chunk is wrapped with the tic() and toc() functions, in order to determine how much time it took for the function to run.", "As shown, it took approximately 9 seconds to run this function. This is quite impressive, considering that there are over 320,000 parameters to be updated (see Check Model Parameters section).", "After the model parameters have been differenced, it\u2019s then necessary to update the relevant parameters of the network (the weights and biases) before re-training the network again by re-running the forward propagation function.", "The method used to update these parameters is called Stochastic Gradient Descent. For more information, see Stochastic Gradient Learning in Neural Networks or Backpropagation and Stochastic Gradient Descent Method.", "For sure, there are other methods of implementing neural network optimisation. The literature has been spending a lot of effort in this area. There are algorithms such as RMSProp and Adam, which implement intelligent methods to achieve a faster convergence, and a more accurate final result. Some good sources for this include An Empirical Comparison of Optimizers for Machine Learning Models and Overview of different Optimizers for Neural Networks. For further enhancement and optimisation, it is important to explore these optimisation options.", "Nonetheless, the process of implementing Stochastic Gradient Descent is really quite simple:", "To set the function for updating the model parameters, the following steps are used:", "Having defined the function, then we run the model our network.", "Now, it\u2019s time to bring it all together. Running the model end-to-end essentially means:", "Each time this is repeated is called an Epoch. It is quite typical for networks to be updated over many many Epochs. Sometimes hundreds, sometimes thousands of Epochs; the exact number of Epochs to run is a discretionary decision by the Data Scientist, based on numerous variables.", "There is one additional step to be added in here, and that is batching the data. For this, we can consider within each Epoch, the data will be batched up in to equally divisible groups, and used for subsequent processing. In this instance, the model parameters will be updated after each batch. And when the full amount of data has been run through the model in it\u2019s entirety, this is then considered one Epoch.", "To illustrate this programmatically, the following function is written.", "Notice here, that there are many custom functions included (the source code for each of which can be found here). These functions include:", "The steps for this function include:", "Having had set up the training function, let\u2019 now run it.", "It works! This plot proves that the model is training, and it is continuing to learn and improve its performance over time.", "Notice here that the cost line begins at approximately 0.4, and very quickly decreases to 0.3 after 20 Epochs. In order for it to complete the full 50 Epochs, this took approximately 12 minutes. This can be seen as a success.", "Due to the nature of how these functions have been set up, it is very easy for further experimentation and optimisation.", "It may be reasonable to try:", "Here is an experimentation of trying some different parameters. Notice how the results also change. This next round of training took 34 minutes to run.", "It is then the job of the Data Scientist to identify the optimal configuration of these parameters, in order to achieve a better performance rate. See Hyperparameter Optimization in Machine Learning Models for more details.", "However, as this article is not to achieve the best results, but more to show how to achieve said results, and the processes/methodologies involved. Therefore, further experimentation will not be conducted in this paper.", "After having trained the network, next step is to apply it to the test data to see how accurate it can predict unknown data.", "The first step is to write two custom functions. The first will take in the test data and trained model, and will return a data frame containing the predicted values and the truth values. Then pretty-print the Confusion Matrix in a ggplot-style of output.", "The next step is to actually run the prediction. This step is quite self explanatory. The parameters for this function includes the test data, and the trained model.", "Having created this prediction, it\u2019s then necessary to visualise the output. In the same way as the Check Images section, the following code chunk visualises the top 16 images, and returns a label for each.", "As seen, the model is clearly able to recognise some, but it also makes mistakes on others.", "The next step then is to statistically test the data to see how accurate it may be. For this, the confusionMatrix() function (from the caret package) used. From this output, it is possible to select the appropriate metric in order for further optimising the neural network.", "Since this article is onlyabout showing the method, then we will not progress any further with more optimisation here.", "Next step is to plot the confusion matrix. A good source of understanding and interpreting confusion matrices can be found here: Confusion Matrix. In this website, it also includes an image (also copied below) for a very good visual understanding.", "When we visualise the confusion matrix of our model, we can see that it successfully predicted a majority of the images. However, the largest number of bad classifications is where it predicted that the image was a car, when it actually was not.", "Another good analytical and plotting tool is to use the ROC Curve (receiver operating characteristics). An ROC Curve is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied (see Receiver Operating Characteristic).", "Essentially, a model that creates a perfect prediction would have the curve perfectly \u2018hugging\u2019 the top-left of this plot. As our model is not doing this, it is clearly obvious that further training and optimisation is necessary. For this, it is necessary to continue experimenting as mentioned in the Further Experimentation section.", "This is an effective representation for how to build Vanilla Neural Networks in R. Here, it has been shown how to:", "It has also been shown how to do this entirely in R, and without using any pre-defined deep learning framework. For sure, there are other packages that can perform all of the steps, and perhaps in a more computationally efficient manner. But the use of these packages is not the target of this article. Here, the aim is to dispel the \u2018blackbox\u2019 phenomenon, and to show how to create these deep learning frameworks from the ground up.", "As shown, the architecture of these feed-forward neural networks effectively use matrix multiplication and differential calculus in order to adjust the \u2018weights\u2019 of a network, and increase it\u2019s predictive power. It does require lots of data, and does require a long time to train and optimise. Furthermore, there are many (many, many) different architectural choices that can be made along the way, all of which are at the discretion of the Data Scientist applying this methodology.", "At the end of the day, this article has proven that deep learning can definitely be achieved in R. Thereby, dispelling the myth that \u2018in order to do Deep Learning, you must use Python\u2019. Certainly, there are many successful implementations of Deep Learning in Python; for example: Building a Feedforward Neural Network from Scratch in Python. There\u2019s also example of achieving the same in Swift (Deep Neural Networks in Swift, lessons learned), and in C++ (Neural Network Implementation in C++ From Scratch), and in Java (Implementing an Artificial Neural Network in Pure Java (No external dependencies)). Therefore, the specific language used is effectively irrelevant. What matters is the use case, the environment, the business artifacts, and the language that the Data Scientist feels comfortable in.", "Acknowledgements: This report was compiled with some assistance from others. Acknowledgements go to: \u2014 Alex Scriven", "Change Log: This publication was modified on the following dates: \u2014 02/Nov/2020: Original Publication date.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "I\u2019m a keen Data Scientist and Business Leader, interested in Innovation, Digitisation, Best Practice & Personal Development. Check me out: chrimaho.com"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F43b028f415&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvanilla-neural-networks-in-r-43b028f415&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvanilla-neural-networks-in-r-43b028f415&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvanilla-neural-networks-in-r-43b028f415&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvanilla-neural-networks-in-r-43b028f415&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----43b028f415--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----43b028f415--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://chrimaho.medium.com/?source=post_page-----43b028f415--------------------------------", "anchor_text": ""}, {"url": "https://chrimaho.medium.com/?source=post_page-----43b028f415--------------------------------", "anchor_text": "Chris Mahoney"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F56d03114dd5a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvanilla-neural-networks-in-r-43b028f415&user=Chris+Mahoney&userId=56d03114dd5a&source=post_page-56d03114dd5a----43b028f415---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F43b028f415&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvanilla-neural-networks-in-r-43b028f415&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F43b028f415&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvanilla-neural-networks-in-r-43b028f415&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.linkedin.com/in/alexjscriven/", "anchor_text": "Alex Scriven"}, {"url": "https://github.com/karanvivekbhargava/vanilla-neural-network", "anchor_text": "GitHub"}, {"url": "http://266e", "anchor_text": "Prepare the Data"}, {"url": "https://www.r-project.org/", "anchor_text": "R"}, {"url": "https://www.tidyverse.org/", "anchor_text": "tidyverse"}, {"url": "http://caret.r-forge.r-project.org/", "anchor_text": "caret"}, {"url": "http://127.0.0.1:17287/rmd_output/0/", "anchor_text": "Python"}, {"url": "https://numpy.org/", "anchor_text": "numpy"}, {"url": "https://pandas.pydata.org/", "anchor_text": "pandas"}, {"url": "https://scikit-learn.org/", "anchor_text": "sci-kit learn"}, {"url": "https://keras.io/", "anchor_text": "keras"}, {"url": "https://pytorch.org/", "anchor_text": "pytorch"}, {"url": "https://www.tensorflow.org/", "anchor_text": "tensorflow"}, {"url": "https://en.wikipedia.org/wiki/Convolutional_neural_network", "anchor_text": "convolution"}, {"url": "https://en.wikipedia.org/wiki/Recurrent_neural_network", "anchor_text": "recursion"}, {"url": "https://machinelearningmastery.com/about/", "anchor_text": "Jason Brownlee"}, {"url": "https://machinelearningmastery.com/implement-backpropagation-algorithm-scratch-python/", "anchor_text": "How to Code a Neural Network with Backpropagation In Python (from scratch)"}, {"url": "https://notebooks.azure.com/goldengrape/projects/deeplearning-ai", "anchor_text": "DeepLearning.ai"}, {"url": "https://notebooks.azure.com/goldengrape/projects/deeplearning-ai/html/COURSE%201%20Neural%20Networks%20and%20Deep%20Learning/Week%204/Deep%20Neural%20Network%20Application_%20Image%20Classification/dnn_app_utils_v2.py", "anchor_text": "dnn_app_utils_v2.py"}, {"url": "https://notebooks.azure.com/", "anchor_text": "Microsoft Azure Notebooks"}, {"url": "https://www.tensorflow.org/tutorials/images/cnn", "anchor_text": "Convolution"}, {"url": "https://www.tensorflow.org/guide/keras/rnn", "anchor_text": "Recursion"}, {"url": "https://www.youtube.com/watch?v=RiqWATOoos8", "anchor_text": "The Math Behind Neural Networks"}, {"url": "https://medium.com/@purnasaigudikandula/a-beginner-intro-to-neural-networks-543267bda3c8", "anchor_text": "A Beginner Intro to Neural Networks"}, {"url": "https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi", "anchor_text": "Neural Networks series"}, {"url": "https://www.rdocumentation.org/packages/tensorflow", "anchor_text": "tensorflow"}, {"url": "https://www.rdocumentation.org/packages/nnet", "anchor_text": "nnet"}, {"url": "https://github.com/chrimaho/VanillaNeuralNetworksInR/blob/master/Functions/functions.R", "anchor_text": "here"}, {"url": "https://www.rdocumentation.org/packages/dplyr", "anchor_text": "dplyr"}, {"url": "https://www.rdocumentation.org/packages/base", "anchor_text": "base"}, {"url": "https://www.cs.toronto.edu/~kriz/cifar.html", "anchor_text": "CIFAR-10"}, {"url": "https://github.com/EN10/CIFAR#classes", "anchor_text": "GitHub > EN10 > CIFAR"}, {"url": "https://github.com/chrimaho/VanillaNeuralNetworksInR/blob/master/Functions/functions.R", "anchor_text": "here"}, {"url": "https://en.wikipedia.org/wiki/Cross-validation_%28statistics%29", "anchor_text": "Wikipedia\u2019s Cross-Validation"}, {"url": "https://developers.google.com/machine-learning/crash-course/training-and-test-sets/splitting-data", "anchor_text": "Google\u2019s Machine Learning Crash Course: Training and Test Sets Splitting Data"}, {"url": "https://eli.thegreenplace.net/2015/memory-layout-of-multi-dimensional-arrays", "anchor_text": "Memory Layout of Multi-Dimensional Arrays"}, {"url": "https://www.tensorflow.org/api_docs/python/tf/keras/datasets/cifar10/load_data", "anchor_text": "TensorFlow"}, {"url": "https://www.cs.toronto.edu/~kriz/cifar.html", "anchor_text": "CIFAR10"}, {"url": "https://en.wikipedia.org/wiki/RGB_color_model", "anchor_text": "Wikipedia"}, {"url": "http://alexlenail.me/NN-SVG/AlexNet.html", "anchor_text": "http://alexlenail.me/NN-SVG/AlexNet.html"}, {"url": "https://prateekvjoshi.com/2016/03/29/understanding-xavier-initialization-in-deep-neural-networks/", "anchor_text": "Xavier"}, {"url": "https://medium.com/@prateekvishnu/xavier-and-he-normal-he-et-al-initialization-8e3d7a087528", "anchor_text": "He"}, {"url": "https://towardsdatascience.com/weight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79", "anchor_text": "Weight Initialization in Neural Networks"}, {"url": "http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf", "anchor_text": "Understanding the difficulty of training deep feedforward neural networks"}, {"url": "https://arxiv.org/pdf/1502.01852.pdf", "anchor_text": "Surpassing Human-Level Performance on ImageNet Classification"}, {"url": "https://github.com/chrimaho/VanillaNeuralNetworksInR/blob/master/Functions/functions.R", "anchor_text": "here"}, {"url": "https://github.com/chrimaho/VanillaNeuralNetworksInR/blob/master/Functions/functions.R", "anchor_text": "here"}, {"url": "http://matrixmultiplication.xyz/", "anchor_text": "Matrix Multiplication"}, {"url": "https://www.desmos.com/", "anchor_text": "Desmos"}, {"url": "https://www.desmos.com/calculator/rhx5tl8ygi", "anchor_text": "Activation Functions"}, {"url": "https://kite.com/python/answers/how-to-calculate-a-logistic-sigmoid-function-in-python", "anchor_text": "How to calculate a logistic sigmoid function in Python"}, {"url": "https://www.geeksforgeeks.org/implement-sigmoid-function-using-numpy/", "anchor_text": "Implement sigmoid function using Numpy"}, {"url": "https://medium.com/ai%C2%B3-theory-practice-business/a-beginners-guide-to-numpy-with-sigmoid-relu-and-softmax-activation-functions-25b840a9a272", "anchor_text": "A beginner\u2019s guide to NumPy with Sigmoid, ReLu and Softmax activation functions"}, {"url": "https://towardsdatascience.com/softmax-activation-function-explained-a7e1bc3ad60", "anchor_text": "Softmax Activation Function Explained"}, {"url": "https://arxiv.org/pdf/1710.05941.pdf", "anchor_text": "Searching for Activation Functions"}, {"url": "https://www.bignerdranch.com/blog/implementing-swish-activation-function-in-keras/", "anchor_text": "Implementing Swish Activation Function in Keras"}, {"url": "https://github.com/chrimaho/VanillaNeuralNetworksInR/blob/master/Functions/functions.R", "anchor_text": "here"}, {"url": "https://en.wikipedia.org/wiki/Stochastic_gradient_descent", "anchor_text": "Stochastic Gradient Descent"}, {"url": "https://leon.bottou.org/publications/pdf/nimes-1991.pdf", "anchor_text": "Stochastic Gradient Learning in Neural Networks"}, {"url": "https://www.sciencedirect.com/science/article/abs/pii/092523129390006O", "anchor_text": "Backpropagation and Stochastic Gradient Descent Method"}, {"url": "https://towardsdatascience.com/understanding-rmsprop-faster-neural-network-learning-62e116fcf29a", "anchor_text": "RMSProp"}, {"url": "https://towardsdatascience.com/adam-latest-trends-in-deep-learning-optimization-6be9a291375c", "anchor_text": "Adam"}, {"url": "https://heartbeat.fritz.ai/an-empirical-comparison-of-optimizers-for-machine-learning-models-b86f29957050", "anchor_text": "An Empirical Comparison of Optimizers for Machine Learning Models"}, {"url": "https://medium.com/datadriveninvestor/overview-of-different-optimizers-for-neural-networks-e0ed119440c3", "anchor_text": "Overview of different Optimizers for Neural Networks"}, {"url": "https://github.com/chrimaho/VanillaNeuralNetworksInR/blob/master/Functions/functions.R", "anchor_text": "here"}, {"url": "https://www.datacamp.com/community/tutorials/parameter-optimization-machine-learning-models", "anchor_text": "Hyperparameter Optimization in Machine Learning Models"}, {"url": "https://en.wikipedia.org/wiki/Confusion_matrix", "anchor_text": "Confusion Matrix"}, {"url": "https://en.wikipedia.org/wiki/Receiver_operating_characteristic", "anchor_text": "Receiver Operating Characteristic"}, {"url": "https://en.wikipedia.org/wiki/Matrix_multiplication", "anchor_text": "matrix multiplication"}, {"url": "https://en.wikipedia.org/wiki/Differential_calculus", "anchor_text": "differential calculus"}, {"url": "https://hackernoon.com/building-a-feedforward-neural-network-from-scratch-in-python-d3526457156b", "anchor_text": "Building a Feedforward Neural Network from Scratch in Python"}, {"url": "https://www.tooploox.com/blog/deep-neural-networks-in-swift-lessons-learned", "anchor_text": "Deep Neural Networks in Swift, lessons learned"}, {"url": "https://www.geeksforgeeks.org/ml-neural-network-implementation-in-c-from-scratch/", "anchor_text": "Neural Network Implementation in C++ From Scratch"}, {"url": "https://medium.com/coinmonks/implementing-an-artificial-neural-network-in-pure-java-no-external-dependencies-975749a38114", "anchor_text": "Implementing an Artificial Neural Network in Pure Java (No external dependencies)"}, {"url": "https://www.linkedin.com/in/alexjscriven/", "anchor_text": "Alex Scriven"}, {"url": "https://rpubs.com/chrimaho/VanillaNeuralNetworksInR", "anchor_text": "RPubs/chrimaho/VanillaNeuralNetworksInR"}, {"url": "https://github.com/chrimaho/VanillaNeuralNetworksInR", "anchor_text": "GitHub/chrimaho/VanillaNeuralNetworksInR"}, {"url": "https://medium.com/@chrimaho/43b028f415?source=friends_link&sk=f47b3d6f9f539e907d272966fa88bcb8", "anchor_text": "Medium/chrimaho/VanillaNeuralNetworksInR"}, {"url": "https://medium.com/tag/r?source=post_page-----43b028f415---------------r-----------------", "anchor_text": "R"}, {"url": "https://medium.com/tag/data-science?source=post_page-----43b028f415---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----43b028f415---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/neural-network-software?source=post_page-----43b028f415---------------neural_network_software-----------------", "anchor_text": "Neural Network Software"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----43b028f415---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F43b028f415&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvanilla-neural-networks-in-r-43b028f415&user=Chris+Mahoney&userId=56d03114dd5a&source=-----43b028f415---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F43b028f415&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvanilla-neural-networks-in-r-43b028f415&user=Chris+Mahoney&userId=56d03114dd5a&source=-----43b028f415---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F43b028f415&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvanilla-neural-networks-in-r-43b028f415&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----43b028f415--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F43b028f415&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvanilla-neural-networks-in-r-43b028f415&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----43b028f415---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----43b028f415--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----43b028f415--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----43b028f415--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----43b028f415--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----43b028f415--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----43b028f415--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----43b028f415--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----43b028f415--------------------------------", "anchor_text": ""}, {"url": "https://chrimaho.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://chrimaho.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Chris Mahoney"}, {"url": "https://chrimaho.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "65 Followers"}, {"url": "http://chrimaho.com", "anchor_text": "chrimaho.com"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F56d03114dd5a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvanilla-neural-networks-in-r-43b028f415&user=Chris+Mahoney&userId=56d03114dd5a&source=post_page-56d03114dd5a--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F50cd7cc4eee7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvanilla-neural-networks-in-r-43b028f415&newsletterV3=56d03114dd5a&newsletterV3Id=50cd7cc4eee7&user=Chris+Mahoney&userId=56d03114dd5a&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}