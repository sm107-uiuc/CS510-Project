{"url": "https://towardsdatascience.com/estimating-probabilities-with-bayesian-modeling-in-python-7144be007815", "time": 1682994025.916928, "path": "towardsdatascience.com/estimating-probabilities-with-bayesian-modeling-in-python-7144be007815/", "webpage": {"metadata": {"title": "Estimating Probabilities with Bayesian Modeling in Python | by Will Koehrsen | Towards Data Science", "h1": "Estimating Probabilities with Bayesian Modeling in Python", "description": "Bayesian modeling provides a robust framework for estimating probabilities from limited data. In this article, we\u2019ll see how to use Bayesian methods in Python to solve a statistics problem."}, "outgoing_paragraph_urls": [{"url": "https://rationalwiki.org/wiki/Bayesian", "anchor_text": "as a Bayesian", "paragraph_index": 1}, {"url": "https://en.wikipedia.org/wiki/Bayesian_inference", "anchor_text": "Bayesian Inference", "paragraph_index": 4}, {"url": "https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers", "anchor_text": "basic probabilistic programming", "paragraph_index": 4}, {"url": "https://docs.pymc.io/api.html", "anchor_text": "PyMC3", "paragraph_index": 4}, {"url": "https://github.com/WillKoehrsen/probabilistic-programming/blob/master/Estimating%20Probabilities%20with%20Bayesian%20Inference.ipynb", "anchor_text": "Jupyter Notebook on GitHub", "paragraph_index": 4}, {"url": "https://course.fast.ai/about.html", "anchor_text": "top-down approach to learning new topics", "paragraph_index": 6}, {"url": "https://www.fast.ai/2016/10/08/teaching-philosophy/", "anchor_text": "exemplified in the excellent fast.ai courses", "paragraph_index": 7}, {"url": "https://en.wikipedia.org/wiki/Probability_mass_function", "anchor_text": "probability mass function", "paragraph_index": 11}, {"url": "http://nbviewer.jupyter.org/github/AllenDowney/ThinkBayes2/blob/master/solutions/dirichlet_soln.ipynb", "anchor_text": "here for Allen Downey\u2019s solution", "paragraph_index": 22}, {"url": "https://en.wikipedia.org/wiki/Statistical_inference", "anchor_text": "Inference in statistics", "paragraph_index": 23}, {"url": "https://towardsdatascience.com/markov-chain-monte-carlo-in-python-44f7e609be98", "anchor_text": "Markov Chain Monte Carlo", "paragraph_index": 24}, {"url": "http://users.cecs.anu.edu.au/~ssanner/MLSS2010/Johnson1.pdf", "anchor_text": "analytically expressed", "paragraph_index": 26}, {"url": "https://arxiv.org/abs/1111.4246", "anchor_text": "No-U-Turn Sampler", "paragraph_index": 32}, {"url": "https://en.wikipedia.org/wiki/Credible_interval", "anchor_text": "credible interval", "paragraph_index": 37}, {"url": "https://andrewgelman.com/2015/05/19/bayesian-inference-the-advantages-and-the-risks/", "anchor_text": "benefits of Bayesian Inference", "paragraph_index": 52}, {"url": "http://twitter.com/@koehrsen_will", "anchor_text": "@koehrsen_will", "paragraph_index": 54}, {"url": "https://willk.online", "anchor_text": "willk.online", "paragraph_index": 54}], "all_paragraphs": ["It started, as the best projects always do, with a few tweets:", "This may seem like a simple problem \u2014 the prevalences are simply the same as the observed data (50% lions, 33% tigers and 17% bears) right? If you believe observations we make are a perfect representation of the underlying truth, then yes, this problem could not be easier. However, as a Bayesian, this view of the world and the subsequent reasoning is deeply unsatisfying.", "First, how can we be sure this single trip to the preserve was indicative of all trips? What if we went during the winter when the bears were hibernating? We need to include uncertainty in our estimate considering the limited data. Second, how can we incorporate prior beliefs about the situation into this estimate? If we have heard from a friend the preserve has an equal number of each animal, then surely this should play some role in our estimate.", "Fortunately, there is a solution that allows to express uncertainty and incorporate prior information into our estimate: Bayesian Inference.", "In this article, we\u2019ll explore the problem of estimating probabilities from data in a Bayesian framework, along the way learning about probability distributions, Bayesian Inference, and basic probabilistic programming with PyMC3. The complete code is available as a Jupyter Notebook on GitHub.", "Often, especially in statistics, I find the theory behind a solution more confusing than actually solving the problem. (I\u2019m convinced statisticians complicate statistics to justify their existence.) Coding an answer and visualizing the solution usually does more for me than reading endless equations. Therefore, when I approached this problem, I studied just enough of the ideas to code a solution, and only after did I dig back into the concepts.", "This reflects my general top-down approach to learning new topics. Instead of starting with the fundamentals \u2014 which are usually tedious and difficult to grasp \u2014 find out how to implement an idea so you know why it\u2019s useful and then go back to the formalisms. So, if you feel yourself getting frustrated with the theory, move on to the solution (starting with the Inference section below), and then come back to the concepts if you\u2019re still interested.", "(This top-down philosophy is exemplified in the excellent fast.ai courses on deep learning. These courses, besides effectively teaching neural networks, have been influential in my approach to learning new techniques.)", "Since we want to solve this problem with Bayesian methods, we need to construct a model of the situation. The basic set-up is we have a series of observations: 3 tigers, 2 lions, and 1 bear, and from this data, we want to estimate the prevalence of each species at the wildlife preserve. That is, we are looking for the posterior probability of seeing each species given the data.", "Before we begin we want to establish our assumptions:", "The overall system, where we have 3 discrete choices (species) each with an unknown probability and 6 total observations is a multinomial distribution. The multinomial distribution is the extension of the binomial distribution to the case where there are more than 2 outcomes. A simple application of a multinomial is 5 rolls of a dice each of which has 6 possible outcomes.", "A probability mass function of a multinomial with 3 discrete outcomes is shown below:", "A Multinomial distribution is characterized by k, the number of outcomes, n, the number of trials, and p, a vector of probabilities for each of the outcomes. For this problem, p is our ultimate objective: we want to figure out the probability of seeing each species from the observed data. In Bayesian statistics, the parameter vector for a multinomial is drawn from a Dirichlet Distribution, which forms the prior distribution for the parameter.", "The Dirichlet Distribution, in turn, is characterized by, k, the number of outcomes, and alpha, a vector of positive real values called the concentration parameter. This is called a hyperparameter because it is a parameter of the prior. (This chain can keep going: if alpha comes from another distribution then this is a hyperprior which could have its own parameters called hyperyhyperparameters!). We\u2019ll stop our model at this level by explicitly setting the values of alpha, which has one entry for each outcome.", "The best way to think of the Dirichlet parameter vector is as pseudocounts, observations of each outcome that occur before the actual data is collected. These pseudocounts capture our prior belief about the situation. For example, because we think the prevalence of each animal is the same before going to the preserve, we set all of the alpha values to be equal, say alpha = [1, 1, 1].", "Conversely, if we expected to see more bears, we could use a hyperparameter vector like [1, 1, 2] (where the ordering is [lions, tigers, bears]. The exact value of the pseudocounts reflects the level of confidence we have in our prior beliefs. Larger pseudocounts will have a greater effect on the posterior estimate while smaller values will have a smaller effect and will let the data dominate the posterior. We\u2019ll see this when we get into inference, but for now, remember that the hyperparameter vector is pseudocounts, which in turn, represent our prior belief.", "A Dirichlet distribution with 3 outcomes is shown below with different values of the hyperparameter vector. Color indicates the concentration weighting.", "There\u2019s a lot more detail we don\u2019t need to get into here, but if you\u2019re still curious, see some of the sources listed below.", "Our ultimate goal is to estimate the posterior distribution for the probability of observing each species, p, conditioned on the data and hyperparameters:", "Our final model, consisting of a multinomial distribution with Dirichlet priors is called a Dirichlet-Multinomial and is visualized below:", "A summary of the problem specifics is below:", "If you still want more background details, here are some of the sources I relied on (the first is probably the most valuable):", "There are also other ways to approach this problem; see here for Allen Downey\u2019s solution which yields similar results.", "Now that we have the model of the problem, we can solve for the posteriors using Bayesian methods. Inference in statistics is the process of estimating (inferring) the unknown parameters of a probability distribution from data. Our unknown parameters are the prevalence of each species while the data is our single set of observations from the wildlife preserve. Our goal is to find the posterior distribution of the probability of seeing each species.", "Our approach to deriving the posterior will use Bayesian inference. This means we build the model and then use it to sample from the posterior to approximate the posterior with Markov Chain Monte Carlo (MCMC) methods. We use MCMC when exact inference is intractable, and, as the number of samples increases, the estimated posterior converges to the true posterior.", "The result of MCMC is not just one number for our answer, but rather a range of samples that lets us quantify our uncertainty especially with limited data. We\u2019ll see how to perform Bayesian inference in Python shortly, but if we do want a single estimate, we can use the Expected Value of the distribution.", "The Expected Value is the mean of the posterior distribution. For a Dirichlet-Multinomial, it can be analytically expressed:", "Once we start plugging in numbers, this becomes easy to solve. N is the number of trials, 6, c_i is the observed count for each category, and alpha_i is the pseudocount (hyperparameter) for each category. Setting all alphas equal to 1, the expected species probabilities can be calculated:", "This represents the expected value taking into account the pseudocounts which corporate our initial belief about the situation.", "We can adjust our level of confidence in this prior belief by increasing the magnitude of the pseudocounts. This forces the expected values closer to our initial belief that the prevalence of each species is equal. The expected values for several different hyperparameters are shown below:", "Our choice of hyperparameters has a large effect. If we are more confident in our belief, then we increase the weight of the hyperparameters. On the other hand, if we want the data to have more weight, we reduce the pseudocounts.", "While this result provides a point estimate, it\u2019s misleading because it does not express any uncertainty. We only went to the wildlife preserve once, so there should be a large amount of uncertainty in these estimates. With Bayesian Inference, we can get both point estimates and the uncertainty.", "To get a range of estimates, we use Bayesian inference by constructing a model of the situation and then sampling from the posterior to approximate the posterior. This is implemented through Markov Chain Monte Carlo (or a more efficient variant called the No-U-Turn Sampler) in PyMC3. Compared to the theory behind the model, setting it up in code is simple:", "Then, we can sample from the posterior:", "This code draws 1000 samples from the posterior in 2 different chains (with 500 samples for tuning that are discarded). We are left with a trace which contains all of the samples drawn during the run. We use this trace to estimate the posterior distribution.", "PyMC3 has many methods for inspecting the trace such as pm.traceplot:", "On the left we have a kernel density estimate for the sampled parameters \u2014 a PDF of the event probabilities. On the right, we have the complete samples drawn for each free parameter in the model. We can see from the KDE that p_bears<p_tigers<p_lions as expected but there is some uncertainty. A better way to view this uncertainty is through pm.posterior_plot:", "Here are histograms indicating the number of times each probability was sampled from the posterior. We have a point estimate for the probabilities \u2014 the mean \u2014 as well as the Bayesian equivalent of the confidence interval \u2014 the 95% highest probability density (also known as a credible interval). We see an extreme level of uncertainty in these estimates, as befits the limited data.", "To quantify the level of uncertainty we can get a dataframe of the results:", "This shows the best estimate (mean) for the prevalence but also that the 95% credible interval is very large. We can only nail down the prevalence of lions to between 16.3% and 73.6% based on our single trip to the preserve!", "Bayesian Inference is so powerful because of this built-in uncertainty. In the real-world, data is always noisy, and we usually have less than we want. Therefore, anytime we make an estimate from data we have to show this uncertainty. For this problem, no one is going to be hurt if we get the percentage of bears at the wildlife preserve incorrect, but what if we were doing a similar method with medical data and inferring disease probability?", "Once we have the trace, we can draw samples from the posterior to simulate additional trips to the preserve. For example, let\u2019s consider going 1000 more times. How many of each species can we expect to see on each trip?", "Based on the evidence, there are times when we go to the preserve and see 5 bears and 1 tiger! Granted, this is not very likely, graphs such as these show the entire range of possible outcomes instead of only one. Our single trip to the preserve was just one outcome: 1000 simulations show that we can\u2019t expect the exact observations every time we go to the preserve.", "If we want to see the new Dirichlet distribution after sampling, it looks like:", "What happens when we go 4 times to the preserve and want to incorporate additional observations in our model? In PyMC3, this is simple:", "The uncertainty in the posterior should be reduced with a greater number of observations, and indeed, that is what we see both quantitatively and visually. Intuitively, this again makes sense: as we gather more data, we become more sure of the state of the world. In the case of infinite data, our estimate will converge on the true values and the priors will play no role.", "Earlier we discussed how the hyperparameters can be thought of as pseudocounts that represent our prior belief. If we set all the values of alpha equal to 1, we get the results we\u2019ve seen so far. What about if we decrease or increase our confidence in our initial theory that the prevalence is equal? To do so, all we have to do is alter the alpha vector. Then, we sample from the posterior again (using the original observations) and inspect the results.", "The hyperparameters have a large influence on the outcome! A lower value means the data itself has a greater weighting in the posterior, while a higher value results in greater weight placed on the pseudocounts. As the value is increased, the distributions converge on one another. We\u2019d need a lot of data to overcome our strong hyperparameters in the last case.", "We can compare the posterior plots with alpha = 0.1 and alpha = 15:", "Ultimately, our choice of the hyperparameters depends on our confidence in our belief. If we have a good reason to think the prevalence of species is equal, then we should make the hyperparameters have a greater weight. If we want to let the data speak, then we can lower the effect of the hyperparameters.", "Well, what should our final answer be to the question of prevalences? If we are good Bayesians, then we can present a point estimate, but only with attached uncertainty (95% credible intervals):", "And our estimate that the next observation is a bear? Based on the posterior sampling, about 23%. While these results may not be satisfying to people who want a simple answer, they should remember that the real world is uncertain.", "The benefits of Bayesian Inference are we can incorporate our prior beliefs and we get uncertainty estimates with our answers. The world is uncertain, and, as responsible data scientists, Bayesian methods provide us with a framework for dealing with uncertainty.", "Furthermore, as we get more data, our answers become more accurate. As with many aspects of Bayesian Inference, this is in line with our intuitions and how we naturally go about the world, becoming less wrong with additional information. Ultimately, Bayesian statistics is enjoyable and useful because it is statistics that finally makes sense.", "As always, I welcome feedback and constructive criticism. I can be reached on Twitter @koehrsen_will or through my personal website willk.online.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Scientist at Cortex Intel, Data Science Communicator"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F7144be007815&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Festimating-probabilities-with-bayesian-modeling-in-python-7144be007815&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Festimating-probabilities-with-bayesian-modeling-in-python-7144be007815&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Festimating-probabilities-with-bayesian-modeling-in-python-7144be007815&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Festimating-probabilities-with-bayesian-modeling-in-python-7144be007815&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----7144be007815--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----7144be007815--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://williamkoehrsen.medium.com/?source=post_page-----7144be007815--------------------------------", "anchor_text": ""}, {"url": "https://williamkoehrsen.medium.com/?source=post_page-----7144be007815--------------------------------", "anchor_text": "Will Koehrsen"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe2f299e30cb9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Festimating-probabilities-with-bayesian-modeling-in-python-7144be007815&user=Will+Koehrsen&userId=e2f299e30cb9&source=post_page-e2f299e30cb9----7144be007815---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7144be007815&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Festimating-probabilities-with-bayesian-modeling-in-python-7144be007815&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7144be007815&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Festimating-probabilities-with-bayesian-modeling-in-python-7144be007815&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.adventure-journal.com/2016/03/where-bears-have-the-right-of-way/", "anchor_text": "Source"}, {"url": "https://www.becomingadatascientist.com/2015/10/04/how-to-use-twitter-to-learn-data-science-or-anything/", "anchor_text": "great resource for data science"}, {"url": "https://rationalwiki.org/wiki/Bayesian", "anchor_text": "as a Bayesian"}, {"url": "https://en.wikipedia.org/wiki/Bayesian_inference", "anchor_text": "Bayesian Inference"}, {"url": "https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers", "anchor_text": "basic probabilistic programming"}, {"url": "https://docs.pymc.io/api.html", "anchor_text": "PyMC3"}, {"url": "https://github.com/WillKoehrsen/probabilistic-programming/blob/master/Estimating%20Probabilities%20with%20Bayesian%20Inference.ipynb", "anchor_text": "Jupyter Notebook on GitHub"}, {"url": "https://course.fast.ai/about.html", "anchor_text": "top-down approach to learning new topics"}, {"url": "https://www.fast.ai/2016/10/08/teaching-philosophy/", "anchor_text": "exemplified in the excellent fast.ai courses"}, {"url": "https://en.wikipedia.org/wiki/Probability_mass_function", "anchor_text": "probability mass function"}, {"url": "https://frnsys.com/ai_notes/machine_learning/bayesian_learning.html", "anchor_text": "source"}, {"url": "http://users.cecs.anu.edu.au/~ssanner/MLSS2010/Johnson1.pdf", "anchor_text": "Bayesian Inference for Dirichlet-Multinomials"}, {"url": "http://christianherta.de/lehre/dataScience/bayesian/Multinomial-Dirichlet.slides.php", "anchor_text": "Categorical Data / Multinomial Distribution"}, {"url": "https://en.wikipedia.org/wiki/Dirichlet-multinomial_distribution#Dirichlet-multinomial_as_a_compound_distribution", "anchor_text": "Dirichlet-Multinomial Wikipedia Article"}, {"url": "https://en.wikipedia.org/wiki/Multinomial_distribution", "anchor_text": "Multinomial Distribution Wikipedia Article"}, {"url": "https://stats.stackexchange.com/questions/244917/what-exactly-is-the-alpha-in-the-dirichlet-distribution/244946", "anchor_text": "Alpha in the Dirichlet Distribution"}, {"url": "https://en.wikipedia.org/wiki/Dirichlet_distribution", "anchor_text": "Dirichlet Distribution Wikipedia Article"}, {"url": "https://en.wikipedia.org/wiki/Hyperparameter", "anchor_text": "Hyperparameter Wikipedia Article"}, {"url": "https://stats.stackexchange.com/questions/304148/deriving-the-map-estimate-for-multinomial-dirichlet", "anchor_text": "Deriving the MAP estimate for Dirichlet-Multinomials"}, {"url": "http://nbviewer.jupyter.org/github/AllenDowney/ThinkBayes2/blob/master/solutions/dirichlet_soln.ipynb", "anchor_text": "here for Allen Downey\u2019s solution"}, {"url": "https://en.wikipedia.org/wiki/Statistical_inference", "anchor_text": "Inference in statistics"}, {"url": "https://towardsdatascience.com/markov-chain-monte-carlo-in-python-44f7e609be98", "anchor_text": "Markov Chain Monte Carlo"}, {"url": "http://users.cecs.anu.edu.au/~ssanner/MLSS2010/Johnson1.pdf", "anchor_text": "analytically expressed"}, {"url": "https://arxiv.org/abs/1111.4246", "anchor_text": "No-U-Turn Sampler"}, {"url": "https://en.wikipedia.org/wiki/Credible_interval", "anchor_text": "credible interval"}, {"url": "https://andrewgelman.com/2015/05/19/bayesian-inference-the-advantages-and-the-risks/", "anchor_text": "benefits of Bayesian Inference"}, {"url": "http://twitter.com/@koehrsen_will", "anchor_text": "@koehrsen_will"}, {"url": "https://willk.online", "anchor_text": "willk.online"}, {"url": "https://medium.com/tag/data-science?source=post_page-----7144be007815---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/statistics?source=post_page-----7144be007815---------------statistics-----------------", "anchor_text": "Statistics"}, {"url": "https://medium.com/tag/education?source=post_page-----7144be007815---------------education-----------------", "anchor_text": "Education"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----7144be007815---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/tag/programming?source=post_page-----7144be007815---------------programming-----------------", "anchor_text": "Programming"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7144be007815&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Festimating-probabilities-with-bayesian-modeling-in-python-7144be007815&user=Will+Koehrsen&userId=e2f299e30cb9&source=-----7144be007815---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7144be007815&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Festimating-probabilities-with-bayesian-modeling-in-python-7144be007815&user=Will+Koehrsen&userId=e2f299e30cb9&source=-----7144be007815---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7144be007815&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Festimating-probabilities-with-bayesian-modeling-in-python-7144be007815&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----7144be007815--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F7144be007815&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Festimating-probabilities-with-bayesian-modeling-in-python-7144be007815&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----7144be007815---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----7144be007815--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----7144be007815--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----7144be007815--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----7144be007815--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----7144be007815--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----7144be007815--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----7144be007815--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----7144be007815--------------------------------", "anchor_text": ""}, {"url": "https://williamkoehrsen.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://williamkoehrsen.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Will Koehrsen"}, {"url": "https://williamkoehrsen.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "38K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe2f299e30cb9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Festimating-probabilities-with-bayesian-modeling-in-python-7144be007815&user=Will+Koehrsen&userId=e2f299e30cb9&source=post_page-e2f299e30cb9--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fe7d4a87a913e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Festimating-probabilities-with-bayesian-modeling-in-python-7144be007815&newsletterV3=e2f299e30cb9&newsletterV3Id=e7d4a87a913e&user=Will+Koehrsen&userId=e2f299e30cb9&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}