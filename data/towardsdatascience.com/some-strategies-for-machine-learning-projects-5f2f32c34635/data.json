{"url": "https://towardsdatascience.com/some-strategies-for-machine-learning-projects-5f2f32c34635", "time": 1683006131.5897222, "path": "towardsdatascience.com/some-strategies-for-machine-learning-projects-5f2f32c34635/", "webpage": {"metadata": {"title": "Some Strategies for Machine Learning Projects | by Yan Ding | Towards Data Science", "h1": "Some Strategies for Machine Learning Projects", "description": "This article will summarize some strategies for machine learning projects. Firstly, we will illustrate the reasons why we need strategies for machine learning. Secondly, we will give you some\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.coursera.org/learn/machine-learning-projects/home/welcome?", "anchor_text": "\u201cStructuring Machine Learning Projects\u201d", "paragraph_index": 77}], "all_paragraphs": ["This article will summarize some strategies for machine learning projects. Firstly, we will illustrate the reasons why we need strategies for machine learning. Secondly, we will give you some insights into directions to improve system performance according to its variance and bias. Thirdly, we will go through error analysis which needs manually examine errors. Furthermore, we will introduce some other methods often used in machine learning such as mismatched training, transfer learning, and multi-task learning. Finally, we will also give some tips on training neural networks, such as tuning hyperparameters and choosing train, dev/test datasets.", "For example, we train a model of recognizing a cat and get an accuracy of 90%. Compared to human-level performance which may reach near 100% accuracy because it is easy for a human to recognize a cat, this result is not good enough. Then what can we do to improve the performance?", "There are a lot of ideas you can try, for example,", "The problem is which way to go?", "Here we need strategies in order to improve machine learning performance more efficiently. Because if we make efforts in the wrong direction, we cannot get much progress and waste a lot of time. Meanwhile, we need to get some expectations in the direction we chose, for example, how much accuracy can be improved. The first thing we can do is to look into the training and dev error in order to find the direction to improve the model performance.", "Let\u2019s first recap some definitions of variance, bias, and introduce a new termination of \u201cAvoidable Bias\u201d [1]which is defined by Andrew Ng.", "First, variance means the difference between the training error and the dev error. That tells you whether your algorithm is able to generalize from the training set to the dev set.", "Secondly, bias means comparing the training error to 0%. It is the estimate of bias. But in most circumstances, it is not possible to get 0% error even Bayes error is non-zero.", "Thirdly, here comes the \u201cAvoidable Bias\u201d which is first called by Andrew Ng. Because it is natural to compare machine learning systems with human-level performance. Generally, the human-level error can be used as the approximation for Bayes error. \u201cAvoidable bias\u201d is the difference between the training error and Bayes error. These three definitions can be drawn as below,", "Having a good estimate of avoidable bias and variance can help make better decisions on whether to focus on variance reduction tactics or bias reduction tactics.", "As the example of \u201crecognizing a cat\u201d above, supposing human-level error is 1%, the training error is 1% and the dev error is 5%, what is the direction to improve performance?", "In this example, it would be better to focus on variance reduction because bias is 0% and variance is 4%. High variance means the training set performance cannot be generalized well to dev/test sets. There are several methods of reducing high variance as follows,", "Suppose human-level error is 1%, the training error is 5% and the dev error is 6%, then what is the direction to improve the system performance?", "It would be better to do bias reduction first because bias is 4% and variance is only 1%. Big bias means the machine learning system cannot fit the training set pretty well.", "There are several methods to reduce bias such as,", "Having a good idea of bias and variances in machine learning is important because \u201cit can help you much more efficient, systematic and strategic than a lot of machine learning teams in terms of how to improve the performance of your machine learning system\u201d[1].", "More often, if the bias is big which means the model cannot do the task as well as a human and you want to improve it, you need to manually examine mistakes that your algorithm is making. And that can give you insights into what to do next. This process is called error analysis.", "As the example of \u201crecognizing a cat\u201d above, supposing you\u2019ve achieved 90% accuracy, or equivalently 10% error on your dev set, you want to know what is misclassified as a cat on the dev set. Then you can collect 100 (for example) mislabeled examples and examine them manually. Just list the true class which is misclassified and get the following table,", "After counting up misrecognized images, it is clear to see the percentage of each class. Clearly, in this example, a lot of the mistakes were made on blurry images and quite a lot on great cat images. So if you can do better in blurry images or great cat images, your model performance can be improved more. Recall the error of dev set is 10%, so the error can be potentially reduced by 6.1% or 4.3%. While no matter how better you do on dogs or Instagram, the error can only be reduced by 0.8% or 1.2%.", "The conclusion of this process gives you an estimate of how worthwhile it might be to work on each of these different categories of errors. There\u2019s a ceiling in terms of how much you could improve performance. This doesn't give you a rigid mathematical formula that tells you what to do, but it gives you a sense of the best options to pursue.", "As to this example, maybe you can collect more data on great cats or do augmentation to meet the demand of blurry images.", "Moreover, you can also generate new categories of errors in the above table of error analysis. For example, if you\u2019re looking through the examples and find there are a lot of Snapchat filters which also messing up the classifier, you can create new categories during that process. It gives you inspiration for new directions to go in.", "There also happens the data is incorrectly labeled by labelers. Then what can we do with this mistake?", "It turns out that deep learning algorithms are robust to random errors in the training set. If the data is accidentally incorrectly labeled by labelers, it is probably fine, just leaving the errors as they are and not spend much time to fix it so long as the total data set is big enough and the percentage of these incorrectly labeled data is not too high.", "When it comes to evaluating dev/test sets, we can analyze errors as before by adding it into the error analysis table as one category and counting it up.", "After counting up the incorrectly labeled errors, we can get its fraction in the overall dev set error to evaluate whether it is worthwhile to correct it. For example, suppose we have two algorithms A and B with overall dev error 10% and 2% respectively as follows,", "Looking into the percentage of incorrectly labeled error, if it is not too high like in algorithm A, it may be not necessary to fix it. While in algorithm B, the fraction of incorrectly labeled error is 0.6% of 2% of total dev set error which reaches 30%, relatively high, so it may much more worthwhile to fix it.", "There is another situation that maybe need to fix the incorrectly labeled data. For example, if you want to evaluate another two algorithms which errors are respectively 2.1% for algorithm C and 1.9% for algorithm D, although the error of D is lower than C, it is hard to tell that algorithm D is better than C because there are 0.6% incorrect labels in the dev set. In this instance, fixing up incorrect labels becomes necessary.", "Although DL(deep learning) algorithms are robust to random errors, it cannot do well in systematic errors. For training data, if the labeler consistently labeled white dogs as cats, that is a problem because your classifier will learn to classify white dogs as cats.", "Firstly, if you plan to correct the dev data set, it is better to correct the test set as well to ensure that both come from the same distribution.", "Secondly, it also needs to consider examples your algorithm \u201cgot right\u201d. Because with incorrect labels, what you got right may be actually wrong.", "Thirdly, when it is necessary to correct dev and test data which is often much smaller than the training set, you may not want to correct the training set because it is a huge work. So what can we do? It turns out learning algorithms are robust when training data and dev data come from different distributions which we will discuss later.", "To summarize, it may a little boring to manually examine errors but it can actually give a promising direction to improve your system performance. Spending a few hours to do manual error analysis can actually save a few months you may spend on a less worthwhile direction.", "Machine learning algorithms have a huge hunger for training data. But in many circumstances, we have not a lot of training data. Then what can we do to help get good performance in our applications while we have not so much data?", "One of the methods often used is training on data that comes from a different distribution than your dev and test sets. This is called mismatched training. Let\u2019s see an example. As the app of \u201cRecognizing a cat\u201d above, we want to give the right classify of pictures uploaded by our users which may have low pixels, but we do not have so much uploading images while we actually have a lot of images from the website which have high pixels as shown below,", "It is obvious that these two sources of data come from different distributions. But images from webpages can actually help a lot in our training model. It turns out that these images can be used in training. When it comes to testing, we still use data from users\u2019 mobile apps to evaluate the accuracy of our model to ensure that our target is these app users. Suppose there are 200,000 images from webpages and 10,000 images from app users. We can choose the train set and dev set as follows,", "Please aware that we use all images from webpages and half of the images from users\u2019 app as the training data, with another half of the images from users\u2019 app as dev and test sets. Because we want to focus on our target which is app users. It is not suggested to shuffle all data from webpages and the app and then divided them into the corresponding percentages like 98% for training, 1% for dev, and 1% for test data. Because it will change our original target and the dev and test result cannot reflect what we actually care about.", "Although we can use data from a different distribution than the dev/test set to help train the model, this method carries out a problem that is the data mismatch problem. Then how to find out whether our model has this data mismatch problem? We can add a training-dev error by cutting a piece of data from training data as follows,", "The neural network now can only use part of the original training set. For example, 1% of the original training set is now used as the training-dev set. Now we can analyze data mismatch according to training-dev error and dev error. The difference between training-dev error and dev error is data mismatch.", "It is easy to understand this \u201cgap\u201d because even the model can do well in the training set, it may not has good performance in the dev set as these are from different distributions.", "Combined with variance and bias we discussed before, we can arrive more comprehensive ideas including \u201cavoidable bias\u201d, variance and data mismatch as follows,", "Let\u2019s see a \u201cspeech recognition\u201d example. There is a \u201cRearview mirror speech recognition\u201d which is used to collect drivers\u2019 voices and recognize them. But there are not so many voice data from drivers while there are a lot of general speech data. As we all know, voices from drivers in-car may have more traffic noise, and the sound may be not so clear while general speech data always has good quality and sounds clear. When using general speech data to train and drivers\u2019 voice data to validate or test, we can arrive at a more general error analysis as follows,", "It is clear to check where is the problem in your model from the above table for example, whether it has a high bias or high variance or data mismatch problem. It can point you a promising direction by only filling the data in blue cells. Furthermore, you can also get more insights by filling these gray cells. The first number 6% of human-level error in \u201cRearview mirror\u201d can be got by asking some humans to label these speech data. Obviously, it is more difficult for a human to recognize as accurately as they can do in general speech data because there is a lot of traffic noise. And you can also get the training error in \u201cRearview mirror\u201d by training in rearview mirror speech data. These two points are not used frequently but can actually give you some insights.", "Now we have a good idea of the problems in our algorithm including bias, variance, and data mismatch problem. We have addressed high bias and high variance problems before. But we have no idea how to solve the data mismatch problem. Next section we will address it.", "The data mismatch problem is caused by the difference between training data and dev data. So manually comparing training and dev sets can help to understand the difference between these two sets. For example, as we have mentioned of \u201cRearview mirror speech recognition\u201d before, you can manually pick some voices from the training set which comes from general speech data and listen to it, then compared with the dev set which comes from drivers\u2019 voices of \u201cRearview Mirror\u201d. Maybe you will find that drivers\u2019 voices from \u201cRearview mirror\u201d are much noisier and not so clear.", "Then how to solve this data mismatch problem? One way you can try is to make training data more similar to dev/test data which can be achieved by \u201cartificial data synthesis\u201d or collect more data similar to dev/test sets. Like the example above, you can add \u201ccar noise\u201d to \u201cgeneral speech data\u201d and get synthesized in-car audio which is much more similar to dev/test sets.", "Please aware that you do not use a small set of car noise because this may lead to your model overfit to the car noise. For example, if you have 10,000 hours of general speech data and just 1 hour of car noise, then you repeat 10,000 times of the 1-hour car noise and add it to the general speech data. Maybe for a human, it sounds perfectly fine to the human ear but there is a chance that your model may overfit to this 1-hour noise. What it actually does is synthesizing data from a very small subset of this space as follows,", "So if you collect more unique noise data, it is more promising that the performance of the model will become much better.", "Transfer learning is to take the knowledge that the neural network has learned from one task and apply that knowledge to a separate task. For example, if you have a neural network that is used to \u201crecognizing a cat\u201d, you can use this model or part of the model to do \u201crecognizing a tumor\u201d by reading X-rays scans. This process is as follows,", "Let\u2019s say that we have trained the above neural network on image recognition to recognize a cat. The input of this model is object images like a bird, a cat, a dog or something else. And we have the trained weights. Then we want to use this trained model which includes the structure and weights to reading X-rays scans. The method is to delete this last output layer of the original neural network and the weights feeding into the last layer and create a new last layer with a set of randomly initialized weights, and now we have the output of radiology diagnosis. Then we can use our radiology images as the input to train the last layer of the model and get the output of radiology diagnosis. You can also add more layers rather than just one last layer,", "It is free for you to decide whether to retrain other layers of the model. In the above example, we \u201cfreeze\u201d the other layers except for the last layer and you can also just \u201cfreeze\u201d parts of layers as follows,", "Furthermore, if you have a lot of data, then maybe you can retrain all the parameters in the network. And if you retrain all the parameters in the neural network, then this initial phase of training on image recognition is called pre-training, because you\u2019re using image recognitions data to pre-initialize or pre-train the weights of the neural network. And then if you are updating all the weights afterward, then training on the radiology data is called fine-tuning.", "The problem is when we can use transfer learning. There are three points should be considered,", "Firstly, Task A (original neural network: above example of \u201cCat recognition\u201d ) and Task B (your target job: example of X-rays scans) must have the same input X. It needs you to feed in images with the same shape of task A to the model of task B.", "Secondly, there are much more data on Task A than Task B.", "Thirdly, low-level features of Task A could be helpful for learning Task B.", "I think transfer learning is a little bit like mismatched training that we have discussed before. The difference is transfer learning directly uses weights of the model of Task A that have been trained while mismatched training only uses the data of Task A. Both need there exists some similarity of Task A and Task B. There are some transfer learning architectures like \u201cInception v3\u201d which used for image recognition and object detection.", "Multi-task learning refers to having one neural network do simultaneously several tasks. If you are familiar with computer vision, you may know a lot of image recognition algorithms like Mask RCNN, YOLO, U Net, Faster RCNN. These algorithms are all using multi-task learning. The architecture of multi-task learning is drawn as follows,", "Let\u2019s see an example of a simplified autonomous vehicle. The vehicle has to detect simultaneously several things: pedestrians, cars, road signs, traffic lights, cyclists, etc. The input \ud835\udc65 is the image with multiple labels. The output Y has 4 labels according to four tasks:", "The result should look like as,", "The cost function can be written as,", "There is a circumstance that labelers did not label all these four classes, maybe just labeled the subset of the labels. For example, the labeler labeled the car in the above picture but forget to label the stop sign. With the data set like this, you can still train your algorithm to do four tasks at the same time. The data set may be as follows,", "You should not sum up these unlabeled examples j into the cost function and only sum up y_j when y_j is equal to 0 or 1.", "Firstly, training on a set of tasks that could benefit from having shared lower-level features. For example, there are some datasets like ImageNet, COCO dataset which contains common objects in context.", "Secondly, the amount of data you have for each task is usually quite similar. Because if one or some tasks have much more data than others, the algorithm will probably predict the output to be that class with more data.", "Thirdly, it can do better in all the tasks when training a big enough neural network than training a separate neural network on each task.", "The most important thing when tuning hyperparameters is keeping orthogonalization. It is not a good idea if you tune hyperparameters to meet multiple targets at the same time. As an example of old television, there are several knobs that can tune the width, length, trapezoidal, or rotation of the image.", "It should also obey orthogonalization when tuning neural networks. When a supervised learning system is designed, these 4 assumptions need to be true and orthogonal.", "Firstly, you usually have to make sure that you\u2019re at least doing well on the training set which means the avoidable bias should be not so high and we have discussed bias reduction tactics before.", "Secondly, after doing well in the training set, it should fit the development set well which means the variance should be not so high and we also have discussed variance reduction tactics before.", "Thirdly, it should fit the test set well on the cost function.", "Finally, it should perform well in the real world. (If it doesn\u2019t perform well, the development or test set is not set correctly or the cost function is not evaluating the right thing.)", "It is a good idea to look into the performance of the model and then tune these four issues and address them respectively. There is a technique called \u201cearly stopping\u201d which is less orthogonalized because it simultaneously affects both training set and dev set performance.", "If you have enough data to train the model, rather than mismatched training which we discussed before, it is more recommended to make the training set and the dev/test set coming from the same distribution. For example, randomly shuffle your whole data set and then divide them into training, development, and test sets. Because if training and dev/test set come from different distributions, it is just like you set different targets for your algorithm.", "If the size of the whole data is small like one hundred, it usually uses a 70/30 or 60/20/20 split into a train and dev/test set as follows,", "But for now, the big data era, a large amount of data is available. So let\u2019s say you have a million training examples, it might be quite reasonable to set up your data with 98% for the training set, 1% for the dev set, and 1% for the test set.", "I know this is a long article but it can really help you a lot when dealing with projects in machine learning. This article is arranged around the topic of \u201cStrategies for machine learning\u201d and illustrates error analysis to improve the performance of the machine learning model. Meanwhile, we also introduced mismatched learning, transfer learning, multi-task learning, and when to use them. It is convenient for you to check according to titles. Finally, we summarized some tips for machine learning. I hope this article can help you with your job.", "[1] Andrew Ng, \u201cStructuring Machine Learning Projects\u201d, Coursera.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F5f2f32c34635&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsome-strategies-for-machine-learning-projects-5f2f32c34635&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsome-strategies-for-machine-learning-projects-5f2f32c34635&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsome-strategies-for-machine-learning-projects-5f2f32c34635&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsome-strategies-for-machine-learning-projects-5f2f32c34635&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----5f2f32c34635--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5f2f32c34635--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://dingyan89.medium.com/?source=post_page-----5f2f32c34635--------------------------------", "anchor_text": ""}, {"url": "https://dingyan89.medium.com/?source=post_page-----5f2f32c34635--------------------------------", "anchor_text": "Yan Ding"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F764c8a8261fe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsome-strategies-for-machine-learning-projects-5f2f32c34635&user=Yan+Ding&userId=764c8a8261fe&source=post_page-764c8a8261fe----5f2f32c34635---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5f2f32c34635&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsome-strategies-for-machine-learning-projects-5f2f32c34635&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5f2f32c34635&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsome-strategies-for-machine-learning-projects-5f2f32c34635&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "http://www.curiousjourneylist.com/author/lara/", "anchor_text": "Lara"}, {"url": "http://www.curiousjourneylist.com/wp_quiz/what-kind-of-traveler-are-you/which-way-to-go/", "anchor_text": "Curious Journeylist"}, {"url": "https://unsplash.com/@madhatterzone", "anchor_text": "Manja Vitolic"}, {"url": "https://unsplash.com/s/photos/cat", "anchor_text": "Unsplash"}, {"url": "https://cloud.google.com/tpu/docs/inception-v3-advanced", "anchor_text": "Inception v3 architecture"}, {"url": "http://cocodataset.org/#home", "anchor_text": "COCO dataset"}, {"url": "https://www.freevector.com/old-television#", "anchor_text": "FreeVector.com"}, {"url": "https://www.coursera.org/learn/machine-learning-projects/home/welcome?", "anchor_text": "\u201cStructuring Machine Learning Projects\u201d"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----5f2f32c34635---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/transfer-learning?source=post_page-----5f2f32c34635---------------transfer_learning-----------------", "anchor_text": "Transfer Learning"}, {"url": "https://medium.com/tag/multi-task-learning?source=post_page-----5f2f32c34635---------------multi_task_learning-----------------", "anchor_text": "Multi Task Learning"}, {"url": "https://medium.com/tag/error-analysis?source=post_page-----5f2f32c34635---------------error_analysis-----------------", "anchor_text": "Error Analysis"}, {"url": "https://medium.com/tag/hyperparameter-tuning?source=post_page-----5f2f32c34635---------------hyperparameter_tuning-----------------", "anchor_text": "Hyperparameter Tuning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5f2f32c34635&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsome-strategies-for-machine-learning-projects-5f2f32c34635&user=Yan+Ding&userId=764c8a8261fe&source=-----5f2f32c34635---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5f2f32c34635&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsome-strategies-for-machine-learning-projects-5f2f32c34635&user=Yan+Ding&userId=764c8a8261fe&source=-----5f2f32c34635---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5f2f32c34635&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsome-strategies-for-machine-learning-projects-5f2f32c34635&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5f2f32c34635--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F5f2f32c34635&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsome-strategies-for-machine-learning-projects-5f2f32c34635&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----5f2f32c34635---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----5f2f32c34635--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----5f2f32c34635--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----5f2f32c34635--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----5f2f32c34635--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----5f2f32c34635--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----5f2f32c34635--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----5f2f32c34635--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----5f2f32c34635--------------------------------", "anchor_text": ""}, {"url": "https://dingyan89.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://dingyan89.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Yan Ding"}, {"url": "https://dingyan89.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "174 Followers"}, {"url": "https://shuffleai.blog/", "anchor_text": "https://shuffleai.blog/"}, {"url": "https://www.linkedin.com/in/dingyan89/", "anchor_text": "https://www.linkedin.com/in/dingyan89/"}, {"url": "https://www.kaggle.com/dingyan", "anchor_text": "https://www.kaggle.com/dingyan"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F764c8a8261fe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsome-strategies-for-machine-learning-projects-5f2f32c34635&user=Yan+Ding&userId=764c8a8261fe&source=post_page-764c8a8261fe--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F1d9a418e049c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsome-strategies-for-machine-learning-projects-5f2f32c34635&newsletterV3=764c8a8261fe&newsletterV3Id=1d9a418e049c&user=Yan+Ding&userId=764c8a8261fe&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}