{"url": "https://towardsdatascience.com/regularization-what-why-when-and-how-d4a329b6b27f", "time": 1683015633.893291, "path": "towardsdatascience.com/regularization-what-why-when-and-how-d4a329b6b27f/", "webpage": {"metadata": {"title": "Regularization. What, Why, When, and How? | by Akash Shastri | Towards Data Science", "h1": "Regularization. What, Why, When, and How?", "description": "Regularization is a method to constraint the model to fit our data accurately and not overfit. It can also be thought of as penalizing unnecessary complexity in our model. There are mainly 3 types of\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf", "anchor_text": "Dropout", "paragraph_index": 19}, {"url": "https://www.fast.ai/", "anchor_text": "fastai", "paragraph_index": 25}, {"url": "https://docs.fast.ai/vision.augment", "anchor_text": "here", "paragraph_index": 25}, {"url": "https://github.com/akash-shastri", "anchor_text": "https://github.com/akash-shastri", "paragraph_index": 28}, {"url": "https://www.linkedin.com/in", "anchor_text": "https://www.linkedin.com/in", "paragraph_index": 28}], "all_paragraphs": ["Regularization is a method to constraint the model to fit our data accurately and not overfit. It can also be thought of as penalizing unnecessary complexity in our model. There are mainly 3 types of regularization techniques deep learning practitioners use. They are:", "Sidebar: Other techniques can also have a regularizing effect on our model. You can prevent overfitting by also having more data to constraint the search space of our function. This can be done with techniques like data augmentation, that create more data to train, hence reducing overfitting.There are many other solutions to overfitting such as ensembling and stopping early, that can help prevent overfitting but are not considered regularization as they do not constraint the search space or penalize complexity. Although you should keep these in mind as regularization is not the only way to cure overfitting.", "In Fig 1, we see 3 curves. The one on the left is doing a poor job of predicting the points, and the one on the right is doing a \u201cTOO GOOD\u201d job of predicting the points. We can intuitively tell the left graph isn\u2019t right, but why is the right one bad? Isn\u2019t it good that our model predicts points exactly where they are?", "The answer is NO, and here\u2019s why. Our data contains some noise, we do not want our model to predict noise, as noise is random. So the graph on the right is not ideal either, we want something like the graph in the middle.", "Underfitting is caused due to our model being too simple, or not being trained long enough. Overfitting is a harder problem.Overfitting can be caused either due to an overly complex model that\u2019s learning noise, or the search space for our model function is large and we do not have enough data to constraint the search.", "So regularization is a way to stop overfitting.", "We use regularization whenever we suspect our model is overfitting. The biggest signs of overfitting are the poor performance of validation metrics. The validation set is part of our dataset that the model has not yet seen.", "As we want to detect if our model is learning just from the data, or is being heavily influenced by noise, we use the validation set which has different noise than our training set. So if our model were to overfit the training data, it would predict poorly on our validation set.", "During training, we also constantly measure validation metrics. If we see the validation metrics not improving significantly, or worsening, this is a telltale sign that our model is overfitting. We need to then apply regularization techniques.", "Note: Some regularization techniques have no downside, and should be used ALL the time. An example of this is data augmentation. There\u2019s no downside to using data augmentation and should be used regardless of whether model is overfitting.", "L1 regularization works by adding a penalty based on the absolute value of parameters scaled by some value l (typically referred to as lambda).", "Initially our loss function was: Loss = f(preds,y)Where y is the target output, and preds is the predictionpreds = WX + b, where W is parameters, X is input and b is bias.", "With L1 regularization we add an extra term of l*|W|, where W is the weight matrix (parameters). So our loss function after L1 regularization is", "L2 regularization is very similar to L1 regularization, except the penalty term is the square of the parameters scaled by some factor l (lambda)", "Difference between L1 and L2 regularization", "The difference between L1 and L2 regularization is that the gradients of the loss function with respect to parameters for L1 regularization are INDEPENDENT of parameters, so some parameters can be set all the way to zero, hence completely ignored.", "But in L2 regularization, the gradients of the loss function are DEPENDENT linearly on the parameters, so the parameters can never be zero. They only asymptotically approach zero. This means that no parameter is entirely ignored, and every parameter always has at least a very minimal effect on predictions.", "This difference is key to choosing the type of regularization, if you know you have useless features, L1 might be a better choice. If you want to consider all features, the L2 might be a better choice.", "Nuance: There is one nuance in deep learning where you can use a best of both worlds approach, by using both L1 and L2 regularization. This is called Elastic net Regularization.", "Dropout is an amazing regularization technique that works only on neural networks (as far as I know). The amazing idea of dropout is to randomly zero some elements of the input tensor with probability p (p is a hyperparameter).", "The intuition behind why this would work is simple, overfitting occurs when our model is too complex, so how can we simplify the model? Just don\u2019t use some neurons and BAM!! a simpler model achieved.", "Dropout is found to work very well in practice and is simple to implement. I definitely recommend giving it a try.", "Data augmentation is our final \u201chow\u201d for regularization. The idea behind data augmentation is very simple, yet extremely elegant.", "We know overfitting is caused by a lack of constraint in the search space of our optimal function. How do we add more constraint? More data. But collecting more data for our problem can be a time consuming and arduous task. This is where data augmentation comes in.", "The idea of data augmentation is to create more data from the data we already have. The way this is done is by applying a few transformations to our image, that change the image to make multiple versions of the same image that are different. And magically (NOT) we have more data.", "The list of data augmentations my favorite deep learning library (fastai) provides can be seen here.", "Check if your model overfits, use one of the above regularization methods to save it from overfitting. ALWAYS USE DATA AUGMENTATION!! Fin.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "I love anything that makes me think. Check out my github here: https://github.com/akash-shastri. Get in touch with me on LinkedIn at https://www.linkedin.com/in"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fd4a329b6b27f&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-what-why-when-and-how-d4a329b6b27f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-what-why-when-and-how-d4a329b6b27f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-what-why-when-and-how-d4a329b6b27f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-what-why-when-and-how-d4a329b6b27f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----d4a329b6b27f--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d4a329b6b27f--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://akashgshastri.medium.com/?source=post_page-----d4a329b6b27f--------------------------------", "anchor_text": ""}, {"url": "https://akashgshastri.medium.com/?source=post_page-----d4a329b6b27f--------------------------------", "anchor_text": "Akash Shastri"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F567a94bbd3d8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-what-why-when-and-how-d4a329b6b27f&user=Akash+Shastri&userId=567a94bbd3d8&source=post_page-567a94bbd3d8----d4a329b6b27f---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd4a329b6b27f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-what-why-when-and-how-d4a329b6b27f&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd4a329b6b27f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-what-why-when-and-how-d4a329b6b27f&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.youtube.com/channel/UCcIXc5mJsHVYTZR1maL5l9w", "anchor_text": "Andrew Ng"}, {"url": "https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf", "anchor_text": "Dropout"}, {"url": "https://www.fast.ai/", "anchor_text": "fastai"}, {"url": "https://docs.fast.ai/vision.augment", "anchor_text": "here"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----d4a329b6b27f---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/optimization?source=post_page-----d4a329b6b27f---------------optimization-----------------", "anchor_text": "Optimization"}, {"url": "https://medium.com/tag/ai?source=post_page-----d4a329b6b27f---------------ai-----------------", "anchor_text": "AI"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----d4a329b6b27f---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----d4a329b6b27f---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "http://creativecommons.org/licenses/by/4.0/", "anchor_text": "Some rights reserved"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd4a329b6b27f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-what-why-when-and-how-d4a329b6b27f&user=Akash+Shastri&userId=567a94bbd3d8&source=-----d4a329b6b27f---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd4a329b6b27f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-what-why-when-and-how-d4a329b6b27f&user=Akash+Shastri&userId=567a94bbd3d8&source=-----d4a329b6b27f---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd4a329b6b27f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-what-why-when-and-how-d4a329b6b27f&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d4a329b6b27f--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fd4a329b6b27f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-what-why-when-and-how-d4a329b6b27f&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----d4a329b6b27f---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----d4a329b6b27f--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----d4a329b6b27f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----d4a329b6b27f--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----d4a329b6b27f--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----d4a329b6b27f--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----d4a329b6b27f--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----d4a329b6b27f--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----d4a329b6b27f--------------------------------", "anchor_text": ""}, {"url": "https://akashgshastri.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://akashgshastri.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Akash Shastri"}, {"url": "https://akashgshastri.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "63 Followers"}, {"url": "https://github.com/akash-shastri", "anchor_text": "https://github.com/akash-shastri"}, {"url": "https://www.linkedin.com/in", "anchor_text": "https://www.linkedin.com/in"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F567a94bbd3d8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-what-why-when-and-how-d4a329b6b27f&user=Akash+Shastri&userId=567a94bbd3d8&source=post_page-567a94bbd3d8--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fdbc934e1d566&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-what-why-when-and-how-d4a329b6b27f&newsletterV3=567a94bbd3d8&newsletterV3Id=dbc934e1d566&user=Akash+Shastri&userId=567a94bbd3d8&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}