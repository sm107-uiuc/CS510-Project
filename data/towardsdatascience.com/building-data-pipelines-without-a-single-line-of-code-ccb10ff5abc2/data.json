{"url": "https://towardsdatascience.com/building-data-pipelines-without-a-single-line-of-code-ccb10ff5abc2", "time": 1683001148.647451, "path": "towardsdatascience.com/building-data-pipelines-without-a-single-line-of-code-ccb10ff5abc2/", "webpage": {"metadata": {"title": "Building Data Pipelines Without a Single Line of Code | by Harshit Tyagi | Towards Data Science", "h1": "Building Data Pipelines Without a Single Line of Code", "description": "A post about the steps to create ETL data pipeline without writing a line of code using Google Cloud Dataprep and BigQuery."}, "outgoing_paragraph_urls": [{"url": "https://cloud.google.com/dataprep/", "anchor_text": "Cloud Dataprep", "paragraph_index": 1}, {"url": "https://cloud.google.com/bigquery/", "anchor_text": "Bigquery", "paragraph_index": 3}, {"url": "https://cloud.google.com/storage/", "anchor_text": "Cloud Storage", "paragraph_index": 3}, {"url": "https://console.cloud.google.com/", "anchor_text": "cloud console", "paragraph_index": 6}, {"url": "https://cloud.google.com/resource-manager/docs/creating-managing-projects", "anchor_text": "documentation", "paragraph_index": 8}, {"url": "https://console.cloud.google.com/marketplace/details/obfuscated-ga360-data/obfuscated-ga360-data?filter=solution-type:dataset&q=ecommerce&id=45f150ac-81d3-4796-9abf-d7a4f98eb4c6", "anchor_text": "Google Analytics 360 data", "paragraph_index": 13}, {"url": "https://towardsdatascience.com/hitchhikers-guide-to-exploratory-data-analysis-6e8d896d3f7e", "anchor_text": "exploratory data analysis.", "paragraph_index": 27}, {"url": "https://support.google.com/analytics/answer/3437719?hl=en", "anchor_text": "schema description", "paragraph_index": 27}, {"url": "https://support.google.com/analytics/answer/3437719?hl=en", "anchor_text": "here", "paragraph_index": 29}, {"url": "https://support.google.com/analytics/answer/3437719?hl=en", "anchor_text": "schema", "paragraph_index": 58}, {"url": "https://www.youtube.com/channel/UCH-xwLTKQaABNs2QmGxK2bQ", "anchor_text": "channel", "paragraph_index": 72}, {"url": "https://www.linkedin.com/in/tyagiharshit/", "anchor_text": "LinkedIn", "paragraph_index": 73}, {"url": "https://twitter.com/tyagi_harshit24", "anchor_text": "Twitter", "paragraph_index": 73}, {"url": "https://www.instagram.com/upgradewithharshit/?hl=en", "anchor_text": "Instagram", "paragraph_index": 73}, {"url": "https://www.youtube.com/c/DataSciencewithHarshit", "anchor_text": "https://www.youtube.com/c/DataSciencewithHarshit", "paragraph_index": 76}], "all_paragraphs": ["Tired of writing tons of lines of python/java to build ETL pipelines or explore your datasets? Scared of programming to analyze datasets? Looking for a UI-based analytical platform that can help you wrangle those large datasets? Is integrating different tools a big unwieldy task for you?", "Voila! All your prayers/queries have been answered with this amazing tool by Trifacta called Cloud Dataprep. It is an integrated partner service with Google Cloud Platform which enables numerous workflows in the data engineering and analysis domain.In this post, you\u2019ll not only learn how to use dataprep but also become aware of potential integrations with Google Cloud Platform(GCP) services. We\u2019re going to build a data transformation pipeline with the help of Cloud Dataprep and Bigquery.", "Dataprep is a web-based, serverless, no-ops, intelligent data service to visually perform exploratory analysis on your dataset. It helps you wrangle and explore structured and unstructured data to be processed further for machine learning, reporting dashboards, analytical frameworks, etcetera.The amazing ability to suggest and predict the next data transformation to the loaded data makes the analysis a cakewalk for all those who hesitate to write code.But where does dataprep position itself in the field of data engineering especially in the GCP infrastructure:", "A GCP based data engineering pipeline typically consists of some or all of the cloud services in the image above. The ingestion of data can take place at any component like Bigquery, Cloud Storage or the usual file upload. Then comes the major section where you engineer your data to prepare it for deeper model building and analysis purposes. This includes raw data to flow into one or more(as per your use case) of data processing engines like Cloud Dataprep, Cloud dataflow, and the transformed data is outputted and stored in either Bigquery or Cloud Storage.", "Let\u2019s dive into the tasks to build get pipeline running:", "Shedding more light on each of the above:", "Once you\u2019re logged into your google cloud console, create a new project which is going to be used for this tutorial. Click the dropdown on the navbar at the top and click +NEW PROJECT.", "After the project is created successfully, make sure that you have the newly created project selected in your environment reflected in the dropdown:", "For more details on project creation and management, here is the link to the documentation.", "After setting up the development environment, we need to set up BigQuery which plays the role of both input and output storage in the pipeline. Here is how the flow would look like:", "The main focus of this post is Dataprep but we\u2019ll need BigQuery for importing the data(ingestion) and exporting the output after the transformation cycle completes in the Dataprep.", "Instructions to importing data into Bigquery:", "2. Click CREATE DATASET and enter the Dataset ID as ecommerce_analytics which is going to be the name of our dataset under our project. With all other fields set as default, click theCreate dataset button at the bottom.", "3. We have our dataset ready to store data in the form of tables. We are going to use the obfuscated Google Analytics 360 data from the Google Merchandise Store by querying a subset of it from the bigquery-public-data project. It comes pinned with every new project. The query is using standard SQL. Type in or paste the SQL query in the query editor:", "4. Click Run. Here is how the left pane should look now:", "The query has created a new table named subset_user_sessions which contains 33,664 user sessions that occurred on 1st January 2017. You can preview the data by selecting the table in the left pane and then clicking thePreview option on the right.", "Now that we have the data imported in BigQuery, let\u2019s move on to Dataprep to link it to dataprep to explore this data.", "Here, we will get started with Cloud Dataprep and connect our bigquery table with it. Following are the steps for the task:", "Note: You need a Chrome browser as Cloud Dataprep works only with Google chrome.", "4. Allow Trifacta to access project data by clicking theAllow button, it\u2019s going to take a few minutes to sync up the account info:", "5. Choose the Google account you want to sign in with and allow Trifacta to access that account information:", "6. Once the account setup is done, you\u2019ll be prompted for the first time set up with a dialog box to provide a location to save files that we create within Dataprep in Google Cloud Storage. Make sure that the right project is selected and then, click Continue.", "7. Welcome to the main playground, Cloud Dataprep dashboard. Once we\u2019re here, we start the connection procedure with BigQuery. Click the Create Flow button in the top-right.", "8. Enter User Analytics Pipeline as the Flow Name and add a flow description.", "9. It\u2019s time to import the dataset from BigQuery, click Import & Add Datasets, select BigQuery from the left pane.", "10. Click on your dataset and as your dataset tables get loaded click the Create Dataset icon as shown below:", "On the right, a pane previews the table data, click the Import & Add to Flow button at the bottom. The connection is successfully built and you\u2019re now off to adding new recipes to the flow as the Add new Recipe option becomes available on the right.", "With our BigQuery project data now imported into dataprep, we can start off with the exploratory data analysis.The first and foremost step in the process of EDA is to make a connection with the data at hand which helps you assess the potential insights that can be drawn from the data. There are different ways to go about the initial data comprehension exercise, the way I do it is I read about the schema description if available and then I try to go through each column and prepare a list of good questions to answer from those columns or the combination of columns.For more on EDA:", "Taking it from where we left in the previous section, click on Add new Recipe button in the right pane and then click Edit Recipe to look at the data in the Transformer view.", "You can read about the dataset here to understand the meaning of each column. So, let\u2019s define a few questions and extract their answers using Dataprep\u2019s tools and features. This will help us get our head around the data and the platform as well:", "At the bottom of the transformer view, you\u2019ll find that our sample contains 32 columns, 12.5K rows, and 4 different data types.", "2. Looking for column\u2019s min, max, lower quartile, higher quartile functions.", "To find out the statistical and pattern details of each column, click the dropdown in front of the name of the column and then select Column Detail. Let\u2019s try to find out the min/max values for the following columns:", "Clicking Column Details will take you to the summary and stats page:", "The minimum quantity bought was 1 while the max was 50.", "Similarly,Max/Min Pageviews: 1/115We\u2019ll find out the min/max time spent on the site after correcting it in a few steps.", "Note: The answers may differ for your subset.", "3. What does the histogram represent under the column header?", "The histogram depicts the categories of responses in the column. For example, version 2 Product Names can be seen in the above screenshot. If you hover over the bar, it tells you the frequency of that category in the column like this:", "Now, using this let\u2019s try to answer the next question.", "4. Top 5 Countries w.r.t the number of sessions origination:", "They are in the order: United States > United Kingdom > India > Canada > Japan", "5. What does the color of the column bar signify?", "You must have noticed that there is a bar under the column heading and it is of different colors signifying missing and mismatched data type values.", "Grey color represents missing values while red denotes mismatched values. We\u2019ll be correcting these in the next section.", "6. What are the common sources of traffic?To answer this, we\u2019d need to look at the channelGrouping column from the schema description. There are 7 sources and the most common ones are:", "Organic search > referral > direct", "7. What are the common product categories?To answer this, we\u2019d need to look at the v2ProductCategorycolumn from the schema description. There are 31 categories and the most common ones are:", "Let\u2019s build the transformational recipe now that we have got to know the problems associated with the dataset we have. We need to define the data cleaning steps for columns we find problems with, just like the mismatched time column in the dataset above.", "An important step here is to keep an eye on the recipe icon which if you click on will show you a step added to it like this:", "This recipe will keep updating itself as we work through our data wrangling process.", "2. Dropping the unused columns: If you observe carefully, there are a few columns that have whole grey bars which indicate that these have all null values(or no valid values). These columns have no use and are redundant.", "Note: We can not delete all the columns with all null values as we are working on a subset of data. Read the description of the column if you are not sure about a column before deleting.", "How to delete the column?Click the grey bar under the column header, dataprep will highlight all the missing value rows. Now, click the menu dropdown > Click Delete.", "Repeat the process for itemQuantity and itemRevenue columns as we\u2019ll be using productQuantity and productRevenue columns as they have values in it which will come in handy for revenue reporting purposes.", "Now, the updated recipe looks like:", "3. Deleting duplicate rows: Another very common and useful step in EDA is dropping duplicate rows from the data for which in pandas we use drop_duplicate function, here we\u2019re going to use the toolbar on the top.", "Click the Filter rows icon and then select Remove duplicate rows.You\u2019ll be asked if you want to add this step in the recipe as a step to be run for all the incoming dataset in the future.Click Add.", "4. Adding a custom formula to the recipe: The totalTransactionRevenue column contains the total revenue generated from the transaction. From the schema documentation, you can see that it has been multiplied by 10\u2076 which is not clear from the values.", "From the dropdown menu > Calculate > Custom formula:", "In the right pane that opens up, add the formula:", "We have added a custom DIVIDE formula to the totalTransactionRevenue column and the preview can be shown in the transformer view. Click Add and it gets added to the recipe.", "5. Merging columns: Merging/Joins are another very important and common task in EDA and Dataprep can do that in the most effortless manner as well. Let\u2019s create a primary key column for each of these user sessions.", "We are using fullVisitorId, time and v2ProductName for this task as it gives unique value for each session separated by a | and name the new column as main_id.", "These are a few steps that I\u2019ve walked you through, there is a bunch of operations and manipulations you can pull of using this intelligent tool.", "Here is how the recipe looks like now:", "With the pipeline now ready to run, click Run Job on the top-right corner.Since we need to add the transformed data back to BigQuery in a new table for analytical or modeling purposes.Click Add Publishing Action on the right and then select BigQuery as the output destination, the synced dataset will become available, here you need to create a new table using Create table option in the right pane.", "Click Add at the bottom and here you go:", "Click Run Job in the bottom right corner of the page after setting your region and machine type that you need. The job will take a few minutes and once it is done you can check your results in BigQuery and the table with transformed data awaits your eyes!!", "Another important feature is the ability to schedule the job runs on dataprep. On the Flows page, click the top 3 dots icon and then select Schedule Flow.", "You can choose the timezone, frequency, days and time as shown below:", "Who would have thought of simplifying a laborious task of pipeline creation to this extent? The Google Cloud team is adding some unmatched tools and features to redefine the meaning of cloud computing as we speak. Fascinated as I was after estimating the number of man-hours dataprep has saved us, there isn\u2019t a more useful, well-connected and UI-friendly tool. Many more interesting workflows are to be talked about and I\u2019m elated to unravel the stories I\u2019ve to tell about data engineering on GCP. Stay tuned!!", "With this channel, I am planning to roll out a couple of series covering the entire data science space. Here is why you should be subscribing to the channel:", "You can connect with me on LinkedIn, Twitter or Instagram(where I talk about health and wellness.)", "Note: During these dark times, self-isolation has made some room for self-improvement which we can utilize to develop new skills, hobbies, and help our future-selves.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Web & Data Science Instructional Designer | YouTuber | Writer https://www.youtube.com/c/DataSciencewithHarshit"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fccb10ff5abc2&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-data-pipelines-without-a-single-line-of-code-ccb10ff5abc2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-data-pipelines-without-a-single-line-of-code-ccb10ff5abc2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-data-pipelines-without-a-single-line-of-code-ccb10ff5abc2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-data-pipelines-without-a-single-line-of-code-ccb10ff5abc2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----ccb10ff5abc2--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ccb10ff5abc2--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://dswharshit.medium.com/?source=post_page-----ccb10ff5abc2--------------------------------", "anchor_text": ""}, {"url": "https://dswharshit.medium.com/?source=post_page-----ccb10ff5abc2--------------------------------", "anchor_text": "Harshit Tyagi"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4530efc184cb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-data-pipelines-without-a-single-line-of-code-ccb10ff5abc2&user=Harshit+Tyagi&userId=4530efc184cb&source=post_page-4530efc184cb----ccb10ff5abc2---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fccb10ff5abc2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-data-pipelines-without-a-single-line-of-code-ccb10ff5abc2&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fccb10ff5abc2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-data-pipelines-without-a-single-line-of-code-ccb10ff5abc2&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://cloud.google.com/dataprep", "anchor_text": "https://cloud.google.com/dataprep"}, {"url": "https://cloud.google.com/dataprep/", "anchor_text": "Cloud Dataprep"}, {"url": "https://cloud.google.com/dataprep/", "anchor_text": "https://cloud.google.com/dataprep/"}, {"url": "https://cloud.google.com/bigquery/", "anchor_text": "Bigquery"}, {"url": "https://cloud.google.com/storage/", "anchor_text": "Cloud Storage"}, {"url": "https://console.cloud.google.com/", "anchor_text": "cloud console"}, {"url": "https://cloud.google.com/resource-manager/docs/creating-managing-projects", "anchor_text": "documentation"}, {"url": "https://console.cloud.google.com/marketplace/details/obfuscated-ga360-data/obfuscated-ga360-data?filter=solution-type:dataset&q=ecommerce&id=45f150ac-81d3-4796-9abf-d7a4f98eb4c6", "anchor_text": "Google Analytics 360 data"}, {"url": "https://towardsdatascience.com/hitchhikers-guide-to-exploratory-data-analysis-6e8d896d3f7e", "anchor_text": "exploratory data analysis."}, {"url": "https://support.google.com/analytics/answer/3437719?hl=en", "anchor_text": "schema description"}, {"url": "https://towardsdatascience.com/hitchhikers-guide-to-exploratory-data-analysis-6e8d896d3f7e", "anchor_text": "Hitchhiker's guide to Exploratory Data AnalysisHow to investigate a dataset with python?towardsdatascience.com"}, {"url": "https://support.google.com/analytics/answer/3437719?hl=en", "anchor_text": "here"}, {"url": "https://support.google.com/analytics/answer/3437719?hl=en", "anchor_text": "schema"}, {"url": "https://www.youtube.com/channel/UCH-xwLTKQaABNs2QmGxK2bQ", "anchor_text": "channel"}, {"url": "https://www.linkedin.com/in/tyagiharshit/", "anchor_text": "LinkedIn"}, {"url": "https://twitter.com/tyagi_harshit24", "anchor_text": "Twitter"}, {"url": "https://www.instagram.com/upgradewithharshit/?hl=en", "anchor_text": "Instagram"}, {"url": "https://medium.com/tag/data-engineering?source=post_page-----ccb10ff5abc2---------------data_engineering-----------------", "anchor_text": "Data Engineering"}, {"url": "https://medium.com/tag/data-science?source=post_page-----ccb10ff5abc2---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/technology?source=post_page-----ccb10ff5abc2---------------technology-----------------", "anchor_text": "Technology"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----ccb10ff5abc2---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/programming?source=post_page-----ccb10ff5abc2---------------programming-----------------", "anchor_text": "Programming"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fccb10ff5abc2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-data-pipelines-without-a-single-line-of-code-ccb10ff5abc2&user=Harshit+Tyagi&userId=4530efc184cb&source=-----ccb10ff5abc2---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fccb10ff5abc2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-data-pipelines-without-a-single-line-of-code-ccb10ff5abc2&user=Harshit+Tyagi&userId=4530efc184cb&source=-----ccb10ff5abc2---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fccb10ff5abc2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-data-pipelines-without-a-single-line-of-code-ccb10ff5abc2&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ccb10ff5abc2--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fccb10ff5abc2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-data-pipelines-without-a-single-line-of-code-ccb10ff5abc2&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----ccb10ff5abc2---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----ccb10ff5abc2--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----ccb10ff5abc2--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----ccb10ff5abc2--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----ccb10ff5abc2--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----ccb10ff5abc2--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----ccb10ff5abc2--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----ccb10ff5abc2--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----ccb10ff5abc2--------------------------------", "anchor_text": ""}, {"url": "https://dswharshit.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://dswharshit.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Harshit Tyagi"}, {"url": "https://dswharshit.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "3.2K Followers"}, {"url": "https://www.youtube.com/c/DataSciencewithHarshit", "anchor_text": "https://www.youtube.com/c/DataSciencewithHarshit"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4530efc184cb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-data-pipelines-without-a-single-line-of-code-ccb10ff5abc2&user=Harshit+Tyagi&userId=4530efc184cb&source=post_page-4530efc184cb--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F7293934ec1a7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-data-pipelines-without-a-single-line-of-code-ccb10ff5abc2&newsletterV3=4530efc184cb&newsletterV3Id=7293934ec1a7&user=Harshit+Tyagi&userId=4530efc184cb&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}