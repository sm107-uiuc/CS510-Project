{"url": "https://towardsdatascience.com/understanding-generative-adversarial-networks-gans-cd6e4651a29", "time": 1682994521.286874, "path": "towardsdatascience.com/understanding-generative-adversarial-networks-gans-cd6e4651a29/", "webpage": {"metadata": {"title": "Understanding Generative Adversarial Networks (GANs) | by Joseph Rocca | Towards Data Science", "h1": "Understanding Generative Adversarial Networks (GANs)", "description": "Generative Adversarial Networks (GANs) are deep generative models composed of two networks, a generator and a discriminator, opposed to each other."}, "outgoing_paragraph_urls": [{"url": "https://medium.com/u/20ad1309823a?source=post_page-----cd6e4651a29--------------------------------", "anchor_text": "Baptiste Rocca", "paragraph_index": 0}, {"url": "https://arxiv.org/abs/1406.2661", "anchor_text": "Generative Adversarial Nets", "paragraph_index": 1}, {"url": "https://en.wikipedia.org/wiki/Rejection_sampling", "anchor_text": "Rejection sampling", "paragraph_index": 11}, {"url": "https://en.wikipedia.org/wiki/Metropolis\u2013Hastings_algorithm", "anchor_text": "Metropolis-Hasting algorithm", "paragraph_index": 12}, {"url": "https://en.wikipedia.org/wiki/Cumulative_distribution_function", "anchor_text": "Cumulative Distribution Function", "paragraph_index": 15}, {"url": "http://www.gatsby.ucl.ac.uk/~gretton/papers/testing_workshop.pdf", "anchor_text": "these slides", "paragraph_index": 31}, {"url": "http://www.gatsby.ucl.ac.uk/~gretton/papers/GreBorRasSchSmo07.pdf", "anchor_text": "this article", "paragraph_index": 31}, {"url": "http://www.jmlr.org/papers/volume13/gretton12a/gretton12a.pdf", "anchor_text": "this article", "paragraph_index": 31}, {"url": "https://medium.com/u/20ad1309823a?source=post_page-----cd6e4651a29--------------------------------", "anchor_text": "Baptiste Rocca", "paragraph_index": 60}, {"url": "http://www.linkedin.com/in/joseph-rocca-b01365158", "anchor_text": "www.linkedin.com/in/joseph-rocca-b01365158", "paragraph_index": 62}], "all_paragraphs": ["This post was co-written with Baptiste Rocca.", "Yann LeCun described it as \u201cthe most interesting idea in the last 10 years in Machine Learning\u201d. Of course, such a compliment coming from such a prominent researcher in the deep learning area is always a great advertisement for the subject we are talking about! And, indeed, Generative Adversarial Networks (GANs for short) have had a huge success since they were introduced in 2014 by Ian J. Goodfellow and co-authors in the article Generative Adversarial Nets.", "So what are Generative Adversarial Networks ? What makes them so \u201cinteresting\u201d ? In this post, we will see that adversarial training is an enlightening idea, beautiful by its simplicity, that represents a real conceptual progress for Machine Learning and more especially for generative models (in the same way as backpropagation is a simple but really smart trick that made the ground idea of neural networks became so popular and efficient).", "Before going into the details, let\u2019s give a quick overview of what GANs are made for. Generative Adversarial Networks belong to the set of generative models. It means that they are able to produce / to generate (we\u2019ll see how) new content. To illustrate this notion of \u201cgenerative models\u201d, we can take a look at some well known examples of results obtained with GANs.", "Naturally, this ability to generate new content makes GANs look a little bit \u201cmagic\u201d, at least at first sight. In the following parts, we will overcome the apparent magic of GANs in order to dive into ideas, maths and modelling behind these models. Not only we will discuss the fundamental notions Generative Adversarial Networks rely on but, more, we will build step by step and starting from the very beginning the reasoning that leads to these ideas.", "Without further ado, let\u2019s re-discover GANs together!", "Note: Although we tried to make this article as self-contained as possible, a basic prior knowledge in Machine Learning is still required. Nevertheless, most of the notions will be remained when needed and some references will be given otherwise. We really tried to make this article as smooth to read as possible. Do not hesitate to mention in the comment section what you would have liked to read more about (for possible further articles on the subject).", "In the first following section we will discuss the process of generating random variables from a given distribution. Then, in section 2 we will show, through an example, that the problems GANs try to tackle can be expressed as random variable generation problems. In section 3 we will discuss matching based generative networks and show how they answer problems described in section 2. Finally in section 4 we will introduce GANs. More especially, we will present the general architecture with its loss function and we will make the link with all the previous parts.", "In this section, we discuss the process of generating random variables: we remind some existing methods and more especially the inverse transform method that allows to generate complex random variables from simple uniform random variables. Although all this could seems a little bit far from our subject of matter, GANs, we will see in the next section the deep link that exists with generative models.", "Computers are fundamentally deterministic. So, it is, in theory, impossible to generate numbers that are really random (even if we could say that the question \u201cwhat really is randomness ?\u201d is a difficult one). However, it is possible to define algorithms that generate sequences of numbers whose properties are very close to the properties of theoretical random numbers sequences. In particular, a computer is able, using a pseudorandom number generator, to generate a sequence of numbers that approximatively follows a uniform random distribution between 0 and 1. The uniform case is a very simple one upon which more complex random variables can be built in different ways.", "There exist different techniques that are aimed at generating more complex random variables. Among them we can find, for example, inverse transform method, rejection sampling, Metropolis-Hasting algorithm and others. All these methods rely on different mathematical tricks that mainly consist in representing the random variable we want to generate as the result of an operation (over simpler random variables) or a process.", "Rejection sampling expresses the random variable as the result of a process that consist in sampling not from the complex distribution but from a well known simple distribution and to accept or reject the sampled value depending on some condition. Repeating this process until the sampled value is accepted, we can show that with the right condition of acceptance the value that will be effectively sampled will follow the right distribution.", "In the Metropolis-Hasting algorithm, the idea is to find a Markov Chain (MC) such that the stationary distribution of this MC corresponds to the distribution from which we would like to sample our random variable. Once this MC found, we can simulate a long enough trajectory over this MC to consider that we have reach a steady state and then the last value we obtain this way can be considered as having been drawn from the distribution of interest.", "We won\u2019t go any further into the details of rejection sampling and Metropolis-Hasting because these methods are not the ones that will lead us to the notion behind GANs (nevertheless, the interested reader can refer to the pointed Wikipedia articles and links therein). However, let\u2019s focus a little bit more on the inverse transform method.", "The idea of the inverse transform method is simply to represent our complex \u2014 in this article \u201ccomplex\u201d should always be understood in the sense of \u201cnot simple\u201d and not in the mathematical sense \u2014 random variable as the result of a function applied to a uniform random variable we know how to generate.", "We consider in what follows a one dimensional example. Let X be a complex random variable we want to sample from and U be a uniform random variable over [0,1] we know how to sample from. We remind that a random variable is fully defined by its Cumulative Distribution Function (CDF). The CDF of a random variable is a function from the domain of definition of the random variable to the interval [0,1] and defined, in one dimension, such that", "In the particular case of our uniform random variable U, we have", "For simplicity, we will suppose here that the function CDF_X is invertible and its inverse is denoted", "(the method can easily be extended to the non-invertible case by using the generalised inverse of the function but it is really not the main point we want to focus on here). Then if we define", "As we can see, Y and X have the same CDF and then define the same random variable. So, by defining Y as above (as a function of a uniform random variable) we have managed to define a random variable with the targeted distribution.", "To summarise, inverse transform method is a way to generate a random variable that follows a given distribution by making a uniform random variable goes through a well designed \u201ctransform function\u201d (inverse CDF). This notion of \u201cinverse transform method\u201d can, in fact, be extended to the notion of \u201ctransform method\u201d that consists, more generally, in generating random variables as function of some simpler random variables (not necessarily uniform and then the transform function is no longer the inverse CDF). Conceptually, the purpose of the \u201ctransform function\u201d is to deform/reshape the initial probability distribution: the transform function takes from where the initial distribution is too high compared to the targeted distribution and puts it where it is too low.", "Suppose that we are interested in generating black and white square images of dogs with a size of n by n pixels. We can reshape each data as a N=nxn dimensional vector (by stacking columns on top of each others) such that an image of dog can then be represented by a vector. However, it doesn\u2019t mean that all vectors represent a dog once shaped back to a square! So, we can say that the N dimensional vectors that effectively give something that look like a dog are distributed according to a very specific probability distribution over the entire N dimensional vector space (some points of that space are very likely to represent dogs whereas it is highly unlikely for some others). In the same spirit, there exists, over this N dimensional vector space, probability distributions for images of cats, birds and so on.", "Then, the problem of generating a new image of dog is equivalent to the problem of generating a new vector following the \u201cdog probability distribution\u201d over the N dimensional vector space. So we are, in fact, facing a problem of generating a random variable with respect to a specific probability distribution.", "At this point, we can mention two important things. First the \u201cdog probability distribution\u201d we mentioned is a very complex distribution over a very large space. Second, even if we can assume the existence of such underlying distribution (there actually exists images that looks like dog and others that doesn\u2019t) we obviously don\u2019t know how to express explicitly this distribution. Both previous points make the process of generating random variables from this distribution pretty difficult. Let\u2019s then try to tackle these two problems in the following.", "Our first problem when trying to generate our new image of dog is that the \u201cdog probability distribution\u201d over the N dimensional vector space is a very complex one and we don\u2019t know how to directly generate complex random variables. However, as we know pretty well how to generate N uncorrelated uniform random variables, we could make use of the transform method. To do so, we need to express our N dimensional random variable as the result of a very complex function applied to a simple N dimensional random variable!", "Here, we can emphasise the fact that finding the transform function is not as straightforward as just taking the closed-form inverse of the Cumulative Distribution Function (that we obviously don\u2019t know) as we have done when describing the inverse transform method. The transform function can\u2019t be explicitly expressed and, then, we have to learn it from data.", "As most of the time in these cases, very complex function naturally implies neural network modelling. Then, the idea is to model the transform function by a neural network that takes as input a simple N dimensional uniform random variable and that returns as output another N dimensional random variable that should follow, after training, the the right \u201cdog probability distribution\u201d. Once the architecture of the network has been designed, we still need to train it. In the next two sections, we will discuss two ways to train these generative networks, including the idea of adversarial training behind GANs!", "Disclaimer: The denomination of \u201cGenerative Matching Networks\u201d is not a standard one. However, we can find in the literature, for example, \u201cGenerative Moments Matching Networks\u201d or also \u201cGenerative Features Matching Networks\u201d. We just want here to use a slightly more general denomination for what we describe bellow.", "So far, we have shown that our problem of generating a new image of dog can be rephrased into a problem of generating a random vector in the N dimensional vector space that follows the \u201cdog probability distribution\u201d and we have suggested to use a transform method, with a neural network to model the transform function.", "Now, we still need to train (optimise) the network to express the right transform function. To this purpose, we can suggest two different training methods: a direct one and an indirect one. The direct training method consists in comparing the true and the generated probability distributions and backpropagating the difference (the error) through the network. This is the idea that rules Generative Matching Networks (GMNs). For the indirect training method, we do not directly compare the true and generated distributions. Instead, we train the generative network by making these two distributions go through a downstream task chosen such that the optimisation process of the generative network with respect to the downstream task will enforce the generated distribution to be close to the true distribution. This last idea is the one behind Generative Adversarial Networks (GANs) that we will present in the next section. But for now, let\u2019s start with the direct method and GMNs.", "As mentioned, the idea of GMNs is to train the generative network by directly comparing the generated distribution to the true one. However, we do not know how to express explicitly the true \u201cdog probability distribution\u201d and we can also say that the generated distribution is far too complex to be expressed explicitly. So, comparisons based on explicit expressions are not possible. However, if we have a way to compare probability distributions based on samples, we can use it to train the network. Indeed, we have a sample of true data and we can, at each iteration of the training process, produce a sample of generated data.", "Although, in theory, any distance (or similarity measure) able to compare effectively two distributions based on samples can be used, we can mention in particular the Maximum Mean Discrepancy (MMD) approach. The MMD defines a distance between two probability distributions that can be computed (estimated) based on samples of these distributions. Although it is not fully out of the scope of this article, we have decided not to spend much more time describing the MMD. However, we have the project to publish soon an article that will contains more details about it. The reader that would like to know more about MMD right now can refer to these slides, this article or this article.", "So, once we have defined a way to compare two distributions based on samples, we can define the training process of the generative network in GMNs. Given a random variable with uniform probability distribution as input, we want the probability distribution of the generated output to be the \u201cdog probability distribution\u201d. The idea of GMNs is then to optimise the network by repeating the following steps:", "As written above, when following these steps we are applying a gradient descent over the network with a loss function that is the distance between the true and the generated distributions at the current iteration.", "The \u201cdirect\u201d approach presented above compare directly the generated distribution to the true one when training the generative network. The brilliant idea that rules GANs consists in replacing this direct comparison by an indirect one that takes the form of a downstream task over these two distributions. The training of the generative network is then done with respect to this task such that it forces the generated distribution to get closer and closer to the true distribution.", "The downstream task of GANs is a discrimination task between true and generated samples. Or we could say a \u201cnon-discrimination\u201d task as we want the discrimination to fail as much as possible. So, in a GAN architecture, we have a discriminator, that takes samples of true and generated data and that try to classify them as well as possible, and a generator that is trained to fool the discriminator as much as possible. Let\u2019s see on a simple example why the direct and indirect approaches we mentioned should, in theory, lead to the same optimal generator.", "In order to better understand why training a generator to fool a discriminator will lead to the same result as training directly the generator to match the target distribution, let\u2019s take a simple one dimensional example. We forget, for the moment, how both generator and discriminator are represented and consider them as abstract notions (that will be specified in the next subsection). Moreover, both are supposed \u201cperfect\u201d (with infinite capacities) in the sense that they are not constrained by any kind of (parametrised) model.", "Suppose that we have a true distribution, for example a one dimensional gaussian, and that we want a generator that samples from this probability distribution. What we called \u201cdirect\u201d training method would then consist in adjusting iteratively the generator (gradient descent iterations) to correct the measured difference/error between true and generated distributions. Finally, assuming the optimisation process perfect, we should end up with the generated distribution that matches exactly the true distribution.", "For the \u201cindirect\u201d approach, we have to consider also a discriminator. We assume for now that this discriminator is a kind of oracle that knows exactly what are the true and generated distribution and that is able, based on this information, to predict a class (\u201ctrue\u201d or \u201cgenerated\u201d) for any given point. If the two distributions are far appart, the discriminator will be able to classify easily and with a high level of confidence most of the points we present to it. If we want to fool the discriminator, we have to bring the generated distribution close to the true one. The discriminator will have the most difficulty to predict the class when the two distributions will be equal in all points: in this case, for each point there are equal chances for it to be \u201ctrue\u201d or \u201cgenerated\u201d and then the discriminator can\u2019t do better than being true in one case out of two in average.", "At this point, it seems legit to wonder whether this indirect method is really a good idea. Indeed, it seems to be more complicated (we have to optimise the generator based on a downstream task instead of directly based on the distributions) and it requires a discriminator that we consider here as a given oracle but that is, in reality, neither known nor perfect. For the first point, the difficulty of directly comparing two probability distributions based on samples counterbalances the apparent higher complexity of indirect method. For the second point, it is obvious that the discriminator is not known. However, it can be learned!", "Let\u2019s now describe the specific form that take the generator and the discriminator in the GANs architecture. The generator is a neural network that models a transform function. It takes as input a simple random variable and must return, once trained, a random variable that follows the targeted distribution. As it is very complicated and unknown, we decide to model the discriminator with another neural network. This neural network models a discriminative function. It takes as input a point (in our dog example a N dimensional vector) and returns as output the probability of this point to be a \u201ctrue\u201d one.", "Notice that the fact that we impose now a parametrised model to express both the generator and the discriminator (instead of the idealised versions in the previous subsection) has, in practice, not a huge impact on the theoretical argument/intuition given above: we just then work in some parametrised spaces instead of ideal full spaces and, so, the optimal points that we should reach in the ideal case can then be seen as \u201crounded\u201d by the precision capacity of the parametrised models.", "Once defined, the two networks can then be trained jointly (at the same time) with opposite goals :", "So, at each iteration of the training process, the weights of the generative network are updated in order to increase the classification error (error gradient ascent over the generator\u2019s parameters) whereas the weights of the discriminative network are updated so that to decrease this error (error gradient descent over the discriminator\u2019s parameters).", "These opposite goals and the implied notion of adversarial training of the two networks explains the name of \u201cadversarial networks\u201d: both networks try to beat each other and, doing so, they are both becoming better and better. The competition between them makes these two networks \u201cprogress\u201d with respect to their respective goals. From a game theory point of view, we can think of this setting as a minimax two-players game where the equilibrium state corresponds to the situation where the generator produces data from the exact targeted distribution and where the discriminator predicts \u201ctrue\u201d or \u201cgenerated\u201d with probability 1/2 for any point it receives.", "Note: This section is a little bit more technical and not absolutely necessary for the overall understanding of GANs. So, the readers that don\u2019t want to read some mathematics right now can skip this section for the moment. For the others, let\u2019s see how the intuitions given above are mathematically formalised.", "Disclaimer: The equations in the following are not the ones of the article of Ian Goodfellow. We propose here an other mathematical formalisation for two reasons: first, to stay a little bit closer to the intuitions given above and, second, because the equations of the original paper are already so clear that it would not have been useful to just rewrite them. Notice also that we absolutely do not enter into the practical considerations (vanishing gradient or other) related to the different possible loss functions. We highly encourage the reader to also take a look at the equations of the original paper: the main difference is that Ian Goodfellow and co-authors have worked with cross-entropy error instead of absolute error (as we do bellow). Moreover, in the following we assume a generator and a discriminator with unlimited capacity.", "Neural networks modelling essentially requires to define two things: an architecture and a loss function. We have already described the architecture of Generative Adversarial Networks. It consists in two networks:", "Let\u2019s take now a closer look at the \u201ctheoretical\u201d loss function of GANs. If we send to the discriminator \u201ctrue\u201d and \u201cgenerated\u201d data in the same proportions, the expected absolute error of the discriminator can then be expressed as", "The goal of the generator is to fool the discriminator whose goal is to be able to distinguish between true and generated data. So, when training the generator, we want to maximise this error while we try to minimise it for the discriminator. It gives us", "For any given generator G (along with the induced probability density p_g), the best possible discriminator is the one that minimises", "In order to minimise (with respect to D) this integral, we can minimise the function inside the integral for each value of x. It then defines the best possible discriminator for a given generator", "(in fact, one of the best because x values such that p_t(x)=p_g(x) could be handled in another way but it doesn\u2019t matter for what follows). We then search G that maximises", "Again, in order to maximise (with respect to G) this integral, we can maximise the function inside the integral for each value of x. As the density p_t is independent of the generator G, we can\u2019t do better than setting G such that", "Of course, as p_g is a probability density that should integrate to 1, we necessarily have for the best G", "So, we have shown that, in an ideal case with unlimited capacities generator and discriminator, the optimal point of the adversarial setting is such that the generator produces the same density as the true density and the discriminator can\u2019t do better than being true in one case out of two, just like the intuition told us. Finally, notice also that G maximises", "Under this form, we better see that G wants to maximise the expected probability of the discriminator to be wrong.", "The main takeaways of this article are:", "Even if the \u201chype\u201d that surrounds GANs is maybe a little bit exaggerated, we can say that the idea of adversarial training suggested by Ian Goodfellow and its co-authors is really a great one. This way to twist the loss function to go from a direct comparison to an indirect one is really something that can be very inspiring for further works in the deep learning area. To conclude, let\u2019s say that we don\u2019t know if the idea of GANs is really \u201cthe most interesting idea in the last 10 years in Machine Learning\u201d\u2026 but it\u2019s pretty obvious that it is, at least, one of the most interesting!", "Note: We highly recommend the interested readers to both read the initial paper \u201cAdversarial Neural Nets\u201d, that is really a model of clarity for a scientific paper, and watch the lecture video about GANs of Ali Ghodsi, who is truly an amazing lecturer/teacher. Additional explanation can be found in the tutorial about GANs written by Ian Goodfellow.", "Other articles written with Baptiste Rocca:", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Machine Learning Engineer at Meta. Towards Data Science writer. Mathematics instructor at UTC. www.linkedin.com/in/joseph-rocca-b01365158"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fcd6e4651a29&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-generative-adversarial-networks-gans-cd6e4651a29&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-generative-adversarial-networks-gans-cd6e4651a29&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-generative-adversarial-networks-gans-cd6e4651a29&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-generative-adversarial-networks-gans-cd6e4651a29&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----cd6e4651a29--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----cd6e4651a29--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@joseph.rocca?source=post_page-----cd6e4651a29--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@joseph.rocca?source=post_page-----cd6e4651a29--------------------------------", "anchor_text": "Joseph Rocca"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb17ebd108358&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-generative-adversarial-networks-gans-cd6e4651a29&user=Joseph+Rocca&userId=b17ebd108358&source=post_page-b17ebd108358----cd6e4651a29---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcd6e4651a29&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-generative-adversarial-networks-gans-cd6e4651a29&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcd6e4651a29&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-generative-adversarial-networks-gans-cd6e4651a29&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://pixabay.com/fr/users/devanath-1785462/", "anchor_text": "Devanath"}, {"url": "https://pixabay.com/", "anchor_text": "Pixabay"}, {"url": "https://medium.com/u/20ad1309823a?source=post_page-----cd6e4651a29--------------------------------", "anchor_text": "Baptiste Rocca"}, {"url": "https://arxiv.org/abs/1406.2661", "anchor_text": "Generative Adversarial Nets"}, {"url": "https://drive.google.com/drive/folders/1lHtjHQ8K7aemRQAnYMylrrwZp6Bsqqrb", "anchor_text": "https://drive.google.com/drive/folders/1lHtjHQ8K7aemRQAnYMylrrwZp6Bsqqrb"}, {"url": "https://en.wikipedia.org/wiki/Rejection_sampling", "anchor_text": "Rejection sampling"}, {"url": "https://en.wikipedia.org/wiki/Metropolis\u2013Hastings_algorithm", "anchor_text": "Metropolis-Hasting algorithm"}, {"url": "https://en.wikipedia.org/wiki/Cumulative_distribution_function", "anchor_text": "Cumulative Distribution Function"}, {"url": "http://www.gatsby.ucl.ac.uk/~gretton/papers/testing_workshop.pdf", "anchor_text": "these slides"}, {"url": "http://www.gatsby.ucl.ac.uk/~gretton/papers/GreBorRasSchSmo07.pdf", "anchor_text": "this article"}, {"url": "http://www.jmlr.org/papers/volume13/gretton12a/gretton12a.pdf", "anchor_text": "this article"}, {"url": "https://medium.com/u/20ad1309823a?source=post_page-----cd6e4651a29--------------------------------", "anchor_text": "Baptiste Rocca"}, {"url": "https://towardsdatascience.com/handling-imbalanced-datasets-in-machine-learning-7a0e84220f28", "anchor_text": "Handling imbalanced datasets in machine learningWhat should and should not be done when facing an imbalanced classes problem?towardsdatascience.com"}, {"url": "https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205", "anchor_text": "Ensemble methods: bagging, boosting and stackingUnderstanding the key concepts of ensemble learning.towardsdatascience.com"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----cd6e4651a29---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----cd6e4651a29---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----cd6e4651a29---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/data-science?source=post_page-----cd6e4651a29---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----cd6e4651a29---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcd6e4651a29&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-generative-adversarial-networks-gans-cd6e4651a29&user=Joseph+Rocca&userId=b17ebd108358&source=-----cd6e4651a29---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcd6e4651a29&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-generative-adversarial-networks-gans-cd6e4651a29&user=Joseph+Rocca&userId=b17ebd108358&source=-----cd6e4651a29---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcd6e4651a29&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-generative-adversarial-networks-gans-cd6e4651a29&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----cd6e4651a29--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fcd6e4651a29&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-generative-adversarial-networks-gans-cd6e4651a29&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----cd6e4651a29---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----cd6e4651a29--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----cd6e4651a29--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----cd6e4651a29--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----cd6e4651a29--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----cd6e4651a29--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----cd6e4651a29--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----cd6e4651a29--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----cd6e4651a29--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@joseph.rocca?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@joseph.rocca?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Joseph Rocca"}, {"url": "https://medium.com/@joseph.rocca/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "4.97K Followers"}, {"url": "http://www.linkedin.com/in/joseph-rocca-b01365158", "anchor_text": "www.linkedin.com/in/joseph-rocca-b01365158"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb17ebd108358&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-generative-adversarial-networks-gans-cd6e4651a29&user=Joseph+Rocca&userId=b17ebd108358&source=post_page-b17ebd108358--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F164289aafbe3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-generative-adversarial-networks-gans-cd6e4651a29&newsletterV3=b17ebd108358&newsletterV3Id=164289aafbe3&user=Joseph+Rocca&userId=b17ebd108358&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}