{"url": "https://towardsdatascience.com/deep-learning-for-tabular-data-using-pytorch-1807f2858320", "time": 1683002806.551507, "path": "towardsdatascience.com/deep-learning-for-tabular-data-using-pytorch-1807f2858320/", "webpage": {"metadata": {"title": "Deep Learning for Tabular Data using PyTorch | by Aakanksha NS | Towards Data Science", "h1": "Deep Learning for Tabular Data using PyTorch", "description": "Deep learning has proved to be groundbreaking in a lot of domains like Computer Vision, Natural Language Processing, Signal Processing, etc. However, when it comes to more structured, tabular data\u2026"}, "outgoing_paragraph_urls": [{"url": "https://docs.fast.ai/tabular.html", "anchor_text": "fast-ai library", "paragraph_index": 1}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html", "anchor_text": "LabelEncoder", "paragraph_index": 13}, {"url": "https://arxiv.org/pdf/1604.06737.pdf", "anchor_text": "Categorical embeddings", "paragraph_index": 17}, {"url": "https://pytorch.org/docs/stable/_modules/torch/utils/data/dataset.html#TensorDataset", "anchor_text": "Dataset", "paragraph_index": 21}, {"url": "https://jovian.ml/aakanksha-ns/shelter-outcome", "anchor_text": "my notebook", "paragraph_index": 26}, {"url": "https://www.linkedin.com/in/aakanksha-ns/", "anchor_text": "https://www.linkedin.com/in/aakanksha-ns/", "paragraph_index": 30}], "all_paragraphs": ["Deep learning has proved to be groundbreaking in a lot of domains like Computer Vision, Natural Language Processing, Signal Processing, etc. However, when it comes to more structured, tabular data consisting of categorical or numerical variables, traditional machine learning approaches (such as Random Forests, XGBoost) are believed to perform better. As expected, Neural nets have caught up and in many instances shown to be performing equally well or even better at times.", "The easiest way to perform deep learning with tabular data is through the fast-ai library and it gives really good results, but it might be a little too abstracted for someone who\u2019s trying to understand what is really going on behind the scenes. Hence, in this article, I\u2019ve covered how to build a simple deep learning model to deal with tabular data in Pytorch on a multiclass classification problem.", "Pytorch is a popular open-source machine library. It is as simple to use and learn as Python. A few other advantages of using PyTorch are its multi-GPU support and custom data loaders. If you\u2019re unfamiliar with the basics or need a revision, here\u2019s a good place to start:", "If you wanna follow along with the code, here\u2019s my Jupyter notebook:", "I\u2019ve used the Shelter Animal Outcomes Kaggle competition data:", "It\u2019s a tabular dataset consisting of about 26k rows and 10 columns in the training set. All columns except DateTime are categorical.", "Given certain features about a shelter animal (like age, sex, color, breed), predict its outcome.", "There are 5 possible outcomes: Return_to_owner, Euthanasia, Adoption, Transfer, Died. We are expected to find the probability of an animal\u2019s outcome belonging to each of the 5 categories.", "Although this step depends largely on the particular data and problem, there are two necessary steps that need to be followed:", "Nan (not a number) indicates a missing value in the dataset. The model doesn\u2019t accept Nan values, hence they must be either deleted or replaced.", "For numerical columns, a popular way of dealing with these values is to impute them with 0, mean, median, mode or some other function of the remaining data. Missing values might sometimes indicate an underlying feature in your dataset, so people often create a new binary column corresponding to the column with missing values to record whether the data was missing or not.", "For categorical columns, Nan values can be considered as their own category!", "Since our model can only take numerical inputs, we convert all our categorical elements to numbers. This means instead of using strings to represent categories, we use numbers. The numbers chosen to represent the categories should be in the range of 0 to the total number of different categories (including Nan ) in the column. This is so that when we create categorical embeddings for the column, we want to be able to index into our embedding matrix which would have one entry for each category. Here\u2019s a simple example of label encoding:", "I\u2019ve used the LabelEncoder class from the scikit-learn library to encode the categorical columns. You could define a custom class to do this and keep track of the category labels because you\u2019d need them to encode test data too.", "We also need to label encode the target if it has string entries. Also, make sure you maintain a dictionary mapping the encodings to original values because you\u2019ll need it to figure out the final output of your model.", "Along with the above-mentioned steps, I did a little more processing for the example problem.", "Note: In my notebook, I stacked the train and test columns and then did the preprocessing to avoid having to do label encoding based on the train set labels on the test set (because it would involve maintaining a dictionary of encoded labels to actual values). It was okay to do the stacking and processing here because there are no numerical columns (hence no imputing done) and the number of categories per column was fixed. In practice, we must never do this because it may leak some data from the test/validation sets to the training data and lead to an inaccurate evaluation of the model. For example, if you had missing values in a numerical column like age and decided to impute it with the average value, the average value should be calculated only on the train set (not stacked train-test-valid set) and this value should be used to impute missing values in validation and test sets too.", "Categorical embeddings are very similar to word embeddings which are commonly used in NLP. The basic idea is to have a fixed-length vector representation of each category in the column. How this is different from a one-hot encoding is that instead of having a sparse matrix, using embeddings, we get a dense matrix for each category with similar categories having values close to each other in the embedding space. Hence, this process not only saves up memory (as the one-hot encoding for columns having too many categories can really blow up the input matrix, also it is a very sparse matrix) but also reveals intrinsic properties of the categorical variables.", "For example, if we had a column of colors and we find embeddings for it, we can expect red and pink to be closer in the embedding space than red and blue", "Categorical embedding layers are equivalent to extra layers on top of each one-hot encoded input:", "For our shelter outcome problem, we have only categorical columns but I\u2019ll be considering columns with less than 3 values as continuous. To decide the length of each column\u2019s embedding vector I\u2019ve taken a simple function from the fast-ai library:", "We extend the Dataset (abstract) class provided by Pytorch for easier access to our dataset while training and for effectively using the DataLoader module to manage batches. This involves overwriting the __len__ and __getitem__ methods as per our particular dataset.", "Since we only need to embed categorical columns, we split our input into two parts: numerical and categorical.", "We then choose our batch size and feed it along with the dataset to the DataLoader. Deep learning is generally done in batches. DataLoader helps us in effectively managing these batches and shuffling the data before training.", "To do a sanity check, you can iterate through the created DataLoaders to look at each batch:", "Our data is split into continuous and categorical parts. We first convert the categorical parts into embedding vectors based on the previously determined sizes and concatenate them with the continuous parts to feed to the rest of the network. This picture demonstrates the model I\u2019ve used:", "Now we train the model on the training set. I\u2019ve used the Adam optimizer to optimize the cross-entropy loss. The training is pretty straightforward: iterate through each batch, do a forward pass, compute gradients, do a gradient descent and repeat this process for as many epochs as needed. You can look at my notebook to understand the code.", "Since we\u2019re interested in finding the probabilities for each class for our test inputs, we apply a Softmax function over our model output. I also made a Kaggle submission to see how well this model performs:", "We\u2019ve done very less feature engineering and data exploration and used a very basic deep learning architecture, yet our model has done better than about 50% of the solutions. This shows that this approach of modeling tabular data using neural networks is pretty powerful!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "ML Engineer @ Snap Inc. | MSDS University of San Francisco | CSE NIT Calicut https://www.linkedin.com/in/aakanksha-ns/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F1807f2858320&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-for-tabular-data-using-pytorch-1807f2858320&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-for-tabular-data-using-pytorch-1807f2858320&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-for-tabular-data-using-pytorch-1807f2858320&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-for-tabular-data-using-pytorch-1807f2858320&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----1807f2858320--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1807f2858320--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@aakanksha.ns?source=post_page-----1807f2858320--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@aakanksha.ns?source=post_page-----1807f2858320--------------------------------", "anchor_text": "Aakanksha NS"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff30e60797a83&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-for-tabular-data-using-pytorch-1807f2858320&user=Aakanksha+NS&userId=f30e60797a83&source=post_page-f30e60797a83----1807f2858320---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1807f2858320&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-for-tabular-data-using-pytorch-1807f2858320&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1807f2858320&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-for-tabular-data-using-pytorch-1807f2858320&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://design-nation.icons8.com/intro-to-data-tables-design-349f55861803", "anchor_text": "source"}, {"url": "https://docs.fast.ai/tabular.html", "anchor_text": "fast-ai library"}, {"url": "https://jovian.ml/aakanksha-ns/shelter-outcome", "anchor_text": "Animal_Shelter_Outcomeshared using jovian.ml"}, {"url": "https://www.kaggle.com/c/shelter-animal-outcomes/data", "anchor_text": "Shelter Animal OutcomesHelp improve outcomes for shelter animalswww.kaggle.com"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html", "anchor_text": "LabelEncoder"}, {"url": "https://arxiv.org/pdf/1604.06737.pdf", "anchor_text": "Categorical embeddings"}, {"url": "https://arxiv.org/pdf/1604.06737.pdf", "anchor_text": "Entity Embeddings of Categorical Variables"}, {"url": "https://pytorch.org/docs/stable/_modules/torch/utils/data/dataset.html#TensorDataset", "anchor_text": "Dataset"}, {"url": "https://jovian.ml/aakanksha-ns/shelter-outcome", "anchor_text": "my notebook"}, {"url": "https://www.usfca.edu/data-institute/certificates/fundamentals-deep-learning", "anchor_text": "https://www.usfca.edu/data-institute/certificates/fundamentals-deep-learning"}, {"url": "https://jovian.ml/aakashns/04-feedforward-nn", "anchor_text": "https://jovian.ml/aakashns/04-feedforward-nn"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----1807f2858320---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----1807f2858320---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/pytorch?source=post_page-----1807f2858320---------------pytorch-----------------", "anchor_text": "Pytorch"}, {"url": "https://medium.com/tag/embedding?source=post_page-----1807f2858320---------------embedding-----------------", "anchor_text": "Embedding"}, {"url": "https://medium.com/tag/multiclass-classification?source=post_page-----1807f2858320---------------multiclass_classification-----------------", "anchor_text": "Multiclass Classification"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1807f2858320&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-for-tabular-data-using-pytorch-1807f2858320&user=Aakanksha+NS&userId=f30e60797a83&source=-----1807f2858320---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1807f2858320&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-for-tabular-data-using-pytorch-1807f2858320&user=Aakanksha+NS&userId=f30e60797a83&source=-----1807f2858320---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1807f2858320&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-for-tabular-data-using-pytorch-1807f2858320&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1807f2858320--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F1807f2858320&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-for-tabular-data-using-pytorch-1807f2858320&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----1807f2858320---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----1807f2858320--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----1807f2858320--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----1807f2858320--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----1807f2858320--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----1807f2858320--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----1807f2858320--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----1807f2858320--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----1807f2858320--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@aakanksha.ns?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@aakanksha.ns?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Aakanksha NS"}, {"url": "https://medium.com/@aakanksha.ns/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "322 Followers"}, {"url": "https://www.linkedin.com/in/aakanksha-ns/", "anchor_text": "https://www.linkedin.com/in/aakanksha-ns/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff30e60797a83&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-for-tabular-data-using-pytorch-1807f2858320&user=Aakanksha+NS&userId=f30e60797a83&source=post_page-f30e60797a83--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fb925b3bc1777&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-for-tabular-data-using-pytorch-1807f2858320&newsletterV3=f30e60797a83&newsletterV3Id=b925b3bc1777&user=Aakanksha+NS&userId=f30e60797a83&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}