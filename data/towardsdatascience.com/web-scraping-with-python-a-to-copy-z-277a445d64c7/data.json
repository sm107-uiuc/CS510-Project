{"url": "https://towardsdatascience.com/web-scraping-with-python-a-to-copy-z-277a445d64c7", "time": 1682994899.435679, "path": "towardsdatascience.com/web-scraping-with-python-a-to-copy-z-277a445d64c7/", "webpage": {"metadata": {"title": "Web scraping with Python \u2014 A to Z | by Shai Ardazi | Towards Data Science", "h1": "Web scraping with Python \u2014 A to Z", "description": "Tracking and predicting the stock market\u2019s prices by enriching the up to date stock prices with the latest news stories. These news stories may not be available from an API and therefore would need\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/u/46f36e995de5?source=post_page-----277a445d64c7--------------------------------", "anchor_text": "Shai Ardazi", "paragraph_index": 0}, {"url": "https://medium.com/u/e0226be1cccd?source=post_page-----277a445d64c7--------------------------------", "anchor_text": "Eitan Kassuto", "paragraph_index": 0}, {"url": "https://www.itc.tech/", "anchor_text": "ITC (Israel Tech Challenge)", "paragraph_index": 2}, {"url": "https://www.crummy.com/software/BeautifulSoup/bs4/doc/", "anchor_text": "here", "paragraph_index": 12}, {"url": "https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol#Request_methods", "anchor_text": "HTTP GET requests", "paragraph_index": 13}, {"url": "https://en.wikipedia.org/robots.txt", "anchor_text": "https://en.wikipedia.org/robots.txt", "paragraph_index": 14}, {"url": "https://en.wikipedia.org/wiki/Proxy_server", "anchor_text": "proxy server", "paragraph_index": 20}, {"url": "https://www.sslproxies.org/", "anchor_text": "here", "paragraph_index": 20}, {"url": "https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Accept", "anchor_text": "Accept header", "paragraph_index": 23}, {"url": "https://en.wikipedia.org/wiki/User_agent#User_agent_identification", "anchor_text": "user agent", "paragraph_index": 23}, {"url": "https://medium.com/p/b04114e7289d#a7a9", "anchor_text": "full function in the appendix below", "paragraph_index": 25}, {"url": "https://medium.com/p/b04114e7289d#a7a9", "anchor_text": "Appendix", "paragraph_index": 27}, {"url": "https://en.wikipedia.org/wiki/Application_programming_interface", "anchor_text": "API", "paragraph_index": 29}, {"url": "https://genderize.io/", "anchor_text": "Genderize.io", "paragraph_index": 29}, {"url": "https://aylien.com/text-api/", "anchor_text": "Aylien Text Analysis", "paragraph_index": 29}, {"url": "https://en.wikipedia.org/wiki/Natural_language_processing", "anchor_text": "NLP", "paragraph_index": 35}, {"url": "https://medium.com/p/277a445d64c7#0a7a", "anchor_text": "here", "paragraph_index": 44}, {"url": "https://redash.io/", "anchor_text": "Redash", "paragraph_index": 50}, {"url": "https://en.wikipedia.org/wiki/Artificial_neural_network", "anchor_text": "Neural Networks", "paragraph_index": 52}, {"url": "https://www.inc.com/magazine/201809/amy-webb/china-artificial-intelligence.html", "anchor_text": "keen interest in AI", "paragraph_index": 57}, {"url": "https://docs.python.org/2/library/logging.html", "anchor_text": "logging package", "paragraph_index": 65}], "all_paragraphs": ["Written by Shai Ardazi & Eitan Kassuto \u2014 February 2019", "Tracking and predicting the stock market\u2019s prices by enriching the up to date stock prices with the latest news stories. These news stories may not be available from an API and therefore would need to be scraped from a news website. This is done by going through a web page and extracting text (or images) of interest.", "Our web scraping project was part of the Data Science fellows program at ITC (Israel Tech Challenge) which was designed to expose us to the real world problems a data scientist faces as well as to improve our coding skills.", "In this post, we show our main steps and challenges along the way. We have included code snippets and recommendations on how to create an end to end pipeline for web scraping. The code snippets we show here are not OOP (Object Oriented Programming) for the sake of simplicity, but we highly recommend to write OOP code in your web scraper implementation.", "In this project we were free to choose any website. The websites chosen by the rest of the cohort ranged from e-commerce to news websites showing the different applications of web scraping.", "We chose a website for scientific articles because we thought it would be interesting to see what kind of data we could obtain and furthermore what insights we could gather as a result of this data.", "We have chosen to keep the website anonymous. In any case the goal of this post is to outline how to build a pipeline for any website of interest.", "First, one must inspect the website in order to determine which data one would like to scrape. It involves a basic understanding of the websites structure so that your code can scrape the data you want.", "In order to inspect the structure of the website, open the inspector of the web page, right click on the page \u2192 hit \u201cInspect element\u201d.", "Then, locate the data you want to scrape and click on it. The highlighted part in the inspector pane shows the underlying HTML text of the webpage section of interest. The CSS class of the element is what Beautifulsoup will use to extract the data from the html.", "In the following screenshot one can see that the \u201ckeywords\u201d section is what needs to be scraped. Using the inspector, one can locate the HTML element of the \u201ckeywords\u201d section and its CSS class.", "Using beautiful soup, the code to get all keywords is as follows:", "From here, it\u2019s pretty much the same. Locate the desired section, inspect the HTML element and get the data. Full documentation and much more examples of beautifulsoup can be found here (very friendly).", "The scraping process involves many HTTP GET requests in a short amount of time because in many cases one may need to navigate automatically between multiple pages in order to get the data. Moreover, having an awesome scraper is not just about getting the data one wants, it\u2019s also about getting new data or updating existing data frequently \u2014 This might lead to being blocked by the website. This leads us to the next section:", "In general, websites don\u2019t like bot scrapers but they probably don\u2019t prevent it completely because of the search engine bots that scrape websites in order to categorize them. There\u2019s a robots exclusion standard that defines the website\u2019s terms and conditions with bot crawlers, which is usually found in the robots.txt file of the website. For example, the robots.txt file of Wikipedia can be found here: https://en.wikipedia.org/robots.txt.", "The first few lines of Wikipedia\u2019s robots.txt:", "As you can see, Wikipedia\u2019s restrictions are not too strict. However, some websites are very strict and do not allow crawling part of the website or all of it. Their robots.txt would include this:", "One way of doing this is by rotating through different proxies and user agents (headers) when making requests to the website. Also, it is important to be considerate in how often you make requests to the website to avoid being a \u2018spammer\u2019.", "Note \u2014 This is only for learning purposes. We do not encourage you to breach terms of any website.", "See below on how to implement this method in just a few simple steps.", "Implementing a proxy server can be done easily in Python. A list of free proxies can be found here (Note that free proxies are usually less stable and slower than paid ones. If you don\u2019t find the free ones good enough for your needs, you may consider getting a paid service).", "Looking at the free proxies list, one can use BeautifulSoup in order to get the IP addresses and ports. The structure of the above-mentioned website can seen below.", "The following function retrieves all the proxies\u2019 IPs and ports and returns a list of them:", "There are many HTTP headers that can be passed as part of a request when using the requests package in Python. We passed two header elements (which were sufficient for us), namely the Accept header (user permissions) and user agent (Pseudo-Browser).", "The pool of pseudo random headers was created as follows (see code below):", "The partial code (full function in the appendix below):", "Using the headers and proxies pools", "The following code shows an example of how to use the function we wrote before. We did not include the OOP code for the sake of simplicity. See Appendix for the full function random_header().", "Up until here we gave a brief introduction of web scraping and spoke about more advanced techniques on how to avoid being blocked by a website.", "In the following section we show 2 examples of how to use API calls for data enrichment: Genderize.io and Aylien Text Analysis.", "Genderize uses the first name of an individual to predict their gender (limited to male and female).", "The output of this API is structured as JSON as seen in the example below:", "This makes it very convenient to enrich the author data with each one\u2019s gender. Since the probability of the predicted gender is included, one can set a threshold to ensure better quality predictions (we set our threshold at 60% \u2014 see below for code snippets). The value this API brings is the ability to determine the gender distribution of authors for a specified topic.", "We did not have to worry about the API limit (1000 calls/day) since we were only able to scrape around 120 articles/day which on average resulted in less than 500 authors per day. If one is able to exceed this daily limit, the API limit would have to be taken into account. One way of avoiding this daily limit would be to check if the first name being evaluated has already been enriched in our database. This would allow us to determine the gender based on the existing data without wasting an API call.", "Some code snippets for the tech hungry:", "We were interested in seeing the growth of keywords over time for a specified topic (think Google Trends) and therefore decided that we should enrich our data with more keywords. To do this, we used an API called Aylien Text Analysis, specifically the concept extraction API. This API allows one to input text which after processing outputs a list of keywords extracted from the text using NLP. Two of the various fields we scraped for each article were the Title and Abstract, these fields were concatenated and used as the input for the API. An example of the output JSON can be seen below:", "In order to avoid duplicate keywords we checked that the keyword did not already exist in the keyword table of our database. In order to avoid adding too many keywords per article, two methods were instituted. The first was a simple keyword limit as seen in the code snippet below. The other made use of the score (probability of relevance) available in the output file for each keyword \u2014 This allows one to set a threshold (we used 80%) to ensure the most relevant keywords were added for each article.", "An example of how the API works is seen in the figure below:", "Below is a snippet of the code we used to connect to the Aylien Text API service:", "Let\u2019s move to the final part. So far we gave an introduction to web scraping and how to avoid being blocked, as well as using API calls in order to enrich one\u2019s data. In the final part of this post we will go through how to set up a database in order to store the data and how to access this data for visualization. Visualizations are a powerful tool one can use to extract insights from the data.", "When setting up the database for a web scraping project (or others in general) the following should be taken into account:", "This stage of the pipeline should be done with caution and one should validate that the chosen structure (in terms of columns types, lengths, keys etc.) is suitable for the data and can handle extreme cases (missing data, non-English characters etc.). Avoid relying on an ID that is used by the website as the primary/unique key unless you have a really good reason (in our case doi_link of an article is a unique string that is acceptable everywhere, so we use it as a unique identifier of an article).", "An example of tables creation using mysql.connector package:", "The function for building the database:", "Note \u2014 in lines 12, 17, 23 and 25 we use the logger object. This is for logging to an external logs file and it\u2019s super important. Creating a Logger class is recommended, you can see more below in this post, or click here.", "Insertion of new data differs a bit from updating existing data. When new data is inserted to DB, one should make sure there are not duplicates. Also, in case of an error, one should catch it, log it and save the portion of data that caused that error for future inspection.", "As seen below, we used again the cursor of mysql.connector in order to execute the SQL insert command.", "Dynamic data requires frequent updates. One should define the time deltas (differences) between two updates which depends on the data type and source. In our project, we had to take into account that the number of citations for all articles would have to be updated periodically. The following piece of code illustrates the update process:", "In order to help make sense of the collected data , one can use visualizations to provide an easy-to-understand overview of the data.", "The visualizations we created enabled us to gain insights into the following use cases:", "In order to explore the above use cases we created visualizations of our data. We did this by using a simple but powerful open-source tool called Redash that was connected to our AWS machine(other kinds of instances are also available).", "In order to set up Redash, do the following:", "Before moving on, here is an overview of the data we collected for the topic \u201cNeural Networks\u201d.", "As you can see, not a lot of data was retrieved \u2014 this was because of the limited time we had available on the AWS (Amazon Web Services) machines. Due to the lack of sufficient data, the reader should evaluate the results with a pinch of salt \u2014 this is at this stage a proof of concept and is by no means a finished product.", "For the high level trends we simply plotted the number of research papers published per month for the past 5 years.", "This visualization makes use of enriched author data from genderize to view the gender distribution of authors within the specified topic. As seen in the figure below there are a large proportion of authors whose gender are unknown due to limitations of the genderize API.", "Identifying leading countries in the field", "When scraping affiliations for each author we were able to extract the country of each one, allowing us to create the visualization below. China publishes the majority of research papers for the topic \u201cNeural Networks\u201d as is expected due to their keen interest in AI. This information could be interesting for policy makers since one can track the advancements in AI in leading countries. Firstly, it can be helpful to monitor these leading countries to find opportunities for partnership in order to advance AI in both countries. Secondly, policy makers can use these insights in order to emulate leading countries in advancing AI within their own country.", "Identifying top lead authors in the field", "As a first approach to identifying the top researchers we decided to to compare the lead authors with the most citations associated to their name.", "The larger the word, the more frequent it is in the database", "Snapshot of our dashboard in Redash", "We have reached the end of our Web Scraping with Python A \u2014 Z series. In the first part we gave a brief introduction of web scraping and spoke about more advanced techniques on how to avoid being blocked by a website. Also, we showed how one can use API calls in order to enrich the data to extract further insights. And lastly, we showed how to create a database for storing the data obtained from web scraping and how to visualize this data using an open source tool \u2014 Redash.", "This may not be as effective as it should be due to the limited speed of the free proxies but it is still worth trying.", "The full function to create random headers is as follows:", "Note \u2014 In line 22 we saved a message into a logs file. It\u2019s super important to have logs in your code! We suggest using logging package which is pretty simple to use.", "The logger class that we built and used everywhere in our code:", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Passionate about the power of data science and artificial intelligence."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F277a445d64c7&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweb-scraping-with-python-a-to-copy-z-277a445d64c7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweb-scraping-with-python-a-to-copy-z-277a445d64c7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweb-scraping-with-python-a-to-copy-z-277a445d64c7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweb-scraping-with-python-a-to-copy-z-277a445d64c7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----277a445d64c7--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----277a445d64c7--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@shaiardazi?source=post_page-----277a445d64c7--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@shaiardazi?source=post_page-----277a445d64c7--------------------------------", "anchor_text": "Shai Ardazi"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F46f36e995de5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweb-scraping-with-python-a-to-copy-z-277a445d64c7&user=Shai+Ardazi&userId=46f36e995de5&source=post_page-46f36e995de5----277a445d64c7---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F277a445d64c7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweb-scraping-with-python-a-to-copy-z-277a445d64c7&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F277a445d64c7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweb-scraping-with-python-a-to-copy-z-277a445d64c7&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/u/46f36e995de5?source=post_page-----277a445d64c7--------------------------------", "anchor_text": "Shai Ardazi"}, {"url": "https://medium.com/u/e0226be1cccd?source=post_page-----277a445d64c7--------------------------------", "anchor_text": "Eitan Kassuto"}, {"url": "https://unsplash.com/@jammypodger7470?utm_source=medium&utm_medium=referral", "anchor_text": "michael podger"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://www.itc.tech/", "anchor_text": "ITC (Israel Tech Challenge)"}, {"url": "https://pypi.org/project/beautifulsoup4/", "anchor_text": "BeautifulSoup"}, {"url": "https://pypi.org/project/requests/", "anchor_text": "requests"}, {"url": "https://pypi.org/project/mysql-connector-python/", "anchor_text": "mysql.connector"}, {"url": "https://www.crummy.com/software/BeautifulSoup/bs4/doc/", "anchor_text": "here"}, {"url": "https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol#Request_methods", "anchor_text": "HTTP GET requests"}, {"url": "https://en.wikipedia.org/robots.txt", "anchor_text": "https://en.wikipedia.org/robots.txt"}, {"url": "http://www.wikipedia.org/", "anchor_text": "http://www.wikipedia.org/"}, {"url": "https://en.wikipedia.org/wiki/Proxy_server", "anchor_text": "proxy server"}, {"url": "https://www.sslproxies.org/", "anchor_text": "here"}, {"url": "https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Accept", "anchor_text": "Accept header"}, {"url": "https://en.wikipedia.org/wiki/User_agent#User_agent_identification", "anchor_text": "user agent"}, {"url": "https://developer.mozilla.org/en-US/docs/Web/HTTP/Content_negotiation/List_of_default_Accept_values", "anchor_text": "here"}, {"url": "https://pypi.org/project/fake-useragent/", "anchor_text": "fake-useragent"}, {"url": "https://medium.com/p/b04114e7289d#a7a9", "anchor_text": "full function in the appendix below"}, {"url": "https://medium.com/p/b04114e7289d#a7a9", "anchor_text": "Appendix"}, {"url": "https://en.wikipedia.org/wiki/Application_programming_interface", "anchor_text": "API"}, {"url": "https://genderize.io/", "anchor_text": "Genderize.io"}, {"url": "https://aylien.com/text-api/", "anchor_text": "Aylien Text Analysis"}, {"url": "https://en.wikipedia.org/wiki/Natural_language_processing", "anchor_text": "NLP"}, {"url": "https://medium.com/p/277a445d64c7#0a7a", "anchor_text": "here"}, {"url": "https://redash.io/", "anchor_text": "Redash"}, {"url": "https://redash.io/help/open-source/setup#aws", "anchor_text": "https://redash.io/help/open-source/setup#aws"}, {"url": "https://en.wikipedia.org/wiki/Artificial_neural_network", "anchor_text": "Neural Networks"}, {"url": "https://www.inc.com/magazine/201809/amy-webb/china-artificial-intelligence.html", "anchor_text": "keen interest in AI"}, {"url": "https://docs.python.org/2/library/logging.html", "anchor_text": "logging package"}, {"url": "https://medium.com/tag/web-scraping?source=post_page-----277a445d64c7---------------web_scraping-----------------", "anchor_text": "Web Scraping"}, {"url": "https://medium.com/tag/mysql?source=post_page-----277a445d64c7---------------mysql-----------------", "anchor_text": "MySQL"}, {"url": "https://medium.com/tag/python?source=post_page-----277a445d64c7---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/proxy?source=post_page-----277a445d64c7---------------proxy-----------------", "anchor_text": "Proxy"}, {"url": "https://medium.com/tag/data-science?source=post_page-----277a445d64c7---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F277a445d64c7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweb-scraping-with-python-a-to-copy-z-277a445d64c7&user=Shai+Ardazi&userId=46f36e995de5&source=-----277a445d64c7---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F277a445d64c7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweb-scraping-with-python-a-to-copy-z-277a445d64c7&user=Shai+Ardazi&userId=46f36e995de5&source=-----277a445d64c7---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F277a445d64c7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweb-scraping-with-python-a-to-copy-z-277a445d64c7&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----277a445d64c7--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F277a445d64c7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweb-scraping-with-python-a-to-copy-z-277a445d64c7&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----277a445d64c7---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----277a445d64c7--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----277a445d64c7--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----277a445d64c7--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----277a445d64c7--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----277a445d64c7--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----277a445d64c7--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----277a445d64c7--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----277a445d64c7--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@shaiardazi?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@shaiardazi?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Shai Ardazi"}, {"url": "https://medium.com/@shaiardazi/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "81 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F46f36e995de5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweb-scraping-with-python-a-to-copy-z-277a445d64c7&user=Shai+Ardazi&userId=46f36e995de5&source=post_page-46f36e995de5--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F4a1daab236da&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweb-scraping-with-python-a-to-copy-z-277a445d64c7&newsletterV3=46f36e995de5&newsletterV3Id=4a1daab236da&user=Shai+Ardazi&userId=46f36e995de5&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}