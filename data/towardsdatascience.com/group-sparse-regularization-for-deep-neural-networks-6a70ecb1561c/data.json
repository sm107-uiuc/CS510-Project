{"url": "https://towardsdatascience.com/group-sparse-regularization-for-deep-neural-networks-6a70ecb1561c", "time": 1683000980.7431092, "path": "towardsdatascience.com/group-sparse-regularization-for-deep-neural-networks-6a70ecb1561c/", "webpage": {"metadata": {"title": "Group Sparse Regularization for Deep Neural Networks | by Himaprasoon P.T | Towards Data Science", "h1": "Group Sparse Regularization for Deep Neural Networks", "description": "With the advancement in hardware technologies and thereby reduced costs, training large neural networks has become a lot easier. Due to this, it has become common to throw in large networks for even\u2026"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/pdf/1607.00485.pdf", "anchor_text": "Group Sparse Regularization for Deep Neural Networks", "paragraph_index": 0}, {"url": "http://www.chioka.in/differences-between-l1-and-l2-as-loss-function-and-regularization/", "anchor_text": "Click here to know more about L1 and L2 regularization", "paragraph_index": 9}, {"url": "https://arxiv.org/pdf/1607.00485.pdf", "anchor_text": "Group Sparse Regularization for Deep Neural Network", "paragraph_index": 22}], "all_paragraphs": ["Paper summary of Group Sparse Regularization for Deep Neural Networks", "With the advancement in hardware technologies and thereby reduced costs, training large neural networks has become a lot easier. Due to this, it has become common to throw in large networks for even simple tasks. Having too many layers in a network can lead to multiple problems like,", "Then there is the problem of feature selection. Feature selection is the process of searching for a subset of all the features which are important for modeling. This is done usually as a separate pipeline before modeling starts.", "Sparse group regularization(SGR) tackles these three problems/ issues", "and creates a single process/pipeline for tackling all of these. In neural networks, feature selection can be considered as pruning of nodes at input layer and that is how it is addressed in sparse group regularization.", "If all the incoming and outgoing weights of a neuron are 0, then that node can be removed.", "In the above ANN image, the red connections are weights which are zero. As all the connections from input feature X2 are zeros, this feature can be removed. Similarly for node H3, as all the outgoing weights are zeros, this node can also be removed. This is the idea of sparsity used in pruning of nodes and features.", "L1 regularization (Lasso) is one way in which sparsity of weights can be achieved. In this technique the sum of absolute values of the weights are penalized during training.", "As the weights are penalized by their absolute magnitude, sparsity can be achieved using L1 regularization.", "Click here to know more about L1 and L2 regularization.", "However, using L1(Lasso) alone cannot guarantee a systematic sparsity using which nodes can be removed. In order to remove a node, all the outgoing weights from the node should be zero. L1 can result to a different type of sparsity where it is unstructured. See the image below", "Here 2 weights are zero but none of the nodes can be removed. Therefore L1 alone cannot help in pruning. This is where sparse group regularization comes to the picture.", "To get a structured sparsity for pruning, we need to group outgoing connections from each neuron and force them to be zero. In order to do so, 3 different types of groups of weights are considered,", "Total set of Groups G is", "Sparse Group Regularization can be written as,", "where |g| denotes the dimensionality of the vector g, and it ensures that each group gets weighted uniformly. Here the L2 norm of each group is used. As L2 norm cannot create sparsity unlike L1, a thresholding step is done to convert low weights to zeros.", "This formulation is still suboptimal (even after thresholding) to get high level of sparsity. To enforce this, a combination of lasso and sparse group penalty is used. Sparse Group Lasso penalty is defined as,", "where the second term is the L1 norm of the weights.", "Figure 4 shows a comparison between the three types of regularization penalties discussed in this blog. The 2x5 matrix represent the weight matrix between a 2 dimensional input layer and a 5 dimensional output layer.", "The lasso penalty (l1) removes connections without taking node level considerations. In the image, it has resulted in 40% sparsity of connections. The second neuron in the output layer (5 dim) may be removed as all the incoming connections from both the input nodes are grayed out (See column 2 in Lasso in Figure 4).", "In Group Lasso (After thresholding in Sparse Group Regularization) , all the connections from the second neuron input layer are zero and therefore can be removed.", "The sparse group lasso penalty combines the advantages from both the previous formulations. Two nodes (2nd and 5th) from the output layer can be removed and also the second input neuron can be removed as well. The number of connections are reduced to 70% and leads to a very compact network.", "[1] Simone Scardapane , Amir Hussain and Aurelio Uncini , Group Sparse Regularization for Deep Neural Networks 2017", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F6a70ecb1561c&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgroup-sparse-regularization-for-deep-neural-networks-6a70ecb1561c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgroup-sparse-regularization-for-deep-neural-networks-6a70ecb1561c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgroup-sparse-regularization-for-deep-neural-networks-6a70ecb1561c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgroup-sparse-regularization-for-deep-neural-networks-6a70ecb1561c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----6a70ecb1561c--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----6a70ecb1561c--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@himaprasoon?source=post_page-----6a70ecb1561c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@himaprasoon?source=post_page-----6a70ecb1561c--------------------------------", "anchor_text": "Himaprasoon P.T"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F60be4a4772f3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgroup-sparse-regularization-for-deep-neural-networks-6a70ecb1561c&user=Himaprasoon+P.T&userId=60be4a4772f3&source=post_page-60be4a4772f3----6a70ecb1561c---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6a70ecb1561c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgroup-sparse-regularization-for-deep-neural-networks-6a70ecb1561c&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6a70ecb1561c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgroup-sparse-regularization-for-deep-neural-networks-6a70ecb1561c&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://arxiv.org/pdf/1607.00485.pdf", "anchor_text": "Group Sparse Regularization for Deep Neural Networks"}, {"url": "http://www.chioka.in/differences-between-l1-and-l2-as-loss-function-and-regularization/", "anchor_text": "Click here to know more about L1 and L2 regularization"}, {"url": "https://arxiv.org/pdf/1607.00485.pdf", "anchor_text": "Group Sparse Regularization for Deep Neural Network"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----6a70ecb1561c---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/data-science?source=post_page-----6a70ecb1561c---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/regularization?source=post_page-----6a70ecb1561c---------------regularization-----------------", "anchor_text": "Regularization"}, {"url": "https://medium.com/tag/feature-selection?source=post_page-----6a70ecb1561c---------------feature_selection-----------------", "anchor_text": "Feature Selection"}, {"url": "https://medium.com/tag/pruning?source=post_page-----6a70ecb1561c---------------pruning-----------------", "anchor_text": "Pruning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6a70ecb1561c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgroup-sparse-regularization-for-deep-neural-networks-6a70ecb1561c&user=Himaprasoon+P.T&userId=60be4a4772f3&source=-----6a70ecb1561c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6a70ecb1561c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgroup-sparse-regularization-for-deep-neural-networks-6a70ecb1561c&user=Himaprasoon+P.T&userId=60be4a4772f3&source=-----6a70ecb1561c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6a70ecb1561c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgroup-sparse-regularization-for-deep-neural-networks-6a70ecb1561c&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----6a70ecb1561c--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F6a70ecb1561c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgroup-sparse-regularization-for-deep-neural-networks-6a70ecb1561c&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----6a70ecb1561c---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----6a70ecb1561c--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----6a70ecb1561c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----6a70ecb1561c--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----6a70ecb1561c--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----6a70ecb1561c--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----6a70ecb1561c--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----6a70ecb1561c--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----6a70ecb1561c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@himaprasoon?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@himaprasoon?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Himaprasoon P.T"}, {"url": "https://medium.com/@himaprasoon/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "13 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F60be4a4772f3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgroup-sparse-regularization-for-deep-neural-networks-6a70ecb1561c&user=Himaprasoon+P.T&userId=60be4a4772f3&source=post_page-60be4a4772f3--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F60be4a4772f3%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgroup-sparse-regularization-for-deep-neural-networks-6a70ecb1561c&user=Himaprasoon+P.T&userId=60be4a4772f3&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}