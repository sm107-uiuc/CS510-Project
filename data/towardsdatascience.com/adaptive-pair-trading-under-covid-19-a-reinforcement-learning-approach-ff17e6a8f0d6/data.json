{"url": "https://towardsdatascience.com/adaptive-pair-trading-under-covid-19-a-reinforcement-learning-approach-ff17e6a8f0d6", "time": 1683011963.876065, "path": "towardsdatascience.com/adaptive-pair-trading-under-covid-19-a-reinforcement-learning-approach-ff17e6a8f0d6/", "webpage": {"metadata": {"title": "Adaptive Pair Trading under COVID, a Reinforcement Learning Approach | by Marshall Chang | Towards Data Science", "h1": "Adaptive Pair Trading under COVID, a Reinforcement Learning Approach", "description": "This is one of the articles of A.I. Capital Management\u2019s Research Article Series, with the intro article here. This one is about applying RL on market neutral strategies, specifically, optimizing a\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/applied-deep-reinforcement-learning-in-quantitative-trading-both-momentum-and-market-neutral-c0eef522ea11", "anchor_text": "here", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/questions-96667b06af5", "anchor_text": "rules and guidelines", "paragraph_index": 20}, {"url": "https://towardsdatascience.com/readers-terms-b5d780a700a4", "anchor_text": "Reader Terms", "paragraph_index": 20}], "all_paragraphs": ["This is one of the articles of A.I. Capital Management\u2019s Research Article Series, with the intro article here. This one is about applying RL on market neutral strategies, specifically, optimizing a simple pair trading strategy with RL agent being the capital allocator on per trade basis, while leaving the entrance/exit signal untouched. The goal is to optimize an existing signal\u2019s sequential trade size allocation while letting the agent adapt its actions to market regimes/conditions.", "Author: Marshall Chang is the founder and CIO of A.I. Capital Management, a quantitative trading firm that is built on Deep Reinforcement Learning\u2019s end-to-end application to momentum and market neutral trading strategies. The company primarily trades the Foreign Exchange markets in mid-to-high frequencies.", "Pairs trading is the foundation of market neutral strategy, which is one of the most sought-after quantitative trading strategies because it does not profit from market directions, but from the relative returns between a pair of assets, avoiding systematic risk and the Random Walk complexity. The profitability of market neutral strategies lie within the assumed underlying relationship between pairs of assets, however, when such relationship no longer withhold, often during volatile regime-shifting times such as this year with COVID-19, returns generally diminishes for such strategies. In fact, according to HFR (Hedge Fund Research, Inc.), the HFRX Equity Hedge Index, by the end of July, 2020, reported a YTD return of -9.74%[1]; its close relative, the HFRX Relative Value Arbitrage Index, reported a YTD return of -0.85%. There is no secret that for market neutral quants, or perhaps any quants, the challenge is not just to find profitable signals, but more in how to quickly detect and adapt complex trading signals during regime-shifting times.", "Within the field of market neutral trading, most research have been focusing on uncovering correlations and refining signals, often using proprietary alternative data purchased at high costs to find an edge. However, optimization of capital allocation at trade size and portfolio level is often neglected. We found that lots of pair trading signals, though complex, still utilizes fixed entry thresholds and linear allocations. With the recent advancement of complex models and learning algorithms such as Deep Reinforcement Learning (RL), these class of algorithm is yearning for innovation with non-linear optimization.", "To address the detection and adaptation of pair trading strategies through regime shifting times, our unique approach is to solve trade allocation optimization with sequential agent-based solution directly trained on top of existing signal generalization process, with clear tracked improvement and limited overhead of deployment.", "Internally named as AlphaSpread, this project demonstrates RL sequential trade size allocation\u2019s ROI (Return on Investment) improvement over standard linear trade size allocation on 1 pair spread trading of U.S. S&P 500 equities. We take the existing pair trading strategy with standard allocation per trade as baseline, train RL allocator represented by a deep neural network model in our customized Spread Trading Gym environment, then test on out-of-sample data and aim to outperform baseline ending ROI.", "Specifically, we select cointegrated pairs based on their stationary spreads our statistical models. Cointegrated pairs are usually within the same industry, but we also include cross sectional pairs that show strong cointegration. The trading signal are generated by reaching pre-defined threshold of z-score on residues predicted by the statistical model using daily close prices. The baseline for this example allocates fixed 50% of overall portfolio to each trading signal, whereas the RL allocator output 0\u2013100% allocation for each trading signal sequentially based on current market condition represented by a lookback of z-score.", "We summarize our RL approach\u2019s pairs trading ROI against baseline linear allocation for 107 U.S. Equity pairs traded. The ROI is calculated with ending NAV of testing period against each pair\u2019s $100,000 starting capital. The result is from back-testing on out-of-sample data between 2018 to April 2020 (COVID-19 months included). The RL allocators are trained with data between 2006 and 2017. In both cases fees are not consider in the testing. We have achieved on average 9.82% per pair ROI improvement over baseline approach, with maximum of 55.62% and minimum of 0.08%.", "In other words, with limited model tuning, this approach is able to result in generalized improvement of ROI through early detecting of regime-shifting and the accordingly capital allocation adaptation by the RL allocator agent.", "The goal of this project is to demonstrate out-of-sample generalization of the underlying improvements on a very simple one-pair trading signals, hence providing guidance on adapting such methodology on large scale complex market neutral strategies to be deployed. Below is a discussion of the 3 goals we set out to achieve in this experiment.", "Repeatability \u2014 This RL framework consists of customized pairs trading RL environment used to accurately train and test RL agents, RL training algorithms including DQN, DDPG and Async Actor Critics, RL automatic training roll out mechanism that integrates memory prioritized replay, dynamic model tuning, exploration/exploitation and etc., enabling repeatability for large datasets with minimum customization and hand tuning. The advantage of running RL compared with other machine learning algorithm is that it is an end-to-end system from training data generalization, reward function design, model and learning algorithm selection to output a sequential policy. A well-tuned system requires minimum maintenance and the retraining / readapting of models to new data is done in the same environment.", "Sustainability \u2014 Under the one-pair trading example, the pairs cointegration test and RL training were done using data from 2006 and 2017, and then trained agents run testing from 2018 to early 2020. The training and testing data split are roughly 80:20. With RL automatic training roll out, we can generalize sustainable improvements over baseline return for more than 2 years across hundreds of pairs. The RL agent learns to allocate according to the lookback of z-scores representing the pathway of the pair\u2019s cointegration as well as volatility and is trained with exploration / exploitation to find policy that maximize ending ROI. Compared with traditional supervised and unsupervised learning with static input \u2014 output, RL algorithms has built-in robustness for generalization in that it directly learns state-policy values with a reward function that reflects realized P/L. The RL training targets are always non-static in that the training experience improves as the agent interacts with the environment and improves its policy, hence the reinforcement of good behavior and vice versa.", "Scalability \u2014 Train and deploy large scale end-to-end Deep RL trading algorithms is still its infancy in quant trading, but we believe it is the future of alpha in our field, as RL has demonstrated dramatic improvement over traditional ML in the game space (AlphaGo, Dota etc.). This RL framework is well versed to apply to different pair trading strategies that is deployed by market neutral funds. With experience running RL system in multiple avenues of quant trading, we can customize environment, training algorithms and reward function to effectively solve unique tasks in portfolio optimization, powered by RL\u2019s agent based sequential learning that traditional supervised and unsupervised learning models cannot achieve.", "If the signal makes money, it makes money with linear allocation (always trade x unit). But when it doesn\u2019t, obviously we want to redo the signal, let it adapt to new market conditions. However, sometimes that\u2019s not easy to do, and a quick fix might be a RL agent/layer on top of existing signal process. In our case, we let the agent observe a dataset that represents volatility of the spreads, and decide on the pertinent allocation based on past trades and P/L.", "Signal Generalization Process \u2014 We first run a linear regression on both assets\u2019 past look back price history (2006\u20132017 daily price), then we do OLS test to obtain the residual, with which we run unit root test (Augmented Dickey\u2013Fuller test) to check the existence of cointegration. In this example, we set the p-value threshold at 0.5% to reject unit root hypothesis, which results in a universe of 2794 S&P 500 pairs that pass the test. Next phrase is how we set the trigger conditions. First, we normalize the residual to get a vector that follows assumed standard normal distribution. Most tests use two sigma level reaches 95% which is relatively difficult to trigger. To generate enough trading for each pair, we set our threshold at one sigma. After normalization, we obtain a white noise follows N(0,1), and set +/- 1 as the threshold. Overall, the signal generation process is very straight forward. If the normalized residual gets above or below threshold, we long the bearish one and short the bullish one, and vice versa. We only need to generate trading signal of one asset, and the other one should be the opposite direction", "Deep Reinforcement Learning \u2014 The RL training regimes starts with running an exploration to exploitation linear annealed policy to generate training data by running the training environment, which in this case runs the same 2006\u20132017 historical data as with the cointegration. The memory is stored in groups of", "State, Action, Reward, next State, next Action (SARSA)", "Here we use a mixture of DQN and Policy Gradient learning target, in that our action outputs are continuous (0\u2013100%) yet sample inefficient (within hundreds of trades per pair due to daily frequency). Our training model updates iteratively with", "Q(State) = reward + Q-max (next States, next Actions)", "Essentially, RL agent is learning the q value of continuous-DQN but trained with policy gradient on the improvements of each policy, hence avoiding the sample inefficiency (Q learning is guaranteed to converge to training global optimal) and tendency to stuck in local minimum too quickly (avoiding all 0 or 1 outputs for PG). Once the warm-up memories are stored, we train the model (in this case is a 3-layer dense net outputting single action) with the memory data as agent continues to interact with the environment and roll out older memories.", "Note from Towards Data Science\u2019s editors: While we allow independent authors to publish articles in accordance with our rules and guidelines, we do not endorse each author\u2019s contribution. You should not rely on an author\u2019s works without seeking professional advice. See our Reader Terms for details.", "Sutton RS, Barto AG. Reinforcement learning: An introduction. MIT press; 2018.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Founder and CIO of A.I. Capital Management"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fff17e6a8f0d6&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadaptive-pair-trading-under-covid-19-a-reinforcement-learning-approach-ff17e6a8f0d6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadaptive-pair-trading-under-covid-19-a-reinforcement-learning-approach-ff17e6a8f0d6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadaptive-pair-trading-under-covid-19-a-reinforcement-learning-approach-ff17e6a8f0d6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadaptive-pair-trading-under-covid-19-a-reinforcement-learning-approach-ff17e6a8f0d6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----ff17e6a8f0d6--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ff17e6a8f0d6--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@czz_64589?source=post_page-----ff17e6a8f0d6--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@czz_64589?source=post_page-----ff17e6a8f0d6--------------------------------", "anchor_text": "Marshall Chang"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9bb673c43bba&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadaptive-pair-trading-under-covid-19-a-reinforcement-learning-approach-ff17e6a8f0d6&user=Marshall+Chang&userId=9bb673c43bba&source=post_page-9bb673c43bba----ff17e6a8f0d6---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fff17e6a8f0d6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadaptive-pair-trading-under-covid-19-a-reinforcement-learning-approach-ff17e6a8f0d6&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fff17e6a8f0d6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadaptive-pair-trading-under-covid-19-a-reinforcement-learning-approach-ff17e6a8f0d6&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@m_b_m?utm_source=medium&utm_medium=referral", "anchor_text": "M. B. M."}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://towardsdatascience.com/applied-deep-reinforcement-learning-in-quantitative-trading-both-momentum-and-market-neutral-c0eef522ea11", "anchor_text": "here"}, {"url": "https://towardsdatascience.com/questions-96667b06af5", "anchor_text": "rules and guidelines"}, {"url": "https://towardsdatascience.com/readers-terms-b5d780a700a4", "anchor_text": "Reader Terms"}, {"url": "https://www.hedgefundresearch.com/family-indices/hfrx", "anchor_text": "https://www.hedgefundresearch.com/family-indices/hfrx"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----ff17e6a8f0d6---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/quant-trading?source=post_page-----ff17e6a8f0d6---------------quant_trading-----------------", "anchor_text": "Quant Trading"}, {"url": "https://medium.com/tag/pair-trading?source=post_page-----ff17e6a8f0d6---------------pair_trading-----------------", "anchor_text": "Pair Trading"}, {"url": "https://medium.com/tag/stock-market?source=post_page-----ff17e6a8f0d6---------------stock_market-----------------", "anchor_text": "Stock Market"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----ff17e6a8f0d6---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fff17e6a8f0d6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadaptive-pair-trading-under-covid-19-a-reinforcement-learning-approach-ff17e6a8f0d6&user=Marshall+Chang&userId=9bb673c43bba&source=-----ff17e6a8f0d6---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fff17e6a8f0d6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadaptive-pair-trading-under-covid-19-a-reinforcement-learning-approach-ff17e6a8f0d6&user=Marshall+Chang&userId=9bb673c43bba&source=-----ff17e6a8f0d6---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fff17e6a8f0d6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadaptive-pair-trading-under-covid-19-a-reinforcement-learning-approach-ff17e6a8f0d6&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ff17e6a8f0d6--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fff17e6a8f0d6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadaptive-pair-trading-under-covid-19-a-reinforcement-learning-approach-ff17e6a8f0d6&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----ff17e6a8f0d6---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----ff17e6a8f0d6--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----ff17e6a8f0d6--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----ff17e6a8f0d6--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----ff17e6a8f0d6--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----ff17e6a8f0d6--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----ff17e6a8f0d6--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----ff17e6a8f0d6--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----ff17e6a8f0d6--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@czz_64589?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@czz_64589?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Marshall Chang"}, {"url": "https://medium.com/@czz_64589/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "87 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9bb673c43bba&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadaptive-pair-trading-under-covid-19-a-reinforcement-learning-approach-ff17e6a8f0d6&user=Marshall+Chang&userId=9bb673c43bba&source=post_page-9bb673c43bba--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F3cab8315b9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadaptive-pair-trading-under-covid-19-a-reinforcement-learning-approach-ff17e6a8f0d6&newsletterV3=9bb673c43bba&newsletterV3Id=3cab8315b9&user=Marshall+Chang&userId=9bb673c43bba&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}