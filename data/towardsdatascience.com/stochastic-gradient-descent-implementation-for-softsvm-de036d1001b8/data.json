{"url": "https://towardsdatascience.com/stochastic-gradient-descent-implementation-for-softsvm-de036d1001b8", "time": 1683017177.3695512, "path": "towardsdatascience.com/stochastic-gradient-descent-implementation-for-softsvm-de036d1001b8/", "webpage": {"metadata": {"title": "Stochastic gradient descent implementation for SoftSVM | by Jaime Dantas | Towards Data Science", "h1": "Stochastic gradient descent implementation for SoftSVM", "description": "This is a quick tutorial on how to implement the Stochastic Gradient Descent (SGD) optimization method for SoftSVM on MATLAB to find a linear classifier with minimal empirical loss. We\u2019ll see how to\u2026"}, "outgoing_paragraph_urls": [{"url": "https://archive.ics.uci.edu/ml/datasets/banknote+authentication", "anchor_text": "UCI repository", "paragraph_index": 2}, {"url": "https://github.com/jaimedantas/SGD-SoftSVM/blob/main/dataset/data_banknote_authentication.txt", "anchor_text": "data_banknote_authentication.txt", "paragraph_index": 3}, {"url": "https://en.wikipedia.org/wiki/Stochastic_gradient_descent", "anchor_text": "article", "paragraph_index": 6}, {"url": "https://en.wikipedia.org/wiki/Support_vector_machine", "anchor_text": "Support Vector Machines", "paragraph_index": 7}, {"url": "https://pt.wikipedia.org/wiki/Perceptron#:~:text=O%20perceptron%20%C3%A9%20um%20tipo,neural%20feedforward%3A%20um%20classificador%20linear.", "anchor_text": "Perceptron", "paragraph_index": 7}, {"url": "https://en.wikipedia.org/wiki/Design_matrix", "anchor_text": "design matrix", "paragraph_index": 8}, {"url": "https://github.com/jaimedantas/SGD-SoftSVM/blob/main/code/run_softSVM.m", "anchor_text": "run_softSVM.m", "paragraph_index": 11}, {"url": "https://github.com/jaimedantas/SGD-SoftSVM/blob/main/code/run_softSVM.m", "anchor_text": "run_softSVM.m", "paragraph_index": 12}, {"url": "http://jaimedantas.com/", "anchor_text": "website", "paragraph_index": 23}], "all_paragraphs": ["This is a quick tutorial on how to implement the Stochastic Gradient Descent (SGD) optimization method for SoftSVM on MATLAB to find a linear classifier with minimal empirical loss.", "We\u2019ll see how to tune our algorithm with different regularizers by analyzing the binary and hinge loss.", "We will use a 4-dimensional dataset with 1,372 data points for this classification. This dataset is the banknote authentication dataset from the UCI repository. All source codes used in this tutorial are available in the repository below.", "After downloading the data_banknote_authentication.txt file, we\u2019ll import it on MATLAB in Home > Import Data. We need to import them as a Numeric Matrix as shown below.", "Note that the last column of this dataset represents the class 0 or 1. We\u2019ll replace this label with +1 and -1 later on.", "Stochastic gradient descent is an interactive method used in machine learning for optimization problems. Unlike the gradient descent (GD) alternative, SGD uses random data points to calculate the direction of the gradient on each interaction. On expectation, the SGS converges to a minimum of the convex.", "There are several advantages when using SGD against GD, but we won\u2019t dive deep into the details of these two machine learning methods. If you want to know more about it, this article can help.", "Support Vector Machines (SVM) is a linear classifier that can be viewed as a similar algorithm to the Perceptron. There are two types of SVM: The hard margin and the soft margin SVM. The former is used when we have a linearly separable dataset, and the latter is for not linearly separable data. For SoftSVM, we allow ourselves to have some slack so we can create a linear classifier even if the data is not linearly separable. The figure below shows a dataset where both the HardSVM and the SoftSVM are identical.", "Enough said, let\u2019s implement the algorithm! We will implement a general linear classifier by adding a column of 1 to the design matrix. During the optimization, we\u2019ll keep track of both the empirical binary loss the empirical hinge loss of the current weight vector w, and we\u2019ll output the last w vector as our predictor.", "First, I loaded the dataset into MATLAB. Then, I created a function that receives the x, t and \u03bb. The number of updates T is 500 in this implementation. Before doing any operation with the dataset, I shuffle it to eliminate any sort of bias that may exist (even though for SGD in SoftSVM it does not matter since it guarantees randomness). I then add a column of ones to the matrix x and initialize with \u03b8 = 0. The algorithm is shown below [1].", "Our algorithm will output the last vector w as well as a cell array with the binary loss and another one with the hinge loss with the values for each execution Ti.", "We\u2019ll run our algorithm against the dataset with various values for the regularization parameter \u03bb = {100, 10, 1, 0.1, 0.01, 0,001}. I created the run_softSVM.m script to run the SoftSVM algorithm with all \u03bb.", "At the end of the run_softSVM.m, I output the value and position of the smallest binary loss out of the 500 executions for each chosen \u03bb. The figures below show the charts for the binary loss with \u03bb = 1, 0.001 and the hinge loss for \u03bb = 100.", "Since we update the gradient and calculate the loss based on a random data point for each interaction, we expect to see some noise on all curves. Below we can see the minimum binary loss for each value of \u03bb we ran our algorithm.", "That being shown, let\u2019s first analyze the binary loss for \u03bb = 0.001. Even though the curve is not totally monotone, we can see that it is approximately monotone for this case. It has a decreasing tendency. This is seen when we draw a decreasing trend curve for this graph as shown in the figure below.", "In other words, the larger the number of interactions, the smaller the binary loss. However, after execution 384, we start to bounce around the minimum and we start to get slightly larger losses. The binary loss for \u03bb = 1 showed a similar behaviour too, with a decreasing tendency. This time, however, we reached the minimum binary loss in execution 367. So, the algorithm converged quicker for \u03bb = 1 for this case. In addition, we have a slightly smaller binary loss when we run with \u03bb = 1 compared to \u03bb = 0.001. So, if we use the last interaction w as our classifier for both these cases, we would be fine.", "Moving on to the hinge loss for \u03bb = 100, we can clearly see that it is not monotone since it begins to increase after around interaction 150 or so. In fact, if this time we pick the last interaction w, we would have a large magnitude difference in error compared with the best predictor. This is because the minimum hinge loss for this case is on execution 83. Also, the minimum hinge loss for \u03bb = 100 was larger than the other cases (\u03bb = {10, 1, 0.1, 0.001}).", "Therefore, the value of \u03bb plays an important role in the number of interactions the algorithm takes to converge and the minimum binary loss. From this analysis, we saw that choosing a larger \u03bb, let\u2019s say 100 or 10, can lead to a faster execution time with a slightly larger loss. Additionally, very small values of \u03bb, let\u2019s say 0.1, may require more interactions to converge, but it can give a smaller loss. This induces that we should test different values of \u03bb to tune the Soft SVM with stochastic gradient descent in order to find out the best parameter for the minimum binary loss with the least number of interactions (to save precious computer execution time).", "We can even combine different values of \u03bb in our algorithm, so we can run it with a large value of \u03bb for the first interactions then change it to a small value of \u03bb for the remaining interactions to converge faster. Even though the binary loss is a great indicator that we are converging, it is important to use the surrogate functions like the hinge loss to analyze whether or not the predicted vector w gives us a good classifier.", "We saw that SGD converges on expectation even though it takes random steps towards the minimum on each interaction. We implemented the SGD for SoftSVM on MATLAB and analyzed the impact of having different values of regularizer \u03bb.", "Choosing the ideal value of \u03bb proved to be quite challenging, and it required a detailed analysis of the different values of \u03bb. We found out that we can even combine distinct values of the regularize during different parts of the algorithm to improve performance.", "I hope you have learnt a bit more about the importance of the regularizer on SGD in SoftSVM.", "I\u2019m an M.A.Sc. student at York University, and a Software Engineer by heart. During the past decade, I\u2019ve been working in several industries in areas such as software development, cloud computing and systems engineering. Currently, I\u2019m developing research on cloud computing and distributes systems.", "You can check my work on my website if you want to.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fde036d1001b8&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstochastic-gradient-descent-implementation-for-softsvm-de036d1001b8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstochastic-gradient-descent-implementation-for-softsvm-de036d1001b8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstochastic-gradient-descent-implementation-for-softsvm-de036d1001b8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstochastic-gradient-descent-implementation-for-softsvm-de036d1001b8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----de036d1001b8--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----de036d1001b8--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@jaimedantas?source=post_page-----de036d1001b8--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@jaimedantas?source=post_page-----de036d1001b8--------------------------------", "anchor_text": "Jaime Dantas"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6ccb66a9e501&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstochastic-gradient-descent-implementation-for-softsvm-de036d1001b8&user=Jaime+Dantas&userId=6ccb66a9e501&source=post_page-6ccb66a9e501----de036d1001b8---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fde036d1001b8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstochastic-gradient-descent-implementation-for-softsvm-de036d1001b8&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fde036d1001b8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstochastic-gradient-descent-implementation-for-softsvm-de036d1001b8&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://archive.ics.uci.edu/ml/datasets/banknote+authentication", "anchor_text": "UCI repository"}, {"url": "https://github.com/jaimedantas/SGD-SoftSVM", "anchor_text": "jaimedantas/SGD-SoftSVMThe stochastic gradient descent is used for classification in machine learning in the SoftSVM algorithm. Stochastic\u2026github.com"}, {"url": "https://github.com/jaimedantas/SGD-SoftSVM/blob/main/dataset/data_banknote_authentication.txt", "anchor_text": "data_banknote_authentication.txt"}, {"url": "https://en.wikipedia.org/wiki/Stochastic_gradient_descent", "anchor_text": "article"}, {"url": "https://en.wikipedia.org/wiki/Support_vector_machine", "anchor_text": "Support Vector Machines"}, {"url": "https://pt.wikipedia.org/wiki/Perceptron#:~:text=O%20perceptron%20%C3%A9%20um%20tipo,neural%20feedforward%3A%20um%20classificador%20linear.", "anchor_text": "Perceptron"}, {"url": "https://en.wikipedia.org/wiki/Design_matrix", "anchor_text": "design matrix"}, {"url": "https://github.com/jaimedantas/SGD-SoftSVM/blob/main/code/run_softSVM.m", "anchor_text": "run_softSVM.m"}, {"url": "https://github.com/jaimedantas/SGD-SoftSVM/blob/main/code/run_softSVM.m", "anchor_text": "run_softSVM.m"}, {"url": "http://jaimedantas.com/", "anchor_text": "website"}, {"url": "https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf", "anchor_text": "https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----de036d1001b8---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/gradient-descent?source=post_page-----de036d1001b8---------------gradient_descent-----------------", "anchor_text": "Gradient Descent"}, {"url": "https://medium.com/tag/data-science?source=post_page-----de036d1001b8---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/data?source=post_page-----de036d1001b8---------------data-----------------", "anchor_text": "Data"}, {"url": "https://medium.com/tag/algorithms?source=post_page-----de036d1001b8---------------algorithms-----------------", "anchor_text": "Algorithms"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fde036d1001b8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstochastic-gradient-descent-implementation-for-softsvm-de036d1001b8&user=Jaime+Dantas&userId=6ccb66a9e501&source=-----de036d1001b8---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fde036d1001b8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstochastic-gradient-descent-implementation-for-softsvm-de036d1001b8&user=Jaime+Dantas&userId=6ccb66a9e501&source=-----de036d1001b8---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fde036d1001b8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstochastic-gradient-descent-implementation-for-softsvm-de036d1001b8&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----de036d1001b8--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fde036d1001b8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstochastic-gradient-descent-implementation-for-softsvm-de036d1001b8&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----de036d1001b8---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----de036d1001b8--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----de036d1001b8--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----de036d1001b8--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----de036d1001b8--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----de036d1001b8--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----de036d1001b8--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----de036d1001b8--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----de036d1001b8--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@jaimedantas?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@jaimedantas?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Jaime Dantas"}, {"url": "https://medium.com/@jaimedantas/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "87 Followers"}, {"url": "http://www.jaimedantas.com", "anchor_text": "www.jaimedantas.com"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6ccb66a9e501&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstochastic-gradient-descent-implementation-for-softsvm-de036d1001b8&user=Jaime+Dantas&userId=6ccb66a9e501&source=post_page-6ccb66a9e501--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fc215e73f278c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstochastic-gradient-descent-implementation-for-softsvm-de036d1001b8&newsletterV3=6ccb66a9e501&newsletterV3Id=c215e73f278c&user=Jaime+Dantas&userId=6ccb66a9e501&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}