{"url": "https://towardsdatascience.com/a-journey-into-big-data-with-apache-spark-part-2-4511aa19a900", "time": 1682994857.354902, "path": "towardsdatascience.com/a-journey-into-big-data-with-apache-spark-part-2-4511aa19a900/", "webpage": {"metadata": {"title": "A Journey Into Big Data with Apache Spark \u2014 Part 2 | by Ash Broadley | Towards Data Science", "h1": "A Journey Into Big Data with Apache Spark \u2014 Part 2", "description": "In this episode, we\u2019ll take a look at getting an application setup in Scala and take a quick look into using Apache Spark to analyse some data."}, "outgoing_paragraph_urls": [{"url": "https://spark.apache.org", "anchor_text": "Apache Spark", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/a-journey-into-big-data-with-apache-spark-part-1-5dfcc2bccdd2", "anchor_text": "first episode", "paragraph_index": 1}, {"url": "https://medium.com/@ls12styler/a-journey-into-big-data-with-apache-spark-part-1-5dfcc2bccdd2", "anchor_text": "here", "paragraph_index": 1}, {"url": "https://github.com/ls12styler/scala-sbt/blob/master/Dockerfile", "anchor_text": "here", "paragraph_index": 4}, {"url": "https://spark.apache.org/docs/latest/quick-start.html#self-contained-applications", "anchor_text": "official Spark documentation", "paragraph_index": 9}, {"url": "https://www.gov.uk/government/publications/uk-space-agency-spending-report-october-2018", "anchor_text": "UK Space Agency spending report: October 2018", "paragraph_index": 29}], "all_paragraphs": ["Welcome back for the second part of (what I hope to be) a series of posts about getting to know Apache Spark.", "In the first episode, we learned how to create and run an Apache Spark cluster using Docker. If you\u2019ve not read that yet, you can do so here. I\u2019ll be using that cluster as a cluster to run my Spark application against, so it will be quite useful for you to have it up and running.", "I\u2019ll be using Scala to build the application because I also want to get to know that. Scala isn\u2019t something I\u2019ve used much at all, so please do bear with me while I figure some things out :). If there\u2019s a better way of doing anything I do, please do let me know \u2014 I\u2019m always open to feedback!", "This isn\u2019t going be an in depth look at how to set up and configure a Scala application, as the aim is to get up and running quickly and dive into the world of Spark. The Scala world does indeed make this easy, by offering a tool called SBT, the Scala Build Tool, and there a few things worth noting that will only help make this simpler.", "If you\u2019ve read my previous post, I allude to the fact I\u2019m a huge Docker fan and, in true fanboy style, I\u2019ll be using a Docker image that already contains Scala and SBT to aid my development. The Dockerfile I\u2019ll be using can be found here. This Dockerfile is great because it takes environment parameters at build time that installs specific versions of Scala and SBT specified by such parameters (Many thanks to original authors & contributors!). I\u2019ve added my own little twist to the Dockerfile, mainly setting the WORKDIR to /project (as that\u2019s where I mount my project directory, like my code) and adding a CMD to launch into the sbt console when we start the container. We can build the image by running the following command:", "You can see we\u2019re tagging the built image as ls12styler/scala-sbt:latest so we can simply run the image by running the following, landing us in a bash shell, in our previously configured WORKDIR:", "We can validate the installations by running scala -version and sbt sbtVersion, leading to the following output:", "In order for us to have access to our local files, we need to mount a volume from our working directory to somewhere on the running container. We can do this by simply adding the -v option to our run command. We\u2019ll remove the /bin/bash so we land right into the sbt console:", "All we\u2019ve done here is mount the pwd (present working directory) under the /project directory on the container. When running the above, we\u2019ll end up in the SBT console on that path:", "Create a new file in your project directory called build.sbt and open it with your favourite editor. Fill it with the below contents, which are actually borrowed from the official Spark documentation, although slightly tweaked:", "This gives us a minimal project definition to get started with. NOTE: We\u2019ve specified the Scala version as 2.11.12 as Spark is compiled against Scala 2.11, but the version of Scala on the container is 2.12. In the SBT console, run the reload command to refresh the SBT project with the new build settings:", "You should notice the console take on the name of our project: MyFirstScalaSpark. Now we have an environment to build our project in. Let\u2019s write some code!", "We\u2019ll follow the Spark documentation a little further just to test where we\u2019ve got to so far.", "SBT applications take the standard directory structure of a Java application, so let\u2019s create some new directories in our project directory (using the -p flag will create the directories recursively): mkdir -p ./src/main/scala/", "Create a new file called MyFirstScalaSpark.scala in the newly created directory and open it in your favourite editor. Add the following contents (again, slightly tweaked from the original):", "As we\u2019ll be running this application on the cluster that we created in Part 1, we know that the $SPARK_HOME environment variable will be set and point to the right directory on the Spark Workers. In the above code, we simply retrieve the contents of the $SPARK_HOME (which should be /spark) environment variable, interpolate it into the file path for the README.md that comes with the distribution of Spark we\u2019re using, create our Spark session and then perform a couple of MapReduce filters to count the various number of lines that contain either the letter a or b. We then output those counts to the console.", "Now that we\u2019ve got some code to actually compile, we can create a jar that we\u2019ll submit to the Spark cluster. In the SBT console, simply run package to generate the jar. You should see some output akin to the following:", "As you can see the jar has been output at /project/target/scala-2.11/myfirstscalaspark_2.11-1.0.jar within the container, which means that locally, we can find the jar in `pwd`/target/scala-2.11/.", "Now it\u2019s time to bring the cluster that we created in Part 1 back to life! Find the directory that contains the docker-compose.yml and run:", "This will bring up a Spark Master and two Spark Workers, which should be plenty to demonstrate that our first application actually works.", "In our project directory, we can use the same Docker image we created to use in the cluster as our Spark Driver. The Spark Driver is the name for the place in which we submit our applications to the Spark cluster and we can launch it by using the following command:", "In this command, we set the contents of the $SPARK_MASTER environment variable, mount the pwd under /project on the container, attach it to the Docker network we created and drop into a bash shell. To submit our application, simply submit it to the spark driver:", "When we submit the application, we specify URI to the Spark Master, the name of the class to run and the jar that class resides in. As we launched the container in our project directory, the Spark Driver container can access the jar we built, without us having to copy it around the underlying filesystem. Amidst the logs when submitting to Spark, you\u2019ll see the line we construct in the code output:", "And if we check the logs of the cluster, we\u2019ll see something like the following:", "This shows our application being registered with the Master and given an ID. Executors are then launched on each Worker and then a bunch of other stuff happens while our application is running.", "We\u2019ve successfully built our first Scala based Spark application and run it on the cluster we built in Part 1. Congratulations!", "There\u2019s a slight gotcha right now: This only currently works because we got lucky and chose a file that is available to ALL containers that use the same image we built in Part 1 (Master, Worker & Driver). If we want to be able to access a file that isn\u2019t bundled in the image, e.g. something on the host filesystem, we need to share the filesystem with the Spark Workers. This is easily achieved by mounting the volume in docker-compose when we bring up the cluster. Open the docker-compose.yml in your editor and add the following YAML at the end of the Worker service declaration:", "Save the file and bring the cluster back up. Now we have a shared directory across our Spark Workers. Next, we need to share that same directory with the Driver (the one we submit our application from). This is only really for convenience, so we can use bash autocompletion to build the file path we pass as an argument to our applicaiton. We can do that by updating our run command to include the new volume (assuming the directory you ran docker-compose up from is at the same level as your project directory):", "This container now has both our project directory and the shared data directory, accessible at /project and /local respectively.", "In the first iteration of our application, we used the read.textFile fucntion in Spark to load a README that\u2019s available to the Workers already. For this next one, we\u2019re going to use read.csv, which will load a CSV file in a way we can perform operations on. I\u2019m going to use the UK Space Agency spending report: October 2018 data for the rest of this post, which I will be putting into the directory I\u2019ve mounted under /local on the containers. To start with, we\u2019ll simply use the count method to see how many lines are in the file. We\u2019ll also pass the file path of the CSV file into the application via the command line arguments passed to the jar.", "In your editor, open the MyFirstScalaSpark.scala file and add the following code:", "We\u2019re really only adding the use of the arguments to specify the file path and using that as the file to open with Spark. Spark will then load the file into a DataFrame. We then print the number of rows in the DataFrame to the console. Run package in the SBT console again to build a new version of our application. Submit the built application, this time passing in a file path to the dataset we\u2019re using:", "Amidst the log output from Spark, you should see the number 689. A quick check on the command line shows the same:", "However, the number of lines may not be the case if we wanted a somewhat \u201ctable\u201d like representation of the data. Checking the first line of the CSV file shows the file contains column headers so we only really have 688 rows of actual data. Let\u2019s take a look at the actual structure of the data we\u2019ve loaded. In our code, we can add data.printSchema to do this:", "package and submit to see output something like the below:", "This doesn\u2019t tell us very much. There are 11 columns, all of the type string and the columns aren\u2019t named very well. When reading CSV files using Spark, we can specify an option that will use the first line as the column headings and then use the remainder as rows for the \u201ctable\u201d. We add this to our application by adding to the read line:", "Run package to build a new jar and then submit it to the cluster again. We should now see the number 688 printed in the logs and the below schema. You can see that we now have named columns instead of them being named by position:", "So we\u2019ve now got the right number of rows and a structure that has named columns \u2014 Nice! Take a closer look at the schema \u2014 all our values are of the type string. Again, this isn\u2019t very helpful. We should be working with proper types. Luckily, Spark comes with another option to try and best guess the schema of the file: inferSchema. We can add this to our code just as we did for the first option:", "package and submit and you\u2019ll get a slightly improved version of the schema:", "More notably, the Transaction Number and Amount fields are now integer and double types respectively. Everything else remains as a string, even the Date of Payment column. Let\u2019s be pedantic and get that to be of a timestamp type. Yet again, Spark comes to the rescue! We can add another option to detail the format of the column that contains the date:", "And the resulting schema should resemble the below:", "Notice the Date of Payment column is now of the timestamp type! We now have a table-like representation of our data that contains properly typed columns. Let\u2019s take a look at our data! After the data.printSchema line, insert the following:", "Package and submit the application and in the output logs, we\u2019ll see the first 20 rows displayed:", "We\u2019ve got a somewhat wide table (sorry for the formatting!), but you can hopefully see the some of the values we\u2019re going to be working with.", "Now that we have a real table-like structure for our data, we can start to perform our analysis. To begin with, let\u2019s simply order the data by the Date of Payment column, in descending order. As we\u2019re operating on a DataFrame, Spark makes lots of functions available to be able to perform such operations. We can simply use the orderBy function to do what we want. We\u2019ll also use the desc function, passing to it the name of the column we want to sort by. Any operation on a DataFrame in Spark returns a new DataFrame, so we\u2019ll assign the returned DataFrame to orderedData and then display it. We\u2019ll also limit the number of rows to output to 5, just to keep the output minimised.", "package and submit, and we should see the following output:", "Congratulations! You\u2019ve been able to load a CSV file using Apache Spark and learned how to perform a basic sort using the orderBy method with desc.", "And we\u2019re done for this edition! Come back next time when we\u2019ll be looking into using Spark to answer some more business-like questions based on the data we have.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Interests include, but not limited to: Google Cloud (Certified Arch & Data Eng), Programming, Squash, Technology, Space, Food. Views are my own."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F4511aa19a900&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-journey-into-big-data-with-apache-spark-part-2-4511aa19a900&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-journey-into-big-data-with-apache-spark-part-2-4511aa19a900&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-journey-into-big-data-with-apache-spark-part-2-4511aa19a900&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-journey-into-big-data-with-apache-spark-part-2-4511aa19a900&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----4511aa19a900--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4511aa19a900--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@ls12styler?source=post_page-----4511aa19a900--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ls12styler?source=post_page-----4511aa19a900--------------------------------", "anchor_text": "Ash Broadley"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6dcc427e3e6a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-journey-into-big-data-with-apache-spark-part-2-4511aa19a900&user=Ash+Broadley&userId=6dcc427e3e6a&source=post_page-6dcc427e3e6a----4511aa19a900---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4511aa19a900&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-journey-into-big-data-with-apache-spark-part-2-4511aa19a900&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4511aa19a900&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-journey-into-big-data-with-apache-spark-part-2-4511aa19a900&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://spark.apache.org", "anchor_text": "Apache Spark"}, {"url": "https://towardsdatascience.com/a-journey-into-big-data-with-apache-spark-part-1-5dfcc2bccdd2", "anchor_text": "first episode"}, {"url": "https://medium.com/@ls12styler/a-journey-into-big-data-with-apache-spark-part-1-5dfcc2bccdd2", "anchor_text": "here"}, {"url": "https://github.com/ls12styler/scala-sbt/blob/master/Dockerfile", "anchor_text": "here"}, {"url": "https://spark.apache.org/docs/latest/quick-start.html#self-contained-applications", "anchor_text": "official Spark documentation"}, {"url": "https://www.gov.uk/government/publications/uk-space-agency-spending-report-october-2018", "anchor_text": "UK Space Agency spending report: October 2018"}, {"url": "https://medium.com/tag/docker?source=post_page-----4511aa19a900---------------docker-----------------", "anchor_text": "Docker"}, {"url": "https://medium.com/tag/scala?source=post_page-----4511aa19a900---------------scala-----------------", "anchor_text": "Scala"}, {"url": "https://medium.com/tag/apache-spark?source=post_page-----4511aa19a900---------------apache_spark-----------------", "anchor_text": "Apache Spark"}, {"url": "https://medium.com/tag/big-data?source=post_page-----4511aa19a900---------------big_data-----------------", "anchor_text": "Big Data"}, {"url": "https://medium.com/tag/programming?source=post_page-----4511aa19a900---------------programming-----------------", "anchor_text": "Programming"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4511aa19a900&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-journey-into-big-data-with-apache-spark-part-2-4511aa19a900&user=Ash+Broadley&userId=6dcc427e3e6a&source=-----4511aa19a900---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4511aa19a900&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-journey-into-big-data-with-apache-spark-part-2-4511aa19a900&user=Ash+Broadley&userId=6dcc427e3e6a&source=-----4511aa19a900---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4511aa19a900&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-journey-into-big-data-with-apache-spark-part-2-4511aa19a900&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4511aa19a900--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F4511aa19a900&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-journey-into-big-data-with-apache-spark-part-2-4511aa19a900&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----4511aa19a900---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----4511aa19a900--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----4511aa19a900--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----4511aa19a900--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----4511aa19a900--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----4511aa19a900--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----4511aa19a900--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----4511aa19a900--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----4511aa19a900--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ls12styler?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ls12styler?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ash Broadley"}, {"url": "https://medium.com/@ls12styler/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "370 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6dcc427e3e6a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-journey-into-big-data-with-apache-spark-part-2-4511aa19a900&user=Ash+Broadley&userId=6dcc427e3e6a&source=post_page-6dcc427e3e6a--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fa575d6e4f79a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-journey-into-big-data-with-apache-spark-part-2-4511aa19a900&newsletterV3=6dcc427e3e6a&newsletterV3Id=a575d6e4f79a&user=Ash+Broadley&userId=6dcc427e3e6a&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}