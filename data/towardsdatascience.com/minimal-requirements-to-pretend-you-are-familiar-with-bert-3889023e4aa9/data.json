{"url": "https://towardsdatascience.com/minimal-requirements-to-pretend-you-are-familiar-with-bert-3889023e4aa9", "time": 1683010562.2760708, "path": "towardsdatascience.com/minimal-requirements-to-pretend-you-are-familiar-with-bert-3889023e4aa9/", "webpage": {"metadata": {"title": "Minimal Requirements to Pretend You are Familiar with BERT | by Yoshiyuki Igarashi | Towards Data Science", "h1": "Minimal Requirements to Pretend You are Familiar with BERT", "description": "Waves of hypes on data science technologies have been surging day after day. Many of you might have been tired of surfing through large and small hypes to bring a solution for a new proposal or to\u2026"}, "outgoing_paragraph_urls": [{"url": "https://huggingface.co/", "anchor_text": "open-source community", "paragraph_index": 1}, {"url": "https://towardsdatascience.com/the-fall-of-rnn-lstm-2d1594c74ce0", "anchor_text": "The fall of RNN / LSTM", "paragraph_index": 1}, {"url": "https://github.com/huggingface/transformers", "anchor_text": "Hugging Face\u2019s framework", "paragraph_index": 1}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "The Illustrated Transformer", "paragraph_index": 7}, {"url": "https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/", "anchor_text": "Turing-NLG", "paragraph_index": 8}, {"url": "https://cloud.google.com/solutions/machine-learning/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning", "anchor_text": "MLOps", "paragraph_index": 10}, {"url": "https://www.youtube.com/watch?v=XfoYk_Z5AkI&list=PLfYUBJiXbdtSIJb-Qd3pw0cqCbkGeS0xn", "anchor_text": "Deep Learning 2019 \u2014 Image Classification", "paragraph_index": 14}, {"url": "http://jalammar.github.io/illustrated-word2vec/", "anchor_text": "Illustrated Word2Vec", "paragraph_index": 17}, {"url": "https://www.youtube.com/watch?v=FKlPCK1uFrc&list=PLam9sigHPGwOBuH4_4fr-XvDbe5uneaf6", "anchor_text": "BERT Research Series", "paragraph_index": 19}, {"url": "https://www.linkedin.com/in/yoshiyuki-igarashi/", "anchor_text": "https://www.linkedin.com/in/yoshiyuki-igarashi/", "paragraph_index": 22}], "all_paragraphs": ["Waves of hypes on data science technologies have been surging day after day. Many of you might have been tired of surfing through large and small hypes to bring a solution for a new proposal or to pretend you are familiar with new AI technologies in a meeting. The targets of this article are managers without a strong data science background, junior-level data scientists, and developers who are interested in NLP, and the goal is to help them to feel comfortable with BERT with minimal knowledge and to say \u201cCan we use BERT for this project?\u201d intelligently in brainstorming sessions.", "It is sometimes a good strategy to postpone learning about new technologies in the data science domain due to hypes and biased success stories. Let\u2019s start with my short answer about \u201cnow.\u201d BERT was developed and published by Google in 2018, and after two years the open-source community around this approach has become mature and we can use an amazing toolbox developed by them. We should learn about BERT because it has alleviated the problems in the efficiency of model training unlike conventional recurrent neural networks as Culurciello discussed in The fall of RNN / LSTM. We should use BERT because it is easy to finetune and use models thanks to Hugging Face\u2019s framework, and we can switch from BERT to other state-of-art NLP models by making small modifications to your code \u2014 sometimes just a few lines of modifications.", "The official name of BERT is sort of long, and it stands for Bidirectional Encoder Representations from Transformers. Let\u2019s start with the last word, which is most important. Transformer is a type of neural network architecture, and BERT and its derivatives inherit a similar architecture. Unlike Recurrent Neural Networks, Transformers do not require the sequence of inputs to be processed in the order. It means that Transformer does not need to process the beginning of a sentence before it processes the middle or the end of the sentence. On the other hand, RNN needs to process the input in the order; thus, it creates bottlenecks. This feature gives Transformer much more freedom to run parallelization during model training.", "Then, let\u2019s go to the first word Bidirectional. If the architecture is directional it processes the input either from left-to-right or right-to-left. BERT is free from the curse of processing inputs in the order, and BERT can learn about the context of a word based on all of its surroundings. This approach is called Masked Language Model unlike a traditional approach creating a language model by predicting the next word given the words beforehand as input.", "The remaining part from BERT is Encoder Representations. Before diving into the explanation, let me ask a question. When do you think someone understands a sentence well? I think it would not be enough to understand the meanings of vocabulary in a sentence. A reasonable answer would be when someone understands the context of a sentence. Then, how can we tell whether someone understands the context or not? To me, one of the measurable, introductory approaches is whether someone understands the source of demonstrative pronouns such as \u201cit,\u201d \u201cthis\u201d or \u201cthat\u201d in a sentence. BERT includes Encoders to understand the context of words. The architecture of Encoders is identical to one another, and each one consists of two components: Self-Attention and Feed-Forward Neural Network. Out of these components, I will only focus on Self-Attention.", "Let me use a sentence \u201cThe animal didn\u2019t cross the street because it was too tired.\u201d to demonstrate how Self-Attention mechanism interprets \u201cit.\u201d We know \u201cit\u201d refers to \u201cthe animal\u201d in this sentence, and it was interpreted correctly by BERT pre-trained model as below.", "Now, we learn Self-Attention has the capability to understand the relationship between words, but a fair next question is how well it interprets. To answer this question, I have modified the original sentence to \u201cThe animal didn\u2019t cross the street because it was too wide.\u201d instead of \u201ctoo tired.\u201d As you see the image below, the strongest connection exists between \u201cthe animal\u201d and \u201cit\u201d unlike my expectation.", "Each BERT\u2019s Encoder has multiple Self-Attention heads, and my expectation was one of the Self-Attention heads would capture the relationship between \u201cit\u201d and \u201cthe street\u201d since \u201cthe street\u201d was too wide. Even though BERT has the capability to understand the context, it has not been perfect against a relatively easy sentence yet. As a disclaimer, I validated my hypothesis using a pre-trained BERT, but other BERT-derivatives would perform better because of the increase in the size of the training dataset and the improvement in its architecture. If you\u2019re interested in understanding how Self-Attention and Multi-Headed Attention mechanisms work step by step, please read The Illustrated Transformer by Jay Alammer.", "Successful applications of BERT are Text Classification, Question Answering, and Sentiment Analysis. The outcomes have not been mature enough yet, but we can also apply BERT to Named Entity Recognition and Text Summarization. Regarding Natural Language Generation, the most successful model is Turing-NLG which is a Transformer-based generative language model developed by Microsoft. If you need to tease out patterns out of texts, like emails, documents, and customer reviews, I suggest that BERT and its derivatives are worth applying at the experiment stage.", "I have listed the common applications of BERT, and BERT is definitely a great method but please try not to search for projects that one method can solve because the true value of BERT or other NLP models is to create new metadata for business units and their customers. A project like an Email classification is intuitive as an application of BERT, but the categories of emails could be the intermediate product instead of the final product for many business problems. In practice, this intermediate product would bring more value to your projects by combining it with traditional approaches.", "As an example, if you are interested in customer churn analysis, you can extract the metadata from text communication between customers and agents using BERT, such as customer sentiments, the quality of responses to customers by agents, and categories of problems. And, you can combine all or some of these metadata with other traditional metadata, such as the recency and frequency of interactions, response times by agents, and the dollar values that customers had spent. As well as the churn analysis, we can run a marketing analysis to suggest cross-sell or up-sell based on the combination of classified problems or challenges customers individually faced to enhance customer satisfaction using the metadata supplied by BERT. Of course, we can use these combinations of problems reported by customers to improve the quality/features of own products by creating hypotheses against the failures. Then, we can track the transition of the distribution of categorized problems over time to test whether the proposed hypotheses were correct. I believe we can provide higher value to business units and customers by connecting the metadata as dots in a creative manner. I would not touch in this article, but the concept of MLOps or ML Pipelines becomes more important as solutions require complicated processes in development and operation.", "Let me share the links to deepen your understanding of BERT since my post only covers the minimal concepts.", "For managers, developers and junior-level data scientists", "This is my favorite blog post, and it would be the best next step. If you don\u2019t feel comfortable with linear algebra, you can focus on the big picture by skipping the calculation of matrices in the middle of this post.", "2. Deep Learning 2019 \u2014 Image Classification by Jeremy Howard", "The series does not cover BERT, but just watching the first video would help you experience the modern framework for Deep Learning and how easy to finetune a model unlike using TensorFlow four years ago. Also, you do not need to watch all videos if you feel overwhelmed by the number of lessons since Lessons 1, 2, and 4 go through both Image Classification and NLP as minimal domains to be covered.", "For developers and junior-level data scientists", "3. Illustrated Word2Vec by Jay Alammer", "This is another great post by Jay Alammer. I had a hard time understanding the word embeddings with high dimensions, such as 256 or 512. This post has taught me about the concept of word embeddings with just two dimensions by showing many visual examples.", "4. BERT Research Series by Chris McCormick", "I prefer Jay Alammer\u2019s blogs to understand the concepts because of the creative visual representations. But, if you like to learn from videos and codes, this series would work better for you since Chris McCormick uses Jupyter Notebooks to show examples of how the concepts can be actually implemented.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "ML Engineer \u2014 9+ years\u2019 experience to deliver end-to-end solutions for real business problems. https://www.linkedin.com/in/yoshiyuki-igarashi/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F3889023e4aa9&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fminimal-requirements-to-pretend-you-are-familiar-with-bert-3889023e4aa9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fminimal-requirements-to-pretend-you-are-familiar-with-bert-3889023e4aa9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fminimal-requirements-to-pretend-you-are-familiar-with-bert-3889023e4aa9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fminimal-requirements-to-pretend-you-are-familiar-with-bert-3889023e4aa9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----3889023e4aa9--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3889023e4aa9--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://igarashi-yoshiyuki.medium.com/?source=post_page-----3889023e4aa9--------------------------------", "anchor_text": ""}, {"url": "https://igarashi-yoshiyuki.medium.com/?source=post_page-----3889023e4aa9--------------------------------", "anchor_text": "Yoshiyuki Igarashi"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe623f98d9c7c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fminimal-requirements-to-pretend-you-are-familiar-with-bert-3889023e4aa9&user=Yoshiyuki+Igarashi&userId=e623f98d9c7c&source=post_page-e623f98d9c7c----3889023e4aa9---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3889023e4aa9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fminimal-requirements-to-pretend-you-are-familiar-with-bert-3889023e4aa9&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3889023e4aa9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fminimal-requirements-to-pretend-you-are-familiar-with-bert-3889023e4aa9&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@jeremybishop", "anchor_text": "Jeremy Bishop"}, {"url": "https://unsplash.com/", "anchor_text": "Unsplash"}, {"url": "https://huggingface.co/", "anchor_text": "open-source community"}, {"url": "https://towardsdatascience.com/the-fall-of-rnn-lstm-2d1594c74ce0", "anchor_text": "The fall of RNN / LSTM"}, {"url": "https://github.com/huggingface/transformers", "anchor_text": "Hugging Face\u2019s framework"}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "The Illustrated Transformer"}, {"url": "https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb#scrollTo=OJKU36QAfqOC", "anchor_text": "Tensor2Tensor notebook"}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "The Illustrated Transformer"}, {"url": "https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/", "anchor_text": "Turing-NLG"}, {"url": "https://cloud.google.com/solutions/machine-learning/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning", "anchor_text": "MLOps"}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "The Illustrated Transformer"}, {"url": "https://www.youtube.com/watch?v=XfoYk_Z5AkI&list=PLfYUBJiXbdtSIJb-Qd3pw0cqCbkGeS0xn", "anchor_text": "Deep Learning 2019 \u2014 Image Classification"}, {"url": "http://jalammar.github.io/illustrated-word2vec/", "anchor_text": "Illustrated Word2Vec"}, {"url": "https://www.youtube.com/watch?v=FKlPCK1uFrc&list=PLam9sigHPGwOBuH4_4fr-XvDbe5uneaf6", "anchor_text": "BERT Research Series"}, {"url": "https://medium.com/tag/nlp?source=post_page-----3889023e4aa9---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/data-science?source=post_page-----3889023e4aa9---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----3889023e4aa9---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3889023e4aa9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fminimal-requirements-to-pretend-you-are-familiar-with-bert-3889023e4aa9&user=Yoshiyuki+Igarashi&userId=e623f98d9c7c&source=-----3889023e4aa9---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3889023e4aa9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fminimal-requirements-to-pretend-you-are-familiar-with-bert-3889023e4aa9&user=Yoshiyuki+Igarashi&userId=e623f98d9c7c&source=-----3889023e4aa9---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3889023e4aa9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fminimal-requirements-to-pretend-you-are-familiar-with-bert-3889023e4aa9&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3889023e4aa9--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F3889023e4aa9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fminimal-requirements-to-pretend-you-are-familiar-with-bert-3889023e4aa9&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----3889023e4aa9---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----3889023e4aa9--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----3889023e4aa9--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----3889023e4aa9--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----3889023e4aa9--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----3889023e4aa9--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----3889023e4aa9--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----3889023e4aa9--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----3889023e4aa9--------------------------------", "anchor_text": ""}, {"url": "https://igarashi-yoshiyuki.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://igarashi-yoshiyuki.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Yoshiyuki Igarashi"}, {"url": "https://igarashi-yoshiyuki.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "104 Followers"}, {"url": "https://www.linkedin.com/in/yoshiyuki-igarashi/", "anchor_text": "https://www.linkedin.com/in/yoshiyuki-igarashi/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe623f98d9c7c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fminimal-requirements-to-pretend-you-are-familiar-with-bert-3889023e4aa9&user=Yoshiyuki+Igarashi&userId=e623f98d9c7c&source=post_page-e623f98d9c7c--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F3b62919d413d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fminimal-requirements-to-pretend-you-are-familiar-with-bert-3889023e4aa9&newsletterV3=e623f98d9c7c&newsletterV3Id=3b62919d413d&user=Yoshiyuki+Igarashi&userId=e623f98d9c7c&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}