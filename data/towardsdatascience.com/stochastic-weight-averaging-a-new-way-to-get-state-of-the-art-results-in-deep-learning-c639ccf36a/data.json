{"url": "https://towardsdatascience.com/stochastic-weight-averaging-a-new-way-to-get-state-of-the-art-results-in-deep-learning-c639ccf36a", "time": 1682988345.454137, "path": "towardsdatascience.com/stochastic-weight-averaging-a-new-way-to-get-state-of-the-art-results-in-deep-learning-c639ccf36a/", "webpage": {"metadata": {"title": "Stochastic Weight Averaging \u2014 a New Way to Get State of the Art Results in Deep Learning | by Max Pechyonkin | Towards Data Science", "h1": "Stochastic Weight Averaging \u2014 a New Way to Get State of the Art Results in Deep Learning", "description": "Update: you can now enjoy this post on my personal blog, where math typography is much better (Medium doesn\u2019t support math rendering, despite numerous requests). In this article, I will discuss two\u2026"}, "outgoing_paragraph_urls": [{"url": "https://pechyonkin.me/stochastic-weight-averaging/", "anchor_text": "on my personal blog", "paragraph_index": 0}, {"url": "http://blog.kaggle.com/2017/10/17/planet-understanding-the-amazon-from-space-1st-place-winners-interview/", "anchor_text": "Kaggle-winning machine learning practitioners", "paragraph_index": 3}, {"url": "https://arxiv.org/abs/1704.00109", "anchor_text": "snapshot ensembling paper", "paragraph_index": 5}, {"url": "https://techburst.io/improving-the-way-we-work-with-learning-rate-5e99554f163b", "anchor_text": "this awesome post", "paragraph_index": 6}, {"url": "https://arxiv.org/abs/1412.0233", "anchor_text": "such surfaces have many local optima", "paragraph_index": 12}, {"url": "https://www.coursera.org/learn/neural-networks/lecture/sPEhK/a-geometrical-view-of-perceptrons-6-min", "anchor_text": "source", "paragraph_index": 13}, {"url": "https://arxiv.org/abs/1802.10026", "anchor_text": "Fast geometric ensembling", "paragraph_index": 19}, {"url": "https://arxiv.org/abs/1803.05407", "anchor_text": "Stochastic weight averaging", "paragraph_index": 21}, {"url": "https://github.com/timgaripov/swa", "anchor_text": "their own implementation", "paragraph_index": 25}, {"url": "https://github.com/fastai/fastai/pull/276/commits", "anchor_text": "awesome fast.ai library", "paragraph_index": 26}, {"url": "http://www.fast.ai/", "anchor_text": "follow", "paragraph_index": 26}, {"url": "http://course.fast.ai/", "anchor_text": "the", "paragraph_index": 26}, {"url": "https://github.com/fastai/fastai", "anchor_text": "links", "paragraph_index": 26}, {"url": "https://twitter.com/max_pechyonkin", "anchor_text": "follow me", "paragraph_index": 27}, {"url": "https://www.linkedin.com/in/maxim-pechyonkin-phd/", "anchor_text": "LinkedIn", "paragraph_index": 27}], "all_paragraphs": ["Update: you can now enjoy this post on my personal blog, where math typography is much better (Medium doesn\u2019t support math rendering, despite numerous requests).", "In this article, I will discuss two interesting recent papers that provide an easy way to improve performance of any given neural network by using a smart way to ensemble. They are:", "Additional prerequisite reading that will make context of this post much more easy to understand:", "Traditional ensembling combines several different models and makes them predict on the same input. Then some way of averaging is used to determine the final prediction of the ensemble. It can be simple voting, an average or even another model that learns to predict correct value or label based on the inputs of models in the ensemble. Ridge regression is one particular way of combining several predictions which is used by Kaggle-winning machine learning practitioners.", "When applied in deep learning, ensembling can be used to combine predictions of several neural networks to produce one final prediction. Usually it is a good idea to use neural networks of different architectures in an ensemble, because they will likely make mistakes on different training samples and therefore the benefit of ensembling will be larger.", "However, you can also ensemble models with the same architecture and it will give surprisingly good results. One very cool trick exploiting this approach was proposed in the snapshot ensembling paper. The authors take weights snapshot while training the same network and then after training create an ensemble of nets with the same architecture but different weights. This allows to improve test performance, and it is a very cheap way too because you just train one model once, just saving weights from time to time.", "You can refer to this awesome post for more details. If you aren\u2019t yet using cyclical learning rates, then you definitely should, as it becomes the standard state-of-the art training technique that is very simple, not computationally heavy and provides significant gains at almost no additional cost.", "All of the examples above are ensembles in the model space, because they combine several models and then use models\u2019 predictions to produce the final prediction.", "In the paper that I am discussing in this post, however, the authors propose to use a novel ensembling in the weights space. This method produces an ensemble by combining weights of the same network at different stages of training and then uses this model with combined weights to make predictions. There are 2 benefits from this approach:", "Let\u2019s see how it works. But first we need to understand some important facts about loss surfaces and generalizable solutions.", "The first important insight is that a trained network is a point in multidimensional weight space. For a given architecture, each distinct combination of network weights produces a separate model. Since there are infinitely many combinations of weights for any given architecture, there will be infinitely many solutions. The goal of training of a neural network is to find a particular solution (point in the weight space) that will provide low value of the loss function both on training and testing data sets.", "During training, by changing weights, training algorithm changes the network and travel in the weight space. Gradient descent algorithm travels on a loss plane in this space where plane elevation is given by the value of the loss function.", "It is very hard to visualize and understand the geometry of multidimensional weight space. At the same time, it is very important to understand it because stochastic gradient descent essentially traverses a loss surface in this highly multidimensional space during training and tries to find a good solution \u2014 a \u201cpoint\u201d on the loss surface where loss value is low. It is known that such surfaces have many local optima. But it turns out that not all of them are equally good.", "Hinton: \u201cTo deal with hyper-planes in a 14-dimensional space, visualize a 3-D space and say \u201cfourteen\u201d to yourself very loudly. Everyone does it.\u201d (source)", "One metric that can distinguish a good solution from a bad one is its flatness. The idea being that training data set and testing data set will produce similar but not exactly the same loss surfaces. You can imagine that a test surface will be shifted a bit relative to the train surface. For a narrow solution, during test time, a point that gave low loss can have a large loss because of this shift. This means that this \u201cnarrow\u201d solution did not generalize well \u2014 training loss is low, while testing loss is large. On the other hand, for a \u201cwide\u201d and flat solution, this shift will lead to training and testing loss being close to each other.", "I explained the difference between narrow and wide solutions because the new method which is the focus of this post leads to nice and wide solutions.", "Initially, SGD will make a big jump in the weight space. Then, as the learning rate gets smaller due to cosine annealing, SGD will converge to some local solution and the algorithm will take a \u201csnapshot\u201d of the model by adding it to the ensemble. Then the rate is reset to high value again and SGD takes a large jump again before converging to some different local solution.", "Cycle length in the snapshot ensembling approach is 20 to 40 epochs. The idea of long learning rate cycles is to be able to find sufficiently different models in the weight space. If the models are too similar, then predictions of the separate networks in the ensemble will be too close and the benefit of ensembling will be negligible.", "Snapshot ensembling works really well and improves model performance, but Fast Geometric Ensembling works even better.", "Fast geometric ensembling is very similar to snapshot ensembling, but is has some distinguishing features. It uses linear piecewise cyclical learning rate schedule instead of cosine. Secondly, the cycle length in FGE is much shorter \u2014 only 2 to 4 epochs per cycle. At first intuition, the short cycle is wrong because the models at the end of each cycle will be close to each other and therefore ensembling them will not give any benefits. However, as the authors discovered, because there exist connected paths of low loss between sufficiently different models, it is possible to travel along those paths in small steps and the models encountered along will be different enough to allow ensembling them with good results. Thus, FGE shows improvement compared to snapshot ensembles and it takes smaller steps to find the model (which makes it faster to train).", "To benefit from both snapshot ensembling or FGE, one needs to store multiple models and then make predictions for all of them before averaging for the final prediction. Thus, for additional performance of the ensemble, one needs to pay with higher amount of computation. So there is no free lunch there. Or is there? This is where the new paper with stochastic weight averaging comes in.", "Stochastic weight averaging closely approximates fast geometric ensembling but at a fraction of computational loss. SWA can be applied to any architecture and data set and shows good result in all of them. The paper suggests that SWA leads to wider minima, the benefits of which I discussed above. SWA is not an ensemble in its classical understanding. At the end of training you get one model, but it\u2019s performance beats snapshot ensembles and approaches FGE.", "Intuition for SWA comes from empirical observation that local minima at the end of each learning rate cycle tend to accumulate at the border of areas on loss surface where loss value is low (points W1, W2 and W3 are at the border of the red area of low loss in the left panel of figure above). By taking the average of several such points, it is possible to achieve a wide, generalizable solution with even lower loss (Wswa in the left panel of the figure above).", "Here is how it works. Instead of an ensemble of many models, you only need two models:", "At the end of each learning rate cycle, the current weights of the second model will be used to update the weight of the running average model by taking weighted mean between the old running average weights and the new set of weights from the second model (formula provided in the figure on the left). By following this approach, you only need to train one model, and store only two models in memory during training. For prediction, you only need the running average model and predicting on it is much faster than using ensemble described above, where you use many models to predict and then average results.", "Authors of the paper provide their own implementation in PyTorch.", "Also, SWA is implemented in the awesome fast.ai library that everyone should be using. And if you haven\u2019t yet seen their course, then follow the links.", "You can follow me on Twitter. Let\u2019s also connect on LinkedIn.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fc639ccf36a&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstochastic-weight-averaging-a-new-way-to-get-state-of-the-art-results-in-deep-learning-c639ccf36a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstochastic-weight-averaging-a-new-way-to-get-state-of-the-art-results-in-deep-learning-c639ccf36a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstochastic-weight-averaging-a-new-way-to-get-state-of-the-art-results-in-deep-learning-c639ccf36a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstochastic-weight-averaging-a-new-way-to-get-state-of-the-art-results-in-deep-learning-c639ccf36a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----c639ccf36a--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c639ccf36a--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@pechyonkin?source=post_page-----c639ccf36a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@pechyonkin?source=post_page-----c639ccf36a--------------------------------", "anchor_text": "Max Pechyonkin"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F38025716cba8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstochastic-weight-averaging-a-new-way-to-get-state-of-the-art-results-in-deep-learning-c639ccf36a&user=Max+Pechyonkin&userId=38025716cba8&source=post_page-38025716cba8----c639ccf36a---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc639ccf36a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstochastic-weight-averaging-a-new-way-to-get-state-of-the-art-results-in-deep-learning-c639ccf36a&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc639ccf36a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstochastic-weight-averaging-a-new-way-to-get-state-of-the-art-results-in-deep-learning-c639ccf36a&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://pechyonkin.me/stochastic-weight-averaging/", "anchor_text": "on my personal blog"}, {"url": "https://arxiv.org/abs/1802.10026", "anchor_text": "Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs"}, {"url": "https://arxiv.org/abs/1803.05407", "anchor_text": "Averaging Weights Leads to Wider Optima and Better Generalization"}, {"url": "https://techburst.io/improving-the-way-we-work-with-learning-rate-5e99554f163b", "anchor_text": "Improving the way we work with learning rate"}, {"url": "http://blog.kaggle.com/2017/10/17/planet-understanding-the-amazon-from-space-1st-place-winners-interview/", "anchor_text": "Kaggle-winning machine learning practitioners"}, {"url": "https://arxiv.org/abs/1704.00109", "anchor_text": "Source"}, {"url": "https://techburst.io/improving-the-way-we-work-with-learning-rate-5e99554f163b", "anchor_text": "Source"}, {"url": "https://arxiv.org/abs/1704.00109", "anchor_text": "snapshot ensembling paper"}, {"url": "https://techburst.io/improving-the-way-we-work-with-learning-rate-5e99554f163b", "anchor_text": "this awesome post"}, {"url": "https://arxiv.org/abs/1412.0233", "anchor_text": "such surfaces have many local optima"}, {"url": "https://www.coursera.org/learn/neural-networks/lecture/sPEhK/a-geometrical-view-of-perceptrons-6-min", "anchor_text": "source"}, {"url": "https://arxiv.org/abs/1609.04836", "anchor_text": "Source"}, {"url": "https://arxiv.org/abs/1802.10026", "anchor_text": "Fast geometric ensembling"}, {"url": "https://arxiv.org/abs/1802.10026", "anchor_text": "Source"}, {"url": "https://arxiv.org/abs/1803.05407", "anchor_text": "Stochastic weight averaging"}, {"url": "https://arxiv.org/abs/1803.05407", "anchor_text": "Source"}, {"url": "https://arxiv.org/abs/1803.05407", "anchor_text": "Source"}, {"url": "https://github.com/timgaripov/swa", "anchor_text": "their own implementation"}, {"url": "https://github.com/fastai/fastai/pull/276/commits", "anchor_text": "awesome fast.ai library"}, {"url": "http://www.fast.ai/", "anchor_text": "follow"}, {"url": "http://course.fast.ai/", "anchor_text": "the"}, {"url": "https://github.com/fastai/fastai", "anchor_text": "links"}, {"url": "https://pechyonkin.me/subscribe/", "anchor_text": "subscribe"}, {"url": "https://pechyonkin.me/", "anchor_text": "my website"}, {"url": "https://twitter.com/max_pechyonkin", "anchor_text": "follow me"}, {"url": "https://www.linkedin.com/in/maxim-pechyonkin-phd/", "anchor_text": "LinkedIn"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----c639ccf36a---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----c639ccf36a---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----c639ccf36a---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----c639ccf36a---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/geoffrey-hinton?source=post_page-----c639ccf36a---------------geoffrey_hinton-----------------", "anchor_text": "Geoffrey Hinton"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc639ccf36a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstochastic-weight-averaging-a-new-way-to-get-state-of-the-art-results-in-deep-learning-c639ccf36a&user=Max+Pechyonkin&userId=38025716cba8&source=-----c639ccf36a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc639ccf36a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstochastic-weight-averaging-a-new-way-to-get-state-of-the-art-results-in-deep-learning-c639ccf36a&user=Max+Pechyonkin&userId=38025716cba8&source=-----c639ccf36a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc639ccf36a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstochastic-weight-averaging-a-new-way-to-get-state-of-the-art-results-in-deep-learning-c639ccf36a&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c639ccf36a--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fc639ccf36a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstochastic-weight-averaging-a-new-way-to-get-state-of-the-art-results-in-deep-learning-c639ccf36a&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----c639ccf36a---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----c639ccf36a--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----c639ccf36a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----c639ccf36a--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----c639ccf36a--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----c639ccf36a--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----c639ccf36a--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----c639ccf36a--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----c639ccf36a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@pechyonkin?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@pechyonkin?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Max Pechyonkin"}, {"url": "https://medium.com/@pechyonkin/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "6.7K Followers"}, {"url": "https://pechyonkin.me/", "anchor_text": "https://pechyonkin.me/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F38025716cba8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstochastic-weight-averaging-a-new-way-to-get-state-of-the-art-results-in-deep-learning-c639ccf36a&user=Max+Pechyonkin&userId=38025716cba8&source=post_page-38025716cba8--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F606fbade266f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstochastic-weight-averaging-a-new-way-to-get-state-of-the-art-results-in-deep-learning-c639ccf36a&newsletterV3=38025716cba8&newsletterV3Id=606fbade266f&user=Max+Pechyonkin&userId=38025716cba8&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}