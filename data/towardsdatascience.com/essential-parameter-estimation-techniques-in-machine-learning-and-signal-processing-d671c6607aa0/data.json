{"url": "https://towardsdatascience.com/essential-parameter-estimation-techniques-in-machine-learning-and-signal-processing-d671c6607aa0", "time": 1683017972.472749, "path": "towardsdatascience.com/essential-parameter-estimation-techniques-in-machine-learning-and-signal-processing-d671c6607aa0/", "webpage": {"metadata": {"title": "Essential Parameter Estimation Techniques in Machine Learning, Data Science, and Signal Processing | by MANIE TADAYON | Towards Data Science", "h1": "Essential Parameter Estimation Techniques in Machine Learning, Data Science, and Signal Processing", "description": "Parameter estimation plays a vital role in machine learning, statistics, communication system, radar, and many other domains. For example, in a digital communication system, you sometimes need to\u2026"}, "outgoing_paragraph_urls": [], "all_paragraphs": ["Parameter estimation plays a vital role in machine learning, statistics, communication system, radar, and many other domains. For example, in a digital communication system, you sometimes need to estimate the parameters of the fading channel, the variance of AWGN (additive white Gaussian noise) noise, IQ (in-phase, quadrature) imbalance parameters, frequency offset, etc. In machine learning and statistics, you constantly need to estimate and learn the parameters of the probability distributions. For example, in Bayesian and causal networks, this corresponds to estimating the CPT (conditional probability table) for discrete nodes and the mean and the variance for the continuous nodes.In this article, I will discuss essential parameter estimation techniques used widely in machine learning, AI, signal processing, and digital communication.", "Following is the outline for this article:", "Frequentists and Bayesian are two well-known schools of thought in statistics. They have different approaches on how to define statistical concepts such as probability and how to perform parameter estimation.", "Frequentists define probability as a relative frequency of an event in the long run, while Bayesians define probability as a measure of uncertainty and belief for any event.", "Furthermore, the frequentists assume the parameter \u03b8 in a population is fixed and unknown. They only use data to construct the likelihood function to estimate the unknown parameter. Bayesians, on the other hand, consider the parameter \u03b8 to be a random variable with an unknown distribution. They use both the prior probability and data to construct the posterior distribution.", "To better understand the difference between these two, consider the well-known Baye\u2019s law:", "P(\u03b8) is the prior belief you have about the parameter before collecting (observing) any data, P(X|\u03b8) is the likelihood function (probability of observing the data given the parameter), P(\u03b8|X) is the posterior distribution (belief about the parameter \u03b8 after you observe the data), and P(X) is the probability of data. The Bayesian approach is more computationally intensive than the frequentists approach, thanks to the denominator of the Bayes law. This is because that integral is usually done in high dimensional, and it may not either have a closed-form solution or be very complex to compute. Another problem with the Bayesian approach is the subjective prior (P(\u03b8)) since, in most problems in the real world, one has no idea what would be the best prior belief. However, the Bayesian approach lets you incorporate the prior belief into your model, which could be beneficial if, for example, due to the domain knowledge, you have a good model for the prior probability.", "ML estimation tries to find the estimate of the parameter \u03b8 by maximizing the likelihood function.", "Assume we have i.i.d random samples x\u2081,x\u2082, . . .,x\u2099 that follow a distribution f(x\u2081,x\u2082, . . .,x\u2099;\u03b8), which depends on the unknown parameter \u03b8. The goal is to estimate this unknown quantity such that it maximizes the probability of observing this random sample. Following is how you formulate this problem using the maximum likelihood:", "3. Take a logarithm (usually a natural log) to change the product to summation. This does not change the optimal estimator since the logarithm is a monotonic function.", "Where LL(\u03b8;x) represents the log-likelihood function.", "4. Differentiate the log-likelihood function with respect to \u03b8 and set it to zero.", "5. The estimator will only be a function of observed data.", "MAP estimation tries to find the estimate of the parameter \u03b8 by maximizing the posterior distribution. Recall the Bayes law again but this time we are not trying to compute the exact value of posterior. This is important since we do not need to worry about the denominator because it is independent of the parameter. Therefore, all is needed is to maximize the product of likelihood and the prior probability.", "Remember, P(X|\u03b8) is the same as the likelihood function. Therefore, the MAP estimate is the same as the ML estimate with the inclusion of the prior probability. Following is how you formulate the problem using the MAP:", "Note: Slight abuse of notation to use letter P for the density function.", "3. Take the logarithm (Usually a natural log) to further simplify the above relationship.", "4. Differentiate the above equation with respect to \u03b8 and set it to zero", "Note: MAP and ML estimate is the same if \u03b8 follows the uniform distribution. What this means intuitively is that all values of \u03b8 have equal weight; therefore, knowing the distribution of \u03b8 does not give us any more useful distribution.", "Example 1: Consider a communication system that the transmitted signal X ~ N(0,\u03c3\u2093\u00b2) (Gaussian distribution with zero mean and variance of \u03c3\u2093\u00b2). The received signal Y can be modeled as follows:", "Where n ~ N(0,\u03c3\u2099\u00b2). We would like to find the MAP and ML estimates for transmitted signal X.", "Y is a received or observed message. You can think of Y as a noisy version of X. This is a standard problem in the communication system. We never know what message is transmitted. (otherwise, there is no point in designing the receiver, error correction codes, etc.) To find the ML estimate, we need to follow the steps outlined above. First, construct the likelihood function: P(Y|X=x). Then find the log-likelihood expression and then differentiate respect to x and set it to zero.", "C\u2081 denotes the constant terms that do not depend on x (We do not care about them since they are zero after differentiation).", "Interpretation: The ML estimate of x is the observed message y. This means under the maximum likelihood estimation, the best estimate for the transmitted signal is the received noisy signal.", "First, we need to construct the posterior distribution by multiplying the likelihood and prior together (Remember, the denominator is not important since it is not a function of the parameter).", "The maximum value of the posterior occurs when the exponent is minimized. Differentiating respect to x and set it to zero will result in the following:", "Interpretation: The MAP estimate of x is linearly proportional to the received signal y. If the variance of signal is infinity (becomes very large), then the normal distribution becomes the uniform distribution, and the MAP and ML estimate becomes the same.", "To understand this concept better, let's look at the simulation of the estimated x under ML and MAP as the variance of the transmitted signal is changing while the variance of the noise is constant at \u03c3\u2099\u00b2 = 10.", "As the above table and figure show as the variance of the signal increases, the Gaussian distribution becomes more similar to a uniform distribution, and ML and MAP estimates become closer to each other. For example, when the signal variance is 10 times noise variance (The red curve) the ML and MAP estimates are almost identical.", "MMSE is one of the most well-known estimation techniques used widely in machine learning and signal processing. For example, Kalman and Wiener filters are both examples of MMSE estimation.", "In MMSE the objective is to minimize the expected value of residual square, where residual is the difference between the true value and the estimated value. The expected residual square is also known as MSE (Mean Square Error)", "Following is a procedure to solve any MMSE estimation:", "2. Construct the MSE (Expected residual square).", "3. Differentiate the MSE with respect to the parameter and set it to zero.", "4. Plug the MMSE estimator in part 3 in the MSE expression to find the minimum residual square.", "Example 2: For the first example, we would like to find the estimate and the MSE of the random variable X using a constant y.", "Interpretation: The best constant estimator of X is the expected value of X (\u03bc). The minimum MSE using the optimal estimator is the variance of X.", "Most of the time, we are interested in finding the MMSE estimator after observing some data or evidence. For example, assume we are interested in finding the best estimator for random variable Y after observing random variable X. It can be shown that the MMSE estimator in this case is:", "Recall that E[Y|X=x] is a function of x and in general, it is nonlinear and can be very complex. Therefore, in practice, we mainly consider the class of linear MMSE estimators like the following:", "To find the optimal a\u2081, a\u2082 and the MSE follow steps 1\u20134 outlined above.", "Now there are two equations and two unknowns, which can be solved in many different ways. For example, we can formulate the problem as follows:", "Solving the above matrix equation results in the following estimates for the coefficients:", "Substituting the optimal coefficients in the E[\u03f5\u00b2] expression will result in the optimal (minimum) MSE:", "Interpretation: If X and Y are independent, then the covariance between them is zero and so is the estimate of a\u2081 and the problem is changed to estimating a random variable with a constant (a\u2082). The above results justify this since the estimate of a\u2082 = E[Y] and E[\u03f5\u00b2] = var(Y) (Same results as estimating a random variable with a constant). \u03c1 is a correlation coefficient between X and Y and can vary between -1 and 1. If two random variables are strongly correlated (either +1 or -1) then the MSE is zero, which means one variable can perfectly estimate another one.", "The orthogonality principle states that the estimator and residual are perpendicular to each other.", "In the above figure X hat is an estimator of the X and X tilde is the residual. Mathematically this can be represented as follows:", "For example, applying the orthogonality principle to the linear MMSE estimator results in:", "This is the exact expression as the derivative of MSE with respect to a\u2081.", "Note: The residual is perpendicular to the plane containing the estimator therefore it is perpendicular to every vector in that plane. This is the reason why in the above equation we can replace a\u2081X + a\u2082 with X. However it is also valid to use a\u2081X + a\u2082 instead of X.", "Least square (LS) estimation is the most common estimation techniques used in communication systems to estimate the channel response, machine learning as a loss or cost function for regression and classification problems, and optimization to find the best line or hyperplane that fits data the best. Regardless of the application, the steps to solve the least square problem are roughly identical. The best way to explain these steps is through an example.", "Example 3: Consider you are given a dataset (x\u2096, y\u2096) k = 1,2, . . . n and you are asked to find the line that best describes the relationship between y\u2096 and x\u2096.", "Following are the steps to formulate the least square problem:", "But wait a minute this is exactly the problem formulation for the MMSE discussed in detail in the last section.", "3. Differentiate the objective function with respect to a and b and set it to zero.", "This is the same result as a linear MMSE. Therefore", "The above figure shows data and the least square fitted line calculated using the estimates of a and b.", "Note: The least-square estimator is a special case of the MMSE (namely the linear MMSE).", "Now, what if X is an m-dimensional, Y is an n-dimensional vector. Then A is m by n matrix. The least-square approximation is concerned with finding the best solution for Y = AX. This is an important problem since sometimes Y is not in column space of A and therefore Y =AX does not have a solution which means we are trying to find the solution that minimizes the norm of the difference between Y and AX.", "Note: The following relationships hold true:", "Recall, the transpose of a scalar quantity is itself and this is the reason why the last relationship is true.", "Now to find the optimal X, we need to differentiate J with respect to X and set it to zero.", "Note: If the product of A transpose and A is invertible then we expect the estimator to be unique and it is called the least square solution. If the inverse does not exist then we can replace it with the pseudoinverse. The last equation above to get the estimate of X is also known as the normal equation.", "In Bayesian estimation, the parameter \u03b8 is modeled as a random variable (Recall Bayesian vs Frequentists section in this article) with a certain probability distribution. The MMSE, LAE (Least Absolute Error), and MAP are all special types of Bayes estimators.", "Define the cost or loss function C as a cost of choosing the estimator instead of the true parameter (Think of it as how much you lose if you use the estimator instead of the true parameter). In the Bayes estimation, we minimize the expected loss function given observed data x. This can be defined mathematically as follows:", "P(\u03b8|x) is the posterior distribution (Refer to the first section in this article) and can be calculated using the Bayes law.", "The procedure to solve the Bayes estimation problems is as follows:", "The cost function can take many different forms however the most well-known cost functions are the quadratic and the absolute functions. For the rest of this section, we will derive the Bayes estimator for the quadratic, absolute, and 0\u20131 cost functions.", "If the cost function is quadratic, C is replaced by a quadratic function and follow the procedure outlined above.", "Interpretation: The estimator is the conditional expectation of the parameter given the data or the posterior mean which is the identical result as the MMSE. Therefore the MMSE is the Bayes estimator when the cost function is quadratic.", "In this case, C is replaced by the absolute error function. The estimator is calculated similarly:", "Therefore, for the posterior distribution, the integral from -\u221e to \u03b8 is equal to the integral from \u221e to \u03b8. However, we also know that the integral over the entire domain would be 1 (Maximum value of probability is 1). Therefore:", "Interpretation: The best estimator under the absolute error cost function is the median of the posterior distribution. This is what one-half represents in the above equation. The estimator under this cost function is known as LAE (Least Absolute Error) estimators.", "In this case, C is 1 in some interval and zero otherwise.", "Interpretation: Minimizing J is equivalent to maximizing the posterior distribution. Therefore the estimator of 0\u20131 loss function is the mode of the posterior distribution (The value of \u03b8 that maximizes the posterior distribution). This is exactly what MAP estimation does. This means MAP estimator is a Bayes estimator when the cost function is 0\u20131.", "Estimators possess some properties that distinguish them from each other.", "An estimator is said to be an unbiased estimator of parameter \u03b8 if its expected value is equal to \u03b8. Mathematically this is represented as follows:", "The bias of an estimator is defined to be:", "it is sometimes referred to as CRLB (Cramer-Rao lower bound) and is the lower bound for a variance of an estimator. The lower is the variance of an estimator the more certain one can be about the range of possible values it can take. CRLB is calculated as follows:", "Intuition: Following curves represent the Gaussian density functions with the mean 0 and different variances. As these curves represent the lower the variance is the narrower is the density functions and the higher is the confidence interval, hence the estimate of the parameter is more accurate.", "All the above curves have negative curvatures, which means they all have negative slopes at any point on the curve. However, the green curve (\u03c3\u00b2 = 1) has a much sharper rate of change of the slope compared to the blue and red curves. This is the intuition on why there is a second derivative with respect to \u03b8 in the CRLB formulation.", "An estimator is said to be consistent if it converges to the true parameter in distribution as the number of samples approaches infinity.", "Intuitively consistency implies that the estimator gets more concentrated around the \u03b8 as the sample size increases.", "Bias-variance trade-off is one of the most well-known concepts used in machine learning and statistics. The idea is that you can express the MSE as the sum of bias square and the variance.", "Therefore for a fixed MSE lowering the bias leads to increasing the variance, hence there is a trade-off between these two quantities.", "If an estimator is unbiased and has the lowest variance among all other unbiased estimators of \u03b8 then it is called MVUE. According to the Bias-Variance trade-off for the MVUE the MSE is equal to the variance of the estimator since the bias is zero.", "Example 4: Consider x\u2081,x\u2082, . . ., x\u2099 are n i.i.d random samples drawn from a normal distribution with mean \u03bc and \u03c3\u00b2 variance. Find the maximum likelihood estimate of \u03bc and verify if the estimator is unbiased, consistent, and satisfies the CRLB?", "Follow the steps outline in the ML section:", "Therefore the ML estimate of \u03bc is the sample mean. Now we need to compute the bias and CRLB as well as check if the estimator is consistent.", "Therefore the sample mean is an unbiased estimator.", "To check the consistency we need to find the distribution of the sample mean.", "Therefore the sample mean is normally distributed with the mean \u03bc and variance of \u03c3\u00b2 divided by n. Now to check for consistency, we let n goes to infinity which causes the variance to go to zero hence, the sample mean converges to a constant which is \u03bc.", "To find the CRLB proceed as follows:", "Based on the above maximum likelihood calculation, we already have the result for the first derivative of log-likelihood (Denoted by LL).", "Therefore the CRLB and variance are both the same. This means that the sample mean has the lowest variance among all the unbiased estimators of the \u03bc. Since the sample mean is unbiased and achieves the CRLB, then it is MVUE.", "According to the bias-variance trade-off since the bias is zero then MSE is equal to CRLB (or the variance in this case).", "In this article, I discussed the difference between the Bayesian and the Frequentists approaches. I discussed some well-known estimation mechanism used widely in machine learning and signal processing. Furthermore, I examined some important properties of estimators.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Applied/Research scientist at Amazon. Ph.D. in Electrical and Computer Engineering at UCLA. Expert in time series, machine learning, and causal inference."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fd671c6607aa0&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fessential-parameter-estimation-techniques-in-machine-learning-and-signal-processing-d671c6607aa0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fessential-parameter-estimation-techniques-in-machine-learning-and-signal-processing-d671c6607aa0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fessential-parameter-estimation-techniques-in-machine-learning-and-signal-processing-d671c6607aa0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fessential-parameter-estimation-techniques-in-machine-learning-and-signal-processing-d671c6607aa0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----d671c6607aa0--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d671c6607aa0--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://manitadayon.medium.com/?source=post_page-----d671c6607aa0--------------------------------", "anchor_text": ""}, {"url": "https://manitadayon.medium.com/?source=post_page-----d671c6607aa0--------------------------------", "anchor_text": "MANIE TADAYON"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2674819db812&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fessential-parameter-estimation-techniques-in-machine-learning-and-signal-processing-d671c6607aa0&user=MANIE+TADAYON&userId=2674819db812&source=post_page-2674819db812----d671c6607aa0---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd671c6607aa0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fessential-parameter-estimation-techniques-in-machine-learning-and-signal-processing-d671c6607aa0&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd671c6607aa0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fessential-parameter-estimation-techniques-in-machine-learning-and-signal-processing-d671c6607aa0&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/tagged/getting-started", "anchor_text": "Getting Started"}, {"url": "https://unsplash.com/@josilito?utm_source=medium&utm_medium=referral", "anchor_text": "Jose Llamas"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://unsplash.com/@cantusamator?utm_source=medium&utm_medium=referral", "anchor_text": "Nature Uninterrupted Photography"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://unsplash.com/@ryanhafey?utm_source=medium&utm_medium=referral", "anchor_text": "Ryan Hafey"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://unsplash.com/@jonflobrant?utm_source=medium&utm_medium=referral", "anchor_text": "Jon Flobrant"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://unsplash.com/@jeisblack?utm_source=medium&utm_medium=referral", "anchor_text": "Jason Blackeye"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "http://fourier.eng.hmc.edu/e59/lectures/signalsystem/node9.html", "anchor_text": "http://fourier.eng.hmc.edu/e59/lectures/signalsystem/node9.html"}, {"url": "https://unsplash.com/@miklevasilyev?utm_source=medium&utm_medium=referral", "anchor_text": "MIKHAIL VASILYEV"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://unsplash.com/@gillystewart?utm_source=medium&utm_medium=referral", "anchor_text": "Gilly Stewart"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://unsplash.com/@v2osk?utm_source=medium&utm_medium=referral", "anchor_text": "v2osk"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----d671c6607aa0---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/signal-processing?source=post_page-----d671c6607aa0---------------signal_processing-----------------", "anchor_text": "Signal Processing"}, {"url": "https://medium.com/tag/bayesian-statistics?source=post_page-----d671c6607aa0---------------bayesian_statistics-----------------", "anchor_text": "Bayesian Statistics"}, {"url": "https://medium.com/tag/editors-pick?source=post_page-----d671c6607aa0---------------editors_pick-----------------", "anchor_text": "Editors Pick"}, {"url": "https://medium.com/tag/getting-started?source=post_page-----d671c6607aa0---------------getting_started-----------------", "anchor_text": "Getting Started"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd671c6607aa0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fessential-parameter-estimation-techniques-in-machine-learning-and-signal-processing-d671c6607aa0&user=MANIE+TADAYON&userId=2674819db812&source=-----d671c6607aa0---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd671c6607aa0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fessential-parameter-estimation-techniques-in-machine-learning-and-signal-processing-d671c6607aa0&user=MANIE+TADAYON&userId=2674819db812&source=-----d671c6607aa0---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd671c6607aa0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fessential-parameter-estimation-techniques-in-machine-learning-and-signal-processing-d671c6607aa0&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d671c6607aa0--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fd671c6607aa0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fessential-parameter-estimation-techniques-in-machine-learning-and-signal-processing-d671c6607aa0&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----d671c6607aa0---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----d671c6607aa0--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----d671c6607aa0--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----d671c6607aa0--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----d671c6607aa0--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----d671c6607aa0--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----d671c6607aa0--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----d671c6607aa0--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----d671c6607aa0--------------------------------", "anchor_text": ""}, {"url": "https://manitadayon.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://manitadayon.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "MANIE TADAYON"}, {"url": "https://manitadayon.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "101 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2674819db812&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fessential-parameter-estimation-techniques-in-machine-learning-and-signal-processing-d671c6607aa0&user=MANIE+TADAYON&userId=2674819db812&source=post_page-2674819db812--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F85f3c71785b9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fessential-parameter-estimation-techniques-in-machine-learning-and-signal-processing-d671c6607aa0&newsletterV3=2674819db812&newsletterV3Id=85f3c71785b9&user=MANIE+TADAYON&userId=2674819db812&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}