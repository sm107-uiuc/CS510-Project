{"url": "https://towardsdatascience.com/multinomial-mixture-model-for-supermarket-shoppers-segmentation-a-complete-tutorial-268974d905da", "time": 1683015163.330282, "path": "towardsdatascience.com/multinomial-mixture-model-for-supermarket-shoppers-segmentation-a-complete-tutorial-268974d905da/", "webpage": {"metadata": {"title": "Multinomial Mixture Model for Supermarket Shoppers Segmentation | by Adrien Biarnes | Towards Data Science", "h1": "Multinomial Mixture Model for Supermarket Shoppers Segmentation", "description": "In my last article, I wrote a detailed explanation of the Gaussian Mixture Model (GMM) and the way it is trained using the Expectation-Maximization (EM) algorithm. This time, I wanted to show that a\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/gaussian-mixture-models-and-expectation-maximization-a-full-explanation-50fa94111ddd", "anchor_text": "my last article", "paragraph_index": 0}, {"url": "http://www.datalab.uci.edu/papers/profiles.pdf", "anchor_text": "Predictive Profiles for Transaction Data using Finite Mixture Models", "paragraph_index": 2}, {"url": "https://www.kaggle.com/frtgnn/dunnhumby-the-complete-journey", "anchor_text": "Dunnhumby \u2014 The Complete Journey", "paragraph_index": 8}, {"url": "https://en.wikipedia.org/wiki/Multinomial_distribution", "anchor_text": "Multinomial distribution", "paragraph_index": 13}, {"url": "https://en.wikipedia.org/wiki/Binomial_distribution", "anchor_text": "Binomial distribution", "paragraph_index": 13}, {"url": "https://en.wikipedia.org/wiki/Bernoulli_distribution", "anchor_text": "Bernoulli distribution", "paragraph_index": 13}, {"url": "https://towardsdatascience.com/gaussian-mixture-models-and-expectation-maximization-a-full-explanation-50fa94111ddd", "anchor_text": "my article", "paragraph_index": 49}, {"url": "https://en.wikipedia.org/wiki/Lagrange_multiplier", "anchor_text": "Lagrangian multipliers", "paragraph_index": 57}, {"url": "https://en.wikipedia.org/wiki/Dirichlet_distribution", "anchor_text": "Dirichlet distribution", "paragraph_index": 64}, {"url": "https://en.wikipedia.org/wiki/Elbow_method_(clustering)", "anchor_text": "elbow pattern", "paragraph_index": 72}, {"url": "https://en.wikipedia.org/wiki/Bayesian_information_criterion", "anchor_text": "Bayesian Information Criterion", "paragraph_index": 78}, {"url": "https://en.wikipedia.org/wiki/Multidimensional_scaling", "anchor_text": "multidimensional scaling", "paragraph_index": 88}, {"url": "https://en.wikipedia.org/wiki/Taxicab_geometry", "anchor_text": "Manhattan distance)", "paragraph_index": 88}, {"url": "http://www.datalab.uci.edu/papers/profiles.pdf", "anchor_text": "Predictive Profiles for Transaction Data using Finite Mixture Models", "paragraph_index": 112}, {"url": "https://github.com/biarne-a/MNMM", "anchor_text": "https://github.com/biarne-a/MNMM", "paragraph_index": 124}, {"url": "https://en.wikipedia.org/wiki/Poisson_point_process", "anchor_text": "Poisson process", "paragraph_index": 125}, {"url": "https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation", "anchor_text": "Maximum A Posteriori", "paragraph_index": 126}, {"url": "https://www.linkedin.com/in/adrien-biarnes-81975717", "anchor_text": "https://www.linkedin.com/in/adrien-biarnes-81975717", "paragraph_index": 129}], "all_paragraphs": ["In my last article, I wrote a detailed explanation of the Gaussian Mixture Model (GMM) and the way it is trained using the Expectation-Maximization (EM) algorithm. This time, I wanted to show that a mixture model is not necessarily a mixture of Gaussian densities. It can be a mixture of any distribution. In this example, we are going to use a mixture of multinomial distributions.", "Also, the idea is, for once, not to solely focus on the mathematical and computer science aspects of a data science project but on the business side too. Therefore we are going to use a real-world data set with a concrete application in the marketing domain. It will hopefully allow the reader to get a better vision of why we do the things we do :-). Additionally, we will introduce a bit of data visualization on the matter of picturing the multinomial distribution.", "This work has been majorly inspired by a research paper from Cadez et al. entitled \u201cPredictive Profiles for Transaction Data using Finite Mixture Models\u201d. A big part of the credit goes to them.", "In this era of supercomputers, machine learning, and data science, almost every business out there is collecting data about the different facets of its activities and try to use it for its own benefit. The retail industry and in particular supermarkets are no exception to that rule. Supermarkets collect purchasing data or what is most commonly known as transaction data. It can be mined in order to extract insights and improve the efficiency of overall operations.", "One way to extract relevant patterns is to start by a clustering process. The idea is to group similar items together. It can be thought of as splitting the data set into clusters in such a way that data points inside a cluster have high similarity and points outside that cluster a low similarity.", "In traditional marketing applications, the most important usage of such a clustering procedure is exploratory data analysis. We want to split the observations into a small number of clusters in order to better describe, interpret, and study them independently. The common usage is to proceed with customer segmentation. Each segment will have its own set of characteristics. For example, an output segment, for an online e-commerce website, could be described as \u201cprice-sensitive customers under 30, engaged primarily through digital channels\u201d. Such projects are among the most frequently performed in marketing analytics because of their strategic importance. It allows building corporate marketing strategies around customer segments and their typical needs and properties.", "The goal of this project is to perform the clustering of a supermarket transaction data set in order to build predictive profiles of individuals. Such profiles support many different types of further analysis. Among them we find building customer segments for targeted marketing strategies, extracting somewhat hidden product associations, forecasting individual purchasing behaviors, change detection, cross-selling, personalization, and more\u2026", "In this article, we are going to:", "The dataset for this project was found on Kaggle: \u201cDunnhumby \u2014 The Complete Journey\u201d. Quoting Kaggle, it: \u201ccontains household level transactions over two years from a group of 2,500 households who are frequent shoppers at a retailer. For certain households, demographic information, as well as direct marketing contact history, are included\u201d.", "In this work, we are going to focus on the transaction data and the associated products. So let's see what the first few transactions look like:", "Every line corresponds to a certain quantity of a product being bought in a specific basket (one basket corresponds to one specific checkout) by a given household. This dataset contains 2595732 transactions made among 276484 baskets. The transactions were performed by 2500 households for 92339 different products in 44 department stores among 308 categories and 2383 subcategories.", "There is a lot of useful information but we are not interested in using everything. This exercise is about the unsupervised clustering of the transaction data set using a multinomial mixture model. As we will see, multinomial mixture models are to be used with categorical data only. Therefore, we will not consider continuously valued predictors like price or retail discount.", "Now in order to understand the data selection and preparation process that we are about to perform, we need to make sure that you properly understand the multinomial distribution (if you are already well versed with the multinomial, you can skip this section).", "The Multinomial distribution is a generalization of the Binomial distribution which itself is a generalization of the Bernoulli distribution. So let's start with the Bernoulli.", "A Bernoulli random variable X depicts the result of a single trial with 2 possible outcomes, 1 or 0, with respective probabilities \u03b8 and 1-\u03b8.", "For example, we could picture the probability mass function of Ber(0.3):", "In the frequentist perspective, the true value of a parameter is obtained by measuring the statistics of interest over an infinite number of experiments. In the case of the Bernoulli trial, if we repeat our single experiment an infinite number of times, record the results and average the number of positive outcomes we get the true parameter value of \u03b8 (in the above example, we get a positive result 30% of the time).", "Now, what if we want to count the number of positive results for not only a single Bernoulli trial but a series of n Bernoulli trials. That is the purpose of a Binomial trial. For example, flipping a coin 10 times, we want to measure the number of times that the coin landed on heads. This time, the output of the Binomial trial can be any discrete value between 0 and 10. The exact formulation of the probability mass function is :", "Let us see how we can derive this formula from the Bernoulli distribution. So let's say for simplicity that we want to record the number of heads that we get out of two coin flips (i.e. two Bernoulli trials). What are the possibilities:", "From this enumeration, we get :", "The binomial coefficient in the formula captures the fact that there are different ways of distributing k successes in a sequence of n trials. In the example above, there are 2 ways of distributing 1 success among 2 trials. If we increase the number of trials, the number of possible ways to obtain central values increases much more rapidly than the number of possible ways to get extreme values. That is why the distribution gets a nice bell shape.", "So the probability mass function can be pictured with the following bar chart in the case of a Binomial trial with 20 independent Bernoulli trials of probability \u03b8 = 0.3:", "The most probable number of positive outcomes for a Binomial trial with 20 independent Bernoulli trials of probability 0.3 is 6. That is the mode of the above distribution.", "Now how can we generalize this distribution one step further? Well, what if we consider a series of n trials but this time with more than two possible outcomes\u2026 For example, we have a bag with 3 different types of balls (blue, red, and yellow) and we want to measure for each color the number of balls that we get out of 2 draws (with replacement). Let's enumerate the possibilities:", "From this enumeration, and given that we have at our disposal the proportions of blue, red, and yellow balls, respectively \u03b8_1, \u03b8_2 and \u03b8_3, we can measure the probabilities :", "And we can continue on, but I hope that you understand how we can derive the analytical formula for the probability mass function of the multinomial:", "Ok, so now, what if we want to draw the probability mass function using some sort of bar plot as we did above? Well, this time is going to be a little trickier.", "This is kind of a disgression but it is important that you understand this representation of the multinomial as we will use it later.", "In order to picture a distribution, we need a way to place the possible outcomes on the graph and, for each of them, be able to show their relative importance. That is the purpose of the bar plots we used for the Bernoulli and the Binomial distributions. On the x-axis, we put the ordered possible outcome values and on the y-axis the associated probabilities.", "Now, what about the multinomial? Well, the issue here is that we can no longer use a single random variable to depict the different possibilities. In the Bernoulli and the Binomial, we use a single random variable X to count the number of successes. We don\u2019t use a second random variable for the number of failures because it is obviously deduced from X. But in the case of the Multinomial, we need to introduce at least C-1 random variables for C possible outcomes of the single trial. Note that in the general we even use C random variables (X_1, X_2, \u2026, X_C) like in the formula above for the PMF because it is less cumbersome to write (even if the last random variable value can be deduced).", "We are only interested in picturing the distribution with at least 2 possible outcomes and more (otherwise we fall back to the Binomial). Let us consider the case of 3 possible outcomes first. As I said, we could use only two random variables X_1 and X_2 (X_3 can be deduced). We would then need 3 dimensions (for X_1, X_2, and the joint probability P(X_1, X_2)=P(X_1, X_2, X3)). Ok, we can use a 3-dimensional bar plot for that purpose:", "Ok, so what is wrong with this visualization? Well, first of all, we don\u2019t explicitly see the different values for X3. We can deduce them. For example, for the first bin with X1=0 and X2=0, we know that X3=20. But we don\u2019t visualize it. Also, half of the dedicated space for this graph is useless. For example, when X2=20, we know for sure that X1 will never take any value other than 0. So the triangle for half of the bottom floor of this 3d histogram will never display any density because probabilities are null.", "So how can do better? Well, first we can reduce the bottom floor of the histogram to a triangle and more precisely an equilateral triangle. We use an equilateral triangle so that the distances from the vertices to the barycenter are equal. The exact location of any point in this triangle determines the relative proportions of our 3 random variables. This is called a simplex and it is very useful to display a discrete probability vector of 3 values:", "Also, if we draw every parallel of the triangle sides, that is we discretize the entire triangle space, each intersection can be used to picture the relative proportions of a 3 valued tuple. In the example below, we picture the multinomial with 16 draws and 3 possible values with probabilities 0.25, 0.5, and 0.25:", "Finally, and I will stop this disgression here, it is not possible to extend this logic to additional dimensions to picture multinomials with degrees higher than 3. For example, we could try with a square for dimension 4 but it won\u2019t work because a point in the square is only at the crossroad of 2 lines and we need the crossroad of 4 lines for this location to be the container of the proportions of 4 valued tuples.", "As I previously said, because we want to perform the clustering using a multinomial mixture model, we don\u2019t consider the continuous variables. For the sake of this exercise, we are only going to consider the products, the households, and the baskets. So the columns to be considered are the following:", "There are 4 columns describing the product purchased in a specific transaction (PRODUCT_ID, DEPARTMENT, COMMODITY_DESC, and SUB_COMMODITY_DESC). We want to group the transactions and, for each group, count the number of products.", "So why do we want to count the number of products? Well, this will allow us to apply the following general model:", "This represents the joint probability of observing the full data set. It is the product of individual probabilities (because the observations are collected i.i.d). The probability of observing an observation x_i is a mixture of multinomial distributions. X represents a matrix of counts for the products that were bought. Each line of the matrix, x_i, corresponds to a specific basket (i.e. one shopper checking out from the supermarket). Each multinomial distribution represents the probability of obtaining the counts that we see in a specific basket given that it was generated by a specific cluster k:", "In the above formula, n_i represents the total number of products bought in a given basket. C represents the total number of different products so there is one parameter \u03b2_kc by multinomial k and product c. This means that \u03b2 is a matrix of parameters of dimension k times c and \u03b2_k a vector of dimension c. n_ic represents the count of a specific product c bought in a given basket i.", "Our model uses a mixture of multinomial distributions. It basically makes the assumption that there exists K clusters. It means that the probabilities of items being bought in any given basket are a combination of K typical baskets. The goal of this project is to extract those typical baskets.", "So we need to group the transactions by baskets, and for every basket, count the number of distinct products. But as we just saw, the number of parameters that we will use for this model highly depends on the number of distinct products. The more products we have, the higher the number of parameters. This means that if we have too many products, we might run into troubles during the optimization procedure to find the optimal values for the parameters or the number of clusters. But we are not obliged to count by distinct products (there are 92339 of them). We can count by the department stores (44), the category (308), or the sub-category (2383). This choice will determine the number C in the above formula.", "Obviously, the ideal scenario would be to model the data all the way down to the product level. And in fact, I have tried to do so, and I ran short of memory on my laptop. Not to mention that with so many different components for the multinomial, the problems we might face to select the correct number of clusters are considerably increased (as we will see later). So, to make the distinction between products, we have to choose between the department store, the category, and the sub-category. Let\u2019s see the repartitions of the number of transaction for each department:", "So as we can see, the vast majority of the transactions were made in the grocery department. Because this is an exercise, I decide not to bother with the transactions outside the grocery department. Now let\u2019s see how the transactions from the grocery store are spread among categories:", "There are 94 categories and although the number of transactions among categories is not homogenous, the least provided category still has 51 transactions. Now let\u2019s see what we have for the sub-categories:", "We have the same decreasing pattern. But now there are so many sub-categories that the ones with the least transactions only have 1 transaction. A single transaction is clearly not enough for the parameters to be accurately estimated.", "So, we are going to distinct the product by categories (and not sub-categories) and produce the matrix of counts X that we will use to resolve the clustering problem. Let\u2019s see what the first 10 rows look like:", "As you can see, the matrix is very sparse. But this is not a problem for the multinomial mixture model.", "So as we said, the full likelihood of observing the data set is defined by:", "In order to understand this part, you need to be familiar with the Expectation-Maximization algorithm (if not, I highly encouraged you to read my article on the matter). With EM, we first need to define a latent variable t, that describes an observation by defining from which cluster it was generated.", "The latent variable t_i defines by which cluster the observation x_i was generated. Also, a variational distribution q is used to describe the posterior distribution of the latent variable taking the range of possible values (from 1 to K). So we can write:", "Recall that the EM algorithm defines a lower bound for the log-likelihood:", "The EM algorithm proceeds with the two following steps alternatively until convergence:", "We maximize the lower bound with respect to q to update the posterior distribution of the latent variables:", "As you can see, the form of the expectation step remains the same as in the case of a mixture of Gaussians, except that the likelihood of an observation given the cluster k, that is P(x_i | \u03b2_k), now has a multinomial density:", "We maximize the lower bound with respect to \u03b1 and \u03b2. We try to solve the following optimization problem:", "The lower bound is defined as:", "In order to resolve this optimization problem, we are going to set the partial derivatives to 0 and solve the equations. Notice that the second term in the subtraction above does not depend on \u03b1 or \u03b2, so we can replace it with a constant. Also, we will make us of Lagrangian multipliers to get rid of the constraints. So we need to resolve the following system of equations:", "Let\u2019s start with the multinomial parameters \u03b2_kc:", "Ok, so we now have the update rules for the variational distribution during the E-step and for the parameters during the M-step. Let\u2019s get to the concrete materials.", "The expectation-maximization algorithm does not give us any guarantee of whether we will find the global maximum of the likelihood function on first run. In order to increase our chances, it is recommended to start the algorithm several times with different random initializations. Each time, we shall compare our maximum with the previous one and keep the parameters associated with the best loss. Let\u2019s implement this routine:", "So I created a class for this model that needs to be instantiated with the number of clusters that we want to try out. I also added some parameters:", "Next, we need to implement the _train_once method that, as its name suggests, will run one full cycle of iterations for the EM algorithm.", "The shape of the algorithm is no big surprise. We start by initializing the parameters randomly. We iterate up to max_iter iterations and each time we alternate between the e-step to compute the posterior distributions of the latent variable t (gamma) and the m-step to update the parameters of the mixture (alpha and beta).", "Notice that the beta parameters are initialized using the Dirichlet distribution which is conjugate to the multinomial distribution. I won\u2019t get into the details here but you just need to know that the Dirichlet will produce a set of parameters that sum up to 1 (for every cluster) as required.", "Now there is also one thing that we did not talk about yet. It is the loss. Remember that at each iteration the EM algorithm is trying to maximize the lower bound. So the loss function that we will use track the convergence of the algorithm is the lower bound:", "This gives us the following implementation:", "Notice that the computation is not fully vectorized. We still iterate through the different K clusters. This is because we use the implementation of the multinomial from scipy.stats. This implementation accepts a matrix of observations for the counts but not for the parameters. So you can compute the multinomial probabilities for all the observations but only for a specific cluster and not all of them at once.", "So this is just the vectorized implementation of the update rule we derived mathematically in the previous section. One thing to notice though is that when computing the multinomial probabilities of observing the count vectors (i.e. the likelihood), the probabilities are sometimes so close to 0 that we get numerical underflows and values are floored to 0. This is no good because we have to normalize the values to get back the posterior probabilities. In order to avoid division by 0, we replace the null values with the minimal floating-point value allowed in Python.", "No big surprise. Those are vectorized implementation of the update rules we derived mathematically.", "We are going to run the algorithm and see how it performs for various values ok K. A nice thing to do is first to use a simulated data set so that we control the data generation process. We can then see how the algorithm performs when we modify the true values for the parameters (i.e. the number of clusters, the mixture weights, the multinomial parameters, and the size of the dataset). The data set is generated with the following routine:", "So we are first going to perform a simple test. The data set is generated as a mixture of 10000 observations coming from 3 very well separated multinomials that can take up to 16 different values. Also, we split the generated data set into 80 % for training and 20 % for testing. Then we compute the likelihood on the test data:", "As we can see, the estimated parameters are very close to real ones. Also, now we see the value of our work regarding the visualization of the multinomial. It is very apparent, from the above plot, that the clusters are very well separated. So the algorithm should clearly have no problem in estimating the true number of clusters. And if we look at the likelihood evolution when we increase the number of clusters, we can clearly see the elbow pattern. When we go from 2 clusters to 3, the fit is much more likely. But as soon as we reach 3 clusters, the likelihood hits a plateau. This clearly suggests that the best number of clusters is 3. Why? Well, because when we increase the number of clusters, we increase the complexity of the model with no additional benefits on the likelihood.", "Now what happens when we modify the mixture weights:", "Once again, the mixture weights are very well estimated which confirms the update rules are correct.", "Now, what happens when the clusters are not very well separated? Let\u2019s see:", "Ok so this time, the mixture weights are a bit less correctly inferred. The multinomial component weights are better though. Also, we still see an elbow pattern in the likelihood values, but they are a bit more noisily distributed. From the distribution of the likelihoods, we can clearly see that the jump from 2 to 3 clusters is the one that gets out the most value in terms of goodness of fit. So we still choose 3 clusters.", "Now, what if we pack the parameter space even more with 10 different multinomials?", "Ok so this time, selecting the best number of clusters based solely on the improvement on the likelihood is much more tricky. It might be 3 clusters or maybe 7\u2026 which clearly is not right. Ok, so how can we do better? Well, we are going to use another selection criterion. It is called the Bayesian Information Criterion (or BIC in short). It is computed with the following formula:", "D represents the number of parameters to be estimated by the model, N the total number of observations, and L_hat the likelihood of the model. It introduces a penalty proportional to the number of observations and the number of parameters. What we want to do is minimize the BIC value, so that the likelihood is maximized but at the same time keeping the number of parameters as low as possible. In the case of the mixture of multinomials, with K multinomials of C components, D, the number of parameters, is equal to (K-1)+K*(C-1). Knowing that the mixture weights and the component weights both sum up to 1, we can deduce the last parameters.", "Applying this new selection criterion to the generated data set with 10 multinomials, we get:", "The optimal number of 10 clusters is correctly selected by the BIC criterion. But we see that it was a very close match. And in fact, running the same procedure again, we might get a different number of clusters. It shows that selecting the correct number is very hard and there is no silver bullet. In the end, nothing beats the judgment call of the business analyst. So it is a good thing to review the clusters and try to see if some of them should be merged (because they are very closed to each other and have very similar properties).", "Ok so now we have the ingredients to perform the clustering of the grocery store transaction data set. Recall that we have prepared a matrix for the transaction counts of distinct product categories grouped by basket:", "After splitting this matrix on a 80/20 rule (ordered by basket transaction times), we fit the model to the training data and record the likelihood and BIC values on the test data for a set of possible cluster values from 2 to 100. We get the following result:", "From the above plot, we understand why with real-world data sets, selecting the optimal number of clusters based solely on the likelihood is not very reliable and why we need to penalize on the complexity of the model. And that comes from the fact that clusters are, most frequently, not very well separated. Using the Bayesian Information Criterion, we select 30 clusters.", "First of all, we can visualize the clusters in terms of probability distributions of product categories. We plot the 6 first clusters :", "What is important to understand is that the distributions above represent the typical baskets. They can help us with the forecast for product purchases.", "Also, we can notice that some typical baskets are dominated by one or a low number of categories whereas some other typical baskets are made of a more diverse set of categories. This highlights shopping behaviors. Sometimes a person will make a quick visit to the store and buy the ingredients for a specific occasion (Saturday brunch, Sunday supper, a drink with friends,\u2026) or it might be a longer visit for the weekly shopping.", "Next, we would like to assess the clustering quality by visualizing their relative distances. Considering that there are 30 clusters, it is not evident to see that they are pretty well separated from one another. In order to visualize the distances between them, the ideal scenario would be to build up a 30-dimensional space in which we could place the observations. Of course, this not feasible so we are going to rely on a dimensionality reduction technique. The technique we use is called multidimensional scaling. Like the PCA, it relies on the eigendecomposition of the input matrix. But this time, the input matrix will be the pairwise distance matrix of the cluster parameters (using the Manhattan distance). We get the following result:", "The cluster sizes are synchronized on the mixture weights which highlights their inequalities. Although some information is lost due to the dimensionality reduction technique, we can still conclude that the clusters are pretty well separated.", "A good way to describe the clusters is by computing the lift ratio for the highly bought individual products. The lift ratio for a product and a cluster is defined as the ratio between the purchase probability conditioned on a cluster over the total purchase probability. So first we need to compute the purchase probabilities of each individual products, which can be defined as:", "It is the ratio between the total quantities for a specific product over the total quantities of all the products. It can be defined as the probability of finding a product in a basket randomly chosen among all the baskets of the data set. Let\u2019s see the distribution of those probabilities for all the products:", "As you can see, there is a high concentration of very low values. An also, a good proportion of probabilities are so low that their computation suffered from numerical underflow (which basically resulted in a 0 probability). We want to consider only items that are frequently bought because they are the most characteristics. So we will restrict this analysis to products with purchase probabilities higher than 0.0001 (2019 products).", "Next, in order to compute the lift ratio, we need to compute the purchase probabilities of those items but for specific clusters. It means that we want to get the probabilities of finding a product in a basket randomly chosen among the baskets of a specific cluster k (and not among all the baskets):", "The issue with this computation is that talking about the basket of a cluster is not, strictly speaking, correct. Indeed, with the current modelization, a basket belongs to several clusters at the same time. Remember that we are performing soft clustering as defined by the posterior probability distribution of the latent variable over the parameters, that is P(t_i|x_i,\u03b1,\u03b2). For example, let\u2019s visualize the posterior distributions of the latent variable for the 6 first baskets:", "The goal of the clustering process is to extract the most typical baskets by grouping them together. Some baskets will be located at the core of a cluster (in assignments probability space). The distribution then exhibits a single high value for a specific cluster (like for the top left basket above). But some baskets will be located in regions where the assignments are a bit fuzzier. So in order to be accurate in the computation of purchase probabilities conditioned on a cluster, we are first going to distribute the counts of basket products among the different clusters. So we compute a new floating-points matrix of size N times K times C with the following rules:", "Finally, we can compute the lift ratio which is:", "Showing the items with a lift ratio greater than 10, we can start to describe the clusters with specific products. For example, the high lift items for the first cluster are the following:", "The items with the highest lift of the first cluster are canned beans and tomatoes.", "Also, in order to describe the clusters a bit more visually, we can make word clouds based on the categories of high lift items (taking into account purchase frequencies too). Let\u2019s see what we get for the 6 first clusters:", "From the word clouds above we can describe the clusters. This first cluster is a good mix although the main topic is vegetables. The second and third clusters are dominated by bakery items, the fourth by products relating to dinner, the fifth by snacks, and the sixth by dairy products.", "Now the high lift items analysis can be pushed one step further by computing the high lift pairs of items. The computation of probabilities is the same except that this time we compute it for pairs of items. For example, one is 17.2 times more likely to find both \u201cVARIETY BEANS \u2014 KIDNEY PINTO\u201d and \u201cTOMATO SAUCE\u201d from a random basket of cluster 1 than from a general random basket.", "By first narrowing down the dataset to transactions from a specific cluster and then checking for purchase probabilities for pairs of products we enveil associations that would otherwise not be relevant!", "So far, the model we have been working with describes the likelihood of an observation as a mixture of multinomial distributions:", "The full data likelihood is written as:", "Now as I previously said, this model considers each basket independently and there is no distinction for the baskets of different individuals. We consider every individual in the same regard which means that they have the same predictive profile, resulting from the estimation of the model parameters. In our case, this gives us the following general profile (simulated by sampling):", "This profile, is on average the one that describes with the better accuracy the purchase probabilities. Now it might suit some individuals pretty well, but in most cases, it makes a poor predictor. So how can we do better? Well, first of all we are going to introduce the distinction between the baskets of individuals by rewriting the full data likelihood:", "Now the index i, refers to a specific individual. The index j refers to the current basket for that individual (from 1 to the total number of baskets n_i). We didn\u2019t change anything to the model. This model above is already what we have implemented. We just introduced the distinction between the baskets of individuals. Now remember that we are working with a latent variable model. Each instance of latent variable describes the probability that the basket j from individual i was generated by a cluster k. So instead of the likelihood, we could consider the classification likelihood which is the complete-data likelihood within the EM framework for mixture models:", "So for a specific individual, the complete-data likelihood is computed as:", "Now, what we can do, is check this implementation for specific individuals by confronting their purchases from the test set and the predictions obtained by sampling from the predictive distribution above.", "In order to build the predictive distribution for a given individual, we iterate through all baskets from the train set for that individual, for every basket, we iterate through every mixture component k and each time we sample from the corresponding multinomial. The sample is then weighted with the mixture weight and the latent probability of the observation belonging to the cluster. In the end, all the samples are summed up and we normalize by the total count to get back a proper probability distribution. Also, in order to score the prediction, we compute the L1-distance between the prediction and the normalized purchases counts from the test set. Here are a two examples:", "We can see that the result is not always as good for every individual. The prediction is obviously better for frequent shoppers than rare ones.", "Ok so is there a way to do even better? The answer is yes and to be found in the research paper that I mentioned at the beginning of this document: \u201cPredictive Profiles for Transaction Data using Finite Mixture Models\u201d.", "The idea is to replace the global mixture weights by individualized ones. The full data likelihood then becomes:", "This time, I won\u2019t get into the mathematical derivations of the update rules. The only thing you need to know is that they remain the same except for the update of the mixture weight during the m-step that becomes:", "If we compare with the previous update rule with the global model, the difference is that now we only use the posterior distribution of the latent variable for the baskets of the individual instead of all the baskets.", "After training of this model, we can compare the new training results (in terms of likelihood and BIC value) with previous ones:", "We see that, with this new model, the likelihood on the test set is better for any number of clusters which means that the fit is more likely. The issue that we now have though is that the BIC criterion proposes to select only 2 clusters. That seems really inappropriate given the nature of the data. And this is where we touch the limits of the BIC criterion. In this modelization, we replaced the K global mixture weights by K times N individualized mixture weights. This means that the penalization of the number of parameters now weighs a lot more in the formula.", "There exists a vast quantity of different criterions for assessing the number of clusters to be selected. Here are the ones that I found:", "I am sure that there other criterions out there and it is still an active research subject. New ways of selecting the best number of clusters are still emerging. So we see the breadth and complexity of the subject. But as I previously said, nothing beats the judgment call of a domain expert.", "Anyhow with this new modelization, we get the following results for the 2 individuals selected in the previous section:", "We went from 1.625 to 1.564 in L1-distance for household 892 and from 0.647 to 0.530 for household 72. Now performing the same exercise for every household, we can record the obtained distances and confront them on a scatter plot:", "Every point below the diagonal corresponds to a household for which the prediction distance was smaller with the individualized model. It is not necessarily obvious from the above plot, but we performed a better prediction for 61.57% of the households.", "So in the end, we consider that the second modelization with individual mixture weights is a better model because it gives a better predictive performance on the test set. We just did not found, yet, a way to correctly assess the number of clusters (although I would probably select 30 clusters as we did with the global model) and I leave that as an exercise for the interested reader.", "Regarding this last point, you can reproduce all the results using the code on my github (along with the notebooks for data exploration, model selection and results analysis) => https://github.com/biarne-a/MNMM", "This model allows to predicts which products an individual is likely to purchase, but not how many or when they will be purchased. The multinomial distribution gives counts of purchased items but requires the total number of purchased items in a basket as input. So ideally we would need another model to predict the total number of items an individual would purchase on a given day. Also in order to model the rate of visits to the store we could have used a Poisson process and we would have had to take care of seasonality patterns.", "One important point is that in the paper that inspired this article, the authors use the EM algorithm to find the Maximum A Posteriori (MAP) estimates of the model parameters, whereas we found the MLE estimates. This is more in line with the Bayesian philosophy. It means that they declared priors on the parameters \u03b1 and \u03b2 of the model (both Dirchlet priors). This approach is better because it allows among other things to regularize the model. We can see it as, starting from plausible guesses for the parameters, we refine the values according to the amount of data we have at our disposal. For the individualized model we start with the global estimates of the model. If we don\u2019t have much data about an individual, then its estimates will stay close to the global ones. I didn\u2019t want to take this approach for the sake of this article because it would have complicated things even more (and I would also have had to introduce the Dirichlet distribution among other things).", "This has been quite a journey! If you read up to this point, it probably means that you have found this interesting and maybe you learned a few things along the way (at least I hope).", "In any case, take care of your selves and loved ones!", "I am a data scientist / ML engineer at Dailymotion working on large scale deep recommender systems \u2014 https://www.linkedin.com/in/adrien-biarnes-81975717"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F268974d905da&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmultinomial-mixture-model-for-supermarket-shoppers-segmentation-a-complete-tutorial-268974d905da&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmultinomial-mixture-model-for-supermarket-shoppers-segmentation-a-complete-tutorial-268974d905da&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmultinomial-mixture-model-for-supermarket-shoppers-segmentation-a-complete-tutorial-268974d905da&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmultinomial-mixture-model-for-supermarket-shoppers-segmentation-a-complete-tutorial-268974d905da&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://biarnes-adrien.medium.com/?source=post_page-----268974d905da--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----268974d905da--------------------------------", "anchor_text": ""}, {"url": "https://biarnes-adrien.medium.com/?source=post_page-----268974d905da--------------------------------", "anchor_text": "Adrien Biarnes"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F151fca431deb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmultinomial-mixture-model-for-supermarket-shoppers-segmentation-a-complete-tutorial-268974d905da&user=Adrien+Biarnes&userId=151fca431deb&source=post_page-151fca431deb----268974d905da---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----268974d905da--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F268974d905da&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmultinomial-mixture-model-for-supermarket-shoppers-segmentation-a-complete-tutorial-268974d905da&user=Adrien+Biarnes&userId=151fca431deb&source=-----268974d905da---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F268974d905da&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmultinomial-mixture-model-for-supermarket-shoppers-segmentation-a-complete-tutorial-268974d905da&source=-----268974d905da---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://unsplash.com/@pvsbond?utm_source=medium&utm_medium=referral", "anchor_text": "Peter Bond"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://towardsdatascience.com/gaussian-mixture-models-and-expectation-maximization-a-full-explanation-50fa94111ddd", "anchor_text": "my last article"}, {"url": "http://www.datalab.uci.edu/papers/profiles.pdf", "anchor_text": "Predictive Profiles for Transaction Data using Finite Mixture Models"}, {"url": "https://www.kaggle.com/frtgnn/dunnhumby-the-complete-journey", "anchor_text": "Dunnhumby \u2014 The Complete Journey"}, {"url": "https://en.wikipedia.org/wiki/Multinomial_distribution", "anchor_text": "Multinomial distribution"}, {"url": "https://en.wikipedia.org/wiki/Binomial_distribution", "anchor_text": "Binomial distribution"}, {"url": "https://en.wikipedia.org/wiki/Bernoulli_distribution", "anchor_text": "Bernoulli distribution"}, {"url": "https://towardsdatascience.com/gaussian-mixture-models-and-expectation-maximization-a-full-explanation-50fa94111ddd", "anchor_text": "my article"}, {"url": "https://en.wikipedia.org/wiki/Lagrange_multiplier", "anchor_text": "Lagrangian multipliers"}, {"url": "https://en.wikipedia.org/wiki/Dirichlet_distribution", "anchor_text": "Dirichlet distribution"}, {"url": "https://en.wikipedia.org/wiki/Elbow_method_(clustering)", "anchor_text": "elbow pattern"}, {"url": "https://en.wikipedia.org/wiki/Bayesian_information_criterion", "anchor_text": "Bayesian Information Criterion"}, {"url": "https://en.wikipedia.org/wiki/Multidimensional_scaling", "anchor_text": "multidimensional scaling"}, {"url": "https://en.wikipedia.org/wiki/Taxicab_geometry", "anchor_text": "Manhattan distance)"}, {"url": "http://www.datalab.uci.edu/papers/profiles.pdf", "anchor_text": "Predictive Profiles for Transaction Data using Finite Mixture Models"}, {"url": "https://github.com/biarne-a/MNMM", "anchor_text": "https://github.com/biarne-a/MNMM"}, {"url": "https://en.wikipedia.org/wiki/Poisson_point_process", "anchor_text": "Poisson process"}, {"url": "https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation", "anchor_text": "Maximum A Posteriori"}, {"url": "https://medium.com/tag/bayesian-statistics?source=post_page-----268974d905da---------------bayesian_statistics-----------------", "anchor_text": "Bayesian Statistics"}, {"url": "https://medium.com/tag/marketing?source=post_page-----268974d905da---------------marketing-----------------", "anchor_text": "Marketing"}, {"url": "https://medium.com/tag/statistics?source=post_page-----268974d905da---------------statistics-----------------", "anchor_text": "Statistics"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----268974d905da---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/mixture-models?source=post_page-----268974d905da---------------mixture_models-----------------", "anchor_text": "Mixture Models"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F268974d905da&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmultinomial-mixture-model-for-supermarket-shoppers-segmentation-a-complete-tutorial-268974d905da&user=Adrien+Biarnes&userId=151fca431deb&source=-----268974d905da---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F268974d905da&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmultinomial-mixture-model-for-supermarket-shoppers-segmentation-a-complete-tutorial-268974d905da&user=Adrien+Biarnes&userId=151fca431deb&source=-----268974d905da---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F268974d905da&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmultinomial-mixture-model-for-supermarket-shoppers-segmentation-a-complete-tutorial-268974d905da&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://biarnes-adrien.medium.com/?source=post_page-----268974d905da--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----268974d905da--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F151fca431deb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmultinomial-mixture-model-for-supermarket-shoppers-segmentation-a-complete-tutorial-268974d905da&user=Adrien+Biarnes&userId=151fca431deb&source=post_page-151fca431deb----268974d905da---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F2e2f1d28e98d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmultinomial-mixture-model-for-supermarket-shoppers-segmentation-a-complete-tutorial-268974d905da&newsletterV3=151fca431deb&newsletterV3Id=2e2f1d28e98d&user=Adrien+Biarnes&userId=151fca431deb&source=-----268974d905da---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://biarnes-adrien.medium.com/?source=post_page-----268974d905da--------------------------------", "anchor_text": "Written by Adrien Biarnes"}, {"url": "https://biarnes-adrien.medium.com/followers?source=post_page-----268974d905da--------------------------------", "anchor_text": "662 Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----268974d905da--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://www.linkedin.com/in/adrien-biarnes-81975717", "anchor_text": "https://www.linkedin.com/in/adrien-biarnes-81975717"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F151fca431deb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmultinomial-mixture-model-for-supermarket-shoppers-segmentation-a-complete-tutorial-268974d905da&user=Adrien+Biarnes&userId=151fca431deb&source=post_page-151fca431deb----268974d905da---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F2e2f1d28e98d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmultinomial-mixture-model-for-supermarket-shoppers-segmentation-a-complete-tutorial-268974d905da&newsletterV3=151fca431deb&newsletterV3Id=2e2f1d28e98d&user=Adrien+Biarnes&userId=151fca431deb&source=-----268974d905da---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/gaussian-mixture-models-and-expectation-maximization-a-full-explanation-50fa94111ddd?source=author_recirc-----268974d905da----0---------------------6f55c353_7f51_4737_823b_4659be7dc90f-------", "anchor_text": ""}, {"url": "https://biarnes-adrien.medium.com/?source=author_recirc-----268974d905da----0---------------------6f55c353_7f51_4737_823b_4659be7dc90f-------", "anchor_text": ""}, {"url": "https://biarnes-adrien.medium.com/?source=author_recirc-----268974d905da----0---------------------6f55c353_7f51_4737_823b_4659be7dc90f-------", "anchor_text": "Adrien Biarnes"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----268974d905da----0---------------------6f55c353_7f51_4737_823b_4659be7dc90f-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/gaussian-mixture-models-and-expectation-maximization-a-full-explanation-50fa94111ddd?source=author_recirc-----268974d905da----0---------------------6f55c353_7f51_4737_823b_4659be7dc90f-------", "anchor_text": "Gaussian Mixture Models and Expectation-Maximization (A full explanation)The full explanation of the Gaussian Mixture Model (a latent variable model) and the way we train them using Expectation-Maximization"}, {"url": "https://towardsdatascience.com/gaussian-mixture-models-and-expectation-maximization-a-full-explanation-50fa94111ddd?source=author_recirc-----268974d905da----0---------------------6f55c353_7f51_4737_823b_4659be7dc90f-------", "anchor_text": "\u00b716 min read\u00b7Sep 11, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F50fa94111ddd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgaussian-mixture-models-and-expectation-maximization-a-full-explanation-50fa94111ddd&user=Adrien+Biarnes&userId=151fca431deb&source=-----50fa94111ddd----0-----------------clap_footer----6f55c353_7f51_4737_823b_4659be7dc90f-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/gaussian-mixture-models-and-expectation-maximization-a-full-explanation-50fa94111ddd?source=author_recirc-----268974d905da----0---------------------6f55c353_7f51_4737_823b_4659be7dc90f-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F50fa94111ddd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgaussian-mixture-models-and-expectation-maximization-a-full-explanation-50fa94111ddd&source=-----268974d905da----0-----------------bookmark_preview----6f55c353_7f51_4737_823b_4659be7dc90f-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----268974d905da----1---------------------6f55c353_7f51_4737_823b_4659be7dc90f-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----268974d905da----1---------------------6f55c353_7f51_4737_823b_4659be7dc90f-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----268974d905da----1---------------------6f55c353_7f51_4737_823b_4659be7dc90f-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----268974d905da----1---------------------6f55c353_7f51_4737_823b_4659be7dc90f-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----268974d905da----1---------------------6f55c353_7f51_4737_823b_4659be7dc90f-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----268974d905da----1---------------------6f55c353_7f51_4737_823b_4659be7dc90f-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----6f55c353_7f51_4737_823b_4659be7dc90f-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----268974d905da----1---------------------6f55c353_7f51_4737_823b_4659be7dc90f-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----268974d905da----1-----------------bookmark_preview----6f55c353_7f51_4737_823b_4659be7dc90f-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----268974d905da----2---------------------6f55c353_7f51_4737_823b_4659be7dc90f-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----268974d905da----2---------------------6f55c353_7f51_4737_823b_4659be7dc90f-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----268974d905da----2---------------------6f55c353_7f51_4737_823b_4659be7dc90f-------", "anchor_text": "Jacob Marks, Ph.D."}, {"url": "https://towardsdatascience.com/?source=author_recirc-----268974d905da----2---------------------6f55c353_7f51_4737_823b_4659be7dc90f-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----268974d905da----2---------------------6f55c353_7f51_4737_823b_4659be7dc90f-------", "anchor_text": "How I Turned My Company\u2019s Docs into a Searchable Database with OpenAIAnd how you can do the same with your docs"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----268974d905da----2---------------------6f55c353_7f51_4737_823b_4659be7dc90f-------", "anchor_text": "15 min read\u00b7Apr 25"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&user=Jacob+Marks%2C+Ph.D.&userId=f7dc0c0eae92&source=-----4f2d34bd8736----2-----------------clap_footer----6f55c353_7f51_4737_823b_4659be7dc90f-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----268974d905da----2---------------------6f55c353_7f51_4737_823b_4659be7dc90f-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "20"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&source=-----268974d905da----2-----------------bookmark_preview----6f55c353_7f51_4737_823b_4659be7dc90f-------", "anchor_text": ""}, {"url": "https://medium.com/mlearning-ai/building-a-multi-stage-recommendation-system-part-1-1-95961ccf3dd8?source=author_recirc-----268974d905da----3---------------------6f55c353_7f51_4737_823b_4659be7dc90f-------", "anchor_text": ""}, {"url": "https://biarnes-adrien.medium.com/?source=author_recirc-----268974d905da----3---------------------6f55c353_7f51_4737_823b_4659be7dc90f-------", "anchor_text": ""}, {"url": "https://biarnes-adrien.medium.com/?source=author_recirc-----268974d905da----3---------------------6f55c353_7f51_4737_823b_4659be7dc90f-------", "anchor_text": "Adrien Biarnes"}, {"url": "https://medium.com/mlearning-ai?source=author_recirc-----268974d905da----3---------------------6f55c353_7f51_4737_823b_4659be7dc90f-------", "anchor_text": "MLearning.ai"}, {"url": "https://medium.com/mlearning-ai/building-a-multi-stage-recommendation-system-part-1-1-95961ccf3dd8?source=author_recirc-----268974d905da----3---------------------6f55c353_7f51_4737_823b_4659be7dc90f-------", "anchor_text": "Building a Multi-Stage Recommendation System (Part 1.1)Understanding candidate generation and the two-tower model"}, {"url": "https://medium.com/mlearning-ai/building-a-multi-stage-recommendation-system-part-1-1-95961ccf3dd8?source=author_recirc-----268974d905da----3---------------------6f55c353_7f51_4737_823b_4659be7dc90f-------", "anchor_text": "\u00b715 min read\u00b7Aug 13, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fmlearning-ai%2F95961ccf3dd8&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fmlearning-ai%2Fbuilding-a-multi-stage-recommendation-system-part-1-1-95961ccf3dd8&user=Adrien+Biarnes&userId=151fca431deb&source=-----95961ccf3dd8----3-----------------clap_footer----6f55c353_7f51_4737_823b_4659be7dc90f-------", "anchor_text": ""}, {"url": "https://medium.com/mlearning-ai/building-a-multi-stage-recommendation-system-part-1-1-95961ccf3dd8?source=author_recirc-----268974d905da----3---------------------6f55c353_7f51_4737_823b_4659be7dc90f-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "8"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F95961ccf3dd8&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fmlearning-ai%2Fbuilding-a-multi-stage-recommendation-system-part-1-1-95961ccf3dd8&source=-----268974d905da----3-----------------bookmark_preview----6f55c353_7f51_4737_823b_4659be7dc90f-------", "anchor_text": ""}, {"url": "https://biarnes-adrien.medium.com/?source=post_page-----268974d905da--------------------------------", "anchor_text": "See all from Adrien Biarnes"}, {"url": "https://towardsdatascience.com/?source=post_page-----268974d905da--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----268974d905da----0---------------------b142ee59_aa01_4f3e_b061_196645eb04a0-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----268974d905da----0---------------------b142ee59_aa01_4f3e_b061_196645eb04a0-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----268974d905da----0---------------------b142ee59_aa01_4f3e_b061_196645eb04a0-------", "anchor_text": "The PyCoach"}, {"url": "https://artificialcorner.com/?source=read_next_recirc-----268974d905da----0---------------------b142ee59_aa01_4f3e_b061_196645eb04a0-------", "anchor_text": "Artificial Corner"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----268974d905da----0---------------------b142ee59_aa01_4f3e_b061_196645eb04a0-------", "anchor_text": "You\u2019re Using ChatGPT Wrong! Here\u2019s How to Be Ahead of 99% of ChatGPT UsersMaster ChatGPT by learning prompt engineering."}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----268974d905da----0---------------------b142ee59_aa01_4f3e_b061_196645eb04a0-------", "anchor_text": "\u00b77 min read\u00b7Mar 17"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fartificial-corner%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&user=The+PyCoach&userId=fb44e21903f3&source=-----886a50dabc54----0-----------------clap_footer----b142ee59_aa01_4f3e_b061_196645eb04a0-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----268974d905da----0---------------------b142ee59_aa01_4f3e_b061_196645eb04a0-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "276"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&source=-----268974d905da----0-----------------bookmark_preview----b142ee59_aa01_4f3e_b061_196645eb04a0-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/7-evaluation-metrics-for-clustering-algorithms-bdc537ff54d2?source=read_next_recirc-----268974d905da----1---------------------b142ee59_aa01_4f3e_b061_196645eb04a0-------", "anchor_text": ""}, {"url": "https://kayjanwong.medium.com/?source=read_next_recirc-----268974d905da----1---------------------b142ee59_aa01_4f3e_b061_196645eb04a0-------", "anchor_text": ""}, {"url": "https://kayjanwong.medium.com/?source=read_next_recirc-----268974d905da----1---------------------b142ee59_aa01_4f3e_b061_196645eb04a0-------", "anchor_text": "Kay Jan Wong"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----268974d905da----1---------------------b142ee59_aa01_4f3e_b061_196645eb04a0-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/7-evaluation-metrics-for-clustering-algorithms-bdc537ff54d2?source=read_next_recirc-----268974d905da----1---------------------b142ee59_aa01_4f3e_b061_196645eb04a0-------", "anchor_text": "7 Evaluation Metrics for Clustering AlgorithmsIn-depth explanation with Python examples of unsupervised learning evaluation metrics"}, {"url": "https://towardsdatascience.com/7-evaluation-metrics-for-clustering-algorithms-bdc537ff54d2?source=read_next_recirc-----268974d905da----1---------------------b142ee59_aa01_4f3e_b061_196645eb04a0-------", "anchor_text": "\u00b710 min read\u00b7Dec 9, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbdc537ff54d2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F7-evaluation-metrics-for-clustering-algorithms-bdc537ff54d2&user=Kay+Jan+Wong&userId=fee8693930fb&source=-----bdc537ff54d2----1-----------------clap_footer----b142ee59_aa01_4f3e_b061_196645eb04a0-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/7-evaluation-metrics-for-clustering-algorithms-bdc537ff54d2?source=read_next_recirc-----268974d905da----1---------------------b142ee59_aa01_4f3e_b061_196645eb04a0-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbdc537ff54d2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F7-evaluation-metrics-for-clustering-algorithms-bdc537ff54d2&source=-----268974d905da----1-----------------bookmark_preview----b142ee59_aa01_4f3e_b061_196645eb04a0-------", "anchor_text": ""}, {"url": "https://pub.towardsai.net/expectation-maximization-em-clustering-every-data-scientist-should-know-2b47fbd0dbc0?source=read_next_recirc-----268974d905da----0---------------------b142ee59_aa01_4f3e_b061_196645eb04a0-------", "anchor_text": ""}, {"url": "https://anmol3015.medium.com/?source=read_next_recirc-----268974d905da----0---------------------b142ee59_aa01_4f3e_b061_196645eb04a0-------", "anchor_text": ""}, {"url": "https://anmol3015.medium.com/?source=read_next_recirc-----268974d905da----0---------------------b142ee59_aa01_4f3e_b061_196645eb04a0-------", "anchor_text": "Anmol Tomar"}, {"url": "https://pub.towardsai.net/?source=read_next_recirc-----268974d905da----0---------------------b142ee59_aa01_4f3e_b061_196645eb04a0-------", "anchor_text": "Towards AI"}, {"url": "https://pub.towardsai.net/expectation-maximization-em-clustering-every-data-scientist-should-know-2b47fbd0dbc0?source=read_next_recirc-----268974d905da----0---------------------b142ee59_aa01_4f3e_b061_196645eb04a0-------", "anchor_text": "Expectation-Maximization(EM) Clustering: Every Data Scientist Should KnowThe most intuitive explanation of the EM Clustering technique"}, {"url": "https://pub.towardsai.net/expectation-maximization-em-clustering-every-data-scientist-should-know-2b47fbd0dbc0?source=read_next_recirc-----268974d905da----0---------------------b142ee59_aa01_4f3e_b061_196645eb04a0-------", "anchor_text": "\u00b77 min read\u00b7Nov 11, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-artificial-intelligence%2F2b47fbd0dbc0&operation=register&redirect=https%3A%2F%2Fpub.towardsai.net%2Fexpectation-maximization-em-clustering-every-data-scientist-should-know-2b47fbd0dbc0&user=Anmol+Tomar&userId=d80580992695&source=-----2b47fbd0dbc0----0-----------------clap_footer----b142ee59_aa01_4f3e_b061_196645eb04a0-------", "anchor_text": ""}, {"url": "https://pub.towardsai.net/expectation-maximization-em-clustering-every-data-scientist-should-know-2b47fbd0dbc0?source=read_next_recirc-----268974d905da----0---------------------b142ee59_aa01_4f3e_b061_196645eb04a0-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2b47fbd0dbc0&operation=register&redirect=https%3A%2F%2Fpub.towardsai.net%2Fexpectation-maximization-em-clustering-every-data-scientist-should-know-2b47fbd0dbc0&source=-----268974d905da----0-----------------bookmark_preview----b142ee59_aa01_4f3e_b061_196645eb04a0-------", "anchor_text": ""}, {"url": "https://leihua-ye.medium.com/why-data-scientists-should-learn-causal-inference-a70c4ffb4809?source=read_next_recirc-----268974d905da----1---------------------b142ee59_aa01_4f3e_b061_196645eb04a0-------", "anchor_text": ""}, {"url": "https://leihua-ye.medium.com/?source=read_next_recirc-----268974d905da----1---------------------b142ee59_aa01_4f3e_b061_196645eb04a0-------", "anchor_text": ""}, {"url": "https://leihua-ye.medium.com/?source=read_next_recirc-----268974d905da----1---------------------b142ee59_aa01_4f3e_b061_196645eb04a0-------", "anchor_text": "Leihua Ye, PhD"}, {"url": "https://leihua-ye.medium.com/why-data-scientists-should-learn-causal-inference-a70c4ffb4809?source=read_next_recirc-----268974d905da----1---------------------b142ee59_aa01_4f3e_b061_196645eb04a0-------", "anchor_text": "Why Data Scientists Should Learn Causal InferenceClimb up the ladder of causation"}, {"url": "https://leihua-ye.medium.com/why-data-scientists-should-learn-causal-inference-a70c4ffb4809?source=read_next_recirc-----268974d905da----1---------------------b142ee59_aa01_4f3e_b061_196645eb04a0-------", "anchor_text": "\u00b77 min read\u00b7Jul 5, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2Fa70c4ffb4809&operation=register&redirect=https%3A%2F%2Fleihua-ye.medium.com%2Fwhy-data-scientists-should-learn-causal-inference-a70c4ffb4809&user=Leihua+Ye%2C+PhD&userId=4e1d06dd743&source=-----a70c4ffb4809----1-----------------clap_footer----b142ee59_aa01_4f3e_b061_196645eb04a0-------", "anchor_text": ""}, {"url": "https://leihua-ye.medium.com/why-data-scientists-should-learn-causal-inference-a70c4ffb4809?source=read_next_recirc-----268974d905da----1---------------------b142ee59_aa01_4f3e_b061_196645eb04a0-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "9"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa70c4ffb4809&operation=register&redirect=https%3A%2F%2Fleihua-ye.medium.com%2Fwhy-data-scientists-should-learn-causal-inference-a70c4ffb4809&source=-----268974d905da----1-----------------bookmark_preview----b142ee59_aa01_4f3e_b061_196645eb04a0-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/density-based-clustering-dbscan-vs-hdbscan-39e02af990c7?source=read_next_recirc-----268974d905da----2---------------------b142ee59_aa01_4f3e_b061_196645eb04a0-------", "anchor_text": ""}, {"url": "https://thomasdorfer.medium.com/?source=read_next_recirc-----268974d905da----2---------------------b142ee59_aa01_4f3e_b061_196645eb04a0-------", "anchor_text": ""}, {"url": "https://thomasdorfer.medium.com/?source=read_next_recirc-----268974d905da----2---------------------b142ee59_aa01_4f3e_b061_196645eb04a0-------", "anchor_text": "Thomas A Dorfer"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----268974d905da----2---------------------b142ee59_aa01_4f3e_b061_196645eb04a0-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/density-based-clustering-dbscan-vs-hdbscan-39e02af990c7?source=read_next_recirc-----268974d905da----2---------------------b142ee59_aa01_4f3e_b061_196645eb04a0-------", "anchor_text": "Density-Based Clustering: DBSCAN vs. HDBSCANWhich algorithm to choose for your data"}, {"url": "https://towardsdatascience.com/density-based-clustering-dbscan-vs-hdbscan-39e02af990c7?source=read_next_recirc-----268974d905da----2---------------------b142ee59_aa01_4f3e_b061_196645eb04a0-------", "anchor_text": "\u00b75 min read\u00b7Dec 5, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F39e02af990c7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdensity-based-clustering-dbscan-vs-hdbscan-39e02af990c7&user=Thomas+A+Dorfer&userId=7c54f9b62b90&source=-----39e02af990c7----2-----------------clap_footer----b142ee59_aa01_4f3e_b061_196645eb04a0-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/density-based-clustering-dbscan-vs-hdbscan-39e02af990c7?source=read_next_recirc-----268974d905da----2---------------------b142ee59_aa01_4f3e_b061_196645eb04a0-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F39e02af990c7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdensity-based-clustering-dbscan-vs-hdbscan-39e02af990c7&source=-----268974d905da----2-----------------bookmark_preview----b142ee59_aa01_4f3e_b061_196645eb04a0-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/using-causal-ml-instead-of-a-b-testing-eeb1067d7fc0?source=read_next_recirc-----268974d905da----3---------------------b142ee59_aa01_4f3e_b061_196645eb04a0-------", "anchor_text": ""}, {"url": "https://medium.com/@mazzanti.sam?source=read_next_recirc-----268974d905da----3---------------------b142ee59_aa01_4f3e_b061_196645eb04a0-------", "anchor_text": ""}, {"url": "https://medium.com/@mazzanti.sam?source=read_next_recirc-----268974d905da----3---------------------b142ee59_aa01_4f3e_b061_196645eb04a0-------", "anchor_text": "Samuele Mazzanti"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----268974d905da----3---------------------b142ee59_aa01_4f3e_b061_196645eb04a0-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/using-causal-ml-instead-of-a-b-testing-eeb1067d7fc0?source=read_next_recirc-----268974d905da----3---------------------b142ee59_aa01_4f3e_b061_196645eb04a0-------", "anchor_text": "Using Causal ML Instead of A/B TestingIn complex environments, Causal ML is a powerful tool because it is more flexible than A/B Testing, and it doesn\u2019t require strong\u2026"}, {"url": "https://towardsdatascience.com/using-causal-ml-instead-of-a-b-testing-eeb1067d7fc0?source=read_next_recirc-----268974d905da----3---------------------b142ee59_aa01_4f3e_b061_196645eb04a0-------", "anchor_text": "\u00b79 min read\u00b7Nov 29, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Feeb1067d7fc0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-causal-ml-instead-of-a-b-testing-eeb1067d7fc0&user=Samuele+Mazzanti&userId=e16f3bb86e03&source=-----eeb1067d7fc0----3-----------------clap_footer----b142ee59_aa01_4f3e_b061_196645eb04a0-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/using-causal-ml-instead-of-a-b-testing-eeb1067d7fc0?source=read_next_recirc-----268974d905da----3---------------------b142ee59_aa01_4f3e_b061_196645eb04a0-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "15"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Feeb1067d7fc0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-causal-ml-instead-of-a-b-testing-eeb1067d7fc0&source=-----268974d905da----3-----------------bookmark_preview----b142ee59_aa01_4f3e_b061_196645eb04a0-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----268974d905da--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----268974d905da--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----268974d905da--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----268974d905da--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----268974d905da--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----268974d905da--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----268974d905da--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----268974d905da--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----268974d905da--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----268974d905da--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}