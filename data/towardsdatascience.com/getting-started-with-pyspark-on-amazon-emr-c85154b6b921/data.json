{"url": "https://towardsdatascience.com/getting-started-with-pyspark-on-amazon-emr-c85154b6b921", "time": 1682997289.8195012, "path": "towardsdatascience.com/getting-started-with-pyspark-on-amazon-emr-c85154b6b921/", "webpage": {"metadata": {"title": "Getting Started with PySpark on AWS EMR | by Brent Lemieux | Towards Data Science", "h1": "Getting Started with PySpark on AWS EMR", "description": "Data Pipelines with PySpark and AWS EMR is a multi-part series. This is part 1 of 2. Check out part 2 if you\u2019re looking for guidance on how to run a data pipeline as a product job. If you have been\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/production-data-processing-with-apache-spark-96a58dfd3fe7", "anchor_text": "part 2", "paragraph_index": 0}, {"url": "https://spark.apache.org/", "anchor_text": "docs", "paragraph_index": 5}, {"url": "https://aws.amazon.com/s3/", "anchor_text": "Amazon S3", "paragraph_index": 9}, {"url": "https://aws.amazon.com/emr/", "anchor_text": "Amazon EMR", "paragraph_index": 10}, {"url": "https://aws.amazon.com/?source=post_page---------------------------", "anchor_text": "AWS account", "paragraph_index": 11}, {"url": "https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users_create.html#id_users_create_console", "anchor_text": "create an IAM user", "paragraph_index": 11}, {"url": "https://docs.aws.amazon.com/IAM/latest/UserGuide/id_root-user.html", "anchor_text": "delete your root access keys", "paragraph_index": 11}, {"url": "https://towardsdatascience.com/quick-setup-guide-for-your-aws-account-423dadb61f99", "anchor_text": "AWS Quick Setup Guide", "paragraph_index": 12}, {"url": "https://registry.opendata.aws/amazon-reviews/", "anchor_text": "Amazon Customer Reviews Dataset", "paragraph_index": 33}, {"url": "https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PySpark_SQL_Cheat_Sheet_Python.pdf", "anchor_text": "excellent cheat sheet from DataCamp", "paragraph_index": 35}, {"url": "https://medium.com/@brent_64035/production-data-processing-with-apache-spark-96a58dfd3fe7", "anchor_text": "create a production data processing workflow", "paragraph_index": 36}, {"url": "https://towardsdatascience.com/set-up-an-airflow-environment-on-aws-in-minutes-f934cf10ec54", "anchor_text": "schedule your Spark job on Airflow", "paragraph_index": 36}, {"url": "https://www.linkedin.com/in/brent-lemieux/?source=post_page---------------------------", "anchor_text": "LinkedIn", "paragraph_index": 38}], "all_paragraphs": ["Data Pipelines with PySpark and AWS EMR is a multi-part series. This is part 1 of 2. Check out part 2 if you\u2019re looking for guidance on how to run a data pipeline as a product job.", "If you have been following business and technology trends over the past decade, you\u2019re likely aware that the amount of data organizations are generating has skyrocketed. Businesses are eager to use all of this data to gain insights and improve processes; however, \u201cbig data\u201d means big challenges.", "Entirely new technologies had to be invented to handle larger and larger datasets. These new technologies include the offerings of cloud computing service providers like Amazon Web Services (AWS) and open-source large-scale data processing engines like Apache Spark. As the amount of data generated continues to soar, aspiring data scientists who can use these \u201cbig data\u201d tools will stand out from their peers in the market.", "In this guide, I will teach you how to get started processing data using PySpark on an Amazon EMR cluster. This tutorial is for current and aspiring data scientists who are familiar with Python but beginners at using Spark.", "Spark is great for processing large datasets for everyday data science tasks like exploratory data analysis and feature engineering. It can also be used to implement many popular machine learning algorithms at scale.", "From the docs, \u201cApache Spark is a unified analytics engine for large-scale data processing.\u201d Spark\u2019s engine allows you to parallelize large data processing tasks on a distributed cluster. A Spark cluster contains a master node that acts as the central coordinator and several worker nodes that handle the tasks doled out by the master node.", "We\u2019ll be using Python in this guide, but Spark developers can also use Scala or Java. The pyspark.sql module contains syntax that users of Pandas and SQL will find familiar. The pyspark.ml module can be used to implement many popular machine learning models.", "Spark uses lazy evaluation, which means it doesn\u2019t do any work until you ask for a result. This way, the engine can decide the most optimal way to execute your DAG (directed acyclical graph \u2014 or list of operations you\u2019ve specified). When I define an operation \u2014 new_df = df.filter(df.user_action == 'ClickAddToCart') \u2014 Spark adds the operation to my DAG but doesn\u2019t execute. Once I ask for a result \u2014 new_df.collect() \u2014 Spark executes my filter and any other operations I specify.", "A quick note before we proceed: using distributed cloud technologies can be frustrating. At first, you\u2019ll likely find Spark error messages to be incomprehensible and difficult to debug. I encourage you to stick with it! Read the errors. Learn what parts are informative and google it. I can\u2019t promise that you\u2019ll eventually stop banging your head on the keyboard, but it will get easier. It wouldn\u2019t be a great way to differentiate yourself from others if there wasn\u2019t a learning curve!", "Amazon S3 (Simple Storage Service) is an easy and relatively cheap way to store a large amount of data securely. A typical Spark workflow is to read data from an S3 bucket or another source, perform some transformations, and write the processed data back to another S3 bucket.", "Amazon EMR (Elastic Map Reduce) is a big data platform that synchronizes multiple nodes into a scaleable cluster that can process large amounts of data. As mentioned above, we submit our jobs to the master node of our cluster, which figures out the optimal way to run it. The master node then doles out tasks to the worker nodes accordingly.", "First things first, create an AWS account and sign in to the console. I recommend taking the time now to create an IAM user and delete your root access keys.", "UPDATE: I\u2019ve created an AWS Quick Setup Guide walking you through how to create IAM users and roles, create an S3 bucket, and configure the AWS CLI.", "I\u2019ll be using the region US West (Oregon) for this tutorial. You can change your region with the drop-down in the top right:", "Warning on AWS expenses: You\u2019ll need to provide a credit card to create your account. To keep costs minimal, don\u2019t forget to terminate your EMR cluster after you are done using it. For this guide, we\u2019ll be using m5.xlarge instances, which at the time of writing cost $0.192 per hour. Also, there is a small monthly charge to host data on Amazon S3 \u2014 this cost will go up with the amount of data you host. To avoid continuing costs, delete your bucket after using it.", "To install useful packages on all of the nodes of our cluster, we\u2019ll need to create the file emr_bootstrap.sh and add it to a bucket on S3.", "Navigate to S3 by searching for it using the \u201cFind Services\u201d search box in the console:", "Click \u201cCreate Bucket\u201d, fill in the \u201cBucket name\u201d field, and click \u201cCreate\u201d:", "Click \u201cUpload\u201d, \u201cAdd files\u201d and open the file you created emr_bootstrap.sh. Click \u201cUpload\u201d to upload the file.", "Navigate to EC2 from the homepage of your console:", "Click \u201cCreate Key Pair\u201d then enter a name and click \u201cCreate\u201d.", "Your file emr-key.pem should download automatically. Store it in a directory you\u2019ll remember. I put my .pem files in ~/.ssh. Be sure to keep this file out of your GitHub repos, or any other public places, to keep your AWS resources more secure.", "Navigate to EMR from your console, click \u201cCreate Cluster\u201d, then \u201cGo to advanced options\u201d.", "Make the following selections, choosing the latest release from the \u201cRelease\u201d dropdown and checking \u201cSpark\u201d, then click \u201cNext\u201d.", "Select the \u201cDefault in us-west-2a\u201d option \u201cEC2 Subnet\u201d dropdown, change your instance types to m5.xlarge to use the latest generation of general-purpose instances, then click \u201cNext\u201d.", "Name your cluster, add emr_bootstrap.sh as a bootstrap action, then click \u201cNext\u201d. The script location of your bootstrap action will be the S3 file-path where you uploaded emr_bootstrap.sh to earlier in the tutorial. Your bootstrap action will install the packages you specified on each node in your cluster.", "Select the key pair you created earlier and click \u201cCreate cluster\u201d. Your cluster will take a few minutes to start, but once it reaches \u201cWaiting\u201d, you are ready to move on to the next step \u2014 connecting to your cluster with a Jupyter notebook.", "Navigate to \u201cNotebooks\u201d in the left panel. Click \u201cCreate notebook\u201d and follow the step below.", "Name your notebook and choose the cluster you just created.", "Once your notebook is \u201cReady\u201d, click \u201cOpen\u201d. You\u2019re now ready to start running Spark on the cloud!", "In the first cell of your notebook, import the packages you intend to use. For example:", "You should get the following output:", "Note: a SparkSession is automatically defined in the notebook as spark \u2014 you will have to define this yourself when creating scripts to submit as Spark jobs.", "Next, let\u2019s import some data from S3. We\u2019ll use data Amazon has made available in a public bucket. Let\u2019s look at the Amazon Customer Reviews Dataset. In particular, let\u2019s look at book reviews:", "The /*.parquet syntax in input_path tells Spark to read all .parquet files in the s3://amazon-reviews-pds/parquet/product_category=Books/ bucket directory.", "I\u2019ll be coming out with a tutorial on data wrangling with the PySpark DataFrame API shortly, but for now, check out this excellent cheat sheet from DataCamp to get started.", "Once you\u2019ve tested your PySpark code in a Jupyter notebook, move it to a script and create a production data processing workflow with Spark and the AWS Command Line Interface. Then, you\u2019re ready to schedule your Spark job on Airflow.", "Thank you for reading! Please let me know if you liked the article or if you have any critiques. If this guide was useful to you, be sure to follow me and so you won\u2019t miss any of my future articles.", "If you need help with a data project or want to say hi, connect with and message me on LinkedIn. Cheers!", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fc85154b6b921&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgetting-started-with-pyspark-on-amazon-emr-c85154b6b921&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgetting-started-with-pyspark-on-amazon-emr-c85154b6b921&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgetting-started-with-pyspark-on-amazon-emr-c85154b6b921&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgetting-started-with-pyspark-on-amazon-emr-c85154b6b921&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----c85154b6b921--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c85154b6b921--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://brentlemieux.medium.com/?source=post_page-----c85154b6b921--------------------------------", "anchor_text": ""}, {"url": "https://brentlemieux.medium.com/?source=post_page-----c85154b6b921--------------------------------", "anchor_text": "Brent Lemieux"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F621935543&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgetting-started-with-pyspark-on-amazon-emr-c85154b6b921&user=Brent+Lemieux&userId=621935543&source=post_page-621935543----c85154b6b921---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc85154b6b921&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgetting-started-with-pyspark-on-amazon-emr-c85154b6b921&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc85154b6b921&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgetting-started-with-pyspark-on-amazon-emr-c85154b6b921&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/production-data-processing-with-apache-spark-96a58dfd3fe7", "anchor_text": "part 2"}, {"url": "https://towardsdatascience.com/production-data-processing-with-apache-spark-96a58dfd3fe7", "anchor_text": "Production Data Processing with PySpark on AWS EMR"}, {"url": "https://spark.apache.org/", "anchor_text": "docs"}, {"url": "https://aws.amazon.com/s3/", "anchor_text": "Amazon S3"}, {"url": "https://aws.amazon.com/emr/", "anchor_text": "Amazon EMR"}, {"url": "https://aws.amazon.com/?source=post_page---------------------------", "anchor_text": "AWS account"}, {"url": "https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users_create.html#id_users_create_console", "anchor_text": "create an IAM user"}, {"url": "https://docs.aws.amazon.com/IAM/latest/UserGuide/id_root-user.html", "anchor_text": "delete your root access keys"}, {"url": "https://towardsdatascience.com/quick-setup-guide-for-your-aws-account-423dadb61f99", "anchor_text": "AWS Quick Setup Guide"}, {"url": "https://registry.opendata.aws/amazon-reviews/", "anchor_text": "Amazon Customer Reviews Dataset"}, {"url": "https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PySpark_SQL_Cheat_Sheet_Python.pdf", "anchor_text": "excellent cheat sheet from DataCamp"}, {"url": "https://medium.com/@brent_64035/production-data-processing-with-apache-spark-96a58dfd3fe7", "anchor_text": "create a production data processing workflow"}, {"url": "https://towardsdatascience.com/set-up-an-airflow-environment-on-aws-in-minutes-f934cf10ec54", "anchor_text": "schedule your Spark job on Airflow"}, {"url": "https://www.linkedin.com/in/brent-lemieux/?source=post_page---------------------------", "anchor_text": "LinkedIn"}, {"url": "https://medium.com/tag/spark?source=post_page-----c85154b6b921---------------spark-----------------", "anchor_text": "Spark"}, {"url": "https://medium.com/tag/data-science?source=post_page-----c85154b6b921---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/aws?source=post_page-----c85154b6b921---------------aws-----------------", "anchor_text": "AWS"}, {"url": "https://medium.com/tag/data?source=post_page-----c85154b6b921---------------data-----------------", "anchor_text": "Data"}, {"url": "https://medium.com/tag/pyspark?source=post_page-----c85154b6b921---------------pyspark-----------------", "anchor_text": "Pyspark"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc85154b6b921&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgetting-started-with-pyspark-on-amazon-emr-c85154b6b921&user=Brent+Lemieux&userId=621935543&source=-----c85154b6b921---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc85154b6b921&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgetting-started-with-pyspark-on-amazon-emr-c85154b6b921&user=Brent+Lemieux&userId=621935543&source=-----c85154b6b921---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc85154b6b921&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgetting-started-with-pyspark-on-amazon-emr-c85154b6b921&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c85154b6b921--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fc85154b6b921&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgetting-started-with-pyspark-on-amazon-emr-c85154b6b921&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----c85154b6b921---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----c85154b6b921--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----c85154b6b921--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----c85154b6b921--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----c85154b6b921--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----c85154b6b921--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----c85154b6b921--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----c85154b6b921--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----c85154b6b921--------------------------------", "anchor_text": ""}, {"url": "https://brentlemieux.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://brentlemieux.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Brent Lemieux"}, {"url": "https://brentlemieux.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "348 Followers"}, {"url": "https://www.linkedin.com/in/brent-lemieux/", "anchor_text": "https://www.linkedin.com/in/brent-lemieux/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F621935543&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgetting-started-with-pyspark-on-amazon-emr-c85154b6b921&user=Brent+Lemieux&userId=621935543&source=post_page-621935543--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fb7be1b858b8c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgetting-started-with-pyspark-on-amazon-emr-c85154b6b921&newsletterV3=621935543&newsletterV3Id=b7be1b858b8c&user=Brent+Lemieux&userId=621935543&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}