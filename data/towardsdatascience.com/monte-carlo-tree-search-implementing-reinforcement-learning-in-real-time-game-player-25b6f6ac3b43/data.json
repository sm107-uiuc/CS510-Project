{"url": "https://towardsdatascience.com/monte-carlo-tree-search-implementing-reinforcement-learning-in-real-time-game-player-25b6f6ac3b43", "time": 1683013456.958434, "path": "towardsdatascience.com/monte-carlo-tree-search-implementing-reinforcement-learning-in-real-time-game-player-25b6f6ac3b43/", "webpage": {"metadata": {"title": "Monte Carlo Tree Search: Implementing Reinforcement Learning in Real-Time Game Player | Part 2 | by Masoud Masoumi Moghadam | Towards Data Science", "h1": "Monte Carlo Tree Search: Implementing Reinforcement Learning in Real-Time Game Player | Part 2", "description": "In the previous article, we covered the fundamental concepts of reinforcement learning. In this article we will first discuss how Monte Carlo methods are combined with tree search to shape monte carlo tree search. then we come to understanding of how MCTS can use randomness to find near optimal solutions in minimal time possible"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/monte-carlo-tree-search-a-case-study-along-with-implementation-part-1-ebc7753a5a3b", "anchor_text": "previous article", "paragraph_index": 0}, {"url": "https://github.com/masouduut94/MCTS-agent-python", "anchor_text": "link", "paragraph_index": 3}, {"url": "https://github.com/masouduut94/MCTS-agent-cythonized", "anchor_text": "here", "paragraph_index": 3}, {"url": "https://www.linkedin.com/in/masoud-masoumi-moghadam/", "anchor_text": "linkedin", "paragraph_index": 43}, {"url": "https://stackoverflow.com/users/6118987/masoud-masoumi-moghadam", "anchor_text": "StackoverFlow", "paragraph_index": 43}], "all_paragraphs": ["In the previous article, we covered the fundamental concepts of reinforcement learning and closed the article with these two key questions:", "1 \u2014 how can we find the best move among others if we cannot process all the successive states one by one due to limited amount of time?", "2 \u2014 how do we map the task of finding best move to long-term rewards if we are limited in terms of computational resources and time?", "In this article, to answer these questions, we go through the Monte Carlo Tree Search fundamentals. Since in the next articles, we will implement this algorithm on \u201cHEX\u201d board game, I try to explain the concepts through examples in this board game environment.If you\u2019re more interested in the code, find it in this link. There is also a more optimized version which is applicable on linux due to utilizing cython and you can find it in here.", "2 \u2014 Exploration and Exploitation Trade-off", "3 \u2014 HEX: A Classic Board Game", "4 \u2014 Algorithm structure: Selection and Expansion", "Monte Carlo method was coined by Stanislaw Ulam for the first time after applying statistical approach \u201cThe Monte Carlo method\u201d. The concept is simple. Using randomness to solve problems that might be deterministic in principle. For example, in mathematics, it is used for estimating the integral when we cannot directly calculate it. Also in this image, you can see how we can calculate pi based on Monte-Carlo simulations.", "The image above indicates the fact that in monte carlo method the more samples we gather, more accurate estimation of target value we will attain.", "We use Monte Carlo method to estimate the quality of states stochastically based on simulations when we cannot process through all the states. Each simulation is a self-play that traverses the game tree from current state until a leaf state (end of game) is reached.", "So this algorithm is just perfect to our problem.", "- Since it samples the future state-action space, it can estimate near optimal action in current state by keeping computation effort low (which addresses the first question).", "- Also the fact that it chooses the best action based on long-term rewards (rewarding based on the result in tree leaves) answers the second question.", "This process is exactly like when a human wants to estimate the future action to come up with the best possible action in the game of chess. He thinks simulates various games (from current state to the last possible state of future) based on self-play in his/her mind and chooses the one that has the best overall results.", "Monte Carlo Tree Search (MCTS), which combines monte carlo methods with tree search, is a method for finding optimal decisions in a given domain by taking random samples in the decision space and building a search tree according to the results.", "Before we explain the algorithm structure, we should first discuss the exploration and exploitation trade-off.", "As explained, In reinforcement learning, an agent always aims to achieve an optimal strategy by repeatedly using the best actions that it has found in that problem (remember the chess example in the previous article). However, there is a probability that the current best action is not actually optimal. As such it will continue to evaluate alternatives periodically during the learning phase by executing them instead of the perceived optimal. In RL terms, this is known as exploration exploitation trade-off. All of the algorithms in RL (MCTS as well) are trying to balance the exploration-exploitation trade-off.", "I think this video best explains the concept of exploration-exploitation:", "Now it\u2019s time to get to know the Hex game. It has simple rules:", "Now let`s go through the algorithm structure.", "In this step, agent takes the current state of the game and selects a node (Each node represents the state resulted by choosing an action) in tree and traverses the tree. Each move in each state is assigned with two parameters namely as total rollouts and wins per rollouts (they will be covered in rollout section).", "The strategy to select optimal node among other nodes really matters. Upper Confidence Bound applied to Trees (UCT) is the simplest and yet effective strategy to select optimal node. This strategy is designed to balance the exploitation-exploration trade-off. This is UCT formula:", "In this formula i indicates i-th node in children nodes. W is the number of wins per rollouts and n is the number of all rollouts. This part of formula represents the exploitation.Cis the exploration coefficient and it\u2019s a constant in range of [0,1]. This parameter indicates how much agent have to favor unexplored nodes. t is the number of rollouts in parent node. the second term represents the exploration term.", "Let\u2019s go through an example in order to have all information provided to sink in. Look at the image below:", "Consider the action C3 in depth 2 (black turn). number of total simulations (n _i in formula) is 2 and number of rollouts won (w_i in formula) is 1. In the formula t would be the number of rollouts for parent node in depth 1 (which in here indicates to move B2 in white turn) and that is 4. As you see selection phase stops in the depth where we have an unvisited node. Then in the expansion phase when we visit B1 in depth 4, we add it to tree.", "In this step, based on predefined policy (like completely random selection) we select actions until we reach a terminal state. The result of game for current player is either 0 (if it loses the rollout) or 1 (if it wins the rollout) at the terminal state. In the game of HEX, the terminal state is always reachable and the result of game is loss or win (no draws). But in games like chess we might get in an infinite loop due to the extensibility of chess branch factor and depth of search tree.", "In the image above, after that black player chose B1 in expansion step, in the simulation step a rollout is started to terminal state of game.", "In here, we chose random actions to reach the terminal state of the game. In terminal state as you see, white player has won the game by connecting the left to right with its stones. Now it\u2019s time to use this information in backpropagation part.", "In this part, we update the statistics (rollout number and the number of wins per total rollouts) in the nodes which we traversed in tree for selection and expansion parts.", "During backpropagation we need to update the rollout numbers and wins/losses stats of nodes. Only thing we need is to figure out the player who won the game in rollout (e.g. white player in figure 4).", "For the figure 4, since the black player is the winner (who chose the action in terminal state), all the states resulted by black player actions are rewarded by 1 and states which resulted by white player actions are given 0 reward (we can choose punishment by set it to -1).", "For all states (tree nodes selected through step 1), total rollouts number increases by one as the figure 6 displays.", "These steps keep repeating until a predefined condition ends the loop (like time limit).", "1 \u2014 MCTS is a simple algorithm to implement.", "2 \u2014 Monte Carlo Tree Search is a heuristic algorithm. MCTS can operate effectively without any knowledge in the particular domain, apart from the rules and end conditions, and can find its own moves and learn from them by playing random playouts.", "3 \u2014 The MCTS can be saved in any intermediate state and that state can be used in future use cases whenever required.", "4 \u2014 MCTS supports asymmetric expansion of the search tree based on the circumstances in which it is operating.", "1 \u2014 As the tree growth becomes rapid after a few iterations, it might require a huge amount of memory.", "2 \u2014 There is a bit of a reliability issue with Monte Carlo Tree Search. In certain scenarios, there might be a single branch or path, that might lead to loss against the opposition when implemented for those turn-based games. This is mainly due to the vast amount of combinations and each of the nodes might not be visited enough number of times to understand its result or outcome in the long run.", "3 \u2014 MCTS algorithm needs a huge number of iterations to be able to effectively decide the most efficient path. So, there is a bit of a speed issue there.", "4 \u2014 MCTS can return a recommended move at any time because the statistics about the simulated games are constantly updated. The recommended moves aren\u2019t great when the algorithm starts, but they continually improve as the algorithm runs.", "Now we figured out how the MCTS algorithm can efficiently use randomness to sample all the possible scenarios and come up with the best action over its simulations. The quality of action chosen by MCTS in each time lies in the fact that how well it can handle the exploration and exploitation in the environment.", "OK, now that we covered necessary theoretical concepts so far, we\u2019re good to go to the next level with getting our hands dirty with code. In the next article, first, we\u2019re going to describe the whole framework and necessary modules to implement, then we will implement the basic MCTS with UCT. After that, we will improve the framework by adding more functionality to our code.", "If you are interested in topics and have questions , I am always available and would love to listen. Find me in linkedin and StackoverFlow.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F25b6f6ac3b43&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmonte-carlo-tree-search-implementing-reinforcement-learning-in-real-time-game-player-25b6f6ac3b43&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmonte-carlo-tree-search-implementing-reinforcement-learning-in-real-time-game-player-25b6f6ac3b43&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmonte-carlo-tree-search-implementing-reinforcement-learning-in-real-time-game-player-25b6f6ac3b43&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmonte-carlo-tree-search-implementing-reinforcement-learning-in-real-time-game-player-25b6f6ac3b43&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----25b6f6ac3b43--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----25b6f6ac3b43--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://m-m-moghadam.medium.com/?source=post_page-----25b6f6ac3b43--------------------------------", "anchor_text": ""}, {"url": "https://m-m-moghadam.medium.com/?source=post_page-----25b6f6ac3b43--------------------------------", "anchor_text": "Masoud Masoumi Moghadam"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5c9c75820911&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmonte-carlo-tree-search-implementing-reinforcement-learning-in-real-time-game-player-25b6f6ac3b43&user=Masoud+Masoumi+Moghadam&userId=5c9c75820911&source=post_page-5c9c75820911----25b6f6ac3b43---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F25b6f6ac3b43&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmonte-carlo-tree-search-implementing-reinforcement-learning-in-real-time-game-player-25b6f6ac3b43&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F25b6f6ac3b43&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmonte-carlo-tree-search-implementing-reinforcement-learning-in-real-time-game-player-25b6f6ac3b43&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@polarmermaid?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Anne Nyg\u00e5rd"}, {"url": "https://unsplash.com/?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://towardsdatascience.com/monte-carlo-tree-search-a-case-study-along-with-implementation-part-1-ebc7753a5a3b", "anchor_text": "previous article"}, {"url": "https://github.com/masouduut94/MCTS-agent-python", "anchor_text": "link"}, {"url": "https://github.com/masouduut94/MCTS-agent-cythonized", "anchor_text": "here"}, {"url": "https://math.stackexchange.com/questions/515302/please-explain-monte-carlo-method", "anchor_text": "source"}, {"url": "https://ieeexplore.ieee.org/abstract/document/5551182", "anchor_text": "[1]"}, {"url": "https://www.linkedin.com/in/masoud-masoumi-moghadam/", "anchor_text": "linkedin"}, {"url": "https://stackoverflow.com/users/6118987/masoud-masoumi-moghadam", "anchor_text": "StackoverFlow"}, {"url": "https://medium.com/tag/ai?source=post_page-----25b6f6ac3b43---------------ai-----------------", "anchor_text": "AI"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----25b6f6ac3b43---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/computer-science?source=post_page-----25b6f6ac3b43---------------computer_science-----------------", "anchor_text": "Computer Science"}, {"url": "https://medium.com/tag/python?source=post_page-----25b6f6ac3b43---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/monte-carlo-tree-search?source=post_page-----25b6f6ac3b43---------------monte_carlo_tree_search-----------------", "anchor_text": "Monte Carlo Tree Search"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F25b6f6ac3b43&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmonte-carlo-tree-search-implementing-reinforcement-learning-in-real-time-game-player-25b6f6ac3b43&user=Masoud+Masoumi+Moghadam&userId=5c9c75820911&source=-----25b6f6ac3b43---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F25b6f6ac3b43&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmonte-carlo-tree-search-implementing-reinforcement-learning-in-real-time-game-player-25b6f6ac3b43&user=Masoud+Masoumi+Moghadam&userId=5c9c75820911&source=-----25b6f6ac3b43---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F25b6f6ac3b43&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmonte-carlo-tree-search-implementing-reinforcement-learning-in-real-time-game-player-25b6f6ac3b43&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----25b6f6ac3b43--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F25b6f6ac3b43&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmonte-carlo-tree-search-implementing-reinforcement-learning-in-real-time-game-player-25b6f6ac3b43&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----25b6f6ac3b43---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----25b6f6ac3b43--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----25b6f6ac3b43--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----25b6f6ac3b43--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----25b6f6ac3b43--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----25b6f6ac3b43--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----25b6f6ac3b43--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----25b6f6ac3b43--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----25b6f6ac3b43--------------------------------", "anchor_text": ""}, {"url": "https://m-m-moghadam.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://m-m-moghadam.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Masoud Masoumi Moghadam"}, {"url": "https://m-m-moghadam.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "63 Followers"}, {"url": "https://www.linkedin.com/in/masoud-masoumi-moghadam/", "anchor_text": "https://www.linkedin.com/in/masoud-masoumi-moghadam/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5c9c75820911&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmonte-carlo-tree-search-implementing-reinforcement-learning-in-real-time-game-player-25b6f6ac3b43&user=Masoud+Masoumi+Moghadam&userId=5c9c75820911&source=post_page-5c9c75820911--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F181bb37885f2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmonte-carlo-tree-search-implementing-reinforcement-learning-in-real-time-game-player-25b6f6ac3b43&newsletterV3=5c9c75820911&newsletterV3Id=181bb37885f2&user=Masoud+Masoumi+Moghadam&userId=5c9c75820911&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}