{"url": "https://towardsdatascience.com/deep-double-descent-when-more-data-and-bigger-models-are-a-bad-thing-3a3f108d5538", "time": 1683001848.468545, "path": "towardsdatascience.com/deep-double-descent-when-more-data-and-bigger-models-are-a-bad-thing-3a3f108d5538/", "webpage": {"metadata": {"title": "Deep Double Descent: when more data is a bad thing | by Rui Aguiar | Towards Data Science", "h1": "Deep Double Descent: when more data is a bad thing", "description": "I recently came across a very interesting paper written at OpenAI on the topic of deep double descent. The paper touches at the very nature of training machine learning systems and model complexity\u2026"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/abs/1912.02292", "anchor_text": "very interesting paper", "paragraph_index": 0}, {"url": "https://papers.nips.cc/paper/1925-occams-razor.pdf", "anchor_text": "Occam\u2019s razor", "paragraph_index": 1}, {"url": "https://arxiv.org/pdf/1912.02292.pdf", "anchor_text": "paper", "paragraph_index": 13}, {"url": "https://openai.com/blog/deep-double-descent/", "anchor_text": "associated summarization", "paragraph_index": 13}], "all_paragraphs": ["I recently came across a very interesting paper written at OpenAI on the topic of deep double descent. The paper touches at the very nature of training machine learning systems and model complexity. I hope to summarize the points made in this paper in an approachable way in this post, and to advance the discussion of the tradeoffs between model size, data quantity and regularization.", "There\u2019s a fundamental conflict between statistical learning and modern ML theory. Classical statistics says that too large models are bad. This is due to the fact that complicated models are more prone to overfitting. In fact, one powerful theorem frequently applied in classical statistics is Occam\u2019s razor, which in essence states that the simplest explanation is usually right.", "This can be clearly explained with a visualization.", "Despite this seemingly obvious tradeoff between complexity and generalizability, you often see that modern ML theory states that bigger models are better. The interesting thing about this statement is that, for the most part, it seems to be working. Research from some of the top AI research teams in the world, including teams from Google and Microsoft indicate that deeper models are not saturating. In fact, by implementing careful regularization and early stopping, it seems that often, the best way to improve your model\u2019s performance by a few points is to simply add more layers or collect more training data.", "The focus of the OpenAI paper provides a practical investigation of the contradiction between classical statistics and modern ML theory.", "Deep double descent is the phenomenon where performance improves, then gets worse as the model begins to overfit, and then finally improves more with increasing model size, data size, or training time. The graph above illustrates this behavior graphically.", "The deep double descent phenomenon has several implications regarding the complexity of models, quantity of data and training time.", "When experimenting with ResNet18, the OpenAI researchers found an interesting note about the tradeoff between bias and variance. Before a model\u2019s complexity passed the interpolation threshold, or the point at which the model is are just large enough to fit the training set, larger models had higher test error. However, after the model\u2019s complexity allowed it to fit the entire training set, larger models with more data started performing far better.", "It seems that there is a region of complexity where models are more prone to overfitting, but if enough complexity is captured in the model, the larger the better.", "Interestingly, for models below the interpolation threshold, it seems that more training data actually produces worse performance on the test set. However, as the model becomes more complex, this tradeoff reverses, and the modern wisdom of \u201cmore data is better\u201d begins to apply again.", "One working hypothesis is that it\u2019s possible that less complicated models cannot capture everything needed in a too large training set, and therefore cannot generalize to unseen data well. As the model becomes complex enough, however, it is able to overcome this limitation.", "In the graphs above with respect to the number of epochs trained, training and testing error first decreases sharply as the number of epochs increases. Eventually, test error starts increasing as the model begins to overfit. Finally, test error decreases again as the overfitting is, somewhat miraculously, undone.", "In the paper, the researchers call this epoch-wise double descent. They also note that this peak in test error is just at the interpolation threshold. The intuition here is that if a model is not very complex, there is only one model that fits the train data best. If the models fits noisy data, its\u2019 performance will dip dramatically. However, if a model is complex enough to pass the interpolation threshold, there are several models that fit the train set and the test set, and as you train longer, that allows you to approximate one of these models. The reason that this occurs is an open research question, and fundamentally important to the future of training deep neural networks.", "If this research question is something that interests you, take a look at the paper and associated summarization, which inspired this post.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Interested in technology, humans and the hard problems in life."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F3a3f108d5538&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-double-descent-when-more-data-and-bigger-models-are-a-bad-thing-3a3f108d5538&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-double-descent-when-more-data-and-bigger-models-are-a-bad-thing-3a3f108d5538&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-double-descent-when-more-data-and-bigger-models-are-a-bad-thing-3a3f108d5538&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-double-descent-when-more-data-and-bigger-models-are-a-bad-thing-3a3f108d5538&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----3a3f108d5538--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3a3f108d5538--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@raguiar2?source=post_page-----3a3f108d5538--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@raguiar2?source=post_page-----3a3f108d5538--------------------------------", "anchor_text": "Rui Aguiar"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F82e0b7141d5c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-double-descent-when-more-data-and-bigger-models-are-a-bad-thing-3a3f108d5538&user=Rui+Aguiar&userId=82e0b7141d5c&source=post_page-82e0b7141d5c----3a3f108d5538---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3a3f108d5538&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-double-descent-when-more-data-and-bigger-models-are-a-bad-thing-3a3f108d5538&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3a3f108d5538&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-double-descent-when-more-data-and-bigger-models-are-a-bad-thing-3a3f108d5538&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@franki?utm_source=medium&utm_medium=referral", "anchor_text": "Franki Chamaki"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://arxiv.org/abs/1912.02292", "anchor_text": "very interesting paper"}, {"url": "https://papers.nips.cc/paper/1925-occams-razor.pdf", "anchor_text": "Occam\u2019s razor"}, {"url": "https://arxiv.org/pdf/1912.02292.pdf", "anchor_text": "paper"}, {"url": "https://openai.com/blog/deep-double-descent/", "anchor_text": "associated summarization"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----3a3f108d5538---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----3a3f108d5538---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----3a3f108d5538---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----3a3f108d5538---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/data?source=post_page-----3a3f108d5538---------------data-----------------", "anchor_text": "Data"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3a3f108d5538&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-double-descent-when-more-data-and-bigger-models-are-a-bad-thing-3a3f108d5538&user=Rui+Aguiar&userId=82e0b7141d5c&source=-----3a3f108d5538---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3a3f108d5538&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-double-descent-when-more-data-and-bigger-models-are-a-bad-thing-3a3f108d5538&user=Rui+Aguiar&userId=82e0b7141d5c&source=-----3a3f108d5538---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3a3f108d5538&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-double-descent-when-more-data-and-bigger-models-are-a-bad-thing-3a3f108d5538&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3a3f108d5538--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F3a3f108d5538&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-double-descent-when-more-data-and-bigger-models-are-a-bad-thing-3a3f108d5538&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----3a3f108d5538---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----3a3f108d5538--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----3a3f108d5538--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----3a3f108d5538--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----3a3f108d5538--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----3a3f108d5538--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----3a3f108d5538--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----3a3f108d5538--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----3a3f108d5538--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@raguiar2?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@raguiar2?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Rui Aguiar"}, {"url": "https://medium.com/@raguiar2/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "68 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F82e0b7141d5c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-double-descent-when-more-data-and-bigger-models-are-a-bad-thing-3a3f108d5538&user=Rui+Aguiar&userId=82e0b7141d5c&source=post_page-82e0b7141d5c--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F82e0b7141d5c%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-double-descent-when-more-data-and-bigger-models-are-a-bad-thing-3a3f108d5538&user=Rui+Aguiar&userId=82e0b7141d5c&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}