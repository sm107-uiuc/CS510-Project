{"url": "https://towardsdatascience.com/interpretability-in-machine-learning-ab0cf2e66e1", "time": 1683015488.611604, "path": "towardsdatascience.com/interpretability-in-machine-learning-ab0cf2e66e1/", "webpage": {"metadata": {"title": "Interpretability in Machine Learning | by Conor O'Sullivan | Towards Data Science", "h1": "Interpretability in Machine Learning", "description": "Should we always trust a model that performs well? A model could reject your application for a mortgage or diagnose you with cancer. The consequences of these decisions are serious and, even if they\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/interperable-vs-explainable-machine-learning-1fa525e12f48", "anchor_text": "In a previous article,", "paragraph_index": 2}, {"url": "https://machinelearningmastery.com/calculate-feature-importance-with-python/#:~:text=Feature%20importance%20refers%20to%20techniques,at%20predicting%20a%20target%20variable.", "anchor_text": "feature importance", "paragraph_index": 4}, {"url": "https://github.com/marcotcr/lime", "anchor_text": "LIME", "paragraph_index": 4}, {"url": "http://www.flaticon.com/", "anchor_text": "www.flaticon.com", "paragraph_index": 25}, {"url": "https://support.flaticon.com/hc/en-us/articles/202798201-What-are-Flaticon-Premium-licenses-", "anchor_text": "Premium Plan", "paragraph_index": 25}], "all_paragraphs": ["Should we always trust a model that performs well? A model could reject your application for a mortgage or diagnose you with cancer. The consequences of these decisions are serious and, even if they are correct, we would expect an explanation. A human would be able to tell you that your income is too low for a mortgage or that a specific cluster of cells is likely malignant. A model that provided similar explanations would be more useful than one that just provided predictions.", "By obtaining these explanations, we say we are interpreting a machine learning model. In the rest of this article, we\u2019ll explain in more detail what is meant by interpretability. We\u2019ll then move on to the importance and benefits of being able to interpret your models. There are, however, still some downsides. We\u2019ll end off by discussing these and why, in some cases, you may prefer a less interpretable model.", "In a previous article, I discuss the concept of model interpretability and how it relates to interpretable and explainable machine learning. To summarise, interpretability is the degree to which a model can be understood in human terms. Model A is more interpretable than model B if it is easier for a human to understand how model A makes predictions. For example, a Convolutional Neural Network is less interpretable than a Random Forest which is less interpretable than a Decision Tree.", "With this in mind, we say a model is an interpretable model if it can be understood without any other aids/techniques. Interpretable models are highly interpretable. In comparison, explainable models are too complicated to be understood without the help of additional techniques. We say these models have low interpretability. We can see how these concepts are related in Figure 1. Generally, models can be classified as either interpretable or explainable but there are grey areas where people would disagree.", "As mentioned, we require additional techniques, such as feature importance or LIME, to understand how explainable models work. Implementing these techniques can be a lot of effort and, importantly, they only provide approximations for how a model works. So, we cannot be completely certain that we understand an explainable model. We can have a similar situation when comparing interpretable models.", "For example, logistic regression and decision trees. Neither of these requires additional techniques but logistic regression may still require more effort to interpret. We would need an understanding of the sigmoid function and how coefficients are related to odds/probability. This complexity may also lead to errors in our interpretations. In general, the more interpretable a model; the easier it is to understand and the more certain we can be that our understanding is correct. Interpretability is important because of the many benefits that flow from this.", "Our first benefit is that interpretable models are easier to explain to other people. For any topic, the better we understand it the easier it is to explain. We should also be able to explain it in simple terms (i.e. without mentioning the technical details). In industry, there are many people who may expect simple explanations for how your model works. These people will not necessarily have technical backgrounds or experience with machine learning.", "For example, suppose we have created a model that predicts whether someone will make a life insurance claim. We want to use this model to automate life insurance underwriting at our company. To sign off on the model, our boss would require a detailed explanation of how it works. A disgruntled customer may rightly demand an explanation for why they were not approved for life cover. A regulator could even require such an explanation by law.", "Trying to explain to these people how a neural network makes predictions may cause a lot of confusion. Due to the uncertainty, they may not even accept the explanation. In comparison, interpretable models like logistic regression can be understood in human terms. This means they can be explained in human terms. For example, we could explain precisely how much the customer\u2019s smoking habit has increased their probability of dying.", "The relationship described above is causal (i.e. smoking causes cancer/death). In general, machine learning models only care about associations. For example, a model could use someone\u2019s country of origin to predict if they had skin cancer. However, as with smoking, can we say someone\u2019s country causes cancer? The reason for this is that skin cancer is caused by sunshine and some countries are just sunnier than others. So we can only say skin cancer is associated with certain countries.", "A good example of where associations can go wrong comes from an experiment performed by researches at the University of Washington. The researches trained an image recognition model to classify animals as a husky or wolf. Using LIME, they tried to understand how their model made predictions. In Figure 2, we can see that the model was basing its predictions on image backgrounds. If the background had snow, the animal was always classified as a wolf. They had essentially built a model that detects snow.", "The issue is that wolves are associated with snow. Wolves will usually be found in the snow whereas huskies are not. What this example shows us is that models can, not only make incorrect predictions, but they can also make correct predictions in the wrong way. As data scientists, we need to sense check our models to make sure they are not making predictions in this way. The more interpretable your model the easier this is to do.", "As time goes on, a model\u2019s prediction power may deteriorate. This is because relationships between model features and the target variable can change. For example, due to the wage gap, income may currently be a good predictor of gender. As society becomes more equal, income would lose its predictive strength. We need to be aware of these potential changes and their impact on our models. This is harder to do for explainable models. As it is less clear how features are being used, even if we know the impact on individual features, we may not be able to tell the impact on the model as a whole.", "It is human nature to try to find meaning in the unknown. Machine learning can help us discover patterns in our data we didn\u2019t know existed. However, we cannot identify these patterns by just looking at the model\u2019s predictions. Any lessons are lost if we can not interpret our model. Ultimately, the less interpretable a model the harder it is to learn from it.", "It is important that your models make unbiased decisions so that they do not perpetuate any historical injustices. Identifying sources of bias can be difficult. It often comes from associations between model features and protected variables (e.g. race or gender). For example, due to a history of forced segregation in South Africa, race is highly associated with someones\u2019 location\\neighbourhood. Location can act as a proxy for race. A model that uses location may be biased towards a certain race.", "Using an interpretable model will not necessarily mean that you will have an unbiased model. It also does not mean that it will be easier to determine if the model is fair or not. This is because most measures of fairness (e.g. false positive rate, disparate impact) are model agnostic. They are just as easy to calculate for any model. What using an interpretable model does do is make it easier to identify and correct the source of bias. We know what features are being used and we can check which of these are associated with the protected variables.", "Okay, we get it\u2026 interpretable models are great. They are easier to understand, explain and learn from. They also allow us to better sense check current performance, future performance and model fairness. There are however downsides to interpretability and situations where we would prefer an explainable model.", "Systems based on ML are open to manipulation or fraud. For example, suppose we have a system that automatically gives out car loans. An important feature could be the number of credit cards. The more cards a customer has the risker she is. If a customer knew this they could temporarily cancel all their cards, take out a car loan and then reapply for all the credit cards.", "The probability of the customer repaying the loan does not change when she cancels her cards. The customer has manipulated the model to make an incorrect prediction. The more interpretable a model the more transparent and easy it is to manipulate. This is the case even if the inner working of a model are kept secret. The relationships between features and target variable are usually simpler making them easier to guess.", "We mentioned that interpretable models are easier to learn from. The flip side is that they are less likely to teach us something new. An explainable model like a neural network can automatically model interactions and non-linear relationships in data. By interpreting these models we can uncover these relationships that we never knew existed.", "In comparison, algorithms like linear regression can only model linear relationships. To model non-linear relationships, we would have to use feature engineering to include any relevant variable in our dataset. This would require prior knowledge of the relationships defeating the purpose of interpreting the model.", "Building interpretable models can require significant domain knowledge and expertise. Generally, interpretable models, like regression, can only model linear relationships in your data. To model non-linear relationships we have to perform feature engineering. For example, for a medical diagnosis model, we may want to calculate BMI using height and weight. Knowing what features will be predictive and, therefore, what features to create requires domain knowledge in a particular field.", "Your team may not have this knowledge. Alternatively, you could use an explainable model which will automatically model non-linear relationships in your data. This removes the need to create any new features; essentially leaving the thinking up to the computer. The downside, as we\u2019ve discussed thoroughly above, is a poorer understanding of how the features are being used to make predictions.", "What we can see from the above is that, generally, the less complicated a model the more interpretable. So, for higher interpretability, there can be the trade-off of lower accuracy. This is because, in some cases, simpler models can make less accurate predictions. This really depends on the problem you are trying to solve. For instance, you would get poor results using logistic regression to do image recognition.", "For many problems, an interpretable model would perform as well as an explainable model. In the article below, we compare an interpretable model, Logistic Regression, to an explainable model, a Neural Network. We show that by putting a bit of thought into our problem and creating new features we can achieve similar accuracy with an interpretable model. It is a good practical take on some of the concepts we\u2019ve discussed in this article.", "All images are my own or obtain from www.flaticon.com. In the case of the latter, I have a \u201cFull license\u201d as defined under their Premium Plan.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Scientist | Writer | Houseplant Addict | I write about IML, XAI, Algorithm Fairness and Data Exploration | New article (nearly) every week!"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fab0cf2e66e1&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterpretability-in-machine-learning-ab0cf2e66e1&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterpretability-in-machine-learning-ab0cf2e66e1&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterpretability-in-machine-learning-ab0cf2e66e1&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterpretability-in-machine-learning-ab0cf2e66e1&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----ab0cf2e66e1--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ab0cf2e66e1--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://conorosullyds.medium.com/?source=post_page-----ab0cf2e66e1--------------------------------", "anchor_text": ""}, {"url": "https://conorosullyds.medium.com/?source=post_page-----ab0cf2e66e1--------------------------------", "anchor_text": "Conor O'Sullivan"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4ae48256fb37&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterpretability-in-machine-learning-ab0cf2e66e1&user=Conor+O%27Sullivan&userId=4ae48256fb37&source=post_page-4ae48256fb37----ab0cf2e66e1---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fab0cf2e66e1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterpretability-in-machine-learning-ab0cf2e66e1&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fab0cf2e66e1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterpretability-in-machine-learning-ab0cf2e66e1&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.flaticon.com/premium-icon/cyborg_901032", "anchor_text": "flaticon"}, {"url": "https://towardsdatascience.com/interperable-vs-explainable-machine-learning-1fa525e12f48", "anchor_text": "In a previous article,"}, {"url": "https://machinelearningmastery.com/calculate-feature-importance-with-python/#:~:text=Feature%20importance%20refers%20to%20techniques,at%20predicting%20a%20target%20variable.", "anchor_text": "feature importance"}, {"url": "https://github.com/marcotcr/lime", "anchor_text": "LIME"}, {"url": "https://www.flaticon.com/free-icon/interpretation_1935181", "anchor_text": "flaticon"}, {"url": "https://www.flaticon.com/free-icon/einstein_892712", "anchor_text": "flaticon"}, {"url": "https://arxiv.org/abs/1602.04938", "anchor_text": "M. Tulio Ribeiro, S. Singh & C. Guestrin"}, {"url": "https://www.flaticon.com/premium-icon/man_3418023", "anchor_text": "flaticon"}, {"url": "https://www.flaticon.com/premium-icon/cooperation_3317601", "anchor_text": "flaticon"}, {"url": "https://www.flaticon.com/free-icon/fair_3260927?term=fairness&page=1&position=14", "anchor_text": "flaticon"}, {"url": "https://www.flaticon.com/free-icon/hacker_3380378", "anchor_text": "flaticon"}, {"url": "https://towardsdatascience.com/the-power-of-feature-engineering-b6f3bb7de39c", "anchor_text": "The Power of Feature EngineeringWhy you should probably just use logistic regression to model non-linear decision boundaries (with Python code)towardsdatascience.com"}, {"url": "http://www.flaticon.com/", "anchor_text": "www.flaticon.com"}, {"url": "https://support.flaticon.com/hc/en-us/articles/202798201-What-are-Flaticon-Premium-licenses-", "anchor_text": "Premium Plan"}, {"url": "https://christophm.github.io/interpretable-ml-book/", "anchor_text": "https://christophm.github.io/interpretable-ml-book/"}, {"url": "https://www.nature.com/articles/s42256-019-0048-x", "anchor_text": "https://www.nature.com/articles/s42256-019-0048-x"}, {"url": "https://arxiv.org/abs/1602.04938", "anchor_text": "https://arxiv.org/abs/1602.04938"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----ab0cf2e66e1---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----ab0cf2e66e1---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----ab0cf2e66e1---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/interpretability?source=post_page-----ab0cf2e66e1---------------interpretability-----------------", "anchor_text": "Interpretability"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----ab0cf2e66e1---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fab0cf2e66e1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterpretability-in-machine-learning-ab0cf2e66e1&user=Conor+O%27Sullivan&userId=4ae48256fb37&source=-----ab0cf2e66e1---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fab0cf2e66e1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterpretability-in-machine-learning-ab0cf2e66e1&user=Conor+O%27Sullivan&userId=4ae48256fb37&source=-----ab0cf2e66e1---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fab0cf2e66e1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterpretability-in-machine-learning-ab0cf2e66e1&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ab0cf2e66e1--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fab0cf2e66e1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterpretability-in-machine-learning-ab0cf2e66e1&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----ab0cf2e66e1---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----ab0cf2e66e1--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----ab0cf2e66e1--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----ab0cf2e66e1--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----ab0cf2e66e1--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----ab0cf2e66e1--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----ab0cf2e66e1--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----ab0cf2e66e1--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----ab0cf2e66e1--------------------------------", "anchor_text": ""}, {"url": "https://conorosullyds.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://conorosullyds.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Conor O'Sullivan"}, {"url": "https://conorosullyds.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "3.3K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4ae48256fb37&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterpretability-in-machine-learning-ab0cf2e66e1&user=Conor+O%27Sullivan&userId=4ae48256fb37&source=post_page-4ae48256fb37--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F93bef42da4fd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finterpretability-in-machine-learning-ab0cf2e66e1&newsletterV3=4ae48256fb37&newsletterV3Id=93bef42da4fd&user=Conor+O%27Sullivan&userId=4ae48256fb37&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}