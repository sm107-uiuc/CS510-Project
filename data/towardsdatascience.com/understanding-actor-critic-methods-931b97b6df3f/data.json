{"url": "https://towardsdatascience.com/understanding-actor-critic-methods-931b97b6df3f", "time": 1682994882.0112262, "path": "towardsdatascience.com/understanding-actor-critic-methods-931b97b6df3f/", "webpage": {"metadata": {"title": "Understanding Actor Critic Methods and A2C | by Chris Yoon | Towards Data Science", "h1": "Understanding Actor Critic Methods and A2C", "description": "In my previous post, we derived policy gradients and implemented the REINFORCE algorithm (also known as Monte Carlo policy gradients). There are, however, some glaring issues with vanilla policy\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/@thechrisyoon/deriving-policy-gradients-and-implementing-reinforce-f887949bd63", "anchor_text": "my previous post", "paragraph_index": 0}, {"url": "https://www.quora.com/Why-does-the-policy-gradient-method-have-a-high-variance", "anchor_text": "Jerry Liu\u2019s post to \u201cWhy does the policy gradient method have high variance\u201d", "paragraph_index": 7}, {"url": "https://arxiv.org/abs/1602.01783", "anchor_text": "Deepmind\u2019s paper \u201cAsynchronous Methods for Deep Reinforcement Learning\u201d (Mnih et al, 2016)", "paragraph_index": 25}, {"url": "https://blog.openai.com/baselines-acktr-a2c/", "anchor_text": "this", "paragraph_index": 26}], "all_paragraphs": ["In my previous post, we derived policy gradients and implemented the REINFORCE algorithm (also known as Monte Carlo policy gradients). There are, however, some glaring issues with vanilla policy gradients: noisy gradients and high variance.", "As in the REINFORCE algorithm, we update the policy parameter through Monte Carlo updates (i.e. taking random samples). This introduces in inherent high variability in log probabilities (log of the policy distribution) and cumulative reward values, because each trajectories during training can deviate from each other at great degrees.", "Consequently, the high variability in log probabilities and cumulative reward values will make noisy gradients, and cause unstable learning and/or the policy distribution skewing to a non-optimal direction.", "Besides high variance of gradients, another problem with policy gradients occurs trajectories have a cumulative reward of 0. The essence of policy gradient is increasing the probabilities for \u201cgood\u201d actions and decreasing those of \u201cbad\u201d actions in the policy distribution; both \u201cgoods\u201d and \u201cbad\u201d actions with will not be learned if the cumulative reward is 0.", "Overall, these issues contribute to the instability and slow convergence of vanilla policy gradient methods.", "One way to reduce variance and increase stability is subtracting the cumulative reward by a baseline:", "Intuitively, making the cumulative reward smaller by subtracting it with a baseline will make smaller gradients, and thus smaller and more stable updates.", "Here is a very illustrative explanation, taken from Jerry Liu\u2019s post to \u201cWhy does the policy gradient method have high variance\u201d:", "The baseline can take various values. The set of equations below illustrates the classic variants of actor critic methods (with respect to REINFORCE). In this post, we will take a look at Q Actor Critic and Advantage Actor Critic.", "Before we return to baselines, let\u2019s first take a look at the vanilla policy gradient again to see how the Actor Critic architecture comes in (and what is really is). Recall that:", "We can then decompose the expectation into:", "The second expectation term should be familiar; it is the Q value! (If you did not already know this, I would suggest that you read up on value iteration and Q learning).", "Plugging that in, we can rewrite the update equation as such:", "As we know, the Q value can be learned by parameterizing the Q function with a neural network (denoted by subscript w above).", "This leads us to Actor Critic Methods, where:", "and both the Critic and Actor functions are parameterized with neural networks. In the derivation above, the Critic neural network parameterizes the Q value \u2014 so, it is called Q Actor Critic.", "Below is the pseudocode for Q-Actor-Critic:", "As illustrated, we update both the Critic network and the Value network at each update step.", "To make an argument from authority (as I was not able to find the reason why), the state-value function makes an optimal baseline function. This is stated in the Carnegie Mellon CS10703 and Berekely CS294 lecture slides, but with no reason provided.", "So, using the V function as the baseline function, we subtract the Q value term with the V value. Intuitively, this means how much better it is to take a specific action compared to the average, general action at the given state. We will call this value the advantage value:", "Does that mean we have to construct two neural networks for both the Q value and the V value (in addition to the policy network)? No. That would be very inefficient. Instead, we can use the relationship between the Q and the V from the Bellman optimality equation:", "So, we can rewrite the advantage as:", "Then, we only have to use one neural network for the V function (parameterized by v above). So we can rewrite the update equation as:", "This is the Advantage Actor Critic.", "The Advantage Actor Critic has two main variants: the Asynchronous Advantage Actor Critic (A3C) and the Advantage Actor Critic (A2C).", "A3C was introduced in Deepmind\u2019s paper \u201cAsynchronous Methods for Deep Reinforcement Learning\u201d (Mnih et al, 2016). In essence, A3C implements parallel training where multiple workers in parallel environments independently update a global value function\u2014hence \u201casynchronous.\u201d One key benefit of having asynchronous actors is effective and efficient exploration of the state space.", "A2C is like A3C but without the asynchronous part; this means a single-worker variant of the A3C. It was empirically found that A2C produces comparable performance to A3C while being more efficient. According to this OpenAI blog post, researchers aren\u2019t completely sure if or how the asynchrony benefits learning:", "After reading the paper, AI researchers wondered whether the asynchrony led to improved performance (e.g. \u201cperhaps the added noise would provide some regularization or exploration?\u201c), or if it was just an implementation detail that allowed for faster training with a CPU-based implementation \u2026", "Our synchronous A2C implementation performs better than our asynchronous implementations \u2014 we have not seen any evidence that the noise introduced by asynchrony provides any performance benefit. This A2C implementation is more cost-effective than A3C when using single-GPU machines, and is faster than a CPU-only A3C implementation when using larger policies.", "Anyhow, we will implement the A2C in this post as it is more simple in implementation. (This can easily be extended to A3C)", "So, recall the new update equation, replacing the discounted cumulative award from vanilla policy gradients with the Advantage function:", "On each learning step, we update both the Actor parameter (with policy gradients and advantage value), and the Critic parameter (with minimizing the mean squared error with the Bellman update equation). Let\u2019s see how this looks like in code:", "Below are the includes and hyper-parameters:", "First, let\u2019s start with implementing the Actor Critic Network with the following configurations:", "The main loop and update loop, as outlined above:", "Running the code, we can see how the performance improves on the OpenAI Gym CartPole-v0 environment:", "Check out my other posts on Reinforcement Learning:", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F931b97b6df3f&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-actor-critic-methods-931b97b6df3f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-actor-critic-methods-931b97b6df3f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-actor-critic-methods-931b97b6df3f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-actor-critic-methods-931b97b6df3f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----931b97b6df3f--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----931b97b6df3f--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@thechrisyoon?source=post_page-----931b97b6df3f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@thechrisyoon?source=post_page-----931b97b6df3f--------------------------------", "anchor_text": "Chris Yoon"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb24112d01863&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-actor-critic-methods-931b97b6df3f&user=Chris+Yoon&userId=b24112d01863&source=post_page-b24112d01863----931b97b6df3f---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F931b97b6df3f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-actor-critic-methods-931b97b6df3f&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F931b97b6df3f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-actor-critic-methods-931b97b6df3f&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/@thechrisyoon/deriving-policy-gradients-and-implementing-reinforce-f887949bd63", "anchor_text": "my previous post"}, {"url": "https://www.quora.com/Why-does-the-policy-gradient-method-have-a-high-variance", "anchor_text": "Jerry Liu\u2019s post to \u201cWhy does the policy gradient method have high variance\u201d"}, {"url": "https://arxiv.org/abs/1602.01783", "anchor_text": "Deepmind\u2019s paper \u201cAsynchronous Methods for Deep Reinforcement Learning\u201d (Mnih et al, 2016)"}, {"url": "https://www.groundai.com/project/a-brandom-ian-view-of-reinforcement-learning-towards-strong-ai/", "anchor_text": "GroundAI blog post"}, {"url": "https://blog.openai.com/baselines-acktr-a2c/", "anchor_text": "this"}, {"url": "https://github.com/thechrisyoon08/Reinforcement-Learning", "anchor_text": "https://github.com/thechrisyoon08/Reinforcement-Learning"}, {"url": "http://rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/lecture_4_policy_gradient.pdf", "anchor_text": "UC Berkeley CS294 Lecture Slides"}, {"url": "http://www.cs.cmu.edu/~rsalakhu/10703/Lecture_PG2.pdf", "anchor_text": "Carnegie Mellon University CS10703 Lecture Slides"}, {"url": "https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html", "anchor_text": "Lilian Weng\u2019s Post on Policy Gradient Algorithms"}, {"url": "https://www.quora.com/Why-does-the-policy-gradient-method-have-a-high-variance", "anchor_text": "Jerry Liu\u2019s answer on Quora Post \u201cWhy does the policy gradient method have a high variance\u201d"}, {"url": "https://www.youtube.com/watch?v=gINks-YCTBs&t=2362s", "anchor_text": "Naver D2 RLCode Lecture Video"}, {"url": "https://blog.openai.com/baselines-acktr-a2c/", "anchor_text": "OpenAI blog post on A2C and ACKTR"}, {"url": "http://Figure 4: Schematic representation of Asynchronous Advantage Actor Critic algorithm (A3C) algorithm.", "anchor_text": "GroundAI\u2019s blog post"}, {"url": "https://medium.com/@thechrisyoon/deriving-policy-gradients-and-implementing-reinforce-f887949bd63", "anchor_text": "Deriving Policy Gradients and Implementing REINFORCE"}, {"url": "https://towardsdatascience.com/understanding-actor-critic-methods-931b97b6df3f", "anchor_text": "Understanding Actor Critic Methods"}, {"url": "https://towardsdatascience.com/deep-deterministic-policy-gradients-explained-2d94655a9b7b", "anchor_text": "Deep Deterministic Policy Gradients Explained"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----931b97b6df3f---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----931b97b6df3f---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----931b97b6df3f---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----931b97b6df3f---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----931b97b6df3f---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F931b97b6df3f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-actor-critic-methods-931b97b6df3f&user=Chris+Yoon&userId=b24112d01863&source=-----931b97b6df3f---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F931b97b6df3f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-actor-critic-methods-931b97b6df3f&user=Chris+Yoon&userId=b24112d01863&source=-----931b97b6df3f---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F931b97b6df3f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-actor-critic-methods-931b97b6df3f&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----931b97b6df3f--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F931b97b6df3f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-actor-critic-methods-931b97b6df3f&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----931b97b6df3f---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----931b97b6df3f--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----931b97b6df3f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----931b97b6df3f--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----931b97b6df3f--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----931b97b6df3f--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----931b97b6df3f--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----931b97b6df3f--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----931b97b6df3f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@thechrisyoon?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@thechrisyoon?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Chris Yoon"}, {"url": "https://medium.com/@thechrisyoon/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "631 Followers"}, {"url": "https://www.linkedin.com/in/chris-yoon-75847418b/", "anchor_text": "https://www.linkedin.com/in/chris-yoon-75847418b/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb24112d01863&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-actor-critic-methods-931b97b6df3f&user=Chris+Yoon&userId=b24112d01863&source=post_page-b24112d01863--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F6d3234499fec&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-actor-critic-methods-931b97b6df3f&newsletterV3=b24112d01863&newsletterV3Id=6d3234499fec&user=Chris+Yoon&userId=b24112d01863&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}