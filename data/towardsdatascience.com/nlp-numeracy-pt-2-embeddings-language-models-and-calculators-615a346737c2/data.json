{"url": "https://towardsdatascience.com/nlp-numeracy-pt-2-embeddings-language-models-and-calculators-615a346737c2", "time": 1683018030.211902, "path": "towardsdatascience.com/nlp-numeracy-pt-2-embeddings-language-models-and-calculators-615a346737c2/", "webpage": {"metadata": {"title": "NLP & Numeracy (Pt 2) \u2014 Embeddings, Language Models and Calculators | by Mihail Dungarov | Towards Data Science", "h1": "NLP & Numeracy (Pt 2) \u2014 Embeddings, Language Models and Calculators", "description": "In the previous article we took a look at what numeracy in natural language looks like and how it can impact NLP applications. We found different types of numeracy to pose different challenges for\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/transformers-numeracy-pt-1-calculators-for-natural-language-e09a471c5ebe", "anchor_text": "In the previous article", "paragraph_index": 0}, {"url": "https://leaderboard.allenai.org/drop/submissions/public", "anchor_text": "F1 is just above 90%, with human performance at ca. 96%", "paragraph_index": 5}, {"url": "https://towardsdatascience.com/transformers-numeracy-pt-1-calculators-for-natural-language-e09a471c5ebe", "anchor_text": "previous article", "paragraph_index": 6}, {"url": "https://arxiv.org/abs/1909.07940", "anchor_text": "[Wallace, et al., 2019]", "paragraph_index": 7}, {"url": "https://www.aclweb.org/anthology/P19-1329/", "anchor_text": "[Naik, et al. 2019]", "paragraph_index": 7}, {"url": "https://arxiv.org/abs/1909.07940", "anchor_text": "[Wallace, et al., 2019]", "paragraph_index": 7}, {"url": "https://arxiv.org/abs/2005.00683", "anchor_text": "[Lin, et al., 2020]", "paragraph_index": 12}, {"url": "https://www.aclweb.org/anthology/P18-1196/", "anchor_text": "[Spithourakis, et al., 2018]", "paragraph_index": 12}, {"url": "https://www.aclweb.org/anthology/P19-1635/", "anchor_text": "[Chen, et al, 2019]", "paragraph_index": 12}, {"url": "https://arxiv.org/abs/2004.04487", "anchor_text": "Geva, et al., 2020]", "paragraph_index": 13}, {"url": "https://www.aclweb.org/anthology/D14-1058/", "anchor_text": "[Hosseini, et al, 2014]", "paragraph_index": 13}, {"url": "https://arxiv.org/abs/1909.00109", "anchor_text": "Andor et al., 2019", "paragraph_index": 14}, {"url": "https://arxiv.org/abs/1903.00161", "anchor_text": "[Dua, et al., 2019]", "paragraph_index": 15}, {"url": "https://arxiv.org/abs/1804.09541", "anchor_text": "Yu, et al., 2018", "paragraph_index": 15}, {"url": "https://leaderboard.allenai.org/drop/submissions/public", "anchor_text": "Leaderboard (LB)", "paragraph_index": 15}, {"url": "https://arxiv.org/abs/1908.05514", "anchor_text": "Hu, et al. 2019", "paragraph_index": 15}, {"url": "https://leaderboard.allenai.org/drop/submissions/public", "anchor_text": "LB", "paragraph_index": 15}, {"url": "https://arxiv.org/abs/1910.06701", "anchor_text": "Ran, et al., 2019", "paragraph_index": 16}, {"url": "https://github.com/llamazing/numnet_plus", "anchor_text": "NumNet+", "paragraph_index": 16}, {"url": "https://arxiv.org/abs/1907.11692", "anchor_text": "RoBERTa", "paragraph_index": 16}, {"url": "https://leaderboard.allenai.org/drop/submissions/public", "anchor_text": "LB", "paragraph_index": 16}, {"url": "https://arxiv.org/abs/2009.07448", "anchor_text": "[Chen, et al., 2020]", "paragraph_index": 17}, {"url": "https://leaderboard.allenai.org/drop/submissions/public", "anchor_text": "LB", "paragraph_index": 17}, {"url": "https://towardsdatascience.com/transformers-numeracy-pt-1-calculators-for-natural-language-e09a471c5ebe", "anchor_text": "previous article", "paragraph_index": 18}, {"url": "https://www.linkedin.com/in/mihail-dungarov-cfa-a0291a88", "anchor_text": "LinkedIn", "paragraph_index": 21}], "all_paragraphs": ["In the previous article we took a look at what numeracy in natural language looks like and how it can impact NLP applications. We found different types of numeracy to pose different challenges for algorithmic processing and require different ML solutions. In this part, I would like to dive deeper into the \u2018how\u2019 of the different approaches.", "We explore how NLP equivalents of traditional calculators work (see picture above). Machines that allow us to pose problems as text and retrieve results as text. Traditionally, when using a calculator the flow is as follows:", "Although I am talking about calculators, this flow applies without loss of generality from abaci to big data distributed systems. Accuracy is generally 100%, barring from rare errors due to data corruption.", "The systems we are looking at in this and the previous article offer a slightly different flow to the above but since the core purpose is the same, I like to refer to them as NLP calculators.", "With NLP Calculators, the flow is different in the sense that now:", "As we have seen in the previous article accuracy widely varies by approach and type of natural language request, however, with the most recent techniques, F1 is just above 90%, with human performance at ca. 96%.", "Similar to the previous article, this one is a literature review with a focus on grouping works by their applications and approach. The rest of the article is organised as follows", "[Wallace, et al., 2019] set out to explore how much numerical awareness is naturally present within embeddings. Across most common embedding techniques, basic numeracy skill seems to be present. By using a wide range of commonly used embeddings (Word2Vec, GloVe, ELMo, BERT) and training a MLP (multi-layer perceptrons) to understand number values, list maxima/minima and perform addition, they show that the embeddings already contain significant numerical awareness. For instance, decoding (\u201cthree\u201d \u2192 3) and addition for numbers up to 1000 show about 1\u20132% RMSE (under 20 for values between 0\u2013999)or less for embedding approaches mentioned abooo,with the exception of BERT which lags the rest. Models are trained on numbers in a certain range (e.g. -500 to +500) and then are asked about unseen numbers from that range, however, the authors note deterioration in performance when asked to work with numbers outside of that range. [Naik, et al. 2019] explore the same topic and confirm that embeddings successfully capture magnitude, however, get mixed results on numeration matching a string to a numeric value. However, their experiments focus on cosine distances between numeric embeddings, not the MLP representation what [Wallace, et al., 2019] allow for. This limits significantly the representations of numeracy allowed, potentially causing poorer performance.", "Notably, these findings are about the properties of embeddings and do not clarify whether models can reason successfully numerically over text. In the diagram from the beginning of the article, this is equivalent to being able to identify which numbers are represented in the text. We still need a means by which to choose and perform the appropriate operation.", "Language models (LM) aim to develop the ability to predict the next word in a sentence, a missing word from a sentence or a next sentence altogether (here we consider the case when predicting a missing word). A common metric for language models is perplexity, which measures how likely the correct answer is, according to the model.", "Every time a language model is asked to fill a missing word in a sentence, it will assign a probability to each of the possible choices (if it has a vocabulary of 300k words, it will produce 300k probability values.) For a good model, when predicting a missing word, most words will be considered very unlikely. In most cases, however, the actual correct word might not be the most likely one according to the model, because in most cases there are many possible correct answers in language (consider: \u201cI got on the [BLANK] to get to work\u201d, if I say \u201cbus\u201d but the answer is \u201ctube\u201d is it that too bad? No. What I want to avoid, however, is that the model is not totally thrown by \u201ctube\u201d being the correct answer. Thus, in order to measure how good a model is at filling a slot, we want to see how \u201cperplexed\u201d it is by the correct word. In the example above, it is desirable to have high probabilities for values such as: bike, bus, car, tube, etc. and not very high ones for words like banana, snake, cat, etc. Although if you have even watched \u201cMy Neighbour Totoro\u201d you might disagree with one of these\u2026", "LMs can be used as foundation for other tasks, e.g. frameworks like BERT where an LM can be fine-tuned to do question-answering, translation, information-retrieval tasks, etc. IN the context of numeracy, several papers have tried to understand how much inherent numeracy is there in the model by performing different \u201cprobing\u201d tasks.", "[Lin, et al., 2020] find out that transformer based language models believe birds have four legs. They explore the model\u2019s ability to pick up numerical commonsense knowledge \u2014 think \u201chow many\u201d-type questions for natural objects (e.g. sides of cubes, wheels of trucks, legs of animals). They demonstrate that LMs struggle to assign the right number. The accuracy at model\u2019s first attempt (@1) is under 50% without pretraining and under 60% after fine-tuning and distant supervision, performance improves by the third attempt (@3) but there are only so many digits available anyway. [Spithourakis, et al., 2018] find that using a hierarchical model to first distinguish between numerals and words (i.e. non-numeral tokens), and separate model to deal with numerals when identified reduces perplexity by 2 to 4 orders of magnitude depending on the dataset. Notice that this pre-dates the Transformer era so the underlying architecture is an RNN. The results are impressive as the perplexity of the best models is half of those that do not allow for numeracy in any way. [Chen, et al, 2019] create a financial news headline dataset. They train a model to try and predict the order of magnitude of missing numeric values in a headline. Seven categories are used \u2014 numerical ranges increasing by a factor of 10 from decimals to 10\u2076 (0 to 1, 1 to 10, 10 to 100, etc) They achieve a micro F1 of 70\u201380% depending on the dataset.", "[Geva, et al., 2020] maintain the framework pre-trained language models serving as a foundation for fine-tuning to specific tasks by adding numerical pre-training steps. Two datasets are added for pre-training: numeric and text data respectively. Additionally, they add random shift (randomized shift of the input tokens to either side to improve positional invariance of the numeric inputs) and a masked LM objective when training the model (aiming to minimize the loss on masked tokens). They demonstrate that they can achieve an F1 of over 70% question-answering on numeracy tasks (DROP dataset), without loss of performance on general question-answering (SQuAD dataset) compared to BERT. Generating numerical natural language examples: the authors follow a framework by [Hosseini, et al, 2014] to generate sentences like: Bridget adopted 4 brown dogs, The king recruited 1337 Irish soldiers, etc", "These are models designed to consume text with numerically relevant content and output results of the numerical procedures needed. Just like a calculator we will input a numerical question and receive a result of the appropriate operation. The tiny difference \u2014 the input and the output here would be text. These are set apart from the rest by their significantly improved numeracy capability \u2014 coming very close to or just above 90% F1. With the exception of BERT-Calculator [Andor et al., 2019] these are purely numeric Question Answering systems \u2014 i.e. they are only trained to answer numerical questions.", "[Dua, et al., 2019] introduce the DROP benchmark as well as NAQANet. The model follows the format of QANet [Yu, et al., 2018] which is a model for traditional question-answering a la SQuaD. It differs by having 4 output layers, dedicated to different numeric tasks: counting, arithmetic, span (dealing with cases where a chunk of the paragraph is the answer) and multi-span (cases where several chunks of the paragraph are the answer). Finally, an Answer predictor is trained on the same attention layers as the four output layers to determine which of the four is the right type of answer. Leaderboard (LB) score \u2014 64% F1. MTMSN [Hu, et al. 2019] extends this by, swapping the self-attention layers with full transformer blocks, adding negation to the possible tools as well as an answer predictor that also predicts the number of answers to specifically improve multi-spans and get holistic answers. Leaderboard (LB) score \u2014 79.99% F1.", "NumNet [Ran, et al., 2019] also starts by first encoding the question and paragraph pair with pre-trained embeddings and ends with a prediction module, consisting of several predefined computational modules to provide the required answer. However, between the encode and prediction modules they introduce a \u201cReasoning Module\u201d which creates a question and paragraph graph representation of the numbers in the inputs (picture below), where the numbers are the nodes and edges are \u201cnumerically aware\u201d \u2014 aware of the greater than / less then relative scales of number nodes. A Graphical neutral network is used to reason over these relationships and feed into the output layers. NumNet+ follows a similar approach but uses Roberta (a version of BERT with robust hyperparameter choices RoBERTa) in the encoder and reasoning layer instead. Several NumNet+ are available on the LB with the best scoring F1 86%", "[Chen, et al., 2020] from the AntGroup introduce a family of models \u2014 QDGAT. The current leader on the DROP task is QDGAT-ALBERT (as of Dec 2020) a variation of QDGAT, achieving 90.1% F1 on the LB. They largely follow the NumNet architecture with significant extension of how Graph representations are handled, which also leads to significant improvement in performance. Graphs now take into account the types of numbers included in each sentence and the entity they relate to. Edge creation still represents the fact that two nodes coexist in a sentence. As before a Graph neutral network (GNN) is trained on those and different output layers are used for the different types of answers", "The previous article gave an overview of the challenges that numeracy brings as well as how to measure it. Here we have seen how different approaches solve for that:", "While I will not trust these models blindly, I can at least feel more comfortable that there are a number of possible solutions when accurate NLP numeracy is needed.", "Special thanks to Rich Knuszka for valuable feedback.", "Hopefully, this was useful or \u2026 curious (either the good or bad curious). Thank you for reading. If you feel like saying hi, do reach out via LinkedIn", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F615a346737c2&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnlp-numeracy-pt-2-embeddings-language-models-and-calculators-615a346737c2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnlp-numeracy-pt-2-embeddings-language-models-and-calculators-615a346737c2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnlp-numeracy-pt-2-embeddings-language-models-and-calculators-615a346737c2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnlp-numeracy-pt-2-embeddings-language-models-and-calculators-615a346737c2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----615a346737c2--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----615a346737c2--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@mihail.dungarov?source=post_page-----615a346737c2--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@mihail.dungarov?source=post_page-----615a346737c2--------------------------------", "anchor_text": "Mihail Dungarov"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff882548c947b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnlp-numeracy-pt-2-embeddings-language-models-and-calculators-615a346737c2&user=Mihail+Dungarov&userId=f882548c947b&source=post_page-f882548c947b----615a346737c2---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F615a346737c2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnlp-numeracy-pt-2-embeddings-language-models-and-calculators-615a346737c2&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F615a346737c2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnlp-numeracy-pt-2-embeddings-language-models-and-calculators-615a346737c2&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.flaticon.com/authors/freepik", "anchor_text": "Freepik"}, {"url": "https://towardsdatascience.com/transformers-numeracy-pt-1-calculators-for-natural-language-e09a471c5ebe", "anchor_text": "In the previous article"}, {"url": "https://leaderboard.allenai.org/drop/submissions/public", "anchor_text": "F1 is just above 90%, with human performance at ca. 96%"}, {"url": "https://towardsdatascience.com/transformers-numeracy-pt-1-calculators-for-natural-language-e09a471c5ebe", "anchor_text": "previous article"}, {"url": "https://arxiv.org/abs/2009.07448", "anchor_text": "[Chen, et al. 2020]"}, {"url": "https://deepart.io/", "anchor_text": "DeepArt"}, {"url": "https://arxiv.org/abs/1909.07940", "anchor_text": "[Wallace, et al., 2019]"}, {"url": "https://www.aclweb.org/anthology/P19-1329/", "anchor_text": "[Naik, et al. 2019]"}, {"url": "https://arxiv.org/abs/1909.07940", "anchor_text": "[Wallace, et al., 2019]"}, {"url": "https://www.flaticon.com/authors/freepik", "anchor_text": "Freepik"}, {"url": "https://www.aclweb.org/anthology/D16-1101/", "anchor_text": "[Spithourakis, et al., 2016]"}, {"url": "https://arxiv.org/abs/2004.04487", "anchor_text": "[Geva, et al., 2020]"}, {"url": "https://arxiv.org/abs/2005.00683", "anchor_text": "[Lin, et al., 2020]"}, {"url": "https://www.aclweb.org/anthology/P18-1196/", "anchor_text": "[Spithourakis, et al., 2018]"}, {"url": "https://www.aclweb.org/anthology/P19-1635/", "anchor_text": "[Chen, et al, 2019]"}, {"url": "https://arxiv.org/abs/2004.04487", "anchor_text": "Geva, et al., 2020]"}, {"url": "https://www.aclweb.org/anthology/D14-1058/", "anchor_text": "[Hosseini, et al, 2014]"}, {"url": "https://arxiv.org/abs/2009.07448", "anchor_text": "[Chen, et al. 2020]"}, {"url": "https://deepart.io/", "anchor_text": "DeepArt"}, {"url": "https://arxiv.org/abs/1909.00109", "anchor_text": "Andor et al., 2019"}, {"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "BERT"}, {"url": "https://arxiv.org/abs/1907.11692v1", "anchor_text": "Roberta"}, {"url": "https://arxiv.org/abs/1909.11942", "anchor_text": "Albert"}, {"url": "https://arxiv.org/abs/1908.05514", "anchor_text": "Hu, et al. 2019"}, {"url": "https://arxiv.org/abs/1903.00161", "anchor_text": "[Dua, et al., 2019]"}, {"url": "https://arxiv.org/abs/1910.06701", "anchor_text": "Ran, et al., 2019"}, {"url": "https://arxiv.org/abs/2009.07448", "anchor_text": "[Chen, et al., 2020]"}, {"url": "https://paperswithcode.com/sota/question-answering-on-drop-test", "anchor_text": "DROP leaderboard in papers-with-code"}, {"url": "https://leaderboard.allenai.org/drop/submissions/public", "anchor_text": "full LB on AllenAI"}, {"url": "https://arxiv.org/abs/1903.00161", "anchor_text": "[Dua, et al., 2019]"}, {"url": "https://arxiv.org/abs/1804.09541", "anchor_text": "Yu, et al., 2018"}, {"url": "https://leaderboard.allenai.org/drop/submissions/public", "anchor_text": "Leaderboard (LB)"}, {"url": "https://arxiv.org/abs/1908.05514", "anchor_text": "Hu, et al. 2019"}, {"url": "https://leaderboard.allenai.org/drop/submissions/public", "anchor_text": "LB"}, {"url": "https://arxiv.org/abs/1910.06701", "anchor_text": "Ran, et al., 2019"}, {"url": "https://github.com/llamazing/numnet_plus", "anchor_text": "NumNet+"}, {"url": "https://arxiv.org/abs/1907.11692", "anchor_text": "RoBERTa"}, {"url": "https://leaderboard.allenai.org/drop/submissions/public", "anchor_text": "LB"}, {"url": "https://arxiv.org/abs/1910.06701", "anchor_text": "Ran, et al., 2019"}, {"url": "https://arxiv.org/abs/1909.00109", "anchor_text": "Andor et al., 2019"}, {"url": "https://leaderboard.allenai.org/drop/submissions/public", "anchor_text": "LB"}, {"url": "https://openreview.net/forum?id=ryxjnREFwH", "anchor_text": "Chen, et al., 2020"}, {"url": "https://arxiv.org/abs/1908.05514", "anchor_text": "Hu, et al. 2019"}, {"url": "https://leaderboard.allenai.org/drop/submissions/public", "anchor_text": "LB"}, {"url": "https://arxiv.org/abs/1909.13375", "anchor_text": "[Segal, et al., 2020]"}, {"url": "https://leaderboard.allenai.org/drop/submissions/public", "anchor_text": "LB"}, {"url": "https://arxiv.org/abs/2009.07448", "anchor_text": "[Chen, et al., 2020]"}, {"url": "https://leaderboard.allenai.org/drop/submissions/public", "anchor_text": "LB"}, {"url": "https://arxiv.org/abs/2009.07448", "anchor_text": "[Chen, et al., 2020]"}, {"url": "https://deepart.io/#", "anchor_text": "DeepArt"}, {"url": "https://arxiv.org/abs/2009.07448", "anchor_text": "[Chen, et al., 2020]"}, {"url": "https://towardsdatascience.com/transformers-numeracy-pt-1-calculators-for-natural-language-e09a471c5ebe", "anchor_text": "previous article"}, {"url": "https://arxiv.org/abs/1909.07940", "anchor_text": "[Wallace, et al., 2019]"}, {"url": "https://www.aclweb.org/anthology/P19-1329/", "anchor_text": "[Naik, et al., 2019]"}, {"url": "https://arxiv.org/abs/1805.08154", "anchor_text": "[Spithourakis, et al., 2018]"}, {"url": "https://arxiv.org/abs/2004.04487", "anchor_text": "[Geva, et al., 2020]"}, {"url": "https://arxiv.org/abs/2009.07448", "anchor_text": "[Chen, et al., 2020]"}, {"url": "https://leaderboard.allenai.org/drop/submissions/public", "anchor_text": "DROP dataset"}, {"url": "https://www.linkedin.com/in/mihail-dungarov-cfa-a0291a88", "anchor_text": "LinkedIn"}, {"url": "https://medium.com/tag/naturallanguageprocessing?source=post_page-----615a346737c2---------------naturallanguageprocessing-----------------", "anchor_text": "Naturallanguageprocessing"}, {"url": "https://medium.com/tag/aritificial-intelligence?source=post_page-----615a346737c2---------------aritificial_intelligence-----------------", "anchor_text": "Aritificial Intelligence"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----615a346737c2---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/bert?source=post_page-----615a346737c2---------------bert-----------------", "anchor_text": "Bert"}, {"url": "https://medium.com/tag/data-science?source=post_page-----615a346737c2---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F615a346737c2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnlp-numeracy-pt-2-embeddings-language-models-and-calculators-615a346737c2&user=Mihail+Dungarov&userId=f882548c947b&source=-----615a346737c2---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F615a346737c2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnlp-numeracy-pt-2-embeddings-language-models-and-calculators-615a346737c2&user=Mihail+Dungarov&userId=f882548c947b&source=-----615a346737c2---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F615a346737c2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnlp-numeracy-pt-2-embeddings-language-models-and-calculators-615a346737c2&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----615a346737c2--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F615a346737c2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnlp-numeracy-pt-2-embeddings-language-models-and-calculators-615a346737c2&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----615a346737c2---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----615a346737c2--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----615a346737c2--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----615a346737c2--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----615a346737c2--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----615a346737c2--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----615a346737c2--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----615a346737c2--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----615a346737c2--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@mihail.dungarov?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@mihail.dungarov?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Mihail Dungarov"}, {"url": "https://medium.com/@mihail.dungarov/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "90 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff882548c947b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnlp-numeracy-pt-2-embeddings-language-models-and-calculators-615a346737c2&user=Mihail+Dungarov&userId=f882548c947b&source=post_page-f882548c947b--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Ff882548c947b%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnlp-numeracy-pt-2-embeddings-language-models-and-calculators-615a346737c2&user=Mihail+Dungarov&userId=f882548c947b&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}