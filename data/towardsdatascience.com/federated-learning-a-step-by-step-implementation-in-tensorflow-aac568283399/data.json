{"url": "https://towardsdatascience.com/federated-learning-a-step-by-step-implementation-in-tensorflow-aac568283399", "time": 1683005617.1452491, "path": "towardsdatascience.com/federated-learning-a-step-by-step-implementation-in-tensorflow-aac568283399/", "webpage": {"metadata": {"title": "Federated Learning: A Step by Step Implementation in Tensorflow | by Saheed Tijani | Towards Data Science", "h1": "Federated Learning: A Step by Step Implementation in Tensorflow", "description": "In this tutorial, I implemented the building blocks of Federated Learning (FL) and trained one from scratch on the MNIST digit data set. Prior to that, I briefly introduced the subject so as to drive\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.linkedin.com/pulse/federated-learning-why-what-how-saheed-tijani", "anchor_text": "introductory article", "paragraph_index": 0}, {"url": "https://www.kaggle.com/scolianni/mnistasjpg", "anchor_text": "MNIST data set from Kaggle", "paragraph_index": 3}, {"url": "https://github.com/datafrick/tutorial.git", "anchor_text": "my GitHub repository", "paragraph_index": 3}, {"url": "https://www.kaggle.com/scolianni/mnistasjpg", "anchor_text": "here", "paragraph_index": 5}, {"url": "https://www.linkedin.com/pulse/federated-learning-why-what-how-saheed-tijani", "anchor_text": "here", "paragraph_index": 33}, {"url": "https://arxiv.org/list/cs/recent", "anchor_text": "arxiv.org/cs", "paragraph_index": 33}, {"url": "http://www.linkedin.com/in/saheedtijani", "anchor_text": "www.linkedin.com/in/saheedtijani", "paragraph_index": 35}], "all_paragraphs": ["In this tutorial, I implemented the building blocks of Federated Learning (FL) and trained one from scratch on the MNIST digit data set. Prior to that, I briefly introduced the subject so as to drive home the overall point in the code. If this is your first time learning about FL, I\u2019m sure you will benefit from my recent introductory article of this technology on LinkedIn.", "Quality data exist as islands on edge devices like mobile phones and personal computers across the globe and are guarded by strict privacy preserving laws. Federated Learning provides a clever means of connecting machine learning models to these disjointed data regardless of their locations, and more importantly, without breaching privacy laws. Rather than taking the data to the model for training as per rule of thumb, FL takes the model to the data instead. All that\u2019s needed is the wiliness of the device hosting the data to commit it\u2019s self to the federation process.", "The FL architecture in it\u2019s basic form consists of a curator or server that sits at its centre and coordinates the training activities. Clients are mainly edge devices which could run into millions in number. These devices communicate at least twice with the server per training iteration. To start with, they each receive the current global model\u2019s weights from the server, train it on each of their local data to generate updated parameters which are then uploaded back to the server for aggregation. This cycle of communication persists until a pre-set epoch number or an accuracy condition is reached. In the Federated Averaging Algorithm, aggregation simply means an averaging operation. That is all there is to the training of a FL model. I hope you caught the most salient point in the process \u2014 rather than moving raw data around, we now communicate model weights.", "Now that we are clear on what FL is and how it work, let\u2019s move on to building one from scratch in Tensorflow and training it on the MNIST data set from Kaggle. Please note that this tutorial is for illustration only. We will neither go into the details of how the server-client communication works in FL nor the rudiments of secure aggregation. Since this is a simulation, clients will merely be represented by data shards and all local models will be trained on the same machine. Here is the link to the full code for this tutorial in my GitHub repository. With no further delays, let\u2019s get after it.", "Don\u2019t worry, I will provide details for each of the imported modules at the point of instantiating their respective objects.", "I\u2019m using the jpeg version of MNIST data set from here. It consists of 42000 digit images with each class kept in separate folder. I will load the data into memory using this code snippet and keep 10% of the data for testing the trained global model later on.", "On line 9, each image will be read from disk as grey scale and then flattened. The flattening step is import because we will be using a MLP network architecture later on. To obtain the class label of an image, we split its path string on line 11. Hope you noticed we also scaled the image to [0, 1] on line 13 to douse the impact of varying pixel brightness.", "A couple of steps took place in this snippet. We applied the load function defined in the previous code block to obtain the list of images (now in numpy arrays) and label lists. After that, we used the LabelBinarizer object from sklearn to 1-hot-encode the labels. Going forward, rather than having the label for digit 1 as number 1, it will now have the form[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]. With this labelling style, we\u2019ll be able to use the cross-entropy loss in Tensorflow as our model\u2019s loss function. Alternatively, I could have left the labels as it was and use the sparse-categorical-entropy loss instead. Finally, I used the sklearn\u2019s train_test_split object to split the data into a train/test with ratio 9:1.", "In the real world implementation of FL, each federated member will have its own data coupled with it in isolation. Remember the aim of FL is to ship models to data and not the other way around. The shard creation step here only happens in experiments. I will share the training set into 10 shards, one per client. I wrote a function called create_clients to achieve this.", "On line 13, I created a list of client names using the prefix (initials). On line 16\u201320, I zipped the data and label lists then randomised the resulting tuple list. Finally I created shards from the tuple list based on the desired number of clients (num_clients) on line 21. On line 26, a dictionary containing each client\u2019s name as key and their data share as value was returned. Let\u2019s now go ahead and apply this function to our training data set.", "Next is to process each of the client\u2019s data into tensorflow data set and batch them. To simplify this step and avoid repetition, I encapsulated the procedure into a small function named batch_data.", "I trust you remember that each of the client data sets came out as a (data, label) tuple list from create_clients. On line 9 above, I split the tuple into separate data and label lists. I then made a shuffled and batched tensorflow dataset object off these lists.", "While applying this function below, I will process the test set as well and keep it aside for later use.", "One thing I didn't mention in the introduction section is that FL is mostly suited for parameterized learning \u2014 all types of neural networks. Machine learning techniques such as KNN or it likes that merely store training data while learning might not benefit from FL. I\u2019m creating a 3-layer MLP to serve as the model for our classification task. I hope you still remember all those Keras modules we imported earlier, this is where they fit in.", "To build a new model, the build method will be invoked. It requires the input data\u2019s shape and the number of classes as arguments. With MNIST, the shape parameter will be 28*28*1 = 784,while the number of classes will be 10.", "Now is the time to define an optimizer, loss function and metrics to compile our models with later on.", "SGD is my default optimizer except when I have a reason not to use it. The loss function is categorical_crossentropy. And finally, the metric I will be using is accuracy. But something looks strange in the decay argument. What\u2019s comms_round? It\u2019s simply the number global epochs (aggregations) I will be running during training. So rather than decaying the learning rate with respect to the number of local epochs as you might be familiar with, here I want to decay with respect to the number of global aggregation. This is obviously an hyper parameter selection choice, but I found it to work pretty well while experimenting. I also found an academic report where this setting worked too [1].", "All I have done up to this point was pretty much standard as per deep learning pipeline. Of course with the exception of the data partitioning or client creation bit. I will now move on to Federated Averaging ( the vanilla algorithm for FL) which is the whole point of the this tutorial. The data I\u2019m using is horizontally partitioned, so I will simply be doing component wise parameter averaging which will be weighed based on the proportion of data points contributed by each participating client. Here\u2019s the federated averaging equation I\u2019m using, it comes one of the pioneering works on federated learning [2].", "Don\u2019t let the complex mathematical notations in the equation fool you, this is a pretty straight forward computation. On the right hand side, we are estimating the weight parameters for each client based on the loss values recorded across every data point they trained with. On the left, we scaled each of those parameters and sum them all component-wise.", "Below I have encapsulated this procedure into three simple functions.", "(1) weight_scalling_factor calculates the proportion of a client\u2019s local training data with the overall training data held by all clients. First we obtained the client\u2019s batch size and used that to calculate its number of data points. We then obtained the overall global training data size on line 6. Finally we calculated the scaling factor as a fraction on line 9. This sure can\u2019t be the approach in a real world application. The training data will be disjointed, therefore no single client can correctly estimate the quantity of the combined set. In that case, each client will be expected to indicate the number of data points they trained with while updating the server with new parameters after each local training step.", "(2) scale_model_weights scales each of the local model\u2019s weights based the value of their scaling factor calculated in (1)", "(3) sum_scaled_weights sums all clients\u2019 scaled weights together.", "The training logic has two main loops, the outer loop is for the global iteration, the inner is for iterating through client\u2019s local training. There\u2019s an implicit third one though, it accounts for the local epochs and will be taken care of by the epochs argument in our model.fit method.", "Starting out I built the global model with input shape of (784,) and number of classes as 10 \u2014 lines 2\u20133. I then stepped into the outer loop. First obtaining the initialised weights of the global model on line 9. Lines 15 and 16 shuffles the clients dictionary order to ensure randomness. From there, I started iterating through client training.", "For each client, I created a new model object, compiled it and set it\u2019s initialisation weights to the current parameters of the global model \u2014 lines 20\u201327. The local model (client) was then trained for one epoch. After training, the new weights were scaled and appended to the scaled_local_weight_list on line 35. That was it for local training.", "Moving back into the outer loop on line 41, I summed up all the scaled local trained weights (of course by components) and updated the global model to this new aggregate. That ends a full global training epoch.", "I ran 100 global training loops as stipulated by the comms_round and on line 48 tested the trained global model after each communication round our test data. Here is the snippet for the test logic:", "With 10 clients each running 1 local epoch on top of 100 global communication rounds, here is the truncated test result:", "Yes, our FL model results are great, 96.5% test accuracy after 100 communication rounds. But how does it compare to a standard SGD model trained on the same data set? To find out, I\u2019ll train a single 3-layer MLP model (rather 10 as we did in FL) on the combined training data. Remember the combined data was our training data prior to partitioning.", "To ensure an equal playing ground, I will retain every hyper parameter used for the FL training except the batch size. Rather than using 32 , our SGD\u2019s batch size will be 320. With this setting, we are sure that the SGD model would see exactly the same number of training samples per epoch as the global model did per communication round in FL.", "There you have it, a 94.5% test accuracy for the SGD model after 100 epochs. Isn\u2019t it surprising that the FL performed a little better than its SGD counterpart with this data set? I warn you not to get too excited about this though. These kind of results are not likely in real world scenario. Yeah! Real world federated data held by clients are mostly NON independent and identically distributed (IID).", "For example, we could have replicated this scenario by constructing our client shards above such that each comprises of images from a single class \u2014 e.g client_1 having only images of digit 1, client_2 having only images of digit 2 and so on. This arrangement would have lead to a significant reduction in the performance of the FL model. I leave this as an exercise for the reader to try out. Meanwhile, here is the code you could use to shard any classification data in a non-IID manner.", "Through this article, I introduced the concept of Federated Learning and took you through the tensorflow implementation of it basic form. I encourage you to check my recent article on LinkedIn here for broader introduction of this technology, particularly if you are not clear about its workings or want to learn more about how it could be applied. For researchers wanting to study this subject in more depth, there are lots of journals around FL on arxiv.org/cs , mostly pushing boundaries on its implementation and addressing its numerous challenges.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "A technology enthusiast and machine learning practitioner. I\u2019m passionate about learning and helping others to do so. www.linkedin.com/in/saheedtijani"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Faac568283399&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffederated-learning-a-step-by-step-implementation-in-tensorflow-aac568283399&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffederated-learning-a-step-by-step-implementation-in-tensorflow-aac568283399&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffederated-learning-a-step-by-step-implementation-in-tensorflow-aac568283399&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffederated-learning-a-step-by-step-implementation-in-tensorflow-aac568283399&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----aac568283399--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----aac568283399--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@saheed.tijani58?source=post_page-----aac568283399--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@saheed.tijani58?source=post_page-----aac568283399--------------------------------", "anchor_text": "Saheed Tijani"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F53a1bb9106c4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffederated-learning-a-step-by-step-implementation-in-tensorflow-aac568283399&user=Saheed+Tijani&userId=53a1bb9106c4&source=post_page-53a1bb9106c4----aac568283399---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Faac568283399&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffederated-learning-a-step-by-step-implementation-in-tensorflow-aac568283399&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Faac568283399&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffederated-learning-a-step-by-step-implementation-in-tensorflow-aac568283399&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.linkedin.com/pulse/federated-learning-why-what-how-saheed-tijani", "anchor_text": "introductory article"}, {"url": "https://unsplash.com/@andrzejsuwara?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Andrzej Suwara"}, {"url": "https://unsplash.com/s/photos/multiple-islands?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://blog.ml.cmu.edu/2019/11/12/federated-learning-challenges-methods-and-future-directions/", "anchor_text": "Image reference"}, {"url": "https://www.kaggle.com/scolianni/mnistasjpg", "anchor_text": "MNIST data set from Kaggle"}, {"url": "https://github.com/datafrick/tutorial.git", "anchor_text": "my GitHub repository"}, {"url": "https://www.kaggle.com/scolianni/mnistasjpg", "anchor_text": "here"}, {"url": "https://www.linkedin.com/pulse/federated-learning-why-what-how-saheed-tijani", "anchor_text": "here"}, {"url": "https://arxiv.org/list/cs/recent", "anchor_text": "arxiv.org/cs"}, {"url": "https://medium.com/tag/federated-learning?source=post_page-----aac568283399---------------federated_learning-----------------", "anchor_text": "Federated Learning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----aac568283399---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deeplearing?source=post_page-----aac568283399---------------deeplearing-----------------", "anchor_text": "Deeplearing"}, {"url": "https://medium.com/tag/data-science?source=post_page-----aac568283399---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/tensorflow?source=post_page-----aac568283399---------------tensorflow-----------------", "anchor_text": "TensorFlow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Faac568283399&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffederated-learning-a-step-by-step-implementation-in-tensorflow-aac568283399&user=Saheed+Tijani&userId=53a1bb9106c4&source=-----aac568283399---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Faac568283399&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffederated-learning-a-step-by-step-implementation-in-tensorflow-aac568283399&user=Saheed+Tijani&userId=53a1bb9106c4&source=-----aac568283399---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Faac568283399&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffederated-learning-a-step-by-step-implementation-in-tensorflow-aac568283399&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----aac568283399--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Faac568283399&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffederated-learning-a-step-by-step-implementation-in-tensorflow-aac568283399&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----aac568283399---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----aac568283399--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----aac568283399--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----aac568283399--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----aac568283399--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----aac568283399--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----aac568283399--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----aac568283399--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----aac568283399--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@saheed.tijani58?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@saheed.tijani58?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Saheed Tijani"}, {"url": "https://medium.com/@saheed.tijani58/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "91 Followers"}, {"url": "http://www.linkedin.com/in/saheedtijani", "anchor_text": "www.linkedin.com/in/saheedtijani"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F53a1bb9106c4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffederated-learning-a-step-by-step-implementation-in-tensorflow-aac568283399&user=Saheed+Tijani&userId=53a1bb9106c4&source=post_page-53a1bb9106c4--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F9227b152da5e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffederated-learning-a-step-by-step-implementation-in-tensorflow-aac568283399&newsletterV3=53a1bb9106c4&newsletterV3Id=9227b152da5e&user=Saheed+Tijani&userId=53a1bb9106c4&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}