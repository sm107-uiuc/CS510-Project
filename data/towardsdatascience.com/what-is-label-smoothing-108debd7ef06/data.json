{"url": "https://towardsdatascience.com/what-is-label-smoothing-108debd7ef06", "time": 1683002136.013067, "path": "towardsdatascience.com/what-is-label-smoothing-108debd7ef06/", "webpage": {"metadata": {"title": "What is Label Smoothing?. A technique to make your model less\u2026 | by Wanshun Wong | Towards Data Science", "h1": "What is Label Smoothing?", "description": "When using deep learning models for classification tasks, we usually encounter the following problems: overfitting, and overconfidence. Overfitting is well studied and can be tackled with early\u2026"}, "outgoing_paragraph_urls": [], "all_paragraphs": ["When using deep learning models for classification tasks, we usually encounter the following problems: overfitting, and overconfidence. Overfitting is well studied and can be tackled with early stopping, dropout, weight regularization etc. On the other hand, we have less tools to tackle overconfidence. Label smoothing is a regularization technique that addresses both problems.", "A classification model is calibrated if its predicted probabilities of outcomes reflect their accuracy. For example, consider 100 examples within our dataset, each with predicted probability 0.9 by our model. If our model is calibrated, then 90 examples should be classified correctly. Similarly, among another 100 examples with predicted probabilities 0.6, we would expect only 60 examples being correctly classified.", "An overconfident model is not calibrated and its predicted probabilities are consistently higher than the accuracy. For example, it may predict 0.9 for inputs where the accuracy is only 0.6. Notice that models with small test errors can still be overconfident, and therefore can benefit from label smoothing.", "Label smoothing replaces one-hot encoded label vector y_hot with a mixture of y_hot and the uniform distribution:", "where K is the number of label classes, and \u03b1 is a hyperparameter that determines the amount of smoothing. If \u03b1 = 0, we obtain the original one-hot encoded y_hot. If \u03b1 = 1, we get the uniform distribution.", "Label smoothing is used when the loss function is cross entropy, and the model applies the softmax function to the penultimate layer\u2019s logit vectors z to compute its output probabilities p. In this setting, the gradient of the cross entropy loss function with respect to the logits is simply", "where y is the label distribution. In particular, we can see that", "One-hot encoded labels encourages largest possible logit gaps to be fed into the softmax function. Intuitively, large logit gaps combined with the bounded gradient will make the model less adaptive and too confident about its predictions.", "In contrast, smoothed labels encourages small logit gaps, as demonstrated by the example below. It is shown in [3] that this results in better model calibration and prevents overconfident predictions.", "Suppose we have K = 3 classes, and our label belongs to the 1st class. Let [a, b, c] be our logit vector.", "If we do not use label smoothing, the label vector is the one-hot encoded vector [1, 0, 0]. Our model will make a \u226b b and a \u226b c. For example, applying softmax to the logit vector [10, 0, 0] gives [0.9999, 0, 0] rounded to 4 decimal places.", "If we use label smoothing with \u03b1 = 0.1, the smoothed label vector \u2248 [0.9333, 0.0333, 0.0333]. The logit vector [3.3322, 0, 0] approximates the smoothed label vector to 4 decimal places after softmax, and it has a smaller gap. This is why we call label smoothing a regularization technique as it restrains the largest logit from becoming much bigger than the rest.", "Q: When do we use label smoothing?", "A: Whenever a classification neural network suffers from overfitting and/or overconfidence, we can try label smoothing.", "Q: How do we choose \u03b1?", "A: Just like other regularization hyperparameters, there is no formula for choosing \u03b1. It is usually done by trial and error, and \u03b1 = 0.1 is a good place to start.", "Q: Can we use distributions other than uniform distribution in label smoothing?", "A: Technically yes. In [4] the theoretical groundwork is developed for arbitrary distributions. That being said, the vast majority of empirical studies on label smoothing use uniform distribution.", "Q: Is label smoothing used outside deep learning?", "A: Not really. Most popular non-deep learning methods do not use the softmax function. Thus label smoothing is usually not applicable.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F108debd7ef06&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-label-smoothing-108debd7ef06&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-label-smoothing-108debd7ef06&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-label-smoothing-108debd7ef06&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-label-smoothing-108debd7ef06&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----108debd7ef06--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----108debd7ef06--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@wanshunwong?source=post_page-----108debd7ef06--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@wanshunwong?source=post_page-----108debd7ef06--------------------------------", "anchor_text": "Wanshun Wong"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb145fb04b8bd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-label-smoothing-108debd7ef06&user=Wanshun+Wong&userId=b145fb04b8bd&source=post_page-b145fb04b8bd----108debd7ef06---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F108debd7ef06&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-label-smoothing-108debd7ef06&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F108debd7ef06&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-label-smoothing-108debd7ef06&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@xusanfeng?utm_source=medium&utm_medium=referral", "anchor_text": "Levi XU"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://www.tensorflow.org/api_docs/python/tf/keras/losses/BinaryCrossentropy", "anchor_text": "BinaryCrossentropy"}, {"url": "https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy", "anchor_text": "CategoricalCrossentropy"}, {"url": "https://github.com/OpenNMT/OpenNMT-py/blob/e8622eb5c6117269bb3accd8eb6f66282b5e67d9/onmt/utils/loss.py#L186", "anchor_text": "example"}, {"url": "http://www.deeplearningbook.org/", "anchor_text": "Deep Learning"}, {"url": "http://proceedings.mlr.press/v70/guo17a/guo17a.pdf", "anchor_text": "On Calibration of Modern Neural Networks"}, {"url": "https://papers.nips.cc/paper/8717-when-does-label-smoothing-help", "anchor_text": "When Does Label Smoothing Help?"}, {"url": "https://arxiv.org/abs/1701.06548", "anchor_text": "Regularizing Neural Networks by Penalizing Confident Output Distributions"}, {"url": "https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Szegedy_Rethinking_the_Inception_CVPR_2016_paper.pdf", "anchor_text": "Rethinking the Inception Architecture for Computer Vision"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----108debd7ef06---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----108debd7ef06---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/label-smoothing?source=post_page-----108debd7ef06---------------label_smoothing-----------------", "anchor_text": "Label Smoothing"}, {"url": "https://medium.com/tag/model-calibration?source=post_page-----108debd7ef06---------------model_calibration-----------------", "anchor_text": "Model Calibration"}, {"url": "https://medium.com/tag/regularization?source=post_page-----108debd7ef06---------------regularization-----------------", "anchor_text": "Regularization"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F108debd7ef06&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-label-smoothing-108debd7ef06&user=Wanshun+Wong&userId=b145fb04b8bd&source=-----108debd7ef06---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F108debd7ef06&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-label-smoothing-108debd7ef06&user=Wanshun+Wong&userId=b145fb04b8bd&source=-----108debd7ef06---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F108debd7ef06&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-label-smoothing-108debd7ef06&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----108debd7ef06--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F108debd7ef06&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-label-smoothing-108debd7ef06&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----108debd7ef06---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----108debd7ef06--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----108debd7ef06--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----108debd7ef06--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----108debd7ef06--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----108debd7ef06--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----108debd7ef06--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----108debd7ef06--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----108debd7ef06--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@wanshunwong?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@wanshunwong?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Wanshun Wong"}, {"url": "https://medium.com/@wanshunwong/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "263 Followers"}, {"url": "https://www.linkedin.com/in/wanshunwong/", "anchor_text": "https://www.linkedin.com/in/wanshunwong/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb145fb04b8bd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-label-smoothing-108debd7ef06&user=Wanshun+Wong&userId=b145fb04b8bd&source=post_page-b145fb04b8bd--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fa149bfd5dd79&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-label-smoothing-108debd7ef06&newsletterV3=b145fb04b8bd&newsletterV3Id=a149bfd5dd79&user=Wanshun+Wong&userId=b145fb04b8bd&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}