{"url": "https://towardsdatascience.com/a-journey-to-airflow-on-kubernetes-472df467f556", "time": 1683011224.1406279, "path": "towardsdatascience.com/a-journey-to-airflow-on-kubernetes-472df467f556/", "webpage": {"metadata": {"title": "A journey to Airflow on Kubernetes | by Marcelo Rabello Rossi | Towards Data Science", "h1": "A journey to Airflow on Kubernetes", "description": "How I got Apache Airflow running on Kubernetes in a logical way, with tips, resource examples and minor details."}, "outgoing_paragraph_urls": [{"url": "https://medium.com/analytics-and-data/10-benefits-to-using-airflow-33d312537bae", "anchor_text": "\u00b9", "paragraph_index": 0}, {"url": "https://www.solita.fi/en/blogs/why-we-switched-to-apache-airflow/", "anchor_text": "\u00b2", "paragraph_index": 0}, {"url": "https://robinhood.engineering/why-robinhood-uses-airflow-aed13a9a90c8", "anchor_text": "\u00b3", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/airflow-how-and-when-to-use-it-2e07108ac9f5", "anchor_text": "\u2074", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/getting-started-with-apache-airflow-df1aa77d7b1b", "anchor_text": "\u2075", "paragraph_index": 0}, {"url": "https://airflow.apache.org/docs/stable/_api/airflow/operators/index.html", "anchor_text": "\u2076", "paragraph_index": 1}, {"url": "https://airflow.apache.org/docs/stable/_api/airflow/contrib/operators/index.html", "anchor_text": "\u2077", "paragraph_index": 1}, {"url": "https://institucional.ifood.com.br/", "anchor_text": "iFood", "paragraph_index": 2}, {"url": "https://www.katacoda.com/courses/kubernetes", "anchor_text": "\u2078", "paragraph_index": 7}, {"url": "https://github.com/hgrif/airflow-tutorial", "anchor_text": "\u2079", "paragraph_index": 7}, {"url": "https://github.com/apache/airflow/tree/master/chart", "anchor_text": "\u00b9\u2070", "paragraph_index": 8}, {"url": "https://github.com/helm/charts", "anchor_text": "https://github.com/helm/charts", "paragraph_index": 9}, {"url": "https://github.com/apache/airflow/tree/master/chart", "anchor_text": "\u00b9\u2070", "paragraph_index": 9}, {"url": "https://github.com/kubernetes-sigs/aws-efs-csi-driver", "anchor_text": "\u00b9\u00b9", "paragraph_index": 13}, {"url": "https://github.com/godaddy/kubernetes-external-secrets", "anchor_text": "\u00b9\u00b2", "paragraph_index": 16}], "all_paragraphs": ["My humble opinion on Apache Airflow: basically, if you have more than a couple of automated tasks to schedule, and you are fiddling around with cron tasks that run even when some dependency of them fails, you should give it a try. But if you are not willing to just accept my words, feel free to check these posts.\u00b9 \u00b2 \u00b3 Delve into Airflow concepts and how it works is beyond the scope of this article. For that matter, please check these other posts.\u2074 \u2075", "Long story short, its task definitions are code based, what means that they could be as dynamic as you wish. You can create tasks and define your task dependencies based on variables or conditionals. It has plenty of native operators (definitions of task types)\u2076 that integrate your workflow with lots of other tools and allow you to run from the most basic shell scripts to parallel data processing with Apache Spark, and a plethora of other options. Contributor operators\u2077 are also available for a great set of commercial tools and the list keeps growing every day. These operators are python classes, so they are extensible and could be modified to fit your needs. You can even create your own operators from scratch, inheriting from the BaseOperator class. Also, it makes your workflow scalable to hundreds or even thousands of tasks with little effort using its distributed executors such as Celery or Kubernetes.", "We are using Airflow in iFood since 2018. Our first implementation was really nice, based on docker containers to run each task in an isolated environment. It underwent a lot of changes since then, from a simple tool to serve our team\u2019s workload to a task scheduling platform to serve the more than 200 people with a lot of abstractions on the top of it. At the end, it does not matter if you are a software engineer with years of experience or a business analyst with minimal SQL knowledge, you can schedule your task using our platform writing a yaml file with three simple fields: the ID of your task, the path of the file containing your queries and the name of its table dependencies (i.e. to run my task I depend on the tables orders and users), and voil\u00e0, you have your task scheduled to run daily. But this, unfortunately, is a topic for a future article.", "It was obvious that we would need to scale our application from an AWS t2.medium EC2 instance to something more powerful. Our first approaches were to scale vertically to an r4.large instance, and then to an r4.xlarge, but the memory usage was constantly increasing.", "Our company grows fast. There are dozens of tasks being created every day and suddenly we would be running on an r4.16xlarge instance. We needed a way to scale the application horizontally and, more than that, to upscale it considering the peak hours and to downscale it at dawn to minimize needless costs. At that point, we were migrating all our platforms to run on a Kubernetes cluster, so why not Airflow?", "I searched on the internet, from the official Apache Airflow documentation to Medium articles, digging for information on how to run a simple implementation of Airflow on Kubernetes with the KubernetesExecutor (I was aware of the CeleryExecutor existence, but it would not fit our needs, considering that you need to spin your workers upfront, with no native auto-scaling). I found a lot of people talking about the benefits of running Airflow on Kubernetes, the architecture behind it and a bunch of Helm charts, but little information on how to deploy it, piece by piece, in a logical way for a Kubernetes beginner. And that is the main point of this article. Assuming that you know Apache Airflow, and how its components work together, the idea is to show you how you can deploy it to run on Kubernetes leveraging the benefits of the KubernetesExecutor, with some extra information on the Kubernetes resources involved (yaml files). The examples will be AWS-based, but I am sure that with little research you can port the information to any cloud service you want or even run the code on-prem.", "To fully understand the sections below and get things running, I am assuming that you have:", "If you do not, I recommend that you play a little with Kubernetes and Airflow locally. You can find awesome tutorials in the internet, even at the official websites. For Kubernetes, you can start with the Katacoda tutorials.\u2078 Regarding Apache Airflow, this was the first tutorial I ever tried.\u2079", "As a starting point, I found a way to get the Kubernetes resource yaml files from the official Helm chart available at the Airflow git repository.\u00b9\u2070 That brought me a lot of resources, some of them came empty (probably because I used the base values.yaml to fill the templates used by Helm) and some of them were useless to the KubernetesExecutor approach (i.e. I do not need a Redis cluster, or a Flower resource, or a result back-end, as these are specific to Celery). Removing those useless resources, I ended up with something around 15 resource\u00a0files and some of them I did not even know at that time. Kinda overwhelming! I also removed all resources that were related to the PostgreSQL instance (i.e. pgbouncer), because I knew that I would use an AWS RDS instance, external to the Kubernetes cluster.", "Obs: I had these charts locally, so when I executed the helm template command, helm whined about not finding the PostgreSQL charts (it will not happen if you are using the Helm repositories). If that is your case, just create the path charts/ inside the folder containing your helm chart and put the postgresql/ helm chart folder inside of it (available at the official Helm charts github repository). It is also important to notice that the Apache Airflow helm chart available at https://github.com/helm/charts will bring you a different set of resources when compared to the chart I used.\u00b9\u2070", "After all the cleaning, I ended up with these 12 resources:", "Most of the articles I found describe two ways to store DAG information: storing the DAGs on a persistent volume accessible from multiple AWS availability zones, such as the AWS Elastic File System (EFS), or syncing them from a git repository to an ephemeral volume mounted inside the cluster. If that pod dies, when another one is created, it will sync with the repository again to get the last modifications.", "Due to our present workflow, we need to build our DAGs dynamically from lots of tasks written in yaml files, meaning that our DAGs are not ready when the files are versioned on a git repository. A simple git-sync to bring information would not work for us, but it could be a starting point. Considering that we also needed some kind of persistence for our logs, we decided to go for the EFS approach too, using some kind of hybrid of what we found online: git-sync our yaml files to a PersistentVolume mounted on the top of an EFS, and to have another pod processing it and throwing the freshly-built DAGs into the folder that the scheduler and the webserver are constantly watching to fill the DagBag.", "As shown above, to mount the EFS inside the EKS cluster, I used the official AWS CSI driver,\u00b9\u00b9 that must be installed in the cluster. And beyond the driver, this approach accounts for five Kubernetes resources:", "These resources were not present in the initial list, as the original deployments used emptyDir volumes instead.", "Anyone that worked with Apache Airflow for some time knows that the airflow.cfg file (and maybe webserver_config.py file) is pretty important to set the things up. But throwing it inside the EFS volume did not seem wise, because of the sensitive information it contains (database passwords, fernet key). Then, I found out that the Kubernetes way to store configuration files is to use ConfigMap, a kind of \"volume\" that you mount inside the pods to expose a configuration file for them. And there is the Kubernetes Secret too, to store sensitive data. They work together, so I can reference a Secret inside a ConfigMap, or even pass a Secret to an environment variable. Mission accomplished!", "As you learn a little bit more about Kubernetes, you will notice that the \u201cplain\u201d secrets are somewhat unsafe to version in a repository. They contain base64 strings that can be readily \u201cdecrypted\u201d in your terminal using the base64 -d command. Take a look on this ExternalSecrets API,\u00b9\u00b2 to store your secrets on AWS Parameter Store and retrieve them from there.", "If you check the list of files above, you will notice that the ConfigMap is already there, you just have to customize it.", "My little experience with Kubernetes was enough at that point to make me think that I would need at least two deployments: one for the scheduler and one for the webserver. And they were there, lying inside the scheduler and webserver folders generated by the Helm Chart explosion. There was a third deployment, of a statsd application, that I found later to be related to metrics collection inside the application. Cool, one thing less to worry! Prometheus will be happy to scrape it.", "I opened the files and noticed that they have some familiar environment variables, related to the fernet key and the database connection string. I filled them with the data retrieved by the Kubernetes secrets. I needed to tweak the volume part a little bit, to match my EFS PersistentVolume and PersistentVolumeClaim.", "It is easy to notice these shell scripts being executed as init containers. They are related to the database migrations that happen when Airflow starts. The scheduler pod runs the migrations as soon as it starts, and the webserver pod keeps waiting for it to finish before starting the webserver container. The webserver deployment has a very similar structure, so I took the liberty of omitting it.", "There was. A Kubernetes service resource exposing the port 8080 of the container. Later I included an Ingress resource to give it an AWS Route53 friendly DNS.", "The statsd application also listens at an endpoint and has a service associated to it. Both the services were included in the files exported by the helm template command.", "I tried to apply those configurations to the cluster. Scheduler and webserver were up, both connected to my external RDS PostgreSQL database. I thought: \u201cIf I throw some DAGs into the dags folder, then it should work, right?\u201d And it kinda did! I created a simple DAG with one task based on the KubernetesPodOperator, using a container image stored at the AWS Elastic Container Registry. I double checked if my pod would be allowed to access the ECR repository.", "Then, I triggered the DAG, but it failed\u00a0(you\u00a0really\u00a0didn\u2019t\u00a0think\u00a0it\u00a0would\u00a0be\u00a0that\u00a0easy,\u00a0right?). Checking the logs I noticed that it happened due some kind of permission issue. My scheduler did not have the permission to spawn new pods. And then I understood the need for that ServiceAccount resources scattered among the folders, and the ClusterRole and ClusterRoleBinding stuff. These guys are there to allow your resources to spawn new resources. After all the configuration, I could make my task run successfully. The KubernetesPodOperator also has the service_account_name parameter, that should be filled with a ServiceAccount resource name able to spawn pods, because that is what it will do: spawn another pod with the image you passed as an argument to the image parameter. That pod will be in charge of running your task.", "If you want to run tasks directly from your webserver, clicking on that \u201cRun\u201d button inside the task menu, you must give your webserver ServiceAccount the permissions to watch and spawn pods too. If you forget that, your tasks will be triggered, but they will never run.", "If you are running your stuff on AWS, you need to make sure your pods will be able to access all AWS resources, such as S3, DynamoDB tables, EMR, and so on. To do so, you need to bind your ServiceAccount resource to an AWS IAM role with IAM policies attached, to grant you all the access you need. Just give your IAM role an assume role policy:", "The ServiceAccount for your worker and tasks should be linked to the IAM Role attached to the policy above. You can do it using annotations:", "If you are following this journey as a tutorial, after all the tweaking you can just create all the above resources in your cluster:", "But, wait! Do not apply them yet! If you are a watchful reader, you noticed that most of the resources above makes reference to the airflow-on-k8s namespace. A Namespace is a way to tell Kubernetes that all resources in the same namespace are somewhat related (i.e. they are part of the same project) and is a nice way to organize things inside the cluster. You should declare your Namespace resource inside the resources/ folder and apply it before applying everything else, otherwise you will get an error.", "FYI, not every resource on Kubernetes is namespaced (i.e. PersistentVolume, StorageClass and other low-level resources), and that is why some of them do not have any reference to a namespace.", "And that was a fast-forward take on my journey to deploy Airflow on Kubernetes. I tried to cover all kinds of resources generated by the helm chart export, but feel free to ask your questions in the comments section if you think I left something behind.", "These yaml resources above were taken from a functional deployment I made. Some of them I built from scratch, and others I adapted from the ones I exported. I suggest that you take your time understanding them and making changes for better organization and performance.", "There is a lot more you can do to\u00a0get\u00a0the\u00a0most\u00a0of\u00a0this\u00a0implementation. You can set the limits and requests fields for your containers\u00a0inside\u00a0the\u00a0deployments, to make sure they will have the necessary resources available for them to work properly. Going further on the benefits of Kubernetes, you will see that the KubernetesPodOperator allows you to label your pods and pass a lot of Kubernetes configurations to it, such as affinities, tolerations and a bunch of other stuff. If you have tainted nodes, you can assure that just some specific pods will run on them, reserving the most powerful nodes to the most critical tasks.", "If you tried this setup and have something to add, something that worked like a charm or turned out to be a bad choice, please tell us in the comments.", "The Apache Airflow logo is either registered trademark or trademark of the Apache Software Foundation in the United States and/or other countries. No endorsement by The Apache Software Foundation is implied by the use of these marks. The Kubernetes logo files are licensed under a choice of either Apache-2.0 or CC-BY-4.0 (Creative Commons Attribution 4.0 International).", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Senior data engineer, computer scientist, chemist and powered by good music"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F472df467f556&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-journey-to-airflow-on-kubernetes-472df467f556&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-journey-to-airflow-on-kubernetes-472df467f556&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-journey-to-airflow-on-kubernetes-472df467f556&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-journey-to-airflow-on-kubernetes-472df467f556&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----472df467f556--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----472df467f556--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://rabello.medium.com/?source=post_page-----472df467f556--------------------------------", "anchor_text": ""}, {"url": "https://rabello.medium.com/?source=post_page-----472df467f556--------------------------------", "anchor_text": "Marcelo Rabello Rossi"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fba1dc7735636&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-journey-to-airflow-on-kubernetes-472df467f556&user=Marcelo+Rabello+Rossi&userId=ba1dc7735636&source=post_page-ba1dc7735636----472df467f556---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F472df467f556&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-journey-to-airflow-on-kubernetes-472df467f556&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F472df467f556&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-journey-to-airflow-on-kubernetes-472df467f556&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/analytics-and-data/10-benefits-to-using-airflow-33d312537bae", "anchor_text": "\u00b9"}, {"url": "https://www.solita.fi/en/blogs/why-we-switched-to-apache-airflow/", "anchor_text": "\u00b2"}, {"url": "https://robinhood.engineering/why-robinhood-uses-airflow-aed13a9a90c8", "anchor_text": "\u00b3"}, {"url": "https://towardsdatascience.com/airflow-how-and-when-to-use-it-2e07108ac9f5", "anchor_text": "\u2074"}, {"url": "https://towardsdatascience.com/getting-started-with-apache-airflow-df1aa77d7b1b", "anchor_text": "\u2075"}, {"url": "https://airflow.apache.org/docs/stable/_api/airflow/operators/index.html", "anchor_text": "\u2076"}, {"url": "https://airflow.apache.org/docs/stable/_api/airflow/contrib/operators/index.html", "anchor_text": "\u2077"}, {"url": "https://institucional.ifood.com.br/", "anchor_text": "iFood"}, {"url": "https://www.katacoda.com/courses/kubernetes", "anchor_text": "\u2078"}, {"url": "https://github.com/hgrif/airflow-tutorial", "anchor_text": "\u2079"}, {"url": "https://github.com/apache/airflow/tree/master/chart", "anchor_text": "\u00b9\u2070"}, {"url": "https://github.com/helm/charts", "anchor_text": "https://github.com/helm/charts"}, {"url": "https://github.com/apache/airflow/tree/master/chart", "anchor_text": "\u00b9\u2070"}, {"url": "https://github.com/kubernetes-sigs/aws-efs-csi-driver", "anchor_text": "\u00b9\u00b9"}, {"url": "https://github.com/godaddy/kubernetes-external-secrets", "anchor_text": "\u00b9\u00b2"}, {"url": "https://medium.com/analytics-and-data/10-benefits-to-using-airflow-33d312537bae", "anchor_text": "https://medium.com/analytics-and-data/10-benefits-to-using-airflow-33d312537bae"}, {"url": "https://www.solita.fi/en/blogs/why-we-switched-to-apache-airflow/", "anchor_text": "https://www.solita.fi/en/blogs/why-we-switched-to-apache-airflow/"}, {"url": "https://robinhood.engineering/why-robinhood-uses-airflow-aed13a9a90c8", "anchor_text": "https://robinhood.engineering/why-robinhood-uses-airflow-aed13a9a90c8"}, {"url": "https://towardsdatascience.com/airflow-how-and-when-to-use-it-2e07108ac9f5", "anchor_text": "https://towardsdatascience.com/airflow-how-and-when-to-use-it-2e07108ac9f5"}, {"url": "https://towardsdatascience.com/getting-started-with-apache-airflow-df1aa77d7b1b", "anchor_text": "https://towardsdatascience.com/getting-started-with-apache-airflow-df1aa77d7b1b"}, {"url": "https://airflow.apache.org/docs/stable/_api/airflow/operators/index.html", "anchor_text": "https://airflow.apache.org/docs/stable/_api/airflow/operators/index.html"}, {"url": "https://airflow.apache.org/docs/stable/_api/airflow/contrib/operators/index.html", "anchor_text": "https://airflow.apache.org/docs/stable/_api/airflow/contrib/operators/index.html"}, {"url": "https://www.katacoda.com/courses/kubernetes", "anchor_text": "https://www.katacoda.com/courses/kubernetes"}, {"url": "https://github.com/hgrif/airflow-tutorial", "anchor_text": "https://github.com/hgrif/airflow-tutorial"}, {"url": "https://github.com/apache/airflow/tree/master/chart", "anchor_text": "https://github.com/apache/airflow/tree/master/chart"}, {"url": "https://github.com/kubernetes-sigs/aws-efs-csi-driver", "anchor_text": "https://github.com/kubernetes-sigs/aws-efs-csi-driver"}, {"url": "https://github.com/godaddy/kubernetes-external-secrets", "anchor_text": "https://github.com/godaddy/kubernetes-external-secrets"}, {"url": "https://medium.com/tag/airflow?source=post_page-----472df467f556---------------airflow-----------------", "anchor_text": "Airflow"}, {"url": "https://medium.com/tag/kubernetes?source=post_page-----472df467f556---------------kubernetes-----------------", "anchor_text": "Kubernetes"}, {"url": "https://medium.com/tag/aws?source=post_page-----472df467f556---------------aws-----------------", "anchor_text": "AWS"}, {"url": "https://medium.com/tag/data-engineering?source=post_page-----472df467f556---------------data_engineering-----------------", "anchor_text": "Data Engineering"}, {"url": "https://medium.com/tag/editors-pick?source=post_page-----472df467f556---------------editors_pick-----------------", "anchor_text": "Editors Pick"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F472df467f556&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-journey-to-airflow-on-kubernetes-472df467f556&user=Marcelo+Rabello+Rossi&userId=ba1dc7735636&source=-----472df467f556---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F472df467f556&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-journey-to-airflow-on-kubernetes-472df467f556&user=Marcelo+Rabello+Rossi&userId=ba1dc7735636&source=-----472df467f556---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F472df467f556&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-journey-to-airflow-on-kubernetes-472df467f556&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----472df467f556--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F472df467f556&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-journey-to-airflow-on-kubernetes-472df467f556&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----472df467f556---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----472df467f556--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----472df467f556--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----472df467f556--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----472df467f556--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----472df467f556--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----472df467f556--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----472df467f556--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----472df467f556--------------------------------", "anchor_text": ""}, {"url": "https://rabello.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://rabello.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Marcelo Rabello Rossi"}, {"url": "https://rabello.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "83 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fba1dc7735636&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-journey-to-airflow-on-kubernetes-472df467f556&user=Marcelo+Rabello+Rossi&userId=ba1dc7735636&source=post_page-ba1dc7735636--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F5425adaa8820&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-journey-to-airflow-on-kubernetes-472df467f556&newsletterV3=ba1dc7735636&newsletterV3Id=5425adaa8820&user=Marcelo+Rabello+Rossi&userId=ba1dc7735636&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}