{"url": "https://towardsdatascience.com/head-pruning-in-transformer-models-ec222ca9ece7", "time": 1683007160.862491, "path": "towardsdatascience.com/head-pruning-in-transformer-models-ec222ca9ece7/", "webpage": {"metadata": {"title": "Head Pruning in Transformer Models! | by Gaurav Ghati | Towards Data Science", "h1": "Head Pruning in Transformer Models!", "description": "As from the above figure you can see that the transformer have three types of attention implementations that are: - Multi-head attention(MHA) of encoder, - Masked multi-head attention of decoder, \u2026"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/pdf/1905.09418.pdf", "anchor_text": "paper", "paragraph_index": 6}, {"url": "https://arxiv.org/pdf/1905.09418.pdf", "anchor_text": "paper", "paragraph_index": 21}, {"url": "http://linkedin.com/in/gauravghati/", "anchor_text": "LinkedIn", "paragraph_index": 27}, {"url": "https://twitter.com/GauravGhati/", "anchor_text": "Twitter", "paragraph_index": 27}, {"url": "http://gauravghati.world/", "anchor_text": "portfolio", "paragraph_index": 27}], "all_paragraphs": ["Just for the recap, here is the basic transformer model", "As from the above figure you can see that the transformer have three types of attention implementations that are:- Multi-head attention(MHA) of encoder,- Masked multi-head attention of decoder,- Multi-head attention encoder-decoder", "Each MHA consists of the concatenation of several scaled dot-product attention heads which are running in parallel unlike recurrent networks, that\u2019s why attention is better than almost all RNNs.", "Each head is attention function of query, key and value with trainable parameters (W\u1d62\u1d69, W\u1d62\u1d4f, W\u1d62\u1d5b).", "All 3 types of MHA play a specific and important role in different ways.", "Self-attention in MHA is the key component of the transformer since MHAs depends on attention heads, it\u2019s important for them to be accurate, depending on multiple heads can improve the efficiency up to 1 BLEU scores.", "Surprising observations are made in the paper, that even after training models normally(with all heads), many heads can be removed at a test time and it will not significantly affect the BLEU score, in fact, some cases removing few heads led to improving BLEU scores.", "For detecting important heads from the MHA, to remove less important heads, we can use many approaches like \u201cconfidence\u201d of the heads or Layer-wise relevance propagation(LRP).", "Confidence is a relatively simple approach than LRP, and it contains the average of its maximum attention weights, where the average is taken over the set of sentences used for evaluation, it is an important consideration for translation.", "As you can see in this figure that almost 80% of the model\u2019s attention is given to the single token head.", "LRP is more reliable than \u201cconfidence\u201d. It helps to find the relative contribution of a head in a network of heads.", "Both confidence and LRP is used for detecting important heads, as you can see in the above figure both LRP and confidence show almost similar results. So, now we know which heads are important, let\u2019s see their role.", "In the transformer\u2019s MHA, each individual head plays a role in influencing translation produced by the models, these are the functions that head might be performing:", "From these head functions, it is clear that certain heads detect and learn syntactic relations with more accuracy than the others.", "This also supports the hypothesis that the encoder does support some amount of syntactic disambiguation in sentences.", "In the original transformer, as we saw in our background work, the MHA is given by:", "For removing less important heads, we modify the equation by using a specific gate g\u1d62:", "Where g\u1d62 is in between {0, 1} if g\u1d62 is 1 then while concatenating we are considering all its attention else if g\u1d62 is 0 then we are just pruning it out.", "So, g\u1d62 is a learnable parameter and is independent of the input sequence, as we want to prune heads for that, we ideally apply L0 regularization to scalar gate g\u1d62. The L0 norm equals the number of non-zero components and forces the models to remove less important heads.", "This model gets converge and at the end, heads are completely removed or stay open, this means that we only use the subset of all attention-heads.", "After removing heads let\u2019s see how it affects the BLEU score of the model, as from the below image you can see that BLEU score hardly fall by 0.25 BLEU while compressing the model from 48 heads to 10 heads.", "This paper demonstrated head pruning performing on two datasets that are WMT and OpenSubtitle as shown in the graph above, there BLEU score is noted according to the attention-heads used.", "Surprisingly, in the case of WMT dataset, after removing a few heads we can see the increment in BLEU score, and as we decrease our heads from 5 heads to 1 head, BLEU score gets drastically reduced. These observations suggest that we only need a few heads(5\u201310) to perform competitive results with BASE models.", "The transformers consist of three MHAs, as covered in background work. These MHAs are :(a) Multi-head attention(MHA) of the encoder (b) Masked multi-head attention of decoder (c) Multi-head attention encoder-decoder", "All these MHA contains a concatenation of the multiple heads. Head pruning can happen from any of these MHAs. It is observed that the model prefers to prune encoders self-attention heads first, while encoder-decoder MHA appears to be important among both datasets. Obviously, without encoder-decoder MHA translation task is not possible. As you can see below, in both cases less important heads are from the encoder MHA.", "Another observation is that the head functions of the MHA retain even after pruning. When pruning model below 5\u20136 heads, due to fewer number of heads, only a few heads have to perform several functions then they are also preserved.", "I hope this article has increased your understanding of MHA and pruning heads for better efficiency.", "Thanks for reading, You can connect with me on LinkedIn, Twitter or my portfolio", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "I'm a Software Engineer at Bank of New York Mellon. I've interest in Natural Language Processing and deep learning."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fec222ca9ece7&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhead-pruning-in-transformer-models-ec222ca9ece7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhead-pruning-in-transformer-models-ec222ca9ece7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhead-pruning-in-transformer-models-ec222ca9ece7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhead-pruning-in-transformer-models-ec222ca9ece7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----ec222ca9ece7--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ec222ca9ece7--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@gauravghati?source=post_page-----ec222ca9ece7--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@gauravghati?source=post_page-----ec222ca9ece7--------------------------------", "anchor_text": "Gaurav Ghati"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc2dc95c5f41&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhead-pruning-in-transformer-models-ec222ca9ece7&user=Gaurav+Ghati&userId=c2dc95c5f41&source=post_page-c2dc95c5f41----ec222ca9ece7---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fec222ca9ece7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhead-pruning-in-transformer-models-ec222ca9ece7&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fec222ca9ece7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhead-pruning-in-transformer-models-ec222ca9ece7&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@tetrakiss?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Arseny Togulev"}, {"url": "https://unsplash.com/s/photos/transformer?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf", "anchor_text": "Attention is all you need"}, {"url": "https://arxiv.org/pdf/1905.09418.pdf", "anchor_text": "paper"}, {"url": "https://arxiv.org/pdf/1905.09418.pdf", "anchor_text": "Analyzing Multi-Head Self-Attention"}, {"url": "https://arxiv.org/pdf/1905.09418.pdf", "anchor_text": "Analyzing Multi-Head Self-Attention"}, {"url": "https://arxiv.org/pdf/1905.09418.pdf", "anchor_text": "Analyzing Multi-Head Self-Attention"}, {"url": "https://lena-voita.github.io/posts/acl19_heads.html", "anchor_text": "Story of Heads"}, {"url": "https://arxiv.org/pdf/1905.09418.pdf", "anchor_text": "Analyzing Multi-Head Self-Attention"}, {"url": "https://arxiv.org/pdf/1905.09418.pdf", "anchor_text": "paper"}, {"url": "https://arxiv.org/pdf/1905.09418.pdf", "anchor_text": "Analyzing Multi-Head Self-Attention"}, {"url": "https://arxiv.org/pdf/1905.09418.pdf", "anchor_text": "Analyzing Multi-Head Self-Attention"}, {"url": "https://lena-voita.github.io/posts/acl19_heads.html", "anchor_text": "Post"}, {"url": "https://arxiv.org/pdf/1905.09418.pdf", "anchor_text": "Original Paper)"}, {"url": "https://arxiv.org/pdf/1905.10650.pdf", "anchor_text": "Original Paper)"}, {"url": "https://arxiv.org/pdf/1706.03762.pdf", "anchor_text": "Original Paper"}, {"url": "http://linkedin.com/in/gauravghati/", "anchor_text": "LinkedIn"}, {"url": "https://twitter.com/GauravGhati/", "anchor_text": "Twitter"}, {"url": "http://gauravghati.world/", "anchor_text": "portfolio"}, {"url": "https://medium.com/tag/head-pruning?source=post_page-----ec222ca9ece7---------------head_pruning-----------------", "anchor_text": "Head Pruning"}, {"url": "https://medium.com/tag/transformers?source=post_page-----ec222ca9ece7---------------transformers-----------------", "anchor_text": "Transformers"}, {"url": "https://medium.com/tag/nlp?source=post_page-----ec222ca9ece7---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/pruning?source=post_page-----ec222ca9ece7---------------pruning-----------------", "anchor_text": "Pruning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----ec222ca9ece7---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fec222ca9ece7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhead-pruning-in-transformer-models-ec222ca9ece7&user=Gaurav+Ghati&userId=c2dc95c5f41&source=-----ec222ca9ece7---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fec222ca9ece7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhead-pruning-in-transformer-models-ec222ca9ece7&user=Gaurav+Ghati&userId=c2dc95c5f41&source=-----ec222ca9ece7---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fec222ca9ece7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhead-pruning-in-transformer-models-ec222ca9ece7&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ec222ca9ece7--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fec222ca9ece7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhead-pruning-in-transformer-models-ec222ca9ece7&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----ec222ca9ece7---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----ec222ca9ece7--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----ec222ca9ece7--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----ec222ca9ece7--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----ec222ca9ece7--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----ec222ca9ece7--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----ec222ca9ece7--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----ec222ca9ece7--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----ec222ca9ece7--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@gauravghati?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@gauravghati?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Gaurav Ghati"}, {"url": "https://medium.com/@gauravghati/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "33 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc2dc95c5f41&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhead-pruning-in-transformer-models-ec222ca9ece7&user=Gaurav+Ghati&userId=c2dc95c5f41&source=post_page-c2dc95c5f41--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F3d7182a5316&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhead-pruning-in-transformer-models-ec222ca9ece7&newsletterV3=c2dc95c5f41&newsletterV3Id=3d7182a5316&user=Gaurav+Ghati&userId=c2dc95c5f41&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}