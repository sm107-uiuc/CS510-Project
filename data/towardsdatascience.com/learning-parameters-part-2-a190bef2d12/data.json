{"url": "https://towardsdatascience.com/learning-parameters-part-2-a190bef2d12", "time": 1682996225.898779, "path": "towardsdatascience.com/learning-parameters-part-2-a190bef2d12/", "webpage": {"metadata": {"title": "Learning Parameters, Part 2: Momentum-Based & Nesterov Accelerated Gradient Descent | by Akshay L Chandra | Towards Data Science", "h1": "Learning Parameters, Part 2: Momentum-Based & Nesterov Accelerated Gradient Descent", "description": "In this post, we look at how the gentle-surface limitation of Gradient Descent can be overcome using the concept of momentum to some extent. Make sure you check out my blog post \u2014 Learning\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/learning-parameters-part-1-eb3e8bb9ffbb", "anchor_text": "Learning Parameters, Part-1: Gradient Descent", "paragraph_index": 0}, {"url": "https://www.cse.iitm.ac.in/~miteshk/CS7015.html", "anchor_text": "CS7015: Deep Learning", "paragraph_index": 2}, {"url": "https://www.cse.iitm.ac.in/~miteshk/", "anchor_text": "Prof. Mitesh Khapra", "paragraph_index": 2}, {"url": "https://gist.github.com/akshaychandra21/703ecc8949a01f472d17db3359d56985", "anchor_text": "here", "paragraph_index": 8}, {"url": "http://asasasasas", "anchor_text": "Learning Parameters, Part-0: Basic Stuff", "paragraph_index": 9}, {"url": "https://www.cse.iitm.ac.in/~miteshk/", "anchor_text": "Prof. Mitesh M Khapra", "paragraph_index": 24}, {"url": "https://www.cse.iitm.ac.in/~miteshk/CS7015.html", "anchor_text": "CS7015: Deep Learning", "paragraph_index": 24}, {"url": "http://M.Sc", "anchor_text": "M.Sc", "paragraph_index": 26}], "all_paragraphs": ["In this post, we look at how the gentle-surface limitation of Gradient Descent can be overcome using the concept of momentum to some extent. Make sure you check out my blog post \u2014 Learning Parameters, Part-1: Gradient Descent, if you are unclear of what this is about. Throughout the blog post, we work with the same toy problem introduced in part-1. You can check out all the posts in the Learning Parameters series by clicking on the kicker tag at the top of this post.", "In part-1, we saw a clear illustration of a curve where the gradient can be small in gentle regions of the error surface, and this could slow things down. Let\u2019s look at what momentum has to offer overcome this drawback.", "Citation Note: Most of the content and figures in this blog are directly taken from Lecture 5 of CS7015: Deep Learning course offered by Prof. Mitesh Khapra at IIT-Madras.", "The intuition behind MBGD from the mountaineer\u2019s perspective (yes the same trite metaphor we used in part-1) is", "If I am repeatedly being asked to move in the same direction then I should probably gain some confidence and start taking bigger steps in that direction. Just as a ball gains momentum while rolling down a slope.", "We accommodate the momentum concept in the gradient update rule as follows:", "In addition to the current update, we also look at the history of updates. I encourage you to take your time to process the new update rule and try and put it on paper how the update term changes in every step. Or keep reading. Breaking it down we get", "You can see that the current update is proportional to not just the present gradient but also gradients of previous steps, although their contribution reduces every time step by \u03b3(gamma) times. And that is how we boost the magnitude of the update at gentle regions.", "We modify the vanilla gradient descent code (shown in Part-1 & also available here) a little bit as follows:", "From now on, we will only work with contour maps. Visualizing things in 3-D can be cumbersome, so contour maps come in as a handy alternative for representing functions with 2-D input and 1-D output. If you are unaware of/uncomfortable with them, please go through section 5 of my basic stuff blog post \u2014 Learning Parameters, Part-0: Basic Stuff (there is even a self-test you can take to get better at interpreting them). Let\u2019s see how effective MBGD is using the same toy neural network we introduced in part-1.", "It works. 100 iterations of vanilla gradient descent make the black patch, and it is evident that even in the regions having gentle slopes, momentum-based gradient descent can take substantial steps because the momentum carries it along.", "On a critical note, is moving fast always good? Would there be a situation where momentum would cause us to overshoot and run past our goal? Let test the MBGD by changing our input data so that we end up with a different error surface.", "Say this one shown above, where the error is high on either side of the minima valley. Could momentum still work well in such cases or could it be detrimental instead? Let\u2019s find out.", "We can observe that momentum-based gradient descent oscillates in and out of the minima valley as the momentum carries it out of the valley. This makes us take a lot of U-turns before finally converging. Despite these U-turns, it still converges faster than vanilla gradient descent. After 100 iterations momentum-based method has reached an error of 0.00001 whereas vanilla gradient descent is still stuck at an error of 0.36.", "Can we do something to reduce the oscillations/U-turns? Yes, Nesterov Accelerated Gradient Descent helps us do just that.", "The intuition behind NAG can be put into a single phrase:", "But why would looking ahead help us in avoiding overshoots? I urge you to pause and ponder. If it\u2019s not clear, I am sure it will be clear in the next few minutes. Take a look at this figure for a moment.", "In figure (a), update 1 is positive i.e., the gradient is negative because as w_0 increases L decreases. Even update 2 is positive as well and you can see that the update is slightly larger than update 1, thanks to momentum. By now, you should be convinced that update 3 will be bigger than both update 1 and 2 simply because of momentum and the positive update history. Update 4 is where things get interesting. In vanilla momentum case, due to the positive history, the update overshoots and the descent recovers by doing negative updates.", "But in NAG\u2019s case, every update happens in two steps \u2014 first, a partial update, where we get to the look_ahead point and then the final update (see the NAG update rule), see figure (b). First 3 updates of NAG are pretty similar to the momentum-based method as both the updates (partial and final) are positive in those cases. But the real difference becomes apparent during update 4. As usual, each update happens in two stages, the partial update (4a) is positive, but the final update (4b) would be negative as the calculated gradient at w_lookahead would be negative (convince yourself by observing the graph). This negative final update slightly reduces the overall magnitude of the update, still resulting in an overshoot but a smaller one when compared to the vanilla momentum-based gradient descent. And that my friend, is how NAG helps us in reducing the overshoots, i.e. making us take shorter U-turns.", "By updating the momentum code slightly to do both the partial update and the full update, we get the code for NAG.", "Let\u2019s compare the convergence of momentum-based method with NAG using the same toy example and the same error surface we used while illustrating the momentum-based method.", "NAG is certainly making smaller oscillations/taking shorter U-turns even when approaching the minima valleys on the error surface. Looking ahead helps NAG in correcting its course quicker than momentum-based gradient descent. Hence the oscillations are smaller and the chances of escaping the minima valley also smaller. Earlier, we proved that looking back helps *ahem momentum ahem* and we now proved that looking ahead also helps.", "In this blog post, we looked at two simple, yet hybrid versions of gradient descent that help us converge faster \u2014 Momentum-Based Gradient Descent and Nesterov Accelerated Gradient Descent (NAG) and also discussed why and where NAG beats vanilla momentum-based method. We looked at the nuances in their update rules, python code implementations of the methods and also illustrated their convergence graphically on a toy example. In the next post, we will digress a little bit and talk about stochastic versions of these algorithms.", "Read all about it in the next post of this series at:", "A lot of credit goes to Prof. Mitesh M Khapra and the TAs of CS7015: Deep Learning course by IIT Madras for such rich content and creative visualizations. I merely just compiled the provided lecture notes and lecture videos concisely.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "M.Sc. Student @ University of Freiburg"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fa190bef2d12&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-parameters-part-2-a190bef2d12&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-parameters-part-2-a190bef2d12&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-parameters-part-2-a190bef2d12&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-parameters-part-2-a190bef2d12&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----a190bef2d12--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a190bef2d12--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@acl21?source=post_page-----a190bef2d12--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@acl21?source=post_page-----a190bef2d12--------------------------------", "anchor_text": "Akshay L Chandra"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F202534492f47&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-parameters-part-2-a190bef2d12&user=Akshay+L+Chandra&userId=202534492f47&source=post_page-202534492f47----a190bef2d12---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa190bef2d12&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-parameters-part-2-a190bef2d12&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa190bef2d12&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-parameters-part-2-a190bef2d12&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/tag/learning-parameters/latest", "anchor_text": "Learning Parameters"}, {"url": "https://towardsdatascience.com/learning-parameters-part-1-eb3e8bb9ffbb", "anchor_text": "Learning Parameters, Part-1: Gradient Descent"}, {"url": "https://www.cse.iitm.ac.in/~miteshk/CS7015.html", "anchor_text": "CS7015: Deep Learning"}, {"url": "https://www.cse.iitm.ac.in/~miteshk/", "anchor_text": "Prof. Mitesh Khapra"}, {"url": "https://gist.github.com/akshaychandra21/703ecc8949a01f472d17db3359d56985", "anchor_text": "here"}, {"url": "http://asasasasas", "anchor_text": "Learning Parameters, Part-0: Basic Stuff"}, {"url": "https://towardsdatascience.com/ee8558f65dd7", "anchor_text": "Learning Parameters, Part 3: Stochastic & Mini-Batch Gradient Descent"}, {"url": "https://www.cse.iitm.ac.in/~miteshk/", "anchor_text": "Prof. Mitesh M Khapra"}, {"url": "https://www.cse.iitm.ac.in/~miteshk/CS7015.html", "anchor_text": "CS7015: Deep Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----a190bef2d12---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/optimization?source=post_page-----a190bef2d12---------------optimization-----------------", "anchor_text": "Optimization"}, {"url": "https://medium.com/tag/gradient-descent?source=post_page-----a190bef2d12---------------gradient_descent-----------------", "anchor_text": "Gradient Descent"}, {"url": "https://medium.com/tag/momentum?source=post_page-----a190bef2d12---------------momentum-----------------", "anchor_text": "Momentum"}, {"url": "https://medium.com/tag/learning-parameters?source=post_page-----a190bef2d12---------------learning_parameters-----------------", "anchor_text": "Learning Parameters"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa190bef2d12&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-parameters-part-2-a190bef2d12&user=Akshay+L+Chandra&userId=202534492f47&source=-----a190bef2d12---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa190bef2d12&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-parameters-part-2-a190bef2d12&user=Akshay+L+Chandra&userId=202534492f47&source=-----a190bef2d12---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa190bef2d12&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-parameters-part-2-a190bef2d12&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a190bef2d12--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fa190bef2d12&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-parameters-part-2-a190bef2d12&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----a190bef2d12---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----a190bef2d12--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----a190bef2d12--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----a190bef2d12--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----a190bef2d12--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----a190bef2d12--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----a190bef2d12--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----a190bef2d12--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----a190bef2d12--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@acl21?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@acl21?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Akshay L Chandra"}, {"url": "https://medium.com/@acl21/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.1K Followers"}, {"url": "http://M.Sc", "anchor_text": "M.Sc"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F202534492f47&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-parameters-part-2-a190bef2d12&user=Akshay+L+Chandra&userId=202534492f47&source=post_page-202534492f47--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F4c4c11d45430&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-parameters-part-2-a190bef2d12&newsletterV3=202534492f47&newsletterV3Id=4c4c11d45430&user=Akshay+L+Chandra&userId=202534492f47&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}