{"url": "https://towardsdatascience.com/xlnet-autoregressive-pre-training-for-language-understanding-7ea4e0649710", "time": 1683010509.2061288, "path": "towardsdatascience.com/xlnet-autoregressive-pre-training-for-language-understanding-7ea4e0649710/", "webpage": {"metadata": {"title": "XLNet: Autoregressive Pre-Training for Language Understanding | by Rohan Jagtap | Towards Data Science", "h1": "XLNet: Autoregressive Pre-Training for Language Understanding", "description": "State of the art Language Models like BERT, OpenAI GPT have been stellar in Natural Language Processing in recent times. These models are based on the Transformer architecture, which has driven\u2026"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "Transformer", "paragraph_index": 0}, {"url": "https://arxiv.org/abs/1906.08237", "anchor_text": "XLNet: Generalized Autoregressive Pretraining for Language Understanding", "paragraph_index": 1}, {"url": "https://medium.com/swlh/bert-pre-training-of-transformers-for-language-understanding-5214fba4a9af", "anchor_text": "this (BERT)", "paragraph_index": 2}, {"url": "https://towardsdatascience.com/transformers-explained-65454c0f3fa7", "anchor_text": "this (Transformer)", "paragraph_index": 2}, {"url": "https://arxiv.org/abs/1901.02860", "anchor_text": "Transformer XL model", "paragraph_index": 24}, {"url": "https://github.com/zihangdai/xlnet", "anchor_text": "here", "paragraph_index": 27}, {"url": "https://huggingface.co/transformers/model_doc/xlnet.html", "anchor_text": "huggingface transformers", "paragraph_index": 28}, {"url": "https://docs.google.com/document/d/1nePIW67OqW1HPrIkoXUB-N8hK2A07-3pnmtRVMQMKTA/edit?usp=sharing", "anchor_text": "https://docs.google.com/document/d/1nePIW67OqW1HPrIkoXUB-N8hK2A07-3pnmtRVMQMKTA/edit?usp=sharing", "paragraph_index": 29}], "all_paragraphs": ["State of the art Language Models like BERT, OpenAI GPT have been stellar in Natural Language Processing in recent times. These models are based on the Transformer architecture, which has driven RNN-based and Convolution-based models out of the business.", "In this article, we\u2019ll be discussing the XLNET model, which was proposed in a recent paper: XLNet: Generalized Autoregressive Pretraining for Language Understanding. This model has addressed certain drawbacks of BERT and has successfully overcome them by outperforming BERT in 20 tasks.", "If you\u2019re interested in knowing the concept behind BERT or Transformers, consider giving this (BERT) and this (Transformer) a read.", "One major issue with BERT is essentially its pre-training objective on masked sequences i.e the Denoising Autoencoding objective. Masking the sequences greatly helps in understanding the trends in the language corpus, however, while fine-tuning, the sequences aren\u2019t expected to be masked.", "However, the artificial symbols like [MASK] used by BERT during pre-training are absent from real data at fine-tuning time, resulting in a pretrain-finetune discrepancy.", "BERT maximizes the joint conditional probability p(x_t | x_hat), where x_t is the masked term and x_hat is the sequence of tokens. It is read as, probability of a masked token x_t to occur at the \u2018t\u2019th position, given all the tokens in that sequence x_hat.", "This gives the intuition of an independence assumption that each of the masked tokens are reconstructed separately. We\u2019ll clear this in a later section.", "As opposed to BERT, XLNet is an auto-regressive model. This essentially removes its dependency on denoising the input.", "However, autoregressive models are mostly criticized for their unidirectional nature. Hence, to overcome this, XLNet proposes a novel Permutation Language Modeling objective that overcomes this unidirectionality.", "As mentioned previously, XLNet proposes a mechanism that takes the good stuff from both worlds (i.e. autoencoding and autoregressive). It doesn\u2019t have the denoising of inputs as in the autoencoding objective and removes the unidirectionality from a traditional autoregressive objective.", "To achieve this, while factorizing the joint probability p(x_t | x_(i < t)), instead of using a fixed forward or backward factorization order as in traditional autoregressive models, XLNet maximizes the log-likelihood of a sequence w.r.t all possible permutations of the factorization order.", "Specifically, for a sequence x of length T, there are T! different orders to perform a valid autoregressive factorization. Intuitively, if model parameters are shared across all factorization orders, in expectation, the model will learn to gather information from all positions on both sides.", "To elaborate more on this objective, let\u2019s take an example. Consider the above figure with a sequence x with 4 tokens. For simplicity, we consider the attention computation only for x_3. Observe the permutation order stated under each of the figures above.", "Note that, while training, it is not correct to actually obtain the permutation of the sequence, as the sequences can\u2019t be permuted while fine-tuning on the downstream task or during inference. Hence, the attention mask in the Transformer is properly manipulated to obtain the correct permutations; which also makes sense because the proposed architecture talks about permuting over the factorization order and not the sequence order.", "The regular Transformer parametrization may not work with the permutation language model. To understand this, let\u2019s consider the standard formulation of the distribution using softmax which is given by:", "Here, the term h_\u03b8(x_(z_(< t))), is the hidden state of the transformer for x_(z_(<t)). This term, is in no way dependent on the position that it predicts i.e. z_(<t). This means, that whatever position is being predicted, this distribution will be the same; thus posing the inability to learn useful trends.", "Hence, to overcome this, the XLNet paper proposes a re-parametrization for the next token distribution to be target aware:", "A modified representation g_\u03b8 is used, which additionally takes the target position z_t as the input. So, two hidden states are used instead of one:", "Note that, initially the content stream (h_i) is essentially the corresponding embedding vector (e_x_i), and the query stream (g_i) is a trainable vector (w) initially. These are updated over each layer using the above expressions.", "Keeping aside all the benefits that permutation LM has, we gotta accept that it is expensive. It is a challenging optimization problem due to permutation.", "Hence, to solve this, in a given sequence z, only a subsequence z_(>c) is selected for prediction, where c is called the cutting point. We consider only z_(>c) since it has the longest context in that sequence.", "Moreover, another hyperparameter K is used such that, K ~ |z|/(|z|\u2212c). And we select only 1/K tokens for prediction. For unselected tokens, their query representations aren\u2019t computed, which saves speed and memory.", "We compare this partial prediction to that of the BERT. BERT uses partial prediction because masking all the tokens doesn\u2019t make any sense. XLNet does partial prediction because of the optimization difficulty. For example: let\u2019s have a sequence: [Deep, Learning, is, great]. Say both BERT and XLNet opt to predict the tokens [Deep, Learning]. Also suppose that XLNet factorizes the sample as [is, great, Deep, Learning]. In this case,", "This clearly explains how XLNet captures more dependency i.e. between Deep and Learning. No doubt that BERT learns most of the dependencies; but XLNet learns more. Also, this is an example of the independence assumption in BERT which was covered in the previous section.", "Finally, a mention of the Transformer XL model from where XLNet borrows the ideas of relational encoding and the segment recurrence mechanism which enables Transformer XL to operate on very long sequences.", "Fun Fact: Transformer XL can attend sequences that 80% longer than RNNs and 450% longer than vanilla Transformer and it is 1800+ times faster than vanilla Transformers during evaluation.", "We\u2019ve covered another state of the art model, XLNet, and have discussed the concept behind it.", "XLNet\u2019s code is open-sourced by the authors, you can find it here.", "You can find the pre-trained weights and an easy to use API for the model architecture by huggingface transformers.", "New: I have written a paper summary and critique for XLNet. You can take a look if interested: https://docs.google.com/document/d/1nePIW67OqW1HPrIkoXUB-N8hK2A07-3pnmtRVMQMKTA/edit?usp=sharing", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Immensely interested in AI Research | I read papers and post my notes on Medium"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F7ea4e0649710&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxlnet-autoregressive-pre-training-for-language-understanding-7ea4e0649710&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxlnet-autoregressive-pre-training-for-language-understanding-7ea4e0649710&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxlnet-autoregressive-pre-training-for-language-understanding-7ea4e0649710&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxlnet-autoregressive-pre-training-for-language-understanding-7ea4e0649710&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----7ea4e0649710--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----7ea4e0649710--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://rojagtap.medium.com/?source=post_page-----7ea4e0649710--------------------------------", "anchor_text": ""}, {"url": "https://rojagtap.medium.com/?source=post_page-----7ea4e0649710--------------------------------", "anchor_text": "Rohan Jagtap"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F39646f947a4c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxlnet-autoregressive-pre-training-for-language-understanding-7ea4e0649710&user=Rohan+Jagtap&userId=39646f947a4c&source=post_page-39646f947a4c----7ea4e0649710---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7ea4e0649710&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxlnet-autoregressive-pre-training-for-language-understanding-7ea4e0649710&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7ea4e0649710&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxlnet-autoregressive-pre-training-for-language-understanding-7ea4e0649710&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@timmossholder?utm_source=medium&utm_medium=referral", "anchor_text": "Tim Mossholder"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "Transformer"}, {"url": "https://arxiv.org/abs/1906.08237", "anchor_text": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"}, {"url": "https://medium.com/swlh/bert-pre-training-of-transformers-for-language-understanding-5214fba4a9af", "anchor_text": "this (BERT)"}, {"url": "https://towardsdatascience.com/transformers-explained-65454c0f3fa7", "anchor_text": "this (Transformer)"}, {"url": "https://demo.allennlp.org/masked-lm?text=The%20doctor%20ran%20to%20the%20emergency%20room%20to%20see%20%5BMASK%5D%20patient.", "anchor_text": "AllenNLP"}, {"url": "https://arxiv.org/abs/1906.08237", "anchor_text": "\u2014 XLNet Paper"}, {"url": "https://arxiv.org/abs/1906.08237", "anchor_text": "\u2014 XLNet Paper"}, {"url": "https://arxiv.org/abs/1906.08237", "anchor_text": "XLNet Paper"}, {"url": "https://arxiv.org/abs/1906.08237", "anchor_text": "XLNet Paper"}, {"url": "https://arxiv.org/abs/1906.08237", "anchor_text": "XLNet Paper"}, {"url": "https://arxiv.org/abs/1901.02860", "anchor_text": "Transformer XL model"}, {"url": "https://github.com/zihangdai/xlnet", "anchor_text": "here"}, {"url": "https://huggingface.co/transformers/model_doc/xlnet.html", "anchor_text": "huggingface transformers"}, {"url": "https://docs.google.com/document/d/1nePIW67OqW1HPrIkoXUB-N8hK2A07-3pnmtRVMQMKTA/edit?usp=sharing", "anchor_text": "https://docs.google.com/document/d/1nePIW67OqW1HPrIkoXUB-N8hK2A07-3pnmtRVMQMKTA/edit?usp=sharing"}, {"url": "https://arxiv.org/abs/1906.08237", "anchor_text": "XLNet: Generalized Autoregressive Pretraining for Language UnderstandingWith the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves\u2026arxiv.org"}, {"url": "https://arxiv.org/abs/1901.02860", "anchor_text": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length ContextTransformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the\u2026arxiv.org"}, {"url": "https://medium.com/swlh/bert-pre-training-of-transformers-for-language-understanding-5214fba4a9af", "anchor_text": "BERT: Pre-Training of Transformers for Language UnderstandingUnderstanding Transformer-Based Self-Supervised Architecturesmedium.com"}, {"url": "https://towardsdatascience.com/transformers-explained-65454c0f3fa7", "anchor_text": "Transformers ExplainedAn exhaustive explanation of Google\u2019s Transformer model; from theory to implementationtowardsdatascience.com"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----7ea4e0649710---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----7ea4e0649710---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/nlp?source=post_page-----7ea4e0649710---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----7ea4e0649710---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----7ea4e0649710---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7ea4e0649710&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxlnet-autoregressive-pre-training-for-language-understanding-7ea4e0649710&user=Rohan+Jagtap&userId=39646f947a4c&source=-----7ea4e0649710---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7ea4e0649710&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxlnet-autoregressive-pre-training-for-language-understanding-7ea4e0649710&user=Rohan+Jagtap&userId=39646f947a4c&source=-----7ea4e0649710---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7ea4e0649710&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxlnet-autoregressive-pre-training-for-language-understanding-7ea4e0649710&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----7ea4e0649710--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F7ea4e0649710&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxlnet-autoregressive-pre-training-for-language-understanding-7ea4e0649710&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----7ea4e0649710---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----7ea4e0649710--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----7ea4e0649710--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----7ea4e0649710--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----7ea4e0649710--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----7ea4e0649710--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----7ea4e0649710--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----7ea4e0649710--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----7ea4e0649710--------------------------------", "anchor_text": ""}, {"url": "https://rojagtap.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://rojagtap.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Rohan Jagtap"}, {"url": "https://rojagtap.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "465 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F39646f947a4c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxlnet-autoregressive-pre-training-for-language-understanding-7ea4e0649710&user=Rohan+Jagtap&userId=39646f947a4c&source=post_page-39646f947a4c--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fe51e2b6202c5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxlnet-autoregressive-pre-training-for-language-understanding-7ea4e0649710&newsletterV3=39646f947a4c&newsletterV3Id=e51e2b6202c5&user=Rohan+Jagtap&userId=39646f947a4c&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}