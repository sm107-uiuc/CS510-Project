{"url": "https://towardsdatascience.com/get-started-with-google-trax-for-nlp-ff8dcd3119cf", "time": 1683017764.054513, "path": "towardsdatascience.com/get-started-with-google-trax-for-nlp-ff8dcd3119cf/", "webpage": {"metadata": {"title": "Get started with Google Trax for NLP | by Tiago Duque | Towards Data Science", "h1": "Get started with Google Trax for NLP", "description": "Machine Learning Engineers and Data Scientists already have a handful of tools to build and deploy the most up-to-date models for Natural Language Processing. Among these tools, we could mention\u2026"}, "outgoing_paragraph_urls": [{"url": "https://huggingface.co/transformers/", "anchor_text": "\ud83e\udd17 Transformers", "paragraph_index": 3}, {"url": "https://trax-ml.readthedocs.io/en/latest/", "anchor_text": "Trax", "paragraph_index": 5}, {"url": "https://research.google.com/teams/brain/", "anchor_text": "Google Brain team", "paragraph_index": 5}, {"url": "https://www.coursera.org/specializations/natural-language-processing", "anchor_text": "Deeplearning.AI NLP Specialization at Coursera", "paragraph_index": 12}, {"url": "https://trax-ml.readthedocs.io/en/latest/trax.models.html", "anchor_text": "pre-made models", "paragraph_index": 31}, {"url": "https://trax-ml.readthedocs.io/en/latest/trax.layers.html", "anchor_text": "predefined layers", "paragraph_index": 39}], "all_paragraphs": ["After reading this article you\u2019ll be able to:", "Machine Learning Engineers and Data Scientists already have a handful of tools to build and deploy the most up-to-date models for Natural Language Processing.", "Among these tools, we could mention Pytorch and Tensorflow 2.0 as strong bases, building up from the most basic elements for Neural Networks and by providing useful high-level tools, such as the Keras library, that is now part of Tensorflow 2.0.", "Aside from that, there\u2019s \ud83e\udd17 Transformers, which makes the life of NLP practitioners a tad easier by providing high-level access to many pretrained models at either Pytorch or Tensorflow format.", "With all these, it seems there\u2019s no need for anything else, right? But Google Brain team, the one behind BERT and the Reformer, decided that the answer is no. Enters Trax.", "Trax is an end-to-end library for deep learning that focuses on clear code and speed. It is actively used and maintained in the Google Brain team.", "Looking more carefully, you\u2019ll note that Trax actually is the \u201cinterface\u201d that runs over more complex resources, such as Jax (from which Trax takes its name) or Tensorflow-numpy as backends.", "If you\u2019re familiar with Keras, you\u2019ll get to see that Trax looks a lot like his older cousin (that\u2019s clear in the fact that Trax models can be easily converted to Keras models). So, again, what\u2019s the benefit in Trax? The answer is at the words mentioned above: Trax has been written from scratch for better readability and speed.", "Chances are that, if you\u2019re the average python developer, you\u2019ll be able to understand Trax\u2019s source. Want some example? Look at the code for the init_weights_and_state method for the Serial layer:", "Besides being clear and attempting to use python default structure\u2019s whenever possible, it is all well documented, some docstrings even having examples.", "Sure, you\u2019ll lack the amount of support and courses that Tensorflow, Keras or Pytorch have, but you\u2019ll be able to figure most of the code out by your own (as I did while writing this article).", "More than an in-detail approach, this article will attempt to give you a shortcut to some of the most useful resources in Trax for NLP, as well as providing some examples.", "As a disclaimer, I have nothing to do with Google Brain Team and my first contact with Trax was at Deeplearning.AI NLP Specialization at Coursera (which I highly recommend).", "Before proceeding, all the code and samples will also be available in a public Google Colab, which can be accessed below:", "Without further ado, let us get started with some interesting features that Trax brings to NLP.", "Important Note: when I wrote this article, Trax\u2019s latest release was version 1.3.6. From recent commits, some significant changes are being added, especially to the metric layers and other useful resources. However, since these are experimental, I decided to cover only what is available in the current release (out in October 2020).", "First, let us understand the imports for the main modules in Trax. For preprocessing, basic model designing and training, we\u2019ll need the following imports:", "Also, as mentioned, Trax can use distinct backends (to do tensor computation). For now, there are two options: Jax and Tensorflow. Jax is being called \u201cnumpy on steroids\u201d since it uses more modern techniques to speed up Numpy and Python computations. It is the default for Trax, but you can set it as such (bonus example on how to set TPU for Jax in colab):", "Trax provides us easy to use resources to implement NLP Pipelines (actually, any preprocessing pipeline). This is where the data module comes in.", "For example, Trax allows us to use many Tensorflow utilities, such as Tensorflow Datasets. We can download any dataset available in TFDS very easily (and use the tool utilities to prepare our own datasets).", "We can, for example, get the \u2018imdb_reviews\u2019 dataset to be used for sentiment analysis.", "To access its contents, we can:", "Trax allows for easy to use and configure preprocessing pipelines.", "To do it, we implement a special type of object: the Serial Layer (for data, there\u2019s another one for modeling). We can import it from trax.data.Serial, and this one allows us to process data one function at a time, in a serial manner.", "We can also make a preprocessing pipeline and then feed the data generator to it. This way, we can do many important tasks, such as:", "Just remember that each of your functions takes as input the same format as the output of the previous function (eg.: (1) takes a string and returns a list; (2) takes a list and returns a list, etc.)", "For you to \u2018plug\u2019 your pipeline into Trax algorithms, the expected format is batch_input, batch_expected_output, mask weights (if any).", "In the following example, we show a real pipeline that makes use of some useful preprocessing steps provided by Trax itself.", "If you want an example on how to include your own preprocessing steps into the pipeline, check the colab notebook.", "Now that we\u2019ve seen preprocessing, it\u2019s time to move into Modeling itself.", "Trax allows the use of models in two ways:", "Lets peek into one of Trax pre-made models, the LSTMSeq2SeqAttn:", "Here are a few lines of the result:", "That\u2019s a lot of things, right? Each of these \u2018names\u2019 appearing there are the network layers, accompanied by their description (Embedding_10000_512 is an Embeddings layer with a 10000 vocab size and 512 embedding dimension).", "Layers are the LEGO blocks of Trax!", "With these layers we can create our own models as well.", "Each layer is a function (or a whole bunch of functions bundled together) that gets some input and returns output in the promised format. Layers always have two important parameters: Expected Input (n_in) and promised Output (n_out).", "Layers can be combined using what are called \u201ccombiner layers\u201d.", "By default, there are 3 \u201ccombiner\u201d layers: \u201cSerial\u201d (sequential model), \u201cBranch\u201d (parallel model) and \u201cResidual\u201d (kind of a merge between Serial and Branch).", "We can use trax predefined layers (such as LSTM Cells, Fully connected [Dense] layers, etc), but we can even make our own layers. In the notebook, I\u2019ve shown how to \u201cimplement\u201d a modified (yet useless) version of the Dense Layer, as a simple example. This means that you can easily implement novel algorithms on your own and integrate them to other preexisting layers or to use Trax facilities for training/preprocessing pipeline.", "Let us now see how to build an entire model with trax layers (this is a simple neural network to perform Sentiment Analysis that we\u2019ll use later for training). We\u2019ll use a Serial Layer as a combiner and the following layers:", "Much simpler right? But it works for the basic. Now, how train it?", "So we already seen the way pipelines are created and how to create/use models.", "Now, it is time for us to do the trick and put both of them to work together.", "Let us put the machine to learn!", "Remember, there are two main methods of Machine Learning:", "We\u2019re only covering supervised models here.", "To do Supervised Training in Trax, we have to define three important \u2018blocks\u2019:", "We get all that from trax.supervised (which we imported as ts in the beginning).", "Before, though, let us use those pipelines we\u2019ve learned before to build the pipelines for training and evaluation data.", "Now we can define the training/eval tasks and the loop.", "Just some important notes before proceeding.", "About the training and eval tasks:", "We first get the size of the model in the stdout. 2 million weights is bigger than we thought, right? Then, we get the outputs at every checkpoint mark we\u2019ve set in the Train Task.", "After some 10 minutes, It gets to about ~85% accuracy. This is not to bad for for a simple model, right?", "Okay, now how to use it?", "So we got a trained model, how do we use it?", "Simple! Just feed a tokenized input to the model!", "But, some words of caution before: Trax models (as all current deep learning frameworks) expects the input to come with a batch dimension besides the expected input dimensions. So we have to wrap our sample around that.", "It got it right! And will do often, but this is a simple model and way below the current state of the art. For this, you will need to use other models and, preferably, finetune pretrained models!", "First, let us see how to restore a checkpoint. This enables us to retake training from a certain point, which is useful if you want to train for a really long time, if for some reason the running session crashes or if you want to test new parameters to the training and not lose all the work.", "Remember that we told Trax to save at each checkpoint (each 200 steps)? Now we can restore the model to continue training:", "(Be careful, this can cause the model to overfit)", "We can also load a pretrained model. This allows us to use models that have been trained before and are to be used in production:", "Great! So this is how we can save our training to use in production, right?", "Well, the thing is: Trax is currently set to be a good tool for developing and training fast and SOTA models. It\u2019s a young project (its first commit was in November 2019) and lacks some perks of the older cousins, such as a native deploy-to-production set of tools.", "But while there\u2019s no current native support for deploying to Tensorflow Serving, for example, there are native tools to turn Trax models into Keras models, which in turn can be deployed into Tensorflow Serving! So, all in all, you get the cleanness of code and performance of Trax allied to the maturity of Keras and Tensorflow.", "That\u2019s another easy step to implement. But you have to keep in mind that (currently) Trax can only be converted to Keras if it\u2019s backend is set to tensorflow-numpy (no straight Jax conversion). But that is very easy to do.", "Start of by changing the backend:", "Next, train the model for a single step over the new backend (this will implicitly convert the backend structures to the new format \u2014 expect this step to be unnecessary soon):", "To make the conversion, it is very simple:", "Now, you can do what you\u2019re used to with Keras and add this layer to a full model:", "With this, we can save our model to use with Tensorflow Serving as easy as:", "That\u2019s it for this article, however it is not all!", "In the Colab Notebook that I provided you, there are two other examples that might be useful: how to Train a Neural Machine Translator with Transformer and how to Train a Named Entity Recognition System with Reformer.", "Be sure to check these examples since they cover other options that are not in the text. Also, I did a lot of work to make it easier to understand than Trax original examples. Maybe you could keep this like a companion cheatsheet to learn Trax \ud83d\ude09.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "A Data Scientist passionate about data and text. Trying to understand and clearly explain all important nuances of Natural Language Processing."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fff8dcd3119cf&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fget-started-with-google-trax-for-nlp-ff8dcd3119cf&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fget-started-with-google-trax-for-nlp-ff8dcd3119cf&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fget-started-with-google-trax-for-nlp-ff8dcd3119cf&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fget-started-with-google-trax-for-nlp-ff8dcd3119cf&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----ff8dcd3119cf--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ff8dcd3119cf--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://tfduque.medium.com/?source=post_page-----ff8dcd3119cf--------------------------------", "anchor_text": ""}, {"url": "https://tfduque.medium.com/?source=post_page-----ff8dcd3119cf--------------------------------", "anchor_text": "Tiago Duque"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff6698b3e89e3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fget-started-with-google-trax-for-nlp-ff8dcd3119cf&user=Tiago+Duque&userId=f6698b3e89e3&source=post_page-f6698b3e89e3----ff8dcd3119cf---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fff8dcd3119cf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fget-started-with-google-trax-for-nlp-ff8dcd3119cf&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fff8dcd3119cf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fget-started-with-google-trax-for-nlp-ff8dcd3119cf&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/photos/YLSwjSy7stw", "anchor_text": "Image by Alfons Morales at Unsplash"}, {"url": "https://huggingface.co/transformers/", "anchor_text": "\ud83e\udd17 Transformers"}, {"url": "https://trax-ml.readthedocs.io/en/latest/", "anchor_text": "Trax"}, {"url": "https://research.google.com/teams/brain/", "anchor_text": "Google Brain team"}, {"url": "https://www.coursera.org/specializations/natural-language-processing", "anchor_text": "Deeplearning.AI NLP Specialization at Coursera"}, {"url": "https://github.com/Sirsirious/trax_start/blob/colabs/TraxStart.ipynb", "anchor_text": "Sirsirious/trax_startPermalink GitHub is home to over 50 million developers working together to host and review code, manage projects, and\u2026github.com"}, {"url": "https://trax-ml.readthedocs.io/en/latest/trax.models.html", "anchor_text": "pre-made models"}, {"url": "https://unsplash.com/photos/kn-UmDZQDjM", "anchor_text": "Image by Xavi Cabrera at Unsplash"}, {"url": "https://trax-ml.readthedocs.io/en/latest/trax.layers.html", "anchor_text": "predefined layers"}, {"url": "https://unsplash.com/photos/GQCYOS_MH0w", "anchor_text": "Image by Brett Jordan at Unsplash"}, {"url": "https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#:~:text=Cross%2Dentropy%20loss%2C%20or%20log,diverges%20from%20the%20actual%20label.", "anchor_text": "here"}, {"url": "https://ml-cheatsheet.readthedocs.io/en/latest/optimizers.html#adam", "anchor_text": "here"}, {"url": "https://trax-ml.readthedocs.io/en/latest/trax.layers.html#module-trax.layers.metrics", "anchor_text": "https://trax-ml.readthedocs.io/en/latest/trax.layers.html#module-trax.layers.metrics"}, {"url": "https://medium.com/tag/nlp?source=post_page-----ff8dcd3119cf---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/ai?source=post_page-----ff8dcd3119cf---------------ai-----------------", "anchor_text": "AI"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----ff8dcd3119cf---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/python?source=post_page-----ff8dcd3119cf---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/trax?source=post_page-----ff8dcd3119cf---------------trax-----------------", "anchor_text": "Trax"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fff8dcd3119cf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fget-started-with-google-trax-for-nlp-ff8dcd3119cf&user=Tiago+Duque&userId=f6698b3e89e3&source=-----ff8dcd3119cf---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fff8dcd3119cf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fget-started-with-google-trax-for-nlp-ff8dcd3119cf&user=Tiago+Duque&userId=f6698b3e89e3&source=-----ff8dcd3119cf---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fff8dcd3119cf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fget-started-with-google-trax-for-nlp-ff8dcd3119cf&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ff8dcd3119cf--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fff8dcd3119cf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fget-started-with-google-trax-for-nlp-ff8dcd3119cf&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----ff8dcd3119cf---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----ff8dcd3119cf--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----ff8dcd3119cf--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----ff8dcd3119cf--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----ff8dcd3119cf--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----ff8dcd3119cf--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----ff8dcd3119cf--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----ff8dcd3119cf--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----ff8dcd3119cf--------------------------------", "anchor_text": ""}, {"url": "https://tfduque.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://tfduque.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Tiago Duque"}, {"url": "https://tfduque.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "225 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff6698b3e89e3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fget-started-with-google-trax-for-nlp-ff8dcd3119cf&user=Tiago+Duque&userId=f6698b3e89e3&source=post_page-f6698b3e89e3--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fbb155ace8381&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fget-started-with-google-trax-for-nlp-ff8dcd3119cf&newsletterV3=f6698b3e89e3&newsletterV3Id=bb155ace8381&user=Tiago+Duque&userId=f6698b3e89e3&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}