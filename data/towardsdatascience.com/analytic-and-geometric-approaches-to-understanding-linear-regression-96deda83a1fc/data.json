{"url": "https://towardsdatascience.com/analytic-and-geometric-approaches-to-understanding-linear-regression-96deda83a1fc", "time": 1683016339.9118552, "path": "towardsdatascience.com/analytic-and-geometric-approaches-to-understanding-linear-regression-96deda83a1fc/", "webpage": {"metadata": {"title": "Analytic and geometric approaches to understanding linear regression | by Niv | Towards Data Science", "h1": "Analytic and geometric approaches to understanding linear regression", "description": "In an experiment, we observe the values of some features or attributes. We would like to predict the value of a target variable based on the observed features. Linear regression tries to guess a\u2026"}, "outgoing_paragraph_urls": [], "all_paragraphs": ["In an experiment, we observe the values of some features or attributes. We would like to predict the value of a target variable based on the observed features. Linear regression tries to guess a linear relationship between the features and the target.", "Let us understand what problem linear regression attempts to solve with the help of an example analyzing the weather. Say we have access to the average daily temperature T, the wind-gut W, and the total minutes of sunshine S per day. These will be our features. We would like to predict the amount of precipitation P each day. This will be our target variable.", "We are trying to find a linear relationship between P, T, W, S. That is, we\u2019d like to find \u03b8\u2080, \u03b8\u2081, \u03b8\u2082, \u03b8\u2083 so that", "Let\u2019s say we have weather data over n days. For day i, we collect that day\u2019s feature values T, W, S into a row vector x\u1d62 = (x\u1d62\u00b9, x\u1d62\u00b2, x\u1d62\u00b3). That day\u2019s precipitation value becomes the target value y\u1d62.", "Abstracting out this setup, we can assume we are given data points x\u2081, x\u2082 , \u2026 x_n where each data point x\u1d62 has k features and has target y\u1d62. That is, each data point x\u1d62 looks like the row vector", "and the target variable in which we are interested in has value y\u1d62 for the data point x\u1d62.", "Linear regression tries to find a linear relationship between the data points x\u1d62 and their targets y\u1d62. That is, we want to find real numbers \u03b8\u2080, \u03b8\u2081, \u03b8\u2082, \u03b8\u2083 \u2026 \u03b8_k so that for each i, we have", "Introducing a dummy feature for uniformity : The \u03b8\u2080 stands out without a binding feature x\u1d62\u2070, so to make it uniform, we introduce a dummy feature x\u1d62\u2070= 1 for each datapoint x\u1d62. That is, each data point x\u1d62 now looks like the row vector", "Then solving the n linear regression equations above simultaneously is the same as solving the matrix equation X \u03b8 = Y", "Unfortunately, you might not always be able to solve this system of linear equations on the nose. Then we aim for an approximate solution which is the best among other approximate solutions in the following sense:", "Let X \u03b8 = \u0176_\u03b8 . That is, our predicted values for the target variable is[\u00b9] \u0176_\u03b8. We would like our predictions to be as close to the real truth as possible. So we want \u0176_\u03b8 to be as close to Y as possible. That is, we want the norm || Y-\u0176_\u03b8|| to be minimal.", "Since this is the same as demanding minimality of ||Y-\u0176_\u03b8||\u00b2, we want to minimize the following sum of squares", "That is, we want a least-squares solution of the matrix equation X \u03b8 = Y", "If you are uncomfortable with multivariable derivatives, skip ahead to the geometric approach!", "Since Y\u1d40Y doesn\u2019t depend on \u03b8, we may as well minimize the following function", "Taking gradient with respect to \u03b8, we get", "Remember we want the gradient to be 0 since we are trying to find extrema of f(\u03b8). So we want 2X\u1d40Y \u2014 2X\u1d40X\u03b8 = 0.", "The matrix equation X\u1d40X\u03b8 = X\u1d40Y is called the normal equation of X\u03b8 = Y. And from our discussion above, it should be more or less clear that a solution \u03b8 of the normal equation gives a least-squares solution of X\u03b8 = Y.", "This is good, but how do we know a solution of the normal equation actually exists ? Also how do we check that the extrema we get are actually minima ? These can of course be shown with some further computation. Instead, we present a geometric approach which avoids derivatives altogether and uses some geometric insight to derive the normal equation and show the existence of a least-squares solution!", "Let\u2019s go back to the equation X\u03b8 = Y. Since we can\u2019t always solve this equation on the nose, we are content with an approximate solution \u03b8 so that X\u03b8 \u2248Y. If we define \u0176_\u03b8 = X\u03b8, then out of all possible approximate solutions \u03b8, we want to pick the one so that ||Y-\u0176_\u03b8|| is minimum.", "Let\u2019s consider the space of ALL possible values for \u03b8. This is just the R^{k+1}, i.e \u03b8 can be any k+1 column vector. Let\u2019s now look at the set V which consists of all possible predictions \u0176_\u03b8. That is,", "Let\u2019s view X as a linear map", "that is X is a linear map from the space of possible \u03b8s to the space of all possible values of the target variable.", "Then V is just the image of X. In particular, it is a subspace of R\u207f. To reiterate, R\u207f is the space of all possible values of the target variable, Y is the particular value of the target variable that we are trying to hit and V is the subspace of all our possible predictions.", "Now if Y lies in V, we can hit it on the nose, that is we can find a solution \u03b8 that actually solves X\u03b8 = Y.. If we can\u2019t, we are trying to find the vector \u0176_\u03b8 in V which is closest[\u00b3] to Y.", "V is a subspace of R\u207f and Y is a given vector in R\u207f. We want to find \u0176_\u03b8 in V which is closest to Y. How do we do that ?Let\u2019s visualize these vectors and also the vector Y-\u0176_\u03b8. Remember we are trying to make the norm||Y-\u0176_\u03b8|| as small as possible", "Since the shortest distance between a point and a subspace is the perpendicular from the point to the subspace, we aim to make Y-\u0176_\u03b8 to be perpendicular to V ! Note that this is the same thing as saying that the closest prediction \u0176_\u03b8 is simply the orthogonal projection of Y to V.", "Now there is a really nice relation[\u2074] between images of a linear map X and the kernel of its transpose X\u1d40. Vectors perpendicular to Image(X) are actually in the kernel of X\u1d40 and vice-versa. In symbols, this is written as", "Now we want Y-\u0176_\u03b8 to be perpendicular to V= Image(X). So we want Y-\u0176_\u03b8 to be in the kernel of X\u1d40. That is, we want", "Voila! We want a solution \u03b8 to the normal equation X\u1d40X\u03b8 = X\u1d40Y", "The geometric approach immediately tells you why a solution \u03b8 exists for the normal equation X\u1d40X\u03b8 = X\u1d40Y. This is because (as we have already figured out) \u03b8 is a solution precisely when X\u03b8= \u0176_\u03b8 is the orthogonal projection of Y onto V=Image(X).", "So backtracking, take Y, project it to V=Image(X). The projection will have to look like X\u03b8 for some \u03b8 because the projection is in the image subspace V. That \u03b8 is a solution of the normal equation and hence a least-squares solution to the equation X\u03b8=Y we are after!", "There is one point that we have glossed over, namely, is the X\u03b8 we are after unique ? Using our geometric approach, the projection of Y onto V (which is \u0176_\u03b8) is unique. But there could be several \u03b8 such that X\u03b8 = \u0176_\u03b8.", "But further suppose, X is an injective map, i.e. it has zero kernel, then the \u03b8 we get is unique. And actually, it turns out[\u2075] that if X is injective, so is X\u1d40X. Since X\u1d40X is a square matrix, this means X\u1d40X is invertible. So we can actually solve for \u03b8 as follows:", "* The least-squares solutions \u03b8 of the equation X\u03b8 = Y are the same as the solutions of the normal equation X\u1d40X\u03b8 = X\u1d40Y.", "* The normal equation always has at least one solution. And further if \u03b8 is a solution, then X\u03b8, the closest prediction, is just the orthogonal projection of Y to Image(X).", "* There *could* be multiple solutions to the normal equation and hence there could be multiple least-squares solutions. However, if X is injective (i.e. has zero kernel), then the solution is unique and given by", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F96deda83a1fc&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanalytic-and-geometric-approaches-to-understanding-linear-regression-96deda83a1fc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanalytic-and-geometric-approaches-to-understanding-linear-regression-96deda83a1fc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanalytic-and-geometric-approaches-to-understanding-linear-regression-96deda83a1fc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanalytic-and-geometric-approaches-to-understanding-linear-regression-96deda83a1fc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----96deda83a1fc--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----96deda83a1fc--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@niv.bhaskhar?source=post_page-----96deda83a1fc--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@niv.bhaskhar?source=post_page-----96deda83a1fc--------------------------------", "anchor_text": "Niv"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F492ceb709366&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanalytic-and-geometric-approaches-to-understanding-linear-regression-96deda83a1fc&user=Niv&userId=492ceb709366&source=post_page-492ceb709366----96deda83a1fc---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F96deda83a1fc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanalytic-and-geometric-approaches-to-understanding-linear-regression-96deda83a1fc&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F96deda83a1fc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanalytic-and-geometric-approaches-to-understanding-linear-regression-96deda83a1fc&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/tag/linear-regression?source=post_page-----96deda83a1fc---------------linear_regression-----------------", "anchor_text": "Linear Regression"}, {"url": "https://medium.com/tag/normal-equation?source=post_page-----96deda83a1fc---------------normal_equation-----------------", "anchor_text": "Normal Equation"}, {"url": "https://medium.com/tag/linear-algebra?source=post_page-----96deda83a1fc---------------linear_algebra-----------------", "anchor_text": "Linear Algebra"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F96deda83a1fc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanalytic-and-geometric-approaches-to-understanding-linear-regression-96deda83a1fc&user=Niv&userId=492ceb709366&source=-----96deda83a1fc---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F96deda83a1fc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanalytic-and-geometric-approaches-to-understanding-linear-regression-96deda83a1fc&user=Niv&userId=492ceb709366&source=-----96deda83a1fc---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F96deda83a1fc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanalytic-and-geometric-approaches-to-understanding-linear-regression-96deda83a1fc&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----96deda83a1fc--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F96deda83a1fc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanalytic-and-geometric-approaches-to-understanding-linear-regression-96deda83a1fc&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----96deda83a1fc---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----96deda83a1fc--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----96deda83a1fc--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----96deda83a1fc--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----96deda83a1fc--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----96deda83a1fc--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----96deda83a1fc--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----96deda83a1fc--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----96deda83a1fc--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@niv.bhaskhar?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@niv.bhaskhar?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Niv"}, {"url": "https://medium.com/@niv.bhaskhar/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "9 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F492ceb709366&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanalytic-and-geometric-approaches-to-understanding-linear-regression-96deda83a1fc&user=Niv&userId=492ceb709366&source=post_page-492ceb709366--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F492ceb709366%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanalytic-and-geometric-approaches-to-understanding-linear-regression-96deda83a1fc&user=Niv&userId=492ceb709366&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}