{"url": "https://towardsdatascience.com/a-guide-to-four-deep-learning-layers-225c93646e61", "time": 1683016565.948338, "path": "towardsdatascience.com/a-guide-to-four-deep-learning-layers-225c93646e61/", "webpage": {"metadata": {"title": "The Four Most Important Deep Learning Layers | Towards Data Science", "h1": "A Guide to Deep Learning Layers", "description": "This post is about four important neural network layer architectures \u2014 the building blocks that machine learning engineers use to construct deep learning models."}, "outgoing_paragraph_urls": [{"url": "https://colah.github.io/posts/2015-08-Understanding-LSTMs/", "anchor_text": "Understanding LSTM Networks", "paragraph_index": 76}, {"url": "https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html", "anchor_text": "Attention? Attention!", "paragraph_index": 111}, {"url": "https://www.tensorflow.org/tutorials/text/transformer", "anchor_text": "Transformer Model for Language Understanding", "paragraph_index": 118}, {"url": "https://medium.com/@adgefficiency", "anchor_text": "Medium", "paragraph_index": 125}, {"url": "https://www.linkedin.com/in/adgefficiency/", "anchor_text": "LinkedIn", "paragraph_index": 125}], "all_paragraphs": ["This post is about four important neural network layer architectures \u2014 the building blocks that machine learning engineers use to construct deep learning models:", "For each layer we will look at:", "All code examples are built using tensorflow==2.2.0 using the Keras Functional API.", "One term I use a lot in this article is inductive bias \u2014 a useful term to sound clever and impress your friends at dinner parties.", "Inductive bias is the hard-coding of assumptions into the structure of a learning algorithm. These assumptions make the method more special purpose and less flexible, but often much more useful. By hard coding in assumptions about the structure of the data & task, we can learn functions in practice that we couldn\u2019t other wise.", "Examples of inductive bias in machine learning include margin maximization (classes should be separated by as large a boundary as possible \u2014 used in Support Vector Machines) and nearest neighbours (samples close together in feature space are in the same class \u2014 used in the k-nearest neighbours algorithm).", "It\u2019s a common lesson in machine learning \u2014 a bit of bias is ok (if you trade it for variance). This also holds in reinforcement learning, where unbiased approxmiation through Monte Carlo returns performs worse than bootstrapped temporal difference methods.", "Also known as a dense or feed-forward layer, the fully connected layer is the most general purpose deep learning layer.", "This layer imposes the least amount of structure of our layers. It will be found in almost all neural networks \u2014 often being used to control the size & shape of the output layer.", "At the heart of the fully connected layer is the artificial neuron \u2014 the distant ancestor of McCulloch & Pitt\u2019s Threshold Logic Unit of 1943.", "The artificial neuron is inspired by the biological neurons in our brains \u2014 however an artificial neuron is a shallow approximation of the complexity of a biological neuron.", "The artificial neuron composed of three sequential steps:", "The strength of the connection between nodes in different layers are controlled by weights \u2014 the shape of these weights depending on the number of nodes layers on either side. Each node has an additional parameter known as a bias, which can be used to shift the output of the node independently of it\u2019s input.", "The weights and biases are learnt \u2014 commonly in modern machine learning backpropagation is used to find good values of these weights \u2014 good values being those that lead to good predictive accuracy of the network on unseen data.", "After applying the weight and bias, all of the inputs into the neuron are summed together to a single number.", "This is then passed through an activation function. The most important activation functions are:", "The output of the activation function is input to all neurons (also known as nodes or units) in the next layer.", "This is where the fully connected layer gets it\u2019s name from \u2014 each node is fully connected to the nodes in the layers before & after it.", "For the first layer, the node gets it\u2019s input from the data being fed into the network (each data point is connected to each node). For last layer, the output is the prediction of the network.", "The intuition behind all the connections in a fully connected layer is to put no restriction on how information can flow through the network. It\u2019s the intuition of having no intuition.", "The fully connected layer imposes no structure and makes no assumption about the data or task the network will perform. A neural network built of fully connected layers can be thought of as a blank canvas. The intuition is to impose no structure and let the network figure everything out.", "This lack of structure is what gives neural networks of fully connected layers (of sufficient depth & width) the ability to approximate any function \u2014 known as the Universal Approximation Theorem.", "The ability to approximate any function at first sounds attractive. Why do we need any other architecture if a fully connected layer can learn anything?", "Being able to learn in theory does not mean we can learn in practice. Actually finding the correct weights, using the data and learning algorithms (such as backpropagation) we have available may be impractical and unreachable.", "The solution to these practical challenges is to use less specialized layers \u2014 layers that have assumptions about the data & task they are expected to perform. This specialization is inductive bias.", "A fully connected layer is the most general deep learning architecture \u2014 it imposes no constraints on connectivity except by depth. Use it when your data has no structure that you can take advantage of \u2014 if your data is a flat array (common in tabular data problems).", "Fully connected layers are common in reinforcement learning when learning from a flat environment observation. For example, a network with a single fully connected layer is used in the Trust Region Policy Optimization (TRPO) paper from 2015:", "Most neural networks will have fully connected layers somewhere. It\u2019s common to have them as the penultimate & final layer as fully connected on convolutional neural networks performing classification.", "The number of units in the fully connected output layer will be equal to the number of classes, with a softmax activation function used to create a distribution over classes.", "The two hyperparameters you\u2019ll often set in a fully connected layer are the:", "A fully connected layer is defined by a number of nodes (also known as units), each with an activation function. While you could have a layer with different activation functions on different nodes, most of the time each node in a layer has the same activation function.", "For hidden layers, the most common choice of activation function is the rectified-linear unit (the ReLu). For the output layer, the correct activation function depends on what the network is predicting:", "Below is an example of how to use a fully connected layer with the Keras functional API.", "We are using input data shaped like an image, to show the flexibility of the fully connected layer. This requires us to use a Flatten layer later in the network.", "If you had to pick one architecture as the most important in deep learning, it\u2019s hard to look past convolution (sea what I did there?).", "The winner of the 2012 ImageNet competition, AlexNet, is seen by many as the start of modern deep learning. Alexnet was a deep convolutional neural network, trained on GPU.", "Another landmark use of convolution was Le-Net-5 in 1998, a 7 layer convolutional neural network developed by Yann LeCun to classify handwritten digits.", "The convolutional neural network is the workhorse of modern deep learning \u2014 it can be used with text, audio, video and images. Convolutional neural networks can be used to classify the contents of the image, recognize faces and create captions for images. They are also easy to parallelize on GPU \u2014 making them fast to train.", "The 2D convolutional layer is inspired by our own visual cortex.", "The history of using convolution in artificial neural networks goes back decades to the neocognitron, an architecture introduced by Kunihiko Fukushima in 1980, inspired by the work of Hubel & Wiesel, who in the 1950\u2019s showed that individual neurons in the visual cortexes of mammals are activated by small regions of vision.", "Convolution itself is a mathematical operation, commonly used in signal processing. A good mental model for convolution is the process of sliding a filter over a signal, at each point checking to see how well the filter matches the signal.", "This checking process is pattern recognition, and is the intuition behind convolution \u2014 looking for small, spatial patterns anywhere in a larger space. Its exactly how our visual cortex works.", "This is the inductive bias of the convolution layer has \u2014 for recognizing local, spatial patterns.", "A 2D convolutional layer is defined by the interaction between two components:", "Above we defined the intuition of convolution being looking for patterns in a larger space. In a 2D convolutional layer, the patterns we are looking for are filters, and the larger space is an image.", "A convolutional layer is defined by it\u2019s filters. These filters are learnt \u2014 they are equivalent to the weights of a fully connected layer. Filters in the first layers of a convolutional neural network detect simple features such as lines or edges. Deeper in the network, filters can detect more complex features that help the network perform it\u2019s task.", "To further understand how these filters work, let\u2019s work with a small image and two filters. The basic operation in a convolutional neural network is to use these filters to detect patterns in the image, by performing element-wise multiplication and summing the result:", "Reusing the same filters over the entire image allows features to be detected in any part of the image \u2014 a property known as translation invariance. This property is ideal for classification \u2014 you want to detect a cat no matter where it occurs in the image.", "For larger images (which are often 32x32 or larger), this same basic operation is performed, with the filter being passed over the entire image. The output of this operation acts as feature detection, for the filters that the network has learnt, producing a 2D feature map.", "The feature maps produced by each filter are concatenated, resulting in a 3D volume (the length of the third dimension being the number of filters).", "The next layer then performs convolution over this new volume, using a new set of learned filters.", "Below is an example of how to use a 2D convolution layer with the Keras functional API:", "The important hyperparameters in a convolutional layer are:", "The number of filters determines how many patterns each layer can learn. It\u2019s common to have the number of filters increasing with the depth of the network. Filter size is commonly set to (3, 3), with a ReLu as the activation function.", "Strides can be used to skip steps in the convolution, resulting in smaller feature maps. Padding can be used to allow pixels on the edge of the image to act as if they are in the middle of an image. Dilation allow the filters to operate over a larger area of the image, while still producing feature maps of the same size.", "Convolution works when your data has a spatial structure \u2014 for example, images have spatial structure in height & width. You can also get this structure from a 1D signal using techniques such as Fourier Transforms, and then perform convolution in the frequency domain.", "If you are working with images, convolution is king. While there is work applying attention based models to computer vision, because of it\u2019s similarity with our own visual cortex, it is likely that convolution will be relevant for many years to come.", "So what other kinds of structure can data have, other than spatial? Many types of data have a sequential structure \u2014 motivating our next two layer architectures.", "The third of our layers is the LSTM, or Long Short-Term Memory layer. The LSTM is recurrent \u2014 it processes data as a sequence.", "Recurrence allows a network to experience the temporal structure of data, such as words in a sentence, or time of day.", "A normal neural network receives a single input tensor $x$ and generates a single output tensor $y$. A recurrent architecture differs from a non-recurrent neural network in two ways:", "The memory of a recurrent architecture is known as the hidden state $h$. What the network chooses to pass forward in the hidden state is learnt by the network.", "Working with recurrent architectures requires being comfortable with the idea of a timestep dimension \u2014 knowing how to shape your data correctly is half the battle of working with recurrence.", "Imagine we have input data `x`, that is a sequence of integers [0, 0] -> [2, 20] -> [4, 40]. If we were using a fully connected layer, we could present this data to the network as a flat array:", "Although the sequence is obvious to us, it\u2019s not obvious to a fully connected layer.", "All a fully connected layer would see is a list of numbers \u2014 the sequential structure would need to be learnt by the network.", "We can restructure our data `x` to explicitly model this sequential structure, by adding a timestep dimension.", "The values in our data do not change \u2014 only the shape changes:", "Our data `x` is now structured with three dimensions \u2014 (batch, timesteps, features). A recurrent neural network will process the features one timestep at a time, experiencing the sequential structure of the data.", "Now that we understand how to structure data to be used with a recurrent neural network, we can take a high-level look at details of how the LSTM layer works.", "The LSTM was first introduced in 1997, and has formed the backbone of modern sequence based deep learning models, excelling on challenging tasks such as machine translation. For years the state of the art in machine translation was the seq2seq model, which is powered by the LSTM.", "The LSTM is a specific type a recurrent neural network. The LSTM addresses a challenge that vanilla recurrent neural networks struggled with \u2014 the ability to think long term.", "In a recurrent neural network all information passed to the next time step has to fit in a single channel, the hidden state `h`.", "The LSTM addresses the long term memory problem by using two hidden states, known as the hidden state `h` and the cell state `c`. Having two channels allows the LSTM to remember on both a long and short term.", "Internally the LSTM makes use of three gates to control the flow of information:", "One important architecture that uses LSTMs is seq2seq. The source sentence is fed through an encoder LSTM to generate a fixed length context vector. A second decoder LSTM takes this contex vector and generates the target sentence.", "For a deeper look at the internal of the LSTM, take a look at the excellent Understanding LSTM Networks from colah\u2019s blog.", "A good intiutive model for the LSTM layer is to think about it like a database. The output, input and delete gates allow the LSTM to work like a database \u2014 matching the GET, POST & DELETE of a REST API, or the read-update-delete operations of a CRUD application.", "The forget gate acts like a DELETE, allowing the LSTM to remove information that isn't useful. The input gate acts like a POST, where the LSTM can choose information to remember. The output gate acts like a GET, where the LSTM chooses what to send back to a user request for information.", "A recurrent neural network has has an inductive bias for processing data as a sequence, and for storing a memory. The LSTM adds on top of this bias for creating one long term and one short term memory channel.", "Below is an example of how to use an LSTM layer with the Keras functional API:", "You\u2019ll notice we only get one output for each of our four samples \u2014 where are the other two timesteps? To get these, we need to use return_sequences=True:", "It\u2019s also common to want to access the hidden states of the LSTM \u2014 this can be done using the argument return_state=True.", "We now get back three tensors \u2014 the output of the network, the LSTM hidden state and the LSTM cell state. The shape of the hidden states is equal to the number of units in the LSTM:", "If you wanted to access the hidden states at each timestep, then you can combine these two and use both return_sequences=True and return_state=True.", "For an LSTM layer, the main hyperparameter is the number of units. The number of units will determine the capacity of the layer and size of the hidden state .", "While not a hyperparameter, it can be useful to include gradient clipping when working with LSTMs, to deal with exploding gradients that can occur from the backpropagation through time. It is also common to use lower learning rates to help manage gradients.", "When working with sequence data, an LSTM (or it\u2019s close cousin the GRU) is a common choice. One major downside of the LSTM is that they are slow to train.", "This is because processing the sequence cannot be easily parallelized, as the error signal must be backpropagated through time.", "Another useful feature of the LSTM is the learnt hidden state. This can be used by other models as a compressed representation of the future \u2014 such as in the 2017 World Models paper.", "Attention is the youngest of our four layers \u2014 the only layer architecture to have been developed during the current deep learning moment.", "Since it\u2019s introduction in 2015, attention has revolutionized natural language processing.", "First used in combination with the LSTM based seq2seq model, attention is also to power the Transformer \u2014 a neural network architecture that forms the backbone of Open AI\u2019s GPT series of language models.", "Attention is important as it is an effective as a sequence model without recurrence \u2014 avoiding the need to do backpropagation through time, making it easier to parallelize and faster to train.", "Attention is a simple and powerful idea \u2014 when processing a sequence, we should choose what part of sequence to take information from. The intuition is simple \u2014 some parts of a sequence are more important that others.", "Take the example of machine translation, to translate the German sentence Ich bin eine Maschine into the English I am a machine.", "When predicting the last word in the translation machine, all of our attention should be placed on the last word of the source sentence Maschine. There is no point looking at earlier words in the source sequence when translating this token.", "If we take a more complex example of translating the German Ich habe ein bisschen Deutsch gelernt into the English I have learnt a little German. When predicting the third token of our English sentence ( learnt), attention should be placed on the last token of the German sentence ( gelernt).", "So what inductive bias does our attention layer give us? One inductive bias of attention is alignment based on similarity \u2014 the attention layer chooses where to look based on how similar things are.", "Another inductive bias of attention is to limit & prioritize information flow. As we will see below, the use of a softmax forces an attention layer to make tradeoffs about information flow \u2014 more weight in one place means less in another.", "There is no such restriction in a fully connected layer, where increasing one weight does not affect another. A fully connected layer can allow information to flow between all nodes in subsequent layers, and could in theory learn a similar pattern that an attention layer does. We know by now however that in theory does note mean it will occur in practice.", "The attention layer receives three inputs:", "The attention layer can be thought of as three mechanisms in sequence:", "Different attention layers (such as Additive Attention or Dot-Product Attention) use different mechanisms in the alignment step. The softmax & key selection steps are common to all attention layers.", "In the same way that understanding the time-step dimension is a key step in understanding recurrent neural networks, understanding what the query, key & value mean is foundational in attention.", "A good analogy is with the Python dictionary. Let\u2019s start with a simple example, where we:", "In the above example, we find an exact match for our query 'dog'. However, in a neural network, we are not working with strings - we are working with tensors. Our query, keys and values are all tensors:", "Now we don\u2019t have an exact match for our query \u2014 instead of using an exact match, we instead can calculate a similarity (i.e. an alignment) between our query and keys, and return the closest value:", "Small technicality \u2014 often the keys are set equal to the values. This simply means that the quantity we are doing the similarity comparison with is also the quantity we will place attention over.", "By now we know that an attention layer involves three steps:", "The second & third steps are common to all attention layers \u2014 the differences all occur in the first step \u2014 how the alignment on similarity is done.", "We will briefly look at two popular mechanisms \u2014 Additive Attention and Dot-Product Attention. For a more detailed look at these mechanisms, have a look at the excellent Attention? Attention! by Lilian Wang.", "This first use of attention (known as Bahdanau or Additive Attention) addressed one of the limitations of the seq2seq model \u2014 namely the use of a fixed length context vector.", "As explained in the LSTM section, the basic process in a seq2seq model is to encode the source sentence into a fixed length context vector. The issue is with all of the information from the encoder must pass through the fixed length context vector. Infomation from the entire source sequence is squeezed through this context vector inbetween the encoder & decoder.", "In Bahdanau et. al 2015, Additive Attention is used to learn an alignment between all the encoder hidden states and the decoder hidden states. As the sequence is processed, the output of this alignment is used in the decoder to predict the next token.", "A second type of attention is Dot-Product Attention \u2014 the alignment mechanism used in the Transformer. Instead of using addition, the Dot-Product Attention layer uses matrix multiplication to measure similarity between the query and the keys.", "The dot-product acts like a similarity between the keys & values \u2014 below is a small program that plots both the dot-product and the cosine similarity for random data:", "Dot-Product Attention is important as it forms part of the Transformer. As you can see in the figure below, the Transformer uses multiple heads of Scaled Dot-Product Attention.", "The code below demonstrates the mechanics for a single head without scaling \u2014 see Transformer Model for Language Understanding for a full implementation of a multi-head attention layer & Transformer in Tensorflow 2.", "This architecture also works with a different length query (now length 8 rather than 16):", "When using attention heads as shown above, hyperparameters to consider are:", "Attention layers should be considered for any sequence problem. Unlike recurrent neural networks, they can be easily parallelized, making training fast. Fast training means either cheaper training, or more training for the same amount of compute.", "The Transformer is a sequence model without recurrence (it doesn\u2019t use an LSTM), allowing it to be efficiently trained (avoiding backpropagation through time).", "One additional benefit of an attention layer is being able to use the alignment scores for interpretability.", "I hope you enjoyed this post and found it useful. Below is a short table summarizing the article:", "Thanks for reading! If you enjoyed this post, feel free to follow me on Medium or connect with me on LinkedIn.", "Make sure to check out my other posts:", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F225c93646e61&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-guide-to-four-deep-learning-layers-225c93646e61&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-guide-to-four-deep-learning-layers-225c93646e61&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-guide-to-four-deep-learning-layers-225c93646e61&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-guide-to-four-deep-learning-layers-225c93646e61&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----225c93646e61--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----225c93646e61--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@adgefficiency?source=post_page-----225c93646e61--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@adgefficiency?source=post_page-----225c93646e61--------------------------------", "anchor_text": "Adam Green"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe78e947c3373&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-guide-to-four-deep-learning-layers-225c93646e61&user=Adam+Green&userId=e78e947c3373&source=post_page-e78e947c3373----225c93646e61---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F225c93646e61&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-guide-to-four-deep-learning-layers-225c93646e61&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F225c93646e61&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-guide-to-four-deep-learning-layers-225c93646e61&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://arxiv.org/abs/1502.05477", "anchor_text": "Schulman et al 2015"}, {"url": "https://colah.github.io/posts/2015-08-Understanding-LSTMs/", "anchor_text": "Understanding LSTM Networks"}, {"url": "https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html", "anchor_text": "Attention? Attention!"}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "Vaswani et al 2017"}, {"url": "https://www.tensorflow.org/tutorials/text/transformer", "anchor_text": "Transformer Model for Language Understanding"}, {"url": "https://medium.com/@adgefficiency", "anchor_text": "Medium"}, {"url": "https://www.linkedin.com/in/adgefficiency/", "anchor_text": "LinkedIn"}, {"url": "https://towardsdatascience.com/seven-steps-to-generate-data-science-project-ideas-8fb69400634d", "anchor_text": "Seven Steps to Generate Data Science Project IdeasFind data science portfolio project ideas with these seven stepstowardsdatascience.com"}, {"url": "https://betterprogramming.pub/should-you-be-using-pathlib-6f3a0fddec7e", "anchor_text": "Should You Use Python pathlib or os?A duel between two Python path librariesbetterprogramming.pub"}, {"url": "https://betterprogramming.pub/3-uncommon-bash-tricks-that-you-should-know-c0fc988065c7", "anchor_text": "3 Uncommon Bash Tricks That You Should KnowType less on the terminal with these underused Bash patternsbetterprogramming.pub"}, {"url": "https://adgefficiency.com/guide-deep-learning/", "anchor_text": "https://adgefficiency.com"}, {"url": "https://medium.com/tag/data-science?source=post_page-----225c93646e61---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----225c93646e61---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----225c93646e61---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/tensorflow?source=post_page-----225c93646e61---------------tensorflow-----------------", "anchor_text": "TensorFlow"}, {"url": "https://medium.com/tag/deep-dives?source=post_page-----225c93646e61---------------deep_dives-----------------", "anchor_text": "Deep Dives"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F225c93646e61&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-guide-to-four-deep-learning-layers-225c93646e61&user=Adam+Green&userId=e78e947c3373&source=-----225c93646e61---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F225c93646e61&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-guide-to-four-deep-learning-layers-225c93646e61&user=Adam+Green&userId=e78e947c3373&source=-----225c93646e61---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F225c93646e61&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-guide-to-four-deep-learning-layers-225c93646e61&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----225c93646e61--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F225c93646e61&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-guide-to-four-deep-learning-layers-225c93646e61&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----225c93646e61---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----225c93646e61--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----225c93646e61--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----225c93646e61--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----225c93646e61--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----225c93646e61--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----225c93646e61--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----225c93646e61--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----225c93646e61--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@adgefficiency?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@adgefficiency?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Adam Green"}, {"url": "https://medium.com/@adgefficiency/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "226 Followers"}, {"url": "http://datasciencesouth.com", "anchor_text": "datasciencesouth.com"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe78e947c3373&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-guide-to-four-deep-learning-layers-225c93646e61&user=Adam+Green&userId=e78e947c3373&source=post_page-e78e947c3373--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F3458b9df45ca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-guide-to-four-deep-learning-layers-225c93646e61&newsletterV3=e78e947c3373&newsletterV3Id=3458b9df45ca&user=Adam+Green&userId=e78e947c3373&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}