{"url": "https://towardsdatascience.com/why-the-gaussian-distribution-is-a-natural-choice-part-1-bee0569b79df", "time": 1683000960.12042, "path": "towardsdatascience.com/why-the-gaussian-distribution-is-a-natural-choice-part-1-bee0569b79df/", "webpage": {"metadata": {"title": "Why the Gaussian distribution is a \u201cnatural\u201d choice (Part 1) | by Manuel Offidani | Towards Data Science", "h1": "Why the Gaussian distribution is a \u201cnatural\u201d choice (Part 1)", "description": "Common to all scientific theories is the ambition of deriving observable quantities starting from some abstract model. The parameters of the theory are usually assumed to be known, for instance\u2026"}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/Entropy_(statistical_thermodynamics)#Gibbs_entropy_formula", "anchor_text": "Gibbs entropy formula", "paragraph_index": 17}, {"url": "https://en.wikipedia.org/wiki/Lagrange_multiplier", "anchor_text": "Lagrange multiplier", "paragraph_index": 18}, {"url": "https://en.wikipedia.org/wiki/Functional_(mathematics)", "anchor_text": "linear mapping from a vector space into its field of scalars", "paragraph_index": 28}, {"url": "https://en.wikipedia.org/wiki/Dirac_delta_function", "anchor_text": "delta function", "paragraph_index": 30}, {"url": "https://medium.com/u/2d8e90b6694c?source=post_page-----bee0569b79df--------------------------------", "anchor_text": "Michail Palaiokostas", "paragraph_index": 34}], "all_paragraphs": ["Common to all scientific theories is the ambition of deriving observable quantities starting from some abstract model. The parameters of the theory are usually assumed to be known, for instance, based on first principles, direct measurement or something more sophisticated like symmetry considerations. On the other hand, in the Big Data era, growing interest is being devoted to the reverse path going from observation to parameters estimation. In such a reverse operation, unprecedented storage and computational capabilities allow data scientists \u2014 working in an increasing number of industries \u2014 to explore a vast region of parameters and possibly (and hopefully) find the \u201cright ones\u201d associated to a mathematical description of some phenomena of their interest, be it (just to mention some quite famous examples) predicting house prices, detect frauds, etc.", "However, no matter how powerful a data scientist\u2019s arsenal is, dealing with too big data \u2014 both from a quantitative (many many examples) and qualitative (large information content) standpoint \u2014 is most of the time a true challenge. On the one hand, exploring the entire universe of possible parameters is in practice an impossible task; on the other hand, while developing procedures for generic types of data is desirable, many algorithms are built making some assumptions on the input data. In this respect, the Gaussian distribution (GD) holds a central place in machine learning. Gaussian Naive Bayes, Linear and Quadratic discriminant analysis are examples of algorithms assuming that the data follow a GD.", "The ubiquity of the GD is often justified in terms of the central limit theorem, which states that (in the limit of large numbers N) the sum of random variables follows a GD. For instance, let us roll four dice obtaining 3, 1, 4, 4. The outcomes sum up to 12, and we take note of it. We repeat the experiment a second time, obtaining a total sum of 18. If we were to repeat the experiment N>>1 times, we would obtain a Gaussian bell describing the number of times a particular number between 4 (minimum sum) and 24 (max sum) has been observed. A simple calculation will show that the bell is centered around the value 3.5*4= 14 [note that as a matter of fact, 3.5 is the expected value for a single die roll].Now, if you\u2019re an abstract mathematician, I am pretty sure that you\u2019re more than happy with the central limit theorem to justify the existence of a privileged object such as the GD. However, I find that the central limit-based intuition, while being a relatively easy way to visualise how to build up a normally-distributed random variable, is not fully satisfactory to elucidating why Nature reserved such a special place to the GD.", "Is there any other way to crown the normal distribution as the queen of the probability distribution functions (PDFs)? Yes, and actually there are many. In the following, I will focus on one of them and explain to you how the GD emerges using a very fundamental physical concept: the entropy. In this post (Part 1) we are going to introduce the basic concepts by looking at the simplest uniform PDF, showing that it is the one that maximises the entropy of a system as such. In the second post (Part 2, soon to be published) we will devote our attention to the GD, and see that this class of PDFs maximises the entropy of distributions describing systems/processes whose mean and standard deviation are known.", "The concept of entropy finds its origin in the theory of thermodynamics, where it was first defined in the context of the Carnot cycle. However, a discussion on this particular aspect is well beyond the scope of this article. The only thing we need to know to carry on here is the following statement (2nd law of thermodynamics):", "In isolated systems (i.e. assumed to exchange no energy/mass with other systems), the total entropy never decreases over time.", "This means that, with the exception of a particular class of systems (i.e. time-reversible)\u2014 associated with constant entropy over time, Nature will let systems evolve in such a way that their entropy is maximised. We all experience this principle in our everyday life. Let me talk about an example; it takes a bit of practice, but you can always obtain a nice Gin&Tonic starting from a bottle of gin and one of tonic water (and maybe a slice of lemon, some pepper etc.). However, you can hardly reverse the operation. The reason behind this fact is that the entropy of the system gin + tonic water+ glass (+ice + lemon +\u2026) would decrease. In other words, we can go from an orderly state to a messier one, not vice versa. Nature, apparently, likes it messy!", "To understand more technically what entropy is, we can think about the particles of tonic water and gin (as we all know, they are fundamental elements in the periodic table\u2026) mixing up to form the cocktail in terms of balls and boxes. At a very high level, such a viewpoint is one of statistical mechanics and indeed there is a profound connection between this field and thermodynamics.", "Let us imagine our glass made up of eight boxes \u2014 I will label them from 1 to 8 going clockwise, see figure\u2014 and our gin and tonic water bottles containing 4 particles each. We can arrange the balls in the boxes as we like, with the only constraint that the number of gin and TW particles has to be conserved, meaning that we have a total of 8 particles before and after having poured the bottles\u2019 content into the glass. Take for instance the configuration in the figure below", "We have 1 gin and 1 tonic water particle in box 1, no particles in box 2, etc. This configuration corresponds to the \u201cgin and tonic water distribution function\u201d in the chart below. However, this is just one amongst all possible configurations of particles and associated gin and tonic water distribution functions! Other possible configurations are shown in Fig. 4 below", "Which configuration is the one we are more likely to observe though? I don\u2019t think anything is to be explained to convince you that a well-behaved gin&tonic will have gin and tonic water particles mixed properly. After a good shake, our intuition and expectation would correspond to something like the one below", "That is, in a good cocktail, the gin and tonic water distribution function is as uniform as possible. The reason behind this fact can be understood in terms of possible configurations corresponding to that particular distribution function. To make it simpler, let us think about two boxes and two particles only (of whatever substance you want to think about). The simple example in Fig. 6 below shows that the uniform the distribution, the higher the number of possible configurations to realise that distribution. In turn, the higher the available configurations, the messier the system, and the larger the entropy.", "Hence, the uniform distribution function is the one that maximises the entropy, including that of our gin and tonic. More technically, the uniform distribution function is the one associated with the highest possible number of microstates. A more comprehensive explanation of what that means is also beyond the scope of this post, but I will be happy to go in more detail with whomever wants to know more (just comment below)!", "We have by now progressed to understand the dynamics of our cocktail-making process in a more technical way, in terms of entropy and, more importantly, in terms of distribution functions. We still lack a mathematical translation (and demonstration) of the ideas illustrated above. That is what we are going to do in the following paragraph. Before proceeding, the reader should keep in mind just one point: the only assumption we have used is that the number of gin and tonic waters are conserved (i.e. they are neither destroyed nor created) while making up the cocktail (same for the simpler 2 boxes/2 balls world). That is the number of particles in all the boxes sum to 8 in our example", "If we divide both sides of the last equation by 8 defining p_i = n_i/8, (the notation _i indicates a subscript as in the following equation) we find some kind of equation that should look familiar to whoever has ever dealt with PDFs:", "The sum of the newly-defined box occupation probabilities p is 1. This justifies the idea of (probability) distribution function introduced above. In other words, what we are simply saying is that the conservation of particles can be seen as conservation of some PDF.", "Our goal here is formally deriving a distribution function by maximising the entropy of the system, given the constraint that the number of particles is conserved \u2014 which as we have just learned is tantamount to having conservation of probability. This is the simplest assumption we can do when we have no knowledge about the system, apart from the fact that its various configurations are described by a distribution function, which as such fulfills the second part of Eq. 2. To generalise and simplify the calculations, we work in the continuous case, replacing the sums with integrals over the entire volume of the glass \u2014 which would be exact in the limit of a glass with an infinite number of boxes", "Having a distribution function, the entropy S associated to it is given by the Gibbs entropy formula:", "We define another functional (i.e. a function of a function) which is just a smart extension of the entropy introduced above. The extension is smart because it introduces an additional term, with Lagrange multiplier (LM ) \u03bb, which encodes the fact that Eq. 2 has to be obeyed (we replace \u201cglass\u201d with a more generic V, indicating any kind of volume):", "Don\u2019t be scared! This functional J is just a number in the end\u2026 To calculate it, the knowledge of p and \u03bb is required. We don\u2019t know them yet, but actually calculating J is not our goal. What we are interested in is, in fact, the form of p such that J is maximised, and so is the entropy S. In the presence of the Lagrange multiplier, what we are really after is", "The value of p which maximises J (and thus S) with the additional constraint that p is a PDF \u2014 i.e. it integrates to 1", "Now to find the value of x which minimise or maximise a generic function f(x), what one usually does is to differentiate with respect to x and solve the equation df(x)/dx=0. Similarly, when dealing with functionals like the one in Eq.5 can define a functional derivative", "The distribution function minimising J is then the one for which Eq.6 vanishes. I will provide here the result of the functional derivative, addressing those interested in the mathematical steps in my appendix below. We obtain", "To fully uncover the form of p(x), as prescribed by the usual LM technique, we need to solve the following equation \u2014 derivative of J with respect to the LM \u2014 as well", "Performing such a derivative, setting it to 0 and replacing p(x) with what found in Eq. 8 we have", "where I used the fact that the integral over the volume is the volume itself. Comparing Eq.8 and Eq. 10, we finally find", "that is the PDF that maximises the entropy is the uniform PDF! So we have discovered that in the absence of any information about the PDF but the fact that it is a PDF, the \u201cnatural\u201d and least biased choice is the uniform distribution function. This result is both amazing and obvious! Knowing nothing about a statistical process, wouldn\u2019t you naturally assign an equal probability to all possible outcomes?", "In the next post, I will follow similar steps to see under which conditions the Gaussian distribution function is \u201cchosen\u201d by Nature. Stay tuned!", "First and foremost, a functional can be defined as a linear mapping from a vector space into its field of scalars. Roughly speaking, and to limit ourselves to our needs here, we can imagine a functional as an operation that takes a function and associates a number to it. One of the simplest functional is the integral, that takes a function f(x) \u2014 defined on some support V \u2014 and returns a scalar", "One can define derivatives of I with respect to its argument f. To understand how to do this, let us look at ordinary differentiation (we are thinking here about functions f: R \u2192 R mapping from the Real to Real numbers domains). This requires that one evaluates the incremental difference of some function f(x) for a very small increment \u03b5. The function f(x) is now itself the operation, and x is its argument. We have", "The latter means asking ourselves the question: how much does f(x) change when we slightly change the point where we are evaluating it? We can state the latter question more generally: how does the result of the mapping change when we slightly vary its argument? In this light, a definition of functional derivative is well-posed: it requires that we evaluate how much I changes when we perform some small change to the function f itself. The only caveat is that we need to choose a point in the support of f(x) to take this small increment. In the image below we are representing a possible example: we are adding a small bit to f(x) at a precise point t. Mathematically this can be represented by using a delta function", "Let us now try to evaluate the functional derivative of the functional defined above \u2014 the integral of f", "where we have used the main property of the delta function, that is it integrates to one if its argument vanishes within the domain of integration. That\u2019s it! The functional derivatives above (Eq.7) have been calculated with the same logic.", "The next post will follow shortly!", "Acknowledgments: thanks to Michail Palaiokostas for reading the first version of this post and suggesting some improvements!", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fbee0569b79df&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-the-gaussian-distribution-is-a-natural-choice-part-1-bee0569b79df&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-the-gaussian-distribution-is-a-natural-choice-part-1-bee0569b79df&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-the-gaussian-distribution-is-a-natural-choice-part-1-bee0569b79df&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-the-gaussian-distribution-is-a-natural-choice-part-1-bee0569b79df&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----bee0569b79df--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----bee0569b79df--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@manuel.offidani?source=post_page-----bee0569b79df--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@manuel.offidani?source=post_page-----bee0569b79df--------------------------------", "anchor_text": "Manuel Offidani"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa7957c561557&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-the-gaussian-distribution-is-a-natural-choice-part-1-bee0569b79df&user=Manuel+Offidani&userId=a7957c561557&source=post_page-a7957c561557----bee0569b79df---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbee0569b79df&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-the-gaussian-distribution-is-a-natural-choice-part-1-bee0569b79df&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbee0569b79df&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-the-gaussian-distribution-is-a-natural-choice-part-1-bee0569b79df&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://en.wikipedia.org/wiki/Entropy_(statistical_thermodynamics)#Gibbs_entropy_formula", "anchor_text": "Gibbs entropy formula"}, {"url": "https://en.wikipedia.org/wiki/Lagrange_multiplier", "anchor_text": "Lagrange multiplier"}, {"url": "https://en.wikipedia.org/wiki/Functional_(mathematics)", "anchor_text": "linear mapping from a vector space into its field of scalars"}, {"url": "https://en.wikipedia.org/wiki/Dirac_delta_function", "anchor_text": "delta function"}, {"url": "https://medium.com/u/2d8e90b6694c?source=post_page-----bee0569b79df--------------------------------", "anchor_text": "Michail Palaiokostas"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----bee0569b79df---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/entropy?source=post_page-----bee0569b79df---------------entropy-----------------", "anchor_text": "Entropy"}, {"url": "https://medium.com/tag/data-science?source=post_page-----bee0569b79df---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/statistics?source=post_page-----bee0569b79df---------------statistics-----------------", "anchor_text": "Statistics"}, {"url": "https://medium.com/tag/physics?source=post_page-----bee0569b79df---------------physics-----------------", "anchor_text": "Physics"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbee0569b79df&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-the-gaussian-distribution-is-a-natural-choice-part-1-bee0569b79df&user=Manuel+Offidani&userId=a7957c561557&source=-----bee0569b79df---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbee0569b79df&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-the-gaussian-distribution-is-a-natural-choice-part-1-bee0569b79df&user=Manuel+Offidani&userId=a7957c561557&source=-----bee0569b79df---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbee0569b79df&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-the-gaussian-distribution-is-a-natural-choice-part-1-bee0569b79df&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----bee0569b79df--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fbee0569b79df&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-the-gaussian-distribution-is-a-natural-choice-part-1-bee0569b79df&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----bee0569b79df---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----bee0569b79df--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----bee0569b79df--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----bee0569b79df--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----bee0569b79df--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----bee0569b79df--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----bee0569b79df--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----bee0569b79df--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----bee0569b79df--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@manuel.offidani?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@manuel.offidani?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Manuel Offidani"}, {"url": "https://medium.com/@manuel.offidani/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "32 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa7957c561557&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-the-gaussian-distribution-is-a-natural-choice-part-1-bee0569b79df&user=Manuel+Offidani&userId=a7957c561557&source=post_page-a7957c561557--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Fa7957c561557%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-the-gaussian-distribution-is-a-natural-choice-part-1-bee0569b79df&user=Manuel+Offidani&userId=a7957c561557&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}