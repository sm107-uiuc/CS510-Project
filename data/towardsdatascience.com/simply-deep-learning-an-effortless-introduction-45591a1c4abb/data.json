{"url": "https://towardsdatascience.com/simply-deep-learning-an-effortless-introduction-45591a1c4abb", "time": 1682994710.769755, "path": "towardsdatascience.com/simply-deep-learning-an-effortless-introduction-45591a1c4abb/", "webpage": {"metadata": {"title": "The Complete Beginner\u2019s Guide to Deep Learning: Artificial Neural Networks | by Anne Bonner | Towards Data Science", "h1": "The Complete Beginner\u2019s Guide to Deep Learning: Artificial Neural Networks", "description": "At a very basic level, deep learning is a machine learning technique. It teaches a computer to filter inputs through layers to learn how to predict and classify information. Observations can be in\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/intro-to-deep-learning-c025efd92535", "anchor_text": "Complete Beginner\u2019s Guide to Deep Learning", "paragraph_index": 0}, {"url": "https://stats.stackexchange.com/questions/154879/a-list-of-cost-functions-used-in-neural-networks-alongside-applications", "anchor_text": "A List of Cost Functions Used in Neural Networks, Alongside Applications", "paragraph_index": 19}, {"url": "http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf", "anchor_text": "Efficient BackProp", "paragraph_index": 21}, {"url": "http://neuralnetworksanddeeplearning.com/", "anchor_text": "Neural Networks and Deep Learning", "paragraph_index": 21}, {"url": "http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf", "anchor_text": "Deep Sparse Rectifier Neural Networks", "paragraph_index": 29}, {"url": "https://iamtrask.github.io/2015/07/27/python-network-part2/", "anchor_text": "A Neural Network in 13 lines of Python-Part 2 Gradient Descent", "paragraph_index": 40}, {"url": "http://neuralnetworksanddeeplearning.com/", "anchor_text": "Neural Networks and Deep Learning", "paragraph_index": 40}, {"url": "https://towardsdatascience.com/wtf-is-image-classification-8e78a8235acb", "anchor_text": "Come on over to part 3", "paragraph_index": 43}, {"url": "https://www.linkedin.com/in/annebonnerdata/", "anchor_text": "@annebonnerdata", "paragraph_index": 44}, {"url": "http://bit.ly/submissions2023", "anchor_text": "http://bit.ly/submissions2023", "paragraph_index": 46}], "all_paragraphs": ["This article is part of the Complete Beginner\u2019s Guide to Deep Learning series.", "It\u2019s learning from examples. That\u2019s pretty much the deal!", "At a very basic level, deep learning is a machine learning technique. It teaches a computer to filter inputs through layers to learn how to predict and classify information. Observations can be in the form of images, text, or sound.", "The inspiration for deep learning is the way that the human brain filters information. Its purpose is to mimic how the human brain works to create some real magic.", "Deep learning attempts to mimic the activity in layers of neurons in the neocortex.", "It\u2019s very literally an artificial neural network.", "In the human brain, there are about 100 billion neurons. Each neuron connects to about 100,000 of its neighbors. That is what we\u2019re trying to create, but in a way and at a level that works for machines.", "What does this mean in terms of neurons, axons, dendrites, and so on? Well, the neuron has a body, dendrites, and an axon. The signal from one neuron travels down the axon and transfers to the dendrites of the next neuron. That connection where the signal passes is called a synapse.", "Neurons by themselves are kind of useless. But when you have lots of them, they work together to create some serious magic. That\u2019s the idea behind a deep learning algorithm! You get input from observation and you put your input into one layer. That layer creates an output which in turn becomes the input for the next layer, and so on. This happens over and over until your final output signal!", "So the neuron (or node) gets a signal or signals (input values), which pass through the neuron. That neuron delivers the output signal. Think of the input layer as your senses: the things you, for example, see, smell, and feel. These are independent variables for one single observation. This information is broken down into numbers and the bits of binary data that a computer can use. (You will need to either standardize or normalize these variables so that they\u2019re within the same range.)", "What about synapses? Each of the synapses gets assigned weights, which are crucial to Artificial Neural Networks (ANNs). Weights are how ANNs learn. By adjusting the weights, the ANN decides to what extent signals get passed along. When you\u2019re training your network, you\u2019re deciding how the weights are adjusted.", "There are two different approaches to get a program to do what you want. First, there\u2019s the specifically guided and hard-programmed approach. In this approach, you tell the program exactly what you want it to do. Then there are neural networks. In neural networks, you tell your network the inputs and what you want for the outputs, and let it learn on its own. By allowing the network to learn on its own, we can avoid the necessity of entering in all the rules. For a neural network, you can create the architecture and then let it go and learn. Once it\u2019s trained up, you can give it a new image and it will be able to distinguish output.", "There are different kinds of neural networks. They\u2019re generally classified into feedforward and feedback networks.", "A feedforward network is a network that contains inputs, outputs, and hidden layers. The signals can only travel in one direction (forward). Input data passes into a layer where calculations are performed. Each processing element computes based upon the weighted sum of its inputs. The new values become the new input values that feed the next layer (feed-forward). This continues through all the layers and determines the output. Feedforward networks are often used in, for example, data mining.", "A feedback network (for example, a recurrent neural network) has feedback paths. This means that they can have signals traveling in both directions using loops. All possible connections between neurons are allowed. Since loops are present in this type of network, it becomes a non-linear dynamic system which changes continuously until it reaches a state of equilibrium. Feedback networks are often used in optimization problems where the network looks for the best arrangement of interconnected factors.", "The majority of modern deep learning architectures are based on artificial neural networks (ANNs). They use many layers of nonlinear processing units for feature extraction and transformation. Each successive layer uses the output of the previous layer for its input. What they learn forms a hierarchy of concepts. In this hierarchy, each level learns to transform its input data into a more and more abstract and composite representation.", "That means that for an image, for example, the input might be a matrix of pixels. The first layer might encode the edges and compose the pixels. The next layer might compose an arrangement of edges. The next layer might encode a nose and eyes. The next layer might recognize that the image contains a face, and so on.", "What happens inside the neuron? The input node takes in information that in a numerical form. The information is presented as an activation value where each node is given a number. The higher the number, the greater the activation.", "Based on the connection strength (weights) and transfer function, the activation value passes to the next node. Each of the nodes sums the activation values that it receives (it calculates the weighted sum) and modifies that sum based on its transfer function. Next, it applies an activation function. An activation function is a function that\u2019s applied to this particular neuron. From that, the neuron understands if it needs to pass along a signal or not. The activation runs through the network until it reaches the output nodes. The output nodes then give us the information in a way that we can understand. Your network will use a cost function to compare the output and the actual expected output. The model performance is evaluated by the cost function. It\u2019s expressed as the difference between the actual value and the predicted value. There are many different cost functions you can use, you\u2019re looking at what the error you have in your network is. You\u2019re working to minimize loss function. (In essence, the lower the loss function, the closer it is to your desired output). The information goes back, and the neural network begins to learn with the goal of minimizing the cost function by tweaking the weights. This process is called backpropagation.", "Interested in learning more about cost functions? Check out A List of Cost Functions Used in Neural Networks, Alongside Applications on Stack Exchange", "In forward propagation, information is entered into the input layer and propagates forward through the network to get our output values. We compare the values to our expected results. Next, we calculate the errors and propagate the info backward. This allows us to train the network and update the weights. Backpropagation allows us to adjust all the weights simultaneously. During this process, because of the way the algorithm is structured, you\u2019re able to adjust all of the weights simultaneously. This allows you to see which part of the error each of your weights in the neural network is responsible for.", "Hungry for more? You might want to read Efficient BackProp by Yann LeCun, et al., as well as Neural Networks and Deep Learning by Michael Nielsen.", "When you\u2019ve adjusted the weights to the optimal level, you\u2019re ready to proceed to the testing phase!", "Inputs to a neuron can either be features from a training set or outputs from the neurons of a previous layer. Each connection between two neurons has a unique synapse with a unique weight attached. If you want to get from one neuron to the next, you have to travel along the synapse and pay the \u201ctoll\u201d (weight). The neuron then applies an activation function to the sum of the weighted inputs from each incoming synapse. It passes the result on to all the neurons in the next layer. When we talk about updating weights in a network, we\u2019re talking about adjusting the weights on these synapses.", "A neuron\u2019s input is the sum of weighted outputs from all the neurons in the previous layer. Each input is multiplied by the weight associated with the synapse connecting the input to the current neuron. If there are 3 inputs or neurons in the previous layer, each neuron in the current layer will have 3 distinct weights: one for each synapse.", "So what is an activation function?", "In a nutshell, the activation function of a node defines the output of that node.", "The activation function (or transfer function) translates the input signals to output signals. It maps the output values on a range like 0 to 1 or -1 to 1. It\u2019s an abstraction that represents the rate of action potential firing in the cell. It\u2019s a number that represents the likelihood that the cell will fire. At it\u2019s simplest, the function is binary: yes (the neuron fires) or no (the neuron doesn\u2019t fire). The output can be either 0 or 1 (on/off or yes/no), or it can be anywhere in a range. If you were using a function that maps a range between 0 and 1 to determine the likelihood that an image is a cat, for example, an output of 0.9 would show a 90% probability that your image is, in fact, a cat.", "What options do we have? There are many activation functions, but these are the four very common ones:", "Want to dive deeper? Check out Deep Sparse Rectifier Neural Networks by Xavier Glorot, et al.", "So let\u2019s say, for example, your desired value is binary. You\u2019re looking for a \u201cyes\u201d or a \u201cno.\u201d Which activation function do you want to use? From the above examples, you could use the threshold function, or you could go with the sigmoid activation function. The sigmoid function would be able to give you the probability of a yes.", "So, how are the weights adjusted, exactly?", "You could use a brute force approach to adjust the weights and test thousands of different combinations. Even with the most simple neural network that has only five input values and a single hidden layer, you\u2019ll wind up with 10\u2077\u2075 possible combinations. Running this on the world\u2019s fastest supercomputer would take longer than the universe has existed so far.", "However, if you go with gradient descent, you can look at the angle of the slope of the weights and find out if it\u2019s positive or negative in order to continue to slope downhill to find the best weights on your quest to reach the global minimum.", "If you go with gradient descent, you can look at the angle of the slope of the weights and find out if it\u2019s positive or negative. This allows you to continue to slope downhill to find the best weights on your quest to reach the global minimum.", "Gradient descent is an algorithm for finding the minimum of a function. The analogy you\u2019ll see over and over is that of someone stuck on top of a mountain and trying to get down (find the minima). There\u2019s heavy fog making it impossible to see the path, so she uses gradient descent to get down to the bottom of the mountain. She looks at the steepness of the hill where she is and proceeds down in the direction of the steepest descent. You should assume that the steepness isn\u2019t immediately obvious. Luckily she has a tool that can measure steepness. Unfortunately, this tool takes forever. She wants to use it as infrequently as she can to get down the mountain before dark. The real difficulty is choosing how often she wants to use her tool so she doesn\u2019t go off track. In this analogy, the person is the algorithm. The steepness of the hill is the slope of the error surface at that point. The direction she goes is the gradient of the error surface at that point. The tool she\u2019s using is differentiation (the slope of the error surface can be calculated by taking the derivative of the squared error function at that point). The rate at which she travels before taking another measurement is the learning rate of the algorithm. It\u2019s not a perfect analogy, but it gives you a good sense of what gradient descent is all about. The machine is learning the gradient, or direction, that the model should take to reduce errors.", "Gradient descent requires the cost function to be convex, but what if it isn\u2019t?", "Normal gradient descent will get stuck at a local minimum rather than a global minimum, resulting in a subpar network. In normal gradient descent, we take all our rows and plug them into the same neural network, take a look at the weights, and then adjust them. This is called batch gradient descent. In stochastic gradient descent, we take the rows one by one, run the neural network, look at the cost functions, adjust the weights, and then move to the next row. Essentially, you\u2019re adjusting the weights for each row.", "Stochastic gradient descent has much higher fluctuations, which allows you to find the global minimum. It\u2019s called \u201cstochastic\u201d because samples are shuffled randomly, instead of as a single group or as they appear in the training set. It looks like it might be slower, but it\u2019s actually faster because it doesn\u2019t have to load all the data into memory and wait while the data is all run together. The main pro for batch gradient descent is that it\u2019s a deterministic algorithm. This means that if you have the same starting weights, every time you run the network you will get the same results. Stochastic gradient descent is always working at random. (You can also run mini-batch gradient descent where you set a number of rows, run that many rows at a time, and then update your weights.)", "Many improvements on the basic stochastic gradient descent algorithm have been proposed and used, including implicit updates (ISGD), momentum method, averaged stochastic gradient descent, adaptive gradient algorithm (AdaGrad), root mean square propagation (RMSProp), adaptive moment estimation (Adam), and more.", "Loving this? You might want to take a look at A Neural Network in 13 lines of Python-Part 2 Gradient Descent by Andrew Trask and Neural Networks and Deep Learning by Michael Nielsen", "So here\u2019s a quick walkthrough of training an artificial neural network with stochastic gradient descent:", "That\u2019s it! You now know the basic ideas behind what\u2019s happening in an artificial neural network!", "Still with me? Come on over to part 3!", "As always, if you do anything cool with this information, leave a comment in the responses below or reach out any time on LinkedIn @annebonnerdata!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Building a community space at VentureBeat \ud83d\udcab Submit your articles here: http://bit.ly/submissions2023"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F45591a1c4abb&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimply-deep-learning-an-effortless-introduction-45591a1c4abb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimply-deep-learning-an-effortless-introduction-45591a1c4abb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimply-deep-learning-an-effortless-introduction-45591a1c4abb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimply-deep-learning-an-effortless-introduction-45591a1c4abb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----45591a1c4abb--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----45591a1c4abb--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@annebonner?source=post_page-----45591a1c4abb--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@annebonner?source=post_page-----45591a1c4abb--------------------------------", "anchor_text": "Anne Bonner"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa71060a2ef24&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimply-deep-learning-an-effortless-introduction-45591a1c4abb&user=Anne+Bonner&userId=a71060a2ef24&source=post_page-a71060a2ef24----45591a1c4abb---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F45591a1c4abb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimply-deep-learning-an-effortless-introduction-45591a1c4abb&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F45591a1c4abb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimply-deep-learning-an-effortless-introduction-45591a1c4abb&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/intro-to-deep-learning-c025efd92535", "anchor_text": "Complete Beginner\u2019s Guide to Deep Learning"}, {"url": "http://pixabay.com", "anchor_text": "Pixabay"}, {"url": "https://media.giphy.com/media/9N2UvCx7wXLnG/giphy.gif", "anchor_text": "GIPHY"}, {"url": "https://pixabay.com/", "anchor_text": "Pixabay"}, {"url": "https://pixabay.com/", "anchor_text": "Pixabay"}, {"url": "https://unsplash.com/@anniespratt?utm_source=medium&utm_medium=referral", "anchor_text": "Annie Spratt"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "http://pixabay.com", "anchor_text": "Pixabay"}, {"url": "https://stats.stackexchange.com/questions/154879/a-list-of-cost-functions-used-in-neural-networks-alongside-applications", "anchor_text": "A List of Cost Functions Used in Neural Networks, Alongside Applications"}, {"url": "http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf", "anchor_text": "Efficient BackProp"}, {"url": "http://neuralnetworksanddeeplearning.com/", "anchor_text": "Neural Networks and Deep Learning"}, {"url": "https://pixabay.com/", "anchor_text": "Pixabay"}, {"url": "http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf", "anchor_text": "Deep Sparse Rectifier Neural Networks"}, {"url": "https://unsplash.com/@rawpixel?utm_source=medium&utm_medium=referral", "anchor_text": "rawpixel"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://pixabay.com/", "anchor_text": "Pixabay"}, {"url": "https://iamtrask.github.io/2015/07/27/python-network-part2/", "anchor_text": "A Neural Network in 13 lines of Python-Part 2 Gradient Descent"}, {"url": "http://neuralnetworksanddeeplearning.com/", "anchor_text": "Neural Networks and Deep Learning"}, {"url": "https://unsplash.com/@sammathews?utm_source=medium&utm_medium=referral", "anchor_text": "Sam Mathews"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://towardsdatascience.com/wtf-is-image-classification-8e78a8235acb", "anchor_text": "Come on over to part 3"}, {"url": "https://www.linkedin.com/in/annebonnerdata/", "anchor_text": "@annebonnerdata"}, {"url": "https://medium.com/tag/technology?source=post_page-----45591a1c4abb---------------technology-----------------", "anchor_text": "Technology"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----45591a1c4abb---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----45591a1c4abb---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----45591a1c4abb---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/tech?source=post_page-----45591a1c4abb---------------tech-----------------", "anchor_text": "Tech"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F45591a1c4abb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimply-deep-learning-an-effortless-introduction-45591a1c4abb&user=Anne+Bonner&userId=a71060a2ef24&source=-----45591a1c4abb---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F45591a1c4abb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimply-deep-learning-an-effortless-introduction-45591a1c4abb&user=Anne+Bonner&userId=a71060a2ef24&source=-----45591a1c4abb---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F45591a1c4abb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimply-deep-learning-an-effortless-introduction-45591a1c4abb&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----45591a1c4abb--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F45591a1c4abb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimply-deep-learning-an-effortless-introduction-45591a1c4abb&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----45591a1c4abb---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----45591a1c4abb--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----45591a1c4abb--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----45591a1c4abb--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----45591a1c4abb--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----45591a1c4abb--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----45591a1c4abb--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----45591a1c4abb--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----45591a1c4abb--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@annebonner?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@annebonner?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Anne Bonner"}, {"url": "https://medium.com/@annebonner/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "6.4K Followers"}, {"url": "http://bit.ly/submissions2023", "anchor_text": "http://bit.ly/submissions2023"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa71060a2ef24&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimply-deep-learning-an-effortless-introduction-45591a1c4abb&user=Anne+Bonner&userId=a71060a2ef24&source=post_page-a71060a2ef24--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F29e21558a591&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimply-deep-learning-an-effortless-introduction-45591a1c4abb&newsletterV3=a71060a2ef24&newsletterV3Id=29e21558a591&user=Anne+Bonner&userId=a71060a2ef24&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}