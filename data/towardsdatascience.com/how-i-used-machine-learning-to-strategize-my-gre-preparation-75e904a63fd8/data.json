{"url": "https://towardsdatascience.com/how-i-used-machine-learning-to-strategize-my-gre-preparation-75e904a63fd8", "time": 1683004078.259827, "path": "towardsdatascience.com/how-i-used-machine-learning-to-strategize-my-gre-preparation-75e904a63fd8/", "webpage": {"metadata": {"title": "How I used machine learning to strategize my GRE preparation. | by Sarthak Vajpayee | Towards Data Science", "h1": "How I used machine learning to strategize my GRE preparation.", "description": "When I started with my GRE preparation, after going through many resources (for the vocab section) I found that there are some words that pretty commonly appear in the exam and Barron\u2019s\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.w3schools.com/html/", "anchor_text": "this tutorial", "paragraph_index": 6}, {"url": "https://quizlet.com/2832581/barrons-333-high-frequency-words-flash-cards/", "anchor_text": "this website", "paragraph_index": 7}, {"url": "https://www.thesaurus.com/browse/", "anchor_text": "this website", "paragraph_index": 11}, {"url": "https://en.wikipedia.org/wiki/Regularization_(mathematics)", "anchor_text": "regularization", "paragraph_index": 32}, {"url": "https://www.visiondummy.com/2014/03/eigenvalues-eigenvectors/", "anchor_text": "https://www.visiondummy.com/2014/03/eigenvalues-eigenvectors/", "paragraph_index": 36}, {"url": "https://youtu.be/Nx0lRBaXoz4", "anchor_text": "this lecture", "paragraph_index": 40}, {"url": "https://medium.com/@jonathan_hui/machine-learning-singular-value-decomposition-svd-principal-component-analysis-pca-1d45e885e491", "anchor_text": "this wonderful blog", "paragraph_index": 40}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html", "anchor_text": "Truncated SVD", "paragraph_index": 41}, {"url": "https://www.machinelearningplus.com/nlp/cosine-similarity/", "anchor_text": "this blog", "paragraph_index": 53}, {"url": "https://networkx.github.io/documentation/stable/", "anchor_text": "networkx library", "paragraph_index": 60}, {"url": "https://networkx.github.io/documentation/stable/", "anchor_text": "here", "paragraph_index": 60}, {"url": "https://en.wikipedia.org/wiki/Student%27s_t-distribution", "anchor_text": "student\u2019s t-distribution", "paragraph_index": 62}, {"url": "https://distill.pub/2016/misread-tsne/", "anchor_text": "this awesome blog", "paragraph_index": 62}], "all_paragraphs": ["When I started with my GRE preparation, after going through many resources (for the vocab section) I found that there are some words that pretty commonly appear in the exam and Barron\u2019s high-frequency word list is one of the renowned resources that solve this problem. To begin with, I picked Barron\u2019s 333 which is one such word list that contains 333 most frequently occurring words in GRE. The next challenge was learning these words so I came up with a plan. If I could somehow group similar words together it would make the learning process much easier. But how to do that? Manually grouping these words would be way more challenging than simply learning the words as they are. After pondering for some time, it occurred to me why not let the machine do all the hard work! I think with a capability of above one million million floating-point operations per second it is much better for these types of tasks than I am so let\u2019s get started and see how to build a model from scratch that clusters similar words together.", "I\u2019ll be covering several machine learning concepts like Natural Language Processing (NLP), Term Frequency-Inverse Document Frequency (TF-IDF), Singular Value Decomposition (SVD), K-Means, t-Distributed Stochastic Neighbor Embedding (t-SNE) and many other techniques for data scraping, feature engineering and data visualization to demonstrate how we can cluster data from scratch.", "Note: I\u2019ll be using python 3.7 for this project.", "The blog will be divided into the following parts-", "Now that we know the problem statement and the data flow, let\u2019s dive in.", "The first task is to collect the data i.e. Barron\u2019s 333 high-frequency words. This can be done either by manually typing the words and creating a list or by automating the process. I used BeaulifulSoup and request to create a function that automatically scraped the data from different websites, let\u2019s briefly understand the libraries and how to use them.", "The code will use requests to get the response from the target websites, then using BeautifulSoup it\u2019ll parse the html response and scrape out the required information from the page(s) and store the information in a tabular format using pandas.To understand the format of an html page, you can check out this tutorial.", "Let\u2019s scrape the Barron\u2019s 333 words and their meanings from this website-", "# The HTML code of a webpage in chrome can be accessed using \u2318+shift+c on mac or ctrl+shift+c on windows/Linux.", "Here I\u2019m using \u2018span\u2019 as tag, class_= \u2018TermText notranslate lang-en\u2019 since the elements containing word and meaning have the same tag and class and there are only 2 such elements in every row element, 1st one corresponding to word and the 2nd one to the meaning.", "This is the scraped data in tabular form.", "This data is not enough so let\u2019s add more data by scraping the synonyms of each word from this website-", "Now let\u2019s join the 2 data frames (meanings and synonyms):", "We can see the data needs some cleaning since it contains stop-words like and, or, the and other elements like punctuation marks. Also, we must take care of the contractions like can\u2019t, won\u2019t, don\u2019t these must be converted to can not, would not, do not respectively.", "Now since the data has been cleaned, let\u2019s do some feature engineering.", "One thing that I noticed was that some of these words have synonyms that also belong to Barron\u2019s 333 list. If I could somehow concatenate the synonyms of these words, it could increase the performance of our model.", "After this step, for each word in Barron\u2019s 333-word list we have, it\u2019s direct synonyms, indirect synonyms (in the above example, the synonyms of convoluted are the indirect synonyms of tortuous) and meaning. I\u2019ll use the notation set for this data further in this blog.", "Set: data about a word (here the word is from barron\u2019s 333) like it\u2019s direct synonyms, indirect synonyms and meaning. The set of a word includes the word itself.", "We have obtained clean sets of words that we need to cluster but remember we first need to convert these sets into some kind of numerical data because our model needs numbers to work on.", "I\u2019ll use TF-IDF to vectorize the data. Before diving in let\u2019s understand what tf-idf is-", "TF-IDF short for term frequency-inverse document frequency is a numerical statistic that is intended to reflect how important a word is to a document in a corpus. Let\u2019s understand this using Bag of Words.", "The bag-of-words model is a simplifying representation used in natural language processing and information retrieval. In this model, a text is represented as the bag of its words. In simple terms, Bag of Words is nothing but a basic numerical representation of documents, it is done by first creating a vocabulary of words that contains all the distinct words from all the documents. Now each document is represented using a vector that has \u2018n\u2019 elements (here, n is the number of words in the vocabulary so each element corresponds to a word in the vocabulary) and each element has a numerical value that tells us how many times that word was seen in that document. let\u2019s consider an example:", "Term frequency: It is the number that represents how often a word is present in the document.", "Inverse document frequency: It is the inverse of the log of the chance of finding a document that has the word in it.", "Think about it this way: If the word is used extensively in all documents, its existence within a specific document will not be able to provide as much specific information about the document itself. So the second term could be seen as a penalty term that penalizes common words such as \u201ca\u201d, \u201cthe\u201d, \u201cand\u201d, etc. tf-idf can, therefore, be seen as a weighting scheme for words relevancy in a specific document.", "Let\u2019s check the TF-IDF of the two documents:Document 1: \u201ctf stands for term frequency\u201d, terms: 5Document 2: \u201cand idf stands for inverse document frequency\u201d, terms: 7", "Bi-grams: A bigram is a sequence of two adjacent elements from a string of tokens, which are typical letters, syllables, or words. Here is an example of uni-grams and bi-grams generated from a document.doc: \u201ctf stands for term frequency\u201duni-grams: [\u2018tf\u2019, \u2018stands\u2019, \u2018for\u2019, \u2018term\u2019, \u2018frequency\u2019]bi-grams: [\u2018tf stands\u2019, \u2018stands for\u2019, \u2018for term\u2019, \u2018term frequency\u2019]", "The advantage of n-grams is that they add information about the sequence of words in a document.", "I\u2019ve written a function for calculating the TF-IDF, the function uses both uni-grams and bi-grams.", "Now, that we have the TF-IDF embeddings for each of the sets, we can proceed to modeling.", "Note: The data we have till now is in a tabular form containing m rows and n columns where \u2018m\u2019 is the number of words in Barron\u2019s 333-word list and \u2018n\u2019 is the size of Bag of Words vocabulary. This tabular data can also be represented as an array of dimension (m x n). Further in the blog, I\u2019ll be using array to represent the data.", "Before Modeling there is something that I would like to share that I found very helpful. Instead of using the TF-IDF values directly for modeling, how about bringing down the dimensions of the data?That means, we have TF-IDF values corresponding to each set and since these TF-IDF values are represented as a vector of n elements, where \u2018n\u2019 also corresponds to the number of distinct words in all of our sets. If 2 sets have almost the same words, the distance between there corresponding points in an n-dimensional hyperplane is going to be very less and vice-versa. Similarly, if the 2 sets have very less words in common, the distance between there corresponding points in n-hyperplane is going to be much more and vice-versa. Now instead of using n-hyperplane to represent these points, I reduced the dimensionality of the points to 32-dimensions (Why 32? is discussed later in the blog). The dimensionality can be reduced by picking 32 random dimensions and ignoring the others but that would just be too stupid, so I tried using different dimensionality reduction techniques and found Truncated SVD to work miracles for the given data.", "I used dimensionality reduction as a method to add some form of regularization to the data.", "SVD abbreviation for Singular Value Decomposition is a matrix factorization technique that factorizes any given matrix into the three matrices U, S, and V.It goes by the equation-", "Let me explain what U, S, and V are.U (aka left singular): is an orthogonal matrix whose columns are the eigenvectors of A\u1d40A.S (singular): is a diagonal matrix whose diagonal elements are the square root of eigenvalues of A\u1d40A or AA\u1d40(both have the same Eigenvalues) arranged in descending order i.e.", "V (aka right singular): is an orthogonal matrix whose columns are the eigenvectors of AA\u1d40.", "Eigenvectors: An eigenvector is a vector whose direction remains unchanged when a linear transformation is applied to it. Consider the image below in which three vectors are shown. The green square is only drawn to illustrate the linear transformation that is applied to each of these three vectors.Note that the direction of these eigenvectors do not change but their length does change, and eigenvalue is the factor by which their length change.source: https://www.visiondummy.com/2014/03/eigenvalues-eigenvectors/", "So now that we know how to factorize a matrix, we can reduce the dimension of a matrix using Truncated SVD which is a simple extension to SVD.", "Truncated SVD: Suppose we have an input matrix of dimensions (m x n) which we want to reduce to (m x r) where r<n. We simply compute the first \u2018r\u2019 eigenvectors of A\u1d40A and store it as the columns of U, then we compute the first \u2018r\u2019 eigenvectors of AA\u1d40 and store it as the columns of V and finally the root of first \u2018r\u2019 eigenvalues as diagonal elements of S.", "So in a nutshell, Truncated-SVD is a smart technique that reduces the dimensionality of given data in a smart way by preserving as much information (variance) as possible.", "If you want to dive deeper into SVD, check out this lecture by Prof. W. Gilbert Strang, and this wonderful blog on SVD.", "Scikit-learn comes with Truncated SVD built in that can be imported and used directly.", "Now that we are done with the data pre-processing and feature engineering part, let\u2019s see how to group similar words together using a clustering algorithm.", "K-means clustering is a type of unsupervised learning, which is used when you have unlabeled data (i.e., data without defined categories or groups). The goal of this algorithm is to find groups in the data, with the number of groups represented by the variable K.", "Let\u2019s see how K-means works.Suppose there are some points in a 2-dimensional plane, and we want to cluster these points into K clusters.", "But how to decide the right value for K (number of clusters)?Let\u2019s define a metric that can be used to measure the right value of K.Distortion is one such metric that uses the sum of squares of the distance of points in a cluster from the cluster mean, for all the clusters summed up.", "Distortion can be used to check how efficient the clustering algorithm is for a given value of K. The optimal value of K can be determined by calculating the distortion value for different values of K and then plotting them. This plot is known as the Elbow plot. Just by looking at the elbow plot we can determine the optimal \u2018K\u2019 as the value where distortion stops decreasing rapidly.", "Here is an Elbow plot and just by looking at it, we can say that the best hyperparameter is k=3.", "It is called elbow plot because it looks like an arm (maybe a stick man\u2019s arm) and the elbow of this arm represents the optimal K.", "One more thing, I\u2019ll be using cosine distance as a measure to compute the distance between points (including centroids). Let\u2019s quickly understand what cosine distance is using cosine similarity.", "Cosine similarity is a measure to calculate how parallel 2 vectors are. It is calculated using the cosine of the angle between 2 vectors. It can easily be calculated using-", "So if 2 vectors a and b are parallel, the angle between them will be 0 and the cosine similarity will be cos(0) = 1. Similarly, if 2 vectors a and b are pointing in the opposite direction, the angle between them will be \ud835\uded1 and the cosine similarity will be cos(\ud835\uded1) = -1.In a nutshell, cosine similarity tells us about what extent 2 vectors are pointing in a similar direction. A value near 1 tells us that the vectors are pointing in a very similar direction whereas a value near -1 corresponds to pointing in opposite directions.", "Now Cosine distance between 2 points is nothing but 1 - (cosine similarity of the vectors representing them in hyperspace).So it ranges from (0 to 2), where 0 corresponds to the points being very similar, and 2 corresponds to the points being very dissimilar.Can you guess when will the cosine distance between 2 points be 1?", "To know more check out this blog.", "I\u2019ll be implementing K-means from scratch since Scikit learns K-Means does not support cosine distance.", "It\u2019s time to run the algorithm on the pre-processed data and check for the right hyperparameters.Hyperparameter tuning is a process of determining the right hyperparameters that make the model work phenomenally well for the given data.In this case, there are 2 hyperparameters-", "I\u2019ve already plotted the elbow plots for different n_component values.", "By looking at the plots, we can say that the best hyperparameters are-- n_components: 32- K (number of clusters): 50", "Finally, It\u2019s time to initialize Truncated-SVD and K-Means using the best hyperparameters and cluster the data.", "These are the results obtained after clustering the data.", "Let\u2019s check out some of the clusters:I\u2019ll be using the networkx library to create the clusters.In each cluster, the red nodes correspond to the words from Barron's 333-word list and how they are linked with each other and their synonyms.You can check out the documentation for networkx here. Also, I\u2019ll demonstrate how wonderful it is with an example in the end.", "Finally, let\u2019s visualize the data in 3d using t-SNE but first,", "t-SNE (t-Distributed Stochastic Neighbor Embedding) is a non-linear dimensionality reduction algorithm used for exploring high-dimensional data. It maps multi-dimensional data to two or more dimensions such that each embedding in the lower dimension represents the value in higher dimension. Also, these embeddings are placed in the lower dimension in such a manner that the distance between neighborhood points is preserved. So, t-SNE preserves the local structure of the data as well.I\u2019ll try to explain how it does what it does.For a given point in n-dimensional hyperspace, it calculates the distance of that point from all the other points and converts these distributions of distances to student\u2019s t-distribution. This is done for all the points such that in the end, each point has its own t-distribution of distances from all the other points.Now the points are randomly scattered in the lower dimensional space and each point is displaced by some distance such that after the displacement of all the points is done if we recalculate the t-distribution of distances of each point from the remaining points (this time this is done in the lower dimensional space), the distribution would be the same as what we obtained in n-dimensional hyperspace.There are 2 main hyperparameters in t-SNE-Perplexity: Instead of calculating the distance from all the other points, we can use only \u2018k\u2019 nearest points. This value of \u2018k\u2019 is called the perplexity value.Iterations: The number of iterations for which we want t-SNE to update the points in lower-dimensional space.Due to stochasticity, the algorithm may perform differently for different perplexity values so as a good practice, it is preferred to run t-SNE for different perplexity values and different numbers of iterations.To know more about t-SNE, check out this awesome blog, it has t-SNE very well explained with interactive visualization.", "Here is the code for representing the points in 3d space using t-SNE.", "In the above picture, each bubble represents a word in 3d space. We can see some of the points are in purple and orange are well clustered. We can also see a point in orange that is too far away meaning it is semantically different from most of the words. This representation may not be great because the clusters were calculated using cosine distance whereas t-SNE uses some different distance measure and also since t-SNE is stochastic, it requires some hyperparameter tuning, so if you want to experiment with another dataset or maybe with less number of clusters using euclidian distance hopefully it will work.", "Here\u2019s another plot using t-SNE using only two dimensions for different perplexity and iteration values. We can see t-SNE working well with perplexity 20 and 2000 iterations.", "Finally, Here is the complete graph of all the words and their synonyms (I used 4 synonyms for each word) using networkx.", "Thank you for reading the blog. I hope it was useful for some of you aspiring to do projects on NLP, unsupervised machine-learning, data processing, data visualizing.", "And if you have any doubts regarding this project, please leave a comment in the response section or in the GitHub repo of this project.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Learning something new epoch by epoch."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F75e904a63fd8&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-used-machine-learning-to-strategize-my-gre-preparation-75e904a63fd8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-used-machine-learning-to-strategize-my-gre-preparation-75e904a63fd8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-used-machine-learning-to-strategize-my-gre-preparation-75e904a63fd8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-used-machine-learning-to-strategize-my-gre-preparation-75e904a63fd8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----75e904a63fd8--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----75e904a63fd8--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@itssarthakvajpayee?source=post_page-----75e904a63fd8--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@itssarthakvajpayee?source=post_page-----75e904a63fd8--------------------------------", "anchor_text": "Sarthak Vajpayee"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd850bda2ea8c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-used-machine-learning-to-strategize-my-gre-preparation-75e904a63fd8&user=Sarthak+Vajpayee&userId=d850bda2ea8c&source=post_page-d850bda2ea8c----75e904a63fd8---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F75e904a63fd8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-used-machine-learning-to-strategize-my-gre-preparation-75e904a63fd8&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F75e904a63fd8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-used-machine-learning-to-strategize-my-gre-preparation-75e904a63fd8&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.w3schools.com/html/", "anchor_text": "this tutorial"}, {"url": "https://quizlet.com/2832581/barrons-333-high-frequency-words-flash-cards/", "anchor_text": "this website"}, {"url": "https://www.thesaurus.com/browse/", "anchor_text": "this website"}, {"url": "https://en.wikipedia.org/wiki/Regularization_(mathematics)", "anchor_text": "regularization"}, {"url": "https://www.visiondummy.com/2014/03/eigenvalues-eigenvectors/", "anchor_text": "https://www.visiondummy.com/2014/03/eigenvalues-eigenvectors/"}, {"url": "https://youtu.be/Nx0lRBaXoz4", "anchor_text": "this lecture"}, {"url": "https://medium.com/@jonathan_hui/machine-learning-singular-value-decomposition-svd-principal-component-analysis-pca-1d45e885e491", "anchor_text": "this wonderful blog"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html", "anchor_text": "Truncated SVD"}, {"url": "http://graphalchemist.github.io/Alchemy/images/features/cluster_team.png", "anchor_text": "http://graphalchemist.github.io/Alchemy/images/features/cluster_team.png"}, {"url": "https://www.youtube.com/watch?v=5I3Ei69I40s&feature=youtu.be", "anchor_text": "https://www.youtube.com/watch?v=5I3Ei69I40s&feature=youtu.be"}, {"url": "http://www.semspirit.com/artificial-intelligence/machine-learning/clustering/k-means-clustering/k-means-clustering-in-python/", "anchor_text": "http://www.semspirit.com/artificial-intelligence/machine-learning/clustering/k-means-clustering/k-means-clustering-in-python/"}, {"url": "https://www.machinelearningplus.com/nlp/cosine-similarity/", "anchor_text": "this blog"}, {"url": "https://networkx.github.io/documentation/stable/", "anchor_text": "networkx library"}, {"url": "https://networkx.github.io/documentation/stable/", "anchor_text": "here"}, {"url": "https://www.youtube.com/watch?v=wvsE8jm1GzE", "anchor_text": "https://www.youtube.com/watch?v=wvsE8jm1GzE"}, {"url": "https://en.wikipedia.org/wiki/Student%27s_t-distribution", "anchor_text": "student\u2019s t-distribution"}, {"url": "https://distill.pub/2016/misread-tsne/", "anchor_text": "this awesome blog"}, {"url": "https://github.com/SarthakV7/Clustering-Barron-s-333-word-list-using-unsupervised-machine-learning", "anchor_text": "https://github.com/SarthakV7/Clustering-Barron-s-333-word-list-using-unsupervised-machine-learning"}, {"url": "http://www.linkedin.com/in/sarthak-vajpayee", "anchor_text": "www.linkedin.com/in/sarthak-vajpayee"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----75e904a63fd8---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/python?source=post_page-----75e904a63fd8---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/nlp?source=post_page-----75e904a63fd8---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/data-science?source=post_page-----75e904a63fd8---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/data-visualization?source=post_page-----75e904a63fd8---------------data_visualization-----------------", "anchor_text": "Data Visualization"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F75e904a63fd8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-used-machine-learning-to-strategize-my-gre-preparation-75e904a63fd8&user=Sarthak+Vajpayee&userId=d850bda2ea8c&source=-----75e904a63fd8---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F75e904a63fd8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-used-machine-learning-to-strategize-my-gre-preparation-75e904a63fd8&user=Sarthak+Vajpayee&userId=d850bda2ea8c&source=-----75e904a63fd8---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F75e904a63fd8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-used-machine-learning-to-strategize-my-gre-preparation-75e904a63fd8&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----75e904a63fd8--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F75e904a63fd8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-used-machine-learning-to-strategize-my-gre-preparation-75e904a63fd8&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----75e904a63fd8---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----75e904a63fd8--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----75e904a63fd8--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----75e904a63fd8--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----75e904a63fd8--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----75e904a63fd8--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----75e904a63fd8--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----75e904a63fd8--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----75e904a63fd8--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@itssarthakvajpayee?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@itssarthakvajpayee?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Sarthak Vajpayee"}, {"url": "https://medium.com/@itssarthakvajpayee/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "136 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd850bda2ea8c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-used-machine-learning-to-strategize-my-gre-preparation-75e904a63fd8&user=Sarthak+Vajpayee&userId=d850bda2ea8c&source=post_page-d850bda2ea8c--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fb72ebbfba4df&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-used-machine-learning-to-strategize-my-gre-preparation-75e904a63fd8&newsletterV3=d850bda2ea8c&newsletterV3Id=b72ebbfba4df&user=Sarthak+Vajpayee&userId=d850bda2ea8c&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}