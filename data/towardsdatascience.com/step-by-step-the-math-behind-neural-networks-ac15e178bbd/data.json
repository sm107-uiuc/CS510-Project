{"url": "https://towardsdatascience.com/step-by-step-the-math-behind-neural-networks-ac15e178bbd", "time": 1682993666.7842371, "path": "towardsdatascience.com/step-by-step-the-math-behind-neural-networks-ac15e178bbd/", "webpage": {"metadata": {"title": "How Do You Find the Partial Derivative of a Function? | by Chi-Feng Wang | Towards Data Science", "h1": "How Do You Find the Partial Derivative of a Function?", "description": "Without understanding the math behind deep learning, we cannot appreciate all the intricacies behind the code. Here, you will learn how to calculate partial derivatives for minimizing loss functions."}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/step-by-step-the-math-behind-neural-networks-490dc1f3cfd9", "anchor_text": "Part 1", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/step-by-step-the-math-behind-neural-networks-d002440227fb", "anchor_text": "Part 3", "paragraph_index": 22}, {"url": "https://towardsdatascience.com/step-by-step-the-math-behind-neural-networks-490dc1f3cfd9", "anchor_text": "here", "paragraph_index": 23}], "all_paragraphs": ["In Part 1, we have been given a problem: to calculate the gradient of this loss function:", "Finding the gradient is essentially finding the derivative of the function. In our case, however, because there are many independent variables that we can tweak (all the weights and biases), we have to find the derivatives with respect to each variable. This is known as the partial derivative, with the symbol \u2202.", "Computing the partial derivative of simple functions is easy: simply treat every other variable in the equation as a constant and find the usual scalar derivative. Here are some scalar derivative rules as a reminder:", "Consider the partial derivative with respect to x (i.e. how y changes as x changes) in the function f(x,y) = 3x\u00b2y. Treating y as a constant, we can find partial of x:", "Similarly, we can find the partial of y:", "The gradient of the function f(x,y) = 3x\u00b2y is a horizontal vector, composed of the two partials:", "This should be pretty clear: since the partial with respect to x is the gradient of the function in the x-direction, and the partial with respect to y is the gradient of the function in the y-direction, the overall gradient is a vector composed of the two partials. This Khan Academy video offers a pretty neat graphical explanation of partial derivatives, if you want to visualize what we\u2019re doing.", "For simple functions like f(x,y) = 3x\u00b2y, that is all we need to know. However, if we want to compute partial derivatives of more complicated functions \u2014 such as those with nested expressions like max(0, w\u2219X+b) \u2014 we need to be able to utilize the multivariate chain rule, known as the single variable total-derivative chain rule in the paper.", "Let\u2019s first review the single variable chain rule. Consider the function y=f(g(x))=sin(x\u00b2). To get the derivative of this expression, we multiply the derivative of the outer expression with the derivative of the inner expression or \u2018chain the pieces together\u2019. In other words:", "It\u2019s nice to think about the single-variable chain rule as a diagram of operations that x goes through, like so:", "This concept of visualizing equations as diagrams will come in extremely handy when dealing with the multivariable chain rule. Also, if you use Tensorflow (or Keras) and TensorBoard, as you build your model and write your training code, you can see a diagram of operations similar to this.", "The multivariable chain rule, also known as the single-variable total-derivative chain rule, as called in the paper, is a variant of the scalar chain rule. Unlike what its name suggests, it can be applied to expressions with only a single variable. However, the expression should have multiple intermediate variables.", "To illustrate this point, let us consider the equation y=f(x)=x+x\u00b2. Using the scalar additional derivative rule, we can immediately calculate the derivative:", "Let\u2019s try doing it with the chain rule. First, we introduce intermediate variables: u\u2081(x) = x\u00b2 and u\u2082(x, u\u2081) = x + u\u2081. If we apply the single-variable chain rule, we get:", "Obviously, 2x\u22601+2x, so something is wrong here. Let\u2019s draw out the graph of our equation:", "The diagram in Image 12 is no longer linear, so we have to consider all the pathways in the diagram that lead to the final result. Since u\u2082 has two parameters, partial derivatives come into play. To calculate the derivative of this function, we have to calculate partial derivative with respect to x of u\u2082(x, u\u2081). Here, a change in x is reflected in u\u2082 in two ways: as an operand of the addition and as an operand of the square operator. In symbols, \u0177 = (x+\u0394x)+(x+\u0394x)\u00b2 and \u0394y = \u0177-y and where \u0177 is the y-value at a tweaked x.", "Hence, to computer the partial of u\u2082(x, u\u2081), we need to sum up all possible contributions from changes in x to the change in y. The total derivative of u\u2082(x, u\u2081) is given by:", "In simpler terms, you add up the effect of a change in x directly to u\u2082 and the effect of a change in x through u\u2081 to u\u2082. I find it easier to visualize it through a graph:", "And that\u2019s it! We got the correct answer: 1+2x. We can now sum that process up in a single rule, the multivariable chain rule (or the single-variable total-derivative chain rule):", "If we introduce an alias for x as x=u(n+1), then we can rewrite that formula into its final form, which look slightly neater:", "How does this relate back to our problem? Remember, we need to find the partial derivative of our loss function with respect to both w (the vector of all our weights) and b (the bias). However, our loss function is not that simple \u2014 there are multiple nested subexpressions (i.e. multiple intermediate variables) which will require us to use the chain rule.", "There\u2019s one more problem left. As you can see, our loss function doesn\u2019t just take in scalars as inputs, it takes in vectors as well. How can we compute the partial derivatives of vector equations, and what does a vector chain rule look like?", "Check out Part 3 to find out!", "If you haven\u2019t already, click here to read Part 1!", "If you like this article, don\u2019t forget to leave some claps! Do leave a comment below if you have any questions or suggestions :)", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Student at UC Berkeley; Machine Learning Enthusiast"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fac15e178bbd&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstep-by-step-the-math-behind-neural-networks-ac15e178bbd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstep-by-step-the-math-behind-neural-networks-ac15e178bbd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstep-by-step-the-math-behind-neural-networks-ac15e178bbd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstep-by-step-the-math-behind-neural-networks-ac15e178bbd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----ac15e178bbd--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ac15e178bbd--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@reina.wang?source=post_page-----ac15e178bbd--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@reina.wang?source=post_page-----ac15e178bbd--------------------------------", "anchor_text": "Chi-Feng Wang"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9ddaaec52a09&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstep-by-step-the-math-behind-neural-networks-ac15e178bbd&user=Chi-Feng+Wang&userId=9ddaaec52a09&source=post_page-9ddaaec52a09----ac15e178bbd---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fac15e178bbd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstep-by-step-the-math-behind-neural-networks-ac15e178bbd&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fac15e178bbd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstep-by-step-the-math-behind-neural-networks-ac15e178bbd&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://pixabay.com/en/geometry-mathematics-volume-surface-1044090/", "anchor_text": "Source"}, {"url": "https://towardsdatascience.com/step-by-step-the-math-behind-neural-networks-490dc1f3cfd9", "anchor_text": "Part 1"}, {"url": "http://explained.ai/matrix-calculus/index.html", "anchor_text": "Source"}, {"url": "http://explained.ai/matrix-calculus/index.html", "anchor_text": "Source"}, {"url": "http://explained.ai/matrix-calculus/index.html", "anchor_text": "Source"}, {"url": "http://explained.ai/matrix-calculus/index.html", "anchor_text": "Source"}, {"url": "http://explained.ai/matrix-calculus/index.html", "anchor_text": "Source"}, {"url": "http://explained.ai/matrix-calculus/index.html", "anchor_text": "Source"}, {"url": "http://explained.ai/matrix-calculus/index.html", "anchor_text": "Source"}, {"url": "http://explained.ai/matrix-calculus/index.html", "anchor_text": "Source"}, {"url": "https://towardsdatascience.com/step-by-step-the-math-behind-neural-networks-d002440227fb", "anchor_text": "Part 3"}, {"url": "https://towardsdatascience.com/step-by-step-the-math-behind-neural-networks-490dc1f3cfd9", "anchor_text": "here"}, {"url": "https://towardsdatascience.com/step-by-step-the-math-behind-neural-networks-d002440227fb", "anchor_text": "Part 3: Vector Calculus"}, {"url": "https://towardsdatascience.com/calculating-gradient-descent-manually-6d9bee09aa0b", "anchor_text": "Part 4: Putting It All Together"}, {"url": "https://arxiv.org/abs/1802.01528", "anchor_text": "here"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----ac15e178bbd---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----ac15e178bbd---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/calculus?source=post_page-----ac15e178bbd---------------calculus-----------------", "anchor_text": "Calculus"}, {"url": "https://medium.com/tag/derivatives?source=post_page-----ac15e178bbd---------------derivatives-----------------", "anchor_text": "Derivatives"}, {"url": "https://medium.com/tag/loss-function?source=post_page-----ac15e178bbd---------------loss_function-----------------", "anchor_text": "Loss Function"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fac15e178bbd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstep-by-step-the-math-behind-neural-networks-ac15e178bbd&user=Chi-Feng+Wang&userId=9ddaaec52a09&source=-----ac15e178bbd---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fac15e178bbd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstep-by-step-the-math-behind-neural-networks-ac15e178bbd&user=Chi-Feng+Wang&userId=9ddaaec52a09&source=-----ac15e178bbd---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fac15e178bbd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstep-by-step-the-math-behind-neural-networks-ac15e178bbd&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ac15e178bbd--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fac15e178bbd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstep-by-step-the-math-behind-neural-networks-ac15e178bbd&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----ac15e178bbd---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----ac15e178bbd--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----ac15e178bbd--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----ac15e178bbd--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----ac15e178bbd--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----ac15e178bbd--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----ac15e178bbd--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----ac15e178bbd--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----ac15e178bbd--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@reina.wang?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@reina.wang?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Chi-Feng Wang"}, {"url": "https://medium.com/@reina.wang/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.5K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9ddaaec52a09&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstep-by-step-the-math-behind-neural-networks-ac15e178bbd&user=Chi-Feng+Wang&userId=9ddaaec52a09&source=post_page-9ddaaec52a09--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F827df2c647b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstep-by-step-the-math-behind-neural-networks-ac15e178bbd&newsletterV3=9ddaaec52a09&newsletterV3Id=827df2c647b&user=Chi-Feng+Wang&userId=9ddaaec52a09&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}