{"url": "https://towardsdatascience.com/transformers-141e32e69591", "time": 1682995356.361332, "path": "towardsdatascience.com/transformers-141e32e69591/", "webpage": {"metadata": {"title": "How Transformers Work. Transformers are a type of neural\u2026 | by Giuliano Giacaglia | Towards Data Science", "h1": "How Transformers Work", "description": "If you liked this post and want to learn how machine learning algorithms work, how did they arise, and where are they going, I recommend the following: Transformers are a type of neural network\u2026"}, "outgoing_paragraph_urls": [{"url": "https://blog.openai.com/better-language-models/", "anchor_text": "models", "paragraph_index": 1}, {"url": "https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/", "anchor_text": "AlphaStar", "paragraph_index": 1}, {"url": "https://arxiv.org/abs/1211.3711", "anchor_text": "sequence transduction", "paragraph_index": 2}, {"url": "http://ai.dinfo.unifi.it/paolo//ps/tnn-94-gradient.pdf", "anchor_text": "them", "paragraph_index": 16}, {"url": "https://www.youtube.com/watch?v=rBCqOTEfxvg&t=8m21s", "anchor_text": "log(N)", "paragraph_index": 37}, {"url": "https://www.youtube.com/watch?v=rBCqOTEfxvg&t=8m21s", "anchor_text": "N", "paragraph_index": 37}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "blog post", "paragraph_index": 44}, {"url": "https://medium.com/deeper-learning/glossary-of-deep-learning-word-embedding-f90c3cec34ca", "anchor_text": "embedding algorithm", "paragraph_index": 45}], "all_paragraphs": ["If you liked this post and want to learn how machine learning algorithms work, how did they arise, and where are they going, I recommend the following:", "Transformers are a type of neural network architecture that have been gaining popularity. Transformers were recently used by OpenAI in their language models, and also used recently by DeepMind for AlphaStar \u2014 their program to defeat a top professional Starcraft player.", "Transformers were developed to solve the problem of sequence transduction, or neural machine translation. That means any task that transforms an input sequence to an output sequence. This includes speech recognition, text-to-speech transformation, etc..", "For models to perform sequence transduction, it is necessary to have some sort of memory. For example let\u2019s say that we are translating the following sentence to another language (French):", "\u201cThe Transformers\u201d are a Japanese [[hardcore punk]] band. The band was formed in 1968, during the height of Japanese music history\u201d", "In this example, the word \u201cthe band\u201d in the second sentence refers to the band \u201cThe Transformers\u201d introduced in the first sentence. When you read about the band in the second sentence, you know that it is referencing to the \u201cThe Transformers\u201d band. That may be important for translation. There are many examples, where words in some sentences refer to words in previous sentences.", "For translating sentences like that, a model needs to figure out these sort of dependencies and connections. Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs) have been used to deal with this problem because of their properties. Let\u2019s go over these two architectures and their drawbacks.", "Recurrent Neural Networks have loops in them, allowing information to persist.", "In the figure above, we see part of the neural network, A, processing some input x_t and outputs h_t. A loop allows information to be passed from one step to the next.", "The loops can be thought in a different way. A Recurrent Neural Network can be thought of as multiple copies of the same network, A, each network passing a message to a successor. Consider what happens if we unroll the loop:", "This chain-like nature shows that recurrent neural networks are clearly related to sequences and lists. In that way, if we want to translate some text, we can set each input as the word in that text. The Recurrent Neural Network passes the information of the previous words to the next network that can use and process that information.", "The following picture shows how usually a sequence to sequence model works using Recurrent Neural Networks. Each word is processed separately, and the resulting sentence is generated by passing a hidden state to the decoding stage that, then, generates the output.", "Consider a language model that is trying to predict the next word based on the previous ones. If we are trying to predict the next word of the sentence \u201cthe clouds in the sky\u201d, we don\u2019t need further context. It\u2019s pretty obvious that the next word is going to be sky.", "In this case where the difference between the relevant information and the place that is needed is small, RNNs can learn to use past information and figure out what is the next word for this sentence.", "But there are cases where we need more context. For example, let\u2019s say that you are trying to predict the last word of the text: \u201cI grew up in France\u2026 I speak fluent \u2026\u201d. Recent information suggests that the next word is probably a language, but if we want to narrow down which language, we need context of France, that is further back in the text.", "RNNs become very ineffective when the gap between the relevant information and the point where it is needed become very large. That is due to the fact that the information is passed at each step and the longer the chain is, the more probable the information is lost along the chain.", "In theory, RNNs could learn this long-term dependencies. In practice, they don\u2019t seem to learn them. LSTM, a special type of RNN, tries to solve this kind of problem.", "When arranging one\u2019s calendar for the day, we prioritize our appointments. If there is anything important, we can cancel some of the meetings and accommodate what is important.", "RNNs don\u2019t do that. Whenever it adds new information, it transforms existing information completely by applying a function. The entire information is modified, and there is no consideration of what is important and what is not.", "LSTMs make small modifications to the information by multiplications and additions. With LSTMs, the information flows through a mechanism known as cell states. In this way, LSTMs can selectively remember or forget things that are important and not so important.", "Internally, a LSTM looks like the following:", "Each cell takes as inputs x_t (a word in the case of a sentence to sentence translation), the previous cell state and the output of the previous cell. It manipulates these inputs and based on them, it generates a new cell state, and an output. I won\u2019t go into detail on the mechanics of each cell. If you want to understand how each cell works, I recommend Christopher\u2019s blog post:", "With a cell state, the information in a sentence that is important for translating a word may be passed from one word to another, when translating.", "The same problem that happens to RNNs generally, happen with LSTMs, i.e. when sentences are too long LSTMs still don\u2019t do too well. The reason for that is that the probability of keeping the context from a word that is far away from the current word being processed decreases exponentially with the distance from it.", "That means that when sentences are long, the model often forgets the content of distant positions in the sequence. Another problem with RNNs, and LSTMs, is that it\u2019s hard to parallelize the work for processing sentences, since you are have to process word by word. Not only that but there is no model of long and short range dependencies. To summarize, LSTMs and RNNs present 3 problems:", "To solve some of these problems, researchers created a technique for paying attention to specific words.", "When translating a sentence, I pay special attention to the word I\u2019m presently translating. When I\u2019m transcribing an audio recording, I listen carefully to the segment I\u2019m actively writing down. And if you ask me to describe the room I\u2019m sitting in, I\u2019ll glance around at the objects I\u2019m describing as I do so.", "Neural networks can achieve this same behavior using attention, focusing on part of a subset of the information they are given. For example, an RNN can attend over the output of another RNN. At every time step, it focuses on different positions in the other RNN.", "To solve these problems, Attention is a technique that is used in a neural network. For RNNs, instead of only encoding the whole sentence in a hidden state, each word has a corresponding hidden state that is passed all the way to the decoding stage. Then, the hidden states are used at each step of the RNN to decode. The following gif shows how that happens.", "The idea behind it is that there might be relevant information in every word in a sentence. So in order for the decoding to be precise, it needs to take into account every word of the input, using attention.", "For attention to be brought to RNNs in sequence transduction, we divide the encoding and decoding into 2 main steps. One step is represented in green and the other in purple. The green step is called the encoding stage and the purple step is the decoding stage.", "The step in green in charge of creating the hidden states from the input. Instead of passing only one hidden state to the decoders as we did before using attention, we pass all the hidden states generated by every \u201cword\u201d of the sentence to the decoding stage. Each hidden state is used in the decoding stage, to figure out where the network should pay attention to.", "For example, when translating the sentence \u201cJe suis \u00e9tudiant\u201d to English, requires that the decoding step looks at different words when translating it.", "Or for example, when you translate the sentence \u201cL\u2019accord sur la zone \u00e9conomique europ\u00e9enne a \u00e9t\u00e9 sign\u00e9 en ao\u00fbt 1992.\u201d from French to English, and how much attention it is paid to each input.", "But some of the problems that we discussed, still are not solved with RNNs using attention. For example, processing inputs (words) in parallel is not possible. For a large corpus of text, this increases the time spent translating the text.", "Convolutional Neural Networks help solve these problems. With them we can", "Some of the most popular neural networks for sequence transduction, Wavenet and Bytenet, are Convolutional Neural Networks.", "The reason why Convolutional Neural Networks can work in parallel, is that each word on the input can be processed at the same time and does not necessarily depend on the previous words to be translated. Not only that, but the \u201cdistance\u201d between the output word and any input for a CNN is in the order of log(N) \u2014 that is the size of the height of the tree generated from the output to the input (you can see it on the GIF above. That is much better than the distance of the output of a RNN and an input, which is on the order of N.", "The problem is that Convolutional Neural Networks do not necessarily help with the problem of figuring out the problem of dependencies when translating sentences. That\u2019s why Transformers were created, they are a combination of both CNNs with attention.", "To solve the problem of parallelization, Transformers try to solve the problem by using Convolutional Neural Networks together with attention models. Attention boosts the speed of how fast the model can translate from one sequence to another.", "Let\u2019s take a look at how Transformer works. Transformer is a model that uses attention to boost the speed. More specifically, it uses self-attention.", "Internally, the Transformer has a similar kind of architecture as the previous models above. But the Transformer consists of six encoders and six decoders.", "Each encoder is very similar to each other. All encoders have the same architecture. Decoders share the same property, i.e. they are also very similar to each other. Each encoder consists of two layers: Self-attention and a feed Forward Neural Network.", "The encoder\u2019s inputs first flow through a self-attention layer. It helps the encoder look at other words in the input sentence as it encodes a specific word. The decoder has both those layers, but between them is an attention layer that helps the decoder focus on relevant parts of the input sentence.", "Note: This section comes from Jay Allamar blog post", "Let\u2019s start to look at the various vectors/tensors and how they flow between these components to turn the input of a trained model into an output. As is the case in NLP applications in general, we begin by turning each input word into a vector using an embedding algorithm.", "Each word is embedded into a vector of size 512. We\u2019ll represent those vectors with these simple boxes.", "The embedding only happens in the bottom-most encoder. The abstraction that is common to all the encoders is that they receive a list of vectors each of the size 512.", "In the bottom encoder that would be the word embeddings, but in other encoders, it would be the output of the encoder that\u2019s directly below. After embedding the words in our input sequence, each of them flows through each of the two layers of the encoder.", "Here we begin to see one key property of the Transformer, which is that the word in each position flows through its own path in the encoder. There are dependencies between these paths in the self-attention layer. The feed-forward layer does not have those dependencies, however, and thus the various paths can be executed in parallel while flowing through the feed-forward layer.", "Next, we\u2019ll switch up the example to a shorter sentence and we\u2019ll look at what happens in each sub-layer of the encoder.", "Let\u2019s first look at how to calculate self-attention using vectors, then proceed to look at how it\u2019s actually implemented \u2014 using matrices.", "The first step in calculating self-attention is to create three vectors from each of the encoder\u2019s input vectors (in this case, the embedding of each word). So for each word, we create a Query vector, a Key vector, and a Value vector. These vectors are created by multiplying the embedding by three matrices that we trained during the training process.", "Notice that these new vectors are smaller in dimension than the embedding vector. Their dimensionality is 64, while the embedding and encoder input/output vectors have dimensionality of 512. They don\u2019t HAVE to be smaller, this is an architecture choice to make the computation of multiheaded attention (mostly) constant.", "Multiplying x1 by the WQ weight matrix produces q1, the \u201cquery\u201d vector associated with that word. We end up creating a \u201cquery\u201d, a \u201ckey\u201d, and a \u201cvalue\u201d projection of each word in the input sentence.", "They\u2019re abstractions that are useful for calculating and thinking about attention. Once you proceed with reading how attention is calculated below, you\u2019ll know pretty much all you need to know about the role each of these vectors plays.", "The second step in calculating self-attention is to calculate a score. Say we\u2019re calculating the self-attention for the first word in this example, \u201cThinking\u201d. We need to score each word of the input sentence against this word. The score determines how much focus to place on other parts of the input sentence as we encode a word at a certain position.", "The score is calculated by taking the dot product of the query vector with the key vector of the respective word we\u2019re scoring. So if we\u2019re processing the self-attention for the word in position #1, the first score would be the dot product of q1 and k1. The second score would be the dot product of q1 and k2.", "The third and forth steps are to divide the scores by 8 (the square root of the dimension of the key vectors used in the paper \u2014 64. This leads to having more stable gradients. There could be other possible values here, but this is the default), then pass the result through a softmax operation. Softmax normalizes the scores so they\u2019re all positive and add up to 1.", "This softmax score determines how much how much each word will be expressed at this position. Clearly the word at this position will have the highest softmax score, but sometimes it\u2019s useful to attend to another word that is relevant to the current word.", "The fifth step is to multiply each value vector by the softmax score (in preparation to sum them up). The intuition here is to keep intact the values of the word(s) we want to focus on, and drown-out irrelevant words (by multiplying them by tiny numbers like 0.001, for example).", "The sixth step is to sum up the weighted value vectors. This produces the output of the self-attention layer at this position (for the first word).", "That concludes the self-attention calculation. The resulting vector is one we can send along to the feed-forward neural network. In the actual implementation, however, this calculation is done in matrix form for faster processing. So let\u2019s look at that now that we\u2019ve seen the intuition of the calculation on the word level.", "Transformers basically work like that. There are a few other details that make them work better. For example, instead of only paying attention to each other in one dimension, Transformers use the concept of Multihead attention.", "The idea behind it is that whenever you are translating a word, you may pay different attention to each word based on the type of question that you are asking. The images below show what that means. For example, whenever you are translating \u201ckicked\u201d in the sentence \u201cI kicked the ball\u201d, you may ask \u201cWho kicked\u201d. Depending on the answer, the translation of the word to another language can change. Or ask other questions, like \u201cDid what?\u201d, etc\u2026", "Another important step on the Transformer is to add positional encoding when encoding each word. Encoding the position of each word is relevant, since the position of each word is relevant to the translation.", "I gave an overview of how Transformers work and why this is the technique used for sequence transduction. If you want to understand in depth how the model works and all its nuances, I recommend the following posts, articles and videos that I used as a base for summarizing the technique", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F141e32e69591&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-141e32e69591&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-141e32e69591&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-141e32e69591&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-141e32e69591&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----141e32e69591--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----141e32e69591--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@giacaglia?source=post_page-----141e32e69591--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@giacaglia?source=post_page-----141e32e69591--------------------------------", "anchor_text": "Giuliano Giacaglia"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd0de4109e381&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-141e32e69591&user=Giuliano+Giacaglia&userId=d0de4109e381&source=post_page-d0de4109e381----141e32e69591---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F141e32e69591&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-141e32e69591&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F141e32e69591&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-141e32e69591&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.holloway.com/g/making-things-think", "anchor_text": "Making Things Think: How AI and Deep Learning Power the Products We Use - HollowayIt is the obvious which is so difficult to see most of the time. People say 'It's as plain as the nose on your face.'\u2026www.holloway.com"}, {"url": "https://blog.openai.com/better-language-models/", "anchor_text": "models"}, {"url": "https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/", "anchor_text": "AlphaStar"}, {"url": "https://arxiv.org/abs/1211.3711", "anchor_text": "sequence transduction"}, {"url": "https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/", "anchor_text": "3"}, {"url": "https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/", "anchor_text": "3"}, {"url": "http://colah.github.io/posts/2015-08-Understanding-LSTMs/", "anchor_text": "6"}, {"url": "http://colah.github.io/posts/2015-08-Understanding-LSTMs/", "anchor_text": "6"}, {"url": "http://ai.dinfo.unifi.it/paolo//ps/tnn-94-gradient.pdf", "anchor_text": "them"}, {"url": "http://colah.github.io/posts/2015-08-Understanding-LSTMs/", "anchor_text": "6"}, {"url": "http://colah.github.io/posts/2015-08-Understanding-LSTMs/", "anchor_text": "Understanding LSTM Networks -- colah's blogThese loops make recurrent neural networks seem kind of mysterious. However, if you think a bit more, it turns out that\u2026colah.github.io"}, {"url": "http://3", "anchor_text": "3"}, {"url": "https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/", "anchor_text": "3"}, {"url": "https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/", "anchor_text": "3"}, {"url": "https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/", "anchor_text": "3"}, {"url": "https://deepmind.com/blog/wavenet-generative-model-raw-audio/", "anchor_text": "10"}, {"url": "https://www.youtube.com/watch?v=rBCqOTEfxvg&t=8m21s", "anchor_text": "log(N)"}, {"url": "https://www.youtube.com/watch?v=rBCqOTEfxvg&t=8m21s", "anchor_text": "N"}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "4"}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "4"}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "4"}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "4"}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "blog post"}, {"url": "https://medium.com/deeper-learning/glossary-of-deep-learning-word-embedding-f90c3cec34ca", "anchor_text": "embedding algorithm"}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "4"}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "4"}, {"url": "http://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture14-transformers.pdf", "anchor_text": "8"}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "4"}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "4"}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "4"}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "4"}, {"url": "http://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture14-transformers.pdf", "anchor_text": "8"}, {"url": "http://karpathy.github.io/2015/05/21/rnn-effectiveness/", "anchor_text": "The Unreasonable Effectiveness of Recurrent Neural Networks"}, {"url": "http://colah.github.io/posts/2015-08-Understanding-LSTMs/", "anchor_text": "Understanding LSTM Networks"}, {"url": "https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/", "anchor_text": "Visualizing A Neural Machine Translation Model"}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "The Illustrated Transformer"}, {"url": "https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/#.XIWlzBNKjOR", "anchor_text": "The Transformer \u2014 Attention is all you need"}, {"url": "http://nlp.seas.harvard.edu/2018/04/03/attention.html", "anchor_text": "The Annotated Transformer"}, {"url": "https://www.youtube.com/watch?v=rBCqOTEfxvg", "anchor_text": "Attention is all you need attentional neural network models"}, {"url": "http://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture14-transformers.pdf", "anchor_text": "Self-Attention For Generative Models"}, {"url": "https://towardsdatascience.com/openai-gpt-2-understanding-language-generation-through-visualization-8252f683b2f8", "anchor_text": "OpenAI GPT-2: Understanding Language Generation through Visualization"}, {"url": "https://deepmind.com/blog/wavenet-generative-model-raw-audio/", "anchor_text": "WaveNet: A Generative Model for Raw Audio"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----141e32e69591---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/transformers?source=post_page-----141e32e69591---------------transformers-----------------", "anchor_text": "Transformers"}, {"url": "https://medium.com/tag/data-science?source=post_page-----141e32e69591---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/nlp?source=post_page-----141e32e69591---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/open-ai?source=post_page-----141e32e69591---------------open_ai-----------------", "anchor_text": "Open Ai"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F141e32e69591&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-141e32e69591&user=Giuliano+Giacaglia&userId=d0de4109e381&source=-----141e32e69591---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F141e32e69591&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-141e32e69591&user=Giuliano+Giacaglia&userId=d0de4109e381&source=-----141e32e69591---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F141e32e69591&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-141e32e69591&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----141e32e69591--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F141e32e69591&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-141e32e69591&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----141e32e69591---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----141e32e69591--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----141e32e69591--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----141e32e69591--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----141e32e69591--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----141e32e69591--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----141e32e69591--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----141e32e69591--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----141e32e69591--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@giacaglia?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@giacaglia?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Giuliano Giacaglia"}, {"url": "https://medium.com/@giacaglia/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.3K Followers"}, {"url": "https://holloway.com/mtt", "anchor_text": "https://holloway.com/mtt"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd0de4109e381&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-141e32e69591&user=Giuliano+Giacaglia&userId=d0de4109e381&source=post_page-d0de4109e381--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fb79e0b1bae8c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-141e32e69591&newsletterV3=d0de4109e381&newsletterV3Id=b79e0b1bae8c&user=Giuliano+Giacaglia&userId=d0de4109e381&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}