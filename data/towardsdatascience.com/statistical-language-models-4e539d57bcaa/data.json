{"url": "https://towardsdatascience.com/statistical-language-models-4e539d57bcaa", "time": 1683015956.116162, "path": "towardsdatascience.com/statistical-language-models-4e539d57bcaa/", "webpage": {"metadata": {"title": "Statistical Language Models. From simple to ++, with use cases\u2026 | by Arun Jagota | Towards Data Science", "h1": "Statistical Language Models", "description": "Introduces models from simple to elaborate, with examples. Independent, first-order Markov, higher-order Markov, Hidden Markov Model, Conditional Markov Model, Conditional Random Fields."}, "outgoing_paragraph_urls": [], "all_paragraphs": ["In NLP, a language model is a probability distribution over strings on an alphabet. In formal language theory, a language is a set of strings on an alphabet. The NLP version is a soft variant of the one in formal language theory.", "The NLP version is better suited to modeling natural languages such as English or French. No hard rules dictate exactly which strings are in the language and which not. Rather we have observations to work with. People write. People talk. Their utterances characterize the language.", "Importantly, the NLP statistical version is good for learning languages over strings from examples. Consider learning a model to recognize product names. The training set may contain iPhone 4 and iPhone 5 but not iPhone 12. It should recognize iPhone 12 as a product name.", "In this post, we cover statistical language models from simple to elaborate. The covered models include: Independent model, first-order Markov model, Kth-order Markov model, Hidden Markov Model, Conditional Markov model, and Conditional Random Fields. Each is illustrated with realistic examples and use cases.", "We\u2019ve defined a language as a probability distribution over strings. In many use cases, what we really want is the probability of the next symbol given the current string. It turns out these two are equivalent, as noted in [1].", "What can we do with a language model? Quite a lot.", "Suggest auto-completes. Smartphones offer auto-complete suggestions as we type. Web search engines offer auto-complete suggestions as we start typing a query. Under-the-hood these are powered by language models.", "Recognize handwriting. Imagine pointing your smartphone at your handwritten notes and asking them to be recognized. Perhaps to be digitized and searchable. Or at least to make them legible!", "Handwriting recognition is challenging. Even people often get it wrong. Sometimes even their own writing!", "Consider trying to recognize the words in a poorly-written text. A pure image processing approach can make a lot of errors. Adding a language model can cut these down a lot. The language model provides a useful context.", "Example 1: Let\u2019s glimpse this in a simple example. Imagine the OCR thought the next word was databasc. The e was misrecognized as a c. They do look similar.", "Now let\u2019s add a language model trained on English words. Specifically to assign high probabilities to strings that look like English words and low probabilities to those that don\u2019t. bath would get a high probability, but not axbs.", "This language model will know that database is much more likely than databasc. So adding it will help detect and correct the OCR\u2019s error.", "By adding a multi-word language model, we can improve the error-detection and correction accuracy further. This model\u2019s alphabet is the lexicon of words. Its high-probability strings model high-probability word sequences. The alphabet becomes way larger. Every distinct word is in the alphabet. So care needs to be exercised to reap its benefits while avoiding the cost (the model getting overly complex). More on this later.", "A multi-word language model can also help fill in missing words in the written text.", "Detect and correct spelling errors. We\u2019ll reinterpret the same example databasc. The last character, c, is now a spelling error.", "Recognize speech. Similar reasoning holds here. Except that the modality is different. The underlying language being expressed is the same. This is not to say that accurate handwriting or speech recognition is easy. Just that adding a language model helps.", "Recognize multi-token named entities. Multi-token named entities often have a language structure. As an example, in a person\u2019s name, the first name word(s) typically appear before the last name words. Sandwiched between the two might be middle name word(s). As another example, in a US street address, the street number typically appears before the street name. As we will see later, such entities are often recognized via latent language models such as Hidden Markov Models.", "Now that we have seen some use cases, let\u2019s dive into", "Independent. This is the simplest approach. It assumes that all characters in the string are generated independently.", "Here Q is a probability distribution over the alphabet.", "This approach is often effective on long strings on a large alphabet. Such as documents with many words. The text is seen as a sequence of words. The alphabet is the lexicon of words.", "In this setting, more elaborate approaches get complex very quickly so they need to work convincingly and significantly better to justify their added complexity.", "This approach would be effective in detecting the document\u2019s language. For each language, we would train a separate Q, a distribution over the words seen in the language\u2019s training set. For a new document W expressed as the sequence of words W1 W2 \u2026 Wn, we would calculate PL(W) = QL(W1]*QL[W2]*\u2026QL[Wn] for every language L we have modeled. We would deem L* = argmax_L PL(W) as the language of W.", "Python Code I1: Not tested, or even run. Plus, needs a runner.", "Exercise I1: Evolve this code snippet into a language recognizer. Say English vs French.", "First-order Markov Model. Here, a symbol is allowed to depend on the previous one.", "A first-order Markov model is described by a state-transition matrix P. pij is the probability of going from state i to state j. Denoting xt as i and xt+1 as j, Q(xt+1,xt) equals pij.", "This Markov model may also be expressed as a directed graph. There is an arc i \u2192 j if the value i can be followed by a value j. This arc has a probability p(j|i) on it. The graph representation merely makes explicit which state transition probabilities are 0.", "Example 1M1: A first-order Markov model is effective in modeling short phrases. Say we want to discover salient short phrases (of 2-to-4 words each) from a large corpus of English documents. A first-order Markov model strikes a good balance between being rich enough to be able to do a reasonable job on it, without becoming too complex.", "Consider the text \u201cdata mining is a phrase. data mining is a method of extracting meaningful information from a large data set\u201d.", "From this text, we will build the Markov graph. This graph\u2019s nodes are distinct words that appear in the text. There is an arc from node u to node v if word u is followed by word v at least once in the text. A few arcs are shown below.", "So the first-order Markov model on its own, while somewhat useful, also has glaring false positives. Adding grammatical information to this approach, specifically part-of-speech tags of words significantly improves its quality. That is, (mostly) reduces the false positives without (hardly) missing on the true ones. The intuition is that salient phrases are comprised of words favoring certain parts of speech, especially nouns. Consider (data, mining). Both words are nouns. Consider (is, a). \u2018is\u2019 is a verb, \u2018a\u2019 an article.", "We won\u2019t go into detail on how to combine the two approaches. The main point is that the Markov model is still useful.", "Python Code 1M1: Not tested, or even run. Plus, needs a runner.", "Kth-order Markov Model. In this model, a character is allowed to depend on the previous K characters.", "With K set to 2 or 3, this approach can discover better salient phrases than a first-order Markov model without incurring a huge increase in complexity. What do we mean by \u201cbetter\u201d? Consider a phrase of length 3. Call it x1, x2, x3. The first-order Markov model loses information as it assumes x3 is independent of x1 given x2. The second-order model doesn\u2019t. P(x1)*P(x2|x1)*P(x3|x2,x1) equals P(x1,x2,x3).", "As before, the need for combining the statistical approach with the grammatical approach remains.", "Example KM1: We\u2019d like to build a second-order Markov model on three-word product names. Why second-order? Consider these made-up examples: 4k tv samsung, 3k tv sony, \u2026 Imagine that sony does not offer a 4k option. If we used a first-order Markov model we would lose the interaction between the first word (4k or 3k or \u2026) and the brand-name. The second-order model would capture this interaction. In other words, the product name would be influenced not only by the second word (tv in our example) but also by the first one (4k or 3k or \u2026).", "Here, strings are probabilistically generated from a model with latent variables, i.e., hidden states.", "Notationally, let X = x1, \u2026, xn denote a string of length n and S = s1, \u2026, sn denote a string of states associated with X. Symbol xi is generated from state si.", "X is called the observation sequence; S the (hidden) state sequence from which X was generated.", "Structurally the HMM is a first-order Markov model over the set of states. Some states are connected to other states by arcs. In addition, each state has a probability distribution over the alphabet of observables.", "This model generates a pair (X, S) as follows. First, it generates s1. Then it emits x1 from s1 with a certain probability. Next, it moves to state s2 with a certain probability. This probability depends only on where it came from, i.e. s1. What was emitted, i.e. x1, is immaterial. Next, it generates x2 from s2 with a certain probability. And so it goes.", "In summary, the HMM generator alternates between generating states and generating observables from those states. The state transition probabilities depend only on the previous state. The emission transition probabilities depend only on the current state, i.e. the state from which the observable is emitted.", "Example HMM1: Say we want to generate sentences in English that resemble real ones. Consider specifically sentences having one of two structures", "An example sentence of the first structure is The boy is fast. An example sentence of the second structure is She sings.", "A realistic sentence generator would accommodate a lot more structures. We chose two because the maximum insight comes in going from one to two.", "What does an HMM that models these two structures look like? First, it helps to add a begin state and an end state. The HMM always starts from the begin state and stops at the end state. The begin and end states are called silent. They don\u2019t emit any token.", "Okay, now to the structure, i.e. the arcs connecting the states.", "Note that some states are shown twice. This is just due to the presentation constraint. Take an example. Since the HMM has arcs begin \u2192 article and begin \u2192 pronoun what this really means is that from the begin state the HMM can move to the article state with a certain probability and to the pronoun state with another probability. The two probabilities sum to 1. From the state begin the HMM has to move somewhere.", "Similarly, from the state verb we can either move to the state adverb or to the state end. Note that the HMM does not track how we arrived at the state verb, which means the state sequence begin \u21d2 article \u21d2 noun \u21d2 verb \u21d2 end is also possible, even though we didn\u2019t ask for it. This type of generalization is good if we want the HMM to be able to generate sentences such as The boy ate and bad if not.", "Training: We can train the emission parameters of this model easily. We can take advantage of the fact that our states are named entities, for whom training sets are readily available. It\u2019s easy to collect words that form articles, those that form nouns, etc.", "Next, we look into training the state transition probabilities. For example, we need to estimate the probability of going from the begin state to the article state. This is less than 1 as from the state begin we can also go to the state pronoun instead.", "For training the transitions, we assume we have access to sentences whose words have POS-tags attached to them. A rich training set of this type is easy to assemble. It is easy to get lots of sentences. It is also easy to get the POS-tag sequences of these sentences by running a suitable POS-tagger on each.", "The transition probabilities of the HMM are easy to train from such a training set. In fact, we only need the POS-tag sequence for each sentence. Let\u2019s illustrate this. The probability on the arc begin \u2192 article is estimated as the number of POS-tag sequences whose first token is article divided by the number of POS-tag sequences whose first token is either article or pronoun.", "Let\u2019s see a few. The first one is below.", "We start from the state begin, walk to article, emit The from it, walk to noun, emit boy from it, and so on. t", "We start from begin, walk to pronoun, emit She from it, move to verb, emit Sing from it, and finally walk to end and stay put there.", "Versus Independent: Imagine, by contrast, the sentences generated by the independent model. The words would be spewed out based on their probabilities with no regard for the desired structure.", "Position-specific Independent Model aka chain HMM", "In some use cases, the generated symbols are significantly influenced by their positions in the string. As one example, consider product names such as iPhone 11, canon camera, and sony tv. There is clearly some sequential structure. In these examples, the brand name (canon, sony) precedes the product type (camera, tv).", "The position-specific generalization of the independent model is suited for such modeling.", "Here Qi is a position-specific probability distribution over the alphabet. So with n positions we have n distributions Q1, \u2026, Qn.", "A position-specific independent model may be viewed as an HMM whose states 1, 2, \u2026, n representing token positions 1 through n respectively. Such an HMM\u2019s graph is a single directed path 1 \u2192 2 \u2192 3 \u2192 \u2026 \u2192 n. Consequently, the transition probabilities on all the arcs are 1. The HMM\u2019s parameters are the position-specific emission probabilities. We will call such a model a chain HMM.", "A chain HMM is hardly an HMM as there is nothing Markovian about it. That said, calling it out as an HMM is useful. It surfaces paths to enhancing the model as needed.", "An example of such an enhanced version is commonly used in biomolecular sequence analysis. It goes by the name profile HMM [3]. A profile HMM is a chain HMM with a few judiciously chosen states and transitions added to detour off the chain at certain positions. Detour paths merge back to the chain at certain other positions. Detour paths model position-specific mutations. In biomolecular sequences, such mutations often happen. A model that captures them recognizes membership of biomolecular sequences in the modeled family more accurately.", "A different enhancement path from a position-specific chain HMM is into a so-called Conditional Markov Model (CMM). We will cover CMMs later in this post. We will also illustrate, in the example below, what benefits we get by moving from a chain HMM to a CMM.", "Example CHMM-Park-Names: Say we want to model national or state park names in the US. Such as Yellowstone National Park, Yosemite National Park, Castle Rock State Park, \u2026 A chain HMM is a good way to go.", "This HMM will be trained off a list of national plus state park names. We will set n to the maximum number of words in an entry in this list. The position-specific emission probabilities of the chain HMM are easy to estimate. The tokenized version of a park name reveals the position of each word in it. For example, in yosemite national park, yosemite is the first word, national the second, and park the third. So from a token in a park name we know which state\u2019s emission probability to train.", "The model above is attractive for its simplicity and flexibility. Just train it on a list of park names. As the list gets richer, the model will improve on its own.", "That said, it seems a bit unnatural. A natural model for the type of park names we have in mind is", "This just means that a park name has one or two words followed by national or state followed by park.", "The model below is closer to the natural model we seek.", "Example HMM-Park-Names: Here we will use three states: prefix_word, regional_word, and park. The state regional_word will emit state and national (for now), with equal probability (for now). The state prefix_word will emit a word in a park name that appears before regional_word. The state park will (for now) emit the word park with probability 1.", "A training set of state sequences is easy to construct? We just need a few:", "Why not just use a regular expression then? The HMM is more accurate. Consider the phrase \u201cthe national park\u201d in some text. This phrase is not an actual national park. The HMM can in principle model this by assigning a very low probability of emitting the from the state prefix_word.", "In fact, this takes us to the topic of how to train the emission probabilities of prefix_word? (The emission probabilities of the other states have already been settled, for now.) Here is a simple approach. Take a list of national and state park names. Strip off the last two words in each park name. The words that remain are prefix words.", "The state park could be extended to emit beach, forest, monument, etc. (In California, state beaches are often classified under state parks.)", "To get a better feel for the trade-offs between a chain HMM and an entity-based HMM, let\u2019s see another example involving the two.", "Example CHMM-Product: Consider a chain HMM to model product names. (By \u2018product name\u2019 we mean both specific products and product categories.) We could use a trained version to recognize product names in unstructured text, some that it hasn\u2019t even been trained on.", "This HMM will be trained off a list of product names. The training is similar to that of the park chain HMM. Remember, we just need to learn (i) the number of states and (ii) the emission probabilities from the various states.", "This HMM has a simple structure and is easy to train. That said, the entity-based version we describe below will likely be more accurate.", "We might choose as our entities brand_token, product_type_token, product_version_token, and product_base_name_token. Example values are brand_token = canon, product_type_token = camera, product_version_token = 12, product_base_name_token = iphone. Our HMM\u2019s states would be these entities.", "Why does each entity\u2019s name end with the word token? Because the entity applies to a single token. So the state sequence associated with \u201csmart phone\u201d would be [product_type_token, product_type_token].", "To train this HMM we would need training sets for the various entities. These training sets could be derived from lists of brands, product types, etc. We say \u201cderived\u201d because we can\u2019t use the lists as-is if they contain multi-word entries. Consider smart phone listed as a product type. From this, we would derive two examples: smart \u2192 product_type_token and phone \u2192 product_type_token.", "We also need state sequences that capture the sequences of entities in tokenized product names. We could construct such a training set manually. This isn\u2019t hard because product names tend to follow some common conventions. For instance, in many two-word product names, the first name is a brand, and the second a product type. We already saw an example: canon camera. Consequently [brand_token, product_type_token] should be in the training set of state sequences.", "Automatically Deriving State Sequences From Product Names", "Constructing state sequences manually will get us quite far. A few state sequences cover a lot of product names. That said, the manual approach risks not scaling to a robust industrial-strength model, one covering millions of products. The product names may span a long tail of state sequences. Consider the state sequence [product_type_token, brand_token]. Some product names follow this convention.", "As it turns out, we can automate the construction of state sequences from the tokenized product names. This way as we add more and more products to our training set, state sequences automatically get extracted from them.", "Basic version: The basic idea is to take the tokenized product name and, for each word in it, find its most-likely entity. We can do this because we have training sets of (word, entity) pairs for the various entities.", "Refined version: We can refine this basic idea as follows. As before, we tokenize the product name. We then run the sequence of tokens through the HMM to find a state sequence that fits it best.", "This refinement uses information from both the entity training sets and from the HMM\u2019s state transitions. Consequently, it can be more accurate.", "I like to call this approach \u201cbootstrap training\u201d. We initialize the HMM manually. This includes seeding it with whatever state sequences we choose to. We can then run tokenized product names through it in the hope of discovering more state sequences.", "To enable the discovery of new state sequences, when initializing the HMM, we should allow transitions from all states to all states. (Other than from the state begin to the state end and any going out from the state end.) We can initialize our training set for these transitions automatically, so that the initial probabilities on these transitions are very low, albeit not zero. We call this pseudo-training.", "Curation: It's possible that some of the new state sequences we discover via \u201cbootstrap training\u201d have errors in them. So these should be reviewed by humans. Nonetheless, we have probably benefited despite the human in the loop. Manually discovering new state sequences is a lot more time consuming than curating automatically discovered ones.", "Just like an HMM, a conditional Markov model (CMM) operates on (token sequence, state sequence) pairs. Unlike an HMM a CMM is optimized for finding the best state sequence for a given token sequence. What exactly does this mean? This will get gradually clear in the examples below.", "That said, in this post, we will not discuss how to find the best state sequence for a given token sequence. We will discuss how to score a particular state sequence for a given token sequence. This scoring will surface the \u201cconditional\u201d part in CMM.", "Example CMM1 (First Part): Imagine training a language model on (tokenized full person name, token entities) pairs, with the aim of using it to parse person names. Parsing means to break up a person\u2019s name into its constituents. Such as first name and last name.", "Consider the name John K Smith. The model should be able to infer that K is a middle name word. Even if the training set does not contain a K emitted from middle_name_word.", "The CMM directly models this as", "How is this beneficial, compared to how the HMM models it.", "The CMM approach may be viewed as a multinomial classifier whose input is the pair (S[i], X[i+1]), and whose output is a probability distribution over the various values S[i+1] can take. This formulation lends itself to extracting any features that we see fit from the input. Possibly overlapping. Possibly taking into account interactions among S[i] and X[i+1].", "We can now use any multinomial classifier algorithm on this problem. Such as (multinomial) logistic regression, decision trees, or random forest.", "By contrast, the HMM cannot capture interactions between S[i] and X[i+1]. Nor can it accommodate arbitrary features. Nor can it leverage sophisticated machine learning classifier algorithms. The CMM on the other hand cannot leverage the entity training sets. We no longer learn emission probabilities.", "Example CMM 1 (completed): Let\u2019s complete this example. Starting with the features. From input (S[i], X[i+1]) we will extract the features state = S[i], token = X[i+1], token length = number of characters in X[i+1]. That is, we have three predictors: state, token, and token length. Each is categorical. The response is the value of S[i+1]. Also categorical.", "Why these features? The feature state lets us model the influence of the current state on the next state. The feature token let\u2019s us use the actual value of the token as a predictor of its state. This is useful. Certain tokens favor certain entities. For example, John is much more likely to be a first name word than the last name word. As an added bonus we get the interaction between state and token for free so long as our learning algorithm can exploit it. (If there is an interaction that is.) The feature token length is useful because we know that a token\u2019s length favors certain entities over others. First or middle names are often one character long. Last names hardly ever.", "Let\u2019s see a few training examples. We\u2019ll start with (token sequence, state sequence) pairs.", "From these let\u2019s derive a few training instances for the CMM formulation. (We don\u2019t show all.) The first three columns list the predictors. The last one lists the response.", "The first training instance above reads as \u201c(previous state equaling first, token equaling K, token\u2019s length equalling 1)\u201d leads to next state being middle\u201d.", "Example CMM 2 (Sketched): Consider parsing US street addresses. For simplicity assume they have the form", "Below are some pairs of tokenized US street addresses together with their entities. The entity names are abbreviated so the examples can fit in the space allocated.", "For the design of our features, let\u2019s start by focusing on those extracted from a token. Which features of a token discriminate among the various entities? The actual value of the token can predict whether it is a street_name_word or a street_suffix or a unit_prefix. The presence of digits predicts that it is a street_num_word or a unit_num_word.", "In view of this, from input (S[i], X[i+1]) we will extract the features state = S[i], token = X[i+1], proportion_of_digits_in_token = fraction of characters in X[i+1] that are digits.", "Just like a CMM, a CRF operates on (S, X) pairs where X denotes a token sequence and S its state sequence. Both model P(S|X), to optimize for the use case of finding a high-probability state sequence S for a given X.", "That is, the probability of state S[i] is conditioned on the previous state S[i-1] and the current token X[i].", "The CRF is less restrictive. That is, it accommodates more general P(S|X).", "The CRF operates on an undirected graph whose nodes model states and whose edges model direct influences among pairs of states. These influences are symmetric.", "In this post, we will limit ourselves to a linear-chain CRF. It has the structure", "This just models that a state is influenced by the previous state and the next state. That is, S[i] is directly influenced by S[i-1] and by S[i+1].", "In the CRF world, the linear CRF is the closest analog to a CMM. So let\u2019s look into it in more detail.", "The linear CRF models P(S|X) as", "The intuition is a bit easier to convey if we define a score C(S, X) from this by taking logs.", "We call it C because we think of it as a compatibility function. As a function of S, C(S, X) is monotone in P(S|X). So for the purposes of finding high-scoring state sequences, C(S, X) works equally well.", "What is f? It\u2019s a feature function that scores the compatibility of the triple (S[i-1], S[i], and X) along some dimension. More positive values are more compatible. w(f) is f\u2019s weight. It controls the compatibility\u2019s strength and direction.", "We can have as many feature functions as we choose.", "Okay, so in a linear CRF, we have a set of possibly overlapping feature functions. Each is applied to every edge. That said, they can use any information from any subset of the tokens in X. By contrast, CMMs only score compatibility of (S[i-1], S[i], X[i]) triples.", "At this point, it\u2019s best to see a concrete example with specific feature functions.", "Example CRF 1: Consider a generalized version of an example we saw earlier. Parse global street addresses. The generalization is that we are going global.", "Street addresses across the globe come in many different formats. In some, street numbers come before the street name, in some after. Some start with units, others end with them. (An example of a unit is \u201cApt 30\u201d.) These are just a few of the variations.", "Global street addresses also have varying quality, in terms of how well they adhere to the format for their locale. We want to be able to parse poor-quality addresses as well as possible.", "The first one is a US one, the second one Belgian. In addition to the positions of the street numbers and names, the positions of the street keywords also vary. (In these examples, \u2018St\u2019 and \u2018Rue\u2019 are street keywords.)", "Are there any global features we should extract from the tokenized street address? Two come to mind: country and language.", "Both the language and the country influence address formats. These factors overlap, but they also have complementary influences. Such is the case of Belgian vs French addresses, both written in French. In some aspects they are similar. In other aspects, they are not. That is the country matters.", "In view of this, we should include both.", "We\u2019ll build two specific feature functions around these.", "We\u2019ll add a third feature function to prefer tokens that are compatible with their tagged states.", "Training: F1 and F2 may be trained from (tokenized street address, state sequence) pairs tagged with their country and language. Transitions from S[i-1] to S[i] which are compatible with the country and language score high, those that are not don\u2019t. F3 may be trained from the (state, token) pairs in the training set.", "PhD, Computer Science, neural nets. 14+ years in industry: data science algos developer. 24+ patents issued. 50 academic pubs. Blogs on ML/data science topics."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F4e539d57bcaa&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstatistical-language-models-4e539d57bcaa&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstatistical-language-models-4e539d57bcaa&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstatistical-language-models-4e539d57bcaa&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstatistical-language-models-4e539d57bcaa&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://jagota-arun.medium.com/?source=post_page-----4e539d57bcaa--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4e539d57bcaa--------------------------------", "anchor_text": ""}, {"url": "https://jagota-arun.medium.com/?source=post_page-----4e539d57bcaa--------------------------------", "anchor_text": "Arun Jagota"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fef9ed921edad&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstatistical-language-models-4e539d57bcaa&user=Arun+Jagota&userId=ef9ed921edad&source=post_page-ef9ed921edad----4e539d57bcaa---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----4e539d57bcaa--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4e539d57bcaa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstatistical-language-models-4e539d57bcaa&user=Arun+Jagota&userId=ef9ed921edad&source=-----4e539d57bcaa---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4e539d57bcaa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstatistical-language-models-4e539d57bcaa&source=-----4e539d57bcaa---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://unsplash.com/@kellysikkema?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Kelly Sikkema"}, {"url": "https://unsplash.com/s/photos/language?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://www.cl.cam.ac.uk/teaching/1718/R228/lectures/lec9.pdf", "anchor_text": "https://www.cl.cam.ac.uk/teaching/1718/R228/lectures/lec9.pdf"}, {"url": "https://en.wikipedia.org/wiki/Maximum-entropy_Markov_model", "anchor_text": "Maximum-entropy Markov model"}, {"url": "https://www.ebi.ac.uk/training-beta/online/courses/pfam-creating-protein-families/what-are-profile-hidden-markov-models-hmms/#:~:text=Profile%20HMMs%20are%20probabilistic%20models,the%20alignment%2C%20see%20Figure%202", "anchor_text": "https://www.ebi.ac.uk/training-beta/online/courses/pfam-creating-protein-families/what-are-profile-hidden-markov-models-hmms/#:~:text=Profile%20HMMs%20are%20probabilistic%20models,the%20alignment%2C%20see%20Figure%202"}, {"url": "http://pages.cs.wisc.edu/~jerryzhu/cs838/CRF.pdf", "anchor_text": "CS838\u20131 Advanced NLP: Conditional Random Fields"}, {"url": "https://repository.upenn.edu/cgi/viewcontent.cgi?referer=https://en.wikipedia.org/&httpsredir=1&article=1162&context=cis_papers", "anchor_text": "Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data"}, {"url": "https://medium.com/tag/hidden-markov-models?source=post_page-----4e539d57bcaa---------------hidden_markov_models-----------------", "anchor_text": "Hidden Markov Models"}, {"url": "https://medium.com/tag/conditional-random-fields?source=post_page-----4e539d57bcaa---------------conditional_random_fields-----------------", "anchor_text": "Conditional Random Fields"}, {"url": "https://medium.com/tag/conditional-markov-models?source=post_page-----4e539d57bcaa---------------conditional_markov_models-----------------", "anchor_text": "Conditional Markov Models"}, {"url": "https://medium.com/tag/markov-models?source=post_page-----4e539d57bcaa---------------markov_models-----------------", "anchor_text": "Markov Models"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4e539d57bcaa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstatistical-language-models-4e539d57bcaa&user=Arun+Jagota&userId=ef9ed921edad&source=-----4e539d57bcaa---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4e539d57bcaa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstatistical-language-models-4e539d57bcaa&user=Arun+Jagota&userId=ef9ed921edad&source=-----4e539d57bcaa---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4e539d57bcaa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstatistical-language-models-4e539d57bcaa&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://jagota-arun.medium.com/?source=post_page-----4e539d57bcaa--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4e539d57bcaa--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fef9ed921edad&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstatistical-language-models-4e539d57bcaa&user=Arun+Jagota&userId=ef9ed921edad&source=post_page-ef9ed921edad----4e539d57bcaa---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F1638f1de39a6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstatistical-language-models-4e539d57bcaa&newsletterV3=ef9ed921edad&newsletterV3Id=1638f1de39a6&user=Arun+Jagota&userId=ef9ed921edad&source=-----4e539d57bcaa---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://jagota-arun.medium.com/?source=post_page-----4e539d57bcaa--------------------------------", "anchor_text": "Written by Arun Jagota"}, {"url": "https://jagota-arun.medium.com/followers?source=post_page-----4e539d57bcaa--------------------------------", "anchor_text": "685 Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----4e539d57bcaa--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fef9ed921edad&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstatistical-language-models-4e539d57bcaa&user=Arun+Jagota&userId=ef9ed921edad&source=post_page-ef9ed921edad----4e539d57bcaa---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F1638f1de39a6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstatistical-language-models-4e539d57bcaa&newsletterV3=ef9ed921edad&newsletterV3Id=1638f1de39a6&user=Arun+Jagota&userId=ef9ed921edad&source=-----4e539d57bcaa---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/text-sentiment-analysis-in-nlp-ce6baba6d466?source=author_recirc-----4e539d57bcaa----0---------------------a81e67d5_e237_4531_bd60_db084c4b3c8c-------", "anchor_text": ""}, {"url": "https://jagota-arun.medium.com/?source=author_recirc-----4e539d57bcaa----0---------------------a81e67d5_e237_4531_bd60_db084c4b3c8c-------", "anchor_text": ""}, {"url": "https://jagota-arun.medium.com/?source=author_recirc-----4e539d57bcaa----0---------------------a81e67d5_e237_4531_bd60_db084c4b3c8c-------", "anchor_text": "Arun Jagota"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----4e539d57bcaa----0---------------------a81e67d5_e237_4531_bd60_db084c4b3c8c-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/text-sentiment-analysis-in-nlp-ce6baba6d466?source=author_recirc-----4e539d57bcaa----0---------------------a81e67d5_e237_4531_bd60_db084c4b3c8c-------", "anchor_text": "Text Sentiment Analysis in NLPProblems, use-cases, and methods: from simple to advanced"}, {"url": "https://towardsdatascience.com/text-sentiment-analysis-in-nlp-ce6baba6d466?source=author_recirc-----4e539d57bcaa----0---------------------a81e67d5_e237_4531_bd60_db084c4b3c8c-------", "anchor_text": "\u00b717 min read\u00b7Jul 19, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fce6baba6d466&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-sentiment-analysis-in-nlp-ce6baba6d466&user=Arun+Jagota&userId=ef9ed921edad&source=-----ce6baba6d466----0-----------------clap_footer----a81e67d5_e237_4531_bd60_db084c4b3c8c-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/text-sentiment-analysis-in-nlp-ce6baba6d466?source=author_recirc-----4e539d57bcaa----0---------------------a81e67d5_e237_4531_bd60_db084c4b3c8c-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fce6baba6d466&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-sentiment-analysis-in-nlp-ce6baba6d466&source=-----4e539d57bcaa----0-----------------bookmark_preview----a81e67d5_e237_4531_bd60_db084c4b3c8c-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----4e539d57bcaa----1---------------------a81e67d5_e237_4531_bd60_db084c4b3c8c-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----4e539d57bcaa----1---------------------a81e67d5_e237_4531_bd60_db084c4b3c8c-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----4e539d57bcaa----1---------------------a81e67d5_e237_4531_bd60_db084c4b3c8c-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----4e539d57bcaa----1---------------------a81e67d5_e237_4531_bd60_db084c4b3c8c-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----4e539d57bcaa----1---------------------a81e67d5_e237_4531_bd60_db084c4b3c8c-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----4e539d57bcaa----1---------------------a81e67d5_e237_4531_bd60_db084c4b3c8c-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----a81e67d5_e237_4531_bd60_db084c4b3c8c-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----4e539d57bcaa----1---------------------a81e67d5_e237_4531_bd60_db084c4b3c8c-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----4e539d57bcaa----1-----------------bookmark_preview----a81e67d5_e237_4531_bd60_db084c4b3c8c-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----4e539d57bcaa----2---------------------a81e67d5_e237_4531_bd60_db084c4b3c8c-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----4e539d57bcaa----2---------------------a81e67d5_e237_4531_bd60_db084c4b3c8c-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----4e539d57bcaa----2---------------------a81e67d5_e237_4531_bd60_db084c4b3c8c-------", "anchor_text": "Jacob Marks, Ph.D."}, {"url": "https://towardsdatascience.com/?source=author_recirc-----4e539d57bcaa----2---------------------a81e67d5_e237_4531_bd60_db084c4b3c8c-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----4e539d57bcaa----2---------------------a81e67d5_e237_4531_bd60_db084c4b3c8c-------", "anchor_text": "How I Turned My Company\u2019s Docs into a Searchable Database with OpenAIAnd how you can do the same with your docs"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----4e539d57bcaa----2---------------------a81e67d5_e237_4531_bd60_db084c4b3c8c-------", "anchor_text": "15 min read\u00b7Apr 25"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&user=Jacob+Marks%2C+Ph.D.&userId=f7dc0c0eae92&source=-----4f2d34bd8736----2-----------------clap_footer----a81e67d5_e237_4531_bd60_db084c4b3c8c-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----4e539d57bcaa----2---------------------a81e67d5_e237_4531_bd60_db084c4b3c8c-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "20"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&source=-----4e539d57bcaa----2-----------------bookmark_preview----a81e67d5_e237_4531_bd60_db084c4b3c8c-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/named-entity-recognition-in-nlp-be09139fa7b8?source=author_recirc-----4e539d57bcaa----3---------------------a81e67d5_e237_4531_bd60_db084c4b3c8c-------", "anchor_text": ""}, {"url": "https://jagota-arun.medium.com/?source=author_recirc-----4e539d57bcaa----3---------------------a81e67d5_e237_4531_bd60_db084c4b3c8c-------", "anchor_text": ""}, {"url": "https://jagota-arun.medium.com/?source=author_recirc-----4e539d57bcaa----3---------------------a81e67d5_e237_4531_bd60_db084c4b3c8c-------", "anchor_text": "Arun Jagota"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----4e539d57bcaa----3---------------------a81e67d5_e237_4531_bd60_db084c4b3c8c-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/named-entity-recognition-in-nlp-be09139fa7b8?source=author_recirc-----4e539d57bcaa----3---------------------a81e67d5_e237_4531_bd60_db084c4b3c8c-------", "anchor_text": "Named Entity Recognition in NLPReal-world use cases, models, methods: from simple to advanced"}, {"url": "https://towardsdatascience.com/named-entity-recognition-in-nlp-be09139fa7b8?source=author_recirc-----4e539d57bcaa----3---------------------a81e67d5_e237_4531_bd60_db084c4b3c8c-------", "anchor_text": "\u00b730 min read\u00b7Jul 9, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbe09139fa7b8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnamed-entity-recognition-in-nlp-be09139fa7b8&user=Arun+Jagota&userId=ef9ed921edad&source=-----be09139fa7b8----3-----------------clap_footer----a81e67d5_e237_4531_bd60_db084c4b3c8c-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/named-entity-recognition-in-nlp-be09139fa7b8?source=author_recirc-----4e539d57bcaa----3---------------------a81e67d5_e237_4531_bd60_db084c4b3c8c-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbe09139fa7b8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnamed-entity-recognition-in-nlp-be09139fa7b8&source=-----4e539d57bcaa----3-----------------bookmark_preview----a81e67d5_e237_4531_bd60_db084c4b3c8c-------", "anchor_text": ""}, {"url": "https://jagota-arun.medium.com/?source=post_page-----4e539d57bcaa--------------------------------", "anchor_text": "See all from Arun Jagota"}, {"url": "https://towardsdatascience.com/?source=post_page-----4e539d57bcaa--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----4e539d57bcaa----0---------------------f1878318_ebec_46f4_b523_f8fddf095477-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----4e539d57bcaa----0---------------------f1878318_ebec_46f4_b523_f8fddf095477-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----4e539d57bcaa----0---------------------f1878318_ebec_46f4_b523_f8fddf095477-------", "anchor_text": "The PyCoach"}, {"url": "https://artificialcorner.com/?source=read_next_recirc-----4e539d57bcaa----0---------------------f1878318_ebec_46f4_b523_f8fddf095477-------", "anchor_text": "Artificial Corner"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----4e539d57bcaa----0---------------------f1878318_ebec_46f4_b523_f8fddf095477-------", "anchor_text": "You\u2019re Using ChatGPT Wrong! Here\u2019s How to Be Ahead of 99% of ChatGPT UsersMaster ChatGPT by learning prompt engineering."}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----4e539d57bcaa----0---------------------f1878318_ebec_46f4_b523_f8fddf095477-------", "anchor_text": "\u00b77 min read\u00b7Mar 17"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fartificial-corner%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&user=The+PyCoach&userId=fb44e21903f3&source=-----886a50dabc54----0-----------------clap_footer----f1878318_ebec_46f4_b523_f8fddf095477-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----4e539d57bcaa----0---------------------f1878318_ebec_46f4_b523_f8fddf095477-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "276"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&source=-----4e539d57bcaa----0-----------------bookmark_preview----f1878318_ebec_46f4_b523_f8fddf095477-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----4e539d57bcaa----1---------------------f1878318_ebec_46f4_b523_f8fddf095477-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----4e539d57bcaa----1---------------------f1878318_ebec_46f4_b523_f8fddf095477-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----4e539d57bcaa----1---------------------f1878318_ebec_46f4_b523_f8fddf095477-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----4e539d57bcaa----1---------------------f1878318_ebec_46f4_b523_f8fddf095477-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----4e539d57bcaa----1---------------------f1878318_ebec_46f4_b523_f8fddf095477-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----4e539d57bcaa----1---------------------f1878318_ebec_46f4_b523_f8fddf095477-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----1-----------------clap_footer----f1878318_ebec_46f4_b523_f8fddf095477-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----4e539d57bcaa----1---------------------f1878318_ebec_46f4_b523_f8fddf095477-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----4e539d57bcaa----1-----------------bookmark_preview----f1878318_ebec_46f4_b523_f8fddf095477-------", "anchor_text": ""}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----4e539d57bcaa----0---------------------f1878318_ebec_46f4_b523_f8fddf095477-------", "anchor_text": ""}, {"url": "https://medium.com/@timothymugayi?source=read_next_recirc-----4e539d57bcaa----0---------------------f1878318_ebec_46f4_b523_f8fddf095477-------", "anchor_text": ""}, {"url": "https://medium.com/@timothymugayi?source=read_next_recirc-----4e539d57bcaa----0---------------------f1878318_ebec_46f4_b523_f8fddf095477-------", "anchor_text": "Timothy Mugayi"}, {"url": "https://betterprogramming.pub/?source=read_next_recirc-----4e539d57bcaa----0---------------------f1878318_ebec_46f4_b523_f8fddf095477-------", "anchor_text": "Better Programming"}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----4e539d57bcaa----0---------------------f1878318_ebec_46f4_b523_f8fddf095477-------", "anchor_text": "How To Build Your Own Custom ChatGPT With Custom Knowledge BaseFeed your ChatGPT bot with custom data sources"}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----4e539d57bcaa----0---------------------f1878318_ebec_46f4_b523_f8fddf095477-------", "anchor_text": "\u00b711 min read\u00b7Apr 7"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fbetter-programming%2F4e61ad82427e&operation=register&redirect=https%3A%2F%2Fbetterprogramming.pub%2Fhow-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e&user=Timothy+Mugayi&userId=34774d6cac27&source=-----4e61ad82427e----0-----------------clap_footer----f1878318_ebec_46f4_b523_f8fddf095477-------", "anchor_text": ""}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----4e539d57bcaa----0---------------------f1878318_ebec_46f4_b523_f8fddf095477-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "83"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4e61ad82427e&operation=register&redirect=https%3A%2F%2Fbetterprogramming.pub%2Fhow-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e&source=-----4e539d57bcaa----0-----------------bookmark_preview----f1878318_ebec_46f4_b523_f8fddf095477-------", "anchor_text": ""}, {"url": "https://medium.com/geekculture/stop-doing-this-on-chatgpt-get-ahead-99-users-ai-artificial-intelligence-productivity-prompt-engineering-4-f3441bf7a25a?source=read_next_recirc-----4e539d57bcaa----1---------------------f1878318_ebec_46f4_b523_f8fddf095477-------", "anchor_text": ""}, {"url": "https://medium.com/@rfeers?source=read_next_recirc-----4e539d57bcaa----1---------------------f1878318_ebec_46f4_b523_f8fddf095477-------", "anchor_text": ""}, {"url": "https://medium.com/@rfeers?source=read_next_recirc-----4e539d57bcaa----1---------------------f1878318_ebec_46f4_b523_f8fddf095477-------", "anchor_text": "Josep Ferrer"}, {"url": "https://medium.com/geekculture?source=read_next_recirc-----4e539d57bcaa----1---------------------f1878318_ebec_46f4_b523_f8fddf095477-------", "anchor_text": "Geek Culture"}, {"url": "https://medium.com/geekculture/stop-doing-this-on-chatgpt-get-ahead-99-users-ai-artificial-intelligence-productivity-prompt-engineering-4-f3441bf7a25a?source=read_next_recirc-----4e539d57bcaa----1---------------------f1878318_ebec_46f4_b523_f8fddf095477-------", "anchor_text": "Stop doing this on ChatGPT and get ahead of the 99% of its usersUnleash the Power of AI Writing with Effective Prompts"}, {"url": "https://medium.com/geekculture/stop-doing-this-on-chatgpt-get-ahead-99-users-ai-artificial-intelligence-productivity-prompt-engineering-4-f3441bf7a25a?source=read_next_recirc-----4e539d57bcaa----1---------------------f1878318_ebec_46f4_b523_f8fddf095477-------", "anchor_text": "\u00b78 min read\u00b7Mar 31"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fgeekculture%2Ff3441bf7a25a&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fgeekculture%2Fstop-doing-this-on-chatgpt-get-ahead-99-users-ai-artificial-intelligence-productivity-prompt-engineering-4-f3441bf7a25a&user=Josep+Ferrer&userId=8213af8f3ccf&source=-----f3441bf7a25a----1-----------------clap_footer----f1878318_ebec_46f4_b523_f8fddf095477-------", "anchor_text": ""}, {"url": "https://medium.com/geekculture/stop-doing-this-on-chatgpt-get-ahead-99-users-ai-artificial-intelligence-productivity-prompt-engineering-4-f3441bf7a25a?source=read_next_recirc-----4e539d57bcaa----1---------------------f1878318_ebec_46f4_b523_f8fddf095477-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "71"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff3441bf7a25a&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fgeekculture%2Fstop-doing-this-on-chatgpt-get-ahead-99-users-ai-artificial-intelligence-productivity-prompt-engineering-4-f3441bf7a25a&source=-----4e539d57bcaa----1-----------------bookmark_preview----f1878318_ebec_46f4_b523_f8fddf095477-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/gpt-4-vs-chatgpt-an-exploration-of-training-performance-capabilities-and-limitations-35c990c133c5?source=read_next_recirc-----4e539d57bcaa----2---------------------f1878318_ebec_46f4_b523_f8fddf095477-------", "anchor_text": ""}, {"url": "https://medium.com/@mary.newhauser?source=read_next_recirc-----4e539d57bcaa----2---------------------f1878318_ebec_46f4_b523_f8fddf095477-------", "anchor_text": ""}, {"url": "https://medium.com/@mary.newhauser?source=read_next_recirc-----4e539d57bcaa----2---------------------f1878318_ebec_46f4_b523_f8fddf095477-------", "anchor_text": "Mary Newhauser"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----4e539d57bcaa----2---------------------f1878318_ebec_46f4_b523_f8fddf095477-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/gpt-4-vs-chatgpt-an-exploration-of-training-performance-capabilities-and-limitations-35c990c133c5?source=read_next_recirc-----4e539d57bcaa----2---------------------f1878318_ebec_46f4_b523_f8fddf095477-------", "anchor_text": "GPT-4 vs. ChatGPT: An Exploration of Training, Performance, Capabilities, and LimitationsGPT-4 is an improvement but temper your expectations."}, {"url": "https://towardsdatascience.com/gpt-4-vs-chatgpt-an-exploration-of-training-performance-capabilities-and-limitations-35c990c133c5?source=read_next_recirc-----4e539d57bcaa----2---------------------f1878318_ebec_46f4_b523_f8fddf095477-------", "anchor_text": "\u00b77 min read\u00b7Mar 17"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F35c990c133c5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgpt-4-vs-chatgpt-an-exploration-of-training-performance-capabilities-and-limitations-35c990c133c5&user=Mary+Newhauser&userId=6b27bdb820b9&source=-----35c990c133c5----2-----------------clap_footer----f1878318_ebec_46f4_b523_f8fddf095477-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/gpt-4-vs-chatgpt-an-exploration-of-training-performance-capabilities-and-limitations-35c990c133c5?source=read_next_recirc-----4e539d57bcaa----2---------------------f1878318_ebec_46f4_b523_f8fddf095477-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "13"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F35c990c133c5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgpt-4-vs-chatgpt-an-exploration-of-training-performance-capabilities-and-limitations-35c990c133c5&source=-----4e539d57bcaa----2-----------------bookmark_preview----f1878318_ebec_46f4_b523_f8fddf095477-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----4e539d57bcaa----3---------------------f1878318_ebec_46f4_b523_f8fddf095477-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----4e539d57bcaa----3---------------------f1878318_ebec_46f4_b523_f8fddf095477-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----4e539d57bcaa----3---------------------f1878318_ebec_46f4_b523_f8fddf095477-------", "anchor_text": "Alexander Nguyen"}, {"url": "https://levelup.gitconnected.com/?source=read_next_recirc-----4e539d57bcaa----3---------------------f1878318_ebec_46f4_b523_f8fddf095477-------", "anchor_text": "Level Up Coding"}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----4e539d57bcaa----3---------------------f1878318_ebec_46f4_b523_f8fddf095477-------", "anchor_text": "Why I Keep Failing Candidates During Google Interviews\u2026They don\u2019t meet the bar."}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----4e539d57bcaa----3---------------------f1878318_ebec_46f4_b523_f8fddf095477-------", "anchor_text": "\u00b74 min read\u00b7Apr 13"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fgitconnected%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&user=Alexander+Nguyen&userId=a148fd75c2e9&source=-----dc8f865b2c19----3-----------------clap_footer----f1878318_ebec_46f4_b523_f8fddf095477-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----4e539d57bcaa----3---------------------f1878318_ebec_46f4_b523_f8fddf095477-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "91"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&source=-----4e539d57bcaa----3-----------------bookmark_preview----f1878318_ebec_46f4_b523_f8fddf095477-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----4e539d57bcaa--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----4e539d57bcaa--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----4e539d57bcaa--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----4e539d57bcaa--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----4e539d57bcaa--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----4e539d57bcaa--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----4e539d57bcaa--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----4e539d57bcaa--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----4e539d57bcaa--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----4e539d57bcaa--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}