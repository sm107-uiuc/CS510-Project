{"url": "https://towardsdatascience.com/linear-regression-from-scratch-with-tensorflow-2-part-1-3e2443804df0", "time": 1683007917.797796, "path": "towardsdatascience.com/linear-regression-from-scratch-with-tensorflow-2-part-1-3e2443804df0/", "webpage": {"metadata": {"title": "Linear Regression from Scratch with Tensorflow 2 | by Cedric Conol | Towards Data Science", "h1": "Linear Regression from Scratch with Tensorflow 2", "description": "Linear regression is one of the most basic and perhaps one of most commonly used machine learning algorithm that beginners and experts alike should know by heart. In this article I will walk you\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/understanding-the-mathematics-behind-gradient-descent-dde5dc9be06e", "anchor_text": "article", "paragraph_index": 18}, {"url": "http://Texthero.org", "anchor_text": "Texthero.org", "paragraph_index": 45}, {"url": "https://www.linkedin.com/in/conolcedric", "anchor_text": "https://www.linkedin.com/in/conolcedric", "paragraph_index": 45}], "all_paragraphs": ["Linear regression is one of the most basic and perhaps one of most commonly used machine learning algorithm that beginners and experts alike should know by heart. In this article I will walk you through how to implement linear regression using only Tensorflow. The discussion will be divided into two parts, the first part explains the concept of linear regression, the second part is a walk through of how to implement linear regression in Tensorflow.", "Linear regression attempts to model the relation of dependent and independent variables by fitting a linear equation. Suppose we have the data of quiz scores and the length of study hours of 100 students.", "By visually inspecting the scatter plot, we can easily draw a line with the equation y=mx+b , where m and b are the slope and y-intercept, respectively. These can be seen on the figure as roughly 40 and 0 for m and b respectively.", "The y=40x line looks good! We can then estimate that a student\u2019s score is just 40 multiplied by the number of hours a student studied.", "Linear regression works exactly like this except that it cannot visually check the slope and y-intercept from the scatter plot. What it does instead is to guess the slope and y-intercept first then measure how good its guess is. If it\u2019s not good enough then it adjusts the slope and y-intercept until the line fits the data well.", "Linear regression is a three-step algorithm:", "Now that we built our intuition around linear regression, let\u2019s talk about each of the step mathematically.", "The first step in linear regression model is to initialize a linear equation, yes, we\u2019ll use y=mx+b but we have to generalize our approach. The reason for such is that we might be facing data with multiple independent variables. Think of it as adding another variable in our Quiz Score data, like the amount of coffee consumed while studying. Having this coffee dimension will make the linear equation look like this: y=m1x1+m2x2+b , where m1 and m2 are slopes for the study hours and coffee dimensions, respectively, and x1 and x2 are the study hours and coffee variables.", "We\u2019ll use dot product to represent the product of matrices, m and x, instead of writing a longer equation for every new variable. Note that it is also valid to use the term, tensor, since it is the generalization of a matrix. Bold type letters are used to denote matrices, so the linear equation should be written as y=m\u22c5x+b.", "There are many ways to initialize the parameters of the equation, the most common ones are using random values, zeros, or ones. You are free to use any type of initialization, this choice will determine how fast your learning algorithm terminates. In the next iteration of the algorithm, these parameters will be updated based on some function discussed in step 2.", "Now let\u2019s say the initial values you set for m and b are ones, so your equation is y=1x+1 . The initial predictions will look like the orange dots in the figure below. It\u2019s obviously a very bad prediction, we need a number to quantify how bad or good these predictions are.", "There are many ways to measure the goodness of our prediction, we\u2019ll use one of them called mean squared error (MSE). In this context, error means difference, so MSE literally means taking the square of the difference between actual and predicted values, then take the average. It is written mathematically as", "Functions like MSE are called loss function or objective function. These are functions that the algorithm wants to minimize. If our linear regression model perfectly predicts the quiz scores, its MSE will be equal to 0. So in every iteration of the algorithm, it should update the parameters such that the MSE comes closer to 0 without overfitting. Overfitting is an entire topic itself, but what it means essentially is that we don\u2019t want our learning algorithm to be so good with the training data and fail miserably on the test set.", "Sure we can keep on guessing the parameters until we get close enough to zero MSE but it will take time and effort \u2014 gradient descent will do this for us. If you\u2019re not familiar with the term, there\u2019s tons of articles and videos explaining its concept.", "Gradient descent is one of the building blocks of artificial intelligence. It is the learning in machine learning. Algorithms like gradient descent allows learning algorithms to learn without being told so explicitly. (You need to brush up on your calculus to understand how gradient descent works.)", "Gradient descent is an optimization algorithm that we will use to minimize our loss function (MSE in this case). It does so by updating the parameters with small changes in each iteration, this change can be big too depending on your preference (learning rate).", "In each new iteration, the updated parameters will be p_new=p_old-(l*dL/dp), where p is the parameter, which could be the slope, m, or the y-intercept, b. The new variables, l and dL/dp, are the learning rate and partial derivative of the loss function with respect to the parameter.", "With enough iterations, the slope and y-intercept will get closer to 40 and 0, the values we consider to be \u201cclose enough\u201d to fit to our data. As you may have observed, if you happen to initialize the parameters close to 40 and 0, say 35 and 0.5, then the algorithm will take less iterations.", "This article is very helpful if you want to dig deeper into the math of gradient descent.", "Here are some of the possible ways to terminate the algorithm:", "In this demo, we\u2019ll follow the three-step approach to linear regression algorithm discussed above and terminate the algorithm using stopping criteria 1.", "These are the only libraries we\u2019ll need for this demo. TensorFlow for building the algorithm, pyplot for visualization purposes, and boston_housing as our toy dataset.", "Let\u2019s start off by creating aSimpleLinearRegression class with initialization options.", "I specified three initialization options, ones , zeros , and random (default). tf.random.uniform will generate a tensor with random values from uniform distribution within the range minval and maxval with shape shape . I defined m as a variable with no particular shape so it can be flexible enough to accept any number of independent variables, this can be done by setting shape=tf.TensorShape(None) .", "Next is to implement our loss function, MSE. To recap, MSE is written mathematically as:", "Here\u2019s my implementation of the function in TensorFlow:", "This is fairly easy to write in Tensorflow. First you take the difference of true and predicted value, use tf.square to square the differences then get the mean of the squared differences using tf.reduce_mean .", "This function accepts true and predicted values, the former comes from the data itself, but the latter have to be computed.", "The predict method is done by simplifying the linear equation. First we take the dot product of m (slope tensor) and x (feature tensor) and add the y-intercept b . I had to specify the axis to which the reduction in reduction_sum will be computed to 1 , otherwise it will reduce the tensor to a single sum.", "We need gradient descent to update our parameters for every iteration. There\u2019s no need to create this algorithm from scratch as Tensorflow has a built it function for this, tf.GradientTape . By default, GradientTape has persistent set to False which means at most one call can be made to the gradient() method in this object. Since we are using this to calculate for two gradients (one for m and another for b ) for every iteration, we have to set this to True. Then we specify the loss function to which gradients will be computed, in this case mse with parameters y and self.predict(X) which represents true and predicted values respectively.", "Each parameter will be updated by subtracting the product of learning rate and the gradient of the parameter.", "Learning is a hyperparameter and should be specified when training the linear regressor. The gradient, dL/dp , is computed using the gradient() method, which accepts the loss function and the parameter. This operation just solves for the partial derivative of the loss function with respect to the parameter. We have to compute for two gradients in each iteration, one for m and b .", "To update the parameter with the new value, which is just the old value minus l*dL/dp , we simply use assign_sub() method of tf.Variable .", "Let\u2019s put everything together in a train method.", "First thing that it does is to check if the data only contain a single independent variable, if it is, then it will reshape it to become a 2D tensor.", "self.m.assign([self.var]*X.shape[-1]) will initialize m with initial values we set during initialization with a shape following the number of independent variables the data have.", "The stopping criteria of our algorithm is the number of iteration, defined by epoch . For each iteration, it will call the update method.", "It\u2019s time to test our algorithm using the Boston Housing dataset.", "Load the dataset using keras.datasets :", "Create and train a SimpleLinearRegression object.", "Here\u2019s the losses of the last five iteration:", "Let\u2019s predict using the test set:", "We have to standardize the input first then reverse the process once we have the prediction.", "Despite its simplicity, linear regression is one of the most commonly used machine learning algorithms in the industry, and some companies test how well you understand it. Although there are much simpler ways to implement this algorithm, like using scikit-learn and even TensorFlow\u2019s LinearRegressor , we implemented the entire algorithm from scratch with the intent of experiencing TensorFlow\u2019s functions. When you go into neural networks, there are high-level libraries like Keras which simplifies the process of building neural networks using TensorFlow as backend. But eventually, especially if you are a researcher, you would want to customize your models however you want it, and this is where the low-level functions of TensorFlow are very useful.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "NLP | Texthero.org | Data @ Lamudi https://www.linkedin.com/in/conolcedric"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F3e2443804df0&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-from-scratch-with-tensorflow-2-part-1-3e2443804df0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-from-scratch-with-tensorflow-2-part-1-3e2443804df0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-from-scratch-with-tensorflow-2-part-1-3e2443804df0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-from-scratch-with-tensorflow-2-part-1-3e2443804df0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----3e2443804df0--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3e2443804df0--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@conolcedric?source=post_page-----3e2443804df0--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@conolcedric?source=post_page-----3e2443804df0--------------------------------", "anchor_text": "Cedric Conol"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F81f6802d94b4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-from-scratch-with-tensorflow-2-part-1-3e2443804df0&user=Cedric+Conol&userId=81f6802d94b4&source=post_page-81f6802d94b4----3e2443804df0---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3e2443804df0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-from-scratch-with-tensorflow-2-part-1-3e2443804df0&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3e2443804df0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-from-scratch-with-tensorflow-2-part-1-3e2443804df0&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@andriklangfield?utm_source=medium&utm_medium=referral", "anchor_text": "Andrik Langfield"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://www.researchgate.net/figure/Mean-Squared-Error-formula-used-to-evaluate-the-user-model_fig1_221515860", "anchor_text": "researchgate.net"}, {"url": "https://towardsdatascience.com/understanding-the-mathematics-behind-gradient-descent-dde5dc9be06e", "anchor_text": "article"}, {"url": "https://www.researchgate.net/figure/Mean-Squared-Error-formula-used-to-evaluate-the-user-model_fig1_221515860", "anchor_text": "researchgate.net"}, {"url": "https://medium.com/tag/tensorflow2?source=post_page-----3e2443804df0---------------tensorflow2-----------------", "anchor_text": "Tensorflow2"}, {"url": "https://medium.com/tag/tensorflow?source=post_page-----3e2443804df0---------------tensorflow-----------------", "anchor_text": "TensorFlow"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----3e2443804df0---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----3e2443804df0---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3e2443804df0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-from-scratch-with-tensorflow-2-part-1-3e2443804df0&user=Cedric+Conol&userId=81f6802d94b4&source=-----3e2443804df0---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3e2443804df0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-from-scratch-with-tensorflow-2-part-1-3e2443804df0&user=Cedric+Conol&userId=81f6802d94b4&source=-----3e2443804df0---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3e2443804df0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-from-scratch-with-tensorflow-2-part-1-3e2443804df0&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3e2443804df0--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F3e2443804df0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-from-scratch-with-tensorflow-2-part-1-3e2443804df0&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----3e2443804df0---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----3e2443804df0--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----3e2443804df0--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----3e2443804df0--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----3e2443804df0--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----3e2443804df0--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----3e2443804df0--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----3e2443804df0--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----3e2443804df0--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@conolcedric?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@conolcedric?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Cedric Conol"}, {"url": "https://medium.com/@conolcedric/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "30 Followers"}, {"url": "http://Texthero.org", "anchor_text": "Texthero.org"}, {"url": "https://www.linkedin.com/in/conolcedric", "anchor_text": "https://www.linkedin.com/in/conolcedric"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F81f6802d94b4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-from-scratch-with-tensorflow-2-part-1-3e2443804df0&user=Cedric+Conol&userId=81f6802d94b4&source=post_page-81f6802d94b4--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F4464285b81c6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-regression-from-scratch-with-tensorflow-2-part-1-3e2443804df0&newsletterV3=81f6802d94b4&newsletterV3Id=4464285b81c6&user=Cedric+Conol&userId=81f6802d94b4&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}