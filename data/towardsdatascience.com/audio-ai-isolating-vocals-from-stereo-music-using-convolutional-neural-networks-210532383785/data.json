{"url": "https://towardsdatascience.com/audio-ai-isolating-vocals-from-stereo-music-using-convolutional-neural-networks-210532383785", "time": 1682994845.6083431, "path": "towardsdatascience.com/audio-ai-isolating-vocals-from-stereo-music-using-convolutional-neural-networks-210532383785/", "webpage": {"metadata": {"title": "Audio AI: isolating vocals from stereo music using Convolutional Neural Networks | by Ale Koretzky | Towards Data Science", "h1": "Audio AI: isolating vocals from stereo music using Convolutional Neural Networks", "description": "What if we could go back to 1965, knock on Abbey Road Studios\u2019 front door holding an \u2018All Access\u2019 badge, and have the privilege of listening to those signature Lennon-McCartney harmonies A-Capella\u2026"}, "outgoing_paragraph_urls": [{"url": "https://ccrma.stanford.edu/~njb/teaching/sstutorial/", "anchor_text": "these tutorial mini-series", "paragraph_index": 1}, {"url": "https://en.wikipedia.org/wiki/Short-time_Fourier_transform", "anchor_text": "Short-Time Fourier Transform (STFT)", "paragraph_index": 9}, {"url": "https://code.soundsoftware.ac.uk/projects/pyin", "anchor_text": "pYIN algorithm", "paragraph_index": 15}, {"url": "https://en.wikipedia.org/wiki/Voice_activity_detection", "anchor_text": "Vocal Activity Detector (VAD)", "paragraph_index": 25}], "all_paragraphs": ["What if we could go back to 1965, knock on Abbey Road Studios\u2019 front door holding an \u2018All Access\u2019 badge, and have the privilege of listening to those signature Lennon-McCartney harmonies A-Capella? Our input here is a medium quality mp3 of We Can Work it Out by The Beatles. The top track is the input mix and the bottom track, the isolated vocals coming out of our model.", "Formally known as Audio Source Separation, the problem we are trying to solve here consists in recovering or reconstructing one or more source signals that, through some -linear or convolutive- process, have been mixed with other signals. The field has many practical applications including but not limited to speech denoising and enhancement, music remixing, spatial audio, remastering, etc. In the context of music production, it is sometimes referred to as unmixing or demixing. There\u2019s a good amount of resources on the subject, going from ICA-based -blind- Source Separation, to semi-supervised Non-negative Matrix Factorization techniques, to more recent neural network-based approaches. For a nice walkthrough on the first two, you can check out these tutorial mini-series from CCRMA, which I found very useful back in the day.", "But before jumping into design stuff.. a liiittle bit of Applied Machine Learning philosophy\u2026", "As someone who\u2019s been working in signal & image processing for a while and prior to the \u2018deep-learning-solves-it-all\u2019 boom, I will introduce the solution as a Feature Engineering journey and show you why, for this particular problem, an artificial neural network ends up being the best approach. Why? Very often I find people writing things like:", "\u201cwith deep learning you don\u2019t have to worry about feature engineering anymore; it does it for you\u201d", "\u201cthe difference between machine learning and deep learning < let me stop you right there\u2026 Deep Learning is still Machine Learning! > is that in ML you do the feature extraction and in deep learning it happens automatically inside the network\u201d.", "These generalizations, probably coming from the fact that DNNs can be pretty effective at learning good latent spaces, are just wrong. It frustrates me to see recent grads and practitioners being sold on the above misconceptions and going for the \u2018deep-learning-solves-it-all\u2019 approach as something you just throw a bunch of raw data at (yes, even after doing some pre-processing you can still be a sinner :)), and expect things to just work as desired. In the real world, where your data is not as simple, clean and pretty as the MNIST dataset and where you have to care about things like real-time, memory and so on, these misconceptions can leave you stuck in experimentation mode for a very long time\u2026", "Feature Engineering not only remains a very important discipline when designing artificial neural networks; in most cases and just like with any other ML technique, it separates production-ready solutions from failing or underperforming experiments. A deep understanding of your data and its domain can still get you very far\u2026", "Ok, now that I\u2019m done preaching, let\u2019s get into what you came for! Just like with every other data problem that I\u2019ve worked on in my career, I\u2019ll begin by asking the question \u201chow does the data look like\u201d?. Let\u2019s take a look at the following fragment of singing voice from an original studio recording.", "Not too interesting right? Well, this is because we are visualizing the waveform or time-domain signal, where all we have access to are the amplitude values of the signal over time. We could extract things such as envelopes, RMS values, zero-crossing rate, etc, but these features are too primitive and not discriminative enough for helping us solve the problem. If we want to extract vocal content from a mix we should somehow expose the structure of human speech, to begin with. Luckily, the Short-Time Fourier Transform (STFT) comes to the rescue.", "Although I love Speech Processing and I would definitely enjoy going through source-filter modeling, cepstrum, quefrencies, LPC, MFCC and so on, I\u2019ll skip all that stuff and focus on the core elements related to our problem, so that the article is digestible by as many people as possible and not exclusively by the Audio Signal Processing / Speech community.", "So, what does the structure of human speech tell us?", "Well, there are 3 main elements that we can identify here:", "Let\u2019s forget for a second about this thing called Machine Learning. Based on our knowledge about the data, can we come up with a method to extract our vocals? Let me give it a try\u2026", "If we do a decent job, the output of this process should be a soft or binary mask that, when applied (element-wise multiplication) to the magnitude STFT of the mix, gives us an approximate reconstruction of the magnitude STFT of the vocals. From there, we then combine this vocal STFT estimate with the phase information of the original mix, compute an inverse STFT, and obtain the time-domain signal of the reconstructed vocals.", "Doing this from scratch is already a lot of work. But for the sake of the demonstration, we\u2019re going to use an implementation of the pYIN algorithm. Even though it\u2019s meant for solving step 3, with the right constraints it takes care of 1 and 2 pretty decently, while tracking the vocal fundamental even in the presence of music. The example below contains the output from this approach, without addressing the unvoiced sections.", "Well\u2026? It sort of did the trick but the quality of the recovered vocal is not there yet. Maybe with additional time, energy and budget we can improve this method and get it to a better place.", "What happens when you have multiple vocals, which is definitely the case in at least 50% of today\u2019s professionally produced tracks?", "What happens when the vocals have been processed with reverberation, delays and other effects? Let\u2019s take a look at the last chorus of Ariana Grande\u2019s One Last Time.", "Are you feeling the pain already\u2026? I am.", "Very soon, ad-hoc methods like the one described above become a house of cards. The problem is just too complex. There are too many rules, too many exceptions to the rules and too many varying conditions (effects and different mixing settings). The multi-step approach also implies that errors in one step propagate issues to the step that comes after. Improving each step would be very costly, it would require a large number of iterations to get right and last but not least, we would probably end up with a computationally expensive pipeline, something that by itself can be a deal-breaker.", "These are the kind of scenarios where we need to start thinking of a more end-to-end approach and let ML figure out -PART- of the underlying processes and operations required to solve the problem. However, we are not throwing the towel when it comes to feature engineering and you\u2019ll see why.", "Inspired by the achievements with CNNs on natural images, why not apply the same reasoning here?", "At the end of the day, we know we can represent an audio signal \u2018as an image\u2019 using the Short-Time Fourier Transform right? Even though these audio images don\u2019t follow the statistical distribution of natural images, they still expose spatial patterns (in the time vs frequency space) that we should be able to learn from.", "At the time, validating this experiment was a costly endeavor, because obtaining or generating the training data required was already a big investment. One of the practices I always try to implement in applied research is to first identify a simpler problem that validates the same principles as the original one, but that does not require as much work. This allows you to keep your hypotheses smaller, iterate faster and pivot with minimum impact when things don\u2019t work as expected.", "An implied condition for the original solution to work is that a CNN must be capable of understanding the structure of human speech. A simpler problem can then be: given a mix fragment, let\u2019s see if a CNN can classify these fragments as containing vocal content or not. We are looking at a music-robust Vocal Activity Detector (VAD), implemented as a binary classifier.", "We know that audio signals such as music and human speech embed temporal dependencies. In simpler terms, nothing happens in isolation at a given time-frame. If I want to know whether a given section of audio contains human speech or not, I should probably look at the neighbor regions as well. That temporal context can give me good information about what\u2019s going on in the region of interest. At the same time, we want to perform our classification in very small time increments, so that we can capture the human voice with the highest temporal resolution possible.", "With the above requirements, the input/output data to our binary classifier looks like this:", "Using Keras, we can build a small CNN model to validate our hypothesis.", "With an 80/20 train-test split and after ~50 epochs we reach ~97% test accuracy, which means there\u2019s enough evidence that our CNN model can discriminate between music sections containing vocal content and music sections without vocal content. By inspecting some of the feature maps coming out of our 4th convolutional layer, it looks like our network has optimized its kernels to perform 2 tasks: filtering out music and filtering out vocals\u2026", "Now that we\u2019ve validated this simpler classification problem, how do we go from detecting vocal activity in music all the way to isolating vocals from music? Well, rescuing some ideas from our naive method described at the beginning, we still want to somehow end up with an estimate of the vocal\u2019s magnitude spectrogram. This now becomes a regression problem. What we want to do is, given a time-frame from the STFT of the mix (with enough temporal context), estimate the corresponding vocal time-frame\u2019s magnitude spectrum.", "What about the training set? (you might be asking yourself at this point)", "oh Lord\u2026 that was something. I\u2019m gonna address this at the end of the article so that we don\u2019t switch contexts yet!", "If our model learns well, during inference, all we need to do is implement a simple sliding window over the STFT of the mix. After each prediction, we move our window to the right by 1 time-frame, predict the next vocal frame and concatenate it with the previous prediction. In regards to the model, we can start by using the same model we used for VAD as a baseline and by making some changes (output shape is now (513,1), linear activation at the output, MSE as loss function), we can begin our training.", "Although the above input/output representation makes sense, after training our vocal separation model several times, with varying parameters and data normalizations, the results are not there yet. It seems like we are asking for too much\u2026", "We went from a binary classifier to trying to do regression on a 513-dimensional vector. Although the network learns the task to a degree, after reconstructing the vocal\u2019s time domain signal, there are obvious artifacts and interferences from other sources. Even after adding more layers and increasing the number of model parameters, the results don\u2019t change much.", "So then the question became: can we trick the network into thinking it is solving a simpler problem and still achieve the desired results?", "What if instead of trying to estimate the vocal\u2019s magnitude STFT, we trained the network to learn a binary mask that, when applied to the STFT of the mix, gives us a simplified but perceptually-acceptable-upon-reconstruction estimate of the vocal\u2019s magnitude spectrogram?", "By experimenting with different heuristics, we came up with a relatively simple (and definitely unorthodox from a Signal Processing perspective) way to extract singing voice from mixes using binary masks. Without going too much into the details, we are going to think of the output as a binary image where, a value of \u20181\u2019 indicates predominant presence of vocal content at a given frequency and timeframe location, and a value of \u20180\u2019 indicates predominant presence of music at the given location. Visually, it looks pretty unattractive, but upon reconstructing the time domain signal, the results are surprisingly good.", "Our problem now becomes some sort of regression-classification hybrid. We are asking the model to \u201cclassify pixels\u201d at the output as vocal or non-vocal, although conceptually (and also in terms of the loss function used -MSE- ), the task is still a regression one.", "Although the distinction might not seem relevant to some, it actually makes a huge difference in the model\u2019s ability to learn the assigned task, the second one being way more simple and constrained. At the same time, it allows us to keep our model relatively small in terms of number of parameters considering the complexity of the task, something highly desired for real-time operation, which was a design requirement in this case. After some minor tweaks, the final model looks like this.", "Basically, as described in the naive method section. In this case, for every inference pass that we do, we are predicting a single timeframe of the vocals\u2019 binary mask. Again, by implementing a simple sliding window with a stride of one timeframe, we keep estimating and concatenating consecutive timeframes, which end up making up the whole vocal binary mask.", "As you know, one of the biggest pain points in supervised Machine Learning (leave aside all those toy examples with available datasets out there) is having the right data (amount and quality) for the particular problem that you\u2019re trying to solve. Based on the input/output representations described, in order to train our model, we first needed a significant number of mixes and their corresponding, perfectly aligned and normalized vocal tracks. There\u2019s more than one way to build this dataset and here we used a combination of strategies, ranging from manually creating mix <> vocal pairs with some acapellas found online, to finding RockBand stems, to web-scraping Youtube. Just to give you an idea of what part of this time-consuming and painful process looked like, our \u201cdataset project\u201d involved creating a tool to automatically build mix <> vocal pairs as illustrated below:", "We knew we needed a good amount of data for the network to learn the transfer function needed to map mixes into vocals. Our final dataset consisted of around 15M examples of ~300-millisecond fragments of mixes and their corresponding vocal binary masks.", "Building a Machine Learning model for a given task is only part of the deal. In production environments, we need to think about software architecture, pipelines, and optimization strategies, especially when we\u2019re dealing with real-time.", "For this particular problem the reconstruction into the time-domain can be done all at once after predicting a full vocal binary mask (offline mode) or, more interestingly, as part of a multithreaded pipeline where we acquire, process, reconstruct and playback in small segments, allowing this to be streaming-friendly, and even capable of delivering real-time deconstruction on music that\u2019s being recorded on the fly, with minimum latency. Given this is a whole topic on its own, I\u2019m going to leave it for another article focused on real-time ML pipelines\u2026", "This article is extensive enough already but given you\u2019ve made it this far I thought you deserved to see one last demo. With the exact same reasoning for extracting vocal content, we can try to split a stereo track into STEMs (drums, bassline, vocals, others) by making some modifications to our model and of course, by having the appropriate training set :)", "Ready to go down the rabbit hole again? Here\u2019s Part 2!", "Thanks for reading and don\u2019t hesitate in leaving questions. I will keep writing articles on Audio AI so stay tuned! As a final remark, you can see that the actual CNN model we ended up building is not that special. The success of this work has been driven by focusing on the Feature Engineering aspect and by implementing a lean process for hypotheses validations, something I\u2019ll be writing about in the near future!", "ps: shoutouts to Naveen Rajashekharappa and Karthiek Reddy Bokka for their contributions to this work!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Head of AI/ML & Audio Science Innovation @Splice . Mentor @Techstars | Advisor @BrkThroughT | USC, Fulbright alum. Love for audio, music and travel."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F210532383785&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faudio-ai-isolating-vocals-from-stereo-music-using-convolutional-neural-networks-210532383785&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faudio-ai-isolating-vocals-from-stereo-music-using-convolutional-neural-networks-210532383785&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faudio-ai-isolating-vocals-from-stereo-music-using-convolutional-neural-networks-210532383785&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faudio-ai-isolating-vocals-from-stereo-music-using-convolutional-neural-networks-210532383785&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----210532383785--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----210532383785--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@ale.koretzky?source=post_page-----210532383785--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ale.koretzky?source=post_page-----210532383785--------------------------------", "anchor_text": "Ale Koretzky"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1a05d06f496d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faudio-ai-isolating-vocals-from-stereo-music-using-convolutional-neural-networks-210532383785&user=Ale+Koretzky&userId=1a05d06f496d&source=post_page-1a05d06f496d----210532383785---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F210532383785&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faudio-ai-isolating-vocals-from-stereo-music-using-convolutional-neural-networks-210532383785&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F210532383785&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faudio-ai-isolating-vocals-from-stereo-music-using-convolutional-neural-networks-210532383785&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://ccrma.stanford.edu/~njb/teaching/sstutorial/", "anchor_text": "these tutorial mini-series"}, {"url": "https://en.wikipedia.org/wiki/Short-time_Fourier_transform", "anchor_text": "Short-Time Fourier Transform (STFT)"}, {"url": "https://code.soundsoftware.ac.uk/projects/pyin", "anchor_text": "pYIN algorithm"}, {"url": "https://en.wikipedia.org/wiki/Voice_activity_detection", "anchor_text": "Vocal Activity Detector (VAD)"}, {"url": "https://en.wikipedia.org/wiki/Mel_scale", "anchor_text": "Mel scale"}, {"url": "https://towardsdatascience.com/audio-ai-isolating-instruments-from-stereo-music-using-convolutional-neural-networks-584ababf69de", "anchor_text": "Audio AI: isolating instruments from stereo music using Convolutional Neural Networkshacking music towards the democratization of derivative contenttowardsdatascience.com"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----210532383785---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/signal-processing?source=post_page-----210532383785---------------signal_processing-----------------", "anchor_text": "Signal Processing"}, {"url": "https://medium.com/tag/audio?source=post_page-----210532383785---------------audio-----------------", "anchor_text": "Audio"}, {"url": "https://medium.com/tag/music?source=post_page-----210532383785---------------music-----------------", "anchor_text": "Music"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----210532383785---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F210532383785&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faudio-ai-isolating-vocals-from-stereo-music-using-convolutional-neural-networks-210532383785&user=Ale+Koretzky&userId=1a05d06f496d&source=-----210532383785---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F210532383785&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faudio-ai-isolating-vocals-from-stereo-music-using-convolutional-neural-networks-210532383785&user=Ale+Koretzky&userId=1a05d06f496d&source=-----210532383785---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F210532383785&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faudio-ai-isolating-vocals-from-stereo-music-using-convolutional-neural-networks-210532383785&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----210532383785--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F210532383785&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faudio-ai-isolating-vocals-from-stereo-music-using-convolutional-neural-networks-210532383785&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----210532383785---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----210532383785--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----210532383785--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----210532383785--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----210532383785--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----210532383785--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----210532383785--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----210532383785--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----210532383785--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ale.koretzky?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ale.koretzky?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ale Koretzky"}, {"url": "https://medium.com/@ale.koretzky/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "764 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1a05d06f496d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faudio-ai-isolating-vocals-from-stereo-music-using-convolutional-neural-networks-210532383785&user=Ale+Koretzky&userId=1a05d06f496d&source=post_page-1a05d06f496d--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F7e5ff42ee152&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faudio-ai-isolating-vocals-from-stereo-music-using-convolutional-neural-networks-210532383785&newsletterV3=1a05d06f496d&newsletterV3Id=7e5ff42ee152&user=Ale+Koretzky&userId=1a05d06f496d&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}