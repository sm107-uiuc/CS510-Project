{"url": "https://towardsdatascience.com/expectation-maximization-explained-c82f5ed438e5", "time": 1683010695.7348921, "path": "towardsdatascience.com/expectation-maximization-explained-c82f5ed438e5/", "webpage": {"metadata": {"title": "Expectation Maximization Explained | by Ravi Charan | Towards Data Science", "h1": "Expectation Maximization Explained", "description": "Expectation Maximization (EM) is a classic algorithm developed in the 60s and 70s with diverse applications from unsupervised clustering to NLP"}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation#Likelihood_maximization", "anchor_text": "Latent Dirichlet Allocation", "paragraph_index": 0}, {"url": "https://en.wikipedia.org/wiki/Baum%E2%80%93Welch_algorithm", "anchor_text": "Baum\u2013Welch", "paragraph_index": 0}, {"url": "https://en.wikipedia.org/wiki/Old_Faithful", "anchor_text": "Old Faithful", "paragraph_index": 3}, {"url": "https://en.wikipedia.org/wiki/K-means_clustering", "anchor_text": "k-means clustering", "paragraph_index": 4}, {"url": "https://en.wikipedia.org/wiki/Chain_rule_(probability)", "anchor_text": "chain rule", "paragraph_index": 24}, {"url": "https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence", "anchor_text": "Kullback-Leibler (KL) Divergence", "paragraph_index": 35}, {"url": "https://en.wikipedia.org/wiki/Variational_Bayesian_methods", "anchor_text": "variational Bayesian", "paragraph_index": 42}, {"url": "https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm#As_a_maximization%E2%80%93maximization_procedure", "anchor_text": "maximization-maximization", "paragraph_index": 42}, {"url": "https://web.stanford.edu/~hastie/ElemStatLearn/", "anchor_text": "Elements of Statistical Learning", "paragraph_index": 43}, {"url": "https://medium.com/u/e4cecbc19a01?source=post_page-----c82f5ed438e5--------------------------------", "anchor_text": "Fangfang Lee", "paragraph_index": 43}, {"url": "https://en.wikipedia.org/wiki/Variational_Bayesian_methods", "anchor_text": "variational Bayes", "paragraph_index": 44}, {"url": "https://en.wikipedia.org/wiki/Variational_Bayesian_methods#:~:text=Variational%20Bayes%20can%20be%20seen,distribution%20of%20the%20parameters%20and", "anchor_text": "extension", "paragraph_index": 44}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html", "anchor_text": "sklearn", "paragraph_index": 44}, {"url": "https://medium.com/u/504c7870fdb6?source=post_page-----c82f5ed438e5--------------------------------", "anchor_text": "Medium", "paragraph_index": 45}], "all_paragraphs": ["Expectation Maximization (EM) is a classic algorithm developed in the 60s and 70s with diverse applications. It can be used as an unsupervised clustering algorithm and extends to NLP applications like Latent Dirichlet Allocation\u00b9, the Baum\u2013Welch algorithm for Hidden Markov Models, and medical imaging. As an optimization procedure, it is an alternative to gradient descent and the like with the major advantage that, in many circumstances, the updates can be computed analytically. More than that, it is a flexible framework for thinking about optimization.", "In this article, we\u2019ll start with a simple clustering example and then discuss the algorithm in generality.", "Consider the situation where you have a variety of data points with some measurements of them. We wish to assign them to different groups.", "In the example here we have data on eruptions of the iconic Old Faithful geyser in Yellowstone. For each eruption, we have measured its length and the time since the previous eruption. We might suppose that there are two \u201ctypes\u201d of eruptions (red and yellow in the diagram), and for each type of eruption the resulting data is generated by a (multivariate) Normal distribution. This is called a Gaussian Mixture Model, incidentally.", "Similar to k-means clustering, we start with a random guess for what those two distributions/clusters are and then proceed to improve iteratively by alternating two steps:", "Note that unlike in k-means clustering, our model is generative: it purports to tell us the process by which the data was generated. And we could, in turn, re-sample the model to generate more (fake) data.", "Got it? Now we are going to do a 1-dimensional example with equations.", "Consider data points with a single measurement x. We suppose these are generated by 2 clusters each following a normal distribution N(\u03bc, \u03c3\u00b2). The probability of data being generated by the first cluster is \u03c0.", "So we have 5 parameters: a mixing probability \u03c0, and a mean \u03bc and standard deviation \u03c3 for each cluster. I will denote them collectively as \u03b8.", "What is the probability of observing a datapoint with value x? Let the normal distribution\u2019s probability-density function be denoted by \u03d5. To keep notation less cluttered, I will use the standard deviation as a parameter instead of the usual variance.", "The probability (likelihood) of observing our entire dataset of n points is:", "And we typically choose to take the logarithm of this to turn our product into a more manageable sum, the log-likelihood.", "Our goal is to maximize this: we want our parameters to be the ones where it is most likely that we observe the data we observed (a Maximum Likelihood Estimator).", "Now the question is, how are we going to optimize it? Doing so directly and analytically would be tricky because of the sum in the logarithm.", "The trick is to imagine there is a latent variable which we will call \u0394. It is a binary (0/1-valued) variable that determines whether a point is in cluster 1 or cluster 2. If we know \u0394 for each point, it would be very easy to calculate the maximum-likelihood estimates of our parameters. For convenience to match our choice of \u0394 being 1 for the second cluster, we will switch \u03c0 to be the probability of a point being in the second cluster.", "Notice that the sums are now outside the logarithm. Also, we pick up an extra sum to account for the likelihood of observing each \u0394.", "Supposing counterfactually that we did observe \u0394, the maximum likelihood estimates are easy to form. For \u03bc, take the sample mean within each cluster; for \u03c3, likewise the standard deviation (the population formula, which is the MLE). For \u03c0, the sample proportion of points in the second cluster. These are the usual maximum likelihood estimators for each parameter.", "Of course, we didn\u2019t observe \u0394. The solution to this is the heart of the Expectation-Maximization algorithm. Our plan is:", "Again, you may find it helpful to think about k-means clustering, where we do the same thing. In k-means clustering, we assign each point to the closest centroid (expectation step). In essence, this is a hard estimate of \u0394. Hard because it is 1 for one of the clusters and 0 for all the others. Then we update the centroids to be the mean of the points in the cluster (maximization step). This is the maximum-likelihood estimator for \u03bc. In k-means clustering, the \u201cmodel\u201d for the data doesn\u2019t have a standard deviation. (The \u201cmodel\u201d is in scare quotes because it isn\u2019t generative).", "In our example, we will instead do a soft assignment of \u0394. We sometimes call this the responsibility (how responsible is each cluster for each observation). We will denote the responsibility as \u0263.", "Now we can write down the full algorithm for this example. But before we do so, we will quickly review a table of symbols we\u2019ve defined (there were a lot).", "Note that the estimates of \u03bc and \u03c3 for cluster 1 are analogous but using 1\u2013\u0263 as the weights instead.", "Now that we have given an example of the algorithm, you hopefully have a feel for it. We\u2019ll move on to discussing the algorithm in general. This basically amounts to dressing up everything we did with slightly more complicated variables. And it will put us in a position to explain why it works.", "Let\u2019s move onto the general setting. Here is the setup:", "Using P to denote the probability, we can now use the chain rule to write:", "The notation can be subtle here. All three terms take the parameters \u03b8 as a given.", "We can take logarithms and rearrange terms. Then in the second line we will make a notation change (and a confusing one at that. Don\u2019t blame me, I didn\u2019t invent it):", "For the first two terms, it\u2019s worth reviewing what they are in the context of our previous example. The first, \u2113(\u03b8; X), is the one we want to optimize. The second, \u2113(\u03b8; X, \u0394), was the one that was analytically tractable.", "Now, remember that I said we can compute the conditional distribution \u0394|X given the parameters \u03b8? This is where things get wild.", "We are going to introduce a second set of the same parameters, call it \u03b8\u02b9. I will also sometimes denote it with a hat (circumflex) over it, like the one this \u201c\u00ea\u201d has.\u00b2 Think of this set of parameters as our current estimate. The \u03b8 currently in our formulas will be optimized to improve our estimate.", "Now we are going to take the expectation of the log likelihoods with respect to the conditional distribution \u0394|X, \u03b8\u02b9 \u2013 namely, the distribution of our latent variables given the data and our current parameter estimate.", "The term on the left hand side doesn\u2019t change since it doesn\u2019t know/care about \u0394 anyways (it\u2019s a constant). Again, the expectation is over the possible values of \u0394. If you are following along with respect to our example, the term \u2113(\u03b8; X, \u0394) changes after we take the expectation so that \u0394 is replaced by \u0263.", "Now, very quickly, to ameliorate the notational nightmare we\u2019ve got going on here, let\u2019s introduce shorthand notation for the two expectations we have on the right hand side", "The heavy lifting to prove this will work is to consider the function R(\u03b8, \u03b8\u02b9). The claim is that R is maximized when \u03b8=\u03b8\u02b9. In lieu of a full proof let\u2019s think about what R computes. Stripping away the dependence on the data X (which is shared between the distribution we are taking an expectation over and the likelihood function), R schematically becomes", "In other words, we have two probability distributions. We use one (parameterized by \u03b8\u02b9) to generate data \u0394 and we use the other (parameterized by \u03b8) to compute the probability of what we saw. If \u0394 represents just a single number and the distributions have probability-density functions, we could write (again, schematically)", "I have suggestively written this in a form similar to that of the Kullback-Leibler (KL) Divergence which is (almost) a measure of the distance between two probability distributions. If we subtract R(q||p) from a constant R(p||p) we will get the KL-divergence which is bounded below at 0 and is only 0 when q=p. (The only thing a distance\u00b3 0 from the distribution p is p itself). In other words, R is maximized when q=p. This is a standard result about the KL-divergence and may be proved with Jensen\u2019s inequality.\u2074", "Now the only thing left to do is consider the difference between the likelihoods before and after our update step:", "We chose the new parameters to maximize Q, so the first term is definitely positive. By the argument above, R is maximized by taking the old parameters as its first argument, so the second term must be negative. Positive minus negative is positive. Therefore the likelihood has increased at each update step. Each step is guaranteed to make things better.", "Notice also that we don\u2019t have to optimize Q. All we have to is find some way to make it better and our updates are still guaranteed to make things better.", "Hopefully, you now have a good feel for the algorithm. In terms of the mathematics, the key equation is just the likelihoods below. After that, we just take expectations with respect to the old parameters (the expectation step) and show that we\u2019re fine to just optimize the first term on the right hand side. As we motivated with the Gaussian Mixture Model example, this second term is often easier to optimize. The third term we don\u2019t have to worry about, it won\u2019t mess anything up.", "Stepping back a bit, I want to emphasize the power and usefulness of the EM algorithm. First of all, it represents the idea that we can introduce latent variables and then compute by alternately dealing with the latent variables (taking the parameters as fixed and known) and dealing with the parameters (taking the latent variables as fixed and known). This is a powerful idea that you\u2019ll see in a variety of contexts.", "Second, the algorithm is inherently fast because it doesn\u2019t depend on computing gradients. Anytime you can solve a model analytically (like using a linear regression), it\u2019s going to be faster. And this lets us take analytically intractable problems and solve parts of them analytically, extending that power to an iterative context.", "Finally, I want to note that there is plenty more to say about the EM algorithm. It generalizes to other forms of doing the maximization step and to variational Bayesian techniques and can be understood in different ways (for example as maximization-maximization or as alternating projections to a submanifold under mutually dual affine connections on a statistical manifold (the e- and m- connections)). More on that in the future!", "This discussion largely follows that in the Elements of Statistical Learning, though a bit more deliberate pace. Special thanks to Fangfang Lee for informing me about this wonderful algorithm.", "[1] Latent Dirichlet Allocation is often fit with variational Bayes methods, an extension of Expectation Maximization. See the sklearn implementation, for example.", "[2] Impossible to type a hat over a \u03b8 on this platform. Come on Medium please give us LaTeX.", "[3] I did not put distance in quotes because I am not referring to the Kullback-Leibler divergence (which is not a metric) but to the Fisher information metric (which is). The two are related in a deep way; for us we can just say that the KL-divergence is only 0 when the actual distance is 0.", "[4] The standard proof of this seems to be a handwavy appeal to Jensen\u2019s inequality. My version is also handwavy but incorporates Jensen\u2019s through the KL-divergence instead.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Scientist, Mathematician. Formerly @MIT, @McKinsey, currently teaching computers to read"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fc82f5ed438e5&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexpectation-maximization-explained-c82f5ed438e5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexpectation-maximization-explained-c82f5ed438e5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexpectation-maximization-explained-c82f5ed438e5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexpectation-maximization-explained-c82f5ed438e5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----c82f5ed438e5--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c82f5ed438e5--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@rmcharan?source=post_page-----c82f5ed438e5--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@rmcharan?source=post_page-----c82f5ed438e5--------------------------------", "anchor_text": "Ravi Charan"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F393ce2bbf82c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexpectation-maximization-explained-c82f5ed438e5&user=Ravi+Charan&userId=393ce2bbf82c&source=post_page-393ce2bbf82c----c82f5ed438e5---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc82f5ed438e5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexpectation-maximization-explained-c82f5ed438e5&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc82f5ed438e5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexpectation-maximization-explained-c82f5ed438e5&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@marvin_ronsdorf?utm_source=medium&utm_medium=referral", "anchor_text": "Marvin Ronsdorf"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation#Likelihood_maximization", "anchor_text": "Latent Dirichlet Allocation"}, {"url": "https://en.wikipedia.org/wiki/Baum%E2%80%93Welch_algorithm", "anchor_text": "Baum\u2013Welch"}, {"url": "https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm", "anchor_text": "Wikipedia"}, {"url": "https://en.wikipedia.org/wiki/Old_Faithful", "anchor_text": "Old Faithful"}, {"url": "https://en.wikipedia.org/wiki/K-means_clustering", "anchor_text": "k-means clustering"}, {"url": "https://en.wikipedia.org/wiki/Chain_rule_(probability)", "anchor_text": "chain rule"}, {"url": "https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence", "anchor_text": "Kullback-Leibler (KL) Divergence"}, {"url": "https://en.wikipedia.org/wiki/Variational_Bayesian_methods", "anchor_text": "variational Bayesian"}, {"url": "https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm#As_a_maximization%E2%80%93maximization_procedure", "anchor_text": "maximization-maximization"}, {"url": "https://web.stanford.edu/~hastie/ElemStatLearn/", "anchor_text": "Elements of Statistical Learning"}, {"url": "https://medium.com/u/e4cecbc19a01?source=post_page-----c82f5ed438e5--------------------------------", "anchor_text": "Fangfang Lee"}, {"url": "https://en.wikipedia.org/wiki/Variational_Bayesian_methods", "anchor_text": "variational Bayes"}, {"url": "https://en.wikipedia.org/wiki/Variational_Bayesian_methods#:~:text=Variational%20Bayes%20can%20be%20seen,distribution%20of%20the%20parameters%20and", "anchor_text": "extension"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html", "anchor_text": "sklearn"}, {"url": "https://medium.com/u/504c7870fdb6?source=post_page-----c82f5ed438e5--------------------------------", "anchor_text": "Medium"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----c82f5ed438e5---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/tag/mathematics?source=post_page-----c82f5ed438e5---------------mathematics-----------------", "anchor_text": "Mathematics"}, {"url": "https://medium.com/tag/statistics?source=post_page-----c82f5ed438e5---------------statistics-----------------", "anchor_text": "Statistics"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----c82f5ed438e5---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----c82f5ed438e5---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc82f5ed438e5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexpectation-maximization-explained-c82f5ed438e5&user=Ravi+Charan&userId=393ce2bbf82c&source=-----c82f5ed438e5---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc82f5ed438e5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexpectation-maximization-explained-c82f5ed438e5&user=Ravi+Charan&userId=393ce2bbf82c&source=-----c82f5ed438e5---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc82f5ed438e5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexpectation-maximization-explained-c82f5ed438e5&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c82f5ed438e5--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fc82f5ed438e5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexpectation-maximization-explained-c82f5ed438e5&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----c82f5ed438e5---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----c82f5ed438e5--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----c82f5ed438e5--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----c82f5ed438e5--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----c82f5ed438e5--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----c82f5ed438e5--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----c82f5ed438e5--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----c82f5ed438e5--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----c82f5ed438e5--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@rmcharan?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@rmcharan?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ravi Charan"}, {"url": "https://medium.com/@rmcharan/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "599 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F393ce2bbf82c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexpectation-maximization-explained-c82f5ed438e5&user=Ravi+Charan&userId=393ce2bbf82c&source=post_page-393ce2bbf82c--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F6bca6dd641ca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexpectation-maximization-explained-c82f5ed438e5&newsletterV3=393ce2bbf82c&newsletterV3Id=6bca6dd641ca&user=Ravi+Charan&userId=393ce2bbf82c&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}