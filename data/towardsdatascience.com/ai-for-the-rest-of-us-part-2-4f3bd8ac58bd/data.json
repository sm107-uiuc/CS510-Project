{"url": "https://towardsdatascience.com/ai-for-the-rest-of-us-part-2-4f3bd8ac58bd", "time": 1682995582.5488021, "path": "towardsdatascience.com/ai-for-the-rest-of-us-part-2-4f3bd8ac58bd/", "webpage": {"metadata": {"title": "AI For the Rest of Us: Part 2. Explaining how AI works for non-coders\u2026 | by Nishul Saperia | Medium", "h1": "AI For the Rest of Us: Part 2", "description": "This is part 2 of a post that is seeking to explain AI in a way anyone can understand it. Part 1 had some background and introduction. This post has an example hopefully anyone can follow! For ease\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/@nishul1/ai-for-the-rest-of-us-part-1-cfe1087918a5", "anchor_text": "Part 1", "paragraph_index": 0}], "all_paragraphs": ["This is part 2 of a post that is seeking to explain AI in a way anyone can understand it. Part 1 had some background and introduction. This post has an example hopefully anyone can follow!", "For ease of understanding I will use a simple example. The process for more complicated examples is the same \u2014 just (many!) more numbers to implement!", "My simple example is \u2014 how many ice creams are sold in a given day based on what the temperature is. This example is so simple that you would never use machine learning to figure this out \u2014 but it still can be used as an example for how a machine actually learns.", "Let\u2019s say you have the following data:", "As you would expect, the number of ice creams sold increases as the temperature goes up.", "Let\u2019s start with a guess of the relationship between temperature and ice creams sold.", "Let\u2019s say my first guess is that whatever the temperature at least 80 ice creams will be sold (even when it is zero degrees \u2014 people still like to eat ice cream all year round right? I know I do!) but that for every increase in temperature the number of ice creams sold goes up by 12.", "If this was expressed in a formula, it would be:", "i = Guess of how many ice creams sold", "If you don\u2019t like algebra, you can just say it in words:", "Ice creams sold = 80 plus (12 times the temperature).", "The brackets mean that whatever is in the brackets is calculated first before other parts of the calculation.", "My machine will then use this initial guessed formula (called a \u2018hypothesis\u2019) and figure out what answers it gets:", "As you can see, we have generally underestimated with our guesses. I have put in a column which shows the error for each guess. The error is all negative i.e. we have guessed less than the amount that was actually sold.", "Note also I have calculated also for each guess \u2014 the error squared. This is the \u2018error times error\u2019 (ie the error multiplied by itself). This is usually done for two reasons:", "After all this, we calculate the total sum of the squared error at the bottom of the table \u2014 we get 68,132. We divide this number by the total number of data points we have \u2014 11 \u2014 and we get what is known as the \u2018cost\u2019 in machine learning. This is just under 6194.", "This cost gives us a number by which we can measure how wrong our guesses are. Our goal is simple: to make this number as small as it can possibly be!", "The next step for the machine is to figure out how to make a better second guess.", "Let\u2019s take a look at the formula we have:", "The two things we can change to make this better are the number 80 and the number 12. They are known as the \u2018coefficients\u2019 (or parameters) in the formula. You may have come across that term when studying maths at school.", "We want to come up with coefficients that produce more accurate guesses. We do that by first examining how the cost changes when we make an adjustment to the coefficients.", "Let\u2019s look at our guess for the second data point:", "How does the cost change as the we adjust the first coefficient (80) by a small amount (while keeping the second one the same)?", "As you can see \u2014 the cost goes down by about 80 as the coefficient goes up by 1.", "This tells us how fast the cost goes up or down as we adjust the coefficient, and is a key step to making a better guess. It\u2019s called the \u2018rate of change\u2019 for the cost.", "Rate of change of cost for coefficient 1 for this data point = -80", "So now we know we need to increase the coefficient to reduce the cost.", "We do the same for how the second coefficient (12) changes the error (this time keeping the first coefficient the same):", "Great! Again we can see we want to increase this coefficient. This time the \u2018rate of change\u2019 is (2025\u20131225) / 2 = -400. This means the cost goes down by 400 every time you increase coefficient 2 by 1.", "Rate of change of cost for coefficient 2 for this data point = -400", "So \u2014 we now know in what direction we need to adjust the coefficients \u2014 for this guess at least. We need to increase them both.", "You then repeat the process above for all the other guesses. Note this number is different for all the different data points. I have listed them here. Feel free to calculate them yourself! The process is the same as above.", "(Before we move on \u2014 let\u2019s call the \u2018rate of change of cost\u2019 (or in simpler terms \u2014 how much the cost changes when you change the coefficient) by a shorter simpler name \u2014 cost sensitivity. Much less of a mouthful.)", "Once we have calculated this \u2018cost sensitivity\u2019 for all guesses, we calculate the average. We then have a rough idea of how much the error goes down across all data points by changing the coefficients. This average is the overall change to the \u2018cost\u2019 for the whole dataset when you adjust each coefficient.", "Average cost sensitivity for coefficient 1 = -133", "Average cost sensitivity coefficient 2 = -2592", "For mathematical convenience when we implement AI algorithms we actually use a number that is half of what we have calculated above. So the actual values we would use are:", "Average cost sensitivity for coefficient 1 that we use = -66.55", "Average cost sensitivity coefficient 2 that we use = -1296.18", "So \u2014 now we know how much our error changes by changing the \u2018coefficients\u2019. Now we can use this information to guide us to making our second guess for what the coefficients should be!", "To make this second guess, we need to decide on how quickly we want our algorithm to learn. We define this by setting a number called the \u2018Learning Rate\u2019.", "This learning rate determines how much closer to the optimal answer we get with each new guess we make. The goal in setting this is to find a learning rate that allows us to learn quite quickly (i.e. improve our guesses at a fast rate) \u2014 but doesn\u2019t make us overshoot.", "First I will explain what we do with this learning rate.", "Above we calculated how much the cost changes as we adjust the coefficients (remember this is the \u2018cost sensitivity). We know in which direction we want to adjust the coefficient, but not necessarily the amount. This is primarily because the cost sensitivity itself changes when you adjust the coefficients eg if coefficient 2 is 12, the cost sensitivity is different than when coefficient 2 is 13.", "If it was the same, we could calculate the approximate amount to change the coefficient quite easily.", "Furthermore, the \u2018cost sensitivity\u2019 tends to be much higher far away from the optimal coefficient, and then much lower close to it. In fact \u2014 by definition the rate of change is effectively zero when you reach the optimal coefficients \u2014 see below how the gradient is zero in Picture 1 below at the optimal coefficient.", "The below is a typical chart of how you might see the cost vary with a coefficient:", "Note how when the cost is far away from the optimal level \u2014 it is very steep in the above graph i.e. it is changing a lot with small changes to the coefficient. This isn\u2019t always the case by any means but it is a general trend that as you reach the optimal number for that coefficient it becomes much flatter in the above graph.", "Hence, you can see it is hard to predict how much you should try to adjust for your second guess. You want to make an adjustment that isn\u2019t too big because then the below (in red) can happen:", "As can be seen \u2014 if you pick an adjustment that is too big you end up moving away from the right answer!", "Normally you pick something pretty small and see how fast it moves towards what looks like a good answer. The problem with picking something too small is that is takes a very long time to get to the answer which isn\u2019t ideal either. But at least you don\u2019t overshoot.", "In practice, a machine learning engineer will try test out a range of learning rates and pick one which seems to make our cost go down consistently and at a rate that doesn\u2019t seem too slow.", "We take the learning rate we have chosen, and multiply it by the cost sensitivity we calculated above \u2014 and we have our second guess!", "If we recap our how much the cost moves when we change each coefficient:", "Average cost sensitivity we use for coefficient 1 = -67", "Average cost sensitivity we use coefficient 2 = -1296", "You can see the total \u2018cost\u2019 changes a lot more when you change coefficient 2 versus coefficient 1.", "As mentioned, we take these numbers and multiply them by the learning rate \u2014 and we adjust our coefficients by this amount as per the following formula:", "New coefficient = Old coefficient \u2014 (learning rate * cost sensitivity for coefficient).", "I have tested a range of learning rates and selected 0.001. We can now calculate our new updated coefficients:", "Hence we now have a new estimate how many ice creams are sold:", "So \u2014 what do our new guesses end up being?", "The key to knowing if we have made progress is to ensure that the cost has gone down. And indeed we can see that it has!", "We can verify further that we are heading in the right direction by looking at our old guesses and comparing them to our new guesses:", "You can see that we have improved our guesses across the board! (This is in fact actually not always the case \u2014 sometimes you might find it gets worse on some data points but should expect it to go down overall).", "Looking at our guess for when the temperature is higher \u2014 we have reduced the error massively!", "To continue, we repeat the above steps we did for our first set of guesses. Namely:", "We check that the cost from the new coefficients is less than the cost from before. In practice \u2014 we check it every 100 or 1000 (or more) iterations depending on how many total iterations the programme will need (for complicated ML programs this number can be very high). We don\u2019t panic if on one occasion it goes up \u2014 but if it keeps going up then something is wrong \u2014 very possibly our learning rate is too big.", "At some point, the cost stops changing in any meaningful way. It can be assumed that we have now found the optimal coefficients with the implementation we have and the machine has been trained!", "Whether it is accurate enough to be useful though would need some analysis though.", "The first thing to do after this is to have a separate test set to check that the machine works well on data it hasn\u2019t seen before. One danger of ML programmes is that they figure out a formula that works really well on the data it has seen \u2014 but doesn\u2019t actually work well for data it hasn\u2019t seen. It\u2019s become so customised to the original dataset that it is overly accurate on that \u2014 but not on new data. This phenomenon is known as \u2018overfitting\u2019 and there are various ways to avoid this, but which are out of the scope of this already very long post.", "Eventually, after creating a program that did the above 100,000 times (took just a few seconds \u2014 there is very little data unlike most other ML programmes!) I yielded my results:", "Let\u2019s see what this means for what the machine is predicting versus the actual results!", "Note how the predictions are pretty good now for some of the days. But there are some days where they were still quite a way off! In particular look at the last 2 lines where the temperature is 27 and 32. The algorithm had to get a bit stuck in the middle as it underestimates on one day by almost 74 and on another day by 68. Whichever way it went to try and reduce the error on one of those days, it is likely to increase the error on the other day.", "When you see results like this, a good question to ask is \u2014 what other information might I be missing? Maybe the day of the week matters? It\u2019s possible that the day when the temperature was 27 \u2014 it was a sunday and many people are having a leisurely day and feeling more like having an ice cream. And maybe the day it was 32 was a weekday when people are busy at work and not so much thinking about ice cream.", "Another thing to look at could be \u2018what was the weather on the days leading up to the day we are looking at?\u2019. If the day it was 27 the temperature had shot up from 17 the day before, you can imagine that people might have thought to treat themselves with an ice cream as it\u2019s a lot hotter. If though on the day it was 32 it had been the same temperature for the past week \u2014 maybe people had eaten a lot of ice cream already and were getting over it.", "This demonstrates how creating and designing these \u2018artificial intelligence\u2019 algorithms actually still need \u2018human intelligence\u2019 in deciding what data should be used and how.", "The reality is that to model the above accurately \u2014 you would want to look at quite a few different data points \u2014 not just temperature. This applies in any machine learning programme that is worth doing.", "What I\u2019ve written above is just the first step in understanding what machine learning is. There is a lot more to know \u2014 but again I hope to be able to find ways to explain the more complicated things that have a lot of fancy words \u2014 like neural networks, convolutional neural networks, logistic regression, unsupervised learning \u2014 even cutting edge techniques like generative algorithms (eg when an algorithm creates art!) etc \u2014 in a way that is distilled so that you can follow in principle what is happening inside. I aim to write more of these and will be doing so over the coming few months. I hope this one makes sense to you!", "Any feedback on how easy it was to follow this post would be most welcome.", "Founding team of @Markit. Too many interests to mention....and my own good!", "Founding team of @Markit. Too many interests to mention....and my own good!"], "all_outgoing_urls": [{"url": "https://towardsdatascience.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/@nishul1?source=post_page-----4f3bd8ac58bd--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/@nishul1?source=post_page-----4f3bd8ac58bd--------------------------------", "anchor_text": "Nishul Saperia"}, {"url": "https://towardsdatascience.com/@nishul1/ai-for-the-rest-of-us-part-1-cfe1087918a5", "anchor_text": "Part 1"}, {"url": "https://towardsdatascience.com/tag/machine-learning?source=post_page-----4f3bd8ac58bd---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://towardsdatascience.com/tag/artificial-intelligence?source=post_page-----4f3bd8ac58bd---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://towardsdatascience.com/tag/simple?source=post_page-----4f3bd8ac58bd---------------simple-----------------", "anchor_text": "Simple"}, {"url": "https://towardsdatascience.com/@nishul1?source=post_page-----4f3bd8ac58bd--------------------------------", "anchor_text": "More from Nishul Saperia"}, {"url": "https://towardsdatascience.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Ff702ff8585a8&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40nishul1%2Fai-for-the-rest-of-us-part-2-4f3bd8ac58bd&newsletterV3=d7f623769f67&newsletterV3Id=f702ff8585a8&user=Nishul+Saperia&userId=d7f623769f67&source=-----4f3bd8ac58bd---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4f3bd8ac58bd--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----4f3bd8ac58bd--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----4f3bd8ac58bd--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----4f3bd8ac58bd--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----4f3bd8ac58bd--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----4f3bd8ac58bd--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----4f3bd8ac58bd--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/plans?source=upgrade_membership---two_column_layout_sidebar----------------------------------", "anchor_text": "Get unlimited access"}, {"url": "https://towardsdatascience.com/@nishul1?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/@nishul1?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Nishul Saperia"}, {"url": "https://towardsdatascience.com/@nishul1/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "338 Followers"}, {"url": "https://towardsdatascience.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Ff702ff8585a8&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40nishul1%2Fai-for-the-rest-of-us-part-2-4f3bd8ac58bd&newsletterV3=d7f623769f67&newsletterV3Id=f702ff8585a8&user=Nishul+Saperia&userId=d7f623769f67&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://towardsdatascience.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}