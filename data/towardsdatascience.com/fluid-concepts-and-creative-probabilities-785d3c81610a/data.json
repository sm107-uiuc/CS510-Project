{"url": "https://towardsdatascience.com/fluid-concepts-and-creative-probabilities-785d3c81610a", "time": 1682993896.654301, "path": "towardsdatascience.com/fluid-concepts-and-creative-probabilities-785d3c81610a/", "webpage": {"metadata": {"title": "Fluid concepts and creative probabilities | by Jacopo Tagliabue | Towards Data Science", "h1": "Fluid concepts and creative probabilities", "description": "Imagine someone came to you with the following puzzle: given the sequence of integers 0, 1, 2, \u2026, guess the next item. It does not seem that hard, does it? 3 will be our obvious answer as we would\u2026"}, "outgoing_paragraph_urls": [{"url": "https://tooso.ai/", "anchor_text": "Tooso", "paragraph_index": 4}, {"url": "https://en.wikipedia.org/wiki/Probabilistic_programming_language", "anchor_text": "often explained", "paragraph_index": 5}, {"url": "https://www.amazon.com/Fluid-Concepts-Creative-Analogies-Fundamental/dp/0465024750", "anchor_text": "Fluid Concepts and Creative Analogies", "paragraph_index": 6}, {"url": "https://en.wikipedia.org/wiki/Douglas_Hofstadter#cite_note-50", "anchor_text": "it was the first book sold on Amazon", "paragraph_index": 6}, {"url": "https://www.quantamagazine.org/to-build-truly-intelligent-machines-teach-them-cause-and-effect-20180515/", "anchor_text": "curve fitting", "paragraph_index": 9}, {"url": "https://github.com/probmods/webppl/tree/master/examples", "anchor_text": "WebPPL examples", "paragraph_index": 12}, {"url": "http://www.jacopotagliabue.it/webppl_tutorial.html", "anchor_text": "simple HTML page", "paragraph_index": 12}, {"url": "http://www.jacopotagliabue.it/webppl_tutorial.html#anchor_rolling_forward", "anchor_text": "here", "paragraph_index": 13}, {"url": "https://en.wikipedia.org/wiki/Tangram", "anchor_text": "Tangram", "paragraph_index": 16}, {"url": "https://probmods.org/chapters/introduction.html", "anchor_text": "many people", "paragraph_index": 17}, {"url": "http://www.pnas.org/content/110/45/18327.short", "anchor_text": "intuitive physics", "paragraph_index": 17}, {"url": "https://web.mit.edu/cocosci/Papers/Science-2015-Lake-1332-8.pdf", "anchor_text": "visual learning", "paragraph_index": 17}, {"url": "https://ai.stanford.edu/~ang/papers/nips01-discriminativegenerative.pdf", "anchor_text": "technical details", "paragraph_index": 19}, {"url": "http://www.jacopotagliabue.it/webppl_tutorial.html#anchor_rolling_forward", "anchor_text": "it runs", "paragraph_index": 23}, {"url": "https://webppl.readthedocs.io/en/master/inference/", "anchor_text": "inference method", "paragraph_index": 23}, {"url": "https://en.wikipedia.org/wiki/Bayes%27_theorem", "anchor_text": "Bayes\u2019 theorem", "paragraph_index": 26}, {"url": "https://en.wikipedia.org/wiki/Willard_Van_Orman_Quine", "anchor_text": "Willard", "paragraph_index": 27}, {"url": "https://en.wikipedia.org/wiki/Indeterminacy_of_translation", "anchor_text": "gavagai", "paragraph_index": 28}, {"url": "http://web.mit.edu/cocosci/Papers/XT-PsychRev-InPress.pdf", "anchor_text": "Xu and Tenenbaum", "paragraph_index": 32}, {"url": "http://www.jacopotagliabue.it/webppl_tutorial.html#gavagai", "anchor_text": "try changing priors, likelihood and observations", "paragraph_index": 35}, {"url": "https://probmods.org/chapters/conditioning.html", "anchor_text": "probmods", "paragraph_index": 36}, {"url": "http://mbmlbook.com/toc.html?twitter=@bigdata", "anchor_text": "WIP", "paragraph_index": 36}, {"url": "https://www.theregister.co.uk/2018/10/05/imicrosoft_nfernet/", "anchor_text": "are getting into PP", "paragraph_index": 36}, {"url": "http://www.jacopotagliabue.it/webppl_tutorial.html#sequence_basic", "anchor_text": "here", "paragraph_index": 46}, {"url": "https://probmods.org/exercises/lot-learning.html", "anchor_text": "Goodman & Tenenbaum", "paragraph_index": 47}, {"url": "https://probmods.org/exercises/lot-learning.html", "anchor_text": "Goodman & Tenenbaum", "paragraph_index": 51}, {"url": "http://www.jacopotagliabue.it/webppl_tutorial.html#sequence_basic", "anchor_text": "tested", "paragraph_index": 52}, {"url": "https://www.amazon.com/Fluid-Concepts-Creative-Analogies-Fundamental/dp/0465024750", "anchor_text": "Fluid Concepts and Creative Analogies", "paragraph_index": 54}, {"url": "https://en.wikipedia.org/wiki/Symbolic_artificial_intelligence", "anchor_text": "symbolic A.I.", "paragraph_index": 54}, {"url": "https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf", "anchor_text": "tasks such as computer vision", "paragraph_index": 54}, {"url": "https://www.amazon.com/Fluid-Concepts-Creative-Analogies-Fundamental/dp/0465024750", "anchor_text": "Fluid Concepts and Creative Analogies", "paragraph_index": 58}, {"url": "http://dippl.org/examples/vision.html", "anchor_text": "computer vision demo", "paragraph_index": 62}, {"url": "http://www.jacopotagliabue.it/webppl_tutorial.html#draw_basic", "anchor_text": "we define a generative model", "paragraph_index": 63}, {"url": "https://gist.github.com/jacopotagliabue/a9c6e7f15b7147ff72e9abe68c57be18", "anchor_text": "small gist", "paragraph_index": 64}, {"url": "https://www.crunchbase.com/organization/gamalon-inc", "anchor_text": "companies", "paragraph_index": 67}, {"url": "https://youtu.be/BiH0TVpO5yk", "anchor_text": "Enjoy the video", "paragraph_index": 67}, {"url": "https://tooso.ai/", "anchor_text": "Tooso", "paragraph_index": 72}, {"url": "http://tooso.ai/", "anchor_text": "Tooso", "paragraph_index": 74}, {"url": "https://www.linkedin.com/company/tooso", "anchor_text": "Linkedin", "paragraph_index": 74}, {"url": "https://twitter.com/tooso_ai", "anchor_text": "Twitter", "paragraph_index": 74}, {"url": "https://www.instagram.com/tooso_ai/", "anchor_text": "Instagram", "paragraph_index": 74}, {"url": "https://medium.com/u/2e4d67ebbcd6?source=post_page-----785d3c81610a--------------------------------", "anchor_text": "Andrea Polonioli", "paragraph_index": 75}, {"url": "https://www.youtube.com/watch?v=1qB600w6eFc", "anchor_text": "brave and awesome", "paragraph_index": 75}, {"url": "https://www.meetup.com/it-IT/The-Future-of-AI-in-IT/events/255678923/", "anchor_text": "The Future of AI", "paragraph_index": 75}, {"url": "https://probmods.org/", "anchor_text": "here", "paragraph_index": 78}, {"url": "https://agentmodels.org/chapters/2-webppl.html", "anchor_text": "here", "paragraph_index": 78}, {"url": "http://www.problang.org/", "anchor_text": "here", "paragraph_index": 78}, {"url": "http://forestdb.org/", "anchor_text": "this repo", "paragraph_index": 78}, {"url": "http://forestdb.org/models/blm.html", "anchor_text": "this", "paragraph_index": 78}, {"url": "https://www.amazon.com/Fluid-Concepts-Creative-Analogies-Fundamental/dp/0465024750", "anchor_text": "Fluid Concepts and Creative Analogies", "paragraph_index": 80}], "all_paragraphs": ["\u201cThen [the term \u2018AI\u2019] slid down the slope that ends up in meaningless buzzwords and empty hype. (\u2026) Luckily, a new term was just then coming into currency \u2014 \u2018cognitive science\u2019 \u2014 and I started to favor that way of describing my research interests, since it clearly stresses the idea of fidelity to what actually goes on in the human mind/brain.\u201d \u2014 Douglas Hofstadter (before 2000, just to be clear)", "Imagine someone came to you with the following puzzle: given the sequence of integers 0, 1, 2, \u2026, guess the next item. It does not seem that hard, does it? 3 will be our obvious answer as we would assume the \u201cgenerator function\u201d could be something as simple as f(x) = x:", "As it turns out, this is not the function generating this sequence. The sequence continues as follows:", "We won\u2019t spoil the surprise (see the very end for the solution), but use this nice puzzle to discuss a more general problem: given the universe of integers and functions built out of arithmetical operations (addition, subtraction, multiplications, etc.), how can we learn the \u201cgenerator function\u201d of a sequence given that the number of possibilities is infinite?", "[At Tooso we specialize in formal models of human language \u2014 if you think solving integer sequences is silly, you can think of English syntax as the \u201cgenerator function\u201d of what you are reading right now and how hard it is for computers to do human-like inferences on text. Enough about language for now, let\u2019s get back to integers!]", "The sequence game came to my mind, serendipitously, while exploring probabilistic programming (PP), which is often explained as an attempt to unify general purpose programming with probabilistic modeling. Our interest in PP lies in the ability to reason explicitly about structures and deal well with uncertainty even without lots of data: good luck with \u201cmachine-learning-your-way\u201d out of this:", "The sequence game is part of the amazing Fluid Concepts and Creative Analogies (fun fact: it was the first book sold on Amazon!), a 1995 book by A.I. pioneer, best-selling author and cognitive scientist Douglas Hofstadter (DH). In his unparalleled writing ability, this is how the problem gets introduced:", "The mathematical origin of this sequence suggests that there will indeed be pattern or order, but as math itself is full of patterns of the most enormous diversity, the field is still wide open. Still, we have built-in prejudices in favor of simplicity and elegance, whether or not we can define those notions. So, given our expectation that we are likely to find a simple and elegant rule, what would we expect to happen next?", "In this brief post we will rediscover the sequence game and discuss concept learning with some simple PP programs. On the one hand, it\u2019s going to be a good exercise in understanding PP, as we shall see how to treat built-in prejudices in a satisfactory and principled way; on the other, we shall use PP to get insights into recent progresses in Artificial Intelligence.", "DISCLAIMER: this post is not a tutorial on PP nor an academic work (if anything, because it is unapologetically biased) nor a collection of production-ready models\u2014 while runnable code samples are included, our interest today lies almost exclusively in the type of modeling that is hard with standard ML tools and comes instead natural with PP. As a bonus, we will get to discuss cognitively plausible ideas about concept learning that go beyond \u201ccurve fitting\u201d.", "\u201cMillion to one odds happen eight times a day in New York.\u201d \u2014 Penn Jillette", "In our life as A.I. practitioners and startup founders, we found multiple times that a lot of ideas in Probabilistic Programming are still far from being mainstream in the data science community. Convinced by this anecdotal evidence, we decided to include an overview of PP and what makes it special for interesting inference problems, such as the ones you meet while teaching language to machines (the reader familiar with PP/WebPPL can quickly skip through the next two sections).", "We will make heavy use of WebPPL examples and take full advantage of the possibility of running code in the browser: all examples are fully runnable from this simple HTML page.", "Let\u2019s start with a simple example (with dice, of course). If we have two fair dice, what is the probability of getting 8 when you throw them? This WebPPL program (remember: you can run it here) displays the distribution of outcomes in a nice histogram (if something is not clear, don\u2019t panic: it will be obvious soon):", "The cool thing about PP is how easy it is to do the inverse inference: given that we observe that the sum is 8, what is the chance of one die being a 2?", "As the reader may have suspected, there is a 20% chance of die #1 being a 2 if the total is 8 (quiz for the lazy reader: why there is no 1 in the histogram above?).", "While this may look like an overtly simple example, it is really important to appreciate the \u201cpower of models\u201d, i.e. simplified versions of the processes generating our data of interest. Models are a very compact representation of the complexity of the world: within a single model, you can in fact reason from cause to effect (forward, when you plan) and from effect to causes (backward, when you make educated guesses). Models are also at the core of our cognitive abilities: by having a \u201cTangram model\u201d, kids know how to build a duck from scattered pieces (forward) and from the shape of a duck they can guess which pieces are used to build it (backward).", "For many people, model-based reasoning is the key to intelligence, with applicability well beyond Tangram (such as intuitive physics and visual learning):", "A generative model describes a process, usually one by which observable data is generated, that represents knowledge about the causal structure of the world. These generative processes are simplified \u201cworking models\u201d of a domain (\u2026); many different questions can be answered by interrogating the mental model.", "For today\u2019s more modest purpose than understanding human intelligence, it is just important to appreciate how PP can help. As we have seen, these models are often called \u201cgenerative models\u201d: without bothering too much with technical details, what is important for us are the following two considerations:", "While people have been writing generative models by hand for a long time, the golden age of probabilistic modeling just started. The rise of PP solved two major problems for practitioners: first, PP brings to the table the expressivity of programming languages: by using familiar tools data scientists can now express arbitrary complex models instead of a few distributions; second, PP pushes the burden of inference down to the compiler (and related tools): researchers focus on modeling since the computer will solve the required inferential challenge.", "\u201cThe limits of my programming language mean the limits of my world.\u201d \u2014 (almost) Ludwig Wittgenstein", "Now that we have some shared context on why generative models are so freakin\u2019 cool, we can go back to the question of how they are implemented in languages such as WebPPL. If you recall our first example, the model is specified as a simple function, wrapped in the Infer method:", "What happens when it runs? Since enumerate is the inference method specified, the program will run the function once for all the possible values of the random variables in the model (i.e. random integers from 1 to 6 for each die) and collect the results in a distribution (that gets visualized through the handy viz utility). Since the model is generative, it should be possible to sample from it and create new values (i.e. simulate dice rolling): the sample function does exactly that, as you can see by adding the following statement to the cell block (it will print an array of 5 integers sampled from the model):", "The second example introduces a key idea of PP: conditioning. The model is the same as before but with a new command:", "What happens when it runs? condition tells the program to ignore runs that do not meet the required condition: this is why no 1 shows up in the distribution \u2014 if one die is 1, there is no scenario where the sum is 8 (as 1+6=7).", "Conditioning is important for at least three reasons: it lets you reason from observations to causes; it helps stating, in a principled way, information that may constrain the space of program runs to be considered; finally, it can be used to model a very important concept in contemporary A.I., learning, through an 18th century idea known as Bayes\u2019 theorem: your confidence in an hypothesis H given some data D should be related to a) how much you believe in H in the first place and b) how much H can explain D.", "We conclude our outrageously short introduction to WebPPL with a slightly juicier example on this topic. Consider a young field linguist\u2014let\u2019s call him Willard \u2014 in an exotic country with a very minimal world (a.k.a. ontology):", "This exotic world has seven objects (named with integers 1\u20137) organized in a tree-like structure, with concepts on three levels of generality (e.g. everything is a mammal, dachshund is a type of dog, #3 is a dachshund). When #2 is present, Willard hears native speakers say bassotto (1000 bonus points to readers expecting the natives to say gavagai). What does bassotto mean: dachshund, dog or mammal? More generally:", "given an unknown word W and a set of objects denoted by W, how can Willard learn the appropriate level of generality for W?", "Unsurprisingly, probabilistic programming is perfectly suited to model this problem, as exemplified by the following sketch:", "learningModel is where the magic happens. We can distinguish three main parts:", "[SMALL TECHNICAL NOTE: the original idea for this example comes from the work of Xu and Tenenbaum \u2014 see their paper for a bit more context on the psychological facts behind the model and an in-depth discussion on priors and likelihood.]", "So, back to our example with #2 observed for the word bassotto, what is Willard thinking? Running the code gives us the following distribution:", "It seems a reasonable guess, doesn\u2019t it? Willard prefers dachshund but he is still keeping his mind open; of course, hypotheses inconsistent with the data (such as retriever, whose extension does not include #2) are not considered at all. The scenario gets really interesting when Willard observes #2 + #3 and hear bassotto:", "What happened? If Willard was just excluding hypotheses inconsistent with the data, seeing two dachshunds instead of one would not have added meaningful information: simply put, Willard would have not learned anything new. Our probabilistic model allows instead to capture Willard\u2019s learning: after seeing two dachshunds, he is much more confident that bassotto means dachshund, while he is almost ready to rule out mammal as a plausible interpretation (even if entirely consistent with the data, from a purely logical perspective). We leave it as an exercise to the reader to make sure all the code is clear: try changing priors, likelihood and observations to get a better grasp of what is going on behind the scenes!", "If you want to toy around some more before continuing, some effective examples of the interplay between forward and backward inference can be found on probmods. While the field is still relatively niche in the data science community, things are moving fast: we discovered this excellent WIP last week and major players are getting into PP. For the convenience of non-lazy readers, we have included a bunch of additional books/links at the end.", "Let\u2019s get back to our sequence game now!", "\u201cDon\u2019t keep a man guessing too long \u2014 he\u2019s sure to find the answer somewhere else.\u201d \u2014 Mae West", "Now that we have a better grasp on the philosophy behind PP, we can try and model our sequence game as a learning problem in an hypotheses space that is infinite. Given the sequence:", "we will try to learn the sequence generator function conditioning (yes!) on the data we have (f(1) and f(2)), and then apply the function to the next item in the sequence (i.e. 3). Even restricting ourselves to elementary arithmetic functions, there are many functions that are compatible with our observations, such as:", "In other words, this is a very different scenario than our previous examples: there is no finite set of hypotheses we can list and somehow score from the most to the least plausible. This problem is what DH had in mind when writing:", "as math itself is full of patterns of the most enormous diversity, the field is still wide open.", "Since we cannot list our hypotheses, we need to turn our attention to a process capable of generating infinite hypotheses: luckily for us, our knowledge of languages and grammars comes to the rescue. Consider the simple language thus defined:", "How many valid expressions can be generated according to the grammar? The key to answer is to realize that instructions 2\u20134 take expressions as input and produce new expressions, so that the output of any instruction can become the input of another instruction (or the same one!):", "So even a simple language like the one above is capable of producing a (countably, for the logically-inclined reader) infinite number of expressions.", "Armed with this intuition, can we define the space of generator functions through a grammar? Of course we can! We just need to build a small grammar of arithmetic expressions, one that could potentially generate x * x, x ^ x, x ^ 2, etc. and priors over them (remember that the space is infinite, so we need a \u201cgenerative\u201d way to assign priors over all the possible functions). Here is a first attempt (this gist contains the grammar, while the full runnable code can be found here):", "The code should be pretty easy to understand (the original code is from the excellent Goodman & Tenenbaum): with 50% probability, make up an expression by picking an integer (1\u20139) or a variable (x), otherwise combine two expressions into a new one (which will recursively pick randomly an integer and so on). We will just stress the \u201cimplicit\u201d prior on the model, which is a prior on simplicity: other things being equal, a simpler function (defined now as grammatically shorter) should be preferred. Since fewer probabilistic choices are required to produce x * x than x * (x / 1), while extensionally equivalent (as x = x/1), the former is considered a much more plausible hypothesis. Obviously, this is the formal equivalent of enforcing in a model what DH called:", "[the] built-in prejudices in favor of simplicity and elegance", "If we run the model on the sequence:", "the preferred suggestion for f(3) is 9:", "As noted by Goodman & Tenenbaum, \u201cmany people find the high probability assigned (\u2026) to 27 to be unintuitive\u201d. This suggests that as a cognitive model of human sequence discovery our priors could use a little reworking: x times x seems a more \u201cnatural\u201d idea than x raised to the power of x (exercise for the non-lazy reader: change the code to reflect this bias and run the prediction again).", "What kind of sequences can this model solve? Below there are some examples that can be tested by changing the condition statement:", "[Exercise for the non-lazy reader: sometime the easiest way to learn generator functions is by taking higher-order sequences, i.e. realizing that 1, 4, 9, 16, 25, ... produces a child sequence of 3, 5, 7, ... as differences between pairs of numbers; you can then use the second sequence (the odd numbers) to make prediction about the original sequence. The reader is encouraged to extend the existing model to second order sequences: how does the prior look like in that case?]", "The last thing we may want to do is going full circle: how close is the \u201cPP approach\u201d to the original ideas in Fluid Concepts and Creative Analogies? One of DH\u2019s main points throughout the book is that concept representation is the problem of A.I.. If you start with human-specified concepts (like \u201csymbolic A.I.\u201d), you are begging the question of how those concepts are learned in the first place; on the other hand, complete bottom-up representations (at that time, a very early version of what today\u2019s deep learning systems can do in tasks such as computer vision) work for low-level perceptual tasks, but lack most of the generality associated with human concepts. The architectures presented in Fluid Concepts are therefore a mixture of top-down and bottom-up processes, in which lower-level concepts are stochastically combined to build higher-level concepts, which in turn make some other low-level concepts more likely to be used to solve a given task.", "When compared to our PP models, the most striking similarities are 1) the probabilistic exploration of an hypotheses space (which is structured through basic concepts) and 2) the application of compositional principles to re-use low-level concepts to build high-level ones.", "We strongly invite the reader to get the unfiltered experience from the book as we would never do it justice with our overview. That said, we want to explore one more \u201cfluid task\u201d before the end: let\u2019s get to the (digital) drawing board!", "\u201cThere are thousands of ways of drawing the letter \u2018A\u2019 , yet we recognize any of them at a glance. For any mental category (\u2026), we immediately recognize thousands of instances, despite vast differences.\u201d \u2014 Douglas Hofstadter", "The last part of Fluid Concepts and Creative Analogies is devoted to Letter Spirit, a project whose goal is \u201cdesigning and implementing a computer program that could itself design full-fledged typefaces\u201d.", "In the characteristic DH\u2019s methodology of using toy problems to enlighten important cognitive phenomena, Letter Spirit is an experiment in human-like machine creativity that produced some pretty interesting font designs:", "As humans know well, visual concepts are both very fluid (e.g. there are countless of ways in which we can draw an A), and yet very distinctive (e.g. humans can recognize instantly that some drawings are varieties on a common theme). It is therefore not surprising that teaching machines how to understand visual concepts is viewed fundamentally as the \u201cquest for the conceptual essence of letters\u201d:", "To recognize something \u2014 that is, to match an item with the proper Platonic abstraction \u2014 there must be room for flexing both the Platonic abstraction and the mental representation of the thing itself, to make them meet. What kinds of distortion are sensible? Readjustment of internal boundaries, reassessment of connection strengths, and rearrangement of parts are at the core of the recognition process.", "As we have seen times and times again in this post, the probabilistic approach highlights how understanding a concept C and generating new instances of C are indeed intimately related activities. Without attempting to reproduce the full complexity of Letter Spirit, can we discuss some interesting cases of visual learning within the PP paradigm? We will start with a variation on WebPPL computer vision demo: if we draw with n lines a (digital) polygon (triangle, square, pentagon), can a probabilistic program figure out how it was made in the first place?", "The basic ingredients should be straightforward by now. First, we define a generative model \u2014 a \u201cvisual grammar\u201d if you will \u2014 of polygons; a possible generative recipe is the following:", "To become familiar with our visual generative model, you can try to create some lines in a separate code block (see a small gist to get you started). At runtime, we then use the power of PP to make the backward inference: given a drawing with such and such features, what are the instructions that could have created it?", "To achieve this, we use WebPPL factor expression to weigh the iterations based on how close our polygon (generated with the recipe above) is to the target image we are trying to decode. Once we generated our distribution, we can inspect what the program learned about the visual concept of, say, square by repeatedly sampling from it:", "Not bad! In other words, we can use the model to generate infinite instances of the visual concept we learned: pretty neat, isn\u2019t it? As usual, the non-lazy reader is encouraged to extend and improve the existing code sample (to go where probably no model has gone before!).", "As a final exercise, inspired by one of our favorite companies, we added a bunch of ideas to the above visual grammar and completed our own \u201crun-on-my-laptop-only\u201d prototype to showcase the power of PP: no big data needed for training, no black-box optimization, no costly GPUs \u2014 everything happens in a standard web browser in a few seconds. Enjoy the video!", "\u201cWe (\u2026) hope that the work we have described in this book may help lead, in the distant future, to architectures that go much further than we have toward capturing the genuine fluid mentality that Alan Turing so clearly envisioned when he first proposed his deservedly celebrated Test.\u201d \u2014 Douglas Hofstadter", "We are (finally!) at the end of our tour de force in probabilistic programming through the lens of \u201cfluid human concepts\u201d. We barely scratched the surface though, as it is inevitable when trying to give an overview of interesting ideas and promising tech tools in a wildly ambitious and yet very uncharted territory, i.e. machine-learned concepts and human-like inference (even if in small domains).", "Without the presumption of exhausting the threads opened above, the following are some parting thoughts and more philosophical reflections on our journey so far in concept-driven Artificial Intelligence:", "If you survived this incredibly long blog post (including weird references, nerd papers and clumsy code samples), you deserve our deepest gratitude.", "We do hope that, whatever your prior beliefs were, observing the data in this article convinced you to follow more closely what we are building at Tooso.", "If you have requests, questions, feedback or crazy ideas, please share your A.I. story with jacopo.tagliabue@tooso.ai.", "Don\u2019t forget to get the latest from Tooso on Linkedin, Twitter and Instagram.", "Special thanks to Andrea Polonioli for brave and awesome story-telling and thanks to the entire Tooso team for helpful comments on previous drafts of this article; we are also grateful to the organizers of The Future of AI meet-up for letting us share there some of these ideas.", "All the remaining mistakes are mine and mine only.", "Our probabilistic programming intro sucks especially on the \u201cprogramming\u201d part, as we could just introduce the very minimum of WebPPL needed to drive our points home. Where to go from here? Unfortunately, there is not much out there as a full-fledge, from-scratch, \u201ccomputer scienc-y\u201d introduction to PP in scripting languages (yes, I do mean Python):", "In the end, our suggestion is to just get lost in the nice examples and plenty of research papers here, here and here (this repo is amazing, but most models are not in WebPPL: if you like natural language semantics, start with this). The field is evolving quickly though: if you think we forgot something, please let us know.", "what is the generator function? This is the solution:", "that is, for every x, x-times factorial x (see Fluid Concepts and Creative Analogies, p. 46).", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "I failed the Turing Test once, but that was many friends ago."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F785d3c81610a&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffluid-concepts-and-creative-probabilities-785d3c81610a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffluid-concepts-and-creative-probabilities-785d3c81610a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffluid-concepts-and-creative-probabilities-785d3c81610a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffluid-concepts-and-creative-probabilities-785d3c81610a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----785d3c81610a--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----785d3c81610a--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@jacopotagliabue?source=post_page-----785d3c81610a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@jacopotagliabue?source=post_page-----785d3c81610a--------------------------------", "anchor_text": "Jacopo Tagliabue"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fbffd30619c10&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffluid-concepts-and-creative-probabilities-785d3c81610a&user=Jacopo+Tagliabue&userId=bffd30619c10&source=post_page-bffd30619c10----785d3c81610a---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F785d3c81610a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffluid-concepts-and-creative-probabilities-785d3c81610a&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F785d3c81610a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffluid-concepts-and-creative-probabilities-785d3c81610a&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://tooso.ai/", "anchor_text": "Tooso"}, {"url": "https://en.wikipedia.org/wiki/Probabilistic_programming_language", "anchor_text": "often explained"}, {"url": "https://www.amazon.com/Fluid-Concepts-Creative-Analogies-Fundamental/dp/0465024750", "anchor_text": "Fluid Concepts and Creative Analogies"}, {"url": "https://en.wikipedia.org/wiki/Douglas_Hofstadter#cite_note-50", "anchor_text": "it was the first book sold on Amazon"}, {"url": "https://www.quantamagazine.org/to-build-truly-intelligent-machines-teach-them-cause-and-effect-20180515/", "anchor_text": "curve fitting"}, {"url": "https://github.com/probmods/webppl/tree/master/examples", "anchor_text": "WebPPL examples"}, {"url": "http://www.jacopotagliabue.it/webppl_tutorial.html", "anchor_text": "simple HTML page"}, {"url": "http://www.jacopotagliabue.it/webppl_tutorial.html#anchor_rolling_forward", "anchor_text": "here"}, {"url": "https://en.wikipedia.org/wiki/Tangram", "anchor_text": "Tangram"}, {"url": "https://probmods.org/chapters/introduction.html", "anchor_text": "many people"}, {"url": "http://www.pnas.org/content/110/45/18327.short", "anchor_text": "intuitive physics"}, {"url": "https://web.mit.edu/cocosci/Papers/Science-2015-Lake-1332-8.pdf", "anchor_text": "visual learning"}, {"url": "https://ai.stanford.edu/~ang/papers/nips01-discriminativegenerative.pdf", "anchor_text": "technical details"}, {"url": "http://www.jacopotagliabue.it/webppl_tutorial.html#anchor_rolling_forward", "anchor_text": "it runs"}, {"url": "https://webppl.readthedocs.io/en/master/inference/", "anchor_text": "inference method"}, {"url": "https://en.wikipedia.org/wiki/Bayes%27_theorem", "anchor_text": "Bayes\u2019 theorem"}, {"url": "https://en.wikipedia.org/wiki/Willard_Van_Orman_Quine", "anchor_text": "Willard"}, {"url": "https://en.wikipedia.org/wiki/Indeterminacy_of_translation", "anchor_text": "gavagai"}, {"url": "http://web.mit.edu/cocosci/Papers/XT-PsychRev-InPress.pdf", "anchor_text": "Xu and Tenenbaum"}, {"url": "http://www.jacopotagliabue.it/webppl_tutorial.html#gavagai", "anchor_text": "try changing priors, likelihood and observations"}, {"url": "https://probmods.org/chapters/conditioning.html", "anchor_text": "probmods"}, {"url": "http://mbmlbook.com/toc.html?twitter=@bigdata", "anchor_text": "WIP"}, {"url": "https://www.theregister.co.uk/2018/10/05/imicrosoft_nfernet/", "anchor_text": "are getting into PP"}, {"url": "http://www.jacopotagliabue.it/webppl_tutorial.html#sequence_basic", "anchor_text": "here"}, {"url": "https://probmods.org/exercises/lot-learning.html", "anchor_text": "Goodman & Tenenbaum"}, {"url": "https://probmods.org/exercises/lot-learning.html", "anchor_text": "Goodman & Tenenbaum"}, {"url": "http://www.jacopotagliabue.it/webppl_tutorial.html#sequence_basic", "anchor_text": "tested"}, {"url": "https://www.amazon.com/Fluid-Concepts-Creative-Analogies-Fundamental/dp/0465024750", "anchor_text": "Fluid Concepts and Creative Analogies"}, {"url": "https://en.wikipedia.org/wiki/Symbolic_artificial_intelligence", "anchor_text": "symbolic A.I."}, {"url": "https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf", "anchor_text": "tasks such as computer vision"}, {"url": "https://www.amazon.com/Fluid-Concepts-Creative-Analogies-Fundamental/dp/0465024750", "anchor_text": "Fluid Concepts and Creative Analogies"}, {"url": "https://www.amazon.com/Fluid-Concepts-Creative-Analogies-Fundamental/dp/0465024750", "anchor_text": "Fluid Concepts and Creative Analogies"}, {"url": "http://dippl.org/examples/vision.html", "anchor_text": "computer vision demo"}, {"url": "http://www.jacopotagliabue.it/webppl_tutorial.html#draw_basic", "anchor_text": "we define a generative model"}, {"url": "https://gist.github.com/jacopotagliabue/a9c6e7f15b7147ff72e9abe68c57be18", "anchor_text": "small gist"}, {"url": "https://www.crunchbase.com/organization/gamalon-inc", "anchor_text": "companies"}, {"url": "https://youtu.be/BiH0TVpO5yk", "anchor_text": "Enjoy the video"}, {"url": "https://christophm.github.io/interpretable-ml-book/", "anchor_text": "explainability in deep learning models"}, {"url": "http://www.problang.org/", "anchor_text": "interesting attempts"}, {"url": "https://plato.stanford.edu/entries/logic-conditionals/index.html#LogOntCon", "anchor_text": "counterfactual statements"}, {"url": "https://www.amazon.com/Book-Why-Science-Cause-Effect/dp/046509760X", "anchor_text": "the relevant literature"}, {"url": "https://cocolab.stanford.edu/ndg.html", "anchor_text": "Goodman"}, {"url": "https://web.mit.edu/cocosci/josh.html", "anchor_text": "Tenenbaum"}, {"url": "https://probmods.org/", "anchor_text": "Probabilistic Models of Cognition"}, {"url": "https://probmods.org/chapters/lot-learning.html", "anchor_text": "Rational Rules example"}, {"url": "https://web.mit.edu/cocosci/Papers/Science-2015-Lake-1332-8.pdf", "anchor_text": "recent work by Tenenbaum et al."}, {"url": "https://en.wikipedia.org/wiki/Douglas_Hofstadter", "anchor_text": "Douglas Hofstadter"}, {"url": "https://www.amazon.com/Fluid-Concepts-Creative-Analogies-Fundamental/dp/0465024750", "anchor_text": "Fluid Concepts and Creative Analogies"}, {"url": "https://www.theatlantic.com/technology/archive/2018/01/the-shallowness-of-google-translate/551570/", "anchor_text": "mostly"}, {"url": "https://twitter.com/tdietterich/status/948811917593780225?lang=en", "anchor_text": "Twitter fights"}, {"url": "https://translate.google.com/#auto/en/bassotto", "anchor_text": "Italian"}, {"url": "https://drive.google.com/file/d/1s4awHXqtfCcgIcpjy5cD0b-l7ubw6lY8/view?usp=sharing", "anchor_text": "Omero"}, {"url": "https://tooso.ai/", "anchor_text": "Tooso"}, {"url": "http://tooso.ai/", "anchor_text": "Tooso"}, {"url": "https://www.linkedin.com/company/tooso", "anchor_text": "Linkedin"}, {"url": "https://twitter.com/tooso_ai", "anchor_text": "Twitter"}, {"url": "https://www.instagram.com/tooso_ai/", "anchor_text": "Instagram"}, {"url": "https://medium.com/u/2e4d67ebbcd6?source=post_page-----785d3c81610a--------------------------------", "anchor_text": "Andrea Polonioli"}, {"url": "https://www.youtube.com/watch?v=1qB600w6eFc", "anchor_text": "brave and awesome"}, {"url": "https://www.meetup.com/it-IT/The-Future-of-AI-in-IT/events/255678923/", "anchor_text": "The Future of AI"}, {"url": "https://www.amazon.com/Practical-Probabilistic-Programming-Avi-Pfeffer/dp/1617292338", "anchor_text": "this Manning book"}, {"url": "https://github.com/uber/pyro", "anchor_text": "Pyro"}, {"url": "https://github.com/uber/pyro/tree/dev/examples/rsa", "anchor_text": "porting"}, {"url": "https://www.amazon.com/Bayesian-Methods-Hackers-Probabilistic-Addison-Wesley/dp/0133902838/ref=pd_bxgy_2?_encoding=UTF8&pd_rd_i=0133902838&pd_rd_r=063b6767-c1f3-11e8-9d8c-cbd4feed3ce3&pd_rd_w=mOgEC&pd_rd_wg=AyjNm&pf_rd_i=desktop-dp-sims&pf_rd_m=ATVPDKIKX0DER&pf_rd_p=3f9889ac-6c45-46e8-b515-3af650557207&pf_rd_r=FNGX7RGB1H91YFBQ6FGM&pf_rd_s=desktop-dp-sims&pf_rd_t=40701&psc=1&refRID=FNGX7RGB1H91YFBQ6FGM", "anchor_text": "this \u201cFor Hackers\u201d small book"}, {"url": "https://probmods.org/", "anchor_text": "Probabilistic Models of Cognition"}, {"url": "https://cocolab.stanford.edu/ndg.html", "anchor_text": "Goodman"}, {"url": "https://web.mit.edu/cocosci/josh.html", "anchor_text": "Tenenbaum"}, {"url": "https://probmods.org/", "anchor_text": "here"}, {"url": "https://agentmodels.org/chapters/2-webppl.html", "anchor_text": "here"}, {"url": "http://www.problang.org/", "anchor_text": "here"}, {"url": "http://forestdb.org/", "anchor_text": "this repo"}, {"url": "http://forestdb.org/models/blm.html", "anchor_text": "this"}, {"url": "https://www.wolframalpha.com/input/?i=6!", "anchor_text": "6! = 720"}, {"url": "https://www.amazon.com/Fluid-Concepts-Creative-Analogies-Fundamental/dp/0465024750", "anchor_text": "Fluid Concepts and Creative Analogies"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----785d3c81610a---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/probabilistic-programming?source=post_page-----785d3c81610a---------------probabilistic_programming-----------------", "anchor_text": "Probabilistic Programming"}, {"url": "https://medium.com/tag/ai?source=post_page-----785d3c81610a---------------ai-----------------", "anchor_text": "AI"}, {"url": "https://medium.com/tag/nlp?source=post_page-----785d3c81610a---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/semantics?source=post_page-----785d3c81610a---------------semantics-----------------", "anchor_text": "Semantics"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F785d3c81610a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffluid-concepts-and-creative-probabilities-785d3c81610a&user=Jacopo+Tagliabue&userId=bffd30619c10&source=-----785d3c81610a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F785d3c81610a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffluid-concepts-and-creative-probabilities-785d3c81610a&user=Jacopo+Tagliabue&userId=bffd30619c10&source=-----785d3c81610a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F785d3c81610a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffluid-concepts-and-creative-probabilities-785d3c81610a&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----785d3c81610a--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F785d3c81610a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffluid-concepts-and-creative-probabilities-785d3c81610a&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----785d3c81610a---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----785d3c81610a--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----785d3c81610a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----785d3c81610a--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----785d3c81610a--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----785d3c81610a--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----785d3c81610a--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----785d3c81610a--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----785d3c81610a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@jacopotagliabue?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@jacopotagliabue?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Jacopo Tagliabue"}, {"url": "https://medium.com/@jacopotagliabue/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "660 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fbffd30619c10&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffluid-concepts-and-creative-probabilities-785d3c81610a&user=Jacopo+Tagliabue&userId=bffd30619c10&source=post_page-bffd30619c10--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F20d6b053a930&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffluid-concepts-and-creative-probabilities-785d3c81610a&newsletterV3=bffd30619c10&newsletterV3Id=20d6b053a930&user=Jacopo+Tagliabue&userId=bffd30619c10&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}