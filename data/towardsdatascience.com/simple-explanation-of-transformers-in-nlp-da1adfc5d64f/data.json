{"url": "https://towardsdatascience.com/simple-explanation-of-transformers-in-nlp-da1adfc5d64f", "time": 1682992976.5871239, "path": "towardsdatascience.com/simple-explanation-of-transformers-in-nlp-da1adfc5d64f/", "webpage": {"metadata": {"title": "Simple Explanation of Transformers in NLP | by Renu Khandelwal | Towards Data Science", "h1": "Simple Explanation of Transformers in NLP", "description": "Let\u2019s dive into the Transformer\u2019s Architecture and key concepts attached to Transformers to understand how Transformer address these challenges The Transformer has a stack of 6 Encoder and 6 Decoder\u2026"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/pdf/1607.06450.pdf", "anchor_text": "Layer Normalization by Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton", "paragraph_index": 37}], "all_paragraphs": ["In this post, we will address the following questions related to Transformer", "Let\u2019s dive into the Transformer\u2019s Architecture and key concepts attached to Transformers to understand how Transformer address these challenges", "The Transformer has a stack of 6 Encoder and 6 Decoder, unlike Seq2Seq; the Encoder contains two sub-layers: multi-head self-attention layer and a fully connected feed-forward network.", "The Decoder contains three sub-layers, a multi-head self-attention layer, an additional layer that performs multi-head self-attention over encoder outputs, and a fully connected feed-forward network.", "Each sub-layer in Encoder and Decoder has a Residual connection followed by a layer normalization.", "All input and output tokens to Encoder/Decoder are converted to vectors using learned embeddings. These input embeddings are then passed to Positional Encoding.", "The Transformer\u2019s architecture does not contain any recurrence or convolution and hence has no notion of word order. All the words of the input sequence are fed to the network with no special order or position as they all flow simultaneously through the Encoder and decoder stack.", "To understand the meaning of a sentence, it is essential to understand the position and the order of words.", "Positional encoding is added to the model to helps inject the information about the relative or absolute position of the words in the sentence", "Positional encoding has the same dimension as the input embedding so that the two can be summed.", "Attention in simplistic terms is to get a better understanding of the meaning and the context of words in a sentence.", "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence", "A self-attention layer connects all positions with a constant number of sequentially executed operations and hence are faster than recurrent layers", "An Attention function in a Transformer is described as mapping a query and a set of key and value pair to an output. Query, key, and value are all vectors. Attention weights are calculated using Scaled Dot-Product Attention for each word in the sentence. The final score is the weighted sum of the values.", "Let\u2019s understand this with a sentence, \u201cI enjoy nature.\u201d", "The inputs are query, key, and value. The dimension of the vectors is 64, as this leads to stable gradients.", "Take the dot product of the query and key for each word in the sentence. Dot-Product determines who much to focus on other words in the input sentence.", "Scale the Dot-Product by dividing by the square root of the dimension of the key vector. Dimension is 64; hence we divide the Dot-Product by 8.", "Softmax normalizes the scaled value. After applying Softmax, all the values are positive and add up to 1", "Step 4: Calculate the weighted sum of the values", "We apply the Dot-Product between the normalized score and the value vector and then calculate the sum", "These steps are repeated for every word in the sentence.", "Instead of using a single attention function where the attention can be dominated by the actual word itself, transformers use multiple attention heads.", "Each attention-head has a different linear transformation applied to the same input representation. The Transformer uses eight different attention heads, which are computed parallelly and independently. With eight different attention heads, we have eight different sets of the query, key, and value and also eight sets of Encoder and Decoder each of these sets is initialized randomly", "\u201cMulti-head attention allows the model to jointly attend to information from different representation subspaces at different positions.\u201d", "With a multi-head attention head, each part of the input representation interacts with the other parts of the input representation to get a better meaning and context. As multi-head attention looks ate different representation subspaces at different positions, it also helps learn the long term dependency.", "Using Multi-head Attention, we get that the word \u201cit\u201d in the above sentence refers to \u201cCoronavirus.\u201d", "The Decoder has masked multi-head attention where it masks or blocks the decoder inputs from the future steps. During training, the multi-head attention of the Decoder hides the future decoder inputs.", "For the machine translation task to translate a sentence, \u201cI enjoy nature\u201d from English to Hindi using the Transformer, the Decoder will consider all the inputs words \u201cI, enjoy, nature\u201d to predict the first word.", "The below table shows how the Decoder would block the inputs from future steps", "Each sub-layer in Encoder and Decoder has a Residual connection followed by a layer normalization.", "Residual connections are \u201cskip connections\u201d that allow gradients to flow through the network without passing through the non-linear activation function. Residual connection helps with avoiding vanishing or exploding gradient issues. For residual connections to work, the output of each sub-layer in the model should be the same. All sub-layers in the Transformer, produce an output of dimension 512.", "Layer Normalization: normalizes the inputs across each of the features and is independent of other examples, as shown below. Layer normalization reduces the training time in feed-forward neural networks. In Layer normalization, we compute mean and variance from all of the summed inputs to the neurons in a layer on a single training case.", "Encoder and Decoder in the Transformer both have a fully connected feed-forward network, and it has two linear transformations containing a ReLU activation in between.", "The final layer of the Decoder applies the linear transformation and softmax function to convert the decoder output to predict the output probabilities", "The drawbacks of the seq2seq model are addressed by Transformer", "Transformers have a simple network architecture based on the Self-Attention mechanism and do not rely on recurrence and convolutions entirely. The computations are executed in parallel making Transformers efficient and requiring less training time", "Layer Normalization by Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton", "A Technology Enthusiast who constantly seeks out new challenges by exploring cutting-edge technologies to make the world a better place!"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fda1adfc5d64f&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimple-explanation-of-transformers-in-nlp-da1adfc5d64f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimple-explanation-of-transformers-in-nlp-da1adfc5d64f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimple-explanation-of-transformers-in-nlp-da1adfc5d64f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimple-explanation-of-transformers-in-nlp-da1adfc5d64f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://arshren.medium.com/?source=post_page-----da1adfc5d64f--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----da1adfc5d64f--------------------------------", "anchor_text": ""}, {"url": "https://arshren.medium.com/?source=post_page-----da1adfc5d64f--------------------------------", "anchor_text": "Renu Khandelwal"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F31b07253bc35&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimple-explanation-of-transformers-in-nlp-da1adfc5d64f&user=Renu+Khandelwal&userId=31b07253bc35&source=post_page-31b07253bc35----da1adfc5d64f---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----da1adfc5d64f--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fda1adfc5d64f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimple-explanation-of-transformers-in-nlp-da1adfc5d64f&user=Renu+Khandelwal&userId=31b07253bc35&source=-----da1adfc5d64f---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fda1adfc5d64f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimple-explanation-of-transformers-in-nlp-da1adfc5d64f&source=-----da1adfc5d64f---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://unsplash.com/@chrislawton?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Chris Lawton"}, {"url": "https://unsplash.com/s/photos/transformation?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://towardsdatascience.com/sequence-2-sequence-model-with-attention-mechanism-9e9ca2a613a", "anchor_text": "equence-2-Sequence-model-with-Attention-mechanism"}, {"url": "https://towardsdatascience.com/intuitive-explanation-of-neural-machine-translation-129789e3c59f", "anchor_text": "Seq2Seq- Neural machine translation"}, {"url": "https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf", "anchor_text": "https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf"}, {"url": "https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf", "anchor_text": "https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf"}, {"url": "https://arxiv.org/pdf/1607.06450.pdf", "anchor_text": "Layer Normalization by Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton"}, {"url": "https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf", "anchor_text": "Attention Is All You Need"}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "The Illustrated TransformerDiscussions: Hacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments) Translations\u2026jalammar.github.io"}, {"url": "https://nlp.seas.harvard.edu/2018/04/03/attention.html", "anchor_text": "https://nlp.seas.harvard.edu/2018/04/03/attention.html"}, {"url": "https://medium.com/tag/nlp?source=post_page-----da1adfc5d64f---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/transformers?source=post_page-----da1adfc5d64f---------------transformers-----------------", "anchor_text": "Transformers"}, {"url": "https://medium.com/tag/self-attention?source=post_page-----da1adfc5d64f---------------self_attention-----------------", "anchor_text": "Self Attention"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----da1adfc5d64f---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/seq2seq?source=post_page-----da1adfc5d64f---------------seq2seq-----------------", "anchor_text": "Seq2seq"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fda1adfc5d64f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimple-explanation-of-transformers-in-nlp-da1adfc5d64f&user=Renu+Khandelwal&userId=31b07253bc35&source=-----da1adfc5d64f---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fda1adfc5d64f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimple-explanation-of-transformers-in-nlp-da1adfc5d64f&user=Renu+Khandelwal&userId=31b07253bc35&source=-----da1adfc5d64f---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fda1adfc5d64f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimple-explanation-of-transformers-in-nlp-da1adfc5d64f&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://arshren.medium.com/?source=post_page-----da1adfc5d64f--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----da1adfc5d64f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F31b07253bc35&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimple-explanation-of-transformers-in-nlp-da1adfc5d64f&user=Renu+Khandelwal&userId=31b07253bc35&source=post_page-31b07253bc35----da1adfc5d64f---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fb1cb44d62203&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimple-explanation-of-transformers-in-nlp-da1adfc5d64f&newsletterV3=31b07253bc35&newsletterV3Id=b1cb44d62203&user=Renu+Khandelwal&userId=31b07253bc35&source=-----da1adfc5d64f---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://arshren.medium.com/?source=post_page-----da1adfc5d64f--------------------------------", "anchor_text": "Written by Renu Khandelwal"}, {"url": "https://arshren.medium.com/followers?source=post_page-----da1adfc5d64f--------------------------------", "anchor_text": "5.9K Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----da1adfc5d64f--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F31b07253bc35&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimple-explanation-of-transformers-in-nlp-da1adfc5d64f&user=Renu+Khandelwal&userId=31b07253bc35&source=post_page-31b07253bc35----da1adfc5d64f---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fb1cb44d62203&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimple-explanation-of-transformers-in-nlp-da1adfc5d64f&newsletterV3=31b07253bc35&newsletterV3Id=b1cb44d62203&user=Renu+Khandelwal&userId=31b07253bc35&source=-----da1adfc5d64f---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/convolutional-neural-network-feature-map-and-filter-visualization-f75012a5a49c?source=author_recirc-----da1adfc5d64f----0---------------------6fc3fd06_6d83_4505_88cd_b06be1f74a43-------", "anchor_text": ""}, {"url": "https://arshren.medium.com/?source=author_recirc-----da1adfc5d64f----0---------------------6fc3fd06_6d83_4505_88cd_b06be1f74a43-------", "anchor_text": ""}, {"url": "https://arshren.medium.com/?source=author_recirc-----da1adfc5d64f----0---------------------6fc3fd06_6d83_4505_88cd_b06be1f74a43-------", "anchor_text": "Renu Khandelwal"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----da1adfc5d64f----0---------------------6fc3fd06_6d83_4505_88cd_b06be1f74a43-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/convolutional-neural-network-feature-map-and-filter-visualization-f75012a5a49c?source=author_recirc-----da1adfc5d64f----0---------------------6fc3fd06_6d83_4505_88cd_b06be1f74a43-------", "anchor_text": "Convolutional Neural Network: Feature Map and Filter VisualizationLearn how Convolutional Neural Networks understand images."}, {"url": "https://towardsdatascience.com/convolutional-neural-network-feature-map-and-filter-visualization-f75012a5a49c?source=author_recirc-----da1adfc5d64f----0---------------------6fc3fd06_6d83_4505_88cd_b06be1f74a43-------", "anchor_text": "\u00b78 min read\u00b7May 18, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff75012a5a49c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconvolutional-neural-network-feature-map-and-filter-visualization-f75012a5a49c&user=Renu+Khandelwal&userId=31b07253bc35&source=-----f75012a5a49c----0-----------------clap_footer----6fc3fd06_6d83_4505_88cd_b06be1f74a43-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/convolutional-neural-network-feature-map-and-filter-visualization-f75012a5a49c?source=author_recirc-----da1adfc5d64f----0---------------------6fc3fd06_6d83_4505_88cd_b06be1f74a43-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "6"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff75012a5a49c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconvolutional-neural-network-feature-map-and-filter-visualization-f75012a5a49c&source=-----da1adfc5d64f----0-----------------bookmark_preview----6fc3fd06_6d83_4505_88cd_b06be1f74a43-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----da1adfc5d64f----1---------------------6fc3fd06_6d83_4505_88cd_b06be1f74a43-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----da1adfc5d64f----1---------------------6fc3fd06_6d83_4505_88cd_b06be1f74a43-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----da1adfc5d64f----1---------------------6fc3fd06_6d83_4505_88cd_b06be1f74a43-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----da1adfc5d64f----1---------------------6fc3fd06_6d83_4505_88cd_b06be1f74a43-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----da1adfc5d64f----1---------------------6fc3fd06_6d83_4505_88cd_b06be1f74a43-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----da1adfc5d64f----1---------------------6fc3fd06_6d83_4505_88cd_b06be1f74a43-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----6fc3fd06_6d83_4505_88cd_b06be1f74a43-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----da1adfc5d64f----1---------------------6fc3fd06_6d83_4505_88cd_b06be1f74a43-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----da1adfc5d64f----1-----------------bookmark_preview----6fc3fd06_6d83_4505_88cd_b06be1f74a43-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----da1adfc5d64f----2---------------------6fc3fd06_6d83_4505_88cd_b06be1f74a43-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----da1adfc5d64f----2---------------------6fc3fd06_6d83_4505_88cd_b06be1f74a43-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----da1adfc5d64f----2---------------------6fc3fd06_6d83_4505_88cd_b06be1f74a43-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----da1adfc5d64f----2---------------------6fc3fd06_6d83_4505_88cd_b06be1f74a43-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----da1adfc5d64f----2---------------------6fc3fd06_6d83_4505_88cd_b06be1f74a43-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----da1adfc5d64f----2---------------------6fc3fd06_6d83_4505_88cd_b06be1f74a43-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----2-----------------clap_footer----6fc3fd06_6d83_4505_88cd_b06be1f74a43-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----da1adfc5d64f----2---------------------6fc3fd06_6d83_4505_88cd_b06be1f74a43-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----da1adfc5d64f----2-----------------bookmark_preview----6fc3fd06_6d83_4505_88cd_b06be1f74a43-------", "anchor_text": ""}, {"url": "https://arshren.medium.com/unleashing-an-end-to-end-predictive-model-pipeline-a-step-by-step-guide-2515a65cd8f6?source=author_recirc-----da1adfc5d64f----3---------------------6fc3fd06_6d83_4505_88cd_b06be1f74a43-------", "anchor_text": ""}, {"url": "https://arshren.medium.com/?source=author_recirc-----da1adfc5d64f----3---------------------6fc3fd06_6d83_4505_88cd_b06be1f74a43-------", "anchor_text": ""}, {"url": "https://arshren.medium.com/?source=author_recirc-----da1adfc5d64f----3---------------------6fc3fd06_6d83_4505_88cd_b06be1f74a43-------", "anchor_text": "Renu Khandelwal"}, {"url": "https://arshren.medium.com/unleashing-an-end-to-end-predictive-model-pipeline-a-step-by-step-guide-2515a65cd8f6?source=author_recirc-----da1adfc5d64f----3---------------------6fc3fd06_6d83_4505_88cd_b06be1f74a43-------", "anchor_text": "Unleashing an End-to-End Predictive Model Pipeline: A Step-by-Step GuideA Detailed ML Ops Pipeline for an End-to-End Predictive Model for Tabular Data"}, {"url": "https://arshren.medium.com/unleashing-an-end-to-end-predictive-model-pipeline-a-step-by-step-guide-2515a65cd8f6?source=author_recirc-----da1adfc5d64f----3---------------------6fc3fd06_6d83_4505_88cd_b06be1f74a43-------", "anchor_text": "\u00b712 min read\u00b7Apr 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F2515a65cd8f6&operation=register&redirect=https%3A%2F%2Farshren.medium.com%2Funleashing-an-end-to-end-predictive-model-pipeline-a-step-by-step-guide-2515a65cd8f6&user=Renu+Khandelwal&userId=31b07253bc35&source=-----2515a65cd8f6----3-----------------clap_footer----6fc3fd06_6d83_4505_88cd_b06be1f74a43-------", "anchor_text": ""}, {"url": "https://arshren.medium.com/unleashing-an-end-to-end-predictive-model-pipeline-a-step-by-step-guide-2515a65cd8f6?source=author_recirc-----da1adfc5d64f----3---------------------6fc3fd06_6d83_4505_88cd_b06be1f74a43-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2515a65cd8f6&operation=register&redirect=https%3A%2F%2Farshren.medium.com%2Funleashing-an-end-to-end-predictive-model-pipeline-a-step-by-step-guide-2515a65cd8f6&source=-----da1adfc5d64f----3-----------------bookmark_preview----6fc3fd06_6d83_4505_88cd_b06be1f74a43-------", "anchor_text": ""}, {"url": "https://arshren.medium.com/?source=post_page-----da1adfc5d64f--------------------------------", "anchor_text": "See all from Renu Khandelwal"}, {"url": "https://towardsdatascience.com/?source=post_page-----da1adfc5d64f--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://medium.com/mlearning-ai/understanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f?source=read_next_recirc-----da1adfc5d64f----0---------------------178ccc34_e0c4_443c_b421_a26255740b3e-------", "anchor_text": ""}, {"url": "https://medium.com/@martin-thissen?source=read_next_recirc-----da1adfc5d64f----0---------------------178ccc34_e0c4_443c_b421_a26255740b3e-------", "anchor_text": ""}, {"url": "https://medium.com/@martin-thissen?source=read_next_recirc-----da1adfc5d64f----0---------------------178ccc34_e0c4_443c_b421_a26255740b3e-------", "anchor_text": "Martin Thissen"}, {"url": "https://medium.com/mlearning-ai?source=read_next_recirc-----da1adfc5d64f----0---------------------178ccc34_e0c4_443c_b421_a26255740b3e-------", "anchor_text": "MLearning.ai"}, {"url": "https://medium.com/mlearning-ai/understanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f?source=read_next_recirc-----da1adfc5d64f----0---------------------178ccc34_e0c4_443c_b421_a26255740b3e-------", "anchor_text": "Understanding and Coding the Attention Mechanism \u2014 The Magic Behind TransformersIn this article, I\u2019ll give you an introduction to the attention mechanism and show you how to code the attention mechanism yourself."}, {"url": "https://medium.com/mlearning-ai/understanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f?source=read_next_recirc-----da1adfc5d64f----0---------------------178ccc34_e0c4_443c_b421_a26255740b3e-------", "anchor_text": "\u00b712 min read\u00b7Dec 6, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fmlearning-ai%2Ffe707a85cc3f&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fmlearning-ai%2Funderstanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f&user=Martin+Thissen&userId=f99c73950195&source=-----fe707a85cc3f----0-----------------clap_footer----178ccc34_e0c4_443c_b421_a26255740b3e-------", "anchor_text": ""}, {"url": "https://medium.com/mlearning-ai/understanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f?source=read_next_recirc-----da1adfc5d64f----0---------------------178ccc34_e0c4_443c_b421_a26255740b3e-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffe707a85cc3f&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fmlearning-ai%2Funderstanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f&source=-----da1adfc5d64f----0-----------------bookmark_preview----178ccc34_e0c4_443c_b421_a26255740b3e-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/using-transformers-for-computer-vision-6f764c5a078b?source=read_next_recirc-----da1adfc5d64f----1---------------------178ccc34_e0c4_443c_b421_a26255740b3e-------", "anchor_text": ""}, {"url": "https://wolfecameron.medium.com/?source=read_next_recirc-----da1adfc5d64f----1---------------------178ccc34_e0c4_443c_b421_a26255740b3e-------", "anchor_text": ""}, {"url": "https://wolfecameron.medium.com/?source=read_next_recirc-----da1adfc5d64f----1---------------------178ccc34_e0c4_443c_b421_a26255740b3e-------", "anchor_text": "Cameron R. Wolfe"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----da1adfc5d64f----1---------------------178ccc34_e0c4_443c_b421_a26255740b3e-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/using-transformers-for-computer-vision-6f764c5a078b?source=read_next_recirc-----da1adfc5d64f----1---------------------178ccc34_e0c4_443c_b421_a26255740b3e-------", "anchor_text": "Using Transformers for Computer VisionAre Vision Transformers actually useful?"}, {"url": "https://towardsdatascience.com/using-transformers-for-computer-vision-6f764c5a078b?source=read_next_recirc-----da1adfc5d64f----1---------------------178ccc34_e0c4_443c_b421_a26255740b3e-------", "anchor_text": "\u00b713 min read\u00b7Oct 5, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6f764c5a078b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-transformers-for-computer-vision-6f764c5a078b&user=Cameron+R.+Wolfe&userId=28aa6026c553&source=-----6f764c5a078b----1-----------------clap_footer----178ccc34_e0c4_443c_b421_a26255740b3e-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/using-transformers-for-computer-vision-6f764c5a078b?source=read_next_recirc-----da1adfc5d64f----1---------------------178ccc34_e0c4_443c_b421_a26255740b3e-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "4"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6f764c5a078b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-transformers-for-computer-vision-6f764c5a078b&source=-----da1adfc5d64f----1-----------------bookmark_preview----178ccc34_e0c4_443c_b421_a26255740b3e-------", "anchor_text": ""}, {"url": "https://jehillparikh.medium.com/u-nets-with-attention-c8d7e9bf2416?source=read_next_recirc-----da1adfc5d64f----0---------------------178ccc34_e0c4_443c_b421_a26255740b3e-------", "anchor_text": ""}, {"url": "https://jehillparikh.medium.com/?source=read_next_recirc-----da1adfc5d64f----0---------------------178ccc34_e0c4_443c_b421_a26255740b3e-------", "anchor_text": ""}, {"url": "https://jehillparikh.medium.com/?source=read_next_recirc-----da1adfc5d64f----0---------------------178ccc34_e0c4_443c_b421_a26255740b3e-------", "anchor_text": "Jehill Parikh"}, {"url": "https://jehillparikh.medium.com/u-nets-with-attention-c8d7e9bf2416?source=read_next_recirc-----da1adfc5d64f----0---------------------178ccc34_e0c4_443c_b421_a26255740b3e-------", "anchor_text": "U-Nets with attentionU-Net are popular NN architecture which are employed for many applications and were initially developed for medical image segmentation."}, {"url": "https://jehillparikh.medium.com/u-nets-with-attention-c8d7e9bf2416?source=read_next_recirc-----da1adfc5d64f----0---------------------178ccc34_e0c4_443c_b421_a26255740b3e-------", "anchor_text": "\u00b72 min read\u00b7Nov 15, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2Fc8d7e9bf2416&operation=register&redirect=https%3A%2F%2Fjehillparikh.medium.com%2Fu-nets-with-attention-c8d7e9bf2416&user=Jehill+Parikh&userId=c972081b627e&source=-----c8d7e9bf2416----0-----------------clap_footer----178ccc34_e0c4_443c_b421_a26255740b3e-------", "anchor_text": ""}, {"url": "https://jehillparikh.medium.com/u-nets-with-attention-c8d7e9bf2416?source=read_next_recirc-----da1adfc5d64f----0---------------------178ccc34_e0c4_443c_b421_a26255740b3e-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc8d7e9bf2416&operation=register&redirect=https%3A%2F%2Fjehillparikh.medium.com%2Fu-nets-with-attention-c8d7e9bf2416&source=-----da1adfc5d64f----0-----------------bookmark_preview----178ccc34_e0c4_443c_b421_a26255740b3e-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/language-models-gpt-and-gpt-2-8bdb9867c50a?source=read_next_recirc-----da1adfc5d64f----1---------------------178ccc34_e0c4_443c_b421_a26255740b3e-------", "anchor_text": ""}, {"url": "https://wolfecameron.medium.com/?source=read_next_recirc-----da1adfc5d64f----1---------------------178ccc34_e0c4_443c_b421_a26255740b3e-------", "anchor_text": ""}, {"url": "https://wolfecameron.medium.com/?source=read_next_recirc-----da1adfc5d64f----1---------------------178ccc34_e0c4_443c_b421_a26255740b3e-------", "anchor_text": "Cameron R. Wolfe"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----da1adfc5d64f----1---------------------178ccc34_e0c4_443c_b421_a26255740b3e-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/language-models-gpt-and-gpt-2-8bdb9867c50a?source=read_next_recirc-----da1adfc5d64f----1---------------------178ccc34_e0c4_443c_b421_a26255740b3e-------", "anchor_text": "Language Models: GPT and GPT-2How smaller language models inspired modern breakthroughs"}, {"url": "https://towardsdatascience.com/language-models-gpt-and-gpt-2-8bdb9867c50a?source=read_next_recirc-----da1adfc5d64f----1---------------------178ccc34_e0c4_443c_b421_a26255740b3e-------", "anchor_text": "\u00b713 min read\u00b7Nov 24, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8bdb9867c50a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flanguage-models-gpt-and-gpt-2-8bdb9867c50a&user=Cameron+R.+Wolfe&userId=28aa6026c553&source=-----8bdb9867c50a----1-----------------clap_footer----178ccc34_e0c4_443c_b421_a26255740b3e-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/language-models-gpt-and-gpt-2-8bdb9867c50a?source=read_next_recirc-----da1adfc5d64f----1---------------------178ccc34_e0c4_443c_b421_a26255740b3e-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8bdb9867c50a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flanguage-models-gpt-and-gpt-2-8bdb9867c50a&source=-----da1adfc5d64f----1-----------------bookmark_preview----178ccc34_e0c4_443c_b421_a26255740b3e-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/translate-with-gpt-3-9903c4a6f385?source=read_next_recirc-----da1adfc5d64f----2---------------------178ccc34_e0c4_443c_b421_a26255740b3e-------", "anchor_text": ""}, {"url": "https://medium.com/@bnjmn_marie?source=read_next_recirc-----da1adfc5d64f----2---------------------178ccc34_e0c4_443c_b421_a26255740b3e-------", "anchor_text": ""}, {"url": "https://medium.com/@bnjmn_marie?source=read_next_recirc-----da1adfc5d64f----2---------------------178ccc34_e0c4_443c_b421_a26255740b3e-------", "anchor_text": "Benjamin Marie"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----da1adfc5d64f----2---------------------178ccc34_e0c4_443c_b421_a26255740b3e-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/translate-with-gpt-3-9903c4a6f385?source=read_next_recirc-----da1adfc5d64f----2---------------------178ccc34_e0c4_443c_b421_a26255740b3e-------", "anchor_text": "Translate with GPT-3Machine translation but without a machine translation system"}, {"url": "https://towardsdatascience.com/translate-with-gpt-3-9903c4a6f385?source=read_next_recirc-----da1adfc5d64f----2---------------------178ccc34_e0c4_443c_b421_a26255740b3e-------", "anchor_text": "\u00b718 min read\u00b7Nov 22, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9903c4a6f385&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftranslate-with-gpt-3-9903c4a6f385&user=Benjamin+Marie&userId=ad2a414578b3&source=-----9903c4a6f385----2-----------------clap_footer----178ccc34_e0c4_443c_b421_a26255740b3e-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/translate-with-gpt-3-9903c4a6f385?source=read_next_recirc-----da1adfc5d64f----2---------------------178ccc34_e0c4_443c_b421_a26255740b3e-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "4"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9903c4a6f385&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftranslate-with-gpt-3-9903c4a6f385&source=-----da1adfc5d64f----2-----------------bookmark_preview----178ccc34_e0c4_443c_b421_a26255740b3e-------", "anchor_text": ""}, {"url": "https://pub.towardsai.net/build-chatgpt-like-chatbots-with-customized-knowledge-for-your-websites-using-simple-programming-f393206c6626?source=read_next_recirc-----da1adfc5d64f----3---------------------178ccc34_e0c4_443c_b421_a26255740b3e-------", "anchor_text": ""}, {"url": "https://lucianosphere.medium.com/?source=read_next_recirc-----da1adfc5d64f----3---------------------178ccc34_e0c4_443c_b421_a26255740b3e-------", "anchor_text": ""}, {"url": "https://lucianosphere.medium.com/?source=read_next_recirc-----da1adfc5d64f----3---------------------178ccc34_e0c4_443c_b421_a26255740b3e-------", "anchor_text": "LucianoSphere"}, {"url": "https://pub.towardsai.net/?source=read_next_recirc-----da1adfc5d64f----3---------------------178ccc34_e0c4_443c_b421_a26255740b3e-------", "anchor_text": "Towards AI"}, {"url": "https://pub.towardsai.net/build-chatgpt-like-chatbots-with-customized-knowledge-for-your-websites-using-simple-programming-f393206c6626?source=read_next_recirc-----da1adfc5d64f----3---------------------178ccc34_e0c4_443c_b421_a26255740b3e-------", "anchor_text": "Build ChatGPT-like Chatbots With Customized Knowledge for Your Websites, Using Simple ProgrammingLike ChatGPT but in a form that you can plug into your website and expand with any kind of tailored information by combining basic\u2026"}, {"url": "https://pub.towardsai.net/build-chatgpt-like-chatbots-with-customized-knowledge-for-your-websites-using-simple-programming-f393206c6626?source=read_next_recirc-----da1adfc5d64f----3---------------------178ccc34_e0c4_443c_b421_a26255740b3e-------", "anchor_text": "\u00b711 min read\u00b7Dec 26, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-artificial-intelligence%2Ff393206c6626&operation=register&redirect=https%3A%2F%2Fpub.towardsai.net%2Fbuild-chatgpt-like-chatbots-with-customized-knowledge-for-your-websites-using-simple-programming-f393206c6626&user=LucianoSphere&userId=d28939b5ab78&source=-----f393206c6626----3-----------------clap_footer----178ccc34_e0c4_443c_b421_a26255740b3e-------", "anchor_text": ""}, {"url": "https://pub.towardsai.net/build-chatgpt-like-chatbots-with-customized-knowledge-for-your-websites-using-simple-programming-f393206c6626?source=read_next_recirc-----da1adfc5d64f----3---------------------178ccc34_e0c4_443c_b421_a26255740b3e-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "14"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff393206c6626&operation=register&redirect=https%3A%2F%2Fpub.towardsai.net%2Fbuild-chatgpt-like-chatbots-with-customized-knowledge-for-your-websites-using-simple-programming-f393206c6626&source=-----da1adfc5d64f----3-----------------bookmark_preview----178ccc34_e0c4_443c_b421_a26255740b3e-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----da1adfc5d64f--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----da1adfc5d64f--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----da1adfc5d64f--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----da1adfc5d64f--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----da1adfc5d64f--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----da1adfc5d64f--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----da1adfc5d64f--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----da1adfc5d64f--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----da1adfc5d64f--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----da1adfc5d64f--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}