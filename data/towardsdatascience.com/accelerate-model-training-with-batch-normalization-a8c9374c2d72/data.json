{"url": "https://towardsdatascience.com/accelerate-model-training-with-batch-normalization-a8c9374c2d72", "time": 1683002798.381686, "path": "towardsdatascience.com/accelerate-model-training-with-batch-normalization-a8c9374c2d72/", "webpage": {"metadata": {"title": "Accelerate Model Training With Batch Normalization | by Anirudh S | Towards Data Science", "h1": "Accelerate Model Training With Batch Normalization", "description": "What is batch normalization and how it works? What enables it to achieve faster training? We'll best answer these with illustrations to develop an intuition."}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/search/cs?searchtype=author&query=Ioffe%2C+S", "anchor_text": "Sergey Ioffe", "paragraph_index": 0}, {"url": "https://arxiv.org/search/cs?searchtype=author&query=Szegedy%2C+C", "anchor_text": "Christian Szegedy", "paragraph_index": 0}, {"url": "https://arxiv.org/pdf/1805.11604.pdf", "anchor_text": "This", "paragraph_index": 15}, {"url": "https://arxiv.org/pdf/1502.03167.pdf", "anchor_text": "https://arxiv.org/pdf/1502.03167.pdf", "paragraph_index": 36}], "all_paragraphs": ["The Batch Normalization paper published back in 2015 by Sergey Ioffe, Christian Szegedy took the deep learning community by storm. It became one of the most implemented techniques in deep learning after it was released. Notably, its ability to accelerate training of deep learning models and achieve the same accuracy in 14 times fewer training steps was a great catch. Indeed that brought in the attention which it gets today (who doesn\u2019t want to train faster?). And there\u2019s been a lot of similar papers like layer normalization, instance normalization and a few others. These try to overcome some of the disadvantages of batch normalization.", "Let\u2019s start by understanding what the typical input data normalization does. Though deep learning models don\u2019t need much feature engineering, scaling input features to the same range helps to train the model faster. Min-Max scaling is one such feature scaling methods that brings all the inputs to the 0 to 1 range. It\u2019s given by the following expression.", "It\u2019s simply subtracting the least value in the data from each data point and dividing it by the range of the values in the data. The range is just the maximum value in the input data distribution minus the least value. You could intuitively try to digest how this improves model training with the following example. Consider a machine learning model that takes the square footage and the number of rooms in a house as input and predicts the price.", "While the square footage of houses may be in hundreds or thousands, the number of rooms would mostly be under 10. Owing to this vast difference in range between the different input features, the model struggles converge. This is because the model values larger features more than the smaller ones.", "One more thing to notice is that a normalized feature column won\u2019t have large variations in values like the unnormalized version. For example, a mansion may have thousands of square feet of land but a small house may be only a few hundred square feet. While the model should value houses with larger area more, too big a difference will hinder the model\u2019s learning.", "We saw how an ML model can benefit from normalized input features in the previous section. Now, let\u2019s consider a fully connected deep neural network as our model. And, what if we added normalization for the hidden layers too? Technically, each hidden layer\u2019s input is the previous layer\u2019s output. It seems about right to apply normalization to that too, right? Yes. And that\u2019s what batch normalization does.", "In naive gradient descent, we feed one input at a time and update the model using backpropagation. In contrast, using mini-batch gradient descent, we pass forward and back a bundle of input examples at once. And this bundle is what we call the \u201cbatch\u201c. Therefore, each hidden layer would see batches of activations coming from the layer before. Let\u2019s explore further with an example.", "Consider a fully connected neural network with 2 hidden layers and let\u2019s assume we\u2019re training it on the MNIST digits dataset. In case you\u2019re not familiar with MNIST, the dataset contains 28\u00d728 sized images of digits from 0 to 9. The digit\u2019s image is given as input to the model by ravelling it into a long 1D vector of size 784. While training in batches, multiple digit image vectors are stacked and fed. Given that, the input shape becomes (batch_size, 784).", "The above illustration\u2019s batch size is 2 and the number of features in each of the inputs is 784 (pixels). For simplicity\u2019s sake, let\u2019s assume that each layer of our model has 50 neurons each. Therefore, the activations of the first hidden layer would be of shape (2,50). Because 2 is our batch size and 50 is the number of neurons. If our batch size is 64, then the shape would be (64,784)", "Further, let\u2019s dive in to inspect the activation of a single neuron of the first hidden layer. A single neuron will produce just one activation if only one input was given. However, if the inputs are given in batches, it produces activation of shape (batch_size,1). Such that 1 activation for each input in the batch.", "Normally, each layer in a network would apply a non-linear activation function like \u2018Sigmoid\u2018 after multiplying the input with the weights. If we\u2019re inserting a batch normalization layer, we have to add it before we apply the non-linearity. The batch normalization layer normalizes the activations by applying the standardization method.", "It subtracts the mean from the activations and divides the difference by the standard deviation. The standard deviation is just the square root of variance. In detail, we calculate the mean and standard deviation across the batch and subtract and divide respectively.", "You see, the layers adjust their parameters during backpropagation with the assumption that their input distribution stays the same. But all the layers are updated with backpropagation thereby changing each of their outputs.", "This changes the inputs of the layers as each layer gets its inputs from the previous layer. This is called internal covariate shift. Hence, the layers experience a constantly changing input distribution. Importantly, this normalization method standardizes the distribution of the output activations.", "Therefore it produces the desired distribution of activations that don\u2019t vary too much when the weights change. In other words, it minimizes the internal covariate shift and aids the model to learn faster.", "Although this is regarded as the main contributor for batch normalization\u2019s success, some papers attribute it to other factors. This paper tells about how the batch normalization layer smoothens the loss curve which helps train faster.", "Again, let\u2019s just consider the activation of a single neuron for clarity and set the batch size as 64. And, our two hidden layers have 50 neurons each. As we saw before, the output of a single neuron would be (64,1). The batch normalization layer computes the mean (\u03bc) and standard deviation (\u03c3) of the 64 values in the batch.", "It is this \u03bc value that is subtracted from the 64 values. And the \u03c3 divides the mean subtracted activations from the neuron. Rather, let\u2019s visualize what the batch normalization layer does.", "Note: This explanation is just for a single neuron in a layer for simplicity.", "Batch Normalization\u2019s output considering all the neurons differs from this in shape but all the processes are the same.", "If we consider all the neurons in a layer, the activation shape would be (64,50). The mean and variance would then be calculated for each of the 50 neurons leading to 50 means and 50 variance values. And each column corresponding to a single neuron should be normalized with its respective mean and variance.", "Well, we\u2019ve successfully normalized a hidden layer\u2019s activations and all that\u2019s left is applying the nonlinearity, right?", "No. It does not end here!", "Well, there\u2019s a problem with normalizing the activations of a hidden layer. If we\u2019re feeding the normalized activations to a Sigmoid, the expressive power of the model is reduced. To clarify, the normalized activations are brought a 0 mean and unit variance distribution. Because of that, the activations fall in the linear region of the sigmoid non-linearity. We\u2019ll go ahead and try to plot the functions using matplotlib and Python.", "Consider this data distribution as the activation that we are gonna normalize. Below, the diagram shows the plot of the sigmoid of unnormalized data [-10 to 10].", "Now, we\u2019ll normalize the same data and see where the data points fall on the curve. And, here\u2019s the Python code to do that.", "As we saw, batch normalization doesn\u2019t let the model take advantage of the sigmoid non-linearity. This makes the sigmoid layer almost linear and we don\u2019t want that! So, the authors of this paper gave a way to restore the representation power of the network. They introduced two parameters that shift and scale the normalized activations.", "These two parameters are introduced so that they can undo the normalization done by the batch normalization layer. If Gamma is set to \u03c3 and Beta to the mean, it\u2019ll fully denormalize the activations back to normal. At first, it may seem counter-intuitive. But this is done to enable the model to adjust the normalization the way it wants if that reduces the loss. And this brings back the expressive power that is lost when we normalized the activations.", "One of the key advantages of using batch normalization is that it is fully differentiable. So, this means that you can backpropagate through it and can train using gradient descent. That said, we can update Gamma and Beta along with the model parameters.", "Batch normalization after a convolution layer is a bit different. Normally, in a convolution layer, the input is fed as a 4-D tensor of shape (batch, Height, Width, Channels). But, the batch normalization layer normalizes the tensor across the batch, height and width dimensions. For each channel, mean and variance are computed across the other three dimensions. So, that gives \u2018C\u2019 number of mean and variance values which are then used for normalization.", "As a result, for each of the 64 channels in the above example, mean and variance are calculated for the 10x200x200 (400000) values. This gives us 64 means and 64 variances to normalize each channel separately.", "Batch normalization can prevent a network from getting stuck in the saturation regions of a non-linearity. It also helps the weights in a layer to learn faster as it normalizes the inputs. You see, a large input value (X) to a layer would cause the activations to be large for even small weights. This pushes the activations to the saturation regions of sigmoid or tanh nonlinearities.", "Saturated activations have very small derivatives. Hence, they make very small gradient steps and slow down the training. This effect grows more as the network depth increases. But batch normalization puts the activations where it wants it to be on the sigmoid.", "And this prevents the network from getting stuck due to saturation. Also, it removes the necessity to initialize the model parameters carefully. Sometimes random initialization makes some weight values high that pushes the activations into saturation. But batch normalization allows us to be less careful about initialization.", "The authors claim that it also regularizes the model and reduces the need for Dropout. This regularization effect is due to normalization with mini-batch statistics (which introduces some noise) rather than the population statistics. If the batch size is very large, the regularization effect diminishes.", "Since the gradients\u2019 dependency on the scale of the weights is reduced, it allows us to use higher learning rates.", "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift https://arxiv.org/pdf/1502.03167.pdf", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Always Believing there's more to learn!"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fa8c9374c2d72&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faccelerate-model-training-with-batch-normalization-a8c9374c2d72&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faccelerate-model-training-with-batch-normalization-a8c9374c2d72&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faccelerate-model-training-with-batch-normalization-a8c9374c2d72&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faccelerate-model-training-with-batch-normalization-a8c9374c2d72&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----a8c9374c2d72--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a8c9374c2d72--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@baakchsu.sprx77?source=post_page-----a8c9374c2d72--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@baakchsu.sprx77?source=post_page-----a8c9374c2d72--------------------------------", "anchor_text": "Anirudh S"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1a92cae35860&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faccelerate-model-training-with-batch-normalization-a8c9374c2d72&user=Anirudh+S&userId=1a92cae35860&source=post_page-1a92cae35860----a8c9374c2d72---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa8c9374c2d72&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faccelerate-model-training-with-batch-normalization-a8c9374c2d72&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa8c9374c2d72&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faccelerate-model-training-with-batch-normalization-a8c9374c2d72&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://hackerstreak.com/batch-normalization-how-it-really-works/", "anchor_text": "Accelerate Model Training With Batch Normalization"}, {"url": "https://unsplash.com/@chillarea?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "stam"}, {"url": "https://unsplash.com/?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://arxiv.org/search/cs?searchtype=author&query=Ioffe%2C+S", "anchor_text": "Sergey Ioffe"}, {"url": "https://arxiv.org/search/cs?searchtype=author&query=Szegedy%2C+C", "anchor_text": "Christian Szegedy"}, {"url": "https://arxiv.org/pdf/1805.11604.pdf", "anchor_text": "This"}, {"url": "https://arxiv.org/pdf/1502.03167.pdf", "anchor_text": "https://arxiv.org/pdf/1502.03167.pdf"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----a8c9374c2d72---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----a8c9374c2d72---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/deep-neural-networks?source=post_page-----a8c9374c2d72---------------deep_neural_networks-----------------", "anchor_text": "Deep Neural Networks"}, {"url": "https://medium.com/tag/batch-normalization?source=post_page-----a8c9374c2d72---------------batch_normalization-----------------", "anchor_text": "Batch Normalization"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa8c9374c2d72&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faccelerate-model-training-with-batch-normalization-a8c9374c2d72&user=Anirudh+S&userId=1a92cae35860&source=-----a8c9374c2d72---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa8c9374c2d72&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faccelerate-model-training-with-batch-normalization-a8c9374c2d72&user=Anirudh+S&userId=1a92cae35860&source=-----a8c9374c2d72---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa8c9374c2d72&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faccelerate-model-training-with-batch-normalization-a8c9374c2d72&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a8c9374c2d72--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fa8c9374c2d72&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faccelerate-model-training-with-batch-normalization-a8c9374c2d72&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----a8c9374c2d72---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----a8c9374c2d72--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----a8c9374c2d72--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----a8c9374c2d72--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----a8c9374c2d72--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----a8c9374c2d72--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----a8c9374c2d72--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----a8c9374c2d72--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----a8c9374c2d72--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@baakchsu.sprx77?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@baakchsu.sprx77?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Anirudh S"}, {"url": "https://medium.com/@baakchsu.sprx77/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "127 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1a92cae35860&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faccelerate-model-training-with-batch-normalization-a8c9374c2d72&user=Anirudh+S&userId=1a92cae35860&source=post_page-1a92cae35860--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Ff34e096e7feb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faccelerate-model-training-with-batch-normalization-a8c9374c2d72&newsletterV3=1a92cae35860&newsletterV3Id=f34e096e7feb&user=Anirudh+S&userId=1a92cae35860&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}