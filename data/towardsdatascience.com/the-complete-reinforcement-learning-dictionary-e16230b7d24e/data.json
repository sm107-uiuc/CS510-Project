{"url": "https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e", "time": 1682995145.764265, "path": "towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e/", "webpage": {"metadata": {"title": "The Complete Reinforcement Learning Dictionary | by Shaked Zychlinski | Towards Data Science", "h1": "The Complete Reinforcement Learning Dictionary", "description": "Whenever I begin learning a subject which is new to me, I find the hardest thing to cope with is its new terminology. Every field have many terms and definitions which are completely obscure to an\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e?sk=9d29f303db3650387c489c9565b43636", "anchor_text": "Friends Link", "paragraph_index": 0}, {"url": "https://hackernoon.com/intuitive-rl-intro-to-advantage-actor-critic-a2c-4ff545978752", "anchor_text": "here", "paragraph_index": 6}, {"url": "https://en.wikipedia.org/wiki/Slot_machine", "anchor_text": "slot-machines", "paragraph_index": 11}, {"url": "https://en.wikipedia.org/wiki/Circular_buffer", "anchor_text": "cyclic memory buffers", "paragraph_index": 22}, {"url": "https://en.wikipedia.org/wiki/Reservoir_sampling", "anchor_text": "reservoir-sampling", "paragraph_index": 22}, {"url": "https://en.wikipedia.org/wiki/Blackjack", "anchor_text": "Blackjack", "paragraph_index": 26}, {"url": "https://medium.com/@shakedzy/qrash-course-deep-q-networks-from-the-ground-up-1bbda41d3677", "anchor_text": "this blogpost", "paragraph_index": 31}, {"url": "https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf", "anchor_text": "paper", "paragraph_index": 34}, {"url": "https://link.springer.com/content/pdf/10.1007%2FBF00992696.pdf", "anchor_text": "source", "paragraph_index": 34}, {"url": "https://en.wikipedia.org/wiki/Dynamic_programming", "anchor_text": "Dynamic Programming", "paragraph_index": 41}, {"url": "http://shakedzy.xyz", "anchor_text": "shakedzy.xyz", "paragraph_index": 48}], "all_paragraphs": ["Read this on Medium without being a Medium member using this Friends Link!", "Whenever I begin learning a subject which is new to me, I find the hardest thing to cope with is its new terminology. Every field have many terms and definitions which are completely obscure to an outsider, and can make a newcomer\u2019s first step quite difficult.", "When I made my first step into the world or Reinforcement Learning, I was quite overwhelmed by the new terms which popped-up every other line, and it always surprised me how behind those complex words stood quite simple and logical ideas. I therefore decided to write them all down in my own words, so I\u2019ll always be able to look them up in case I forget. This is how this dictionary came to be.", "This is not an introduction post to Reinforcement Learning, rather it\u2019s a supplementary tool to assist while studying. If you do look to start your path in this field too, I can recommend the following:", "I will do my best to try and keep on updating this dictionary. Feel free to let me know if I\u2019ve missed anything important or got something wrong.", "Actions: Actions are the Agent\u2019s methods which allow it to interact and change its environment, and thus transfer between states. Every action performed by the Agent yields a reward from the environment. The decision of which action to choose is made by the policy.", "Actor-Critic: When attempting to solve a Reinforcement Learning problem, there are two main methods one can choose from: calculating the Value Functions or Q-Values of each state and choosing actions according to those, or directly compute a policy which defines the probabilities each action should be taken depending on the current state, and act according to it. Actor-Critic algorithms combine the two methods in order to create a more robust method. A great illustrated-comics explanation can be found here.", "Advantage Function: Usually denoted as A(s,a), the Advantage function is a measure of how much is a certain action a good or bad decision given a certain state \u2014 or more simply, what is the advantage of selecting a certain action from a certain state. It is defined mathematically as:", "where r(s,a) is the expected reward of action a from state s, and r(s) is the expected reward of the entire state s, before an action was selected. It can also be viewed as:", "where Q(s,a) is the Q Value and V(s) is the Value function.", "Agent: The learning and acting part of a Reinforcement Learning problem, which tries to maximize the rewards it is given by the Environment. Putting it simply, the Agent is the model which you try to design.", "Bandits: Formally named \u201ck-Armed Bandits\u201d after the nickname \u201cone-armed bandit\u201d given to slot-machines, these are considered to be the simplest type of Reinforcement Learning tasks. Bandits have no different states, but only one \u2014 and the reward taken under consideration is only the immediate one. Hence, bandits can be thought of as having single-state episodes. Each of the k-arms is considered an action, and the objective is to learn the policy which will maximize the expected reward after each action (or arm-pulling). Contextual Bandits are a slightly more complex task, where each state may be different and affect the outcome of the actions \u2014 hence every time the context is different. Still, the task remains a single-state episodic task, and one context cannot have an influence on others.", "Bellman Equation: Formally, Bellman equation defines the relationships between a given state (or state-action pair) to its successors. While many forms exist, the most common one usually encountered in Reinforcement Learning tasks is the Bellman equation for the optimal Q-Value , which is given by:", "or when no uncertainty exists (meaning, probabilities are either 1 or 0):", "where the asterisk sign indicates optimal value. Some algorithms, such as Q-Learning, are basing their learning procedure over it.", "Continuous Tasks: Reinforcement Learning tasks which are not made of episodes, but rather last forever. This tasks have no terminal states. For simplicity, they are usually assumed to be made of one never-ending episode.", "Deep Reinforcement Learning: The use of a Reinforcement Learning algorithm with a deep neural network as an approximator for the learning part. This is usually done in order to cope with problems where the number of possible states and actions scales fast, and an exact solution in no longer feasible.", "Discount Factor (\u03b3): The discount factor, usually denoted as \u03b3, is a factor multiplying the future expected reward, and varies on the range of [0,1]. It controls the importance of the future rewards versus the immediate ones. The lower the discount factor is, the less important future rewards are, and the Agent will tend to focus on actions which will yield immediate rewards only.", "Environment: Everything which isn\u2019t the Agent; everything the Agent can interact with, either directly or indirectly. The environment changes as the Agent performs actions; every such change is considered a state-transition. Every action the Agent performs yields a reward received by the Agent.", "Episode: All states that come in between an initial-state and a terminal-state; for example: one game of Chess. The Agent\u2019s goal it to maximize the total reward it receives during an episode. In situations where there is no terminal-state, we consider an infinite episode. It is important to remember that different episodes are completely independent of one another.", "Episodic Tasks: Reinforcement Learning tasks which are made of different episodes (meaning, each episode has a terminal state).", "Expected Return: Sometimes referred to as \u201coverall reward\u201d and occasionally denoted as G, is the expected reward over an entire episode.", "Experience Replay: As Reinforcement Learning tasks have no pre-generated training sets which they can learn from, the Agent must keep records of all the state-transitions it encountered so it can learn from them later. The memory-buffer used to store this is often referred to as Experience Replay. There are several types and architectures of these memory buffers \u2014 but some very common ones are the cyclic memory buffers (which makes sure the Agent keeps training over its new behavior rather than things that might no longer be relevant) and reservoir-sampling-based memory buffers (which guarantees each state-transition recorded has an even probability to be inserted to the buffer).", "Exploitation & Exploration: Reinforcement Learning tasks have no pre-generated training sets which they can learn from \u2014 they create their own experience and learn \u201con the fly\u201d. To be able to do so, the Agent needs to try many different actions in many different states in order to try and learn all available possibilities and find the path which will maximize its overall reward; this is known as Exploration, as the Agent explores the Environment. On the other hand, if all the Agent will do is explore, it will never maximize the overall reward \u2014 it must also use the information it learned to do so. This is known as Exploitation, as the Agent exploits its knowledge to maximize the rewards it receives. The trade-off between the two is one of the greatest challenges of Reinforcement Learning problems, as the two must be balanced in order to allow the Agent to both explore the environment enough, but also exploit what it learned and repeat the most rewarding path it found.", "Greedy Policy, \u03b5-Greedy Policy: A greedy policy means the Agent constantly performs the action that is believed to yield the highest expected reward. Obviously, such a policy will not allow the Agent to explore at all. In order to still allow some exploration, an \u03b5-greedy policy is often used instead: a number (named \u03b5) in the range of [0,1] is selected, and prior selecting an action, a random number in the range of [0,1] is selected. if that number is larger than \u03b5, the greedy action is selected \u2014 but if it\u2019s lower, a random action is selected. Note that if \u03b5=0, the policy becomes the greedy policy, and if \u03b5=1, always explore.", "Markov Decision Process (MDP): The Markov Property means that each state is dependent solely on its preceding state, the selected action taken from that state and the reward received immediately after that action was executed. Mathematically, it means: s\u2019 = s\u2019(s,a,r), where s\u2019 is the future state, s is its preceding state and a and r are the action and reward. No prior knowledge of what happened before s is needed \u2014 the Markov Property assumes that s holds all the relevant information within it. A Markov Decision Process is decision process based on these assumptions.", "Model-Based & Model-Free: Model-based and model-free are two different approaches an Agent can choose when trying to optimize its policy. This is best explained using an example: assume you\u2019re trying to learn how to play Blackjack. You can do so in two ways: one, you calculate in advance, before the game begins, the winning probabilities of all states and all the state-transition probabilities given all the possible actions, and then simply act according to you calculations. The second option is to simply play without any prior knowledge, and gain information using \u201ctrial-and-error\u201d. Note that using the first approach, you\u2019re basically modeling your environment, while the second approach requires no information about the environment. This is exactly the difference between model-based and model-free; the first method is model-based, while the latter is model-free.", "Monte Carlo (MC): Monte Carlo methods are algorithms which use repeated random sampling in order to achieve a result. They are used quite often in Reinforcement Learning algorithms to obtain expected values; for example \u2014 calculating a state Value function by returning to the same state over and over again, and averaging over the actual cumulative reward received each time.", "On-Policy & Off-Policy: Every Reinforcement Learning algorithm must follow some policy in order to decide which actions to perform at each state. Still, the learning procedure of the algorithm doesn\u2019t have to take into account that policy while learning. Algorithms which concern about the policy which yielded past state-action decisions are referred to as on-policy algorithms, while those ignoring it are known as off-policy. A well known off-policy algorithm is Q-Learning, as its update rule uses the action which will yield the highest Q-Value, while the actual policy used might restrict that action or choose another. The on-policy variation of Q-Learning is known as Sarsa, where the update rule uses the action chosen by the followed policy.", "Policy (\u03c0): The policy, denoted as \u03c0 (or sometimes \u03c0(a|s)), is a mapping from some state s to the probabilities of selecting each possible action given that state. For example, a greedy policy outputs for every state the action with the highest expected Q-Value.", "Q-Learning: Q-Learning is an off-policy Reinforcement Learning algorithm, considered as one of the very basic ones. In its most simplified form, it uses a table to store all Q-Values of all possible state-action pairs possible. It updates this table using the Bellman equation, while action selection is usually made with an \u03b5-greedy policy. In its simplest form (no uncertainties in state-transitions and expected rewards), the update rule of Q-Learning is:", "A more complex version of it, though far more popular, is the Deep Q-Network variant (which is sometimes even referred to simply as Deep Q-Learning or just Q-Learning). This variant replaces the state-action table with a neural network in order to cope with large-scale tasks, where the number of possible state-action pairs can be enormous. You can find a tutorial for this algorithm in this blogpost.", "Q Value (Q Function): Usually denoted as Q(s,a) (sometimes with a \u03c0 subscript, and sometimes as Q(s,a; \u03b8) in Deep RL), Q Value is a measure of the overall expected reward assuming the Agent is in state s and performs action a, and then continues playing until the end of the episode following some policy \u03c0. Its name is an abbreviation of the word \u201cQuality\u201d, and it is defined mathematically as:", "where N is the number of states from state s till the terminal state, \u03b3 is the discount factor and r\u2070 is the immediate reward received after performing action a in state s.", "REINFORCE Algorithms: REINFORCE algorithms are a family of Reinforcement Learning algorithms which update their policy parameters according to the gradient of the policy with respect to the policy-parameters [paper]. The name is typically written using capital letters only, as it\u2019s originally an acronym for the original algorithms group design: \u201cREward Increment = Nonnegative Factor x Offset Reinforcement x Characteristic Eligibility\u201d [source]", "Reinforcement Learning (RL): Reinforcement Learning is, like Supervised Learning and Unsupervised Learning, one the main areas of Machine Learning and Artificial Intelligence. It is concerned with the learning process of an arbitrary being, formally known as an Agent, in the world surrounding it, known as the Environment. The Agent seeks to maximize the rewards it receives from the Environment, and performs different actions in order to learn how the Environment responds and gain more rewards. One of the greatest challenges of RL tasks is to associate actions with postponed rewards \u2014 which are rewards received by the Agent long after the reward-generating action was made. It is therefore heavily used to solve different kind of games, from Tic-Tac-Toe, Chess, Atari 2600 and all the way to Go and StarCraft.", "Reward: A numerical value received by the Agent from the Environment as a direct response to the Agent\u2019s actions. The Agent\u2019s goal is to maximize the overall reward it receives during an episode, and so rewards are the motivation the Agent needs in order to act in a desired behavior. All actions yield rewards, which can be roughly divided to three types: positive rewards which emphasize a desired action, negative rewards which emphasize an action the Agent should stray away from, and zero, which means the Agent didn\u2019t do anything special or unique.", "Sarsa: The Sarsa algorithm is pretty much the Q-Learning algorithm with a slight modification in order to make it an on-policy algorithm. The Q-Learning update rule is based on the Bellman equation for the optimal Q-Value, and so in the case on no uncertainties in state-transitions and expected rewards, the Q-Learning update rule is:", "In order to transform this into an on-policy algorithm, the last term is modified:", "when here, both actions a and a\u2019 are chosen by the same policy. The name of the algorithm is derived from its update rule, which is based on (s,a,r,s\u2019,a\u2019), all coming from the same policy.", "State: Every scenario the Agent encounters in the Environment is formally called a state. The Agent transitions between different states by performing actions. It is also worth mentioning the terminal states, which mark the end of an episode. There are no possible states after a terminal state has been reached, and a new episode begins. Quite often, a terminal state is represented as a special state where all actions transition to the same terminal state with reward 0.", "Temporal-Difference (TD): Temporal Difference is a learning method which combines both Dynamic Programming and Monte Carlo principles; it learns \u201con the fly\u201d similarly to Monte Carlo, yet updates its estimates like Dynamic Programming. One of the simplest Temporal Difference algorithms it known as one-step TD or TD(0). It updates the Value Function according to the following update rule:", "where V is the Value Function, s is the state, r is the reward, \u03b3 is the discount factor, \u03b1 is a learning rate, t is the time-step and the \u2018=\u2019 sign is used as an update operator and not equality. The term found in the squared brackets is known as the temporal difference error.", "Upper Confident Bound (UCB): UCB is an exploration method which tries to ensure that each action is well explored. Consider an exploration policy which is completely random \u2014 meaning, each possible action has the same chance of being selected. There is a chance that some actions will be explored much more than others. The less an action is selected, the less confident the Agent can be about its expected reward, and the its exploitation phase might be harmed. Exploration by UCB takes into account the number of times each action was selected, and gives extra weight to those less-explored. Formalizing this mathematically, the selected action is picked by:", "where R(a) is the expected overall reward of action a, t is the number of steps taken (how many actions were selected overall), N(a) is the number of times action a was selected and c is a configureable hyperparameter. This method is also referred to sometimes as \u201cexploration through optimism\u201d, as it gives less-explored actions a higher value, encouraging the model to select them.", "Value Function: Usually denoted as V(s) (sometimes with a \u03c0 subscript), the Value function is a measure of the overall expected reward assuming the Agent is in state s and then continues playing until the end of the episode following some policy \u03c0. It is defined mathematically as:", "While it does seem similar to the definition of Q Value, there is an implicit \u2014 yet important \u2014 difference: for n=0, the reward r\u2070 of V(s) is the expected reward from just being in state s, before any action was played, while in Q Value, r\u2070 is the expected reward after a certain action was played. This difference also yields the Advantage function.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Head of Recommendations at Lightricks. Lives in Tel-Aviv, Israel. See me on shakedzy.xyz"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fe16230b7d24e&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-complete-reinforcement-learning-dictionary-e16230b7d24e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-complete-reinforcement-learning-dictionary-e16230b7d24e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-complete-reinforcement-learning-dictionary-e16230b7d24e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-complete-reinforcement-learning-dictionary-e16230b7d24e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----e16230b7d24e--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e16230b7d24e--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://shakedzy.medium.com/?source=post_page-----e16230b7d24e--------------------------------", "anchor_text": ""}, {"url": "https://shakedzy.medium.com/?source=post_page-----e16230b7d24e--------------------------------", "anchor_text": "Shaked Zychlinski"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F43218078e688&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-complete-reinforcement-learning-dictionary-e16230b7d24e&user=Shaked+Zychlinski&userId=43218078e688&source=post_page-43218078e688----e16230b7d24e---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe16230b7d24e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-complete-reinforcement-learning-dictionary-e16230b7d24e&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe16230b7d24e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-complete-reinforcement-learning-dictionary-e16230b7d24e&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e?sk=9d29f303db3650387c489c9565b43636", "anchor_text": "Friends Link"}, {"url": "https://medium.com/@shakedzy/qrash-course-deep-q-networks-from-the-ground-up-1bbda41d3677", "anchor_text": "Introduction to RL and Q-Learning"}, {"url": "https://medium.com/@shakedzy/qrash-course-ii-from-q-learning-to-gradient-policy-actor-critic-in-12-minutes-8e8b47129c8c", "anchor_text": "Policy Gradients and Actor-Critics"}, {"url": "https://medium.com/freecodecamp/an-introduction-to-reinforcement-learning-4339519de419", "anchor_text": "this series"}, {"url": "https://medium.com/@thomassimonini", "anchor_text": "Thomas Simonini"}, {"url": "https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0", "anchor_text": "this series"}, {"url": "https://medium.com/@awjuliani", "anchor_text": "Arthur Juliani"}, {"url": "https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf", "anchor_text": "PDF file"}, {"url": "https://hackernoon.com/intuitive-rl-intro-to-advantage-actor-critic-a2c-4ff545978752", "anchor_text": "here"}, {"url": "https://en.wikipedia.org/wiki/Slot_machine", "anchor_text": "slot-machines"}, {"url": "https://en.wikipedia.org/wiki/Circular_buffer", "anchor_text": "cyclic memory buffers"}, {"url": "https://en.wikipedia.org/wiki/Reservoir_sampling", "anchor_text": "reservoir-sampling"}, {"url": "https://en.wikipedia.org/wiki/Blackjack", "anchor_text": "Blackjack"}, {"url": "https://medium.com/@shakedzy/qrash-course-deep-q-networks-from-the-ground-up-1bbda41d3677", "anchor_text": "this blogpost"}, {"url": "https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf", "anchor_text": "paper"}, {"url": "https://link.springer.com/content/pdf/10.1007%2FBF00992696.pdf", "anchor_text": "source"}, {"url": "https://en.wikipedia.org/wiki/Dynamic_programming", "anchor_text": "Dynamic Programming"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----e16230b7d24e---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----e16230b7d24e---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----e16230b7d24e---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/data-science?source=post_page-----e16230b7d24e---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/algorithms?source=post_page-----e16230b7d24e---------------algorithms-----------------", "anchor_text": "Algorithms"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe16230b7d24e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-complete-reinforcement-learning-dictionary-e16230b7d24e&user=Shaked+Zychlinski&userId=43218078e688&source=-----e16230b7d24e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe16230b7d24e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-complete-reinforcement-learning-dictionary-e16230b7d24e&user=Shaked+Zychlinski&userId=43218078e688&source=-----e16230b7d24e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe16230b7d24e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-complete-reinforcement-learning-dictionary-e16230b7d24e&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e16230b7d24e--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fe16230b7d24e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-complete-reinforcement-learning-dictionary-e16230b7d24e&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----e16230b7d24e---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----e16230b7d24e--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----e16230b7d24e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----e16230b7d24e--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----e16230b7d24e--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----e16230b7d24e--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----e16230b7d24e--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----e16230b7d24e--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----e16230b7d24e--------------------------------", "anchor_text": ""}, {"url": "https://shakedzy.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://shakedzy.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Shaked Zychlinski"}, {"url": "https://shakedzy.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.3K Followers"}, {"url": "http://shakedzy.xyz", "anchor_text": "shakedzy.xyz"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F43218078e688&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-complete-reinforcement-learning-dictionary-e16230b7d24e&user=Shaked+Zychlinski&userId=43218078e688&source=post_page-43218078e688--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F4123ceb9438d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-complete-reinforcement-learning-dictionary-e16230b7d24e&newsletterV3=43218078e688&newsletterV3Id=4123ceb9438d&user=Shaked+Zychlinski&userId=43218078e688&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}