{"url": "https://towardsdatascience.com/automating-pac-man-with-deep-q-learning-an-implementation-in-tensorflow-ca08e9891d9c", "time": 1683002323.972029, "path": "towardsdatascience.com/automating-pac-man-with-deep-q-learning-an-implementation-in-tensorflow-ca08e9891d9c/", "webpage": {"metadata": {"title": "Automating Pac-man with Deep Q-learning: An Implementation in Tensorflow. | by Adrian Yijie Xu | Towards Data Science", "h1": "Automating Pac-man with Deep Q-learning: An Implementation in Tensorflow.", "description": "Over the course of our articles covering the fundamentals of reinforcement learning at GradientCrescent, we\u2019ve studied both model-based and sample-based approaches to reinforcement learning. Briefly\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-markov-decision-processes-policies-value-functions-94f7389e1e82", "anchor_text": "model-based", "paragraph_index": 0}, {"url": "https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-markov-decision-processes-policies-value-functions-94f7389e1e82", "anchor_text": "Markovian Decision Processes", "paragraph_index": 0}, {"url": "https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-automating-pong-in-using-a-policy-model-an-implementation-b71f64c158ff", "anchor_text": "sample-based learning methods", "paragraph_index": 0}, {"url": "https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-understanding-blackjack-strategy-through-monte-carlo-88c9b85194ed", "anchor_text": "Monte Carlo", "paragraph_index": 0}, {"url": "https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-illustrating-online-learning-through-temporal-differences-ec4833b6b06a", "anchor_text": "Temporal Differenc", "paragraph_index": 0}, {"url": "https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-markov-decision-processes-policies-value-functions-94f7389e1e82", "anchor_text": "Markov Decision Processes", "paragraph_index": 3}, {"url": "https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-markov-decision-processes-policies-value-functions-94f7389e1e82", "anchor_text": "Bellman Optimality Equations.", "paragraph_index": 4}, {"url": "https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-automating-pong-in-using-a-policy-model-an-implementation-b71f64c158ff", "anchor_text": "Pong before with a MDP-based model", "paragraph_index": 6}, {"url": "https://github.com/EXJUSTICE/GradientCrescent", "anchor_text": "GradientCrescent Github.", "paragraph_index": 7}, {"url": "https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-automating-pong-in-using-a-policy-model-an-implementation-b71f64c158ff", "anchor_text": "Pong automation implementation", "paragraph_index": 10}, {"url": "https://medium.com/gradientcrescent", "anchor_text": "GradientCrescent", "paragraph_index": 28}], "all_paragraphs": ["Over the course of our articles covering the fundamentals of reinforcement learning at GradientCrescent, we\u2019ve studied both model-based and sample-based approaches to reinforcement learning. Briefly, the former class is characterized by requiring knowledge of the complete probability distributions of all possible state transitions, and can be exemplified by Markovian Decision Processes. In contrast, sample-based learning methods allow for the determination of state values simply through repeated observations, without the need for transition dynamics. Within this domain, we\u2019ve covered both Monte Carlo and Temporal Difference learning. Briefly, the two can be separated by the frequency of state-value updates: while a Monte Carlo approach requires that an episode be finished for a round of updates to take place, Temporal Difference approaches update intra-episode incrementally, using old estimations of state-values together with discounted rewards to generate new updates.", "The rapid reactivity of TD or \u201conline\u201d learning approaches makes them suitable for highly dynamic environments, as the values of states and actions is continuously updated throughout time through sets of estimates. Perhaps most notably, TD is the foundation of Q-learning, a more advanced algorithm used to train agents tackling game environments such as those observed in the OpenAI Atari gyms, and the focus of this article.", "Recall that in Temporal Difference learning, we observed that an agent behaves cyclically in an environment, through sequence of States (S), Actions (A), and (Rewards).", "Due to this cyclic behavior, we can update the value of the previous state as soon as we reach the next state. However, we can expand the scope of our training to include state-action values, just as we did with Markov Decision Processes prior. This is generally known as SARSA. Let\u2019s compare the state-action and state-value TD update equations:", "Q-learning takes this a step further by forcing a selection of the action with the highest action value during an update, in a similar way to what\u2019s observed with Bellman Optimality Equations. We can inspect SARSA and Q-learning next to the Bellman and Bellman Optimality Equations, below:", "You may be wondering about how ensure complete exploration of our state-action space, given the need to constantly select actions for a state with the highest existing action-value. In theory, we could be avoiding the optimal action simply by failing to evaluate it in the first place. To encourage exploration, we can use a decaying e-greedy policy, essentially forcing the agent to select an apparent sub-optimal action in order to learn more about its value, at a certain percentage of the time. By introducing a decaying process, we can limit exploration once all of the states have been evaluated, after which we\u2019ll permanently select the optimal actions for each state.", "As we\u2019ve tackled Pong before with a MDP-based model, let\u2019s take what we\u2019ve learned about Q-learning and apply it to a game of Atari\u2019s Ms. Pac-man.", "Our Google Colaboratory implementation is written in Python utilizing Tensorflow Core, and can be found on the GradientCrescent Github. It\u2019s based on that by Ravichandiran et. al, but upgraded to be compatible with Tensorflow 2.0, and significantly expanded to facilitate improved visualization and explanations. As the implementation for this approach is quite convoluted, let\u2019s summarize the order of actions required:", "Let\u2019s get started. With Tensorflow 2 on the horizon for Colaboratory environments, we\u2019ve converted our code to be TF2 compliant, using the new compat package. Note that this code is not TF2 native.", "Let\u2019s by importing all of the necessary packages, including the OpenAI gym environments and Tensorflow core.", "Next, we define a preprocessing function to crop the images from our gym environment and convert them into one-dimensional tensors. We\u2019ve seen this before in our Pong automation implementation.", "Next, let\u2019s initialize the gym environment, and inspect a few screens of gameplay, and also understand the 9 actions available within the gamespace. Naturally, this information is not available to our agent.", "We can take this chance to compare our original and preprocessed input images:", "Next, let\u2019s define our model, a deep Q-network. This is essentially a three layer convolutional network that takes preprocessed input images, flattens and feeds them to a fully-connected layer, and outputs the probabilities of taking each action in the game space. As previously mentioned, there\u2019s no activation layer here, as the presence of one would result in a binary output distribution.", "Let\u2019s also take this chance to define our hyperparameters for our model and training process", "Recall, that Q-learning requires us to select actions with the highest action values. To ensure that we still visit every single possible state-action combination, we\u2019ll have our agent follow an epsilon-greedy policy, with an exploration rate of 5%. We\u2019ll set a this exploration rate to decay with time, as we eventually assume all combinations have already been explored \u2014 any exploration after that point would simply result in the forced selection of sub-optimal actions.", "Recall from the equations above, that the update function for Q-learning requires the following:", "To supply these parameters in meaningful quantities, we need to evaluate our current policy following a set of parameters and store all of the variables in a buffer, from which we\u2019ll draw data in minibatches during training.. This is unlike in our previous implementation in Pong, where we used an incremental approach. Let\u2019s go ahead and create our buffer and a simple sampling function:", "Next, let\u2019s copy the weight parameters of our original network into a target network. This dual-network approach allows us to generate data during the training process using an existing policy while still optimizing our parameters for the next policy iteration.", "Finally, we\u2019ll also define our loss. This is simply the squared difference of our target action (with the highest action value) and our predicted action. We\u2019ll use an ADAM optimizer to minimize our loss during training.", "With all of our code defined, let\u2019s run our network and go over the training process. We\u2019ve defined most of this in the initial summary, but let\u2019s recall for posterity.", "Once training is complete, we can plot the reward distribution against incremental episodes. The first 550 episodes (roughly 2 hours) looks something like this:", "After an additional 800 episodes, this converges into the following:", "To evaluate our results within the confinement of the Colaboratory environment, we can record an entire episode and display it within a virtual display using a wrapped based on the IPython library:", "We then run a new session of our environment using our model, and record it.", "You should observe something a few rounds of the game! Here\u2019s a couple of episodes we recorded.", "Not bad for a model trained in a few hours, scoring well above 400. In particular, it seems our agent performs quite well when directly chased by a ghost, but is still poor at anticipating incoming ones, probably as it hasn\u2019t had enough experience observing their movements yet.", "That wraps up this introduction to Q-learning. In our next article, we\u2019ll move on from the world of Atari to tackling one of the most well known FPS games in the world. Stay tuned!", "We hope you enjoyed this article, and hope you check out the many other articles on GradientCrescent, covering applied and theoretical aspects of AI. To stay up to date with the latest updates on GradientCrescent, please consider following the publication and following our Github repository.", "White et. al, Fundamentals of Reinforcement Learning, University of Alberta", "Silva et. al, Reinforcement Learning, UCL", "Ravichandiran et. al, Hands-On Reinforcement Learning with Python"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fca08e9891d9c&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomating-pac-man-with-deep-q-learning-an-implementation-in-tensorflow-ca08e9891d9c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomating-pac-man-with-deep-q-learning-an-implementation-in-tensorflow-ca08e9891d9c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomating-pac-man-with-deep-q-learning-an-implementation-in-tensorflow-ca08e9891d9c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomating-pac-man-with-deep-q-learning-an-implementation-in-tensorflow-ca08e9891d9c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/@adrianitsaxu?source=post_page-----ca08e9891d9c--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ca08e9891d9c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@adrianitsaxu?source=post_page-----ca08e9891d9c--------------------------------", "anchor_text": "Adrian Yijie Xu"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc834a59b6354&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomating-pac-man-with-deep-q-learning-an-implementation-in-tensorflow-ca08e9891d9c&user=Adrian+Yijie+Xu&userId=c834a59b6354&source=post_page-c834a59b6354----ca08e9891d9c---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----ca08e9891d9c--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fca08e9891d9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomating-pac-man-with-deep-q-learning-an-implementation-in-tensorflow-ca08e9891d9c&user=Adrian+Yijie+Xu&userId=c834a59b6354&source=-----ca08e9891d9c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fca08e9891d9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomating-pac-man-with-deep-q-learning-an-implementation-in-tensorflow-ca08e9891d9c&source=-----ca08e9891d9c---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-markov-decision-processes-policies-value-functions-94f7389e1e82", "anchor_text": "model-based"}, {"url": "https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-markov-decision-processes-policies-value-functions-94f7389e1e82", "anchor_text": "Markovian Decision Processes"}, {"url": "https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-automating-pong-in-using-a-policy-model-an-implementation-b71f64c158ff", "anchor_text": "sample-based learning methods"}, {"url": "https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-understanding-blackjack-strategy-through-monte-carlo-88c9b85194ed", "anchor_text": "Monte Carlo"}, {"url": "https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-illustrating-online-learning-through-temporal-differences-ec4833b6b06a", "anchor_text": "Temporal Differenc"}, {"url": "https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-markov-decision-processes-policies-value-functions-94f7389e1e82", "anchor_text": "Markov Decision Processes"}, {"url": "https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-markov-decision-processes-policies-value-functions-94f7389e1e82", "anchor_text": "Bellman Optimality Equations."}, {"url": "https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-automating-pong-in-using-a-policy-model-an-implementation-b71f64c158ff", "anchor_text": "Pong before with a MDP-based model"}, {"url": "https://github.com/EXJUSTICE/GradientCrescent", "anchor_text": "GradientCrescent Github."}, {"url": "https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-automating-pong-in-using-a-policy-model-an-implementation-b71f64c158ff", "anchor_text": "Pong automation implementation"}, {"url": "https://medium.com/gradientcrescent", "anchor_text": "GradientCrescent"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----ca08e9891d9c---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/atari?source=post_page-----ca08e9891d9c---------------atari-----------------", "anchor_text": "Atari"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----ca08e9891d9c---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/q-learning?source=post_page-----ca08e9891d9c---------------q_learning-----------------", "anchor_text": "Q Learning"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----ca08e9891d9c---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fca08e9891d9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomating-pac-man-with-deep-q-learning-an-implementation-in-tensorflow-ca08e9891d9c&user=Adrian+Yijie+Xu&userId=c834a59b6354&source=-----ca08e9891d9c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fca08e9891d9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomating-pac-man-with-deep-q-learning-an-implementation-in-tensorflow-ca08e9891d9c&user=Adrian+Yijie+Xu&userId=c834a59b6354&source=-----ca08e9891d9c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fca08e9891d9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomating-pac-man-with-deep-q-learning-an-implementation-in-tensorflow-ca08e9891d9c&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/@adrianitsaxu?source=post_page-----ca08e9891d9c--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ca08e9891d9c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc834a59b6354&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomating-pac-man-with-deep-q-learning-an-implementation-in-tensorflow-ca08e9891d9c&user=Adrian+Yijie+Xu&userId=c834a59b6354&source=post_page-c834a59b6354----ca08e9891d9c---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F362de3a1de04&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomating-pac-man-with-deep-q-learning-an-implementation-in-tensorflow-ca08e9891d9c&newsletterV3=c834a59b6354&newsletterV3Id=362de3a1de04&user=Adrian+Yijie+Xu&userId=c834a59b6354&source=-----ca08e9891d9c---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/@adrianitsaxu?source=post_page-----ca08e9891d9c--------------------------------", "anchor_text": "Written by Adrian Yijie Xu"}, {"url": "https://medium.com/@adrianitsaxu/followers?source=post_page-----ca08e9891d9c--------------------------------", "anchor_text": "604 Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----ca08e9891d9c--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://github.com/EXJUSTICE/", "anchor_text": "https://github.com/EXJUSTICE/"}, {"url": "https://www.linkedin.com/in/yijie-xu-0174a325/", "anchor_text": "https://www.linkedin.com/in/yijie-xu-0174a325/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc834a59b6354&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomating-pac-man-with-deep-q-learning-an-implementation-in-tensorflow-ca08e9891d9c&user=Adrian+Yijie+Xu&userId=c834a59b6354&source=post_page-c834a59b6354----ca08e9891d9c---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F362de3a1de04&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomating-pac-man-with-deep-q-learning-an-implementation-in-tensorflow-ca08e9891d9c&newsletterV3=c834a59b6354&newsletterV3Id=362de3a1de04&user=Adrian+Yijie+Xu&userId=c834a59b6354&source=-----ca08e9891d9c---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/gradientcrescent/generating-swipeable-tinder-profiles-using-ai-adversarial-recurrent-neural-networks-in-dd68bd98c2f3?source=author_recirc-----ca08e9891d9c----0---------------------3cb4a1c1_4174_41b5_b12c_3e191b5e2d68-------", "anchor_text": ""}, {"url": "https://medium.com/@adrianitsaxu?source=author_recirc-----ca08e9891d9c----0---------------------3cb4a1c1_4174_41b5_b12c_3e191b5e2d68-------", "anchor_text": ""}, {"url": "https://medium.com/@adrianitsaxu?source=author_recirc-----ca08e9891d9c----0---------------------3cb4a1c1_4174_41b5_b12c_3e191b5e2d68-------", "anchor_text": "Adrian Yijie Xu"}, {"url": "https://medium.com/gradientcrescent?source=author_recirc-----ca08e9891d9c----0---------------------3cb4a1c1_4174_41b5_b12c_3e191b5e2d68-------", "anchor_text": "GradientCrescent"}, {"url": "https://medium.com/gradientcrescent/generating-swipeable-tinder-profiles-using-ai-adversarial-recurrent-neural-networks-in-dd68bd98c2f3?source=author_recirc-----ca08e9891d9c----0---------------------3cb4a1c1_4174_41b5_b12c_3e191b5e2d68-------", "anchor_text": "Generating Swipeable Tinder Profiles using AI: Adversarial & Recurrent Neural Networks in\u2026This is a edited article based on the original publication, which was removed due to the privacy risks created through the use of the the\u2026"}, {"url": "https://medium.com/gradientcrescent/generating-swipeable-tinder-profiles-using-ai-adversarial-recurrent-neural-networks-in-dd68bd98c2f3?source=author_recirc-----ca08e9891d9c----0---------------------3cb4a1c1_4174_41b5_b12c_3e191b5e2d68-------", "anchor_text": "10 min read\u00b7May 15, 2019"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fgradientcrescent%2Fdd68bd98c2f3&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fgradientcrescent%2Fgenerating-swipeable-tinder-profiles-using-ai-adversarial-recurrent-neural-networks-in-dd68bd98c2f3&user=Adrian+Yijie+Xu&userId=c834a59b6354&source=-----dd68bd98c2f3----0-----------------clap_footer----3cb4a1c1_4174_41b5_b12c_3e191b5e2d68-------", "anchor_text": ""}, {"url": "https://medium.com/gradientcrescent/generating-swipeable-tinder-profiles-using-ai-adversarial-recurrent-neural-networks-in-dd68bd98c2f3?source=author_recirc-----ca08e9891d9c----0---------------------3cb4a1c1_4174_41b5_b12c_3e191b5e2d68-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdd68bd98c2f3&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fgradientcrescent%2Fgenerating-swipeable-tinder-profiles-using-ai-adversarial-recurrent-neural-networks-in-dd68bd98c2f3&source=-----ca08e9891d9c----0-----------------bookmark_preview----3cb4a1c1_4174_41b5_b12c_3e191b5e2d68-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----ca08e9891d9c----1---------------------3cb4a1c1_4174_41b5_b12c_3e191b5e2d68-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----ca08e9891d9c----1---------------------3cb4a1c1_4174_41b5_b12c_3e191b5e2d68-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----ca08e9891d9c----1---------------------3cb4a1c1_4174_41b5_b12c_3e191b5e2d68-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----ca08e9891d9c----1---------------------3cb4a1c1_4174_41b5_b12c_3e191b5e2d68-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----ca08e9891d9c----1---------------------3cb4a1c1_4174_41b5_b12c_3e191b5e2d68-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----ca08e9891d9c----1---------------------3cb4a1c1_4174_41b5_b12c_3e191b5e2d68-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----3cb4a1c1_4174_41b5_b12c_3e191b5e2d68-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----ca08e9891d9c----1---------------------3cb4a1c1_4174_41b5_b12c_3e191b5e2d68-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----ca08e9891d9c----1-----------------bookmark_preview----3cb4a1c1_4174_41b5_b12c_3e191b5e2d68-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----ca08e9891d9c----2---------------------3cb4a1c1_4174_41b5_b12c_3e191b5e2d68-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----ca08e9891d9c----2---------------------3cb4a1c1_4174_41b5_b12c_3e191b5e2d68-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----ca08e9891d9c----2---------------------3cb4a1c1_4174_41b5_b12c_3e191b5e2d68-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----ca08e9891d9c----2---------------------3cb4a1c1_4174_41b5_b12c_3e191b5e2d68-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----ca08e9891d9c----2---------------------3cb4a1c1_4174_41b5_b12c_3e191b5e2d68-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----ca08e9891d9c----2---------------------3cb4a1c1_4174_41b5_b12c_3e191b5e2d68-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----2-----------------clap_footer----3cb4a1c1_4174_41b5_b12c_3e191b5e2d68-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----ca08e9891d9c----2---------------------3cb4a1c1_4174_41b5_b12c_3e191b5e2d68-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----ca08e9891d9c----2-----------------bookmark_preview----3cb4a1c1_4174_41b5_b12c_3e191b5e2d68-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/optimizing-blackjack-strategy-through-monte-carlo-methods-cbb606e52d1b?source=author_recirc-----ca08e9891d9c----3---------------------3cb4a1c1_4174_41b5_b12c_3e191b5e2d68-------", "anchor_text": ""}, {"url": "https://medium.com/@adrianitsaxu?source=author_recirc-----ca08e9891d9c----3---------------------3cb4a1c1_4174_41b5_b12c_3e191b5e2d68-------", "anchor_text": ""}, {"url": "https://medium.com/@adrianitsaxu?source=author_recirc-----ca08e9891d9c----3---------------------3cb4a1c1_4174_41b5_b12c_3e191b5e2d68-------", "anchor_text": "Adrian Yijie Xu"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----ca08e9891d9c----3---------------------3cb4a1c1_4174_41b5_b12c_3e191b5e2d68-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/optimizing-blackjack-strategy-through-monte-carlo-methods-cbb606e52d1b?source=author_recirc-----ca08e9891d9c----3---------------------3cb4a1c1_4174_41b5_b12c_3e191b5e2d68-------", "anchor_text": "Optimizing Blackjack Strategy through Monte Carlo MethodsFundamentals of Reinforcement Learning"}, {"url": "https://towardsdatascience.com/optimizing-blackjack-strategy-through-monte-carlo-methods-cbb606e52d1b?source=author_recirc-----ca08e9891d9c----3---------------------3cb4a1c1_4174_41b5_b12c_3e191b5e2d68-------", "anchor_text": "\u00b711 min read\u00b7Nov 19, 2019"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcbb606e52d1b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimizing-blackjack-strategy-through-monte-carlo-methods-cbb606e52d1b&user=Adrian+Yijie+Xu&userId=c834a59b6354&source=-----cbb606e52d1b----3-----------------clap_footer----3cb4a1c1_4174_41b5_b12c_3e191b5e2d68-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/optimizing-blackjack-strategy-through-monte-carlo-methods-cbb606e52d1b?source=author_recirc-----ca08e9891d9c----3---------------------3cb4a1c1_4174_41b5_b12c_3e191b5e2d68-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcbb606e52d1b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimizing-blackjack-strategy-through-monte-carlo-methods-cbb606e52d1b&source=-----ca08e9891d9c----3-----------------bookmark_preview----3cb4a1c1_4174_41b5_b12c_3e191b5e2d68-------", "anchor_text": ""}, {"url": "https://medium.com/@adrianitsaxu?source=post_page-----ca08e9891d9c--------------------------------", "anchor_text": "See all from Adrian Yijie Xu"}, {"url": "https://towardsdatascience.com/?source=post_page-----ca08e9891d9c--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://medium.com/@anandmishra.kanha/deep-reinforcement-learning-current-state-of-art-383190b14464?source=read_next_recirc-----ca08e9891d9c----0---------------------bd489508_341d_4274_8616_394bd2cdf774-------", "anchor_text": ""}, {"url": "https://medium.com/@anandmishra.kanha?source=read_next_recirc-----ca08e9891d9c----0---------------------bd489508_341d_4274_8616_394bd2cdf774-------", "anchor_text": ""}, {"url": "https://medium.com/@anandmishra.kanha?source=read_next_recirc-----ca08e9891d9c----0---------------------bd489508_341d_4274_8616_394bd2cdf774-------", "anchor_text": "Anand Mishra"}, {"url": "https://medium.com/@anandmishra.kanha/deep-reinforcement-learning-current-state-of-art-383190b14464?source=read_next_recirc-----ca08e9891d9c----0---------------------bd489508_341d_4274_8616_394bd2cdf774-------", "anchor_text": "Deep reinforcement learning \u2014 current state of artCurrent"}, {"url": "https://medium.com/@anandmishra.kanha/deep-reinforcement-learning-current-state-of-art-383190b14464?source=read_next_recirc-----ca08e9891d9c----0---------------------bd489508_341d_4274_8616_394bd2cdf774-------", "anchor_text": "5 min read\u00b7Dec 15, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F383190b14464&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40anandmishra.kanha%2Fdeep-reinforcement-learning-current-state-of-art-383190b14464&user=Anand+Mishra&userId=86f86a9a5573&source=-----383190b14464----0-----------------clap_footer----bd489508_341d_4274_8616_394bd2cdf774-------", "anchor_text": ""}, {"url": "https://medium.com/@anandmishra.kanha/deep-reinforcement-learning-current-state-of-art-383190b14464?source=read_next_recirc-----ca08e9891d9c----0---------------------bd489508_341d_4274_8616_394bd2cdf774-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F383190b14464&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40anandmishra.kanha%2Fdeep-reinforcement-learning-current-state-of-art-383190b14464&source=-----ca08e9891d9c----0-----------------bookmark_preview----bd489508_341d_4274_8616_394bd2cdf774-------", "anchor_text": ""}, {"url": "https://piotrkrosniak.medium.com/vaccine-supply-chain-optimization-with-ai-powered-capacitated-vehicle-routing-problem-cvrp-part-1-ca79519e9ad7?source=read_next_recirc-----ca08e9891d9c----1---------------------bd489508_341d_4274_8616_394bd2cdf774-------", "anchor_text": ""}, {"url": "https://piotrkrosniak.medium.com/?source=read_next_recirc-----ca08e9891d9c----1---------------------bd489508_341d_4274_8616_394bd2cdf774-------", "anchor_text": ""}, {"url": "https://piotrkrosniak.medium.com/?source=read_next_recirc-----ca08e9891d9c----1---------------------bd489508_341d_4274_8616_394bd2cdf774-------", "anchor_text": "Piotr Krosniak"}, {"url": "https://piotrkrosniak.medium.com/vaccine-supply-chain-optimization-with-ai-powered-capacitated-vehicle-routing-problem-cvrp-part-1-ca79519e9ad7?source=read_next_recirc-----ca08e9891d9c----1---------------------bd489508_341d_4274_8616_394bd2cdf774-------", "anchor_text": "Vaccine Supply Chain Optimization with AI-Powered Capacitated Vehicle Routing Problem(CVRP)- Part 1The world is facing a global health crisis, and one of the most important challenges is to ensure an efficient and timely distribution of\u2026"}, {"url": "https://piotrkrosniak.medium.com/vaccine-supply-chain-optimization-with-ai-powered-capacitated-vehicle-routing-problem-cvrp-part-1-ca79519e9ad7?source=read_next_recirc-----ca08e9891d9c----1---------------------bd489508_341d_4274_8616_394bd2cdf774-------", "anchor_text": "6 min read\u00b7Jan 14"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2Fca79519e9ad7&operation=register&redirect=https%3A%2F%2Fpiotrkrosniak.medium.com%2Fvaccine-supply-chain-optimization-with-ai-powered-capacitated-vehicle-routing-problem-cvrp-part-1-ca79519e9ad7&user=Piotr+Krosniak&userId=b791abcfafd5&source=-----ca79519e9ad7----1-----------------clap_footer----bd489508_341d_4274_8616_394bd2cdf774-------", "anchor_text": ""}, {"url": "https://piotrkrosniak.medium.com/vaccine-supply-chain-optimization-with-ai-powered-capacitated-vehicle-routing-problem-cvrp-part-1-ca79519e9ad7?source=read_next_recirc-----ca08e9891d9c----1---------------------bd489508_341d_4274_8616_394bd2cdf774-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fca79519e9ad7&operation=register&redirect=https%3A%2F%2Fpiotrkrosniak.medium.com%2Fvaccine-supply-chain-optimization-with-ai-powered-capacitated-vehicle-routing-problem-cvrp-part-1-ca79519e9ad7&source=-----ca08e9891d9c----1-----------------bookmark_preview----bd489508_341d_4274_8616_394bd2cdf774-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----ca08e9891d9c----0---------------------bd489508_341d_4274_8616_394bd2cdf774-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData?source=read_next_recirc-----ca08e9891d9c----0---------------------bd489508_341d_4274_8616_394bd2cdf774-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData?source=read_next_recirc-----ca08e9891d9c----0---------------------bd489508_341d_4274_8616_394bd2cdf774-------", "anchor_text": "Andrew Austin"}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----ca08e9891d9c----0---------------------bd489508_341d_4274_8616_394bd2cdf774-------", "anchor_text": "AI Anyone Can Understand Part 1: Reinforcement LearningReinforcement learning is a way for machines to learn by trying different things and seeing what works best. For example, a robot could\u2026"}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----ca08e9891d9c----0---------------------bd489508_341d_4274_8616_394bd2cdf774-------", "anchor_text": "\u00b74 min read\u00b7Dec 11, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F6c3b3d623a2d&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40MoneyAndData%2Fai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d&user=Andrew+Austin&userId=42d388912d13&source=-----6c3b3d623a2d----0-----------------clap_footer----bd489508_341d_4274_8616_394bd2cdf774-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----ca08e9891d9c----0---------------------bd489508_341d_4274_8616_394bd2cdf774-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6c3b3d623a2d&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40MoneyAndData%2Fai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d&source=-----ca08e9891d9c----0-----------------bookmark_preview----bd489508_341d_4274_8616_394bd2cdf774-------", "anchor_text": ""}, {"url": "https://medium.com/@sthanikamsanthosh1994/custom-gym-environment-stock-trading-for-reinforcement-learning-stable-baseline3-629a489d462d?source=read_next_recirc-----ca08e9891d9c----1---------------------bd489508_341d_4274_8616_394bd2cdf774-------", "anchor_text": ""}, {"url": "https://medium.com/@sthanikamsanthosh1994?source=read_next_recirc-----ca08e9891d9c----1---------------------bd489508_341d_4274_8616_394bd2cdf774-------", "anchor_text": ""}, {"url": "https://medium.com/@sthanikamsanthosh1994?source=read_next_recirc-----ca08e9891d9c----1---------------------bd489508_341d_4274_8616_394bd2cdf774-------", "anchor_text": "Sthanikam Santhosh"}, {"url": "https://medium.com/@sthanikamsanthosh1994/custom-gym-environment-stock-trading-for-reinforcement-learning-stable-baseline3-629a489d462d?source=read_next_recirc-----ca08e9891d9c----1---------------------bd489508_341d_4274_8616_394bd2cdf774-------", "anchor_text": "Custom Gym environment(Stock trading) for Reinforcement Learning(Stable baseline3)For designing any Reinforcement Learning(RL) the environment plays an important role. The success of any reinforcement learning model\u2026"}, {"url": "https://medium.com/@sthanikamsanthosh1994/custom-gym-environment-stock-trading-for-reinforcement-learning-stable-baseline3-629a489d462d?source=read_next_recirc-----ca08e9891d9c----1---------------------bd489508_341d_4274_8616_394bd2cdf774-------", "anchor_text": "8 min read\u00b7Dec 22, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F629a489d462d&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40sthanikamsanthosh1994%2Fcustom-gym-environment-stock-trading-for-reinforcement-learning-stable-baseline3-629a489d462d&user=Sthanikam+Santhosh&userId=1f162875aadc&source=-----629a489d462d----1-----------------clap_footer----bd489508_341d_4274_8616_394bd2cdf774-------", "anchor_text": ""}, {"url": "https://medium.com/@sthanikamsanthosh1994/custom-gym-environment-stock-trading-for-reinforcement-learning-stable-baseline3-629a489d462d?source=read_next_recirc-----ca08e9891d9c----1---------------------bd489508_341d_4274_8616_394bd2cdf774-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F629a489d462d&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40sthanikamsanthosh1994%2Fcustom-gym-environment-stock-trading-for-reinforcement-learning-stable-baseline3-629a489d462d&source=-----ca08e9891d9c----1-----------------bookmark_preview----bd489508_341d_4274_8616_394bd2cdf774-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----ca08e9891d9c----2---------------------bd489508_341d_4274_8616_394bd2cdf774-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----ca08e9891d9c----2---------------------bd489508_341d_4274_8616_394bd2cdf774-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----ca08e9891d9c----2---------------------bd489508_341d_4274_8616_394bd2cdf774-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----ca08e9891d9c----2---------------------bd489508_341d_4274_8616_394bd2cdf774-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----ca08e9891d9c----2---------------------bd489508_341d_4274_8616_394bd2cdf774-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----ca08e9891d9c----2---------------------bd489508_341d_4274_8616_394bd2cdf774-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----2-----------------clap_footer----bd489508_341d_4274_8616_394bd2cdf774-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----ca08e9891d9c----2---------------------bd489508_341d_4274_8616_394bd2cdf774-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----ca08e9891d9c----2-----------------bookmark_preview----bd489508_341d_4274_8616_394bd2cdf774-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----ca08e9891d9c----3---------------------bd489508_341d_4274_8616_394bd2cdf774-------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=read_next_recirc-----ca08e9891d9c----3---------------------bd489508_341d_4274_8616_394bd2cdf774-------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=read_next_recirc-----ca08e9891d9c----3---------------------bd489508_341d_4274_8616_394bd2cdf774-------", "anchor_text": "Wouter van Heeswijk, PhD"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----ca08e9891d9c----3---------------------bd489508_341d_4274_8616_394bd2cdf774-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----ca08e9891d9c----3---------------------bd489508_341d_4274_8616_394bd2cdf774-------", "anchor_text": "Proximal Policy Optimization (PPO) ExplainedThe journey from REINFORCE to the go-to algorithm in continuous control"}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----ca08e9891d9c----3---------------------bd489508_341d_4274_8616_394bd2cdf774-------", "anchor_text": "\u00b713 min read\u00b7Nov 29, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fabed1952457b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fproximal-policy-optimization-ppo-explained-abed1952457b&user=Wouter+van+Heeswijk%2C+PhD&userId=33f45c9ab481&source=-----abed1952457b----3-----------------clap_footer----bd489508_341d_4274_8616_394bd2cdf774-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----ca08e9891d9c----3---------------------bd489508_341d_4274_8616_394bd2cdf774-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fabed1952457b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fproximal-policy-optimization-ppo-explained-abed1952457b&source=-----ca08e9891d9c----3-----------------bookmark_preview----bd489508_341d_4274_8616_394bd2cdf774-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----ca08e9891d9c--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----ca08e9891d9c--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----ca08e9891d9c--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----ca08e9891d9c--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----ca08e9891d9c--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----ca08e9891d9c--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----ca08e9891d9c--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----ca08e9891d9c--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----ca08e9891d9c--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----ca08e9891d9c--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}