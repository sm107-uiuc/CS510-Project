{"url": "https://towardsdatascience.com/optimizing-individual-level-models-for-group-level-predictions-a66e675138ff", "time": 1682996745.48464, "path": "towardsdatascience.com/optimizing-individual-level-models-for-group-level-predictions-a66e675138ff/", "webpage": {"metadata": {"title": "Optimizing Individual-Level Models for Group-Level Predictions | by Hilaf Hasson | Towards Data Science", "h1": "Optimizing Individual-Level Models for Group-Level Predictions", "description": "The goal of this post is to present some of my thoughts about a very common, yet scantily addressed, problem in machine learning at large (and healthcare in particular): how do you construct\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/@hilafhasson/optimizing-individual-level-models-for-group-level-predictions-f48f491363f8", "anchor_text": "Part 2", "paragraph_index": 0}, {"url": "https://github.com/lumiata/tech_blog/blob/master/Individual_Model_Optimization/Individual_Model_Blog_Code.ipynb", "anchor_text": "this GitHub repo", "paragraph_index": 1}, {"url": "https://en.wikipedia.org/wiki/Recurrent_neural_network", "anchor_text": "Recurrent Neural Networks", "paragraph_index": 5}, {"url": "https://en.wikipedia.org/wiki/Borel%E2%80%93Kolmogorov_paradox", "anchor_text": "Borel\u2013Kolmogorov paradox", "paragraph_index": 10}, {"url": "https://github.com/lumiata/tech_blog/blob/master/Individual_Model_Optimization/Individual_Model_Blog_Code.ipynb", "anchor_text": "this GitHub repo", "paragraph_index": 18}, {"url": "https://medium.com/@hilafhasson/optimizing-individual-level-models-for-group-level-predictions-f48f491363f8", "anchor_text": "Part 2", "paragraph_index": 25}, {"url": "https://github.com/lumiata/tech_blog/blob/master/Individual_Model_Optimization/Individual_Model_Blog_Code.ipynb", "anchor_text": "this GitHub repo", "paragraph_index": 27}, {"url": "http://proceedings.mlr.press/v48/gal16.pdf", "anchor_text": "by the work of Yarin Gal and Zoubin Ghahramani", "paragraph_index": 31}, {"url": "https://www.lumiata.com/careers.html", "anchor_text": "open positions", "paragraph_index": 32}, {"url": "http://www.lumiata.com/", "anchor_text": "www.lumiata.com", "paragraph_index": 33}, {"url": "http://www.twitter.com/lumiata", "anchor_text": "@lumiata", "paragraph_index": 33}], "all_paragraphs": ["The goal of this post is to present some of my thoughts about a very common, yet scantily addressed, problem in machine learning at large (and healthcare in particular): how do you construct group-level predictions given only individual-level data in a way that optimizes a pre-determined loss function (e.g., MAE or MSE)? If you first create an individual-level model optimizing for some loss function and then take the average of the predictions, do you automatically optimize for the same loss function at the group level? As it turns out, while the answer happens to be \u201cyes\u201d in the case of MSE, the answer is a resounding \u201cNO\u201d in the case of MAE! In this post I explain why the knee-jerk method of using the same loss function at the individual level is The Wrong Thing To Do. This is a story about uncovering the real nature of this problem. As part of this exploration I use an interesting theorem by Peter Hall whose proof I found to be somewhat difficult to penetrate. As a service to the community, in Part 2 I will present the backstory for the theorem, and provide a slightly more natural proof than the one he provided in his paper, with all of the crucial details that were left out of the original paper filled in.", "All code for plots seen in this post can be found in this GitHub repo.", "In order to make this exploration easier to follow, I will use a concrete example and fix my assumptions.", "It only remains to specify what \u201ctrue values\u201d to compare against at the group level. In order to show why optimizing for the same loss function at the individual level that you do at the group level is misguided, it would be easiest using the following assumption:", "We will later also consider another assumption that arises more naturally if you allow for membership to vary.", "One way to optimize a particular loss function at the group level is simply to create a group-level model that optimizes this loss function, whose features are engineered aggregated individual-level features. But the number of groups is presumably much smaller than the number of individuals, so one would expect that such a model would suffer from high variance. A combined individual-level and group-level approach would be wisest. \u201cWhy not use Recurrent Neural Networks? That way you can make a group-level model that uses all the data!\u201d, I hear you cry. Well\u2026 That would be using all the individual-level features, but not the unaggregated target values, unless a special loss function is employed. And even then, RNN\u2019s are order-dependent, and group membership is not! So let\u2019s go ahead and call that \u201cexperimental\u201d. Either way, your group-level predictions can only gain from doing an individual-level model well. So let\u2019s get to it!", "The bottom line is: optimizing for MSE means you are estimating the mean; optimizing for MAE means you are estimating the median. What does that actually mean? Let Y be the target value, and let X_1,\u2026,X_n be the features. If your feature values are X_1=x_1,\u2026,X_n=x_n, then the target value given those features Y|X_1=x_1,\u2026,X_n=x_n is a random variable, not a constant. In other words, your feature values don\u2019t determine the target. For example, if you are predicting cost, then it is perfectly conceivable that two individuals with the same feature values have different costs; though knowing these feature values does change the distribution of cost.", "Indeed, if f(x_1,\u2026,x_n) is the model\u2019s prediction at these feature values for a machine learning model whose loss function is MSE, then it would attempt to approximate the a that minimizes:", "Similarly, if f(x_1,\u2026,x_n) is the model\u2019s prediction at these feature values for a machine learning model whose loss function is MAE, then it would attempt to approximate the a that minimizes:", "The a that minimizes this expression is median(Y|X_1=x_1,\u2026,X_n=x_n). (To see this you have to play around with integrals; it\u2019s an easy but annoying exercise.)", "Technical note: Conditional probability of a random variable given an event only makes sense if the event that you\u2019re conditioning on has positive probability. Attempts to extend the definition to events with zero probability are doomed to be ill-defined unless we specify a limiting procedure; see the Borel\u2013Kolmogorov paradox. If at least one of the X_i\u2019s is continuous, then P(X_1=x_1,\u2026,X_n=x_n)=0, which would imply that Y|X_1=x_1,\u2026,X_n=x_n doesn\u2019t make any sense. (The definition commonly found in undergraduate textbooks for Y|X_1=x_1,\u2026,X_n=x_n is not coordinate invariant. For the more advanced readers: conditioning on sub-sigma-algebras rather than events yields the same issue, since the resulting random variable is unique up to \u201calmost sure equality\u201d, which means probability zero events can be exceptions.) We can and will elegantly avoid this issue by restricting the values of the features to values that a computer can represent. In this way, even \u201ccontinuous\u201d variables are actually discrete, and everything is well-defined.", "Fix a group of size m, and let", "be the features of the i-th person. If we are creating an individual-level model that optimizes MSE, then the average of the results is an estimate of", "By the linearity of expectation, this is equal to", "Great! In other words, by optimizing MSE at the individual level, you\u2019re optimizing MSE at the group level!  Now let\u2019s do the same analysis for MAE. If we are creating an individual-level model that optimizes MAE, then the aggregation of the results is an estimate of", "But that is very, very, veeeery far from being", "Here\u2019s a quick illustration that the median of the sum is very far from being the sum of the medians, even if the random variables are all i.i.d\u2019s:", "In order to set up this example, I will be using a Gamma distribution with scale 200 and shape 1. This is what this distribution looks like:", "(Code for plots seen in this post can be found in this GitHub repo.)", "Now take X_i\u2019s to be i.i.d\u2019s following this distribution. Then here is a comparison of the average of their medians versus the median of their average:", "As you can see, this gap is quite large!", "If a group is large then it is somewhat reasonable to assume that", "is approximately normal; and the median of a normal distribution is its mean. So if you optimize for MSE at the individual level and then take the average of the predictions, you\u2019ll be approximately estimating the median for large groups! This might seem counter-intuitive at first: optimizing MSE at the individual level is superior to optimizing MAE at the individual level not only for group-level MSE, but also for group-level MAE.", "But how bad should we expect the bias to be for small groups? (As it will turn out, secretly the key is to use estimates of the error of the Central Limit Theorem.)", "This piqued my interest. So I started looking for results about the median of a sum of i.i.d\u2019s. I found \u201cOn the Limiting Behaviour of the Mode and Median of a Sum of Independent Variables\u201d by Peter Hall\u00b9, where he proved the following result.", "(See Part 2 of this post for a deeper understanding of why this theorem is true.) Since the X_i\u2019s are all identically distributed, let\u2019s simply let X := X_1 for notational ease. If we do not assume that the X_i\u2019s have mean 0 and variance 1, a quick back of the envelope computation, reducing to the normalized case, shows that this reduces to the following approximation:", "Notice that this is an asymptotic result! For us to be able to use it in the machine learning context discussed above, we must first make sure that this approximation is reasonable for small n. So let\u2019s do a quick proof-of-concept:", "(Code for plots seen in this post can be found in this GitHub repo.)", "For each group we would be attempting to estimate", "for that group. The summands are not i.i.d\u2019s, and so Hall\u2019s result is not directly applicable. To that end, we can try to replace Assumption A with:", "(Arguably, Assumption B comes up more often in real life. Often in group level predictions the members don\u2019t stay fixed, but the current members\u2019 target values are indicative of the distribution target values of future members.) Now that the target value is the mean of i.i.d\u2019s (because the sampling is done with repetition), we may employ Hall\u2019s approximation. Fix a group, and let T_1, \u2026, T_m be the sampling, with repetition, mentioned in Assumption B. The T_i\u2019s are now i.i.d\u2019s, and so for the sake of clarity let T:=T_1. We are now faced with the challenge of approximating V(T) and E((T-\u03bc)\u00b3) for each group. This can be difficult, given that the groups that we want the most to bias-correct are small groups.  One simplistic approach is to estimate V(T) as V(Y), and estimate E((T-\u03bc)\u00b3) as E((Y-E(Y))\u00b3). In turn, estimate V(Y) and E((Y-E(Y))\u00b3) by taking the average values over the target values in our data, and denote these estimates as V\u02c6 and \u03ba\u02c6_3 respectively. Whether these estimates work well or not depends on how different the groups are from one another \u2014 the more similar they are, the better this bias correction will be. Either way, these approximations are good enough to get a relatively clear picture of how small a group needs to be so that the bias is beyond your level of comfort. Namely, a rough estimate is that if the size of a group is m, then averaging an individual model trained using MSE should have a bias of about", "For groups for which this number is intolerably big, I would suggest extra care: V\u02c6 and \u03ba\u02c6_3 may not be good enough estimates, and may lead to a bad bias correction. I would simply suggest to flag groups of this size as requiring a bias correction, and do more research on the bias correction needed that is informed by the data and the particular application you have in mind.  (Notice that while it is tempting to use Bayesian approaches that will allow you to sample from the distributions of the Y|X_1=x_{i, 1},\u2026,X_n=x_{i, n}\u2019s, the vanilla version of these approaches assumes that the Y|X_1=x_{i, 1},\u2026,X_n=x_{i, n}\u2019s are normally distributed. If that were the case, there wouldn\u2019t be any bias to correct for\u2026 This holds also for using dropout at prediction time in a neural network, which, by the work of Yarin Gal and Zoubin Ghahramani\u00b2, is roughly equivalent to sampling the posterior in an appropriately defined Gaussian process setting \u2014 but that setting also assumes normally distributed error.)", "If this post got your interest and you would like to challenge yourself with similar problems, Lumiata is hiring! Please check Lumiata\u2019s open positions.", "Visit Lumiata at www.lumiata.com and follow on Twitter via @lumiata.", "1. Hall, P. (1980) On the limiting behaviour of the mode and median of a sum of independent random variables. Ann. Probability 8 419\u2013430.", "2. Gal, Y. and Ghahramani, Z. (2016) Dropout as a Bayesian approximation: Representing model uncertainty in deep learning.ICML.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Machine Learning Scientist @ AWS AI Labs; former Stanford Professor of Mathematics"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fa66e675138ff&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimizing-individual-level-models-for-group-level-predictions-a66e675138ff&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimizing-individual-level-models-for-group-level-predictions-a66e675138ff&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimizing-individual-level-models-for-group-level-predictions-a66e675138ff&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimizing-individual-level-models-for-group-level-predictions-a66e675138ff&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----a66e675138ff--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a66e675138ff--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@hilafhasson?source=post_page-----a66e675138ff--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@hilafhasson?source=post_page-----a66e675138ff--------------------------------", "anchor_text": "Hilaf Hasson"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9ec1226c49bb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimizing-individual-level-models-for-group-level-predictions-a66e675138ff&user=Hilaf+Hasson&userId=9ec1226c49bb&source=post_page-9ec1226c49bb----a66e675138ff---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa66e675138ff&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimizing-individual-level-models-for-group-level-predictions-a66e675138ff&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa66e675138ff&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimizing-individual-level-models-for-group-level-predictions-a66e675138ff&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/@hilafhasson/optimizing-individual-level-models-for-group-level-predictions-f48f491363f8", "anchor_text": "Part 2"}, {"url": "https://github.com/lumiata/tech_blog/blob/master/Individual_Model_Optimization/Individual_Model_Blog_Code.ipynb", "anchor_text": "this GitHub repo"}, {"url": "https://en.wikipedia.org/wiki/Recurrent_neural_network", "anchor_text": "Recurrent Neural Networks"}, {"url": "https://en.wikipedia.org/wiki/Borel%E2%80%93Kolmogorov_paradox", "anchor_text": "Borel\u2013Kolmogorov paradox"}, {"url": "https://github.com/lumiata/tech_blog/blob/master/Individual_Model_Optimization/Individual_Model_Blog_Code.ipynb", "anchor_text": "this GitHub repo"}, {"url": "https://medium.com/@hilafhasson/optimizing-individual-level-models-for-group-level-predictions-f48f491363f8", "anchor_text": "Part 2"}, {"url": "https://github.com/lumiata/tech_blog/blob/master/Individual_Model_Optimization/Individual_Model_Blog_Code.ipynb", "anchor_text": "this GitHub repo"}, {"url": "http://proceedings.mlr.press/v48/gal16.pdf", "anchor_text": "by the work of Yarin Gal and Zoubin Ghahramani"}, {"url": "https://medium.com/lumiata", "anchor_text": "Lumiata"}, {"url": "https://www.lumiata.com/careers.html", "anchor_text": "open positions"}, {"url": "https://github.com/lumiata/tech_blog", "anchor_text": "https://github.com/lumiata/tech_blog"}, {"url": "http://www.lumiata.com/", "anchor_text": "www.lumiata.com"}, {"url": "http://www.twitter.com/lumiata", "anchor_text": "@lumiata"}, {"url": "https://www.linkedin.com/company/lumiata/", "anchor_text": "www.linkedin.com/company/lumiata"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----a66e675138ff---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----a66e675138ff---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/groups?source=post_page-----a66e675138ff---------------groups-----------------", "anchor_text": "Groups"}, {"url": "https://medium.com/tag/loss-function?source=post_page-----a66e675138ff---------------loss_function-----------------", "anchor_text": "Loss Function"}, {"url": "https://medium.com/tag/statistics?source=post_page-----a66e675138ff---------------statistics-----------------", "anchor_text": "Statistics"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa66e675138ff&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimizing-individual-level-models-for-group-level-predictions-a66e675138ff&user=Hilaf+Hasson&userId=9ec1226c49bb&source=-----a66e675138ff---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa66e675138ff&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimizing-individual-level-models-for-group-level-predictions-a66e675138ff&user=Hilaf+Hasson&userId=9ec1226c49bb&source=-----a66e675138ff---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa66e675138ff&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimizing-individual-level-models-for-group-level-predictions-a66e675138ff&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a66e675138ff--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fa66e675138ff&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimizing-individual-level-models-for-group-level-predictions-a66e675138ff&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----a66e675138ff---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----a66e675138ff--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----a66e675138ff--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----a66e675138ff--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----a66e675138ff--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----a66e675138ff--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----a66e675138ff--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----a66e675138ff--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----a66e675138ff--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@hilafhasson?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@hilafhasson?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Hilaf Hasson"}, {"url": "https://medium.com/@hilafhasson/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "24 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9ec1226c49bb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimizing-individual-level-models-for-group-level-predictions-a66e675138ff&user=Hilaf+Hasson&userId=9ec1226c49bb&source=post_page-9ec1226c49bb--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fbea8ab878e05&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimizing-individual-level-models-for-group-level-predictions-a66e675138ff&newsletterV3=9ec1226c49bb&newsletterV3Id=bea8ab878e05&user=Hilaf+Hasson&userId=9ec1226c49bb&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}