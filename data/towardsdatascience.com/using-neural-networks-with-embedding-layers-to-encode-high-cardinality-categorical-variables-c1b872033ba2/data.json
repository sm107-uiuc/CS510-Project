{"url": "https://towardsdatascience.com/using-neural-networks-with-embedding-layers-to-encode-high-cardinality-categorical-variables-c1b872033ba2", "time": 1683015575.9377122, "path": "towardsdatascience.com/using-neural-networks-with-embedding-layers-to-encode-high-cardinality-categorical-variables-c1b872033ba2/", "webpage": {"metadata": {"title": "Using neural networks with embedding layers to encode high cardinality categorical variables | by Sebastian Telsemeyer | Towards Data Science", "h1": "Using neural networks with embedding layers to encode high cardinality categorical variables", "description": "There are multiple ways to encode categorical features. If no ordered relation between the categories exists one-hot-encoding is a popular candidate (i.e. adding a binary feature for every category)\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/smarter-ways-to-encode-categorical-data-for-machine-learning-part-1-of-3-6dca2f71b159", "anchor_text": "many others", "paragraph_index": 0}, {"url": "https://keras.io/layers/embeddings/", "anchor_text": "docs", "paragraph_index": 28}], "all_paragraphs": ["There are multiple ways to encode categorical features. If no ordered relation between the categories exists one-hot-encoding is a popular candidate (i.e. adding a binary feature for every category), alongside many others. But one-hot-encoding has some drawbacks \u2014 which can be tackled by using embeddings.", "One drawback is that it does not work well for high cardinality categories: it will produce very large/wide and sparse datasets, and a lot of memory and regularization due to the shear amount of features will be needed. Moreover one-hot-encoding does not exploit relationships between the categories. Imagine you have animal species as a feature, with values like domestic cat, tiger and elephant. There are probably more similarities between domestic cat and tiger compared to domestic cat and elephant. This is something a smarter encoding could take into account. In very practical scenarios these categoricals can be things like customers, products or locations \u2014 very high cardinality categories with relevant relationships between the individual observations.", "One way to tackle these problems is to use embeddings, a popular example or embeddings are word embeddings in NLP problems. Instead of encoding the categories using a huge binary space we use a smaller dense space. And instead of manually encoding them, we rather define the size of the embedding space and then try to make the model learn a useful presentation. For our animal species example we could retrieve representations like [0.5, 0, 0] for domestic cat,[1, 0.4, 0.1] for tiger and [0, 0.1, 0.6] for elephant, [-0.5, 0.5, 0.4] for shark, and so on.", "In the following post we will build a neural network using embeddings to encode the categorical features, moreover we will benchmark the model against a very naive linear model without categorical variables, and a more sophisticated regularized linear model with one-hot-encoded features.", "Let us look at a generated toy problem. Imagine we repeatedly buy different products from various suppliers and we want to predict their size. Now let us assume every product comes labelled with a supplier_id and a product_id, identifiers for the supplier and product itself. Let us also assume the items have some obvious measurements/features x1 and x2, like price and weight, and some secret, unmeasurable features, s1, s2 and s3, from which the size could theoretically be computed like this (s3 has no impact to the size):", "The problem is that we do not know the secret features s1, s2 and s3, and we cannot measure them directly, which is actually a pretty common problem in machine learning. But we have a little bit of a margin here, because we have the product and supplier ids \u2014 but there are too many to one-hot-encode them and use them straightforwardly. And let us assume from experience we know that products from different sellers have different sizes, so it is reasonable to assume that the items from the seller have very similar secret properties.", "The question is, can our model learn the relation g above just from the product and supplier ids, even if we have hundred thousand of different ones? The answer is yes, if we have enough observations.", "Let us look at a small dataset to get a better picture. We generate 300 samples with 4 different values in s1 and 3 different values in s2 (remember that s3 had no impact) and visualize the obvious impact the secret properties have on the relationship between price, weight and size.", "But now the problem: we do not know the features s1, s2 and s3, but only the product ids & supplier ids. To simulate how this dataset would appear in the wild let us introduce a hash function that we will use to obfuscate our unmeasurable features and generate some product and supplier ids.", "We can now generate our data with obfuscated properties, replaced by product ids:", "We still see that different products tend to have different values, but we cannot easily compute the size value from the product id any more:", "Let us generate a bigger dataset now. To be able to fairly compare our embedding model with a more naive baseline, and to be able to validate our approach, we will assume rather small values for our categorical cardinalities S1_BINS, S2_BINS, S3_BINS.", "If S1_BINS, S2_BINS, S3_BINS >> 10000 the benchmark models will run into memory issues and will perform poorly.", "We will now split the data into features and response, and train and test.", "Let us first assemble a very naive linear model, which performs quite poorly and only tries to estimate size from price and weight:", "The bad performance close to the overall variance of the response is obvious if we look at the correlations between price and weight and the response size, ignoring the ids:", "For a better benchmark we can one-hot-encode the categorical features and standardize the numeric data, using the sklearns ColumnTransformer to apply these transformations to different columns. Due to the amount of features we will use Ridge regression instead of normal linear regression to keep the coefficients small (but non-zero, unlike with Lasso, which would lead to losing information about specific classes).", "The one hot preprocessor spreads the features to columns and makes the data wide, additionally the numerical features get standardized to zero mean and unit variance:", "Now let us build a neural net model with embedding layers for our categoricals. To feed them to the embedding layer we need to map the categorical variables to numerical sequences first, i.e. integers from the intervals [0, #supplier ids] resp. [0, #product ids].", "Since sklearns OrdinalEncoder cannot handle unknown values as of now, we need to improvise. Unknown categories might occur when splitting test data randomly or seeing new data in the wild during prediction time. Therefore we have to use a simple implementation (working with dataframes instead of arrays, not optimized for speed) of an encoder which can handle unknown values: we essentially use an ordered dictionary as a hashmap to map values to positive integer ranges, where unknown values will get mapped to 0 (we need to map to non-negative values to conform with the embedding layer later on).", "The encoder can now encode and decode our data:", "It can also handle unknown data by mapping it to the zero-category:", "To feed the data to the model we need to split the input to pass it to different layers, essentially into X = [X_embedding1, X_embedding2, X_other]. We can do this using a transformer again, this time working with np.arrays, since the StandardScaler returns arrays:", "Let\u2019s combine those two now, and fit the preprocessor with the training data to encode the categories, perform the scaling and bring it into the right format:", "If we feed this data to the model now it will not be able to learn anything reasonable for unknown categories, i.e. categories that did not exist in the training data x_train when we did the fit. So once we try to make predictions for those we might receive unreasonable estimates.", "One way to tackle this out-of-vocabulary problem is to set some random training observations to unknown categories. Therefore during the training of the model, the transformation will encode these with 0, which is the token for unknowns, and it will allow the model to learn something close to the mean for unknown categories. With more domain knowledge we could also pick any other category as default, instead of sampling at random.", "We can now transform the data", "Time to build the neural network! We have 3 inputs (2 embeddings, 1 normal). The embedding inputs are both passed to Embedding layers, flattened and concatenated with the normal inputs. The following hidden layers consist of Dense & Dropout, and are finally activated linearly.", "If you see an error like InvalidArgumentError: indices[37,0] = 30 is not in [0, 30) you chose the wrong vocabulary size, which should be the largest index of the embedded values plus 1 according to the docs.", "As we can see the model performs comparably good and even better as the baseline linear model for our linear problem, though its true strength it will only come into play when we blow up the space of the categoricals or add non-linearity to the response:", "We can also extract the weights of the embedding layers (the first row contains the weights for the zero-label):", "These weights could potentially be persisted somewhere and used as features in other models (pre-trained-embeddings). Looking at the first 10 categories we can see that values from supplier_id with similar responses y have similar weights:", "Moreover the model performs reasonably for unknown data, since it performs similar as our very naive baseline model and also similar to assuming a simple conditional mean:", "We have seen how we can leverage embedding layers to encode high cardinality categorical variables, and depending on the cardinality we can also play around with the dimension of our dense feature space for better performance. The price for this is a much more complicated model opposed to running a classical ML approach with one-hot-encoding.If a classical model is preferred, the category weights can be extracted from the embedding layer and used as features in a simpler model, therefore replacing the one-hot-encoding step.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Scientist based in Sydney, Australia"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fc1b872033ba2&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-neural-networks-with-embedding-layers-to-encode-high-cardinality-categorical-variables-c1b872033ba2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-neural-networks-with-embedding-layers-to-encode-high-cardinality-categorical-variables-c1b872033ba2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-neural-networks-with-embedding-layers-to-encode-high-cardinality-categorical-variables-c1b872033ba2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-neural-networks-with-embedding-layers-to-encode-high-cardinality-categorical-variables-c1b872033ba2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----c1b872033ba2--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c1b872033ba2--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://dsdx.medium.com/?source=post_page-----c1b872033ba2--------------------------------", "anchor_text": ""}, {"url": "https://dsdx.medium.com/?source=post_page-----c1b872033ba2--------------------------------", "anchor_text": "Sebastian Telsemeyer"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb7f2b6fc948e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-neural-networks-with-embedding-layers-to-encode-high-cardinality-categorical-variables-c1b872033ba2&user=Sebastian+Telsemeyer&userId=b7f2b6fc948e&source=post_page-b7f2b6fc948e----c1b872033ba2---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc1b872033ba2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-neural-networks-with-embedding-layers-to-encode-high-cardinality-categorical-variables-c1b872033ba2&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc1b872033ba2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-neural-networks-with-embedding-layers-to-encode-high-cardinality-categorical-variables-c1b872033ba2&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/smarter-ways-to-encode-categorical-data-for-machine-learning-part-1-of-3-6dca2f71b159", "anchor_text": "many others"}, {"url": "http://draw.io", "anchor_text": "draw.io"}, {"url": "https://keras.io/layers/embeddings/", "anchor_text": "docs"}, {"url": "https://blog.telsemeyer.com/2020/02/23/using-neural-networks-with-embedding-layers-to-encode-high-cardinality-categorical-variables/", "anchor_text": "https://blog.telsemeyer.com"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----c1b872033ba2---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----c1b872033ba2---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/feature-engineering?source=post_page-----c1b872033ba2---------------feature_engineering-----------------", "anchor_text": "Feature Engineering"}, {"url": "https://medium.com/tag/data-science?source=post_page-----c1b872033ba2---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc1b872033ba2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-neural-networks-with-embedding-layers-to-encode-high-cardinality-categorical-variables-c1b872033ba2&user=Sebastian+Telsemeyer&userId=b7f2b6fc948e&source=-----c1b872033ba2---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc1b872033ba2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-neural-networks-with-embedding-layers-to-encode-high-cardinality-categorical-variables-c1b872033ba2&user=Sebastian+Telsemeyer&userId=b7f2b6fc948e&source=-----c1b872033ba2---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc1b872033ba2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-neural-networks-with-embedding-layers-to-encode-high-cardinality-categorical-variables-c1b872033ba2&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c1b872033ba2--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fc1b872033ba2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-neural-networks-with-embedding-layers-to-encode-high-cardinality-categorical-variables-c1b872033ba2&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----c1b872033ba2---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----c1b872033ba2--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----c1b872033ba2--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----c1b872033ba2--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----c1b872033ba2--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----c1b872033ba2--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----c1b872033ba2--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----c1b872033ba2--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----c1b872033ba2--------------------------------", "anchor_text": ""}, {"url": "https://dsdx.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://dsdx.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Sebastian Telsemeyer"}, {"url": "https://dsdx.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "60 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb7f2b6fc948e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-neural-networks-with-embedding-layers-to-encode-high-cardinality-categorical-variables-c1b872033ba2&user=Sebastian+Telsemeyer&userId=b7f2b6fc948e&source=post_page-b7f2b6fc948e--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F9dda054f4b51&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-neural-networks-with-embedding-layers-to-encode-high-cardinality-categorical-variables-c1b872033ba2&newsletterV3=b7f2b6fc948e&newsletterV3Id=9dda054f4b51&user=Sebastian+Telsemeyer&userId=b7f2b6fc948e&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}