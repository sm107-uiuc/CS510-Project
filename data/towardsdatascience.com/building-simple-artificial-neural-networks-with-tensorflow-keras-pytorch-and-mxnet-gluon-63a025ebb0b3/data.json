{"url": "https://towardsdatascience.com/building-simple-artificial-neural-networks-with-tensorflow-keras-pytorch-and-mxnet-gluon-63a025ebb0b3", "time": 1682993345.772773, "path": "towardsdatascience.com/building-simple-artificial-neural-networks-with-tensorflow-keras-pytorch-and-mxnet-gluon-63a025ebb0b3/", "webpage": {"metadata": {"title": "Build simple artificial neural networks with popular deep learning frameworks | by Sau Sheong | Towards Data Science", "h1": "Build simple artificial neural networks with popular deep learning frameworks", "description": "A few weeks ago I went through the steps of building a very simple neural network and implemented it from scratch in Go. However there are many deep learning frameworks that are already available, so\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/how-to-build-a-simple-artificial-neural-network-with-go-ac2e8c49ae37", "anchor_text": "went through the steps of building a very simple neural network", "paragraph_index": 0}, {"url": "https://blog.thedataincubator.com/2017/10/ranking-popular-deep-learning-libraries-for-data-science/", "anchor_text": "ranking by Data Incubator", "paragraph_index": 1}, {"url": "https://sausheong.github.io/posts/building-neural-networks-with-tensorflow-keras-pytorch-mxnet/%5BDeepMind%5D(https://deepmind.com/)", "anchor_text": "DeepMind", "paragraph_index": 10}, {"url": "https://sausheong.github.io/posts/building-neural-networks-with-tensorflow-keras-pytorch-mxnet/%5BResearch%20Blog:%20DeepMind%20moves%20to%20TensorFlow%5D(https://research.googleblog.com/2016/04/deepmind-moves-to-tensorflow.html)", "anchor_text": "converted from Torch to TensorFlow in 2016", "paragraph_index": 10}, {"url": "https://sausheong.github.io/posts/building-neural-networks-with-tensorflow-keras-pytorch-mxnet/%5BOne-hot%20-%20Wikipedia%5D(https://en.wikipedia.org/wiki/One-hot)", "anchor_text": "one-hot", "paragraph_index": 12}, {"url": "https://sausheong.github.io/posts/how-to-build-a-simple-artificial-neural-network-with-go/", "anchor_text": "the previous neural network I created", "paragraph_index": 14}, {"url": "https://sausheong.github.io/posts/building-neural-networks-with-tensorflow-keras-pytorch-mxnet/%5BEverything%20you%20need%20to%20know%20about%20Adam%20Optimizer%20%E2%80%93%20Nishant%20Nikhil%20%E2%80%93%20Medium%5D(https://medium.com/@nishantnikhil/adam-optimizer-notes-ddac4fd7218)", "anchor_text": "Adam", "paragraph_index": 18}, {"url": "https://sausheong.github.io/posts/building-neural-networks-with-tensorflow-keras-pytorch-mxnet/%5BBig%20deep%20learning%20news:%20Google%20TensorFlow%20chooses%20Keras%20%C2%B7%20fast.ai%5D(http://www.fast.ai/2017/01/03/keras/)", "anchor_text": "support Keras in TensorFlow\u2019s core library", "paragraph_index": 30}, {"url": "https://sausheong.github.io/posts/building-neural-networks-with-tensorflow-keras-pytorch-mxnet/%5BPyTorch%5D(http://pytorch.org/)", "anchor_text": "PyTorch", "paragraph_index": 43}, {"url": "https://sausheong.github.io/posts/building-neural-networks-with-tensorflow-keras-pytorch-mxnet/%5BGitHub%20-%20HIPS/autograd:%20Efficiently%20computes%20derivatives%20of%20numpy%20code.%5D(https://github.com/HIPS/autograd)", "anchor_text": "autograd", "paragraph_index": 44}, {"url": "https://sausheong.github.io/posts/building-neural-networks-with-tensorflow-keras-pytorch-mxnet/%5BChainer:%20A%20flexible%20framework%20for%20neural%20networks%5D(https://chainer.org/)", "anchor_text": "Chainer", "paragraph_index": 44}, {"url": "https://sausheong.github.io/posts/building-neural-networks-with-tensorflow-keras-pytorch-mxnet/%5BMXNet:%20A%20Scalable%20Deep%20Learning%20Framework%5D(https://mxnet.incubator.apache.org/)", "anchor_text": "MXNet", "paragraph_index": 55}, {"url": "https://sausheong.github.io/posts/building-neural-networks-with-tensorflow-keras-pytorch-mxnet/%5BWhy%20Amazon%20picked%20MXNet%20for%20deep%20learning%20%7C%20InfoWorld%5D(https://www.infoworld.com/article/3144025/cloud-computing/why-amazon-picked-mxnet-for-deep-learning.html)", "anchor_text": "Amazon chose MXNet as a deep learning framework of choice", "paragraph_index": 56}, {"url": "https://sausheong.github.io/posts/building-neural-networks-with-tensorflow-keras-pytorch-mxnet/%5BAWS,%20Microsoft%20launch%20deep%20learning%20interface%20Gluon%20%7C%20ZDNet%5D(https://www.zdnet.com/article/aws-microsoft-launch-deep-learning-interface-gluon/)", "anchor_text": "October 2017 Amazon and Microsoft launched a new interface for MXNet called Gluon", "paragraph_index": 56}], "all_paragraphs": ["A few weeks ago I went through the steps of building a very simple neural network and implemented it from scratch in Go. However there are many deep learning frameworks that are already available, so doing it from scratch isn\u2019t normally what you\u2019ll do if you want to use deep learning as a tool to solve problems.", "The question is with the many that deep learning frameworks, which one should I use? There are many ways to compare deep learning frameworks. Here\u2019s a relatively recent (September 2017) ranking by Data Incubator that gives an interesting popularity ranking based on their Github, Stack Overflow and Google search results scores.", "From the results, it\u2019s quite clear that TensorFlow is easily the most popular framework and with Keras now being a part of TensorFlow itself, things won\u2019t change much in the near future. Also, almost all popular deep learning frameworks have Python APIs, so a combination of TensorFlow/Keras with Python seems the way to go.", "Nonetheless, I was curious about some of the other frameworks and so I began a mini-journey to write the same (or almost the same) simple artificial neural network I did in a few of these frameworks for comparison.", "As a quick refresher, the neural network I created was a simple feed-forward neural network, also commonly called a multi-level perceptron (MLP). Using this simple MLP, I took the MNIST dataset of 60,000 handwritten digits and trained the neural network with it. After that I used the test dataset of 10,000 handwritten digits to test the accuracy of the neural network.", "The neural network had 3 layers, the first (input) layer has 784 neurons (28 x 28 pixels), the second (hidden) layer has 200 neurons and the final (output) layer has 10 neurons. I used the sigmoid function as the activation function, and also mean square error as the loss function. Finally, I used 0.1 as the learning rate and didn\u2019t use any bias neurons at all.", "All the implementations below follow the same generic steps:", "This handwriting recognition of digits with the MNIST dataset is so often used in deep learning tutorials it\u2019s almost the \u2018hello world\u2019 of writing deep learning programs. As a disclaimer though, the implementations you see below are not optimized in any way and are not the definitive way of doing it. In fact there are many other more optimal ways of doing it, these are just a few.", "Now let\u2019s start and see how to implement this in TensorFlow first.", "TensorFlow was originally developed by researchers and engineers who worked on the Google Brain project for internal use and open sourced in 2015. It\u2019s the most popular deep learning framework to date by far.", "Amongst the more famous projects that are running on TensorFlow includes DeepMind (the Google-owned company that developed AlphaGo), which converted from Torch to TensorFlow in 2016.", "This implementation uses TensorFlow 1.6. Let\u2019s start.", "The is rather simple and self explanatory. Note that we\u2019re setting up the data output to be one-hot. This just means the position of the ndarray element with the highest value is the correct one.", "This is where we define the neural network. It\u2019s relatively straightforward. If the hidden and output weights are not passed in, the weights are randomly generated using the tf.random_uniform function. This happens when we train the neural network.", "As in the previous neural network I created, we first multiply (using tf.matmul) the input x with the hidden weights to get the hidden outputs. Remember we're working with matrices so tf.matmul is actually a dot product function and the hidden weights and the inputs are both matrices.", "The hidden outputs are then passed through an activation function, in this case, a sigmoid function. The output is then multiplied with the output weight to get the final outputs.", "The final outputs are returned after they have been passed through a sigmoid activation function again.", "Let\u2019s look at how we train our neural network model. First, we create it using the mlp function, passing it the inputs. We also define our error function aptly named error to be the squared difference between the target and the output (mean square error).", "Next, we define the optimizer, and we use the Adam optimizer here, passing it the learning rate and also our error function. When I first started dabbling with this, I used the gradient descent optimizer but the values take a very long time to converge. When when I switched over to the Adam optimizer it converged nicely so I used the Adam optimizer instead.", "Now that we have our optimizer, we initialise all the variables and define a saver so that we can save the model. We start a session and run the mini-batches by epochs, passing it the training dataset we loaded earlier on.", "Once we\u2019re done with the training, we save the model. A TensorFlow model consists of two parts. The first is the meta graph, which saves information on the TensorFlow graph. This is saved into a file with a .meta extension, in this case, it will be model.meta.", "The second are a bunch of checkpoint files. The model.index stores a list of variable names and shapes, while the model.data-00000-of-00001 stores the actual values of the variables.", "We\u2019ll be re-using these files later when we want to load the model for doing the prediction.", "After we\u2019ve trained the model we would want to have something that we can use for predicting the values. In this case, what we actually want is to run our predict function over the 10,000 images in the test dataset and see how many of them our trained model gets correctly.", "We start off with importing the meta graph, which is from the model.meta file. Next, we restore the checkpoint and use the default graph to get the hidden weights and output weights by their respective names.", "Finally, we restore the trained model by calling the mlp function and passing it the saved weights.", "Armed with the trained model, we try to predict output as we pass in the test dataset, and get the accuracy of the model. The predict function prints out the accuracy of the prediction of all the test images.", "The last bit is pretty trivial, it\u2019s just a main function that allows the user to either predict or train according. This part is actually the same in the other implementations so I won\u2019t be showing this code again later on.", "The model predicts correctly 97.25% of the time, which is not too good but ok. Now let\u2019s look at Keras next.", "Keras isn\u2019t a separate framework but an interface built on top of TensorFlow, Theano and CNTK. Keras is designed for fast prototyping and being easy to use and user-friendly.", "In 2017, TensorFlow decided to support Keras in TensorFlow\u2019s core library though nothing changed for Keras itself.", "Let\u2019s see how things are different in Keras.", "Setting up the dataset seems a bit more elaborate than before but it\u2019s not a big deal, in fact, it\u2019s clearer that we\u2019re reshaping the train and test datasets to the correct shapes and sizes.", "You might notice that I didn\u2019t define the neural network here. I could have created a separate mlp function to do that but it's not really necessary because I used one of the built-in Keras models called Sequential and simply stacked layers on top of it to build the network.", "The first two lines added the hidden and output layers (the input later is assumed by default, given the input shape of the hidden layer). This includes the activation function sigmoid.", "We define the optimizer next, using optimizers.Adam which is the built-in Adam optimizer.", "The model is compiled with the optimizer, and assigned an error (or loss) function mean_squared_error which is also built-in.", "Finally, we use the fit method to train the model using the images and labels, with the given batch size and number of epochs.", "As before, we save the model after training it.", "If you think the training function was rather simple, check out the predict function! You simply need to load up the model, then use it to evaluate the test images and labels!", "Here\u2019s what you see when training.", "And here\u2019s the results when predicting.", "The accuracy here is much better, we have 99.42% accuracy in detecting the correct images.", "PyTorch, as the name suggests, is the Python version of the Torch framework. Torch was originally developed in C, with a wrapper using the Lua programming language. PyTorch is primarily developed by Facebook\u2019s AI research group, and wraps around the Torch binaries with Python instead.", "A key feature in PyTorch is the ability to modify existing neural networks without having to rebuild it from scratch, using dynamic computation graphs. PyTorch describes it like using and replaying a tape recorder and it\u2019s inspired by other works such as autograd and Chainer.", "In implementing the simple neural network, I didn\u2019t have the chance to use this feature properly but it seems an interesting approach to building up a neural network that I\u2019d like to explore more later.", "Let\u2019s see how PyTorch works for our simple neural network.", "Loading the datasets take a few steps, but they are rather straightforward. What\u2019s interesting to note is that the transformation normalises with a mean of 0.1307 and standard deviation of 0.3081, which is the mean and standard deviation of the MNIST dataset.", "Defining the neural network is simple. We define some methods in the class, with sigmoid being nn.Sigmoid, hidden_layer and output_layer being linear layers with the appropriate sizes.", "The forward method then passes the input x into the hidden layer, and then to the sigmoid activation function. After that, it goes into the output layer and again to the sigmoid activation function one more time before returning the output.", "As with the other implementations, we first create the neural network model, the error function loss (which we defined to be a mean square error loss function) and also the Adam optimizer.", "We run the training for 50 epochs as usual. Because the training labels are not in the correct format, we need to convert it to a one-hot vector, target. Then we compute the error using the loss function, passing it the actual output values, as well as the target, then apply backpropagation to it.", "Finally, we save the model before ending the training. There are a couple of ways to save a PyTorch model. The more generic Python way is to save it as a pickle file, with a .pkl extension. This is what I used in this implementation. An alternative is to use PyTorch's own serialisation mechanism which saves into a file with a .pth extension.", "Predicting is simpler than training. Here we need to first create a neural network and load it with the saved state to reproduce the trained model. Then using the trained model we predict the output and then check if it\u2019s correct using the labels. Finally, we total up all the correctly predicted values and get the percentage of accuracy.", "As you can see, the network couldn\u2019t converge properly within 50 epochs with the same learning rate. The prediction accuracy here is quite poor, only 95.17%. On the other hand, when I switched over to using the SGD optimizer, the accuracy was better at 98.29%.", "MXNet is an Apache Foundation project that\u2019s currently being incubated in Apache. It has support in multiple languages and supported by a number of large industry players, prominently including Amazon and Microsoft.", "Amazon chose MXNet as a deep learning framework of choice because it claims that MXNet scales and runs better than other frameworks. MXNet models are portable and can be deployed on devices as well. In October 2017 Amazon and Microsoft launched a new interface for MXNet called Gluon, to make deep learning easier.", "Gluon is relatively easy to use and to build our simple neural network from my perspective it seems pretty much the same. Admittedly I probably haven\u2019t used it to it\u2019s best capabilities.", "Unlike other frameworks, you have to be more explicit where you want the context of operations are to be run on. In this case I\u2019m running on the CPU only so I created a context ctx that is based on the CPU.", "Loading the datasets are not much different from the other frameworks.", "Defining the neural network is relatively simple and quite similar to Keras. We simply use a built-in model add layers on it with the appropriate activation function then initialise it with the context and weights with random value sampled from a uniform distribution. I used the uniform distribution here to be consistent with the earlier implementations. I did try other distributions but the results are somewhat the same so at least in this post I am sticking to this distribution.", "To train the model, we first create it with our mlp function. We define an error function loss using L2Loss which is essential a mean square error function.", "We also define an optimiser (called a Trainer in MXNet), which uses the Adam optimizer algorithm.", "Next, we enumerate the train dataset and reshape into a one-hot ndarray. We pass the train dataset through the trained model to get an output. The output and labels are passed to the error function.", "After the training, we save the network model. MXNet allows us to save the parameters with a simple save_params method. It's not too particular about the file name so we can use any name we like.", "The predict function recreates our trained model by loading it from the file we saved earlier on. We reshape the data in the test dataset, and pass it through the loaded trained model and we get the predictions as an output. Then using the labels we find the accuracy of the predictions.", "Here\u2019s the result of the prediction using the MXNet framework with Gluon.", "The accuracy is 97.49% which is pretty much the same as the rest of the frameworks.", "Obviously this post doesn\u2019t have all the deep learning frameworks. It\u2019s more like a rambling walk through a few selected frameworks that piqued my fancy as I explored various frameworks. I missed out quite a number of popular ones including Caffe and Caffe2, CNTK, Theano, Torch, Sonnet and many many others.", "I didn\u2019t do any comparisons either \u2014 that\u2019s not the intention, any comparisons would require a much deeper understanding of these frameworks and a lot more time. And in a sense, since all these frameworks are growing (as I wrote this post over the past weeks, TensorFlow released 1.7 and 1.8 in a row!) and changing any comparisons would be inaccurate very quickly. Rather, my purpose was to figure out how easy it is to actually write deep learning software and how much these frameworks can help me do that.", "As I was using these frameworks I realised that they are largely the same in terms of what the goals are. In each framework, the goals are always to have an easy way to load the datasets, define a model, train that model then use it to predict the results. The way to achieve might be different from framework to framework and the underlying philosophies might differ but the goals remain the same.", "In a sense it\u2019s very similar to all web frameworks that I\u2019ve been using for the past 20 years. While amazing web applications have been created over the years, web frameworks have essentially worked about the same way, with the same views, controllers and services and working with HTTP.", "No doubt I\u2019m entirely oversimplifying everything, but in a sense, I\u2019m comforted at the same time.", "You can find all the source code here:", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F63a025ebb0b3&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-simple-artificial-neural-networks-with-tensorflow-keras-pytorch-and-mxnet-gluon-63a025ebb0b3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-simple-artificial-neural-networks-with-tensorflow-keras-pytorch-and-mxnet-gluon-63a025ebb0b3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-simple-artificial-neural-networks-with-tensorflow-keras-pytorch-and-mxnet-gluon-63a025ebb0b3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-simple-artificial-neural-networks-with-tensorflow-keras-pytorch-and-mxnet-gluon-63a025ebb0b3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://me.dm/@sausheong", "anchor_text": "Mastodon"}, {"url": "https://towardsdatascience.com/?source=post_page-----63a025ebb0b3--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----63a025ebb0b3--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://sausheong.com/?source=post_page-----63a025ebb0b3--------------------------------", "anchor_text": ""}, {"url": "https://sausheong.com/?source=post_page-----63a025ebb0b3--------------------------------", "anchor_text": "Sau Sheong"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa358058b2410&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-simple-artificial-neural-networks-with-tensorflow-keras-pytorch-and-mxnet-gluon-63a025ebb0b3&user=Sau+Sheong&userId=a358058b2410&source=post_page-a358058b2410----63a025ebb0b3---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F63a025ebb0b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-simple-artificial-neural-networks-with-tensorflow-keras-pytorch-and-mxnet-gluon-63a025ebb0b3&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F63a025ebb0b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-simple-artificial-neural-networks-with-tensorflow-keras-pytorch-and-mxnet-gluon-63a025ebb0b3&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/how-to-build-a-simple-artificial-neural-network-with-go-ac2e8c49ae37", "anchor_text": "went through the steps of building a very simple neural network"}, {"url": "https://blog.thedataincubator.com/2017/10/ranking-popular-deep-learning-libraries-for-data-science/", "anchor_text": "ranking by Data Incubator"}, {"url": "https://blog.thedataincubator.com/2017/10/ranking-popular-deep-learning-libraries-for-data-science/", "anchor_text": "https://blog.thedataincubator.com/2017/10/ranking-popular-deep-learning-libraries-for-data-science/"}, {"url": "https://sausheong.github.io/posts/building-neural-networks-with-tensorflow-keras-pytorch-mxnet/%5BDeepMind%5D(https://deepmind.com/)", "anchor_text": "DeepMind"}, {"url": "https://sausheong.github.io/posts/building-neural-networks-with-tensorflow-keras-pytorch-mxnet/%5BResearch%20Blog:%20DeepMind%20moves%20to%20TensorFlow%5D(https://research.googleblog.com/2016/04/deepmind-moves-to-tensorflow.html)", "anchor_text": "converted from Torch to TensorFlow in 2016"}, {"url": "https://sausheong.github.io/posts/building-neural-networks-with-tensorflow-keras-pytorch-mxnet/%5BOne-hot%20-%20Wikipedia%5D(https://en.wikipedia.org/wiki/One-hot)", "anchor_text": "one-hot"}, {"url": "https://sausheong.github.io/posts/how-to-build-a-simple-artificial-neural-network-with-go/", "anchor_text": "the previous neural network I created"}, {"url": "https://sausheong.github.io/posts/building-neural-networks-with-tensorflow-keras-pytorch-mxnet/%5BEverything%20you%20need%20to%20know%20about%20Adam%20Optimizer%20%E2%80%93%20Nishant%20Nikhil%20%E2%80%93%20Medium%5D(https://medium.com/@nishantnikhil/adam-optimizer-notes-ddac4fd7218)", "anchor_text": "Adam"}, {"url": "https://sausheong.github.io/posts/building-neural-networks-with-tensorflow-keras-pytorch-mxnet/%5BBig%20deep%20learning%20news:%20Google%20TensorFlow%20chooses%20Keras%20%C2%B7%20fast.ai%5D(http://www.fast.ai/2017/01/03/keras/)", "anchor_text": "support Keras in TensorFlow\u2019s core library"}, {"url": "https://sausheong.github.io/posts/building-neural-networks-with-tensorflow-keras-pytorch-mxnet/%5BPyTorch%5D(http://pytorch.org/)", "anchor_text": "PyTorch"}, {"url": "https://sausheong.github.io/posts/building-neural-networks-with-tensorflow-keras-pytorch-mxnet/%5BGitHub%20-%20HIPS/autograd:%20Efficiently%20computes%20derivatives%20of%20numpy%20code.%5D(https://github.com/HIPS/autograd)", "anchor_text": "autograd"}, {"url": "https://sausheong.github.io/posts/building-neural-networks-with-tensorflow-keras-pytorch-mxnet/%5BChainer:%20A%20flexible%20framework%20for%20neural%20networks%5D(https://chainer.org/)", "anchor_text": "Chainer"}, {"url": "https://sausheong.github.io/posts/building-neural-networks-with-tensorflow-keras-pytorch-mxnet/%5BMXNet:%20A%20Scalable%20Deep%20Learning%20Framework%5D(https://mxnet.incubator.apache.org/)", "anchor_text": "MXNet"}, {"url": "https://sausheong.github.io/posts/building-neural-networks-with-tensorflow-keras-pytorch-mxnet/%5BWhy%20Amazon%20picked%20MXNet%20for%20deep%20learning%20%7C%20InfoWorld%5D(https://www.infoworld.com/article/3144025/cloud-computing/why-amazon-picked-mxnet-for-deep-learning.html)", "anchor_text": "Amazon chose MXNet as a deep learning framework of choice"}, {"url": "https://sausheong.github.io/posts/building-neural-networks-with-tensorflow-keras-pytorch-mxnet/%5BAWS,%20Microsoft%20launch%20deep%20learning%20interface%20Gluon%20%7C%20ZDNet%5D(https://www.zdnet.com/article/aws-microsoft-launch-deep-learning-interface-gluon/)", "anchor_text": "October 2017 Amazon and Microsoft launched a new interface for MXNet called Gluon"}, {"url": "https://github.com/sausheong/pynn", "anchor_text": "https://github.com/sausheong/pynn"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----63a025ebb0b3---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/tensorflow?source=post_page-----63a025ebb0b3---------------tensorflow-----------------", "anchor_text": "TensorFlow"}, {"url": "https://medium.com/tag/keras?source=post_page-----63a025ebb0b3---------------keras-----------------", "anchor_text": "Keras"}, {"url": "https://medium.com/tag/pytorch?source=post_page-----63a025ebb0b3---------------pytorch-----------------", "anchor_text": "Pytorch"}, {"url": "https://medium.com/tag/python?source=post_page-----63a025ebb0b3---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F63a025ebb0b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-simple-artificial-neural-networks-with-tensorflow-keras-pytorch-and-mxnet-gluon-63a025ebb0b3&user=Sau+Sheong&userId=a358058b2410&source=-----63a025ebb0b3---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F63a025ebb0b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-simple-artificial-neural-networks-with-tensorflow-keras-pytorch-and-mxnet-gluon-63a025ebb0b3&user=Sau+Sheong&userId=a358058b2410&source=-----63a025ebb0b3---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F63a025ebb0b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-simple-artificial-neural-networks-with-tensorflow-keras-pytorch-and-mxnet-gluon-63a025ebb0b3&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----63a025ebb0b3--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F63a025ebb0b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-simple-artificial-neural-networks-with-tensorflow-keras-pytorch-and-mxnet-gluon-63a025ebb0b3&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----63a025ebb0b3---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----63a025ebb0b3--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----63a025ebb0b3--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----63a025ebb0b3--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----63a025ebb0b3--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----63a025ebb0b3--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----63a025ebb0b3--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----63a025ebb0b3--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----63a025ebb0b3--------------------------------", "anchor_text": ""}, {"url": "https://sausheong.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://sausheong.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Sau Sheong"}, {"url": "https://sausheong.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.6K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa358058b2410&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-simple-artificial-neural-networks-with-tensorflow-keras-pytorch-and-mxnet-gluon-63a025ebb0b3&user=Sau+Sheong&userId=a358058b2410&source=post_page-a358058b2410--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fb7ba9640486a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-simple-artificial-neural-networks-with-tensorflow-keras-pytorch-and-mxnet-gluon-63a025ebb0b3&newsletterV3=a358058b2410&newsletterV3Id=b7ba9640486a&user=Sau+Sheong&userId=a358058b2410&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://www.manning.com/books/go-web-programming", "anchor_text": "Go Web Programming2016"}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}