{"url": "https://towardsdatascience.com/machine-learnings-security-layer-an-overview-cfc11841f34e", "time": 1682993772.120973, "path": "towardsdatascience.com/machine-learnings-security-layer-an-overview-cfc11841f34e/", "webpage": {"metadata": {"title": "A Security Overview of ML Systems | by Maxence Prevost | Towards Data Science", "h1": "A Security Overview of ML Systems", "description": "Within a few scrolls we\u2019ll go through adversarial examples, model theft, dataset poisoning and dataset protection."}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/search/advanced?advanced=&terms-0-operator=AND&terms-0-term=adversarial+examples&terms-0-field=title&classification-computer_science=y&classification-physics_archives=all&classification-statistics=y&date-filter_by=all_dates&date-year=&date-from_date=&date-to_date=&size=50&order=-announced_date_first", "anchor_text": "active", "paragraph_index": 0}, {"url": "https://blog.openai.com/adversarial-example-research/", "anchor_text": "OpenAI", "paragraph_index": 0}, {"url": "https://github.com/maxpv/maxpv.github.io/blob/master/notebooks/Adversarial_ResNet50.ipynb", "anchor_text": "considered", "paragraph_index": 1}, {"url": "https://arxiv.org/pdf/1809.08336.pdf", "anchor_text": "adversarial recommendation", "paragraph_index": 3}, {"url": "https://arxiv.org/abs/1607.02533", "anchor_text": "print", "paragraph_index": 4}, {"url": "https://arxiv.org/abs/1710.08864", "anchor_text": "One pixel attack for fooling deep neural networks", "paragraph_index": 5}, {"url": "https://www.youtube.com/watch?v=SA4YEAWVpbk", "anchor_text": "One minute paper", "paragraph_index": 5}, {"url": "https://github.com/Hyperparticle/one-pixel-attack-keras", "anchor_text": "Keras implementation", "paragraph_index": 5}, {"url": "https://arxiv.org/abs/1602.02697", "anchor_text": "papers", "paragraph_index": 6}, {"url": "https://arxiv.org/abs/1607.02533", "anchor_text": "this paper", "paragraph_index": 11}, {"url": "https://www.usenix.org/system/files/conference/usenixsecurity16/sec16_paper_tramer.pdf", "anchor_text": "Source", "paragraph_index": 15}, {"url": "https://arxiv.org/abs/1602.02697", "anchor_text": "Black Box Attacks", "paragraph_index": 16}, {"url": "https://github.com/tensorflow/cleverhans/blob/023c3061f073e09204285949c85122da42e43d63/cleverhans/attacks_tf.py", "anchor_text": "github", "paragraph_index": 17}, {"url": "https://github.com/labsix/limited-blackbox-attacks", "anchor_text": "Github", "paragraph_index": 21}, {"url": "https://arxiv.org/abs/1706.03691", "anchor_text": "Certified Defenses for Data Poisoning Attacks", "paragraph_index": 23}, {"url": "https://arxiv.org/pdf/1804.00792.pdf", "anchor_text": "Poison Frogs! Targeted Clean-Label Poisoning Attacks on Neural Networks", "paragraph_index": 25}, {"url": "https://researcher.watson.ibm.com/researcher/view.php?person=us-cbgentry", "anchor_text": "Craig Gentry", "paragraph_index": 28}, {"url": "https://researcher.watson.ibm.com/researcher/view.php?person=us-cbgentry", "anchor_text": "IBM", "paragraph_index": 29}, {"url": "https://arxiv.org/abs/1711.05189", "anchor_text": "CryptoDL", "paragraph_index": 30}, {"url": "https://arxiv.org/abs/1610.05820", "anchor_text": "Membership Inference Attacks Against Machine Learning Models", "paragraph_index": 31}, {"url": "https://github.com/csong27/membership-inference", "anchor_text": "here", "paragraph_index": 33}], "all_paragraphs": ["The adversarial examples (AE) topic is fascinating and an active area of research. It raises fundamental questions related to the limits and the security of our current gradient-based classifier architectures. AE are cleverly crafted data designed to be misclassified by a targeted model. They are \u201cdesigned to cause the model to make a mistake\u201d (OpenAI, Attacking Machine Learning with Adversarial Examples). The image on the right is an adversarial example.", "The difference between the left and the rightmost dog is probably unperceptible. This can be due to our eyes limitations (or the bit depth of your monitor). And yet they are crucial to various models. The last image is indeed considered as a plane by a ResNet50 initialized with default training weight in Keras, and one AE will probably work on another architecture. The only difference are small pixels values, amplified in the second picture.", "We can notice that ResNet50 was pretty confident that the dog on the left picture is a golden_retriever (~80%) and the crafted image is a plane with a higher confidence (~99%). So a model can be tricked into making a mistake with the confidence score we desire, we in general just need to train it long enough. What are the impacts of misclassifying with an arbitrary confidence score?", "Recommendation systems are also studied for adversarial recommendation, influencing a recommendation system through indistinguishable fake users.", "In most known models, any image can be crafted into another class with an arbitrary confidence score. So our dog can be misclassified as anything we wish with any arbitrary accuracy. It has been shown that it works in the physical world too, for instance, if we print them. A famous example is tricking a car\u2019s sensor to see a speed limit instead of a STOP sign. The output of a model can be manipulated into making to some extent, a desired decision or at generating unhandled behavior by the application that relies on it.", "By the end of 2017 some showed that modifying one pixel can be enough in some cases. If you want to know more about this you can read the paper One pixel attack for fooling deep neural networks, enjoy a high-level presentation by the One minute paper channel or check this Keras implementation.", "Adversarial examples are simple attack and don\u2019t require much computation. On relatively small images a good GPU can craft an AE in less than a minute. This is a real security issue and that is probably why we can read those line at the end of some related subject papers:", "Research was also supported in part by the Army Research Laboratory, under Cooperative Agreement Number W911NF-13\u20132\u20130045 (ARL Cyber Security CRA), and the Army Research Office under grant W911NF-13\u20131\u20130421.", "We know that adversarial examples play with the decision boundaries of a classifier. We can, for instance, add random pixels on an image and change the classification or wisely choose those added pixels and choose the classification. Depending on the threat objective we denote:", "Depending on the opponent\u2019s knowledge, there are three ways of crafting adversarial examples. Each with their own assumed prior knowledge of the target. Knowing:", "A simplified copy of a diagram by Papernot. et al in The Limitations of Deep Learning in Adversarial Settings (page 3):", "One example of gradient-base attack consists in computing the loss\u2019 gradient function an image. Following by a tiny step in the opposite gradient\u2019s direction. In order to keep valid RGB values, the image might be clipped between 0 and 255 and the value of the noise between 0 and a small value, M. This value M determine the maximum difference between the original image and the adversarial one, so M should be smaller than a human\u2019s color sensibility (through a monitor). M smaller than 5 should be fine. The previous technique is called the iterative least-likely class method. Other types of gradient techniques exist like a fast gradient sign method. You can read this paper (part 2, page 3). We can note that they all require a complete knowledge of the model and its weights.", "Score-based attacks rely only on the predicted model\u2019s score to estimate the gradient and then apply the previous technique. Transfer-based attacks rely exclusively on the output label. This is a more realistic scenario compared to score-based and gradient-based. You can find an example of a transfer-based attack in the section Model Theft.", "Here we won\u2019t go much in depth I encourage you to search the keywords that attract you, it deserves a blog post on its own. We can see two big categories of defences:", "Trying to rebuild someone\u2019s else model or retrieve data that were used to train the model. The dataset and or the model might be confidential for their sensitive or commercial value.", "The tension between model confidentiality and public access motivates our investigation of model extraction attacks. (Source)", "We\u2019ll summarize briefly the Black Box Attacks by Nicolas Papernot et al. If you want to dig this subject you might enjoy reading it. The main idea described here is to create a local substitute neural network trained with a substitute dataset crafted by the adversary. Then, using gradient-based techniques adversarial examples can be generated.", "There\u2019s no need for a labeled dataset, which can be expensive to produce. The substitute dataset is labeled using the remote DNN\u2019s output. Then the local dataset is locally augmented through a technique called Jacobian-based Dataset Augmentation. Here is a pseudo code describing the Jacobian data augmentation (full code available on github).", "Basically, each example is augmented by adding a small variation in direction of the gradient.", "[\u2026] this technique is not designed to maximize the substitute DNN\u2019s accuracy but rather ensure that it approximates the oracle\u2019s decision boundaries with few label queries.", "The choice of architecture isn\u2019t very important since we can assume some details beforehand. There is a high chance that a CNN was used for an image classification task. It is also possible to train simultaneously several architectures.", "An implementation of a similar attack is available on Github.", "Dataset poisoning attacks aim at manipulating model\u2019s behavior at test time.", "Poisoning 3% of a training set managed to drop the test accuracy by 11% (Certified Defenses for Data Poisoning Attacks by Steinhardt at al. (2017)).", "Label flipping attack the objective is to maximize the loss function if a subset of the training example\u2019s label is flipped, this is basically done by gradient ascent:", "An attacker first chooses a target instance from the test set; a successful poisoning attack causes this target example to be misclassified during test time. Next, the attacker samples a base instance from the base class, and makes imperceptible changes to it to craft a poison instance; this poison is injected into the training data with the intent of fooling the model into labeling the target instance with the base label at test time. Finally, the model is trained on the poisoned dataset (clean dataset + poison instances). If during test time the model mistakes the target instance as being in the base class, then the poisoning attack is considered successful.Poison Frogs! Targeted Clean-Label Poisoning Attacks on Neural Networks", "Fully homomorphic encryption is an encryption scheme that preserves the operation on data through encryption and decryption function. If the scheme is preserved over the addition, encrypting a sum or summing the encrypted members will give the same result. This means that you can encrypt your data locally and send it to a server, let it do a job using only the supported operators and return you the encrypted result. You don\u2019t need to trust the server since it won\u2019t understand what it is manipulating.", "Let ENC and DEC the encryption and decryption function respectively:", "If you would need to follow one person in this field, it would be Craig Gentry. He found the first FHE scheme in 2009.", "Much of Craig\u2019s recent work, including FHE and cryptographic multilinear maps, generally falls into the area of \u201clattice-based cryptography\u201d. Unlike commonly-used cryptosystems like RSA and elliptic-curve cryptography, lattice-based cryptosystems cannot feasibly (as far as we know) be broken by quantum computers. (IBM)", "The most important part here is that if one day this encryption schemes exists we can (almost) not care about the privacy of our data we\u2019re sending on a remote machine. If this machine is malicious it can just give you wrong results but can\u2019t exploit your data\u2026 Unless\u2026 If we\u2019re talking about an FH encrypted machine learning model trying to predict something, nothing guarantees you that the model is empty at first and your opponent can still do inferences on the young model (by observing boundaries decisions and such). You should check out CryptoDL.", "It is also possible to recover the data used at training by simply looking at the model\u2019s output, Membership Inference Attacks Against Machine Learning Models:", "given a data record and black-box access to a model, determine if the record was in the model\u2019s training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model\u2019s predictions on the inputs that it trained on versus the inputs that it did not train on.", "An implementation can be found here.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "ML Engineer drinking this data soup sip by sip \ud83c\udf5c"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fcfc11841f34e&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learnings-security-layer-an-overview-cfc11841f34e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learnings-security-layer-an-overview-cfc11841f34e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learnings-security-layer-an-overview-cfc11841f34e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learnings-security-layer-an-overview-cfc11841f34e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----cfc11841f34e--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----cfc11841f34e--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@maxpv?source=post_page-----cfc11841f34e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@maxpv?source=post_page-----cfc11841f34e--------------------------------", "anchor_text": "Maxence Prevost"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd2a77aff36b1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learnings-security-layer-an-overview-cfc11841f34e&user=Maxence+Prevost&userId=d2a77aff36b1&source=post_page-d2a77aff36b1----cfc11841f34e---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcfc11841f34e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learnings-security-layer-an-overview-cfc11841f34e&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcfc11841f34e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learnings-security-layer-an-overview-cfc11841f34e&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://arxiv.org/search/advanced?advanced=&terms-0-operator=AND&terms-0-term=adversarial+examples&terms-0-field=title&classification-computer_science=y&classification-physics_archives=all&classification-statistics=y&date-filter_by=all_dates&date-year=&date-from_date=&date-to_date=&size=50&order=-announced_date_first", "anchor_text": "active"}, {"url": "https://blog.openai.com/adversarial-example-research/", "anchor_text": "OpenAI"}, {"url": "http://github.com/maxpv/maxpv.github.io/blob/master/notebooks/Adversarial_ResNet50.ipynb", "anchor_text": "notebook"}, {"url": "https://github.com/maxpv/maxpv.github.io/blob/master/notebooks/Adversarial_ResNet50.ipynb", "anchor_text": "considered"}, {"url": "https://arxiv.org/pdf/1809.08336.pdf", "anchor_text": "adversarial recommendation"}, {"url": "https://arxiv.org/abs/1607.02533", "anchor_text": "print"}, {"url": "https://arxiv.org/abs/1710.08864", "anchor_text": "One pixel attack for fooling deep neural networks"}, {"url": "https://www.youtube.com/watch?v=SA4YEAWVpbk", "anchor_text": "One minute paper"}, {"url": "https://github.com/Hyperparticle/one-pixel-attack-keras", "anchor_text": "Keras implementation"}, {"url": "https://arxiv.org/abs/1602.02697", "anchor_text": "papers"}, {"url": "https://arxiv.org/abs/1607.02533", "anchor_text": "this paper"}, {"url": "https://arxiv.org/abs/1602.02697", "anchor_text": "Black Box Attacks"}, {"url": "https://arxiv.org/abs/1705.09064", "anchor_text": "MagNet"}, {"url": "https://arxiv.org/abs/1806.05789", "anchor_text": "Random depth-wise signed"}, {"url": "https://www.usenix.org/system/files/conference/usenixsecurity16/sec16_paper_tramer.pdf", "anchor_text": "Source"}, {"url": "https://arxiv.org/abs/1602.02697", "anchor_text": "Black Box Attacks"}, {"url": "https://github.com/tensorflow/cleverhans/blob/023c3061f073e09204285949c85122da42e43d63/cleverhans/attacks_tf.py", "anchor_text": "github"}, {"url": "https://github.com/labsix/limited-blackbox-attacks", "anchor_text": "Github"}, {"url": "https://arxiv.org/abs/1706.03691", "anchor_text": "Certified Defenses for Data Poisoning Attacks"}, {"url": "https://arxiv.org/pdf/1804.00792.pdf", "anchor_text": "Poison Frogs! Targeted Clean-Label Poisoning Attacks on Neural Networks"}, {"url": "https://eprint.iacr.org/2017/1114.pdf", "anchor_text": "eprint.iacr.org/2017/1114.pdf"}, {"url": "https://researcher.watson.ibm.com/researcher/view.php?person=us-cbgentry", "anchor_text": "Craig Gentry"}, {"url": "https://researcher.watson.ibm.com/researcher/view.php?person=us-cbgentry", "anchor_text": "IBM"}, {"url": "https://arxiv.org/abs/1711.05189", "anchor_text": "CryptoDL"}, {"url": "https://arxiv.org/abs/1610.05820", "anchor_text": "Membership Inference Attacks Against Machine Learning Models"}, {"url": "https://github.com/csong27/membership-inference", "anchor_text": "here"}, {"url": "https://data-soup.github.io/blog/", "anchor_text": "data-soup.github.io/blog/"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----cfc11841f34e---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/security?source=post_page-----cfc11841f34e---------------security-----------------", "anchor_text": "Security"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----cfc11841f34e---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----cfc11841f34e---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/tag/data-science?source=post_page-----cfc11841f34e---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcfc11841f34e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learnings-security-layer-an-overview-cfc11841f34e&user=Maxence+Prevost&userId=d2a77aff36b1&source=-----cfc11841f34e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcfc11841f34e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learnings-security-layer-an-overview-cfc11841f34e&user=Maxence+Prevost&userId=d2a77aff36b1&source=-----cfc11841f34e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcfc11841f34e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learnings-security-layer-an-overview-cfc11841f34e&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----cfc11841f34e--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fcfc11841f34e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learnings-security-layer-an-overview-cfc11841f34e&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----cfc11841f34e---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----cfc11841f34e--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----cfc11841f34e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----cfc11841f34e--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----cfc11841f34e--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----cfc11841f34e--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----cfc11841f34e--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----cfc11841f34e--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----cfc11841f34e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@maxpv?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@maxpv?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Maxence Prevost"}, {"url": "https://medium.com/@maxpv/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "24 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd2a77aff36b1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learnings-security-layer-an-overview-cfc11841f34e&user=Maxence+Prevost&userId=d2a77aff36b1&source=post_page-d2a77aff36b1--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Fd2a77aff36b1%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learnings-security-layer-an-overview-cfc11841f34e&user=Maxence+Prevost&userId=d2a77aff36b1&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}