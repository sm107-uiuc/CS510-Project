{"url": "https://towardsdatascience.com/7-popular-activation-functions-you-should-know-in-deep-learning-and-how-to-use-them-with-keras-and-27b4d838dfe6", "time": 1683018435.023062, "path": "towardsdatascience.com/7-popular-activation-functions-you-should-know-in-deep-learning-and-how-to-use-them-with-keras-and-27b4d838dfe6/", "webpage": {"metadata": {"title": "7 popular activation functions you should know in Deep Learning and how to use them with Keras and TensorFlow 2 | by B. Chen | Towards Data Science", "h1": "7 popular activation functions you should know in Deep Learning and how to use them with Keras and TensorFlow 2", "description": "In artificial neural networks (ANNs), the activation function is a mathematical \u201cgate\u201d in between the input feeding the current neuron and its output going to the next layer [1]. The activation\u2026"}, "outgoing_paragraph_urls": [{"url": "https://github.com/BindiChen/machine-learning/blob/master/tensorflow2/010-popular-activation-functions/popular-activation-functions.ipynb", "anchor_text": "Notebook", "paragraph_index": 3}, {"url": "https://keras.io/api/layers/activations", "anchor_text": "documentation", "paragraph_index": 45}, {"url": "https://github.com/BindiChen/machine-learning/blob/master/tensorflow2/010-popular-activation-functions/popular-activation-functions.ipynb", "anchor_text": "notebook", "paragraph_index": 46}, {"url": "https://github.com/BindiChen/machine-learning", "anchor_text": "Github", "paragraph_index": 47}], "all_paragraphs": ["In artificial neural networks (ANNs), the activation function is a mathematical \u201cgate\u201d in between the input feeding the current neuron and its output going to the next layer [1].", "The activation functions are at the very core of Deep Learning. They determine the output of a model, its accuracy, and computational efficiency. In some cases, activation functions have a major effect on the model\u2019s ability to converge and the convergence speed.", "In this article, you\u2019ll learn the following most popular activation functions in Deep Learning and how to use them with Keras and TensorFlow 2.", "Please check out Notebook for the source code.", "The Sigmoid function (also known as the Logistic function) is one of the most widely used activation function. The function is defined as:", "The plot of the function and its derivative.", "As we can see in the plot above,", "The Sigmoid function was introduced to Artificial Neural Networks (ANN) in the 1990s to replace the Step function [2]. It was a key change to ANN architecture because the Step function doesn\u2019t have any gradient to work with Gradient Descent, while the Sigmoid function has a well-defined nonzero derivative everywhere, allowing Gradient Descent to make some progress at every step during training.", "The main problems with the Sigmoid function are:", "To use the Sigmoid activation function with Keras and TensorFlow 2, we can simply pass 'sigmoid' to the argument activation :", "To apply the function for some constant inputs:", "Another very popular and widely used activation function is the Hyperbolic Tangent, also known as Tanh. It is defined as:", "The plot of the function and its derivative:", "We can see that the function is very similar to the Sigmoid function.", "Tanh has characteristics similar to Sigmoid that can work with Gradient Descent. One important point to mention is that Tanh tends to make each layer\u2019s output more or less centered around 0 and this often helps speed up convergence [2].", "Since Tanh has characteristics similar to Sigmoid, it also faces the following two problems:", "To use the Tanh, we can simply pass 'tanh' to the argument activation:", "To apply the function for some constant inputs:", "The Rectified Linear Unit (ReLU) is the most commonly used activation function in deep learning. The function returns 0 if the input is negative, but for any positive input, it returns that value back. The function is defined as:", "The plot of the function and its derivative:", "It\u2019s surprising that such a simple function works very well in deep neural networks.", "ReLU works great in most applications, but it is not perfect. It suffers from a problem known as the dying ReLU.", "During training, some neurons effectively die, meaning they stop outputting anything other than 0. In some cases, you may find that half of your network\u2019s neurons are dead, especially if you used a large learning rate. A neuron dies when its weights get tweaked in such a way that the weighted sum of its inputs are negative for all instances in the training set. When this happens, it just keeps outputting 0s, and gradient descent does not affect it anymore since the gradient of the ReLU function is 0 when its input is negative.", "To use ReLU with Keras and TensorFlow 2, just set activation='relu'", "To apply the function for some constant inputs:", "Leaky ReLU is an improvement over the ReLU activation function. It has all properties of ReLU, plus it will never have dying ReLU problem. Leaky ReLU is defined as:", "The hyperparameter \u03b1 defines how much the function leaks. It is the slope of the function for x < 0 and is typically set to 0.01. The small slope ensures that Leaky ReLU never dies.", "To use the Leaky ReLU activation function, you must create a LeakyReLU instance like below:", "Parametric leaky ReLU (PReLU) is a variation of Leaky ReLU, where \u03b1 is authorized to be learned during training (instead of being a hyperparameter, it becomes a parameter that can be modified by backpropagation like any other parameters). This was reported to strongly outperform ReLU on large image datasets, but on smaller datasets it runs the risk of overfitting the training set [2].", "To use Parametric leaky ReLU, you must create a PReLU instance like below:", "Exponential Linear Unit (ELU) is a variation of ReLU with a better output for z < 0. The function is defined as:", "The hyperparameter \u03b1 controls the value to which an ELU saturates for negative net inputs.", "The plot of the function and its derivative:", "We can see in the plot above,", "According to the authors, ELU outperformed all the ReLU variants in their experiments [3].", "According to [2, 3], the main drawback of the ELU activation is that it is slower to compute than the ReLU and its variants (due to the use of the exponential function), but during training this is compensated by the faster convergence rate. However, at test time, an ELU network will be slower than a ReLU network.", "Implementing ELU in TensorFlow 2 is trivial, just specify the activation function when building each layer:", "To apply the function for some constant inputs:", "Exponential Linear Unit (SELU) activation function is another variation of ReLU proposed by Gu\u0308nter Klambauer et al. [4] in 2017. The authors showed that if you build a neural network composed exclusively of a stack of dense layers, and if all hidden layers use the SELU activation function, then the network will self-normalize (the output of each layer will tend to preserve mean 0 and standard deviation 1 during training, which resolves the vanishing/exploding gradients problem). This activation function often outperforms other activation functions very significantly.", "The plot of SELU and its derivative:", "The main problem with SELU is that there are a few conditions for SELU to work:", "To use SELU with Keras and TensorFlow 2, just set activation='selu' and kernel_initializer='lecun_normal':", "We have gone through 7 different activation functions in deep learning. When building a model, the selection of activation functions is critical. So which activation function should you use? Here is a general suggestion from the book Hands-on ML", "Although your mileage will vary, in general SELU > ELU > leaky ReLU (and its variants) > ReLU > tanh > logistic. If the network\u2019s architecture prevents it from self-normalizing, then ELU may perform better than SELU (since SELU is not smooth at z = 0). If you care a lot about runtime latency, then you may prefer leaky ReLU. If you don\u2019t want to tweak yet another hyperparameter, you may just use the default \u03b1 values used by Keras (e.g., 0.3 for the leaky ReLU). If you have spare time and computing power, you can use cross-validation to evaluate other activation functions, in particular, RReLU if your network is over\u2010fitting, or PReLU if you have a huge training set.", "In this article, we have gone through 7 different activation functions in Deep Learning and how to use them with Keras and TensorFlow.", "I hope this article will help you to save time in building and tuning your own Deep Learning model. I recommend you to check out the Keras documentation for the activation functions and to know about other things you can do.", "Thanks for reading. Please check out the notebook for the source code and stay tuned if you are interested in the practical aspect of machine learning.", "More can be found from my Github", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F27b4d838dfe6&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F7-popular-activation-functions-you-should-know-in-deep-learning-and-how-to-use-them-with-keras-and-27b4d838dfe6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2F7-popular-activation-functions-you-should-know-in-deep-learning-and-how-to-use-them-with-keras-and-27b4d838dfe6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F7-popular-activation-functions-you-should-know-in-deep-learning-and-how-to-use-them-with-keras-and-27b4d838dfe6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2F7-popular-activation-functions-you-should-know-in-deep-learning-and-how-to-use-them-with-keras-and-27b4d838dfe6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----27b4d838dfe6--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----27b4d838dfe6--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://bindichen.medium.com/?source=post_page-----27b4d838dfe6--------------------------------", "anchor_text": ""}, {"url": "https://bindichen.medium.com/?source=post_page-----27b4d838dfe6--------------------------------", "anchor_text": "B. Chen"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F563d09da62a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F7-popular-activation-functions-you-should-know-in-deep-learning-and-how-to-use-them-with-keras-and-27b4d838dfe6&user=B.+Chen&userId=563d09da62a&source=post_page-563d09da62a----27b4d838dfe6---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F27b4d838dfe6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F7-popular-activation-functions-you-should-know-in-deep-learning-and-how-to-use-them-with-keras-and-27b4d838dfe6&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F27b4d838dfe6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F7-popular-activation-functions-you-should-know-in-deep-learning-and-how-to-use-them-with-keras-and-27b4d838dfe6&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.canva.com/", "anchor_text": "canva.com"}, {"url": "https://github.com/BindiChen/machine-learning/blob/master/tensorflow2/010-popular-activation-functions/popular-activation-functions.ipynb", "anchor_text": "Notebook"}, {"url": "https://keras.io/api/layers/activations", "anchor_text": "documentation"}, {"url": "https://github.com/BindiChen/machine-learning/blob/master/tensorflow2/010-popular-activation-functions/popular-activation-functions.ipynb", "anchor_text": "notebook"}, {"url": "https://towardsdatascience.com/building-custom-callbacks-with-keras-and-tensorflow-2-85e1b79915a3", "anchor_text": "Building custom callbacks with Keras and TensorFlow 2"}, {"url": "https://towardsdatascience.com/a-practical-introduction-to-keras-callbacks-in-tensorflow-2-705d0c584966", "anchor_text": "A practical introduction to Keras Callbacks in TensorFlow 2"}, {"url": "https://towardsdatascience.com/learning-rate-schedule-in-practice-an-example-with-keras-and-tensorflow-2-0-2f48b2888a0c", "anchor_text": "Learning Rate Schedules in practice"}, {"url": "https://towardsdatascience.com/the-googles-7-steps-of-machine-learning-in-practice-a-tensorflow-example-for-structured-data-96ccbb707d77", "anchor_text": "The Google\u2019s 7 steps of Machine Learning in practice: a TensorFlow example for structured data"}, {"url": "https://towardsdatascience.com/3-ways-to-create-a-machine-learning-model-with-keras-and-tensorflow-2-0-de09323af4d3", "anchor_text": "3 ways to create a Machine Learning Model with Keras and TensorFlow 2.0"}, {"url": "https://towardsdatascience.com/batch-normalization-in-practice-an-example-with-keras-and-tensorflow-2-0-b1ec28bde96f", "anchor_text": "Batch normalization in practice: an example with Keras and TensorFlow 2.0"}, {"url": "https://towardsdatascience.com/a-practical-introduction-to-early-stopping-in-machine-learning-550ac88bc8fd", "anchor_text": "Early stopping in Practice: an example with Keras and TensorFlow"}, {"url": "https://github.com/BindiChen/machine-learning", "anchor_text": "Github"}, {"url": "https://missinglink.ai/guides/neural-network-concepts/7-types-neural-network-activation-functions-right/", "anchor_text": "7 Types of Neural Network activation function"}, {"url": "https://arxiv.org/abs/1511.07289", "anchor_text": "Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)"}, {"url": "https://arxiv.org/abs/1706.02515", "anchor_text": "Self-Normalizing Neural Networks"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----27b4d838dfe6---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----27b4d838dfe6---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/keras?source=post_page-----27b4d838dfe6---------------keras-----------------", "anchor_text": "Keras"}, {"url": "https://medium.com/tag/activation-functions?source=post_page-----27b4d838dfe6---------------activation_functions-----------------", "anchor_text": "Activation Functions"}, {"url": "https://medium.com/tag/neurons?source=post_page-----27b4d838dfe6---------------neurons-----------------", "anchor_text": "Neurons"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F27b4d838dfe6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F7-popular-activation-functions-you-should-know-in-deep-learning-and-how-to-use-them-with-keras-and-27b4d838dfe6&user=B.+Chen&userId=563d09da62a&source=-----27b4d838dfe6---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F27b4d838dfe6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F7-popular-activation-functions-you-should-know-in-deep-learning-and-how-to-use-them-with-keras-and-27b4d838dfe6&user=B.+Chen&userId=563d09da62a&source=-----27b4d838dfe6---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F27b4d838dfe6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F7-popular-activation-functions-you-should-know-in-deep-learning-and-how-to-use-them-with-keras-and-27b4d838dfe6&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----27b4d838dfe6--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F27b4d838dfe6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F7-popular-activation-functions-you-should-know-in-deep-learning-and-how-to-use-them-with-keras-and-27b4d838dfe6&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----27b4d838dfe6---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----27b4d838dfe6--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----27b4d838dfe6--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----27b4d838dfe6--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----27b4d838dfe6--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----27b4d838dfe6--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----27b4d838dfe6--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----27b4d838dfe6--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----27b4d838dfe6--------------------------------", "anchor_text": ""}, {"url": "https://bindichen.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://bindichen.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "B. Chen"}, {"url": "https://bindichen.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "4K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F563d09da62a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F7-popular-activation-functions-you-should-know-in-deep-learning-and-how-to-use-them-with-keras-and-27b4d838dfe6&user=B.+Chen&userId=563d09da62a&source=post_page-563d09da62a--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F56a3bfbb80dc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F7-popular-activation-functions-you-should-know-in-deep-learning-and-how-to-use-them-with-keras-and-27b4d838dfe6&newsletterV3=563d09da62a&newsletterV3Id=56a3bfbb80dc&user=B.+Chen&userId=563d09da62a&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}