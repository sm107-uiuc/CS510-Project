{"url": "https://towardsdatascience.com/ants-and-the-problems-with-neural-networks-778caa73f77b", "time": 1682997010.885576, "path": "towardsdatascience.com/ants-and-the-problems-with-neural-networks-778caa73f77b/", "webpage": {"metadata": {"title": "Ants and the Problems with Neural Networks | by Manuel Brenner | Towards Data Science", "h1": "Ants and the Problems with Neural Networks", "description": "Ants are pretty dumb. They live for a week and don\u2019t do much besides walking around, looking for food and carrying twigs to their anthill (now that I think about it, we humans also don\u2019t do much\u2026"}, "outgoing_paragraph_urls": [{"url": "https://psycnet.apa.org/record/1988-22413-001", "anchor_text": "this paper", "paragraph_index": 3}, {"url": "https://neurophysics.ucsd.edu/courses/physics_171/annurev.neuro.28.061604.135703.pdf", "anchor_text": "the role of the dendrites", "paragraph_index": 29}, {"url": "https://medium.com/@haxelschnurtz/a-non-technical-guide-to-turing-machines-f8c6da9596e5", "anchor_text": "here,", "paragraph_index": 36}, {"url": "https://www.ncbi.nlm.nih.gov/pubmed/9221787", "anchor_text": "this paper", "paragraph_index": 48}, {"url": "https://knowingneurons.files.wordpress.com/2013/04/place-cell-animation.gif?w=1000&h=607,", "anchor_text": "this animation", "paragraph_index": 57}, {"url": "http://amygdala.psychdept.arizona.edu/Jclub/Moser-Annreview+2008.pdf", "anchor_text": "the way they would function as a path integrator has not been determined yet", "paragraph_index": 58}], "all_paragraphs": ["Ants are pretty dumb. They live for a week and don\u2019t do much besides walking around, looking for food and carrying twigs to their anthill (now that I think about it, we humans also don\u2019t do much else).", "But they are also dumb apart from living uninspired ant lives. They are dumb in a technical sense: an individual ant has just at the order of 250.000 (2,5*10\u2075) neurons. As a comparison: an average homo sapiens has on average 80 billion neurons (8*10\u00b9\u2070), so if we assume that intelligence scales in at least some ways with the size of the brain (ignoring the fact that some animals have larger brains than we do), then we are approximately 320.000 times as smart as ants.", "But despite the tininess of their brains, ants have their moments. They are constantly doing things that are so sophisticated that they could be taken straight out of a university-level math exam. One of these things is called dead reckoning.", "When ants like the Cataglyphis bicolor go out foraging for food, they leave their home and move into the direction of an expected food source. As they don\u2019t know precisely where the food source is to be found, they walk around in wobbly lines and circles (see this paper by Maroudas and Harkness for more details and an illustration). If you have observed ants in the wild, you might have thought that it all looks completely random.", "But once the ant has found the food source and taken its share, it goes straight back to its home. And when I say straight, I mean literally along a straight line, without the help of any landmarks to guide them.", "Dead reckoning is the art of knowing where you are by remembering where you came from. No one really knows why it\u2019s called dead reckoning. You need to be very much alive to do it. And it\u2019s no trivial task.", "For ants to keep an understanding of where they are, they need to take into account the time with which they travel in a certain direction and the velocity with which they do it. They need to sum up all the small displacements of their path over long stretches of time, and they need to be able to store this information reliably and accessibly over time.", "In mathematics, you call this procedure path integration, because you integrate up little displacements over time to get a sum that you can then use to compute useful things.", "The ants then need to use this information to figure out the optimal route back to their home. To do able to this, the ant must implicitly understand how a right triangle looks like. Knowing the total displacement of the path it has walked in total (say, 10 meters north and 5 meters east), if it wants to go back the optimal path, it needs to travel on the hypotenuse of a right triangle.", "To travel along the hypotenuse, it needs to find the proper angle, which is the arctan of the proportion of the north and east displacement.", "We don\u2019t really know how the ant does this.", "But if we were to try to build a device that does the job, we would assume that we need to build it in a such a way that information about the position of the ant is stored in a reliable way that is also computationally accessible in times of need, because it needs it to compute the angle for the route the ant is going to take on the way back, and it needs to be able to do that independent of where it went and how much time it spent looking for food.", "This point is made repeatedly in the mind-changing Memory and the Computational Brain: Why Cognitive Science will Transform Neuroscience by A. King and C.R. Gallistel (I have sneakily stolen a variation of the subtitle for my headline).", "This article is a TL:DR version of some important aspects of the book. The book goes through much greater lengths to make similar points. I apologize if I have to sacrifice some details for the sake of brevity, but I believe the insights are highly relevant and wanted to condense them in a less time-consuming format.", "Now, remember that dead reckoning is something ants routinely do, among many other tasks required for living the ant life. And remember that they only have 250000 neurons.", "The brains of ants haven\u2019t been built by us, but have been shaped by millions of years of evolution. Neuroscientists are faced with the question of how the brain of an ant is wired in order to carry out these kinds of computations optimally. Nature has proven again and again to be parsimonious when it comes to spending its resources, so it\u2019s likely to assume it uses an architecture that is optimized for the task, especially considering the size of the ant brain.", "The brain, as a physical process, has to abide by the rules of physics and the basic rules of computations (worked out by people like Turing) just as much as any other computing device would have to. Even though it is true that the brain \u201cworks very differently from a normal computer\u201d, does \u201ca lot of parallel processing\u201d, and has \u201chuge amounts of neuronal connections\u201d at its disposition, these properties of brains shouldn\u2019t be taken as an excuse to reascribe some kind of magical abilities to brains per se.", "So we can continue by posing the question: how well can this kind of task be done by a neuronal architecture based on neural networks?", "\u201cLet\u2019s start by telling the truth: machines don\u2019t learn\u201d \u2014 Andriy Burkov in The Hundred-Page Machine Learning Book", "Machine Learning (ML) has given us extremely powerful learning algorithms for classifications. It\u2019s the art of inferring patterns and structures from data without giving explicit instructions on how to infer these patterns.", "ML methods have allowed us to do a lot of crazy and useful things. If the current hype around neural networks is to be believed, it looks like they could learn to do anything. We need to take care that a neural network doesn\u2019t get out of hand in the next couple of years, ushering in the dystopian future of mankind dominated by an artificial superintelligence.", "But following Andriy Burkov, it\u2019s a bit far-fetched to say that machines actually \u201clearn\u201d in the same sense that we or an animal would learn. For the usual supervised learning procedure, we just do a huge amount of parameter tuning: we are trying to find decision boundary (hyperplanes in the so-called \u201cfeature space\u201d, which would just be a line in two-dimensional space) given by the equation", "to give us good classification results for the training data.", "When the machine \u201clearns\u201d, the weights w are adjusted so as to find the decision boundary in feature space that gives us optimal classification results for any arbitrary feature vector x.", "For more sophistication, we can use kernel methods to make the metric in feature space scale non-linearily in the environment of samples, or we can use support vector machines to map into feature spaces with other dimensionality, we can adjust the activation functions of the neurons etc.", "But that doesn\u2019t change a fundamental fact of machine learning algorithms: there is no actual computation being done in a neural network.", "Neural networks in this sense are nothing more than sophisticated lookup tables (which in computer science is referred to as a finite state architecture for a computing device, which is similar to a Turing machine, but without a read/write memory): they prespecify a certain output to a given input. How they do this mapping is hidden from the user (especially when using something literally called hidden layers), and the information about how the network classifies is usually encoded in the structure and spread out through the entire network.", "Their function, therefore, is not representational: no representation of the input is carried through the network and can be traced through every step in the network. The input and output are everything. There are no intermediate steps, and the network does not need to know what the input means in order to process it.", "\u201cAll models are wrong, but some are useful.\u201d \u2014 George Box", "While Neural Networks were in some way inspired by the way the brain operates, neuroscientists don\u2019t necessarily believe that neural networks adequately represent how the brain works. A lot of the complexities of the biology are discarded, and the neuron is usually represented in an extremely simplified manner (the role of the dendrites or calcium flows at the synapses, among many other things, are all ignored or strongly simplified in models).", "Nevertheless, the current success of Neural Networks coincides with what is probably the most prominent learning theory in psychology: that of Associative Learning. Information is processed and incorporated in the brain by strengthening or weakening the connectivity between different neurons of the brain following the rule given by the famous catchphrase: \u201cwhat fires together wires together\u201d.", "The synaptic connections are the parameters of the brain that are tweaked during learning, so they serve a similar function as the weights in artificial neural networks.", "Speaking for myself, this has lead me to believe that Neural Networks really could in principle solve almost every task they are faced with and that the brain, therefore, had an architecture and a learning mechanism pretty similar to that of artificial neural networks.", "But there are some intrinsic problems with this view.", "One issue is that the most popular methods for learning in artificial neural networks (like backpropagation and gradient descent) are unrealistic as methods actually used by a biological system, because they require calculations to be carried out with the values of the weights, which in turn implies an omniscient presence outside of the network that can access the values of the weights and alter them accordingly.", "But synaptic weights are not computationally accessible to a biological system. They are in an artifical neural network, but only because the values of these variables are stored somewhere else on the computer as well, which is not itself a neural network. This, in turn, means that the functional architecture of the neurons is opaque to the brain itself, and it can not be used to store information in a form that would allow the brain to use it in times of need.", "There is an ongoing controversy in the neuroscience community about whether the brain uses symbolic representations of the things it is processing, as would be the case for a computing device based on the concepts of a Turing Machine (I go through the basics of Turing Machines in detail here, and explain what function the memory and the symbols serve).", "Namely, this implies a read/write head that can access the symbols it is calculating on, and a symbolic language in which the computation is encoded. As I described earlier, neural networks are not representational, and they do not work on symbolic representations of the input.", "This point leads us back to the ants, and how they return to their hill after having foraged for food.", "As we have seen, for an ant to successfully navigate back to its anthill, it sure looks a lot like it has some means to store its current position in a variable and to use this current position variable to calculate the optimal route for the way back.", "King\u2019s and Gallistel\u2019s point out that for the task of dead reckoning to be solved by a neural network, the computational and physical demands for the network would skyrocket.", "This is because a network isn\u2019t good at remembering things, i.e. storing information dynamically. After it has been optimized for a learning procedure, it basically stays the same while it operates. This is what Burkov means when he says that networks don\u2019t learn, because they only exert flexibility while learning a task, not while executing it.", "In light of this problem, some attempts that have been made to build architecture in networks that isable to store information dynamically. They are based on so-called recurrent feedback loops, which could serve as the network\u2019s memory.", "The term memory is here again used in Turing\u2019s broader senses: as a means to reliably carry information necessary for computation forward in time.", "Recurrent feedback loops are bumps of self-sustaining activity within a neural network. This sustained activity could function as a symbol, because the signal of the activity takes time to travel, and therefore carries the symbol it encodes forward in time. As the weights are not computationally accessible in neural networks, the symbols would have to be encoded dynamically, and memory would, therefore, operate dynamically.", "This method was used in early analog computing machines but was quickly discarded when new developments provided better ways to store information.", "One way to implement the technique in a neural network is by using recurrent neurons that are connected to themselves with weight 1, so they store information by activating themselves again and again.", "In models for dead reckoning, the position of the bump of activity in the neural network could, for instance, encode the position of the ant. The neurons at the respective position in the network would then keep on self-activating until the ant moved on.", "It gets complicated when one tries to implement this idea in practice (see this paper by Samsonovitch and McNaughton for a detailed model).", "Without going into too many details here, there are a couple of inherent problems with recurrent feedback loops as memory, especially if it comes to longer-term memory:", "From the beginning, computer scientists have appreciated that the key features of a powerful computing machine were speed, efficiency, and reliability.", "None of these concerns are satisfyingly dealt with when designing a memory mechanism in the brain based on feedback loops. The efficiency of natural computing devices makes it seem improbable that the brain really operates that way. Remember that ants only have around 250 000 neurons. The simplified model by Samsonovitch and McNaughton linked above would already require around 50 000 neurons to do the job of dead reckoning, which is much too high a number if one considers all the other complicated things ants have to do as well. Nature does not like spending resources, and ants definitely don\u2019t have many resources to spare.", "The moral of the story is that for a lot of cognitive tasks, the computational hardware carrying out that task needs something like an accessible symbolic memory mechanism. This becomes apparent when looking at the efficiency with which ants navigate after being out for foraging.", "To sum up the two problems we encountered:", "Synaptic strengths in neural networks probably wouldn\u2019t really do the job, because they are both inefficient in the way they carry out computation (by essentially building sophisticated finite states automata limited in their computational abilities) and the information stored in them is spread out and opaque to the brain itself. Therefore, associate learning implemented by adjusting synaptic strengths very likely can\u2019t be the whole answer when it comes to understanding how the brain computes.", "Recurrent feedback loops in networks similarily seem unfit as a good symbolic memory, because they are energy cost-inefficient and prone to noise, therefore unreliable as a long-term means of storing information.", "Gallistel and King propose that they are too wasteful to be the route that nature actually took when constructing the brain and that there needs to be another memory mechanism in the brain that we haven\u2019t discovered yet.", "This is not in any way to claim that neurons don\u2019t process information, or that information is not in some way related to their activity, location, and connectivity. An example of this is the role of place cells within the hippocampus, discovered by O\u2019Keefe and Dostrovsky in 1971 (for which they received the Nobel prize for medicine in 2014). Their firing is related to the spatial position of animals such as rats, and they are believed to serve as a cognitive map of places (see this animation for an illustration).", "Although they can provide better answers to questions about how animals navigate, the way they would function as a path integrator has not been determined yet. And they also do not resolve the question of a potential read/write symbolic memory, which in any case is also necessary for many other tasks that I couldn\u2019t cover here (like interval timing, learning the time of day, or jays accessing food they had hidden away).", "This brings us back to the subtitle of this article.", "Cognitive Science studies how devices capable of cognition, and more specifically, the brain, have to function. Computational constraints indicate that the setup of the brain as a computing device has to adhere to certain fundamental principles, which can inform the way we think about the neuroscientific foundations of the brain.", "Gallistel and King propose that a relevant memory mechanism could reside at different layers in the brain, e.g. at the molecular or submolecular layer. From a physics perspective, neurons are absolutely huge, with around 20 \u03bcm in humans, compared to an average atomic size of 30 pm. As Feynman said, there is usually a lot of space at the bottom, and in these six orders of magnitude (something like the difference between your size vs. the distance between New York and Los Angeles), there is a lot of potential room for new mechanisms.", "As we can see at the example of SSD drives, Blu-rays, or even DNA, storing information does not automatically presuppose a lot of space and a lot of resources, and can be done with more frugal means.", "Nevertheless, Gallistel and King do not offer any concrete ideas:", "In sum, we do not know what the physical mechanism of memory is. Moreover, we refuse to conjecture, except by way of illustration, what it might be. We refuse because we believe that we are unlikely to conjecture the correct answer in the face of our present level of ignorance.", "I think this is all very exciting in that it shows that we are just at the very beginning of understanding the workings of the brain. We won\u2019t come close to understanding how it computes if we don\u2019t even know how it stores and processes information over time. Neural networks are cool and a great invention, but it looks like they won\u2019t come close to providing us with all the answers.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F778caa73f77b&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fants-and-the-problems-with-neural-networks-778caa73f77b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fants-and-the-problems-with-neural-networks-778caa73f77b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fants-and-the-problems-with-neural-networks-778caa73f77b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fants-and-the-problems-with-neural-networks-778caa73f77b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----778caa73f77b--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----778caa73f77b--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://manuel-brenner.medium.com/?source=post_page-----778caa73f77b--------------------------------", "anchor_text": ""}, {"url": "https://manuel-brenner.medium.com/?source=post_page-----778caa73f77b--------------------------------", "anchor_text": "Manuel Brenner"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1fde95441432&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fants-and-the-problems-with-neural-networks-778caa73f77b&user=Manuel+Brenner&userId=1fde95441432&source=post_page-1fde95441432----778caa73f77b---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F778caa73f77b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fants-and-the-problems-with-neural-networks-778caa73f77b&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F778caa73f77b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fants-and-the-problems-with-neural-networks-778caa73f77b&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@miklevasilyev?utm_source=medium&utm_medium=referral", "anchor_text": "Mikhail Vasilyev"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://psycnet.apa.org/record/1988-22413-001", "anchor_text": "this paper"}, {"url": "https://unsplash.com/@franckinjapan?utm_source=medium&utm_medium=referral", "anchor_text": "Franck V."}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://neurophysics.ucsd.edu/courses/physics_171/annurev.neuro.28.061604.135703.pdf", "anchor_text": "the role of the dendrites"}, {"url": "https://medium.com/@haxelschnurtz/a-non-technical-guide-to-turing-machines-f8c6da9596e5", "anchor_text": "here,"}, {"url": "https://unsplash.com/@thomaskinto?utm_source=medium&utm_medium=referral", "anchor_text": "Thomas Kinto"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://www.ncbi.nlm.nih.gov/pubmed/9221787", "anchor_text": "this paper"}, {"url": "https://knowingneurons.files.wordpress.com/2013/04/place-cell-animation.gif?w=1000&h=607,", "anchor_text": "this animation"}, {"url": "http://amygdala.psychdept.arizona.edu/Jclub/Moser-Annreview+2008.pdf", "anchor_text": "the way they would function as a path integrator has not been determined yet"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----778caa73f77b---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----778caa73f77b---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/cognitive-science?source=post_page-----778caa73f77b---------------cognitive_science-----------------", "anchor_text": "Cognitive Science"}, {"url": "https://medium.com/tag/computer-science?source=post_page-----778caa73f77b---------------computer_science-----------------", "anchor_text": "Computer Science"}, {"url": "https://medium.com/tag/science?source=post_page-----778caa73f77b---------------science-----------------", "anchor_text": "Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F778caa73f77b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fants-and-the-problems-with-neural-networks-778caa73f77b&user=Manuel+Brenner&userId=1fde95441432&source=-----778caa73f77b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F778caa73f77b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fants-and-the-problems-with-neural-networks-778caa73f77b&user=Manuel+Brenner&userId=1fde95441432&source=-----778caa73f77b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F778caa73f77b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fants-and-the-problems-with-neural-networks-778caa73f77b&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----778caa73f77b--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F778caa73f77b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fants-and-the-problems-with-neural-networks-778caa73f77b&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----778caa73f77b---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----778caa73f77b--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----778caa73f77b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----778caa73f77b--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----778caa73f77b--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----778caa73f77b--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----778caa73f77b--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----778caa73f77b--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----778caa73f77b--------------------------------", "anchor_text": ""}, {"url": "https://manuel-brenner.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://manuel-brenner.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Manuel Brenner"}, {"url": "https://manuel-brenner.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "3.8K Followers"}, {"url": "https://anchor.fm/acit-science", "anchor_text": "https://anchor.fm/acit-science"}, {"url": "https://www.linkedin.com/in/manuel-brenner-772261191", "anchor_text": "https://www.linkedin.com/in/manuel-brenner-772261191"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1fde95441432&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fants-and-the-problems-with-neural-networks-778caa73f77b&user=Manuel+Brenner&userId=1fde95441432&source=post_page-1fde95441432--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fd29a36fb919d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fants-and-the-problems-with-neural-networks-778caa73f77b&newsletterV3=1fde95441432&newsletterV3Id=d29a36fb919d&user=Manuel+Brenner&userId=1fde95441432&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}