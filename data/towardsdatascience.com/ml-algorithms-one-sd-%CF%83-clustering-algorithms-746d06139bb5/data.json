{"url": "https://towardsdatascience.com/ml-algorithms-one-sd-%CF%83-clustering-algorithms-746d06139bb5", "time": 1682995099.829196, "path": "towardsdatascience.com/ml-algorithms-one-sd-%CF%83-clustering-algorithms-746d06139bb5/", "webpage": {"metadata": {"title": "ML Algorithms: One SD (\u03c3)- Clustering Algorithms | by Sagi Shaier | Medium", "h1": "ML Algorithms: One SD (\u03c3)- Clustering Algorithms", "description": "The obvious questions to ask when facing a wide variety of machine learning algorithms, is \u201cwhich algorithm is better for a specific task, and which one should I use?\u201d This is one section of the many\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/ml-algorithms-one-sd-%CF%83-74bcb28fafb6", "anchor_text": "article", "paragraph_index": 2}, {"url": "https://github.com/shaier", "anchor_text": "Github", "paragraph_index": 60}, {"url": "https://scholar.google.com/citations?user=paO-O00AAAAJ&hl=en&oi=sra", "anchor_text": "scholar page", "paragraph_index": 60}, {"url": "https://shaier.github.io/", "anchor_text": "website", "paragraph_index": 60}], "all_paragraphs": ["The obvious questions to ask when facing a wide variety of machine learning algorithms, is \u201cwhich algorithm is better for a specific task, and which one should I use?\u201d", "Answering these questions vary depending on several factors, including: (1) The size, quality, and nature of data; (2) The available computational time; (3) The urgency of the task; and (4) What do you want to do with the data.", "This is one section of the many algorithms I wrote about in a previous article. In this part I tried to display and briefly explain the main algorithms (though not all of them) that are available for clustering tasks as simply as possible.", "The idea behind clustering tasks is to group a set of objects in a way that objects in the same group (called a cluster) are similar (in some way) to each other. Clustering is technique used in machine learning, image analysis, pattern recognition, and many other fields.", "This algorithm tries to partition N data points into K clusters where each data point is assigned to its closest cluster. For example, if you have 100 data points and choose K to be 2 (choosing 2 centroids), the algorithm will partition your dataset into 2 groups, where each group contains the points that are closest to the group\u2019s center (i.e. the centroid). This technique is defined by the objective function which tries to minimize the sum of all squared distances within a cluster, for all clusters.", "Meaning, finding the optimal center point for each group (closest one to all points in the group).", "An unsupervised algorithm- can be used with data that has not been labeled.", "K means can handle big data.", "The result of the algorithm is the cluster centers.", "This algorithm is one of several methods of hierarchical clustering. It\u2019s based on grouping clusters in bottom-up fashion. Meaning, we assign each of the observations to their own cluster. Then, we compute the similarity (e.g., distance) between each of the clusters and join the two most similar clusters. In single-linkage clustering, the similarity of two clusters is the similarity of their most similar members. Meaning, at each iteration of the algorithm two clusters are combined if the distance between the closest points is the lowest among all clusters combinations.", "At the end, you can see the results using a dendogram:", "K medians is simply a variation of K means clustering. Instead of calculating the mean for each cluster to determine its centroid, it simply calculates the median.", "k-medians minimizes absolute deviations, which equals Manhattan distance.", "If your distance is squared Euclidean distance, use k-means.", "If your distance is Taxicab metric, use k-medians.", "K-means is easily affected by outliers. K-medians is robust to outliers and results in compact clusters.", "Works similarly to K-means except that the data is assigned to each cluster with the weights being soft probabilities instead of distances. The advantage is that the model becomes generative as we define the probability distribution for each model. The more complex EM algorithm can find parameters even if the data have missing values. It works by first choosing random values for the missing data points, and using those guesses to estimate a second set of data. Then, the new values are used to create a better guess for the first set, and the process continues until the algorithm converges on a fixed point.", "Works best in situations where the data is incomplete (missing values).", "The EM algorithm can very slow.", "Works best when you only have a small percentage of missing data and the dimensionality of the data isn\u2019t too big.", "Does not partition the dataset into clusters in a single step. Instead it involves multiple steps which run from a single cluster containing all the data points to N clusters containing single data point. The result is a set of clusters, where each cluster is somewhat distinct from each other cluster, and the items within each cluster are similar to each other in a way.", "The main output of Hierarchical Clustering is a dendrogram", "After selecting which distance metric you want to use (e.g. Euclidean), you need to determine from where the distance is computed. For example, it can be computed between the two most similar parts of a cluster (single-linkage), the two least similar bits of a cluster (complete-linkage), or the center of the clusters (mean or average-linkage).", "Easier to decide on the number of clusters by looking at the dendrogram (in comparison to K means for example).", "Not suitable for large datasets for time complexity.", "Initial seeds have a strong impact on the final results.", "A form of clustering in which data points can belong to more than one cluster.", "The algorithm works by assigning a class to each data point corresponding to each cluster center based on the distance between the cluster center and the data point. The more the point is near the cluster center, the more its class towards the particular cluster center. This algorithm is frequently used in pattern recognition.", "At the end, you need to remember that choosing which clustering algorithm to use is solely dependent on the type of the data you have, and the purpose of the clustering applications. For example, K-Means algorithm is suitable for exclusive clustering task. While fuzzy clustering algorithm is suitable for overlapping clustering task. Sometimes we can\u2019t consider that the data belongs to only one cluster (some data properties contribute to more than one cluster). An example of this is document clustering. A particular document may be categorized into two different categories.", "It gives flexibility to the model as the data points can belong to more than one cluster.", "Long computational time is an issue.", "\u00b7 DBSCAN (Density-Based Spatial Clustering of Applications with Noise)", "Used to separate clusters of high density from clusters of low density. DBSCAN requires just two parameters: the minimum distance between two points and the minimum number of points to form a dense region. Meaning, it groups together points that are close to each other (usually Euclidean distance) and a minimum number of points.", "DBSCAN does not require one to specify the number of clusters in the data (as opposed to k-means).", "Does not assume clusters have a particular shape.", "Mostly insensitive to the ordering of the points in the database.", "DBSCAN can find arbitrarily shaped clusters.", "It can even find a cluster completely surrounded by (but not connected to) a different cluster.", "DBSCAN depends on the distance measure (which can be rendered almost useless because of the so-called \u201cCurse of dimensionality\u201d). Note that this is also true for any other algorithm based on Euclidean distance.", "\u00b7 OPTICS (Ordering Points to Identify Cluster Structure)", "OPTICS can be seen as a generalization of DBSCAN. The idea behind it is similar to DBSCAN, but it addresses one of DBSCAN\u2019s major weaknesses: the problem of detecting meaningful clusters in data of varying density. OPTICS is based on a very clever idea: instead of fixing min points and the radius, you only need to fix min points, and plot the radius at which an object would be considered dense by DBSCAN. However, OPTICS comes at a cost compared to DBSCAN: it is slower.", "OPTICS doesn\u2019t have an as well-defined concept of noise as DBSCAN.", "\u00b7 Non negative matrix factorization (NMF)", "A linear-algebraic model that factors high-dimensional vectors into a low-dimensionality representation. Similar to Principal component analysis (PCA), NMF takes advantage of the fact that the vectors are non-negative. By factoring them into the lower-dimensional form, NMF forces the coefficients to also be non-negative (in comparison to PCA where it creates factors that can be both positive and negative). Meaning, all features must be positive or zero value. NMF is used in tasks where the underlying factors can be interpreted as non-negative.", "Performs dimensionality reduction with nonnegative constraint.", "A type of probabilistic model and an algorithm used to discover the topics that are present in a corpus. For example, if observations are words collected into documents, to obtain the cluster assignments, it needs two probability values: P( word | topics), the probability of a word given topics. And P( topics | documents), the probability of topics given documents. These values are calculated based on an initial random assignment. Then, you iterate them for each word in each document, to decide their topic assignment. Basically, it\u2019s a way of automatically discovering topics that sentences contain.", "For example, suppose you have the following sentences:", "I like to eat bananas and apples.", "I ate a pomegranate and olives smoothie for breakfast.", "My girlfriend wants to adopt a kitten.", "Would you just look at this cute penguin chewing on this piece of apple?", "Topic A: 30% apple, 15% bananas, 10% breakfast, 10% eating (so now you could interpret topic A to be about food)", "Topic B: 20% armadillos, 20% kittens, 20% cute, 15% leopards (so you could interpret topic B to be about cute animals)", "LDA is a probabilistic model with interpretable topics.", "LDA gives you categories for free, in any data set.", "It can be hard to know when LDA is actually working since the topics are soft-clusters (there is no objective metric to say \u201cthis is the best choice\u201d of the hyperparameters).", "Its goal is to find a mixture of multi-dimensional Gaussian probability distributions that best model any input dataset. In general, mixture models don\u2019t require knowing which subpopulation a data point belongs to. They allow the model to learn the subpopulations automatically. It can be used for finding clusters in the same way that k-means does. The idea is quite simple, find the parameters of the Gaussians that best explain our data. We assume that the data is normal and we want to find parameters that maximize the likelihood of observing these data.", "For example, if we model human height we can notice that height typically follows a normal distribution for each gender. Now, given only the height data and not the gender assignments, the distribution of all heights would follow the sum of two scaled (different variance) and shifted (different mean) normal distributions. A model making this assumption is an example of a GMM.", "If you think your model is having some hidden, not observable parameters, then you should use GMM.", "GMM is a lot more flexible in terms of cluster covariance than k-means.", "If you\u2019re interested in more of my work you can check out my Github, my scholar page, or my website", "My sights are set on using the intersection of artificial intelligence and neuroscience to improve people\u2019s lives", "My sights are set on using the intersection of artificial intelligence and neuroscience to improve people\u2019s lives"], "all_outgoing_urls": [{"url": "https://towardsdatascience.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/@Shaier?source=post_page-----746d06139bb5--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/@Shaier?source=post_page-----746d06139bb5--------------------------------", "anchor_text": "Sagi Shaier"}, {"url": "https://towardsdatascience.com/ml-algorithms-one-sd-%CF%83-74bcb28fafb6", "anchor_text": "article"}, {"url": "https://github.com/shaier", "anchor_text": "Github"}, {"url": "https://scholar.google.com/citations?user=paO-O00AAAAJ&hl=en&oi=sra", "anchor_text": "scholar page"}, {"url": "https://shaier.github.io/", "anchor_text": "website"}, {"url": "https://towardsdatascience.com/tag/machine-learning?source=post_page-----746d06139bb5---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://towardsdatascience.com/tag/clustering?source=post_page-----746d06139bb5---------------clustering-----------------", "anchor_text": "Clustering"}, {"url": "https://towardsdatascience.com/tag/data-science?source=post_page-----746d06139bb5---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://towardsdatascience.com/tag/algorithms?source=post_page-----746d06139bb5---------------algorithms-----------------", "anchor_text": "Algorithms"}, {"url": "https://towardsdatascience.com/tag/pancakes?source=post_page-----746d06139bb5---------------pancakes-----------------", "anchor_text": "Pancakes"}, {"url": "https://towardsdatascience.com/@Shaier?source=post_page-----746d06139bb5--------------------------------", "anchor_text": "More from Sagi Shaier"}, {"url": "https://towardsdatascience.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F2fb0ad492bd7&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40Shaier%2Fml-algorithms-one-sd-%CF%83-clustering-algorithms-746d06139bb5&newsletterV3=81451e5c44f&newsletterV3Id=2fb0ad492bd7&user=Sagi+Shaier&userId=81451e5c44f&source=-----746d06139bb5---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----746d06139bb5--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----746d06139bb5--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----746d06139bb5--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----746d06139bb5--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----746d06139bb5--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----746d06139bb5--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----746d06139bb5--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/plans?source=upgrade_membership---two_column_layout_sidebar----------------------------------", "anchor_text": "Get unlimited access"}, {"url": "https://towardsdatascience.com/@Shaier?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/@Shaier?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Sagi Shaier"}, {"url": "https://towardsdatascience.com/@Shaier/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "234 Followers"}, {"url": "https://towardsdatascience.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F2fb0ad492bd7&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40Shaier%2Fml-algorithms-one-sd-%CF%83-clustering-algorithms-746d06139bb5&newsletterV3=81451e5c44f&newsletterV3Id=2fb0ad492bd7&user=Sagi+Shaier&userId=81451e5c44f&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://towardsdatascience.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}