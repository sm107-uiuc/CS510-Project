{"url": "https://towardsdatascience.com/tl-dr-dirichlet-process-gaussian-mixture-models-made-easy-12b4d492e5f9", "time": 1683003832.507871, "path": "towardsdatascience.com/tl-dr-dirichlet-process-gaussian-mixture-models-made-easy-12b4d492e5f9/", "webpage": {"metadata": {"title": "tl;dr: Dirichlet Process Gaussian Mixture Models made easy. | by Leon Chlon | Towards Data Science", "h1": "tl;dr: Dirichlet Process Gaussian Mixture Models made easy.", "description": "A generative model is one that gives us observations. We can use a Bernoulli distribution model to generate coin flip observations. We can use a Poisson distribution model to simulate radioactive\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/@alexissa122/generating-original-classical-music-with-an-lstm-neural-network-and-attention-abf03f9ddcb4", "anchor_text": "classical music", "paragraph_index": 1}, {"url": "https://en.wikipedia.org/wiki/K-means_clustering", "anchor_text": "K-Means", "paragraph_index": 3}, {"url": "http://blog.shakirm.com/2015/12/machine-learning-trick-of-the-day-6-tricks-with-sticks/", "anchor_text": "here", "paragraph_index": 10}, {"url": "https://towardsdatascience.com/mcmc-intuition-for-everyone-5ae79fff22b1", "anchor_text": "Markov Chain Monte Carlo sampling", "paragraph_index": 13}, {"url": "https://towardsdatascience.com/bayesian-inference-problem-mcmc-and-variational-inference-25a8aa9bce29", "anchor_text": "Variational Inference", "paragraph_index": 13}, {"url": "https://machinelearningmastery.com/expectation-maximization-em-algorithm/", "anchor_text": "Expectation Maximisation", "paragraph_index": 13}, {"url": "https://github.com/lchlon/medium/blob/master/DPGMM.ipynb", "anchor_text": "here", "paragraph_index": 14}, {"url": "https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence", "anchor_text": "Kullback\u2013Leibler divergence", "paragraph_index": 18}], "all_paragraphs": ["First and foremost: Clustering is the inverse of Generating.", "A generative model is one that gives us observations. We can use a Bernoulli distribution model to generate coin flip observations. We can use a Poisson distribution model to simulate radioactive decay. We can use a trained neural network to generate classical music. The more realistic the model, the closer these observations will align with reality.", "Below is a mixture of 400 samples generated from four independent bi-variate normal distributions with distinct means and equal standard deviations. The name for this model of mixed Gaussian distributions is, surprise surprise, a Gaussian Mixture Model.", "Using the K-Means algorithm and incredible Sherlockesque reasoning for the cluster number (the kernel density plot literally tells you there is 4 clusters), I was able to recover the generative model and colour-coded each cluster below. The recovery/clustering is clean because the data is squeaky clean. Each Gaussian contributed 100 samples, so we didn\u2019t have to worry about mixing probabilities. I chose four very distinct Gaussian means to sample from and a small standard deviation so I didn\u2019t have to worry about distribution overlap.", "In reality Nature will never be this kind to you, we cannot afford to ignore the uncertainty around our estimates of cluster number, mixing probabilities and the moments of each Gaussian in the mixture. So how do we scale our clustering approach with data complexity?", "Let\u2019s distinguish between a parametric and non-parametric model. A parametric model is one we can write down on a piece of paper, using a fixed number of parameters. y=mx + c has two fixed parameters (m,c) and is therefore parametric.", "A random forest classifier may take M parameters when fit to a dataset (y,X={x_{1},x_{2},\u2026,x_{n}}) but take K more if we introduce another feature x_{n+1}; As our data increases in complexity, so does the model. The number of parameters has no upper bound, meaning that for data with sufficient complexity, there isn\u2019t enough paper in the world to write down the full model. This is an example of a non-parametric model.", "Who cares right? Well it turns out that when we have limited prior belief over the cluster number or mixing probabilities, we can turn non-parametric and consider infinitely many of them. Naturally many of these clusters will be redundant, and have mixing probabilities so close to 0 that we can just ignore them. Incredibly, this framework lets the data determine the most likely number of clusters.", "So cool, how do we get started? First we need a way of describing a mixture of infinitely many distributions, and this is where Dirichlet Processes come in.", "A Dirichlet Process prior can be described using enough mathematical jargon to send one fleeing back to K-Means, so I\u2019ll the migraine and give an intuitive overview instead.", "We want to assign mixing probabilities \ud835\udf0b = {\ud835\udf0b_{1},\ud835\udf0b_{2},\u2026\ud835\udf0b_{n}\u2026} to an infinite number of clusters. We know that the sum of \ud835\udf0b must be 1 and that each \ud835\udf0b_{i} is greater than or equal to 0. If we imagine the probability between 0 and 1 as a stick, we are looking for ways to break that stick up such that it accurately reflects the mixing contributions of each Gaussian in our GMM. This approach to defining a Dirichlet Process prior is called the stick-breaking process, which itself has a Beta distribution prior. I highly recommend reading up on the statistical details here.", "Dirichlet Process Gaussian Mixture Models (DPGMMs)", "Now for the big reveal: since \ud835\udf0b tells us the relative contribution of each Gaussian in our GMM, it is effectively a distribution over distributions. Each \ud835\udf0b_{i} corresponds to a unique Gaussian N(\u03bc_{i}, \u03a3_{i}) parameterised by a mean \u03bc_{i} and covariance matrix \u03a3_{i}. If we let \u03b8_{i} =(\u03bc_{i}, \u03a3_{i}), our problem reduces to assigning a probability \ud835\udf0b_{i} to each \u03b8_{i}, reflecting its degree of contribution to the data mixture.", "Given some k-dimensional multivariate Gaussian data X, we start out with a prior belief that all p(\ud835\udf0b) are equally likely. We then want to use our data X to compute the likelihood p(X|\ud835\udf0b) of this new everything-is-likely construction. Finally, we update our model using Bayes theorem for the model posterior p(\ud835\udf0b|X) \u221d p(X|\ud835\udf0b)p(\ud835\udf0b). Typically p(\ud835\udf0b|X) involves some pretty nasty intractable integrals, so we rely on something like Markov Chain Monte Carlo sampling or Variational Inference to approximate p(\ud835\udf0b|X). Luckily, given the right choice of priors, we can use the Expectation Maximisation (EM) algorithm for inference instead! Lets see a real application of this in scikit-learn.", "Firstly, we generate some really messy toy GMM dataset, rich with random underlying correlation structures. Using a 20-dimensional, 8 component GMM, 10,000 samples were drawn according to a weight schema initialised from a Dirichlet distribution with a uniform prior (link to code here).", "A Kernel Density estimation across the first two features tells us little about the potential number of clusters in the data, but does reveal striking heterogeneity.", "A scatterplot of the clusters across the first two PCA components of the data shows us how difficult recovering the general model is likely to be:", "Fitting a DPGMM truncated to 15 components using the EM algorithm, we see that most of the probability mass is concentrated around the first 8 components:", "In fact, if we use a metric like the Kullback\u2013Leibler divergence, we can compute an asymmetric measure of the distance between our estimated parameters and the ground truth parameters in our dataset:", "Each estimated DPGMM cluster maps its own unique ground truth cluster with a remarkably tiny KL-divergence relative to the remaining ground truth clusters it is compared against. With this kind of matchup, it\u2019s no surprise that the DPGMM recovers the clustering almost perfectly, whilst a K-Means classifier with the cluster number chosen according to the largest average silhouette score struggles to recover the actual number of clusters.", "Please check out the code linked above to try and implement this yourself and on your own data. Good luck and happy clustering!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Research Data Scientist \u2014 Facebook; Past: McKinsey Analytics Consultant | Harvard Medical School Postdoc | University of Cambridge PhD, MPhil"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F12b4d492e5f9&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftl-dr-dirichlet-process-gaussian-mixture-models-made-easy-12b4d492e5f9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftl-dr-dirichlet-process-gaussian-mixture-models-made-easy-12b4d492e5f9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftl-dr-dirichlet-process-gaussian-mixture-models-made-easy-12b4d492e5f9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftl-dr-dirichlet-process-gaussian-mixture-models-made-easy-12b4d492e5f9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----12b4d492e5f9--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----12b4d492e5f9--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://chleon.medium.com/?source=post_page-----12b4d492e5f9--------------------------------", "anchor_text": ""}, {"url": "https://chleon.medium.com/?source=post_page-----12b4d492e5f9--------------------------------", "anchor_text": "Leon Chlon"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fff2d21af7d63&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftl-dr-dirichlet-process-gaussian-mixture-models-made-easy-12b4d492e5f9&user=Leon+Chlon&userId=ff2d21af7d63&source=post_page-ff2d21af7d63----12b4d492e5f9---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F12b4d492e5f9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftl-dr-dirichlet-process-gaussian-mixture-models-made-easy-12b4d492e5f9&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F12b4d492e5f9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftl-dr-dirichlet-process-gaussian-mixture-models-made-easy-12b4d492e5f9&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/@alexissa122/generating-original-classical-music-with-an-lstm-neural-network-and-attention-abf03f9ddcb4", "anchor_text": "classical music"}, {"url": "https://en.wikipedia.org/wiki/K-means_clustering", "anchor_text": "K-Means"}, {"url": "http://blog.shakirm.com/2015/12/machine-learning-trick-of-the-day-6-tricks-with-sticks/", "anchor_text": "here"}, {"url": "http://blog.shakirm.com/2015/12/machine-learning-trick-of-the-day-6-tricks-with-sticks/", "anchor_text": "http://blog.shakirm.com/2015/12/machine-learning-trick-of-the-day-6-tricks-with-sticks/"}, {"url": "https://towardsdatascience.com/mcmc-intuition-for-everyone-5ae79fff22b1", "anchor_text": "Markov Chain Monte Carlo sampling"}, {"url": "https://towardsdatascience.com/bayesian-inference-problem-mcmc-and-variational-inference-25a8aa9bce29", "anchor_text": "Variational Inference"}, {"url": "https://machinelearningmastery.com/expectation-maximization-em-algorithm/", "anchor_text": "Expectation Maximisation"}, {"url": "https://github.com/lchlon/medium/blob/master/DPGMM.ipynb", "anchor_text": "here"}, {"url": "https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence", "anchor_text": "Kullback\u2013Leibler divergence"}, {"url": "https://medium.com/tag/clustering?source=post_page-----12b4d492e5f9---------------clustering-----------------", "anchor_text": "Clustering"}, {"url": "https://medium.com/tag/towards?source=post_page-----12b4d492e5f9---------------towards-----------------", "anchor_text": "Towards"}, {"url": "https://medium.com/tag/bayesian-statistics?source=post_page-----12b4d492e5f9---------------bayesian_statistics-----------------", "anchor_text": "Bayesian Statistics"}, {"url": "https://medium.com/tag/sklearn?source=post_page-----12b4d492e5f9---------------sklearn-----------------", "anchor_text": "Sklearn"}, {"url": "https://medium.com/tag/dirichlet-process?source=post_page-----12b4d492e5f9---------------dirichlet_process-----------------", "anchor_text": "Dirichlet Process"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F12b4d492e5f9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftl-dr-dirichlet-process-gaussian-mixture-models-made-easy-12b4d492e5f9&user=Leon+Chlon&userId=ff2d21af7d63&source=-----12b4d492e5f9---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F12b4d492e5f9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftl-dr-dirichlet-process-gaussian-mixture-models-made-easy-12b4d492e5f9&user=Leon+Chlon&userId=ff2d21af7d63&source=-----12b4d492e5f9---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F12b4d492e5f9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftl-dr-dirichlet-process-gaussian-mixture-models-made-easy-12b4d492e5f9&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----12b4d492e5f9--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F12b4d492e5f9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftl-dr-dirichlet-process-gaussian-mixture-models-made-easy-12b4d492e5f9&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----12b4d492e5f9---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----12b4d492e5f9--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----12b4d492e5f9--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----12b4d492e5f9--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----12b4d492e5f9--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----12b4d492e5f9--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----12b4d492e5f9--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----12b4d492e5f9--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----12b4d492e5f9--------------------------------", "anchor_text": ""}, {"url": "https://chleon.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://chleon.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Leon Chlon"}, {"url": "https://chleon.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "187 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fff2d21af7d63&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftl-dr-dirichlet-process-gaussian-mixture-models-made-easy-12b4d492e5f9&user=Leon+Chlon&userId=ff2d21af7d63&source=post_page-ff2d21af7d63--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F25471836b4f1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftl-dr-dirichlet-process-gaussian-mixture-models-made-easy-12b4d492e5f9&newsletterV3=ff2d21af7d63&newsletterV3Id=25471836b4f1&user=Leon+Chlon&userId=ff2d21af7d63&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}