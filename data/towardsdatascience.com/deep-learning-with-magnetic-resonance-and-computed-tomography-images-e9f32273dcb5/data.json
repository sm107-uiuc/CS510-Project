{"url": "https://towardsdatascience.com/deep-learning-with-magnetic-resonance-and-computed-tomography-images-e9f32273dcb5", "time": 1682994500.968721, "path": "towardsdatascience.com/deep-learning-with-magnetic-resonance-and-computed-tomography-images-e9f32273dcb5/", "webpage": {"metadata": {"title": "Deep Learning with Magnetic Resonance and Computed Tomography Images | by Jacob Reinhold | Towards Data Science", "h1": "Deep Learning with Magnetic Resonance and Computed Tomography Images", "description": "Getting started with applying deep learning to magnetic resonance (MR) or computed tomography (CT) images is not straightforward; finding appropriate data sets, preprocessing the data, and creating\u2026"}, "outgoing_paragraph_urls": [{"url": "https://github.com/fastai/fastai", "anchor_text": "fastai", "paragraph_index": 0}, {"url": "https://www.mriquestions.com/index.html", "anchor_text": "this website,", "paragraph_index": 3}, {"url": "http://mriquestions.com/hellippulse-sequences.html", "anchor_text": "pulse sequences", "paragraph_index": 3}, {"url": "https://openneuro.org", "anchor_text": "OpenNeuro", "paragraph_index": 9}, {"url": "https://grand-challenge.org/challenges/", "anchor_text": "here", "paragraph_index": 9}, {"url": "http://academictorrents.com/", "anchor_text": "this website", "paragraph_index": 10}, {"url": "http://nipy.org/nibabel/", "anchor_text": "nibabel", "paragraph_index": 12}, {"url": "http://nipy.org/nibabel/dicom/dcm2nii_algorithms.html", "anchor_text": "Here is a tool", "paragraph_index": 13}, {"url": "https://gist.github.com/jcreinhold/a26d6555b0e7aa28b79757f766640dd6", "anchor_text": "here is a script", "paragraph_index": 13}, {"url": "https://gist.github.com/jcreinhold/fdd701211191450284c5718502eabbd4", "anchor_text": "here is a script to convert PAR/REC", "paragraph_index": 13}, {"url": "http://johnmuschelli.com/imaging_in_r/inhomogeneity_correction_ms/index.pdf", "anchor_text": "inhomogeneous image intensities due to the scanner", "paragraph_index": 17}, {"url": "https://en.wikipedia.org/wiki/Hounsfield_scale", "anchor_text": "Hounsfield units", "paragraph_index": 18}, {"url": "https://arxiv.org/pdf/1812.04652.pdf", "anchor_text": "quite large", "paragraph_index": 18}, {"url": "https://github.com/jcreinhold/intensity-normalization", "anchor_text": "this repository", "paragraph_index": 19}, {"url": "https://en.wikipedia.org/wiki/Feature_scaling#Standardization", "anchor_text": "standardization", "paragraph_index": 19}, {"url": "https://github.com/Jfortin1/RAVEL", "anchor_text": "RAVEL", "paragraph_index": 19}, {"url": "https://www.ncbi.nlm.nih.gov/pubmed/10571928", "anchor_text": "Ny\u00fal & Udupa method", "paragraph_index": 19}, {"url": "https://intensity-normalization.readthedocs.io/en/latest/algorithm.html#z-score", "anchor_text": "z-score normalization", "paragraph_index": 19}, {"url": "https://www.nitrc.org/projects/robex", "anchor_text": "ROBEX", "paragraph_index": 20}, {"url": "https://mipav.cit.nih.gov/", "anchor_text": "MIPAV", "paragraph_index": 21}, {"url": "https://horosproject.org/", "anchor_text": "Horos", "paragraph_index": 21}, {"url": "https://gist.github.com/jcreinhold/01daf54a6002de7bd8d58bad78b4022b", "anchor_text": "command-line script", "paragraph_index": 23}, {"url": "https://github.com/jcreinhold/niftidataset", "anchor_text": "here", "paragraph_index": 25}, {"url": "https://pytorch.org/", "anchor_text": "pytorch", "paragraph_index": 26}, {"url": "https://github.com/fastai/fastai", "anchor_text": "fastai", "paragraph_index": 26}, {"url": "https://nbviewer.jupyter.org/gist/jcreinhold/78943cdeca1c5fca4a5af5d066bd8a8d", "anchor_text": "here", "paragraph_index": 26}, {"url": "https://arxiv.org/abs/1512.03385", "anchor_text": "resnet", "paragraph_index": 27}, {"url": "https://www.nitrc.org/projects/multimodal/", "anchor_text": "Kirby 21", "paragraph_index": 28}, {"url": "https://www.ncbi.nlm.nih.gov/pubmed/20378467", "anchor_text": "N4", "paragraph_index": 28}, {"url": "https://github.com/jcreinhold/intensity-normalization/blob/master/intensity_normalization/utilities/preprocess.py", "anchor_text": "here", "paragraph_index": 28}, {"url": "https://github.com/jcreinhold/intensity-normalization/blob/master/intensity_normalization/exec/coregister.py", "anchor_text": "here", "paragraph_index": 28}, {"url": "https://github.com/jcreinhold/intensity-normalization", "anchor_text": "intensity-normalization", "paragraph_index": 28}, {"url": "https://github.com/jcreinhold/intensity-normalization/blob/master/intensity_normalization/normalize/zscore.py", "anchor_text": "z-score normalized", "paragraph_index": 28}, {"url": "https://docs.fast.ai/data_block.html", "anchor_text": "data_block", "paragraph_index": 29}, {"url": "https://docs.fast.ai/tutorial.itemlist.html", "anchor_text": "ItemList tutorial", "paragraph_index": 30}, {"url": "https://en.wikipedia.org/wiki/Anatomical_plane", "anchor_text": "axial plane", "paragraph_index": 31}, {"url": "https://nbviewer.jupyter.org/gist/jcreinhold/78943cdeca1c5fca4a5af5d066bd8a8d", "anchor_text": "notebook", "paragraph_index": 32}, {"url": "https://docs.fast.ai/data_block.html", "anchor_text": "here", "paragraph_index": 32}, {"url": "https://github.com/fastai/fastai/blob/master/fastai/layers.py#L94", "anchor_text": "fastai repository", "paragraph_index": 33}, {"url": "https://arxiv.org/abs/1506.01186", "anchor_text": "here", "paragraph_index": 35}, {"url": "https://sgugger.github.io/the-1cycle-policy.html", "anchor_text": "one-cycle", "paragraph_index": 35}, {"url": "https://arxiv.org/pdf/1803.09820.pdf", "anchor_text": "policy", "paragraph_index": 35}, {"url": "http://www.upstate.edu/radiology/education/rsna/intro/display.php", "anchor_text": "window/level settings", "paragraph_index": 37}, {"url": "http://www.niftynet.io/", "anchor_text": "NiftyNet", "paragraph_index": 40}, {"url": "https://github.com/perone/medicaltorch", "anchor_text": "medicaltorch", "paragraph_index": 40}, {"url": "https://neuroconductor.org/", "anchor_text": "neuroconductor", "paragraph_index": 41}, {"url": "https://github.com/JuliaIO/NIfTI.jl", "anchor_text": "NIfTI.jl", "paragraph_index": 41}, {"url": "https://fluxml.ai/", "anchor_text": "Flux.jl", "paragraph_index": 41}, {"url": "https://pytorch.org/docs/master/hub.html", "anchor_text": "share the network weights", "paragraph_index": 42}, {"url": "http://jcreinhold.com", "anchor_text": "jcreinhold.com", "paragraph_index": 43}], "all_paragraphs": ["Getting started with applying deep learning to magnetic resonance (MR) or computed tomography (CT) images is not straightforward; finding appropriate data sets, preprocessing the data, and creating the data loader structures necessary to do the work is a pain to figure out. In this post I hope to alleviate some of that pain for newcomers. To do so, I\u2019ll link to several freely-available datasets, review some common/necessary preprocessing techniques specific to MR and CT, and show how to use the fastai library to load (structural) MR images and train a deep neural network for a synthesis task.", "Before we get into the meat and bones of this post, it will be useful to do a quick overview of the medical images that we\u2019ll be talking about and some idiosyncrasies of the types of images in discussion. I\u2019ll only be talking about structural MR images and (to a lesser degree) computed tomography (CT) images. Both of these types of imaging modalities are used to view the structure of the tissue; this is opposed to functional MR images (fMRI) or positron emission tomography (PET) scans which image blood flow activity and metabolic activity, respectively.", "For people not acquainted with medical images at all, note that medical image statistics are different from natural image statistics. For example, a mammography image looks nothing like any picture that a human would take with their smart phone. This is obvious of course; however, I think it is important to have this in mind when designing networks and working with the data to make some sort of machine learning (ML) algorithm. That is not to say that using common networks or transfer learning from domains outside medical imaging won\u2019t work; it is only to say that knowing the characteristics of common issues regarding medical imaging will help you debug your algorithm. I\u2019ll discuss specific examples of these characteristics in the preprocessing section below and show ways to reduce the impact of some of these unique problems.", "I\u2019m not going to go into much detail about the intricacies of structural MR imaging. A good place to start for more in-depth details of MR is this website, which goes into depth regarding any topic that an ML practitioner working with MR would care about. I\u2019ll note that there are many different types of MR images that an MR scanner can produce. For instance, there are T1-weighted, T2-weighted, PD-weighted, FLuid-Attenuated Inversion Recovery (FLAIR), among others. To make things more complicated, there are sub-types of those types of images (e.g., T1-weighted images come in the flavors: MPRAGE, SPGR, etc.). Depending on your task, this information may be extremely useful because of the unique characteristics of each of these types and sub-types of images. The reason for all these different types of images is because MR scanners are flexible machines that can be programmed to collect different information according to different pulse sequences. The upshot is that all of these images are not just redundant information; they contain useful and unique information regarding clinical markers that radiologists (or us as image processors) care about. Again, I\u2019ll discuss more details regarding unique aspects of MR in the preprocessing section.", "While there is contrast and non-contrast CT, there are not as many varieties of images that can be created with a CT scanner. Vaguely, the CT scanner shoots high-energy photons through you whose energy is calculated via a detector on the other side of your body which the photons hit. When images like this are taken from a variety of angles, we can use our knowledge of the geometry at which the images were acquired to reconstruct the image into a 3D volume. The physical representation of the energy lets us map the found intensity values to a standard scale which also simplifies our life and is discussed more in the preprocessing section. I should note that while MR is good at soft-tissue contrast (e.g., the ability to discern between gray-matter and white-matter in the brain), CT has somewhat poor soft-tissue contrast. See the below head scans from an MR image and a CT image as an example, noting the contrast between the grey-matter (along the outside of the brain) and white-matter (the brighter tissue interior to the grey-matter) as well as the general noise level present in the brain for both images.", "Some of the reasons that MR scans are not always used are: 1) some people can\u2019t due to a variety of reasons (e.g., no access, certain types of metal implants, etc.), 2) MR scans take a relatively long time compared to CT scans and 3) radiologists are interested in the particular measurements that CT can provide (e.g., looking at bone structure). Now that we have a basic understanding of the data and some of the intricacies of the imaging modalities, let\u2019s discuss some datasets.", "Labeled data is somewhat sparse for medical images because radiologists are expensive, hospitals are concerned about lawsuits, and researchers are (often overly) protective of their data. As a result, there is not an ImageNet-equivalent in MR or CT. However, there are many commonly used datasets depending on the application domain. Since I mostly work with brain MR images, I\u2019ll supply a small list of easily accessible datasets for MR and CT (brain) images along with the data format in parenthesis at the end of the bullet:", "Here is a list of not so easy to download (but very useful) datasets.", "I have not worked with the set of datasets below, but I know people who have and am including them for completeness.", "Another place to look for datasets is in OpenNeuro which is a repository for researchers to host their brain imaging datasets; it mostly consists of fMRI from what I can tell. If your passion lies somewhere besides MR and CT brain images, then I\u2019m unfortunately not a great resource. My first guess would be to look at the \u201cgrand challenges\u201d listed here and see if it is possible to gain access to the data.", "Not to bury the lede too much, but perhaps the easiest way to get access to some of the above data is through this website. I\u2019m not sure that everything is sanctioned to be on there, which is why I have delayed to bring this up.", "The amount of data wrangling and preprocessing required to work with MR and CT can be considerable. I\u2019ll outline the bare necessities below.", "The first thing to consider is how to load the images into python. The simplest route is to use nibabel. Then you can simply use", "to get a numpy array containing the data inside the mydata.nii.gz file. Note that I\u2019ll refer to the indices of this 3D volume as a voxel which is the 3D-equivalent of a pixel for a 2D image. For work with brain images at least, I\u2019d recommend always converting the files to NIfTI (which corresponds to the .nii or .nii.gz extension). I find converting everything to NIfTI first makes my life easier since I can assume all input images are of type NIfTI. Here is a tool to convert DICOM to NIfTI, here is a script to convert MHA to NIfTI and here is a script to convert PAR/REC files to NIfTI. There are more file formats that you\u2019ll probably need to work with, and you can use some of those scripts as inspiration to convert those file types.", "We\u2019ll first outline resampling, bias-field correction and registration which are staples of any medical image analysis. For these preprocessing steps, I\u2019d recommend ANTs and specifically the ANTsPy variety (assuming you are coming from a python background). ANTs is actively maintained and has reliable tools to solve all of these (and many more) problems. Unfortunately, ANTsPy is not always easy to install, but I believe work is being done on it to solve some of the issues and once you are up and running with it you can access most of the tools ANTs offers natively from python. In particular, it supports the resampling, bias-field correction and registration preprocessing steps I\u2019ll be discussing next.", "As with natural images, MR and CT images do not have a standard resolution or standard image size. I\u2019d argue that this fact is of greater importance in MR and CT though and must be considered for optimal ML performance. Consider the following: you train a 3D convolutional neural network with data acquired at 1x1x3 mm\u00b3 resolution and then you input an image into the network with 1x1x1 mm\u00b3. I would expect the result to be sub-optimal since the convolutional kernels will not be using the same spatial information. This is debatable and I haven\u2019t examined the problem closely, but the non-standard resolution is something to keep in mind if you run into problems at test time. We can naively address the non-standard resolution problem by resampling the image to a desired, standard resolution (with cubic B-splines, of course, for the best quality).", "For many applications, both MR and CT often require a process called registration in order to align objects across a set of images for direct comparison. Why would we want to do this? Let\u2019s say you want to learn a function that takes an MR image and outputs an estimate of what the CT image would look like. If you have paired data (that is, an MR and CT image from the same patient), then a simple way of approaching this problem would be to learn the voxel-wise map between the image intensities. However, if the anatomy is not aligned in the image space, then we cannot learn this map in a supervised way. We solve this problem by registering the images and, in fact, we examine this problem in the experiment section.", "The next two problems (described in the next two paragraphs) are specific to MR. First is that we have inhomogeneous image intensities due to the scanner in MR images. Since this inhomogeneity is not a biological feature, we generally want to remove it and we do so with a process referred to as bias-field correction (I\u2019ll discuss one solution in the experiment section).", "Another issue in MR are inconsistent tissue intensities across different MR scanners. While CT images have a standard intensity scale (see Hounsfield units), we are not so lucky with MR images. MR images absolutely do not have a standard scale, and the impact on algorithm performance can be quite large if not accounted for in preprocessing. See the images below for an example where we plot the histograms of a set of T1-weighted MR images without any intensity normalization applied (see the image with \u201cRaw\u201d in the title). This variation is due to effects caused by the scanner and not due to the biology, which is the thing we generally care about.", "There are a litany of intensity normalization techniques that attempt to remove this scanner variation (several of which I have collected in this repository called intensity-normalization). The techniques range from the very simple (e.g., simple standardization which I\u2019ll refer to as z-score normalization) to the fairly technical (e.g., RAVEL). For neuroimaging, a good combination of speed and quality can be found in the Fuzzy C-Means (FCM) normalization technique which creates a rough tissue-class segmentation between the white-matter (WM), grey-matter and cerebrospinal fluid based on the T1-weighted image. The WM segmentation mask is then used to calculate the mean of the WM in the image which is set to some user-defined constant. This normalization technique seems to almost always produce the desired result in brain images. If you are not working on brain images, then you may want to look at either the Ny\u00fal & Udupa method or simple z-score normalization. All of these normalization methods are available as command-line interfaces (or importable modules) in the intensity-normalization repository.", "The last preprocessing step we\u2019ll consider is specific to brain images. In brain images, we generally only care about the brain and not necessarily the tissues outside of brain (e.g., the skull, fat and skin surrounding the brain). Furthermore, this extraneous tissue can complicate the learning procedure and trip up classification, segmentation, or regression tasks. To get around this we can use skull-stripping algorithms to create a mask of the brain and zero out the background. The simplest way to go about this (in MR) \u2014 with reasonable results \u2014 is with ROBEX: a command-line tool that generally does a good job at extracting the brain from the image. I\u2019ve seen it fail a few times on some data containing large pathologies or imaging artifacts, but other than that it is usually good enough for most machine learning tasks. For what it\u2019s worth, I\u2019d try to avoid skull-stripping your data since it is just another point of possible failure in your preprocessing routine, but sometimes it substantially helps.", "Since MR and CT images aren\u2019t standard like JPEG, your computer doesn\u2019t have a native way to display it. If you want to visualize your data, take a look at MIPAV for non-DICOM images (e.g., NIfTI) and Horos for DICOM images. It is always good to look at your data, especially after preprocessing so we can verify that everything looks reasonable. For instance, perhaps the registration failed (it often does) or perhaps the skull-stripping failed (again, it often occurs). If you pipe your crappy data into your ML algorithm, you\u2019re probably going to get crappy output and you\u2019ll waste a lot of time doing unnecessary debugging. So be kind to yourself and examine the data.", "While deep neural networks applied to MR and CT are increasingly moving to 3D models, there has been good success with 2D models. If you have limited memory on your GPU or you have very limited training data, you may want to use a 2D network to squeeze the most performance out of the network. If you use a 3D network, you will quickly run into memory issues when passing a full image or patches through the network.", "If you decide a 2D network is the way to go for your application (a reasonable choice), you\u2019ll need to figure out/design a data loader to handle this. After fussing around with complicated data loaders that take the 3D image to a 2D image patch or slice for a while, I realized that that was all an unnecessary burden that made it harder to use pre-built data loader/data augmentation tools that aid in training. Thus my recommended solution to this problem is to simply convert the 3D volumes to 2D images. Since the original volumes are floating point numbers, I went with the TIFF image format which supports such types. Here is a command-line script which takes a directory of NIfTI images and creates a directory of corresponding 2D TIFF images (with some options to create slices based on axis and to only create slices from a portion of the image in order to avoid background slices).", "In the following section, I\u2019ll build a deep neural network with 3D convolutional layers. I\u2019m doing this as opposed to using 2D convolutional layers because \u2014 once you convert the 3D volume to 2D images like TIFF \u2014 you can basically just use any 2D architecture you have lying around substituting the head for the appropriate application. Since the 3D problem is slightly more tricky to approach, I\u2019ll dig into it below.", "*** If you are just coming to this blog post (after 05/07/20), note that the fastai package has changed significantly and the code below may not work as expected. However, the code examples and general experimental setup below should still be useful for learning purposes. For what it\u2019s worth, I\u2019d recommend using PyTorch over fastai for future deep learning projects. If you want NIfTI support in PyTorch, I have an actively maintained package which has working code examples and importable functions here. ***", "In this section, I\u2019ll outline the steps required to train a 3D convolutional neural network for a MR-to-MR synthesis task using pytorch and fastai. If you just want to look at the code, then there is also a notebook which contains most of the experiment (excluding preprocessing) here.", "The setup is as follows: we\u2019ll train a very small resnet to take an entire 3D volume from one MR contrast to another MR contrast; we\u2019ll be learning the transform to map T1-weighted images to FLAIR images. This task is called MR image synthesis and we\u2019ll refer to the network as a synthesis network. There are a variety of applications for this type of synthesis, but motivation for this problem is mostly that: MR scan time is limited, so not all contrasts can be collected. But we want to eat our cake and have it too, and we sometimes want those uncollected contrasts for image processing purposes. Thus we create some fake data using the data that actually was collected, where the fake data will be the result of our synthesis network.", "In this experiment, I\u2019ll be using 11 and 7 images as training and validation, respectively, from the Kirby 21 dataset. All images have been resampled to 1x1x1 mm\u00b3, bias-field corrected using N4, and the FLAIR images have been (affine) registered to the T1-weighted images using ANTsPy. Look here and here for the actual code I used to do the preprocessing (both are available as command-line interfaces when the intensity-normalization package is installed along with ANTsPy). Finally, all the images were individually z-score normalized using the entire image.", "Now that we have motivated the problem somewhat and talked about the data we will use, let\u2019s get to the code. The code block below defines some necessary constructs to work with fastai, specifically to use the data_block API.", "There is nothing particular to remark on here, except that once you figure out how to setup these types of structures, they are quite convenient (see the ItemList tutorial for more details). Note that not all functionality is supported with the current setup \u2014 I stripped it down to make it as simple as possible \u2014 but it\u2019ll get the job done. I\u2019ll show how this creates the training and validation dataloaders below. First, let\u2019s define a preprocessing transform:", "Why am I defining this odd cropping function? The reason is two-fold. The first reason is that the neck is not present in the FLAIR images but is present in the T1-weighted images. I don\u2019t want the network to learn to take tissue to zero, so I\u2019m removing that part of the data by only using the data in the 20\u201380 percent range along the axis corresponding to the axial plane. The second reason is that I can fit twice as many samples into a batch (that means a batch size of 2). The reason for the small batch size is that, like I previously mentioned, 3D networks with large images are memory-intensive. Why no other data augmentation? Unfortunately, 3D transforms are not natively supported with pytorch or fastai so I\u2019d have to incorporate my own and I am not doing this for simplicity. Now let\u2019s use the data_block API of fastai to create the training and validation dataloaders:", "You can see the notebook for more details, but essentially I have the T1-weighted images in one directory with train, valid, test subdirectories and a parallel directory with the FLAIR images. The get_y_fn function grabs the FLAIR image corresponding to the source T1-weighted image. Look here for more in-depth explanation of the remaining commands. Note that the (tfms,tfms) means that I am applying the previously defined crop to both the training and validation set. Applying that transform to the validation set isn\u2019t ideal, but is required because of memory-constraints. Now let\u2019s create some 3D convolutional and residual block layers which we\u2019ll use to define our model:", "I\u2019m closely following the definition of the 2D convolutional and residual block layers as defined in the fastai repository. As a side note, I left the spectral normalization and weight normalization routines in the conv3d definition, but disappointingly received worse results with those methods than when using batch norm (and I\u2019m still not sure whether batch norm is applied before or after the activation). Now let\u2019s define our model using the above layers:", "Here I have just defined the very small resnet model. Why so few layers? I am using as large of a network as my GPU can contain in memory. The creation of many channels with the entire 3D volume and the residual connections are burdens on the GPU memory. The only other thing of possible intrigue is that I use a 1x1x1 kernel at the end, which empirically produces crisper images (and I think is fairly standard). As a note, I realize that I should have removed the activation from the final layer; however, it is not a problem because I am z-score normalizing (i.e., mean subtracting and dividing by the standard deviation) the images with their backgrounds. The backgrounds, which are approximately zero, take up the vast majority of the volume of the image. Thus the z-score normalization essentially puts the background (corresponding to the mean) at zero, which makes the intensities of the head greater than zero. A fine result for the ReLU. Now let\u2019s train this network:", "Again fairly normal. Mean square error is used because we want each voxel intensity in our source T1-weighted image to match the voxel intensity of the corresponding target FLAIR image. We use lr_find to help us pick a larger learning rate (as described here) for faster training, in addition to using the one-cycle policy. I always collect my training and validation data into a CSV file to look at how the network is converging, especially on machines where launching a jupyter notebook is a pain. I picked 100 epochs because I ran this a couple times and did not notice a great amount performance gain with more epochs.", "After training completes, we input an entire image (not seen in either training or validation) into the network. An example is shown in below figure, where the synthesis result is the right-most image (with the title \u201cSyn\u201d).", "While the above figure could have been had better window/level settings for better comparison, we see that the T1-weighted image does take on many of the characteristics of the true FLAIR image. Most notably, inside the brain, we see that the white-matter becomes less bright than the grey-matter while the cerebrospinal fluid remains dark. The noise characteristics are not the same though and there are some bright spots in the true FLAIR that are not captured in the synthesized image.", "This result is not state-of-the-art by any means, but it\u2019s interesting to see that we can learn an approximate transform with such an incredibly small dataset, no data augmentation, and a very small network. This network would assuredly be better with more data, data augmentation and a larger network, but this is just a simple, pedagogical toy example. I should note that unless you have a particularly large GPU (and contradicting my last statement), you may not be able to train this network with the full images. You\u2019ll probably have to use either 3D patches or 2D slices (or 2D patches).", "Hopefully this post provided you with a starting point for applying deep learning to MR and CT images with fastai. Like most machine learning tasks, there is a considerable amount of domain-specific knowledge, data-wrangling and preprocessing that is required to get started, but once you have this under your belt, it is fairly easy to get up-and-running with training a network with pytorch and fastai. Where to go from here? I\u2019d download a dataset from one of the links I posted in the Datasets section and try to do something similar to what I showed above, or even just try to recreate what I did. If you can get to that stage, you\u2019ll be in a comfortable place to apply deep learning to other problems in MR and CT.", "I should note that there is work being done to create standard code bases from which you can apply deep learning to MR and CT images. The two that I am aware of are NiftyNet and medicaltorch. NiftyNet abstracts away most of the neural network design and data handling, so that the user only has to call some command-line interfaces by which to download a pre-trained network, fine-tune it, and do whatever. So if that is good enough for your needs, then go right ahead; it seems like a great tool and has some pre-trained networks available. medicaltorch provides some dataloaders and generic deep learning models with medical images in pytorch. I have not tested either extensively, so I cannot comment on their utility.", "If you don\u2019t like python, there is neuroconductor in R or NIfTI.jl and Flux.jl packages in Julia which can read NIfTI images and build neural networks, respectively. There are countless other relevant software packages, but those are the ones the first come to mind and that I\u2019ve worked with.", "As a final note, if you have luck creating a nice application for MR or CT make sure to share your work! Write a paper/blog post, put it up on a forum, share the network weights. It would be great to see more people apply deep learning techniques to this domain and push the boundary of the field where possible. Best of luck.", "machine learning | medical image analysis | jcreinhold.com | @JacobCReinhold"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fe9f32273dcb5&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-with-magnetic-resonance-and-computed-tomography-images-e9f32273dcb5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-with-magnetic-resonance-and-computed-tomography-images-e9f32273dcb5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-with-magnetic-resonance-and-computed-tomography-images-e9f32273dcb5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-with-magnetic-resonance-and-computed-tomography-images-e9f32273dcb5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/@jcreinhold?source=post_page-----e9f32273dcb5--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e9f32273dcb5--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@jcreinhold?source=post_page-----e9f32273dcb5--------------------------------", "anchor_text": "Jacob Reinhold"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd6cf1d0b0aa7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-with-magnetic-resonance-and-computed-tomography-images-e9f32273dcb5&user=Jacob+Reinhold&userId=d6cf1d0b0aa7&source=post_page-d6cf1d0b0aa7----e9f32273dcb5---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----e9f32273dcb5--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe9f32273dcb5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-with-magnetic-resonance-and-computed-tomography-images-e9f32273dcb5&user=Jacob+Reinhold&userId=d6cf1d0b0aa7&source=-----e9f32273dcb5---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe9f32273dcb5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-with-magnetic-resonance-and-computed-tomography-images-e9f32273dcb5&source=-----e9f32273dcb5---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://github.com/fastai/fastai", "anchor_text": "fastai"}, {"url": "https://www.mriquestions.com/index.html", "anchor_text": "this website,"}, {"url": "http://mriquestions.com/hellippulse-sequences.html", "anchor_text": "pulse sequences"}, {"url": "http://headctstudy.qure.ai/dataset", "anchor_text": "Qure.ai"}, {"url": "https://www.nitrc.org/projects/multimodal/", "anchor_text": "Kirby 21"}, {"url": "http://brainweb.bic.mni.mcgill.ca/brainweb/", "anchor_text": "Brainweb"}, {"url": "https://www.nitrc.org/projects/multimodal/", "anchor_text": "Kirby 21"}, {"url": "https://brain-development.org/ixi-dataset/", "anchor_text": "IXI dataset"}, {"url": "http://headctstudy.qure.ai/dataset", "anchor_text": "Qure.ai CT head scan data"}, {"url": "http://braintumorsegmentation.org/", "anchor_text": "BraTS 2018 Brain Tumor data"}, {"url": "https://smart-stats-tools.org/lesion-challenge", "anchor_text": "ISBI 2015 Multiple Sclerosis Challenge data"}, {"url": "https://www.oasis-brains.org/#dictionary", "anchor_text": "OASIS"}, {"url": "https://www.aapm.org/GrandChallenge/LowDoseCT/", "anchor_text": "Low Dose CT"}, {"url": "https://fastmri.med.nyu.edu/", "anchor_text": "fastMRI"}, {"url": "http://adni.loni.usc.edu/data-samples/access-data/", "anchor_text": "ADNI"}, {"url": "https://openneuro.org", "anchor_text": "OpenNeuro"}, {"url": "https://grand-challenge.org/challenges/", "anchor_text": "here"}, {"url": "http://academictorrents.com/", "anchor_text": "this website"}, {"url": "http://nipy.org/nibabel/", "anchor_text": "nibabel"}, {"url": "http://nipy.org/nibabel/dicom/dcm2nii_algorithms.html", "anchor_text": "Here is a tool"}, {"url": "https://gist.github.com/jcreinhold/a26d6555b0e7aa28b79757f766640dd6", "anchor_text": "here is a script"}, {"url": "https://gist.github.com/jcreinhold/fdd701211191450284c5718502eabbd4", "anchor_text": "here is a script to convert PAR/REC"}, {"url": "http://johnmuschelli.com/imaging_in_r/inhomogeneity_correction_ms/index.pdf", "anchor_text": "inhomogeneous image intensities due to the scanner"}, {"url": "https://en.wikipedia.org/wiki/Hounsfield_scale", "anchor_text": "Hounsfield units"}, {"url": "https://arxiv.org/pdf/1812.04652.pdf", "anchor_text": "quite large"}, {"url": "https://github.com/jcreinhold/intensity-normalization", "anchor_text": "this repository"}, {"url": "https://en.wikipedia.org/wiki/Feature_scaling#Standardization", "anchor_text": "standardization"}, {"url": "https://github.com/Jfortin1/RAVEL", "anchor_text": "RAVEL"}, {"url": "https://www.ncbi.nlm.nih.gov/pubmed/10571928", "anchor_text": "Ny\u00fal & Udupa method"}, {"url": "https://intensity-normalization.readthedocs.io/en/latest/algorithm.html#z-score", "anchor_text": "z-score normalization"}, {"url": "https://www.nitrc.org/projects/robex", "anchor_text": "ROBEX"}, {"url": "https://mipav.cit.nih.gov/", "anchor_text": "MIPAV"}, {"url": "https://horosproject.org/", "anchor_text": "Horos"}, {"url": "https://gist.github.com/jcreinhold/01daf54a6002de7bd8d58bad78b4022b", "anchor_text": "command-line script"}, {"url": "https://github.com/jcreinhold/niftidataset", "anchor_text": "here"}, {"url": "https://pytorch.org/", "anchor_text": "pytorch"}, {"url": "https://github.com/fastai/fastai", "anchor_text": "fastai"}, {"url": "https://nbviewer.jupyter.org/gist/jcreinhold/78943cdeca1c5fca4a5af5d066bd8a8d", "anchor_text": "here"}, {"url": "https://arxiv.org/abs/1512.03385", "anchor_text": "resnet"}, {"url": "https://www.nitrc.org/projects/multimodal/", "anchor_text": "Kirby 21"}, {"url": "https://www.ncbi.nlm.nih.gov/pubmed/20378467", "anchor_text": "N4"}, {"url": "https://github.com/jcreinhold/intensity-normalization/blob/master/intensity_normalization/utilities/preprocess.py", "anchor_text": "here"}, {"url": "https://github.com/jcreinhold/intensity-normalization/blob/master/intensity_normalization/exec/coregister.py", "anchor_text": "here"}, {"url": "https://github.com/jcreinhold/intensity-normalization", "anchor_text": "intensity-normalization"}, {"url": "https://github.com/jcreinhold/intensity-normalization/blob/master/intensity_normalization/normalize/zscore.py", "anchor_text": "z-score normalized"}, {"url": "https://docs.fast.ai/data_block.html", "anchor_text": "data_block"}, {"url": "https://docs.fast.ai/tutorial.itemlist.html", "anchor_text": "ItemList tutorial"}, {"url": "https://en.wikipedia.org/wiki/Anatomical_plane", "anchor_text": "axial plane"}, {"url": "https://nbviewer.jupyter.org/gist/jcreinhold/78943cdeca1c5fca4a5af5d066bd8a8d", "anchor_text": "notebook"}, {"url": "https://docs.fast.ai/data_block.html", "anchor_text": "here"}, {"url": "https://github.com/fastai/fastai/blob/master/fastai/layers.py#L94", "anchor_text": "fastai repository"}, {"url": "https://arxiv.org/abs/1506.01186", "anchor_text": "here"}, {"url": "https://sgugger.github.io/the-1cycle-policy.html", "anchor_text": "one-cycle"}, {"url": "https://arxiv.org/pdf/1803.09820.pdf", "anchor_text": "policy"}, {"url": "http://www.upstate.edu/radiology/education/rsna/intro/display.php", "anchor_text": "window/level settings"}, {"url": "http://www.niftynet.io/", "anchor_text": "NiftyNet"}, {"url": "https://github.com/perone/medicaltorch", "anchor_text": "medicaltorch"}, {"url": "https://neuroconductor.org/", "anchor_text": "neuroconductor"}, {"url": "https://github.com/JuliaIO/NIfTI.jl", "anchor_text": "NIfTI.jl"}, {"url": "https://fluxml.ai/", "anchor_text": "Flux.jl"}, {"url": "https://pytorch.org/docs/master/hub.html", "anchor_text": "share the network weights"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----e9f32273dcb5---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----e9f32273dcb5---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/medical-imaging?source=post_page-----e9f32273dcb5---------------medical_imaging-----------------", "anchor_text": "Medical Imaging"}, {"url": "https://medium.com/tag/python?source=post_page-----e9f32273dcb5---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/pytorch?source=post_page-----e9f32273dcb5---------------pytorch-----------------", "anchor_text": "Pytorch"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe9f32273dcb5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-with-magnetic-resonance-and-computed-tomography-images-e9f32273dcb5&user=Jacob+Reinhold&userId=d6cf1d0b0aa7&source=-----e9f32273dcb5---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe9f32273dcb5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-with-magnetic-resonance-and-computed-tomography-images-e9f32273dcb5&user=Jacob+Reinhold&userId=d6cf1d0b0aa7&source=-----e9f32273dcb5---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe9f32273dcb5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-with-magnetic-resonance-and-computed-tomography-images-e9f32273dcb5&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/@jcreinhold?source=post_page-----e9f32273dcb5--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e9f32273dcb5--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd6cf1d0b0aa7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-with-magnetic-resonance-and-computed-tomography-images-e9f32273dcb5&user=Jacob+Reinhold&userId=d6cf1d0b0aa7&source=post_page-d6cf1d0b0aa7----e9f32273dcb5---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F8c3f698931d6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-with-magnetic-resonance-and-computed-tomography-images-e9f32273dcb5&newsletterV3=d6cf1d0b0aa7&newsletterV3Id=8c3f698931d6&user=Jacob+Reinhold&userId=d6cf1d0b0aa7&source=-----e9f32273dcb5---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/@jcreinhold?source=post_page-----e9f32273dcb5--------------------------------", "anchor_text": "Written by Jacob Reinhold"}, {"url": "https://medium.com/@jcreinhold/followers?source=post_page-----e9f32273dcb5--------------------------------", "anchor_text": "218 Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----e9f32273dcb5--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "http://jcreinhold.com", "anchor_text": "jcreinhold.com"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd6cf1d0b0aa7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-with-magnetic-resonance-and-computed-tomography-images-e9f32273dcb5&user=Jacob+Reinhold&userId=d6cf1d0b0aa7&source=post_page-d6cf1d0b0aa7----e9f32273dcb5---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F8c3f698931d6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-with-magnetic-resonance-and-computed-tomography-images-e9f32273dcb5&newsletterV3=d6cf1d0b0aa7&newsletterV3Id=8c3f698931d6&user=Jacob+Reinhold&userId=d6cf1d0b0aa7&source=-----e9f32273dcb5---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/dropout-on-convolutional-layers-is-weird-5c6ab14f19b2?source=author_recirc-----e9f32273dcb5----0---------------------dfccc24b_9d08_404c_9fb6_81e4f59b1515-------", "anchor_text": ""}, {"url": "https://medium.com/@jcreinhold?source=author_recirc-----e9f32273dcb5----0---------------------dfccc24b_9d08_404c_9fb6_81e4f59b1515-------", "anchor_text": ""}, {"url": "https://medium.com/@jcreinhold?source=author_recirc-----e9f32273dcb5----0---------------------dfccc24b_9d08_404c_9fb6_81e4f59b1515-------", "anchor_text": "Jacob Reinhold"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----e9f32273dcb5----0---------------------dfccc24b_9d08_404c_9fb6_81e4f59b1515-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/dropout-on-convolutional-layers-is-weird-5c6ab14f19b2?source=author_recirc-----e9f32273dcb5----0---------------------dfccc24b_9d08_404c_9fb6_81e4f59b1515-------", "anchor_text": "Dropout on convolutional layers is weirdWhy dropout on convolutional layers is fundamentally different from dropout on fully-connected layers."}, {"url": "https://towardsdatascience.com/dropout-on-convolutional-layers-is-weird-5c6ab14f19b2?source=author_recirc-----e9f32273dcb5----0---------------------dfccc24b_9d08_404c_9fb6_81e4f59b1515-------", "anchor_text": "6 min read\u00b7Feb 10, 2019"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5c6ab14f19b2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdropout-on-convolutional-layers-is-weird-5c6ab14f19b2&user=Jacob+Reinhold&userId=d6cf1d0b0aa7&source=-----5c6ab14f19b2----0-----------------clap_footer----dfccc24b_9d08_404c_9fb6_81e4f59b1515-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/dropout-on-convolutional-layers-is-weird-5c6ab14f19b2?source=author_recirc-----e9f32273dcb5----0---------------------dfccc24b_9d08_404c_9fb6_81e4f59b1515-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "10"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5c6ab14f19b2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdropout-on-convolutional-layers-is-weird-5c6ab14f19b2&source=-----e9f32273dcb5----0-----------------bookmark_preview----dfccc24b_9d08_404c_9fb6_81e4f59b1515-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----e9f32273dcb5----1---------------------dfccc24b_9d08_404c_9fb6_81e4f59b1515-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----e9f32273dcb5----1---------------------dfccc24b_9d08_404c_9fb6_81e4f59b1515-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----e9f32273dcb5----1---------------------dfccc24b_9d08_404c_9fb6_81e4f59b1515-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----e9f32273dcb5----1---------------------dfccc24b_9d08_404c_9fb6_81e4f59b1515-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----e9f32273dcb5----1---------------------dfccc24b_9d08_404c_9fb6_81e4f59b1515-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----e9f32273dcb5----1---------------------dfccc24b_9d08_404c_9fb6_81e4f59b1515-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----dfccc24b_9d08_404c_9fb6_81e4f59b1515-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----e9f32273dcb5----1---------------------dfccc24b_9d08_404c_9fb6_81e4f59b1515-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----e9f32273dcb5----1-----------------bookmark_preview----dfccc24b_9d08_404c_9fb6_81e4f59b1515-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----e9f32273dcb5----2---------------------dfccc24b_9d08_404c_9fb6_81e4f59b1515-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----e9f32273dcb5----2---------------------dfccc24b_9d08_404c_9fb6_81e4f59b1515-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----e9f32273dcb5----2---------------------dfccc24b_9d08_404c_9fb6_81e4f59b1515-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----e9f32273dcb5----2---------------------dfccc24b_9d08_404c_9fb6_81e4f59b1515-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----e9f32273dcb5----2---------------------dfccc24b_9d08_404c_9fb6_81e4f59b1515-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----e9f32273dcb5----2---------------------dfccc24b_9d08_404c_9fb6_81e4f59b1515-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----2-----------------clap_footer----dfccc24b_9d08_404c_9fb6_81e4f59b1515-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----e9f32273dcb5----2---------------------dfccc24b_9d08_404c_9fb6_81e4f59b1515-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----e9f32273dcb5----2-----------------bookmark_preview----dfccc24b_9d08_404c_9fb6_81e4f59b1515-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/knowing-known-unknowns-with-deep-neural-networks-caac1c4c1f5d?source=author_recirc-----e9f32273dcb5----3---------------------dfccc24b_9d08_404c_9fb6_81e4f59b1515-------", "anchor_text": ""}, {"url": "https://medium.com/@jcreinhold?source=author_recirc-----e9f32273dcb5----3---------------------dfccc24b_9d08_404c_9fb6_81e4f59b1515-------", "anchor_text": ""}, {"url": "https://medium.com/@jcreinhold?source=author_recirc-----e9f32273dcb5----3---------------------dfccc24b_9d08_404c_9fb6_81e4f59b1515-------", "anchor_text": "Jacob Reinhold"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----e9f32273dcb5----3---------------------dfccc24b_9d08_404c_9fb6_81e4f59b1515-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/knowing-known-unknowns-with-deep-neural-networks-caac1c4c1f5d?source=author_recirc-----e9f32273dcb5----3---------------------dfccc24b_9d08_404c_9fb6_81e4f59b1515-------", "anchor_text": "Knowing known unknowns with deep neural networksDefining and quantifying aleatory and epistemic uncertainty in deep neural networks"}, {"url": "https://towardsdatascience.com/knowing-known-unknowns-with-deep-neural-networks-caac1c4c1f5d?source=author_recirc-----e9f32273dcb5----3---------------------dfccc24b_9d08_404c_9fb6_81e4f59b1515-------", "anchor_text": "14 min read\u00b7Jun 21, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcaac1c4c1f5d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknowing-known-unknowns-with-deep-neural-networks-caac1c4c1f5d&user=Jacob+Reinhold&userId=d6cf1d0b0aa7&source=-----caac1c4c1f5d----3-----------------clap_footer----dfccc24b_9d08_404c_9fb6_81e4f59b1515-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/knowing-known-unknowns-with-deep-neural-networks-caac1c4c1f5d?source=author_recirc-----e9f32273dcb5----3---------------------dfccc24b_9d08_404c_9fb6_81e4f59b1515-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcaac1c4c1f5d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknowing-known-unknowns-with-deep-neural-networks-caac1c4c1f5d&source=-----e9f32273dcb5----3-----------------bookmark_preview----dfccc24b_9d08_404c_9fb6_81e4f59b1515-------", "anchor_text": ""}, {"url": "https://medium.com/@jcreinhold?source=post_page-----e9f32273dcb5--------------------------------", "anchor_text": "See all from Jacob Reinhold"}, {"url": "https://towardsdatascience.com/?source=post_page-----e9f32273dcb5--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://towardsdatascience.com/please-stop-drawing-neural-networks-wrong-ffd02b67ad77?source=read_next_recirc-----e9f32273dcb5----0---------------------19ed77a0_6182_48b0_9760_ca1653c1ed3e-------", "anchor_text": ""}, {"url": "https://medium.com/@amaster_37400?source=read_next_recirc-----e9f32273dcb5----0---------------------19ed77a0_6182_48b0_9760_ca1653c1ed3e-------", "anchor_text": ""}, {"url": "https://medium.com/@amaster_37400?source=read_next_recirc-----e9f32273dcb5----0---------------------19ed77a0_6182_48b0_9760_ca1653c1ed3e-------", "anchor_text": "Aaron Master"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----e9f32273dcb5----0---------------------19ed77a0_6182_48b0_9760_ca1653c1ed3e-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/please-stop-drawing-neural-networks-wrong-ffd02b67ad77?source=read_next_recirc-----e9f32273dcb5----0---------------------19ed77a0_6182_48b0_9760_ca1653c1ed3e-------", "anchor_text": "Please Stop Drawing Neural Networks WrongThe Case for GOOD Diagrams"}, {"url": "https://towardsdatascience.com/please-stop-drawing-neural-networks-wrong-ffd02b67ad77?source=read_next_recirc-----e9f32273dcb5----0---------------------19ed77a0_6182_48b0_9760_ca1653c1ed3e-------", "anchor_text": "12 min read\u00b7Mar 21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fffd02b67ad77&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplease-stop-drawing-neural-networks-wrong-ffd02b67ad77&user=Aaron+Master&userId=31905cfe67ce&source=-----ffd02b67ad77----0-----------------clap_footer----19ed77a0_6182_48b0_9760_ca1653c1ed3e-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/please-stop-drawing-neural-networks-wrong-ffd02b67ad77?source=read_next_recirc-----e9f32273dcb5----0---------------------19ed77a0_6182_48b0_9760_ca1653c1ed3e-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "33"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fffd02b67ad77&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplease-stop-drawing-neural-networks-wrong-ffd02b67ad77&source=-----e9f32273dcb5----0-----------------bookmark_preview----19ed77a0_6182_48b0_9760_ca1653c1ed3e-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----e9f32273dcb5----1---------------------19ed77a0_6182_48b0_9760_ca1653c1ed3e-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----e9f32273dcb5----1---------------------19ed77a0_6182_48b0_9760_ca1653c1ed3e-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----e9f32273dcb5----1---------------------19ed77a0_6182_48b0_9760_ca1653c1ed3e-------", "anchor_text": "The PyCoach"}, {"url": "https://artificialcorner.com/?source=read_next_recirc-----e9f32273dcb5----1---------------------19ed77a0_6182_48b0_9760_ca1653c1ed3e-------", "anchor_text": "Artificial Corner"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----e9f32273dcb5----1---------------------19ed77a0_6182_48b0_9760_ca1653c1ed3e-------", "anchor_text": "You\u2019re Using ChatGPT Wrong! Here\u2019s How to Be Ahead of 99% of ChatGPT UsersMaster ChatGPT by learning prompt engineering."}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----e9f32273dcb5----1---------------------19ed77a0_6182_48b0_9760_ca1653c1ed3e-------", "anchor_text": "\u00b77 min read\u00b7Mar 17"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fartificial-corner%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&user=The+PyCoach&userId=fb44e21903f3&source=-----886a50dabc54----1-----------------clap_footer----19ed77a0_6182_48b0_9760_ca1653c1ed3e-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----e9f32273dcb5----1---------------------19ed77a0_6182_48b0_9760_ca1653c1ed3e-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "275"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&source=-----e9f32273dcb5----1-----------------bookmark_preview----19ed77a0_6182_48b0_9760_ca1653c1ed3e-------", "anchor_text": ""}, {"url": "https://jehillparikh.medium.com/u-nets-with-attention-c8d7e9bf2416?source=read_next_recirc-----e9f32273dcb5----0---------------------19ed77a0_6182_48b0_9760_ca1653c1ed3e-------", "anchor_text": ""}, {"url": "https://jehillparikh.medium.com/?source=read_next_recirc-----e9f32273dcb5----0---------------------19ed77a0_6182_48b0_9760_ca1653c1ed3e-------", "anchor_text": ""}, {"url": "https://jehillparikh.medium.com/?source=read_next_recirc-----e9f32273dcb5----0---------------------19ed77a0_6182_48b0_9760_ca1653c1ed3e-------", "anchor_text": "Jehill Parikh"}, {"url": "https://jehillparikh.medium.com/u-nets-with-attention-c8d7e9bf2416?source=read_next_recirc-----e9f32273dcb5----0---------------------19ed77a0_6182_48b0_9760_ca1653c1ed3e-------", "anchor_text": "U-Nets with attentionU-Net are popular NN architecture which are employed for many applications and were initially developed for medical image segmentation."}, {"url": "https://jehillparikh.medium.com/u-nets-with-attention-c8d7e9bf2416?source=read_next_recirc-----e9f32273dcb5----0---------------------19ed77a0_6182_48b0_9760_ca1653c1ed3e-------", "anchor_text": "\u00b72 min read\u00b7Nov 15, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2Fc8d7e9bf2416&operation=register&redirect=https%3A%2F%2Fjehillparikh.medium.com%2Fu-nets-with-attention-c8d7e9bf2416&user=Jehill+Parikh&userId=c972081b627e&source=-----c8d7e9bf2416----0-----------------clap_footer----19ed77a0_6182_48b0_9760_ca1653c1ed3e-------", "anchor_text": ""}, {"url": "https://jehillparikh.medium.com/u-nets-with-attention-c8d7e9bf2416?source=read_next_recirc-----e9f32273dcb5----0---------------------19ed77a0_6182_48b0_9760_ca1653c1ed3e-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc8d7e9bf2416&operation=register&redirect=https%3A%2F%2Fjehillparikh.medium.com%2Fu-nets-with-attention-c8d7e9bf2416&source=-----e9f32273dcb5----0-----------------bookmark_preview----19ed77a0_6182_48b0_9760_ca1653c1ed3e-------", "anchor_text": ""}, {"url": "https://mark-riedl.medium.com/a-very-gentle-introduction-to-large-language-models-without-the-hype-5f67941fa59e?source=read_next_recirc-----e9f32273dcb5----1---------------------19ed77a0_6182_48b0_9760_ca1653c1ed3e-------", "anchor_text": ""}, {"url": "https://mark-riedl.medium.com/?source=read_next_recirc-----e9f32273dcb5----1---------------------19ed77a0_6182_48b0_9760_ca1653c1ed3e-------", "anchor_text": ""}, {"url": "https://mark-riedl.medium.com/?source=read_next_recirc-----e9f32273dcb5----1---------------------19ed77a0_6182_48b0_9760_ca1653c1ed3e-------", "anchor_text": "Mark Riedl"}, {"url": "https://mark-riedl.medium.com/a-very-gentle-introduction-to-large-language-models-without-the-hype-5f67941fa59e?source=read_next_recirc-----e9f32273dcb5----1---------------------19ed77a0_6182_48b0_9760_ca1653c1ed3e-------", "anchor_text": "A Very Gentle Introduction to Large Language Models without the Hype[This is a work in progress]"}, {"url": "https://mark-riedl.medium.com/a-very-gentle-introduction-to-large-language-models-without-the-hype-5f67941fa59e?source=read_next_recirc-----e9f32273dcb5----1---------------------19ed77a0_6182_48b0_9760_ca1653c1ed3e-------", "anchor_text": "38 min read\u00b7Apr 14"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F5f67941fa59e&operation=register&redirect=https%3A%2F%2Fmark-riedl.medium.com%2Fa-very-gentle-introduction-to-large-language-models-without-the-hype-5f67941fa59e&user=Mark+Riedl&userId=7247bdeb9655&source=-----5f67941fa59e----1-----------------clap_footer----19ed77a0_6182_48b0_9760_ca1653c1ed3e-------", "anchor_text": ""}, {"url": "https://mark-riedl.medium.com/a-very-gentle-introduction-to-large-language-models-without-the-hype-5f67941fa59e?source=read_next_recirc-----e9f32273dcb5----1---------------------19ed77a0_6182_48b0_9760_ca1653c1ed3e-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "53"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5f67941fa59e&operation=register&redirect=https%3A%2F%2Fmark-riedl.medium.com%2Fa-very-gentle-introduction-to-large-language-models-without-the-hype-5f67941fa59e&source=-----e9f32273dcb5----1-----------------bookmark_preview----19ed77a0_6182_48b0_9760_ca1653c1ed3e-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----e9f32273dcb5----2---------------------19ed77a0_6182_48b0_9760_ca1653c1ed3e-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----e9f32273dcb5----2---------------------19ed77a0_6182_48b0_9760_ca1653c1ed3e-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----e9f32273dcb5----2---------------------19ed77a0_6182_48b0_9760_ca1653c1ed3e-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----e9f32273dcb5----2---------------------19ed77a0_6182_48b0_9760_ca1653c1ed3e-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----e9f32273dcb5----2---------------------19ed77a0_6182_48b0_9760_ca1653c1ed3e-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----e9f32273dcb5----2---------------------19ed77a0_6182_48b0_9760_ca1653c1ed3e-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----2-----------------clap_footer----19ed77a0_6182_48b0_9760_ca1653c1ed3e-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----e9f32273dcb5----2---------------------19ed77a0_6182_48b0_9760_ca1653c1ed3e-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----e9f32273dcb5----2-----------------bookmark_preview----19ed77a0_6182_48b0_9760_ca1653c1ed3e-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/understanding-nerfs-2a082e13c6eb?source=read_next_recirc-----e9f32273dcb5----3---------------------19ed77a0_6182_48b0_9760_ca1653c1ed3e-------", "anchor_text": ""}, {"url": "https://wolfecameron.medium.com/?source=read_next_recirc-----e9f32273dcb5----3---------------------19ed77a0_6182_48b0_9760_ca1653c1ed3e-------", "anchor_text": ""}, {"url": "https://wolfecameron.medium.com/?source=read_next_recirc-----e9f32273dcb5----3---------------------19ed77a0_6182_48b0_9760_ca1653c1ed3e-------", "anchor_text": "Cameron R. Wolfe"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----e9f32273dcb5----3---------------------19ed77a0_6182_48b0_9760_ca1653c1ed3e-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/understanding-nerfs-2a082e13c6eb?source=read_next_recirc-----e9f32273dcb5----3---------------------19ed77a0_6182_48b0_9760_ca1653c1ed3e-------", "anchor_text": "Understanding NeRFsA massive breakthrough in scene representation"}, {"url": "https://towardsdatascience.com/understanding-nerfs-2a082e13c6eb?source=read_next_recirc-----e9f32273dcb5----3---------------------19ed77a0_6182_48b0_9760_ca1653c1ed3e-------", "anchor_text": "\u00b711 min read\u00b73 days ago"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2a082e13c6eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-nerfs-2a082e13c6eb&user=Cameron+R.+Wolfe&userId=28aa6026c553&source=-----2a082e13c6eb----3-----------------clap_footer----19ed77a0_6182_48b0_9760_ca1653c1ed3e-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/understanding-nerfs-2a082e13c6eb?source=read_next_recirc-----e9f32273dcb5----3---------------------19ed77a0_6182_48b0_9760_ca1653c1ed3e-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2a082e13c6eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-nerfs-2a082e13c6eb&source=-----e9f32273dcb5----3-----------------bookmark_preview----19ed77a0_6182_48b0_9760_ca1653c1ed3e-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----e9f32273dcb5--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----e9f32273dcb5--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----e9f32273dcb5--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----e9f32273dcb5--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----e9f32273dcb5--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----e9f32273dcb5--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----e9f32273dcb5--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----e9f32273dcb5--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----e9f32273dcb5--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----e9f32273dcb5--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}