{"url": "https://towardsdatascience.com/bert-s-for-relation-extraction-in-nlp-2c7c3ab487c4", "time": 1683017313.388671, "path": "towardsdatascience.com/bert-s-for-relation-extraction-in-nlp-2c7c3ab487c4/", "webpage": {"metadata": {"title": "BERT(S) for Relation Extraction in NLP | by Wee Tee Soh | Towards Data Science", "h1": "BERT(S) for Relation Extraction in NLP", "description": "Relationships are everywhere, be it with your family, with your significant other, with friends, or with your pet/plant. Or in this particular case, between entity mentions within paragraphs of text\u2026"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/pdf/1906.03158.pdf", "anchor_text": "paper", "paragraph_index": 1}, {"url": "https://github.com/plkmo/BERT-Relation-Extraction", "anchor_text": "https://github.com/plkmo/BERT-Relation-Extraction", "paragraph_index": 2}, {"url": "https://github.com/plkmo/BERT-Relation-Extraction", "anchor_text": "implement", "paragraph_index": 18}, {"url": "https://t.me/followai_bot", "anchor_text": "https://t.me/followai_bot", "paragraph_index": 21}], "all_paragraphs": ["Relationships are everywhere, be it with your family, with your significant other, with friends, or with your pet/plant. Or in this particular case, between entity mentions within paragraphs of text. The associations within real-life relationships are pretty much well-defined (eg. mother-daughter, father-son etc), whereas the relationships between entities in a paragraph of text would require significantly more thought to extract and hence, will be the focus of this article.", "Being able to automatically extract relationships between entities in free-text is very useful \u2014 not for a student to automate his/her English homework \u2014 but more for data scientists to do their work better, to build knowledge graphs etc. It has been one of the focus research areas of AI giants like Google, and they have recently published a paper on this topic, \u201cMatching the Blanks: Distributional Similarity for Relation Learning\u201d. In this article, I am going to detail some of the core concepts behind this paper, and, since their implementation code wasn\u2019t open-sourced, I am going to also implement some of the models and training pipelines on sample datasets and open-source my codes.", "If you are the TL;DR kind of guy/gal who just wants to cut to the chase and jump straight to using it on your exciting text, you can find it here on my Github page: https://github.com/plkmo/BERT-Relation-Extraction", "How do you prepare an AI model to extract relations between textual entities, without giving it any specific labels (unsupervised)? Well, you will first have to frame the task/problem for the model to understand.", "Consider the two relation statements above. Here, a relation statement refers to a sentence in which two entities have been identified for relation extraction/classification. Mathematically, we can represent a relation statement as follows:", "Here, x is the tokenized sentence, with s1 and s2 being the spans of the two entities within that sentence. While the two relation statements r1 and r2 above consist of two different sentences, they both contain the same entity pair, which have been replaced with the \u201c[BLANK]\u201d symbol. Now, the intuition is that if both r1 and r2 contain the same entity pair (s1 and s2), they should have the same s1-s2 relation.", "Therefore, the pre-training task for the AI model is that given any r1 and r2, to embed them such that their inner product is high when r1 and r2 both contain the same entity pair (s1 and s2), and low when their entity pairs are different. Noise-contrastive estimation is implemented here for this learning process, since it is not feasible to explicitly compare every single r1 and r2 pair during training.", "The good thing about this is that you can pre-train it on just about any chunk of text, from your personal data in WhatsApp messages to open-source data on Wikipedia, as long as you use something like spaCy NER or dependency parsing tools to extract and annotate any two entities within each sentence.", "Why the \u201c[BLANK]\u201d symbol then? Well, the entities within the relation statement are intentionally masked with \u201c[BLANK]\u201d symbol with a certain probability, so that during pre-training, the model can\u2019t just rely on the entity names themselves to learn the relations (if it does that, the model will simply be memorizing, not actually learning anything useful), but also need to take into account their context (surrounding tokens) as well.", "The model used here is the standard BERT architecture, with some slight modifications below to encode the input relation statements and to extract their pre-trained output representations for loss calculation & downstream fine-tuning tasks.", "The above is what the paper calls Entity Markers \u2014 Entity Start (or EM) representation. In the input relation statement x, \u201c[E1]\u201d and \u201c[E2]\u201d markers are used to mark the positions of their respective entities so that BERT knows exactly which ones you are interested in. The output hidden states of BERT at the \u201c[E1]\u201d and \u201c[E2]\u201d token positions are concatenated as the final output representation of x, which is then used along with that from other relation statements for loss calculation, such that the output representations of two relation statements with the same entity pair should have a high inner product.", "Once the BERT model has been pre-trained this way, its output representation of any x can then be used for any downstream task.", "Suppose now we want to do relation classification i.e. given any two relations within a sentence, to classify the relationship between them (eg. Cause-Effect, Entity-Location, etc). Using the pre-trained BERT model on MTB task, we can do just that!", "As above, simply stack a linear classifier on top of it (the output hidden states representation), and train this classifier on labelled relation statements. Thereafter, we can run inference on some sentences. The output, from me training it with the SemEval2010 Task 8 dataset, looks something like", "In this case, the model successfully predicted that the entity \u201ca sore throat\u201d is caused by the act of \u201cafter eating the chicken\u201d.", "Now, you might wonder if the model can still predict the relation classes well if it is only given one labelled relation statement per relation class for training. Well, it turns out that it can, or at least do much better than vanilla BERT models.", "For the prediction, suppose we have 5 relation classes with each class only containing one labelled relation statement x, and we use this to predict the relation class of another unlabelled x. (Known as 5-way 1-shot) We can proceed to take this BERT model with EM representation (whether pre-trained with MTB or not), and run all the 6 x\u2019s (5 labelled, 1 unlabelled) through this model to get their corresponding output representations.", "We then simply compare the inner products between the unlabelled x\u2019s output representation and that of all the other 5 labelled x\u2019s, and take the relation class with the highest inner product as the final prediction.", "The Google Research team used the entire English Wikipedia for their BERT MTB pre-training, with Google Cloud Natural Language API to annotate their entities. Well, my wife only allows me to purchase a 8 GB RTX 2070 personal laptop GPU for now, so while I did attempt to implement their model, I could only pre-train it on the rather small CNN/DailyMail dataset, using the free spaCy NLP library to annotate entities.", "So naturally, the prediction results weren\u2019t as impressive. Nevertheless, the baseline BERT with EM representation is still pretty good for fine-tuning on relation classification and produces reasonable results. Also, since now BERTs of all forms are everywhere and uses the same baseline architecture, I have implemented this for ALBERT and BioBERT as well.", "That\u2019s all folks, I hope this article has helped in your journey to demystify AI/deep learning/data science. Stay tuned for more of my paper implementations!", "To keep up-to-date with the latest AI/data science trends, papers & news, check out my @followai_bot (https://t.me/followai_bot), your personalized AI/data science Telegram bot.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F2c7c3ab487c4&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbert-s-for-relation-extraction-in-nlp-2c7c3ab487c4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbert-s-for-relation-extraction-in-nlp-2c7c3ab487c4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbert-s-for-relation-extraction-in-nlp-2c7c3ab487c4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbert-s-for-relation-extraction-in-nlp-2c7c3ab487c4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----2c7c3ab487c4--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2c7c3ab487c4--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@weeteesoh345?source=post_page-----2c7c3ab487c4--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@weeteesoh345?source=post_page-----2c7c3ab487c4--------------------------------", "anchor_text": "Wee Tee Soh"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F836f30b28f75&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbert-s-for-relation-extraction-in-nlp-2c7c3ab487c4&user=Wee+Tee+Soh&userId=836f30b28f75&source=post_page-836f30b28f75----2c7c3ab487c4---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2c7c3ab487c4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbert-s-for-relation-extraction-in-nlp-2c7c3ab487c4&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2c7c3ab487c4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbert-s-for-relation-extraction-in-nlp-2c7c3ab487c4&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@davidkm", "anchor_text": "@davidkm"}, {"url": "https://arxiv.org/pdf/1906.03158.pdf", "anchor_text": "paper"}, {"url": "https://github.com/plkmo/BERT-Relation-Extraction", "anchor_text": "https://github.com/plkmo/BERT-Relation-Extraction"}, {"url": "https://arxiv.org/pdf/1906.03158.pdf", "anchor_text": "https://arxiv.org/pdf/1906.03158.pdf"}, {"url": "https://arxiv.org/pdf/1906.03158.pdf", "anchor_text": "https://arxiv.org/pdf/1906.03158.pdf"}, {"url": "https://arxiv.org/pdf/1906.03158.pdf", "anchor_text": "https://arxiv.org/pdf/1906.03158.pdf"}, {"url": "https://github.com/plkmo/BERT-Relation-Extraction", "anchor_text": "implement"}, {"url": "https://arxiv.org/pdf/1906.03158.pdf", "anchor_text": "https://arxiv.org/pdf/1906.03158.pdf"}, {"url": "https://t.me/followai_bot", "anchor_text": "https://t.me/followai_bot"}, {"url": "https://medium.com/tag/bert?source=post_page-----2c7c3ab487c4---------------bert-----------------", "anchor_text": "Bert"}, {"url": "https://medium.com/tag/relation-extraction?source=post_page-----2c7c3ab487c4---------------relation_extraction-----------------", "anchor_text": "Relation Extraction"}, {"url": "https://medium.com/tag/nlp?source=post_page-----2c7c3ab487c4---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/naturallanguageprocessing?source=post_page-----2c7c3ab487c4---------------naturallanguageprocessing-----------------", "anchor_text": "Naturallanguageprocessing"}, {"url": "https://medium.com/tag/entity-extraction?source=post_page-----2c7c3ab487c4---------------entity_extraction-----------------", "anchor_text": "Entity Extraction"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2c7c3ab487c4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbert-s-for-relation-extraction-in-nlp-2c7c3ab487c4&user=Wee+Tee+Soh&userId=836f30b28f75&source=-----2c7c3ab487c4---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2c7c3ab487c4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbert-s-for-relation-extraction-in-nlp-2c7c3ab487c4&user=Wee+Tee+Soh&userId=836f30b28f75&source=-----2c7c3ab487c4---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2c7c3ab487c4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbert-s-for-relation-extraction-in-nlp-2c7c3ab487c4&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2c7c3ab487c4--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F2c7c3ab487c4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbert-s-for-relation-extraction-in-nlp-2c7c3ab487c4&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----2c7c3ab487c4---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----2c7c3ab487c4--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----2c7c3ab487c4--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----2c7c3ab487c4--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----2c7c3ab487c4--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----2c7c3ab487c4--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----2c7c3ab487c4--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----2c7c3ab487c4--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----2c7c3ab487c4--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@weeteesoh345?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@weeteesoh345?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Wee Tee Soh"}, {"url": "https://medium.com/@weeteesoh345/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "216 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F836f30b28f75&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbert-s-for-relation-extraction-in-nlp-2c7c3ab487c4&user=Wee+Tee+Soh&userId=836f30b28f75&source=post_page-836f30b28f75--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F95d486611d9b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbert-s-for-relation-extraction-in-nlp-2c7c3ab487c4&newsletterV3=836f30b28f75&newsletterV3Id=95d486611d9b&user=Wee+Tee+Soh&userId=836f30b28f75&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}