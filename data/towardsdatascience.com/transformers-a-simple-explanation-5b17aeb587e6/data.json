{"url": "https://towardsdatascience.com/transformers-a-simple-explanation-5b17aeb587e6", "time": 1683017525.2806861, "path": "towardsdatascience.com/transformers-a-simple-explanation-5b17aeb587e6/", "webpage": {"metadata": {"title": "Transformers for Machine Learning: A Simple Explanation | by Fran\u00e7ois St-Amant | Towards Data Science", "h1": "Transformers for Machine Learning: A Simple Explanation", "description": "If you are here to learn more about the movies, sadly, this is not the article you are looking for. I love Optimus Prime and Megatron as much as the next guy, but here, I will be talking about\u2026"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "here", "paragraph_index": 1}, {"url": "https://towardsdatascience.com/the-fall-of-rnn-lstm-2d1594c74ce0", "anchor_text": "https://towardsdatascience.com/the-fall-of-rnn-lstm-2d1594c74ce0", "paragraph_index": 6}], "all_paragraphs": ["If you are here to learn more about the movies, sadly, this is not the article you are looking for. I love Optimus Prime and Megatron as much as the next guy, but here, I will be talking about Transformer, the deep learning model!", "The Transformer was first introduced in 2017 in the paper \u201cAttention is all you need\u201d, which can be found right here. You will see, the title is revealing.", "It really has revolutionized the NLP world, so you should definitely learn all about it. The issue? It\u2019s not the easiest model to understand.", "Thankfully, in this article, I will present all you need to know about it in simple terms that hopefully you will understand. Let\u2019s get to it!", "You might ask yourself, why would I need this? I already know about RNN and LSTM. Isn\u2019t that good enough?", "The issue with those two models is that long term information tends to be forgotten by the model, the longer the sequence gets. Theoretically, the information from a token can propagate far down the sequence but in practice, the probability that we keep the information diminishes exponentially, the further away we get from a specific word.", "This concept is called the vanishing gradient. LSTMs do better than RNNs thanks to the introduction of a \u201cforget gates\u201d, but they don\u2019t do so great with much larger sequences. The following article explains the issue quite clearly, if you are interested: https://towardsdatascience.com/the-fall-of-rnn-lstm-2d1594c74ce0", "That\u2019s where transformers come in! In the upcoming sections, I will present how they are built.", "Transformers don\u2019t use the notion of recurrence. Instead, they use an attention mechanism called self-attention.", "So what is that? The idea is that by using a function (the scaled dot product attention), we can learn a vector of context, meaning that we use other words in the sequence to get a better understanding of a specific word.", "The word \u201cit\u201d is strongly linked to the words \u201cThe\u201d and \u201cAnimal\u201d. We now have more than just a word as information, we also have an association with other words. That can only help in making a prediction.", "Below, we will quickly see how this self-attention is calculated exactly.", "The authors of the original paper on Transformers define the output of their attention function as follows:", "It can be defined as the process of representing a Query Q and a pair of Key-Value K and V as an output.", "I want to keeps things simple, so what needs to be understood here is that the weights that will be assigned to V are calculated as being to what extent every word in a sequence Q is influenced by all the other words K of the sequence.", "The division by the root of dk, which represents the length of the sequence, ensures that long sequences don\u2019t push the results towards too small of gradients after going through the softmax function.", "In case you did not know, the softmax function is used to normalize the output to a distribution (0,1).", "Before we go into the architecture of the model, I want to explain one last thing, the Multi-Head Attention.", "The goal of this is to have 8 different representations of Q, K and V (with different random weights) go through the attention mechanism in parallel. Afterwards, results are aggregated and transformed into the expected output of the attention mechanism. An image is worth a thousand words.", "The intuition behind this is that it allows the model to learn different representations in a different manner, which should give more reliable results in the end.", "Now that we understand the attention mechanism, let\u2019s look at the architecture of the Transformer.", "On the left, we have an encoder, on the right a decoder. What is shown here is a single stack, but keep in mind we have in the real architecture 6 identical stacks.", "In the encoder, we have 2 main sublayers, i.e. one Multi-Head attention layer and one feed forward layer. The input of the encoder is the embedding of the sequence itself.", "The decoder is very similar to the encoder. However, we have one additional sublayer, the Masked Multi-Head Attention layer. Why is it called like that? Well, it\u2019s simply because the decoder will \u201chide\u201d future inputs to ensure that a prediction made at time i only depends on what is known prior to it. The decoder takes as input the output of the encoder.", "One last thing you might have noticed in the figure above is the positional encoding. That is introduced here because unlike with LSTM and RNN, we have no recurrence. Words are not processed in a sequential manner, but rather as a sequence as a whole.", "The positional encoding fixes that. It allows the model to know the position of a word in it\u2019s sequence, also considering the overall length of the sequence, in order to have a relative position.", "That makes sense, since where the word is used in a sentence (beginning or end) can change it\u2019s meaning.", "Pretrained models have been developed in recent years, to facilitate the usage of Transformers. It\u2019s possible to use those models to save a LOT of training time, while still obtaining amazing results.", "There are many options (over 50 if you are using Pytorch), but the most popular one is without a doubt the Bidirectional Encoder Representation from Transformer model, better known as BERT.", "It was created by engineers at Google in 2018. It\u2019s trained on more than 2,500 million words and it can be used to do any NLP tasks. With more than 110M parameters, that baby is huge.", "Thankfully, by using the pretrained model, you can simply \u201cfinetune\u201d it a little bit on your data or for a specific task and achieve great results.", "Thanks for reading, I hope I could help in your understanding of the Transformer. Watching the movies will never be the same.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F5b17aeb587e6&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-a-simple-explanation-5b17aeb587e6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-a-simple-explanation-5b17aeb587e6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-a-simple-explanation-5b17aeb587e6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-a-simple-explanation-5b17aeb587e6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----5b17aeb587e6--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5b17aeb587e6--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://francoisstamant.medium.com/?source=post_page-----5b17aeb587e6--------------------------------", "anchor_text": ""}, {"url": "https://francoisstamant.medium.com/?source=post_page-----5b17aeb587e6--------------------------------", "anchor_text": "Fran\u00e7ois St-Amant"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4f8974d28029&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-a-simple-explanation-5b17aeb587e6&user=Fran%C3%A7ois+St-Amant&userId=4f8974d28029&source=post_page-4f8974d28029----5b17aeb587e6---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5b17aeb587e6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-a-simple-explanation-5b17aeb587e6&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5b17aeb587e6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-a-simple-explanation-5b17aeb587e6&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/photos/vuMTQj6aQQ0", "anchor_text": "https://unsplash.com/photos/vuMTQj6aQQ0"}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "here"}, {"url": "https://towardsdatascience.com/the-fall-of-rnn-lstm-2d1594c74ce0", "anchor_text": "https://towardsdatascience.com/the-fall-of-rnn-lstm-2d1594c74ce0"}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "http://jalammar.github.io/illustrated-transformer/"}, {"url": "https://arxiv.org/pdf/1706.03762.pdf", "anchor_text": "https://arxiv.org/pdf/1706.03762.pdf"}, {"url": "https://arxiv.org/pdf/1706.03762.pdf", "anchor_text": "https://arxiv.org/pdf/1706.03762.pdf"}, {"url": "https://arxiv.org/pdf/1706.03762.pdf", "anchor_text": "https://arxiv.org/pdf/1706.03762.pdf"}, {"url": "https://medium.com/tag/transformers?source=post_page-----5b17aeb587e6---------------transformers-----------------", "anchor_text": "Transformers"}, {"url": "https://medium.com/tag/nlp?source=post_page-----5b17aeb587e6---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/attention?source=post_page-----5b17aeb587e6---------------attention-----------------", "anchor_text": "Attention"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----5b17aeb587e6---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/attention-is-all-you-need?source=post_page-----5b17aeb587e6---------------attention_is_all_you_need-----------------", "anchor_text": "Attention Is All You Need"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5b17aeb587e6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-a-simple-explanation-5b17aeb587e6&user=Fran%C3%A7ois+St-Amant&userId=4f8974d28029&source=-----5b17aeb587e6---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5b17aeb587e6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-a-simple-explanation-5b17aeb587e6&user=Fran%C3%A7ois+St-Amant&userId=4f8974d28029&source=-----5b17aeb587e6---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5b17aeb587e6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-a-simple-explanation-5b17aeb587e6&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5b17aeb587e6--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F5b17aeb587e6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-a-simple-explanation-5b17aeb587e6&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----5b17aeb587e6---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----5b17aeb587e6--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----5b17aeb587e6--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----5b17aeb587e6--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----5b17aeb587e6--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----5b17aeb587e6--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----5b17aeb587e6--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----5b17aeb587e6--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----5b17aeb587e6--------------------------------", "anchor_text": ""}, {"url": "https://francoisstamant.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://francoisstamant.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Fran\u00e7ois St-Amant"}, {"url": "https://francoisstamant.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "501 Followers"}, {"url": "https://francoisstamant.medium.com/membership", "anchor_text": "https://francoisstamant.medium.com/membership"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4f8974d28029&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-a-simple-explanation-5b17aeb587e6&user=Fran%C3%A7ois+St-Amant&userId=4f8974d28029&source=post_page-4f8974d28029--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fc94c3a4f2e83&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-a-simple-explanation-5b17aeb587e6&newsletterV3=4f8974d28029&newsletterV3Id=c94c3a4f2e83&user=Fran%C3%A7ois+St-Amant&userId=4f8974d28029&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}