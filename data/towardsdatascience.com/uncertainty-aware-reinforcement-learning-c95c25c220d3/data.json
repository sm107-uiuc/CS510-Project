{"url": "https://towardsdatascience.com/uncertainty-aware-reinforcement-learning-c95c25c220d3", "time": 1683010365.644593, "path": "towardsdatascience.com/uncertainty-aware-reinforcement-learning-c95c25c220d3/", "webpage": {"metadata": {"title": "Uncertainty Aware Reinforcement Learning | by mugoh mwaura | Towards Data Science", "h1": "Uncertainty Aware Reinforcement Learning", "description": "Model-based Reinforcement Learning (RL) gets most of its favour from sample efficiency. It\u2019s generous and undemanding on the amount desired as input, with a cap on what we should expect the model to\u2026"}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/Latent_variable", "anchor_text": "latent variables", "paragraph_index": 8}, {"url": "https://raw.githubusercontent.com/mugoh/curious/master/20200628_191322-01.jpeg", "anchor_text": "good babies can do that", "paragraph_index": 12}, {"url": "https://en.m.wikipedia.org/wiki/Measurement_uncertainty", "anchor_text": "uncertainty in the observations", "paragraph_index": 26}, {"url": "https://en.wikipedia.org/wiki/Model_predictive_control", "anchor_text": "Model Predictive Control", "paragraph_index": 33}, {"url": "https://en.wikipedia.org/wiki/Bernoulli_distribution", "anchor_text": "Bernoulli distribution", "paragraph_index": 33}, {"url": "https://en.wikipedia.org/wiki/Discriminative_model", "anchor_text": "discriminative model", "paragraph_index": 37}, {"url": "https://en.wikipedia.org/wiki/Posterior_probability", "anchor_text": "posterior", "paragraph_index": 39}, {"url": "https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence", "anchor_text": "as close as possible", "paragraph_index": 41}, {"url": "https://medium.com/@jonathan_hui/machine-learning-variational-inference-273d8e6480bb", "anchor_text": "variational inference", "paragraph_index": 41}, {"url": "https://ai.stackexchange.com/a/8295", "anchor_text": "bad idea", "paragraph_index": 42}, {"url": "http://jmlr.org/proceedings/papers/v48/gal16.pdf", "anchor_text": "underestimates uncertainty severely", "paragraph_index": 45}, {"url": "https://en.wikipedia.org/wiki/Evidence_lower_bound", "anchor_text": "the variational lower bound", "paragraph_index": 45}, {"url": "https://en.wikipedia.org/wiki/Sampling_%28statistics%29#Replacement_of_selected_units", "anchor_text": "sampled with replacement", "paragraph_index": 51}, {"url": "https://en.wikipedia.org/wiki/Expected_value", "anchor_text": "expectation", "paragraph_index": 52}, {"url": "https://en.wikipedia.org/wiki/Normal_distribution", "anchor_text": "Gaussian distribution", "paragraph_index": 57}, {"url": "https://math.stackexchange.com/a/544268", "anchor_text": "symmetry of Gaussians", "paragraph_index": 61}, {"url": "https://medium.com/@jonathan_hui/rl-lqr-ilqr-linear-quadratic-regulator-a5de5104c750", "anchor_text": "LQR optimization in Model-based RL", "paragraph_index": 63}, {"url": "https://arxiv.org/pdf/1708.02596", "anchor_text": "Neural Network Dynamics for Model-Based Deep Reinforcement Learning with Model-Free Fine-Tuning", "paragraph_index": 66}, {"url": "http://jmlr.org/proceedings/papers/v48/gal16.pdf", "anchor_text": "Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning", "paragraph_index": 67}, {"url": "https://arxiv.org/pdf/1703.02914.pdf", "anchor_text": "Dropout Inference in Bayesian Neural Networks with Alpha-divergences", "paragraph_index": 68}, {"url": "http://rail.eecs.berkeley.edu/deeprlcourse/", "anchor_text": "Deep RL Decision making and Control", "paragraph_index": 69}, {"url": "https://arxiv.org/pdf/1702.01182v1.pdf", "anchor_text": "Uncertainty-Aware Reinforcement Learning for Collision Avoidance", "paragraph_index": 70}], "all_paragraphs": ["Model-based Reinforcement Learning (RL) gets most of its favour from sample efficiency. It\u2019s generous and undemanding on the amount desired as input, with a cap on what we should expect the model to achieve.", "It\u2019s unlikely for the model to turn out a perfect representation of the environment. While interacting with the real world through the trained agent, we might meet states and rewards different from the ones seen during training. For model-based RL to work, we need to overcome this problem. It\u2019s vital. It\u2019s what will help our agent know what it\u2019s doing.", "First, what of model-free RL? Model-free RL uses the ground-truth transitions of the environment in training and testing the agent, always. Unless there are offsets we introduce, such as simulation-to-real transfer, in which case we can\u2019t blame the algorithm.Uncertainty is, therefore, not a big worry here. For something like a Q function Q(s, a), which optimizes over actions, we can attempt to integrate certainty awareness on action selection. But since it works well anyway, it\u2019s no harm, for now, closing our eyes and pretending we didn\u2019t see that.", "1. Source of uncertainty in Model-based RL2. The benefit of uncertainty awareness3. Building uncertainty aware models- What might seem to work- What does work4. Conclusion", "Model uncertainty results from the distribution mismatch between the data the model sees during testing and that used to train the model. We test the agent on a distribution different from that seen during training.", "At the start of training, the model p(s\u209c \u208a\u2081| s\u209c, a\u209c) has exposed itself to quite small real-world data. We hope the function doesn\u2019t over-fit to this small quantity because we need it to be expressive enough to capture the transitions in later time steps. Then, real data will have accumulated to learn the precise model.", "This is challenging to achieve in Model-based RL. Why? The simple goal of RL is maximizing the future cumulative reward. The planner, while aiming for this, attempts to follow plans for which the model predicts high reward. So if the model overestimates the rewards it will get for a particular action sequence, the planner will be glad to follow that gleaming but erroneous estimate. Selecting such actions in the real-world then results in funny behaviour. In short, the planner is motivated to exploit the positive mistakes of the model.", "(We can think of the planner as the method we use to select optimal actions given the world states).", "And can this get worse? In high dimensional spaces \u2014 where input is an image, for instance \u2014 the mistakes the model makes will be a lot more owing to latent variables. It\u2019s common in model-based RL to alleviate the distribution mismatch by using on-policy data collection \u2014 transitions observed in the real world are added to the training data and used to replan and correct deviations in the model. In this case, though, the mistakes will be too plenty for the on-policy fix to catch up with the lost model. The plenty of errors might result in the policy changing every time we re-plan, and as a result, the model may never converge.", "We may choose to collect data for every mistake the model might make, but wouldn\u2019t it better if we could detect where the model might go wrong, so the model-based planner can avoid actions likely to result in severe outcomes?", "First, let\u2019s phrase what we know as a simple story.", "A loving couple gets the blessing of a baby and a robot \u2014 not necessarily at the same time. The robot\u2019s goal is, as a babysitter, to keep baby Juliet happy. While it\u2019s motivated with rewards to achieve this, it\u2019s also desirable that the robot avoids anything damaging, or that might injure the baby.", "Now the baby grows fond of crying while pointing at bugs \u2014 because good babies can do that \u2014 and the robot\u2019s optimal-reward plan becomes squashing the bug and letting baby Juliet watch it feed the vermin to the cat.", "For a change, though, say the robot encounters the baby crying while pointing at something scary on the Television \u2014 an unfamiliar state, seemingly close to baby Juliet\u2019s cry-pointing behaviour. Unsure of the dynamics, the robot\u2019s best plan might be to squash the TV and feed it to the cat. We are not sure if that will make the baby happy, but it\u2019s sure to cause damage.", "However, if the model, being unconfident, evaded that action, it would have been better off not touching the TV and avoiding the damage at the expense of a sad Juliet.", "An uncertainty-aware model would let the agent know where it has a high chance of an undesired outcome \u2014 where it needs to be more careful. But if the model is unconfident about what will result after taking action, then it\u2019s probably good to use that to reach its goal.", "If our robot is confident that pickles calm baby Juliet while posing no risk, then it might consider running to the kitchen and letting her chew on one, because then, it will achieve its goal of keeping her happy.", "A model that can get accurate estimates of its uncertainty gives the model-based planner ability to avoid actions with a non-slight chance of resulting in undesired outcomes. Gradually, the model will learn to make better estimates. Uncertainty awareness will also inform the model on states it needs to explore more.", "We know entropy as a measure of randomness, or the degree of spread in a probability distribution of a random variable.", "The entropy will peak when each of the outcomes (x\u1d62) occur with equal probability, i.e., maximum uncertainty, and will be minimum when there is a single outcome with high probability p(almost 1), while the rest share a probability close to zero(1 \u2014 p), i.e., maximum certainty.", "Here\u2019s the catch, though \u2014 does uncertainty in the data affect our model uncertainty? To help phrase that question better, here are two plots:", "You are about to see visual plots of hand-cooked data. Just focus on the concept we shall use them to convey rather than what they represent, okay? Thanks.", "These two plots will help us separate two related concepts. Between them, on which one do you think the is model uncertain? Grab an easy moment to observe them again, and try coming up with an answer.", "Have you picked one? Here, let me give you some space to think.", "In the first plot, the model fits a linear regression (LR) to a few thousand samples. It seems certain about the data. Over-fitting shows certainty, correct? Granted, the model is confident about the data. The variance of the red line prediction will pretty much be close to zero. But does this estimate give the best explanation about the observations?", "To achieve this, the model would need to explain the observations by creating a good linear relation between the data and the possible covariates. But being unsure which to include and which to omit, it includes (almost) everything! This effects model uncertainty as we are not confident about the model.", "In the Polynomial Regression (PR), there\u2019s noise in the data. It creates uncertainty in the observations. The samples show significant deviation from the mean, but the model fits relatively well \u2014 we can still snatch a few laughs from a psychology class without providing milk, which seems true enough. Compared to the error received by over-fitting to the Linear function, this would give a larger MSE.", "So coming back to our initial question, uncertainty in the data will not reveal our model-uncertainty. This makes entropy, as a measure of model uncertainty, not always work because when the variance of the data is close to zero, the entropy is low even when the model might still be uncertain, as seen in the LR plot.", "a) Learning a function that predicts bad behaviour", "Consider drone learning to fly in a rain-forest. We wish it to learn to navigate that environment while avoiding collision with trees. In RL, an agent learns the consequence of an action by trying out that action. So to learn to dodge trees, it must experience a couple of hits. But high-speed blows would certainly cause destruction.", "We can train the drone by letting it experience gentle, low-speed hits, so it learns the forest environment. When it encounters a section of the forest absent in the training distribution, it needs knowledge about the uncertainty of its policy to enable safe interaction with that section while collecting new training data. Once confident about that section, it can fly at high speeds in future. This is an example of safe exploration.", "To achieve this, we integrate a cost for hitting trees in the RL cost function c(st, at) to have c(st, at) + C_bad. C_bad is the new cost assigned to behaviour that results in bad behaviour (collision). It influences when the drone can fly fast, and when it should tread with care.", "To estimate C_bad, we use a bad-behaviour-prediction neural network P, with weights \u03f4. It takes as input the current state st of the drone, it\u2019s observation o\u209c plus a sequence of actions [a\u209c, a\u209c \u208a\u2081\u2026 aH] the drone plans to execute and estimates the probability of a collision occurring.", "The action sequence is selected and optimized by Model Predictive Control (MPC) in a receding time horizon from the current time step t up to t + H. The bad-behaviour model P\u03f4 outputs a Bernoulli distribution (binary 0 or 1) indicating whether a collision occurred within this horizon.", "The collision-labels are recorded for each horizon H. This means that for a label 1, bad behaviour occurred in the sub-sequence between time steps t and t + H. With this probability label conditioned on the above inputs, the bad-behaviour model can be simply expressed as:", "Similarly, a naive implementation would look like this:", "However, you might have noticed that P\u03f4 outputs the probability distribution over bad behaviour, and not actually the expense for that behaviour. So the actual bad-behaviour cost would be multiplied by this probability p to give pC_bad. Finally, we tune it with a scalar \u03bb, that determines how important it is for the agent to avoid risky outcomes compared to achieving its goal.", "It\u2019s good to note, while we want a function that predicts unsafe actions, a discriminative model, which takes an input and gives a safety estimate might not always make us happy \u2014 its predictions might be quite meaningless in unfamiliar states. Preferably, it\u2019s beneficial to incorporate model uncertainty in its predictions.", "A neural network can be termed as a conditional model P(y|x, w), which given an input x, allocates a probability to each possible output y using weights w. With Bayesian neural networks, instead of having a single output for each neuron, the weights are denoted as probability distributions over the possible values.", "Using a set of training samples D, we find a posterior on the model-weights, conditioned on these samples P (w|D). To predict the distribution of a particular label \u00fd, each viable combination of the weights, scaled by the posterior distribution, makes a prediction on the same input x.", "If a unit is uncertain about the observation, this will be expressed in the output as weights with higher uncertainty introduce more variability in the prediction. This is common in regions the model has seen minimal or no data and will encourage exploration. As more observations are made, the model makes more deterministic decisions.", "The posterior-distribution on the weights P (w|D) is approximated. This is done by trying to find a parameter \u03f4 of a different distribution on the weights q(w| \u03f4) by making it as close as possible to the true posterior distribution P (w| D). This is variational inference; a little beyond our current scope :).", "Dropout in RL is a bad idea. This isn\u2019t the risk we take here, though. Remember we said a discriminative model will not always make us happy unless it can incorporate uncertainty in bad-behaviour predictions? Dropout is a simple way to do that.", "Dropout is a regularization technique that randomly drops a unit in a neural network with probability p, or retains it with probability 1 \u2014 p. It\u2019s frequently used during training to prevent neurons from over-depending on each other. This creates a new but related neural network during each training iteration.", "In practice, dropout is known to be applied only during training and removed at test time to achieve high test accuracy. However, by retaining dropout at test time, we can estimate uncertainty by finding the sample mean and variance of different forward-passes. It\u2019s a simple approach to estimate uncertainty.", "Its caveat is that dropout, as a variational inference method, underestimates uncertainty severely owing to the variational lower bound.", "To understand this, we need to introduce KL Divergence \u2014 a measure of the difference between two probability distributions over the same random variable.", "At times, finding the true probability over large real-valued distributions is expensive. So an approximation to that distribution is used instead, and the KL divergence (difference) between the two minimized.", "In the above illustration, q(x) is an approximation to the precise distribution p(x). This approximation aims to place a high chance of occurrence where p(x) has a high probability. On the illustration, notice q(x) is a single Gaussian, while p(x) is a mixture of two Gaussians? To place high probability where the probability of p(x) is high, q(x) evens the two Gaussians in p to place high probability mass on both, equally.", "Similarly, dropout has a true posterior p(w| x, y) on the model\u2019s weights w conditioned on the inputs x and the labels y. q(w) is used as an approximating distribution on this posterior. We then lower the KL divergence between q(w) and the actual posterior p(w| x, y) to make them as close as possible. However, doing so will penalise q(w) for placing probability mass where p(w) has no probability mass but just ignores q(w) for not placing high probability mass where p(w) actually has a high probability. This is what underestimates the model\u2019s uncertainty.", "Multiple independent models are trained, and their predictions averaged. Should these models approximate an almost similar output, it would show they agree, indicating certainty in their predictions.", "To make the models independent of each other, each model\u2019s weights \u03f4\u1d62 is trained with a subset of data sampled with replacement from the training set. However, random initialisation of the weights \u03f4\u1d62 and stochastic gradient descent during training is known to make them independent enough.", "Dropout, as a measure of uncertainty, can be assumed a cheap approximation to an ensemble method, where each sampled dropout acts as a different model. The Bayesian Neural Network has the ensemble concept too \u2014 by taking an expectation under the posterior distribution on the weights P(w|D), the Bayesian network becomes equivalent to an infinite number of ensembles \u2014 many means better.", "Think of curiosity as an inspiration to solve for uncertainties in the agent\u2019s environment. Let\u2019s see how we can add curious behaviour to an agent\u2019s control loop.", "In RL, a Linear Quadratic Regulator (LQR) outputs a linear controller which is used to exploit the model. When working with non-linear dynamics, we fit the model p(s\u209c \u208a\u2081 | s\u209c, a\u209c) at each time step using linear regression. This iterative control process is called iterative LQR (iLQR), a form of differential dynamic programming (DDP).", "The system dynamics is represented by the equation:", "f represents the learned dynamics model while x\u209c \u208a\u2081 is the state at the next time step, expressed as the current state x\u209c plus the model\u2019s predicted change on the current state x\u209c when action u\u209c is taken. For instance, if the state is a robot\u2019s velocity, x\u209c would be the current velocity, while f(x\u209c, u\u209c)\u0394t would be the predicted change when u\u209c is selected, resulting in a new velocity x\u209c \u208a\u2081.", "For the integration of uncertainty in the above system dynamics, it\u2019s written as a Gaussian distribution, represented by a mean vector, \u03bc, and a covariance matrix, \u03a3.", "A Gaussian Policy has a neural network mapping a state and action pair to the mean change in state. This change in state is the mean vector expressed by the model \u03bc(f).", "We can implement the system dynamics as a Gaussian Process (GP) by drawing the model f from a normal distribution, where we attempt to learn the best mean vector \u03bc that minimizes the cost function. The GP then delivers predictions using the equation:", "where f(x\u209c, u\u209c) is the mean vector represented by the trainable dynamics function, and \u03a3\u209c \u208a\u2081 the covariance matrix of the GP predictions at the current state and action.", "This GP is identical to an ordinary non-curious LQR stochastic dynamics equations. What\u2019s different? In non-curious iLQR, we would ignore the variance parameter \u03a3\u209c \u208a\u2081 owing to symmetry of Gaussians. However, curious iLQR needs the covariance of the predictive distribution to ascertain the model uncertainty. High model uncertainty equals high variance. \u03a3\u209c \u208a\u2081 represents the model\u2019s uncertainty on the prediction x\u209c \u208a\u2081 at current state and action (x\u209c, u\u209c).", "This uncertainty from the GP model is then used to plead with the agent to take actions that resolve the model\u2019s future-uncertainty on such states. In short, the agent is encouraged to select actions that reduce the model\u2019s variance. This is done by rewarding the agent for acts that include some degree of uncertainty while still maximizing the goal-specific reward.", "Understanding LQR optimization in Model-based RL can a bit juggling but is essential for grasping how the curiosity algorithm is derived. Let\u2019s not scare ourselves with those equations now though.", "Rewarding curious actions enables the agent to reach its goal faster than using standard iLQR. It prevents the model from getting stuck in local optima, which finds better solutions in a short time.", "Uncertainty awareness can be used to influence an agent\u2019s learned policy depending on how it\u2019s added to the training cost. It can encourage exploration or pessimistic behaviour where the agent avoids outcomes likely to be risky. The latter is of interest in AI safety. For more robustness, the solutions to adding uncertainty awareness can be used together.", "[1] A. Nagabandi, G. Kahn, S. Fearing, S. Levine, Neural Network Dynamics for Model-Based Deep Reinforcement Learning with Model-Free Fine-Tuning (2018), ICRA 2018.", "[4] Y. Gal and Z. Ghahramani. Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning. ICML, 2016.", "[5] Y. Li and Y. Gal. Dropout Inference in Bayesian Neural Networks with Alpha-divergences, 2017.", "[6] Model-Based Reinforcement Learning, Deep RL Decision making and Control (2019), Berkley.", "[7] G. Kahn, A. Villaflor, V. Pong, S. Levine. Uncertainty-Aware Reinforcement Learning for Collision Avoidance, 2017.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Fascinated by bread | Learning to smile | RL & Decision control"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fc95c25c220d3&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funcertainty-aware-reinforcement-learning-c95c25c220d3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funcertainty-aware-reinforcement-learning-c95c25c220d3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funcertainty-aware-reinforcement-learning-c95c25c220d3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funcertainty-aware-reinforcement-learning-c95c25c220d3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----c95c25c220d3--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c95c25c220d3--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@mugoh?source=post_page-----c95c25c220d3--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@mugoh?source=post_page-----c95c25c220d3--------------------------------", "anchor_text": "mugoh mwaura"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1bcb39f5cedf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funcertainty-aware-reinforcement-learning-c95c25c220d3&user=mugoh+mwaura&userId=1bcb39f5cedf&source=post_page-1bcb39f5cedf----c95c25c220d3---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc95c25c220d3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funcertainty-aware-reinforcement-learning-c95c25c220d3&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc95c25c220d3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funcertainty-aware-reinforcement-learning-c95c25c220d3&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/uncertainty-aware-reinforcement-learning-c95c25c220d3?", "anchor_text": ""}, {"url": "https://unsplash.com/@randyfath", "anchor_text": "Randy Fath"}, {"url": "https://unsplash.com/photos/osXl4JI3vcQ", "anchor_text": "Unsplash"}, {"url": "https://en.wikipedia.org/wiki/Latent_variable", "anchor_text": "latent variables"}, {"url": "https://raw.githubusercontent.com/mugoh/curious/master/20200628_191322-01.jpeg", "anchor_text": "good babies can do that"}, {"url": "https://en.wikipedia.org/wiki/Unimodality", "anchor_text": "unimodal"}, {"url": "https://en.wikipedia.org/wiki/Gaussian_function", "anchor_text": "Gaussian distribution"}, {"url": "https://en.m.wikipedia.org/wiki/Measurement_uncertainty", "anchor_text": "uncertainty in the observations"}, {"url": "https://en.wikipedia.org/wiki/Model_predictive_control", "anchor_text": "Model Predictive Control"}, {"url": "https://en.wikipedia.org/wiki/Bernoulli_distribution", "anchor_text": "Bernoulli distribution"}, {"url": "https://en.wikipedia.org/wiki/Discriminative_model", "anchor_text": "discriminative model"}, {"url": "https://arxiv.org/pdf/1505.05424v2.pdf", "anchor_text": "source"}, {"url": "https://en.wikipedia.org/wiki/Posterior_probability", "anchor_text": "posterior"}, {"url": "https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence", "anchor_text": "as close as possible"}, {"url": "https://medium.com/@jonathan_hui/machine-learning-variational-inference-273d8e6480bb", "anchor_text": "variational inference"}, {"url": "https://ai.stackexchange.com/a/8295", "anchor_text": "bad idea"}, {"url": "http://jmlr.org/proceedings/papers/v48/gal16.pdf", "anchor_text": "underestimates uncertainty severely"}, {"url": "https://en.wikipedia.org/wiki/Evidence_lower_bound", "anchor_text": "the variational lower bound"}, {"url": "http://www.deeplearningbook.org/", "anchor_text": "source"}, {"url": "https://en.wikipedia.org/wiki/Sampling_%28statistics%29#Replacement_of_selected_units", "anchor_text": "sampled with replacement"}, {"url": "https://en.wikipedia.org/wiki/Expected_value", "anchor_text": "expectation"}, {"url": "https://en.wikipedia.org/wiki/Normal_distribution", "anchor_text": "Gaussian distribution"}, {"url": "https://math.stackexchange.com/a/544268", "anchor_text": "symmetry of Gaussians"}, {"url": "https://arxiv.org/pdf/1904.06786.pdf", "anchor_text": "source"}, {"url": "https://medium.com/@jonathan_hui/rl-lqr-ilqr-linear-quadratic-regulator-a5de5104c750", "anchor_text": "LQR optimization in Model-based RL"}, {"url": "https://arxiv.org/pdf/1708.02596", "anchor_text": "Neural Network Dynamics for Model-Based Deep Reinforcement Learning with Model-Free Fine-Tuning"}, {"url": "https://arxiv.org/pdf/1602.04450v1.pdf", "anchor_text": "Introspective Perception: Learning to Predict Failures in Vision Systems"}, {"url": "https://arxiv.org/pdf/1505.05424v2.pdf", "anchor_text": "Weight Uncertainty in Neural Networks"}, {"url": "http://jmlr.org/proceedings/papers/v48/gal16.pdf", "anchor_text": "Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning"}, {"url": "https://arxiv.org/pdf/1703.02914.pdf", "anchor_text": "Dropout Inference in Bayesian Neural Networks with Alpha-divergences"}, {"url": "http://rail.eecs.berkeley.edu/deeprlcourse/", "anchor_text": "Deep RL Decision making and Control"}, {"url": "https://arxiv.org/pdf/1702.01182v1.pdf", "anchor_text": "Uncertainty-Aware Reinforcement Learning for Collision Avoidance"}, {"url": "https://arxiv.org/pdf/1904.06786.pdf", "anchor_text": "Curious iLQR"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----c95c25c220d3---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----c95c25c220d3---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/model-based-rl?source=post_page-----c95c25c220d3---------------model_based_rl-----------------", "anchor_text": "Model Based Rl"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----c95c25c220d3---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/editors-pick?source=post_page-----c95c25c220d3---------------editors_pick-----------------", "anchor_text": "Editors Pick"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc95c25c220d3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funcertainty-aware-reinforcement-learning-c95c25c220d3&user=mugoh+mwaura&userId=1bcb39f5cedf&source=-----c95c25c220d3---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc95c25c220d3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funcertainty-aware-reinforcement-learning-c95c25c220d3&user=mugoh+mwaura&userId=1bcb39f5cedf&source=-----c95c25c220d3---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc95c25c220d3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funcertainty-aware-reinforcement-learning-c95c25c220d3&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c95c25c220d3--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fc95c25c220d3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funcertainty-aware-reinforcement-learning-c95c25c220d3&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----c95c25c220d3---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----c95c25c220d3--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----c95c25c220d3--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----c95c25c220d3--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----c95c25c220d3--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----c95c25c220d3--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----c95c25c220d3--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----c95c25c220d3--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----c95c25c220d3--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@mugoh?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@mugoh?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "mugoh mwaura"}, {"url": "https://medium.com/@mugoh/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "43 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1bcb39f5cedf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funcertainty-aware-reinforcement-learning-c95c25c220d3&user=mugoh+mwaura&userId=1bcb39f5cedf&source=post_page-1bcb39f5cedf--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F1bcb39f5cedf%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funcertainty-aware-reinforcement-learning-c95c25c220d3&user=mugoh+mwaura&userId=1bcb39f5cedf&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}