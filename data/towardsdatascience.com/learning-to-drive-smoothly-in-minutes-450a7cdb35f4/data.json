{"url": "https://towardsdatascience.com/learning-to-drive-smoothly-in-minutes-450a7cdb35f4", "time": 1682994800.9697149, "path": "towardsdatascience.com/learning-to-drive-smoothly-in-minutes-450a7cdb35f4/", "webpage": {"metadata": {"title": "Learning to Drive Smoothly in Minutes | by Antonin RAFFIN | Towards Data Science", "h1": "Learning to Drive Smoothly in Minutes", "description": "Train a smooth control policy for an autonomous car in minutes, using Soft Actor-Critic (SAC) and a VAE"}, "outgoing_paragraph_urls": [{"url": "https://wayve.ai/", "anchor_text": "Wayve.ai", "paragraph_index": 0}, {"url": "https://github.com/araffin/learning-to-drive-in-5-minutes", "anchor_text": "associated GitHub repository", "paragraph_index": 1}, {"url": "https://www.youtube.com/watch?v=ngK33h00iBE&list=PL42jkf1t1F7dFXE7f0VTeFLhW0ZEQ4XJV", "anchor_text": "https://www.youtube.com/watch?v=ngK33h00iBE&list=PL42jkf1t1F7dFXE7f0VTeFLhW0ZEQ4XJV", "paragraph_index": 2}, {"url": "https://diyrobocars.com/", "anchor_text": "DIY Robocars", "paragraph_index": 3}, {"url": "http://toulouse-robot-race.org/", "anchor_text": "Toulouse Robot Race", "paragraph_index": 3}, {"url": "http://www.ironcar.org/", "anchor_text": "Iron Car,", "paragraph_index": 3}, {"url": "http://www.donkeycar.com/", "anchor_text": "Donkey Car", "paragraph_index": 4}, {"url": "https://github.com/tawnkramer/sdsandbox/tree/donkey", "anchor_text": "unity simulator", "paragraph_index": 4}, {"url": "https://becominghuman.ai/autonomous-racing-robot-with-an-arduino-a-raspberry-pi-and-a-pi-camera-3e72819e1e63", "anchor_text": "previous blog post", "paragraph_index": 7}, {"url": "https://spinningup.openai.com/en/latest/", "anchor_text": "reinforcement learning", "paragraph_index": 13}, {"url": "https://flyyufelix.github.io/2018/09/11/donkey-rl-simulation.html", "anchor_text": "previous approaches", "paragraph_index": 14}, {"url": "https://wayve.ai/blog/learning-to-drive-in-a-day-with-reinforcement-learning", "anchor_text": "Wayve.ai", "paragraph_index": 16}, {"url": "https://github.com/araffin/robotics-rl-srl", "anchor_text": "State Representation Learning (SRL)", "paragraph_index": 18}, {"url": "https://openreview.net/forum?id=Hkl-di09FQ", "anchor_text": "research topic", "paragraph_index": 18}, {"url": "https://arxiv.org/pdf/1802.04181.pdf", "anchor_text": "is not the only solution", "paragraph_index": 19}, {"url": "https://github.com/araffin/srl-zoo", "anchor_text": "inverse dynamics model", "paragraph_index": 19}, {"url": "https://stable-baselines.readthedocs.io/en/master/modules/ddpg.html", "anchor_text": "Deep Deterministic Policy Gradient (DDPG)", "paragraph_index": 20}, {"url": "https://xbpeng.github.io/projects/DeepMimic/index.html", "anchor_text": "Deep Mimic", "paragraph_index": 21}, {"url": "https://colab.research.google.com/drive/1mF2abRb_yi4UNqYXVBF-t4FuCy6fl1c1#scrollTo=9bIR_N7R11XI", "anchor_text": "Colab notebook", "paragraph_index": 25}, {"url": "https://github.com/araffin/srl-zoo", "anchor_text": "explore what the VAE has learned", "paragraph_index": 26}, {"url": "https://arxiv.org/abs/1709.06560", "anchor_text": "unstable", "paragraph_index": 27}, {"url": "https://stable-baselines.readthedocs.io/en/master/modules/sac.html", "anchor_text": "Soft Actor-Critic", "paragraph_index": 27}, {"url": "https://stable-baselines.readthedocs.io/en/master/modules/ppo2.html", "anchor_text": "PPO", "paragraph_index": 28}, {"url": "https://github.com/hill-a/stable-baselines", "anchor_text": "stable-baselines", "paragraph_index": 29}, {"url": "https://bair.berkeley.edu/blog/2018/12/14/sac/", "anchor_text": "latest improvements", "paragraph_index": 29}, {"url": "http://robotics.sciencemag.org/content/4/26/eaau5872", "anchor_text": "researchers from ETH Zurich", "paragraph_index": 40}, {"url": "https://www.youtube.com/watch?v=6JUjDw9tfD4", "anchor_text": "reproduced by Roma Sokolkov on a real RC car", "paragraph_index": 50}, {"url": "https://github.com/r7vme/learning-to-drive-in-a-day", "anchor_text": "Roma Sokolkov", "paragraph_index": 51}, {"url": "https://github.com/tawnkramer", "anchor_text": "Tawn Kramer", "paragraph_index": 51}, {"url": "https://flyyufelix.github.io/2018/09/11/donkey-rl-simulation.html", "anchor_text": "Felix Yu", "paragraph_index": 51}, {"url": "https://github.com/hardmaru/WorldModelsExperiments", "anchor_text": "David Ha", "paragraph_index": 51}, {"url": "https://github.com/hill-a/stable-baselines", "anchor_text": "Stable-Baselines", "paragraph_index": 51}, {"url": "https://github.com/araffin/rl-baselines-zoo", "anchor_text": "model zoo", "paragraph_index": 51}, {"url": "https://github.com/sergionr2/RacingRobot", "anchor_text": "Racing Robot project", "paragraph_index": 51}, {"url": "https://github.com/araffin/robotics-rl-srl", "anchor_text": "S-RL Toolbox", "paragraph_index": 51}, {"url": "https://araffin.github.io/", "anchor_text": "https://araffin.github.io/", "paragraph_index": 62}], "all_paragraphs": ["In this post, we will see how to train an autonomous racing car in minutes and how to smooth its control. The method, based on Reinforcement Learning (RL) and presented here in simulation (Donkey Car simulator), was designed to be applicable in the real world. It builds on the work of a startup named Wayve.ai that focuses on autonomous driving.", "The code and simulator used in this article are open source and public. Please check the associated GitHub repository for more information ;) (pre-trained controllers are also available for download)", "April 2022 Update: I did a series of videos on learning to race with reinforcement learning: https://www.youtube.com/watch?v=ngK33h00iBE&list=PL42jkf1t1F7dFXE7f0VTeFLhW0ZEQ4XJV", "Since DIY Robocars creation few years ago, numerous autonomous racing car competitions exist now (e.g. Toulouse Robot Race, Iron Car, \u2026). In those, the goal is simple: you have a racing car and it must go as fast as possible while staying on a track, given only an image from its on-board camera as input.", "Self-driving challenges are a good way to get into robotics. To facilitate learning, the Donkey Car, an open source self-driving platform was developed. In its ecosystem, there is now a unity simulator featuring that small robot. We will be testing the proposed approach on this Donkey Car.", "After briefly reviewing the different methods used in small autonomous car competitions, we will present what is reinforcement learning and then go into the details of our approach.", "Before presenting RL, we will first quickly review what are the different solutions currently used in the RC car competitions.", "In a previous blog post, I\u2019ve described a first approach to drive autonomously, that combines computer vision and a PID controller. Although the idea is simple and applicable to many settings, it requires manual labeling of data (to tell the car where the center of the track is) which is costly and exhausting (trust me, manual labeling is not fun!).", "As an other approach, lots of competitors use supervised learning to reproduce a human driver behavior. For that, a human needs to drive manually the car during several laps, recording camera image and associated control input from the joystick. Then, a model is trained to reproduce the human driving. However, this technique is not really robust, requires homogeneous driving and retraining for each track, because it generalizes quite badly.", "In view of the above issues, reinforcement learning (RL) appears to be an interesting alternative.", "In a reinforcement learning setting, an agent (or robot) acts on its environment and receives a reward as feedback. It can be a positive reward (the robot did something good) or negative reward (the robot should be penalized).", "The goal of the robot is to maximize the cumulative reward. To do so, it learns, through interaction with the world, what is called a policy (or behavior/controller) that maps its sensory input to actions.", "In our case, the input is the camera image and the actions are the throttle and steering angle. So if we model the reward in a way that the car stays on the track and maximizes its velocity, we\u2019re done!", "That is the beauty of reinforcement learning, you need very little assumption (here only designing a reward function) and it will optimize directly what you want (go fast on the track to win the race!).", "Note: This is not the first blog post about reinforcement learning on a small self-driving car, but compared to previous approaches, the presented technique takes only minutes (and not hours) to learn a good and smooth control policy (~5 to 10 minutes for a smooth controller, ~20 minutes for a very smooth one).", "Now that we have briefly presented what is RL, we will go into the details, starting by dissecting the Wayve.ai approach, the base of our method.", "Wayve.ai describes a method to train a self-driving car in the real world on a simple road. This method is composed of several key elements.", "First, they train a feature extractor (here a Variational Auto-Encoder or VAE) to compress the image to a lower dimensional space. The model is trained to reconstruct the input image but contains a bottleneck that forces it to compress the information.", "This step of extracting relevant information from raw data is called State Representation Learning (SRL), and was my main research topic. That notably allows to reduce the search space and therefore accelerate training. Below is a diagram that shows the connection between SRL and end-to-end reinforcement learning, that is to say, learn directly the control policy from pixels.", "Note: training an auto-encoder is not the only solution to extract useful features, you can also train for instance an inverse dynamics model.", "The second key element is the use of an RL algorithm named Deep Deterministic Policy Gradient (DDPG), which learns a control policy using the VAE features as input. This policy is updated after each episode. One important aspect of the algorithm is that it has a memory, called replay buffer, where its interactions with its environment are recorded and can be \u201creplayed\u201d afterward. So, even when the car does not interact with the world, it can sample experience from this buffer to update its policy.", "The car is trained to maximize the number of meters traveled before human intervention. And that is the final key ingredient: the human operator ends the episode as soon as the car starts going off the road. This early termination is really important (as shown by Deep Mimic) and prevents the car from exploring regions that are not interesting to solve the task.", "Until now, nothing new was presented, we have only summarized Wayve.ai approach. Below are all the modifications I made to the base technique.", "Although Wayve.ai technique may work in principle, it has some issues that needs to be addressed to apply it to a self-driving RC car.", "First, because the feature extractor (VAE) is trained after each episode, the distribution of features is not stationary. That is to say, the features are changing over time and can lead to instabilities in the policy training. Also, training a VAE on a laptop (without a GPU) is quite slow, so we would like to avoid retraining the VAE after each episode.", "To address those two problems, I decided to train a VAE beforehand and used Google Colab notebook to preserve my computer. That way, the policy is trained with a fixed feature extractor.", "In the image below, we explore what the VAE has learned. We navigate in its latent space (using the sliders) and observe the reconstructed image.", "Then, DDPG is known to be unstable (in the sense that its performance can drop catastrophically during training) and is quite hard to tune. Fortunately, a recent algorithm named Soft Actor-Critic (SAC) has equivalent performances and is much easier to tune*.", "*during my experiments, I tried PPO, SAC and DDPG. DDPG and SAC were giving the best results in few episodes but SAC was simpler to tune.", "For this project, I used the Soft Actor-Critic (SAC) implementation I wrote for stable-baselines (if you are working with RL, I definitely recommend you to take a look ;) ), that has the latest improvements of the algorithm in it.", "Finally, I updated the reward function and the action space to smooth the control and maximize the speed.", "The robot car does not have any odometry (nor speed sensor), so the number of meters traveled (nor speed) cannot be used as a reward.", "Therefore, I decided to give a \u201clife bonus\u201d at each time-step (i.e., a +1 reward for staying on the track) and penalize the robot, using a \u201ccrash penalty\u201d (-10 reward) for leaving the track. Additionally, I found it beneficial to also punish the car for getting off the road too fast: an additional negative reward proportional to the throttle is added to the crash penalty.", "Finally, because we want to go fast as it is a racing car, I added a \u201cthrottle bonus\u201d proportional to the current throttle. That way, the robot will try to stay on the track and maximize its speed at the same time.", "where w1 and w2 are just constant that allows to balance the objectives (with w1 << 10 and w2 << 1 because they are secondary objectives)", "The world is not really stochastic. If you\u2019ve noticed \u2014 a robot doesn\u2019t just spontaneously start shaking. Unless you hook up an RL algorithm to it. \u2014 Emo Todorov", "If you apply the presented approach so far, it will work: the car will stay on the track and will try to go fast. However, you will probably end up with a shaky control: the car will oscillate as shown in the above image, because it has no incentive not to do so, it just tries to maximize its reward.", "The solution to smooth the control is to constrain the change in steering angle while augmenting the input with the history of previous commands (steering and throttle). That way, you impose continuity in the steering.", "As an example, if the current car steering angle is 0\u00b0 and it tries suddenly to steer at 90\u00b0, the continuity constrain will only allow it to steer at 40\u00b0 for instance. Hence, the difference between two consecutive steering commands stays in a given range. This additional constrain comes at a cost of a little more training.", "I passed several days trying to solve that issue before finding a satisfying solution, so here is what I tried that did not work:", "Note: recently, researchers from ETH Zurich suggested to use curriculum learning to have continuous and energy-efficient control. This could be a second solution (although a bit harder to tune).", "In our approach, we decouple policy learning from feature extraction and add an additional constrain to smooth the control.", "First, a human collects data by driving manually the car (10k images in ~5 minutes of manual driving). Those images are used to train a VAE.", "Then, we alternate between exploration episodes (a stochastic policy is used) and policy training (done when the human puts the car back on the track to optimize the time spent).", "For training the policy, the images are first encoded using a VAE (here with a latent space of dimension 64) and concatenated with an history of the last ten actions taken (throttle and steering) creating a 84D feature vector.", "The control policy is represented by a neural network (2 fully-connected layers of 32 and 16 units, with ReLU or ELU activation function).", "This controller outputs steering angle and throttle. We constrain the throttle to be in a given range and also limits the difference between current and previous steering angle.", "In this article, we have presented an approach to learn a smooth control policy for the Donkey Car in minutes, using only a camera.", "As this method is designed to be applied in the real world, this is definitely my next step in this project: test the approach on a real RC car* (see below). This will require to shrink the VAE model (the policy network is already quite small) in order to make it run on a raspberry pi.", "That\u2019s all for today, don\u2019t hesitate to test the code, comment or ask questions, and remember, sharing is caring ;)!", "*the wayve.ai approach was reproduced by Roma Sokolkov on a real RC car, however that does not include the latest improvements for smooth control", "This work would not have been possible without Roma Sokolkov\u2019s re-implementation of Wayve.ai approach, Tawn Kramer\u2019s Donkey Car simulator, Felix Yu\u2019s blog post for inspiration, David Ha for his VAE implementation, Stable-Baselines and its model zoo for SAC implementation and training scripts, the Racing Robot project for the teleoperation and the S-RL Toolbox for VAE debugging and training tools.", "I would like also to thanks Roma, Sebastian, Tawn, Florence, Johannes, Jonas, Gabriel, Alvaro, Arthur and Sergio for the feedbacks.", "Influence of the latent space dimension and number of samples", "The latent space dimension of the VAE just need to be big enough so the VAE manages to reconstruct the important part of the input image. For instance, there were no huge differences in the resulting control policy between a 64D and 512D VAE.", "One important thing is not really the number of samples but rather the diversity and the representativeness of samples. If your training images does not cover all the environment diversity, then you need more samples.", "Can we learn a control policy from random features?", "I tried to fix the weights of the VAE right after initialization and then learn a policy on those random features. However, that did not work.", "I did not have the time (because my laptop does not have a GPU) to compare the approach from learning the policy directly from pixels. However, I\u2019ll be interested in the results if someone could do that using my codebase.", "What is the minimal policy that works?", "A one-layer mlp works. I tried also with a linear policy, however, I did not succeed to obtain a good controller.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Research Engineer in Robotics and Machine Learning https://araffin.github.io/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F450a7cdb35f4&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-to-drive-smoothly-in-minutes-450a7cdb35f4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-to-drive-smoothly-in-minutes-450a7cdb35f4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-to-drive-smoothly-in-minutes-450a7cdb35f4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-to-drive-smoothly-in-minutes-450a7cdb35f4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----450a7cdb35f4--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----450a7cdb35f4--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@araffin?source=post_page-----450a7cdb35f4--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@araffin?source=post_page-----450a7cdb35f4--------------------------------", "anchor_text": "Antonin RAFFIN"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2409d35123af&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-to-drive-smoothly-in-minutes-450a7cdb35f4&user=Antonin+RAFFIN&userId=2409d35123af&source=post_page-2409d35123af----450a7cdb35f4---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F450a7cdb35f4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-to-drive-smoothly-in-minutes-450a7cdb35f4&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F450a7cdb35f4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-to-drive-smoothly-in-minutes-450a7cdb35f4&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://wayve.ai/", "anchor_text": "Wayve.ai"}, {"url": "https://github.com/araffin/learning-to-drive-in-5-minutes", "anchor_text": "associated GitHub repository"}, {"url": "https://github.com/DLR-RM/stable-baselines3", "anchor_text": "Stable-Baselines3"}, {"url": "https://github.com/araffin/aae-train-donkeycar/releases/tag/live-twitch-2", "anchor_text": "https://github.com/araffin/aae-train-donkeycar/releases/tag/live-twitch-2"}, {"url": "https://www.youtube.com/watch?v=ngK33h00iBE&list=PL42jkf1t1F7dFXE7f0VTeFLhW0ZEQ4XJV", "anchor_text": "https://www.youtube.com/watch?v=ngK33h00iBE&list=PL42jkf1t1F7dFXE7f0VTeFLhW0ZEQ4XJV"}, {"url": "https://github.com/araffin/learning-to-drive-in-5-minutes", "anchor_text": "araffin/learning-to-drive-in-5-minutesImplementation of reinforcement learning approach to make a car learn to drive smoothly in minutes - araffin/learning-to-drive-in-5-minutesgithub.com"}, {"url": "https://diyrobocars.com/", "anchor_text": "DIY Robocars"}, {"url": "http://toulouse-robot-race.org/", "anchor_text": "Toulouse Robot Race"}, {"url": "http://www.ironcar.org/", "anchor_text": "Iron Car,"}, {"url": "http://www.donkeycar.com/", "anchor_text": "Donkey Car"}, {"url": "https://github.com/tawnkramer/sdsandbox/tree/donkey", "anchor_text": "unity simulator"}, {"url": "https://becominghuman.ai/autonomous-racing-robot-with-an-arduino-a-raspberry-pi-and-a-pi-camera-3e72819e1e63", "anchor_text": "Autonomous Racing Robot With an Arduino, a Raspberry Pi and a Pi Camera"}, {"url": "https://becominghuman.ai/autonomous-racing-robot-with-an-arduino-a-raspberry-pi-and-a-pi-camera-3e72819e1e63", "anchor_text": "previous blog post"}, {"url": "https://www.youtube.com/watch?v=xhI71ZdSh6k", "anchor_text": "Predicting where is the center of the track"}, {"url": "https://github.com/hill-a/stable-baselines", "anchor_text": "Stable-Baselines: an easy to use reinforcement learning library"}, {"url": "https://spinningup.openai.com/en/latest/", "anchor_text": "reinforcement learning"}, {"url": "https://flyyufelix.github.io/2018/09/11/donkey-rl-simulation.html", "anchor_text": "previous approaches"}, {"url": "https://wayve.ai/blog/learning-to-drive-in-a-day-with-reinforcement-learning", "anchor_text": "Wayve.ai"}, {"url": "https://www.youtube.com/watch?v=eRwTbRtnT1I", "anchor_text": "Wayve.ai approach"}, {"url": "https://github.com/araffin/robotics-rl-srl", "anchor_text": "State Representation Learning (SRL)"}, {"url": "https://openreview.net/forum?id=Hkl-di09FQ", "anchor_text": "research topic"}, {"url": "https://arxiv.org/pdf/1802.04181.pdf", "anchor_text": "is not the only solution"}, {"url": "https://github.com/araffin/srl-zoo", "anchor_text": "inverse dynamics model"}, {"url": "https://openreview.net/forum?id=Hkl-di09FQ", "anchor_text": "Decoupling Feature Extraction from Policy Learning"}, {"url": "https://stable-baselines.readthedocs.io/en/master/modules/ddpg.html", "anchor_text": "Deep Deterministic Policy Gradient (DDPG)"}, {"url": "https://xbpeng.github.io/projects/DeepMimic/index.html", "anchor_text": "Deep Mimic"}, {"url": "https://colab.research.google.com/drive/1mF2abRb_yi4UNqYXVBF-t4FuCy6fl1c1#scrollTo=9bIR_N7R11XI", "anchor_text": "Colab notebook"}, {"url": "https://github.com/araffin/srl-zoo", "anchor_text": "explore what the VAE has learned"}, {"url": "https://github.com/araffin/srl-zoo", "anchor_text": "Exploring the latent space"}, {"url": "https://arxiv.org/abs/1709.06560", "anchor_text": "unstable"}, {"url": "https://stable-baselines.readthedocs.io/en/master/modules/sac.html", "anchor_text": "Soft Actor-Critic"}, {"url": "https://stable-baselines.readthedocs.io/en/master/modules/ppo2.html", "anchor_text": "PPO"}, {"url": "https://github.com/hill-a/stable-baselines", "anchor_text": "stable-baselines"}, {"url": "https://bair.berkeley.edu/blog/2018/12/14/sac/", "anchor_text": "latest improvements"}, {"url": "https://github.com/hill-a/stable-baselines", "anchor_text": "hill-a/stable-baselinesA fork of OpenAI Baselines, implementations of reinforcement learning algorithms - hill-a/stable-baselinesgithub.com"}, {"url": "https://danieltakeshi.github.io/2016/11/25/frame-skipping-and-preprocessing-for-deep-q-networks-on-atari-2600-games/", "anchor_text": "stack several frames"}, {"url": "http://robotics.sciencemag.org/content/4/26/eaau5872", "anchor_text": "researchers from ETH Zurich"}, {"url": "https://www.youtube.com/watch?v=6JUjDw9tfD4", "anchor_text": "reproduced by Roma Sokolkov on a real RC car"}, {"url": "https://github.com/r7vme/learning-to-drive-in-a-day", "anchor_text": "Roma Sokolkov"}, {"url": "https://github.com/tawnkramer", "anchor_text": "Tawn Kramer"}, {"url": "https://flyyufelix.github.io/2018/09/11/donkey-rl-simulation.html", "anchor_text": "Felix Yu"}, {"url": "https://github.com/hardmaru/WorldModelsExperiments", "anchor_text": "David Ha"}, {"url": "https://github.com/hill-a/stable-baselines", "anchor_text": "Stable-Baselines"}, {"url": "https://github.com/araffin/rl-baselines-zoo", "anchor_text": "model zoo"}, {"url": "https://github.com/sergionr2/RacingRobot", "anchor_text": "Racing Robot project"}, {"url": "https://github.com/araffin/robotics-rl-srl", "anchor_text": "S-RL Toolbox"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----450a7cdb35f4---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----450a7cdb35f4---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/self-driving-cars?source=post_page-----450a7cdb35f4---------------self_driving_cars-----------------", "anchor_text": "Self Driving Cars"}, {"url": "https://medium.com/tag/autonomous-cars?source=post_page-----450a7cdb35f4---------------autonomous_cars-----------------", "anchor_text": "Autonomous Cars"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----450a7cdb35f4---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "http://creativecommons.org/licenses/by/4.0/", "anchor_text": "Some rights reserved"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F450a7cdb35f4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-to-drive-smoothly-in-minutes-450a7cdb35f4&user=Antonin+RAFFIN&userId=2409d35123af&source=-----450a7cdb35f4---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F450a7cdb35f4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-to-drive-smoothly-in-minutes-450a7cdb35f4&user=Antonin+RAFFIN&userId=2409d35123af&source=-----450a7cdb35f4---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F450a7cdb35f4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-to-drive-smoothly-in-minutes-450a7cdb35f4&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----450a7cdb35f4--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F450a7cdb35f4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-to-drive-smoothly-in-minutes-450a7cdb35f4&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----450a7cdb35f4---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----450a7cdb35f4--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----450a7cdb35f4--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----450a7cdb35f4--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----450a7cdb35f4--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----450a7cdb35f4--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----450a7cdb35f4--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----450a7cdb35f4--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----450a7cdb35f4--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@araffin?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@araffin?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Antonin RAFFIN"}, {"url": "https://medium.com/@araffin/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "449 Followers"}, {"url": "https://araffin.github.io/", "anchor_text": "https://araffin.github.io/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2409d35123af&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-to-drive-smoothly-in-minutes-450a7cdb35f4&user=Antonin+RAFFIN&userId=2409d35123af&source=post_page-2409d35123af--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fbf4aff8f2752&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-to-drive-smoothly-in-minutes-450a7cdb35f4&newsletterV3=2409d35123af&newsletterV3Id=bf4aff8f2752&user=Antonin+RAFFIN&userId=2409d35123af&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}