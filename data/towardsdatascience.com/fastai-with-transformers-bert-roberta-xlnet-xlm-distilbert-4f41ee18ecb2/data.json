{"url": "https://towardsdatascience.com/fastai-with-transformers-bert-roberta-xlnet-xlm-distilbert-4f41ee18ecb2", "time": 1683001637.2891352, "path": "towardsdatascience.com/fastai-with-transformers-bert-roberta-xlnet-xlm-distilbert-4f41ee18ecb2/", "webpage": {"metadata": {"title": "Fastai with \ud83e\udd17Transformers (BERT, RoBERTa, XLNet, XLM, DistilBERT) | by Maximilien Roberti | Towards Data Science", "h1": "Fastai with \ud83e\udd17Transformers (BERT, RoBERTa, XLNet, XLM, DistilBERT)", "description": "In early 2018, Jeremy Howard (co-founder of fast.ai) and Sebastian Ruder introduced the Universal Language Model Fine-tuning for Text Classification (ULMFiT) method. ULMFiT was the first Transfer\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/u/34ab754f8c5e?source=post_page-----4f41ee18ecb2--------------------------------", "anchor_text": "Jeremy Howard", "paragraph_index": 0}, {"url": "https://medium.com/u/e3999e445181?source=post_page-----4f41ee18ecb2--------------------------------", "anchor_text": "Sebastian Ruder", "paragraph_index": 0}, {"url": "https://arxiv.org/pdf/1801.06146.pdf", "anchor_text": "Universal Language Model Fine-tuning for Text Classification", "paragraph_index": 0}, {"url": "https://course.fast.ai/videos/?lesson=4", "anchor_text": "fast.ai course", "paragraph_index": 1}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "Attention is all you need", "paragraph_index": 2}, {"url": "https://huggingface.co/", "anchor_text": "Hugging Face", "paragraph_index": 3}, {"url": "https://github.com/huggingface/transformers", "anchor_text": "transformers", "paragraph_index": 3}, {"url": "https://github.com/huggingface/transformers", "anchor_text": "library", "paragraph_index": 3}, {"url": "https://www.kaggle.com/maroberti/fastai-with-transformers-bert-roberta", "anchor_text": "Kaggle kernel", "paragraph_index": 8}, {"url": "https://github.com/fastai/fastai/blob/master/README.md#installation", "anchor_text": "here", "paragraph_index": 9}, {"url": "https://github.com/huggingface/transformers#installation", "anchor_text": "here", "paragraph_index": 9}, {"url": "https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/overview", "anchor_text": "Movie Reviews", "paragraph_index": 12}, {"url": "https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/data", "anchor_text": "dataset", "paragraph_index": 13}, {"url": "https://www.kaggle.com/maroberti/fastai-with-transformers-bert-roberta", "anchor_text": "Notebook", "paragraph_index": 13}, {"url": "https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification", "anchor_text": "BertForSequenceClassification", "paragraph_index": 18}, {"url": "https://huggingface.co/transformers/model_doc/bert.html#berttokenizer", "anchor_text": "BertTokenizer", "paragraph_index": 18}, {"url": "https://huggingface.co/transformers/model_doc/bert.html#bertconfig", "anchor_text": "BertConfig", "paragraph_index": 18}, {"url": "https://huggingface.co/transformers/pretrained_models.html#pretrained-models", "anchor_text": "here", "paragraph_index": 19}, {"url": "https://huggingface.co/transformers/", "anchor_text": "Hugging Face documentation", "paragraph_index": 29}, {"url": "https://docs.fast.ai/text.data.html#NumericalizeProcessor", "anchor_text": "NumericalizeProcessor", "paragraph_index": 31}, {"url": "https://docs.fast.ai/text.data.html#NumericalizeProcessor", "anchor_text": "object", "paragraph_index": 31}, {"url": "https://docs.fast.ai/text.transform.html#Vocab", "anchor_text": "Vocab", "paragraph_index": 31}, {"url": "https://docs.fast.ai/text.transform.html#Vocab", "anchor_text": "object", "paragraph_index": 31}, {"url": "https://docs.fast.ai/basic_train.html#Learner.export", "anchor_text": "export", "paragraph_index": 33}, {"url": "https://docs.fast.ai/basic_train.html#load_learner", "anchor_text": "load_learner", "paragraph_index": 33}, {"url": "https://github.com/huggingface/transformers#models-always-output-tuples", "anchor_text": "here", "paragraph_index": 37}, {"url": "https://mlexplained.com/2019/05/13/a-tutorial-to-fine-tuning-bert-with-fast-ai/", "anchor_text": "Keita Kurita\u2019s article", "paragraph_index": 38}, {"url": "https://docs.fast.ai/basic_train.html#Discriminative-layer-training", "anchor_text": "here", "paragraph_index": 40}, {"url": "https://docs.fast.ai/callbacks.one_cycle.html", "anchor_text": "here", "paragraph_index": 47}, {"url": "https://docs.fast.ai/callbacks.lr_finder.html#callbacks.lr_finder", "anchor_text": "lr_find", "paragraph_index": 48}, {"url": "https://mlexplained.com/2019/05/13/a-tutorial-to-fine-tuning-bert-with-fast-ai/", "anchor_text": "article", "paragraph_index": 54}, {"url": "https://www.kaggle.com/maroberti/fastai-with-transformers-bert-roberta", "anchor_text": "Kaggle example", "paragraph_index": 55}, {"url": "https://mlexplained.com/2019/05/13/a-tutorial-to-fine-tuning-bert-with-fast-ai/", "anchor_text": "A Tutorial to Fine-Tuning BERT with Fast AI", "paragraph_index": 60}], "all_paragraphs": ["In early 2018, Jeremy Howard (co-founder of fast.ai) and Sebastian Ruder introduced the Universal Language Model Fine-tuning for Text Classification (ULMFiT) method. ULMFiT was the first Transfer Learning method applied to NLP. As a result, besides significantly outperforming many state-of-the-art tasks, it allowed, with only 100 labeled examples, to match performances equivalent to models trained on 100\u00d7 more data.", "The first time I heard about ULMFiT was during a fast.ai course given by Jeremy Howard. He demonstrated how it is easy \u2014 thanks to the fastai library \u2014 to implement the complete ULMFiT method with only a few lines of codes. In his demo, he used an AWD-LSTM neural network pre-trained on Wikitext-103 and get rapidly state-of-the-art results. He also explained key techniques \u2014 also demonstrated in ULMFiT \u2014 to fine-tune models like Discriminate Learning Rate, Gradual Unfreezing or Slanted Triangular Learning Rates.", "Since the introduction of ULMFiT, Transfer Learning became very popular in NLP and yet Google (BERT, Transformer-XL, XLNet), Facebook (RoBERTa, XLM) and even OpenAI (GPT, GPT-2) begin to pre-train their own model on very large corpora. This time, instead of using the AWD-LSTM neural network, they all used a more powerful architecture based on the Transformer (cf. Attention is all you need).", "Although these models are powerful, fastai do not integrate all of them. Fortunately, Hugging Face \ud83e\udd17 created the well know transformers library. Formerly known as pytorch-transformers or pytorch-pretrained-bert, this library brings together over 40 state-of-the-art pre-trained NLP models (BERT, GPT-2, RoBERTa, CTRL\u2026). The implementation gives interesting additional utilities like tokenizer, optimizer or scheduler.", "The transformers library can be self-sufficient but incorporating it within the fastai library provides simpler implementation compatible with powerful fastai tools like Discriminate Learning Rate, Gradual Unfreezing or Slanted Triangular Learning Rates. The point here is to allow anyone \u2014 expert or non-expert \u2014 to get easily state-of-the-art results and to \u201cmake NLP uncool again\u201d.", "It is worth noting that integrating the Hugging Face transformers library in fastai has already been demonstrated in:", "Although these articles are of high quality, some part of their demonstration is not anymore compatible with the last version of transformers.", "Before beginning the implementation, note that integrating transformers within fastai can be done in multiple ways. For that reason, I brought \u2014 what I think are \u2014 the most generic and flexible solutions. More precisely, I tried to make the minimum modification in both libraries while making them compatible with the maximum amount of transformer architectures. However, if you find a clever way to make this implementation, please let us know in the comment section!", "A Jupiter Notebook version of this tutorial is available on this Kaggle kernel.", "First, you will need to install the fastai and transformers libraries. To do so, just follow the instructions here and here.", "For this demonstration, I used Kaggle which already has the fastai library installed. So I just installed transformers with the command :", "The versions of the libraries used for this demonstration are fastai 1.0.58 and transformers 2.1.1.", "The chosen task is a multi-class text classification on Movie Reviews.", "The dataset and the respective Notebook of this article can be found on Kaggle.", "For each text movie review, the model has to predict a label for the sentiment. We evaluate the outputs of the model on classification accuracy. The sentiment labels are:", "0 \u2192Negative1 \u2192Somewhat negative2 \u2192Neutral3 \u2192Somewhat positive4 \u2192Positive", "The data is loaded into a DataFrame using pandas.", "In transformers, each model architecture is associated with 3 main types of classes:", "For example, if you want to use the BERT architecture for text classification, you would use BertForSequenceClassification for the model class, BertTokenizer for the tokenizer class and BertConfig for the configuration class.", "Later, you will see that those classes share a common class method from_pretrained(pretrained_model_name, ...). In our case, the parameter pretrained_model_name is a string with the shortcut name of a pre-trained model/tokenizer/configuration to load, e.g bert-base-uncased. We can find all the shortcut names in the transformers documentation here.", "In order to switch easily between classes \u2014 each related to a specific model type \u2014 I created a dictionary that allows loading the correct classes by just specifying the correct model type name.", "It is worth noting that in this case, we use the transformers library only for a multi-class text classification task. For that reason, this tutorial integrates only the transformer architectures that have a model for sequence classification implemented. These model types are :", "However, if you want to go further \u2014 by implementing another type of model or NLP task \u2014 this tutorial still an excellent starter.", "To match pre-training, we have to format the model input sequence in a specific format. To do so, you have to first tokenize and then numericalize the texts correctly.The difficulty here is that each pre-trained model, that we will fine-tune, requires exactly the same specific pre-process \u2014 tokenization & numericalization \u2014 than the pre-process used during the pre-train part.Fortunately, the tokenizer class from transformers provides the correct pre-process tools that correspond to each pre-trained model.", "In the fastai library, data pre-processing is done automatically during the creation of the DataBunch. As you will see in the DataBunch implementation part, the tokenizer and the numericalizer are passed in the processor argument under the following format :", "Let\u2019s first analyze how we can integrate the transformers tokenizer within the TokenizeProcessor function.", "This part can be a little confusing because a lot of classes are wrapped in each other and with similar names.To resume, if we look attentively at the fastai implementation, we notice that :", "Therefore, we can simply create a new class TransformersBaseTokenizer that inherits from BaseTokenizer and overwrite a new tokenizer function.", "In this implementation, be careful about 3 things:", "Below, you can find the resume of each pre-process requirement for the 5 model types used in this tutorial. You can also find this information on the Hugging Face documentation in each model section.", "It is worth noting that we don\u2019t add padding in this part of the implementation. As we will see later, fastai manage it automatically during the creation of the DataBunch.", "In fastai, NumericalizeProcessor object takes as vocab argument a Vocab object.From this analyze, I suggest two ways to adapt the fastai numericalizer:", "Even if the first solution seems to be simpler, Transformers does not provide, for all models, a straightforward way to retrieve his list of tokens. Therefore, I implemented the second solution, which runs for each model type. It consists of using the functions convert_tokens_to_ids and convert_ids_to_tokens in respectively numericalize and textify.", "NB: The functions __gestate__ and __setstate__ allow the functions export and load_learner to work correctly with TranformersVocab.", "Now that we have our custom tokenizer and numericalizer, we can create the custom processor. Notice we are passing the include_bos = False and include_eos = False options. This is because fastai adds its own special tokens by default which interferes with the [CLS] and [SEP] tokens added by our custom tokenizer.", "For the DataBunch creation, you have to pay attention to set the processor argument to our new custom processor transformer_processor and manage correctly the padding.", "As mentioned in the Hugging Face documentation, BERT, RoBERTa, XLM, and DistilBERT are models with absolute position embeddings, so it\u2019s usually advised to pad the inputs on the right rather than the left. Regarding XLNET, it is a model with relative position embeddings, therefore, you can either pad the inputs on the right or on the left.", "As mentioned here, every model's forward method always outputs a tuple with various elements depending on the model and the configuration parameters. In our case, we are interested to access only to the logits. One way to access them is to create a custom model.", "To make our transformers adapted to multiclass classification, before loading the pre-trained model, we need to precise the number of labels. To do so, you can modify the config instance or either modify like in Keita Kurita\u2019s article (Section: Initializing the Learner) the num_labels argument.", "In pytorch-transformers, Hugging Face had implemented two specific optimizers \u2014 BertAdam and OpenAIAdam \u2014 that have been replaced by a single AdamW optimizer.This optimizer matches Pytorch Adam optimizer Api, therefore, it becomes straightforward to integrate it within fastai.Note that for reproducing BertAdam specific behavior, you have to set correct_bias = False.", "To use Discriminative Learning Rate and Gradual Unfreezing, fastai provides one tool that allows to \u201csplit\u201d the structure model into groups. An instruction to perform that \u201csplit\u201d is described in the fastai documentation here.", "Unfortunately, the model architectures are too different to create a unique generic function that can \u201csplit\u201d all the model types in a convenient way. Thereby, you will have to implement a custom \u201csplit\u201d for each different model architecture.", "For example, if we use the DistilBERT model and that we observe his architecture by making print(learner.model). We can decide to divide the model in 8 blocks :", "In this case, we can split our model in this way:", "Note that I didn\u2019t found any document that has studied the influence of Discriminative Learning Rate and Gradual Unfreezing or even Slanted Triangular Learning Rates with transformer architectures. Therefore, using these tools does not guarantee better results. If you found any interesting documents, please let us know in the comment.", "Now we can finally use all the fastai build-in features to train our model. Like the ULMFiT method, we will use Slanted Triangular Learning Rates, Discriminate Learning Rate and gradually unfreeze the model.", "Therefore, we first freeze all the groups but the classifier with :", "For Slanted Triangular Learning Rates you have to use the function fit_one_cycle. For more information, please check the fastai documentation here.", "To use our fit_one_cycle we will need an optimum learning rate. We can find this learning rate by using a learning rate finder, which can be called by using lr_find. Our graph would look something like this:", "We will pick a value a bit before the minimum, where the loss still improves. Here 2x10\u20133 seems to be a good value.", "The graph of the loss would look like this:", "We then unfreeze the second group and repeat the operations until all the groups are unfrozen. If you want to use Discriminative Learning Rate you can use slice as follow :", "To unfreeze all the groups, use learner.unfreeze() .", "Now that we have trained the model, we want to generate predictions from the test dataset.", "As specified in Keita Kurita\u2019s article, as the function get_preds does not return elements in order by default, you will have to resort the elements into their correct order.", "In the Kaggle example, without playing too much with the parameters, we get a Public Score of 0.70059, which leads us to the 5th position on the leaderboard!", "In this article, I explain how to combine the transformers library with the beloved fastai library. It aims to make you understand where to look and modify both libraries to make them work together. Likely, it allows you to use Slanted Triangular Learning Rates, Discriminate Learning Rate and even Gradual Unfreezing. As a result, without even tunning the parameters, you can obtain rapidly state-of-the-art results.", "This year, the transformers became an essential tool for NLP. Because of that, I think that pre-trained transformers architectures will be integrated soon to future versions of fastai. Meanwhile, this tutorial is a good starter.", "I hope you enjoyed this first article and found it useful. Thanks for reading and don\u2019t hesitate in leaving questions or suggestions.", "I will keep writing articles on NLP so stay tuned!", "[4] Keita Kurita , A Tutorial to Fine-Tuning BERT with Fast AI (May 2019)", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Machine Learning Engineer passionate about cutting-edge technology and solving real-world problems."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F4f41ee18ecb2&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffastai-with-transformers-bert-roberta-xlnet-xlm-distilbert-4f41ee18ecb2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffastai-with-transformers-bert-roberta-xlnet-xlm-distilbert-4f41ee18ecb2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffastai-with-transformers-bert-roberta-xlnet-xlm-distilbert-4f41ee18ecb2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffastai-with-transformers-bert-roberta-xlnet-xlm-distilbert-4f41ee18ecb2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----4f41ee18ecb2--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4f41ee18ecb2--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@maximilienroberti?source=post_page-----4f41ee18ecb2--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@maximilienroberti?source=post_page-----4f41ee18ecb2--------------------------------", "anchor_text": "Maximilien Roberti"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F29c8f5cf1dc4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffastai-with-transformers-bert-roberta-xlnet-xlm-distilbert-4f41ee18ecb2&user=Maximilien+Roberti&userId=29c8f5cf1dc4&source=post_page-29c8f5cf1dc4----4f41ee18ecb2---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4f41ee18ecb2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffastai-with-transformers-bert-roberta-xlnet-xlm-distilbert-4f41ee18ecb2&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4f41ee18ecb2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffastai-with-transformers-bert-roberta-xlnet-xlm-distilbert-4f41ee18ecb2&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/u/34ab754f8c5e?source=post_page-----4f41ee18ecb2--------------------------------", "anchor_text": "Jeremy Howard"}, {"url": "https://medium.com/u/e3999e445181?source=post_page-----4f41ee18ecb2--------------------------------", "anchor_text": "Sebastian Ruder"}, {"url": "https://arxiv.org/pdf/1801.06146.pdf", "anchor_text": "Universal Language Model Fine-tuning for Text Classification"}, {"url": "https://arxiv.org/abs/1801.06146", "anchor_text": "Howard and Ruder, ACL 2018"}, {"url": "https://course.fast.ai/videos/?lesson=4", "anchor_text": "fast.ai course"}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "Attention is all you need"}, {"url": "https://huggingface.co/", "anchor_text": "Hugging Face"}, {"url": "https://github.com/huggingface/transformers", "anchor_text": "transformers"}, {"url": "https://github.com/huggingface/transformers", "anchor_text": "library"}, {"url": "https://github.com/huggingface/transformers", "anchor_text": "huggingface/transformersState-of-the-art Natural Language Processing for TensorFlow 2.0 and PyTorch \ud83e\udd17 Transformers (formerly known as\u2026github.com"}, {"url": "https://mlexplained.com/2019/05/13/a-tutorial-to-fine-tuning-bert-with-fast-ai/", "anchor_text": "A Tutorial to Fine-Tuning BERT with Fast AI"}, {"url": "https://medium.com/u/795cbf38e25d?source=post_page-----4f41ee18ecb2--------------------------------", "anchor_text": "Dev Sharma"}, {"url": "https://medium.com/analytics-vidhya/using-roberta-with-fastai-for-nlp-7ed3fed21f6c", "anchor_text": "Using RoBERTa with Fastai for NLP"}, {"url": "https://www.kaggle.com/maroberti/fastai-with-transformers-bert-roberta", "anchor_text": "Kaggle kernel"}, {"url": "https://github.com/fastai/fastai/blob/master/README.md#installation", "anchor_text": "here"}, {"url": "https://github.com/huggingface/transformers#installation", "anchor_text": "here"}, {"url": "https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/overview", "anchor_text": "Movie Reviews"}, {"url": "https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/data", "anchor_text": "dataset"}, {"url": "https://www.kaggle.com/maroberti/fastai-with-transformers-bert-roberta", "anchor_text": "Notebook"}, {"url": "https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/data", "anchor_text": "Sentiment Analysis on Movie ReviewsDownload Open Datasets on 1000s of Projects + Share Projects on One Platform. Explore Popular Topics Like Government\u2026www.kaggle.com"}, {"url": "https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification", "anchor_text": "BertForSequenceClassification"}, {"url": "https://huggingface.co/transformers/model_doc/bert.html#berttokenizer", "anchor_text": "BertTokenizer"}, {"url": "https://huggingface.co/transformers/model_doc/bert.html#bertconfig", "anchor_text": "BertConfig"}, {"url": "https://huggingface.co/transformers/pretrained_models.html#pretrained-models", "anchor_text": "here"}, {"url": "https://docs.fast.ai/text.data.html#TokenizeProcessor", "anchor_text": "TokenizeProcessor"}, {"url": "https://docs.fast.ai/text.data.html#TokenizeProcessor", "anchor_text": "object"}, {"url": "https://docs.fast.ai/text.transform.html#Tokenizer", "anchor_text": "Tokenizer"}, {"url": "https://docs.fast.ai/text.transform.html#Tokenizer", "anchor_text": "object"}, {"url": "https://docs.fast.ai/text.transform.html#BaseTokenizer", "anchor_text": "BaseTokenizer"}, {"url": "https://docs.fast.ai/text.transform.html#BaseTokenizer", "anchor_text": "object"}, {"url": "https://huggingface.co/transformers/", "anchor_text": "Hugging Face documentation"}, {"url": "https://docs.fast.ai/text.data.html#NumericalizeProcessor", "anchor_text": "NumericalizeProcessor"}, {"url": "https://docs.fast.ai/text.data.html#NumericalizeProcessor", "anchor_text": "object"}, {"url": "https://docs.fast.ai/text.transform.html#Vocab", "anchor_text": "Vocab"}, {"url": "https://docs.fast.ai/text.transform.html#Vocab", "anchor_text": "object"}, {"url": "https://medium.com/analytics-vidhya/using-roberta-with-fastai-for-nlp-7ed3fed21f6c", "anchor_text": "Dev Sharma\u2019s article"}, {"url": "https://docs.fast.ai/basic_train.html#Learner.export", "anchor_text": "export"}, {"url": "https://docs.fast.ai/basic_train.html#load_learner", "anchor_text": "load_learner"}, {"url": "https://github.com/huggingface/transformers#models-always-output-tuples", "anchor_text": "here"}, {"url": "https://mlexplained.com/2019/05/13/a-tutorial-to-fine-tuning-bert-with-fast-ai/", "anchor_text": "Keita Kurita\u2019s article"}, {"url": "https://docs.fast.ai/basic_train.html#Discriminative-layer-training", "anchor_text": "here"}, {"url": "https://docs.fast.ai/callbacks.one_cycle.html", "anchor_text": "here"}, {"url": "https://docs.fast.ai/callbacks.lr_finder.html#callbacks.lr_finder", "anchor_text": "lr_find"}, {"url": "https://mlexplained.com/2019/05/13/a-tutorial-to-fine-tuning-bert-with-fast-ai/", "anchor_text": "article"}, {"url": "https://www.kaggle.com/maroberti/fastai-with-transformers-bert-roberta", "anchor_text": "Kaggle example"}, {"url": "https://github.com/huggingface/transformers", "anchor_text": "https://github.com/huggingface/transformers"}, {"url": "https://docs.fast.ai/text.html", "anchor_text": "https://docs.fast.ai/text.html"}, {"url": "https://arxiv.org/abs/1801.06146", "anchor_text": "https://arxiv.org/abs/1801.06146"}, {"url": "https://mlexplained.com/2019/05/13/a-tutorial-to-fine-tuning-bert-with-fast-ai/", "anchor_text": "A Tutorial to Fine-Tuning BERT with Fast AI"}, {"url": "https://medium.com/u/795cbf38e25d?source=post_page-----4f41ee18ecb2--------------------------------", "anchor_text": "Dev Sharma"}, {"url": "https://medium.com/analytics-vidhya/using-roberta-with-fastai-for-nlp-7ed3fed21f6c", "anchor_text": "Using RoBERTa with Fastai for NLP"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----4f41ee18ecb2---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/fastai?source=post_page-----4f41ee18ecb2---------------fastai-----------------", "anchor_text": "Fastai"}, {"url": "https://medium.com/tag/transformers?source=post_page-----4f41ee18ecb2---------------transformers-----------------", "anchor_text": "Transformers"}, {"url": "https://medium.com/tag/nlp?source=post_page-----4f41ee18ecb2---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----4f41ee18ecb2---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://creativecommons.org/licenses/by-sa/4.0/", "anchor_text": "Some rights reserved"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4f41ee18ecb2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffastai-with-transformers-bert-roberta-xlnet-xlm-distilbert-4f41ee18ecb2&user=Maximilien+Roberti&userId=29c8f5cf1dc4&source=-----4f41ee18ecb2---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4f41ee18ecb2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffastai-with-transformers-bert-roberta-xlnet-xlm-distilbert-4f41ee18ecb2&user=Maximilien+Roberti&userId=29c8f5cf1dc4&source=-----4f41ee18ecb2---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4f41ee18ecb2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffastai-with-transformers-bert-roberta-xlnet-xlm-distilbert-4f41ee18ecb2&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4f41ee18ecb2--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F4f41ee18ecb2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffastai-with-transformers-bert-roberta-xlnet-xlm-distilbert-4f41ee18ecb2&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----4f41ee18ecb2---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----4f41ee18ecb2--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----4f41ee18ecb2--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----4f41ee18ecb2--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----4f41ee18ecb2--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----4f41ee18ecb2--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----4f41ee18ecb2--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----4f41ee18ecb2--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----4f41ee18ecb2--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@maximilienroberti?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@maximilienroberti?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Maximilien Roberti"}, {"url": "https://medium.com/@maximilienroberti/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "208 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F29c8f5cf1dc4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffastai-with-transformers-bert-roberta-xlnet-xlm-distilbert-4f41ee18ecb2&user=Maximilien+Roberti&userId=29c8f5cf1dc4&source=post_page-29c8f5cf1dc4--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fd1ca9f09813f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffastai-with-transformers-bert-roberta-xlnet-xlm-distilbert-4f41ee18ecb2&newsletterV3=29c8f5cf1dc4&newsletterV3Id=d1ca9f09813f&user=Maximilien+Roberti&userId=29c8f5cf1dc4&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}