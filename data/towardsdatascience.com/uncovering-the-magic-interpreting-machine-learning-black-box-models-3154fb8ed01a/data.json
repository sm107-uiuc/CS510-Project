{"url": "https://towardsdatascience.com/uncovering-the-magic-interpreting-machine-learning-black-box-models-3154fb8ed01a", "time": 1683011746.5813608, "path": "towardsdatascience.com/uncovering-the-magic-interpreting-machine-learning-black-box-models-3154fb8ed01a/", "webpage": {"metadata": {"title": "Uncovering the Magic: interpreting Machine Learning black-box models | Towards Data Science", "h1": "Uncovering the Magic: interpreting Machine Learning black-box models", "description": "Have you ever developed a machine learning model with a great accuracy and an awesome AUC, but had no clue about how to explain the predictions?"}, "outgoing_paragraph_urls": [{"url": "https://christophm.github.io/interpretable-ml-book/", "anchor_text": "Interpretable Machine Learning", "paragraph_index": 2}, {"url": "https://github.com/fpretto/interpretable_and_fair_ml", "anchor_text": "GitHub", "paragraph_index": 3}, {"url": "https://www.kaggle.com/uciml/adult-census-income", "anchor_text": "Adult Census Income", "paragraph_index": 4}, {"url": "https://towardsdatascience.com/be-careful-when-interpreting-your-features-importance-in-xgboost-6e16132588e7", "anchor_text": "here", "paragraph_index": 10}, {"url": "https://github.com/fpretto/interpretable_and_fair_ml", "anchor_text": "GitHub", "paragraph_index": 62}], "all_paragraphs": ["The trade-off between predictive power and interpretability is a common issue to face when working with black-box models, especially in business environments where results have to be explained to non-technical audiences. Interpretability is crucial to being able to question, understand, and trust AI and ML systems. It also provides data scientists and engineers better means for debugging models and ensuring that they are working as intended.", "This tutorial aims to present different techniques for approaching model interpretation in black-box models.", "Disclaimer: this article seeks to introduce some useful techniques from the field of interpretable machine learning to the average data scientist and to motivate its adoption . Most of them have been summarized from this highly recommendable book from Christoph Molnar: Interpretable Machine Learning.", "The entire code used in this article can be found in my GitHub", "The dataset used for this article is the Adult Census Income from UCI Machine Learning Repository. The prediction task is to determine whether a person makes over $50K a year.", "Since the focus of this article is not centered in the modelling phase of the ML pipeline, minimum feature engineering was performed in order to model the data with an XGBoost.", "The performance metrics obtained for the model are the following:", "The model\u2019s performance seems to be pretty acceptable.", "The techniques used to evaluate the global behavior of the model will be:", "3.1 - Feature Importance (evaluated by the XGBoost model and by SHAP)3.2 - Summary Plot (SHAP)3.3 - Permutation Importance (ELI5)3.4 - Partial Dependence Plot (PDPBox and SHAP)3.5 - Global Surrogate Model (Decision Tree and Logistic Regression)", "When working with XGBoost, one must be careful when interpreting features importances, since the results might be misleading. This is because the model calculates several importance metrics, with different interpretations. It creates an importance matrix, which is a table with the first column including the names of all the features actually used in the boosted trees, and the other with the resulting \u2018importance\u2019 values calculated with different metrics (Gain, Cover, Frequence). A more thourough explanation of these can be found here.", "The Gain is the most relevant attribute to interpret the relative importance (i.e. improvement in accuracy) of each feature.", "In general, SHAP library is considered to be a model-agnostic tool for addressing interpretability (we will cover SHAP\u2019s intuition in the Local Importance section). However, the library has a model-specific method for tree-based machine learning models such as decision trees, random forests and gradient boosted trees.", "The XGBoost feature importance was used to evaluate the relevance of the predictors in the model\u2019s outputs for the Train dataset and the SHAP one to evaluate it for Test dataset, in order to assess if the most important features were similar in both approaches and sets.", "It is observed that the most important variables of the model are maintained, although in different order of importance (age seems to take much more relevance in the test set by SHAP approach).", "The SHAP Summary Plot is a very interesting plot to evaluate the features of the model, since it provides more information than the traditional Feature Importance:", "Another way to assess the global importance of the predictors is to randomly permute the order of the instances for each feature in the dataset and predict with the trained model. If by doing this disturbance in the order, the evaluation metric does not change substantially, then the feature is not so relevant. If instead the evaluation metric is affected, then the feature is considered important in the model. This process is done individually for each feature.", "To evaluate the trained XGBoost model, the Area Under the Curve (AUC) of the ROC Curve will be used as the performance metric. Permutation Importance will be analyzed in both Train and Test:", "Even though the order of the most important features changes, it looks like that the most relevant ones remain the same. It is interesting to note that, unlike the XGBoost Feature Importance, the age variable in the Train set has a fairly strong effect (as showed by SHAP Feature Importance in the Test set). Furthermore, the 6 most important variables according to the Permutation Importance are kept in Train and Test (the difference in order may be due to the distribution of each sample).", "The coherence between the different approaches to approximate the global importance generates more confidence in the interpretation of the model\u2019s output.", "The Partial Dependence Plot (PDP) indicates the marginal effect that a feature has individually on the predicted output. For this, the feature is modified, ceteris paribus, and the changes in the mean prediction are observed. The process carried out is as follows:", "1) Select feature2) Define grid of values3) For each value of the grid: 3.1) Replace feature with grid value 3.2) Average predictions4) Plot curve", "The PDP can indicate if the relationship between the feature and the output is linear, monotonic or if it is more complex. It is relevant to note that the observed relationship is with the prediction, not with the target variable. However, depending on the performance of the model, an intuition of the dependence of the target for the evaluated feature could be generated.", "The advantage of PDP is that it is very easy to implement and it is quite intuitive: the function in a particular feature represents the average prediction if all data points are forced to assume each particular value.", "We will analyze Partial Dependence Plots using PDPBox and SHAP.", "As an example, the PDP for 2 of the most relevant observed features will be analyzed:", "It looks like there is a linear relationship between the years of education (from 7 years onwards) and the probability of earning more than $50K. The impact of this feature in the model\u2019s output proved to be as high as 0.6 (out of 1).", "It seems that people are more likely to earn more than $50K in their 50\u2019s.", "The same PDPs will be generated using the SHAP approach. This library, in addition to indicating the marginal effect the feature has on the model\u2019s output, also indicates by color the relationship with the feature which it most interacts with.", "Even though the y-axis scale is different from the PDPBox plot (we will see why in the Local Interpretability section), the trend for \u201ceducation.num\u201d appears to be the same than in the previous approach. In addition, SHAP has identified that the feature \u201cmarried_1\u201d is the one with which it interacts most (this means that, for the model, married people with a high number of education years are more likely to earn more than $50K).", "The trend for age in this method is consistent with the PDPBox approach. The feature with which it interacts the most is \u201ceducation.num\u201d.", "Having stated the advantages PDP contributes to the interpretability field, it is worth it (and fair) to also present the disadvantages:", "To overcome some of the disadvantages of PDPs, Individual Conditional Expectation (ICE) and Accumulated Local Effects (ALE) plots can be used. Even though the implementations of these methods are not covered in this article, we will briefly explain them to show how they improve the PDP approach.", "ICE Plot is the PDP equivalent for individual data points. The plot displays a line for each instance of the dataset, indicating how the prediction of that instance varies as the value of the feature varies. A PDP is an average of all the lines in an ICE plot. The ICE plots allow to visualize the variance in the marginal effects, being able to detect the heterogeneous effects.", "PDPs present serious problems when a feature is highly correlated with other predictors, since synthetic instance predictions that are very unlikely to happen in reality are averaged (for example, it would be very unlikely that age were 16 and education.num were 10 simultaneously). This can generate a significant bias when estimating the effect of the feature. ALE plots, in addition to being computed more quickly, are an unbiased solution to calculate the effect of a feature on model predictions, since they evaluate over its conditional distribution. This is, for a value x1 of the grid, they estimate using only the predictions of the instances that have a value similar to x1, thus avoiding the use of improbable instances in reality.", "Furthermore, in order to estimate the effect of a feature on the prediction, instead of using the average (which mixes the effect of the feature with the effects of all the correlated predictors), they calculate the differences between predictions.", "The ICE plot solves the problem of heterogeneous effects that PDPs can present, but not the bias due to correlated features. Instead, ALE plot solves bias problems, taking into consideration the conditional distribution of the feature and its correlation with the rest of the predictors.", "A global surrogate model is an interpretable model that is trained to approximate the predictions of a black-box model. We can draw conclusions about the black box model by interpreting the surrogate model. In Christoph Molnar\u2019s words: \u201cSolving machine learning interpretability by using more machine learning!\u201d", "We will try to approximate the XGBoost using a Logistic Regression and a Decision Tree as global surrogate models.", "The R-squared is negative for both Train and Test sets. This happens when the fit is worse than simply using the mean. Therefore, it is concluded that Logistic Regression is not a good surrogate model.", "The variance in the XGBoost model predictions is fairly well approximated by the Decision Tree, so it can serve as a surrogate model for interpreting the main model. In fact, the performance metrics are also quite close to the original model.", "It is important to note that while the variance of the XGBoost predictions is well explained by the Decision Tree, it is not guaranteed that the latter uses the features in the same way as the former. It could happen that the Tree approximates the XGBoost correctly in some areas of the input space, but behaves drastically differently in other regions.", "The resulting tree will be analyzed in order to assess whether the features used correspond to the most important features that have been detected so far:", "The 5 features that the tree used to estimate the Income, in order of importance, are:", "These features correspond to the most important ones that have been detected by the other methodologies.", "Local surrogate models are interpretable models that are used to explain individual predictions of black-box machine learning models.", "LIME analyzes what happens in model predictions when variations are made to the input data. It generates a new dataset with permuted samples and their corresponding predictions from the original model. On this synthetic set LIME trains interpretable models (Logistic Regression, Decision Tree, LASSO Regression, etc.), which are then weighted by the proximity of the sampled instances to the instance of interest.", "The explanation for instance X will be that of the surrogate model that minimizes the loss function (performance measure -e.g. MSE- between the prediction of the surrogate model and the prediction of the original model), keeping the complexity of the model low.", "It is observed that the most influential feature in all individual interpretations that separate the classes is capital.gain. Following this, depending on the instance, the predictors of greatest relevance are married, education.num, age and sex. These features are the same that were identified in the algorithms of global importance.", "4.2 - SHapley Additive exPlanations (SHAP)", "SHAP is a method to explain individual predictions based on the calculation of Shapley Values, a method from coalitional game theory. It seeks to answer the question \u201cHow much has each feature value contributed to the prediction, relative to the average prediction?\u201d. To do this, the Shapley Values assign \u201cpayments\u201d to \u201cplayers\u201d depending on their contribution to the \u201ctotal payment\u201d. Players cooperate in a coalition and receive certain rewards for such cooperation.", "In the machine learning context, the \u201cgame\u201d is the prediction task for an instance of the dataset. The \u201ctotal payment\u201d is the prediction for that instance, minus the average prediction for the entire dataset. The \u201cplayers\u201d are the values of the features for the instance, which cooperate in a coalition to receive the \u201cpayment\u201d (the prediction). The Shapley Value is the average marginal contribution of a feature value for all possible coalitions. It indicates how the \u201ctotal payout\u201d (prediction) is distributed among all \u201cplayers\u201d (the feature values).", "One innovation that SHAP brings to the table is that the Shapley value explanation is represented as an additive feature attribution method, namely a linear model. In this way, SHAP connects the benefits of LIME with the Shapley Values.", "There are several methods for visualizaing SHAP\u2019s explanations. We will cover two of them in this article: Force Plot and Decision Plot.", "The force plot indicates, for each feature, the impact it had on the prediction. There are two relevant values to notice: the output value (model prediction for the instance) and the base value (average prediction for the entire dataset). A bigger bar means a higher impact and the color indicates if the feature value moved the prediction from the base value towards 1 (red) or 0 (blue).", "The Decision Plot shows essentially the same information than the Force Plot. The grey vertical line is the base value and the red line indicates if each feature moved the output value to a higher or lower value than the average prediction.", "This plot can be a little bit more clear and intuitive than the previous one, especially when there are many features to analyze. In the Force Plot the information may look very condensed when the number of predictors is high.", "This article is meant to help data scientists get a better understanding of how their machine learning models work and to be able to explain the results in a clear way. It is also useful for debugging models and ensuring that they are working as intended.", "We have presented different classifications of interpretability methods (intrinsic/post-hoc, model-specific/model-agnostic, local/global) and we used several libraries and techniques for assesing both global and local importance.", "In summary, the libraries and techniques used are:", "So, which is the single best library to address ML model interpretability? In my opinion, the use of several libraries and techniques helps to build credibility on the model\u2019s output (provided than the results are consistent). However, if I had to choose one, I would definitely go for SHAP.", "SHAP had a great contribution to the field of interpretable Machine Learning. This is so because here the global interpretations are consistent with the individual explanations, since the Shapley Values are the \u201catomic unit\u201d of the global interpretations (which have a solid theoretical foundation in Game Theory). If, for example, LIME were used for local explanations and PDP or Permutation Importance for global interpretations, there is no common theoretical foundation between the methods.", "I hope this article serves its purpose as a general guide into cracking black-box models. The entire code can be found in my GitHub", "In my next article I will be addressing model fairness, which has been gaining increasing awareness over the past years. This field aims to assess how fair the model is when treating pre-existing biases in data: is it fair that a job-matching system favors male candidates for CEO interviews, because that matches historical data?", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Industrial Engineer & Data Scientist \u2014 MSc in Business Analytics. Passionate about AI, Data Science, and data-driven decision-making."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F3154fb8ed01a&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funcovering-the-magic-interpreting-machine-learning-black-box-models-3154fb8ed01a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funcovering-the-magic-interpreting-machine-learning-black-box-models-3154fb8ed01a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funcovering-the-magic-interpreting-machine-learning-black-box-models-3154fb8ed01a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funcovering-the-magic-interpreting-machine-learning-black-box-models-3154fb8ed01a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----3154fb8ed01a--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3154fb8ed01a--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://fpretto.medium.com/?source=post_page-----3154fb8ed01a--------------------------------", "anchor_text": ""}, {"url": "https://fpretto.medium.com/?source=post_page-----3154fb8ed01a--------------------------------", "anchor_text": "Fabricio Pretto"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F48aea8e648cc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funcovering-the-magic-interpreting-machine-learning-black-box-models-3154fb8ed01a&user=Fabricio+Pretto&userId=48aea8e648cc&source=post_page-48aea8e648cc----3154fb8ed01a---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3154fb8ed01a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funcovering-the-magic-interpreting-machine-learning-black-box-models-3154fb8ed01a&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3154fb8ed01a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funcovering-the-magic-interpreting-machine-learning-black-box-models-3154fb8ed01a&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.pexels.com/@fauxels?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels", "anchor_text": "fauxels"}, {"url": "https://www.pexels.com/photo/person-using-a-laptop-3183131/?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels", "anchor_text": "Pexels"}, {"url": "https://christophm.github.io/interpretable-ml-book/", "anchor_text": "Interpretable Machine Learning"}, {"url": "https://github.com/fpretto/interpretable_and_fair_ml", "anchor_text": "GitHub"}, {"url": "https://www.kaggle.com/uciml/adult-census-income", "anchor_text": "Adult Census Income"}, {"url": "https://towardsdatascience.com/be-careful-when-interpreting-your-features-importance-in-xgboost-6e16132588e7", "anchor_text": "here"}, {"url": "https://github.com/fpretto/interpretable_and_fair_ml", "anchor_text": "GitHub"}, {"url": "https://medium.com/tag/model-interpretability?source=post_page-----3154fb8ed01a---------------model_interpretability-----------------", "anchor_text": "Model Interpretability"}, {"url": "https://medium.com/tag/interpretability?source=post_page-----3154fb8ed01a---------------interpretability-----------------", "anchor_text": "Interpretability"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----3154fb8ed01a---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/shapley-values?source=post_page-----3154fb8ed01a---------------shapley_values-----------------", "anchor_text": "Shapley Values"}, {"url": "https://medium.com/tag/model-fairness?source=post_page-----3154fb8ed01a---------------model_fairness-----------------", "anchor_text": "Model Fairness"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3154fb8ed01a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funcovering-the-magic-interpreting-machine-learning-black-box-models-3154fb8ed01a&user=Fabricio+Pretto&userId=48aea8e648cc&source=-----3154fb8ed01a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3154fb8ed01a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funcovering-the-magic-interpreting-machine-learning-black-box-models-3154fb8ed01a&user=Fabricio+Pretto&userId=48aea8e648cc&source=-----3154fb8ed01a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3154fb8ed01a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funcovering-the-magic-interpreting-machine-learning-black-box-models-3154fb8ed01a&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3154fb8ed01a--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F3154fb8ed01a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funcovering-the-magic-interpreting-machine-learning-black-box-models-3154fb8ed01a&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----3154fb8ed01a---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----3154fb8ed01a--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----3154fb8ed01a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----3154fb8ed01a--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----3154fb8ed01a--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----3154fb8ed01a--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----3154fb8ed01a--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----3154fb8ed01a--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----3154fb8ed01a--------------------------------", "anchor_text": ""}, {"url": "https://fpretto.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://fpretto.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Fabricio Pretto"}, {"url": "https://fpretto.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "43 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F48aea8e648cc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funcovering-the-magic-interpreting-machine-learning-black-box-models-3154fb8ed01a&user=Fabricio+Pretto&userId=48aea8e648cc&source=post_page-48aea8e648cc--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F568fb7c00ec9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funcovering-the-magic-interpreting-machine-learning-black-box-models-3154fb8ed01a&newsletterV3=48aea8e648cc&newsletterV3Id=568fb7c00ec9&user=Fabricio+Pretto&userId=48aea8e648cc&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}