{"url": "https://towardsdatascience.com/understanding-the-motivation-of-sigmoid-output-units-e2c560d4b2c4", "time": 1683007622.529263, "path": "towardsdatascience.com/understanding-the-motivation-of-sigmoid-output-units-e2c560d4b2c4/", "webpage": {"metadata": {"title": "Understanding the Motivation of Sigmoid Output Units | by In\u00eas Pedro | Towards Data Science", "h1": "Understanding the Motivation of Sigmoid Output Units", "description": "While reading Chapter 6.2.2.2 Sigmoid Units for Bernoulli Output Distributions from the Deep Learning book, some questions arose related to the intuition behind the sigmoid function, which I will try\u2026"}, "outgoing_paragraph_urls": [{"url": "http://www.deeplearningbook.org/", "anchor_text": "Deep Learning", "paragraph_index": 0}, {"url": "https://www.coursera.org/specializations/deep-learning?utm_source=gg&utm_medium=sem&utm_content=17-DeepLearning-ROW&campaignid=6465471773&adgroupid=76541762319&device=c&keyword=python%20neural%20network&matchtype=b&network=g&devicemodel=&adpostion=&creativeid=379493352694&hide_mobile_promo&gclid=CjwKCAjw7-P1BRA2EiwAXoPWA6uTmuQmMQ_IAD0Qj76rUHxQHywuIjsF0Hgssh70c6bRDCP5fek4dxoCxxoQAvD_BwE", "anchor_text": "Andrew Ng\u2019s Deep Learning specialization in Coursera", "paragraph_index": 6}, {"url": "https://onionesquereality.wordpress.com/2016/05/18/where-does-the-sigmoid-in-logistic-regression-come-from/", "anchor_text": "0\u20131 loss can motivate the use of log odds", "paragraph_index": 10}, {"url": "http://deeplearningbook.org/", "anchor_text": "Deep Learning", "paragraph_index": 20}, {"url": "https://alexander-schiendorfer.github.io/2020/02/17/why-so.sigmoid.html", "anchor_text": "blog post", "paragraph_index": 23}], "all_paragraphs": ["While reading Chapter 6.2.2.2 Sigmoid Units for Bernoulli Output Distributions from the Deep Learning book, some questions arose related to the intuition behind the sigmoid function, which I will try to address throughout this post. This article attempts to give a thorough explanation of the chapter mentioned above, by elaborating on the motivation of the sigmoid function and its use on output units.", "Suppose we are trying to solve a supervised binary classification problem with a neural network. Given that the output variable y can only take two values, which we will assume to be 0 and 1, the network only needs to predict P(y=1|x), since the probability of both classes must add to 1. So, the conditional probability is a Bernoulli variable with parameter p=P(y=1|x).", "The output layer will receive h from the previous hidden layer and will compute a linear combination of its input, that we define as", "which is represented in blue in the image below. The output unit must then apply a function f to compute the probability of the input being of class 1, illustrated in yellow. Our goal is to understand how we can define this mapping f.", "Problem: How can we map a real value (a linear combination from the last hidden layer, z) to a probability, i.e., to a number between 0 and 1? In other words, we want to define a function f", "One simple solution would be to only consider the part of z that is between 0 and 1. All the negative values would be mapped to 0, while all the values larger than 1 would be mapped to 1, as plotted below.", "For purposes of simplification, we define \u0177=P(y=1|x), and we let \u2112(\u0177, y) denote the loss function. As shown in the plot above, the derivative of \u0177 is 0 almost everywhere (except between 0 and 1). Below is depicted how the chain rule can be applied to update the weights, where black and red arrows denote forward and backward propagation, respectively (this illustration is inspired by Andrew Ng\u2019s Deep Learning specialization in Coursera). We can see that the fact that \u2202\u0177/\u2202z is 0 implies that all the derivatives that follow (using the red arrows) will also be 0, as the chain rule multiplies them.", "For example, using the gradient descent and letting \u03b1 denote the step size, the update for \u03b8 is", "which will leave \u03b8 unchanged every time \u2202\u0177/\u2202z is 0 (which occurs often by inspecting the plot of \u0177). Consequently, it can be quite challenging to reach the optimum value of the parameter \u03b8, if most updates leave \u03b8 unchanged. This is known as the vanishing gradient problem, which we will detail later. Thus, this choice of function for modeling the conditional probability would not make training very efficient using gradient descent, and so, we need another way to map z to [0, 1].", "In this section, we will derive the sigmoid function as a solution to our problem of finding a mapping f : \u211d \u2192 [0, 1]. We will show two ways to obtain the sigmoid:", "Here we will describe how the 0\u20131 loss can motivate the use of log odds. Suppose we have a function g that maps x to a class in {0, 1}. We can look at the 0\u20131 loss that counts the number of misclassifications, defined by", "We would like to minimize the number of mistakes made by the algorithm and so, we can minimize the expected conditional loss (also called conditional risk):", "Minimizing the expected conditional loss is equivalent to maximizing P(y=g(x)|x). So the predicted class g(x) should be 1 if the conditional probability is higher for y=1 than it is for y=0. This can be expressed as", "The odds of an event are defined in terms of how likely is the event to happen compared to not happening.", "Note that while the probability of an event, P(E), is a number between 0 and 1, the odds of an event, odds(E), can take any non-negative value. By taking the log of the odds of an event, called log odds, we get a real-valued number, which we can model linearly, as we will see next.", "On the left-hand side of inequation (1) we have the odds of the conditional probability y=1|x. By applying the log to this inequality we get the following inequation", "The log odds variable can now take any real value and, thus, we can make the simplifying assumption that it is a linear function of h, the input to the output unit. Thus, we can write the log odds as", "By letting z denote the linear combination of h, shown on the right-hand side of the equation above, solving this equation for P(y=1|x) yields the sigmoid function", "Thus, the sigmoid function is defined as", "Starting from the 0\u20131 loss allowed us to derive the sigmoid function by assuming that the log odds is linear on the data.", "Let us now analyze another way to obtain the sigmoid function, by first understanding the rationale behind another function that also represents probabilities, called softmax, and then trying to apply it to the sigmoid function. This approach was motivated by the assumption that the authors of the Deep Learning book make on page 179:", "(\u2026) the unnormalized log probabilities are linear in y and z.", "They start by modeling the unnormalized log probability as", "This assumption was not obvious to me, but this blog post helped me better understand it by starting with the softmax function. Softmax can be seen as a generalization of the sigmoid function, where we can have any number of classes, not just 2. Let k be the total number of classes. Softmax outputs a vector of probabilities, one for each class. The input for the softmax function is a vector z of real values and the output is a probability vector of the same size as z containing P(y=c|x), for each class c. Similarly to the sigmoid function, in softmax we have k problems of mapping real values to [0, 1]. Additionally, for the softmax to represent a valid probability distribution, all of its outputs must sum to 1. By denoting the softmax function as f, this can be expressed as", "For k\u2a7e2, we can construct such function f in 2 steps:", "The same idea can be used for the sigmoid function, where we start with z for class 1, and we define the unnormalized log to be 0 for class 0. Applying the same rationale yields the sigmoid function.", "Again, we define P(y=1|x) as the sigmoid function of z. By looking at the image above, we can write the conditional probability more compactly as", "After analyzing two different ways to obtain the sigmoid function, let us examine whether or not this function is a good candidate for the output unit of a binary classification network. By inspecting the plot of the sigmoid function below, we can see that large values of z will produce a very confident prediction that the input example belongs to class 1, while very negative values of z indicate a strong probability of the input belonging to class 0.", "The sigmoid function saturates for both very positive and negative values. This will cause the gradient \u2202\u0177/\u2202z to be 0 in those cases. Remember that this was also a problem in the simpler function we analyzed first, f(z)=max{0, min{1, z}}. Consequently, as the chain rule multiplies the partial derivatives, learning may not be effective. This is called the vanishing gradient problem, in which vanishingly small gradients prevent the weights from being updated.", "We will now introduce a loss function \u2112 that will nullify this inconvenient saturation effect of the sigmoid.", "Remember that, since the conditional output variable has a Bernoulli distribution, we can relate its maximum likelihood with the neural network\u2019s loss function. In maximum likelihood estimation, we estimate the distribution\u2019s parameter as the one that maximizes the likelihood of the observed data", "where m is the total number of examples and we use superscript notation to denote the index of each example. Since the optimal parameter can be written as the argument that minimizes the negative log of the conditional probability, it makes sense to define the loss function as the negative log. For a single example, and by denoting \u0177=P(y|x), the loss function is defined as", "Let us now compute the partial derivatives.", "Before computing the next derivative, we compute the derivative of the sigmoid function", "Now it is straightforward to compute the derivate of \u0177 using equation (2)", "As we will see next, the use of the log in the loss function will undo the exponential effect of the sigmoid function using the chain rule", "Let us now inspect which values of z have a zero gradient for the loss function. Below, on the left-hand side, the loss is plotted as a function of z, and on the right-hand side, we depict the respective gradient.", "We can observe that, for y=1 (red curve), the loss saturates when z takes very positive values. Equivalently, the gradient approaches 0 as z tends to +\u221e. In this case, P(y=1|x) = \u03c3(z) is close to 1 and, thus, the neural network already has the right answer. Similarly, for y=0, the loss only saturates when z is very negative (the gradient tends to 0) and, consequently, P(y=0|x) = 1-\u03c3(z) is close to 1, which is also the correct response.", "In short, the loss only saturates when the algorithm is already predicting the correct label, and so it is okay to stop learning in those cases.", "Moreover, when the model makes incorrect predictions, i.e., when y=1 and z is negative, or when y=0 and z is positive, the magnitude of the gradient tends to 1, as the model is (wrongly) more confident of its misclassifications.", "This property allows the parameters to be rapidly (as the gradient does not shrink) updated in the correct direction.", "The fact that the loss function applies the logarithm to the sigmoid cancels out its unwanted saturation effect. Otherwise, we would have gradients close to 0 for both very positive and negative values of z, which would make this function undesirable for learning. This effect is illustrated below.", "The main purpose of this article was to design an output unit for a binary classification neural network. We motivated the sigmoid function as the solution for the problem of mapping a real-valued number to a probability, i.e., to a number between 0 and 1. We used two different approaches to derive the sigmoid: the 0\u20131 loss, and the softmax function.", "We also addressed the problem of vanishing gradients in the sigmoid function by using the negative log loss function. This allowed us to conclude that the sigmoid is an appropriate output unit for the binary classification problem.", "However, it is not recommended to use the sigmoid as an activation function for hidden units, as we would not have the maximum likelihood loss following this unit. Therefore, as \u03c3 saturates in a significant part of its domain, the network would suffer from the vanishing gradient problem. Other functions such as the rectified linear unit (ReLU) are commonly used as activations in hidden layers.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Research Data Scientist working on NLP"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fe2c560d4b2c4&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-the-motivation-of-sigmoid-output-units-e2c560d4b2c4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-the-motivation-of-sigmoid-output-units-e2c560d4b2c4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-the-motivation-of-sigmoid-output-units-e2c560d4b2c4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-the-motivation-of-sigmoid-output-units-e2c560d4b2c4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----e2c560d4b2c4--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e2c560d4b2c4--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@inesjpedro?source=post_page-----e2c560d4b2c4--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@inesjpedro?source=post_page-----e2c560d4b2c4--------------------------------", "anchor_text": "In\u00eas Pedro"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb18d568fcb20&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-the-motivation-of-sigmoid-output-units-e2c560d4b2c4&user=In%C3%AAs+Pedro&userId=b18d568fcb20&source=post_page-b18d568fcb20----e2c560d4b2c4---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe2c560d4b2c4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-the-motivation-of-sigmoid-output-units-e2c560d4b2c4&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe2c560d4b2c4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-the-motivation-of-sigmoid-output-units-e2c560d4b2c4&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://br.pinterest.com/pin/305681893459272481/", "anchor_text": "Pinterest"}, {"url": "https://www.dlf.pt/ddetail/obxomw_cartoon-bear-standing-up-hd-png-download/", "anchor_text": "here"}, {"url": "http://www.deeplearningbook.org/", "anchor_text": "Deep Learning"}, {"url": "https://www.coursera.org/specializations/deep-learning?utm_source=gg&utm_medium=sem&utm_content=17-DeepLearning-ROW&campaignid=6465471773&adgroupid=76541762319&device=c&keyword=python%20neural%20network&matchtype=b&network=g&devicemodel=&adpostion=&creativeid=379493352694&hide_mobile_promo&gclid=CjwKCAjw7-P1BRA2EiwAXoPWA6uTmuQmMQ_IAD0Qj76rUHxQHywuIjsF0Hgssh70c6bRDCP5fek4dxoCxxoQAvD_BwE", "anchor_text": "Andrew Ng\u2019s Deep Learning specialization in Coursera"}, {"url": "https://onionesquereality.wordpress.com/2016/05/18/where-does-the-sigmoid-in-logistic-regression-come-from/", "anchor_text": "0\u20131 loss can motivate the use of log odds"}, {"url": "http://deeplearningbook.org/", "anchor_text": "Deep Learning"}, {"url": "https://alexander-schiendorfer.github.io/2020/02/17/why-so.sigmoid.html", "anchor_text": "blog post"}, {"url": "https://medium.com/tag/sigmoid?source=post_page-----e2c560d4b2c4---------------sigmoid-----------------", "anchor_text": "Sigmoid"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----e2c560d4b2c4---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/explained?source=post_page-----e2c560d4b2c4---------------explained-----------------", "anchor_text": "Explained"}, {"url": "https://medium.com/tag/vanishing-gradient?source=post_page-----e2c560d4b2c4---------------vanishing_gradient-----------------", "anchor_text": "Vanishing Gradient"}, {"url": "https://medium.com/tag/binary-classification?source=post_page-----e2c560d4b2c4---------------binary_classification-----------------", "anchor_text": "Binary Classification"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe2c560d4b2c4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-the-motivation-of-sigmoid-output-units-e2c560d4b2c4&user=In%C3%AAs+Pedro&userId=b18d568fcb20&source=-----e2c560d4b2c4---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe2c560d4b2c4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-the-motivation-of-sigmoid-output-units-e2c560d4b2c4&user=In%C3%AAs+Pedro&userId=b18d568fcb20&source=-----e2c560d4b2c4---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe2c560d4b2c4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-the-motivation-of-sigmoid-output-units-e2c560d4b2c4&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e2c560d4b2c4--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fe2c560d4b2c4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-the-motivation-of-sigmoid-output-units-e2c560d4b2c4&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----e2c560d4b2c4---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----e2c560d4b2c4--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----e2c560d4b2c4--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----e2c560d4b2c4--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----e2c560d4b2c4--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----e2c560d4b2c4--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----e2c560d4b2c4--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----e2c560d4b2c4--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----e2c560d4b2c4--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@inesjpedro?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@inesjpedro?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "In\u00eas Pedro"}, {"url": "https://medium.com/@inesjpedro/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "27 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb18d568fcb20&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-the-motivation-of-sigmoid-output-units-e2c560d4b2c4&user=In%C3%AAs+Pedro&userId=b18d568fcb20&source=post_page-b18d568fcb20--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F32a61bce4eac&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-the-motivation-of-sigmoid-output-units-e2c560d4b2c4&newsletterV3=b18d568fcb20&newsletterV3Id=32a61bce4eac&user=In%C3%AAs+Pedro&userId=b18d568fcb20&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}