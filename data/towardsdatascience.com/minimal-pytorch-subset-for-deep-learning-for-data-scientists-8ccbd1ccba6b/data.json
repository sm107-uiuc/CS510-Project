{"url": "https://towardsdatascience.com/minimal-pytorch-subset-for-deep-learning-for-data-scientists-8ccbd1ccba6b", "time": 1683013495.986889, "path": "towardsdatascience.com/minimal-pytorch-subset-for-deep-learning-for-data-scientists-8ccbd1ccba6b/", "webpage": {"metadata": {"title": "The Most Complete Guide to PyTorch for Data Scientists | by Rahul Agarwal | Towards Data Science", "h1": "The Most Complete Guide to PyTorch for Data Scientists", "description": "PyTorch has sort of became one of the de facto standards for creating Neural Networks now, and I love its interface. Yet, it is somehow a little difficult for beginners to get a hold of. I remember\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/moving-from-keras-to-pytorch-f0d4fff4ce79", "anchor_text": "Keras to PyTorch", "paragraph_index": 1}, {"url": "https://www.exxactcorp.com/Deep-Learning-NVIDIA-GPU-Workstations?utm_source=web%20referral&utm_medium=backlink&utm_campaign=Rahul%20Agarwal", "anchor_text": "AI-based workstations and servers", "paragraph_index": 4}, {"url": "https://pytorch.org/docs/stable/tensors.html", "anchor_text": "more things", "paragraph_index": 9}, {"url": "https://pytorch.org/docs/stable/torch.html?highlight=mm#math-operations", "anchor_text": "here", "paragraph_index": 10}, {"url": "https://pytorch.org/docs/stable/autograd.html#variable-deprecated", "anchor_text": "deprecated", "paragraph_index": 11}, {"url": "https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#parameter", "anchor_text": "docs", "paragraph_index": 17}, {"url": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor", "anchor_text": "Tensor", "paragraph_index": 18}, {"url": "https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear", "anchor_text": "nn.Linear", "paragraph_index": 21}, {"url": "https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d", "anchor_text": "nn.Conv2d", "paragraph_index": 21}, {"url": "https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d", "anchor_text": "nn.MaxPool2d", "paragraph_index": 21}, {"url": "https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU", "anchor_text": "nn.ReLU", "paragraph_index": 21}, {"url": "https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html#torch.nn.BatchNorm2d", "anchor_text": "nn.BatchNorm2d", "paragraph_index": 21}, {"url": "https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#torch.nn.Dropout", "anchor_text": "nn.Dropout", "paragraph_index": 21}, {"url": "https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding", "anchor_text": "nn.Embedding", "paragraph_index": 21}, {"url": "https://pytorch.org/docs/stable/generated/torch.nn.GRU.html#torch.nn.GRU", "anchor_text": "nn.GRU", "paragraph_index": 21}, {"url": "https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM", "anchor_text": "nn.LSTM", "paragraph_index": 21}, {"url": "https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html#torch.nn.Softmax", "anchor_text": "nn.Softmax", "paragraph_index": 21}, {"url": "https://pytorch.org/docs/stable/generated/torch.nn.LogSoftmax.html#torch.nn.LogSoftmax", "anchor_text": "nn.LogSoftmax", "paragraph_index": 21}, {"url": "https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html#torch.nn.MultiheadAttention", "anchor_text": "nn.MultiheadAttention", "paragraph_index": 21}, {"url": "https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html#torch.nn.TransformerEncoder", "anchor_text": "nn.TransformerEncoder", "paragraph_index": 21}, {"url": "https://pytorch.org/docs/stable/generated/torch.nn.TransformerDecoder.html#torch.nn.TransformerDecoder", "anchor_text": "nn.TransformerDecoder", "paragraph_index": 21}, {"url": "https://pytorch.org/docs/stable/torchvision/datasets.html", "anchor_text": "torchvision.datasets", "paragraph_index": 27}, {"url": "https://pytorch.org/text/datasets.html", "anchor_text": "torchtext.datasets", "paragraph_index": 27}, {"url": "https://towardsdatascience.com/end-to-end-pipeline-for-setting-up-multiclass-image-classification-for-data-scientists-2e051081d41c", "anchor_text": "here", "paragraph_index": 33}, {"url": "https://pytorch.org/docs/stable/nn.html#loss-functions", "anchor_text": "loss functions", "paragraph_index": 55}, {"url": "https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss", "anchor_text": "nn.CrossEntropyLoss", "paragraph_index": 55}, {"url": "https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss", "anchor_text": "nn.NLLLoss", "paragraph_index": 55}, {"url": "https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html#torch.nn.KLDivLoss", "anchor_text": "nn.KLDivLoss", "paragraph_index": 55}, {"url": "https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss", "anchor_text": "nn.MSELoss", "paragraph_index": 55}, {"url": "https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss", "anchor_text": "nn.NLLLoss", "paragraph_index": 55}, {"url": "https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss", "anchor_text": "nn.CrossEntropyLoss", "paragraph_index": 57}, {"url": "https://pytorch.org/docs/stable/optim.html#torch.optim.Adadelta", "anchor_text": "torch.optim.Adadelta", "paragraph_index": 64}, {"url": "https://pytorch.org/docs/stable/optim.html#torch.optim.Adagrad", "anchor_text": "torch.optim.Adagrad", "paragraph_index": 64}, {"url": "https://pytorch.org/docs/stable/optim.html#torch.optim.RMSprop", "anchor_text": "torch.optim.RMSprop", "paragraph_index": 64}, {"url": "https://pytorch.org/docs/stable/optim.html#torch.optim.Adam", "anchor_text": "torch.optim.Adam", "paragraph_index": 64}, {"url": "https://pytorch-optimizer.readthedocs.io/en/latest/", "anchor_text": "pytorch-optimizer", "paragraph_index": 67}, {"url": "https://github.com/pytorch/pytorch/tree/master/torch/optim", "anchor_text": "PyTorch", "paragraph_index": 67}, {"url": "https://github.com/jettify/pytorch-optimizer/tree/master/torch_optimizer", "anchor_text": "pytorch-optimizers", "paragraph_index": 67}, {"url": "https://github.com/MLWhiz/data_science_blogs/tree/master/pytorch_guide", "anchor_text": "GitHub", "paragraph_index": 71}, {"url": "https://coursera.pxf.io/jWG2Db", "anchor_text": "Deep Neural Networks with PyTorch", "paragraph_index": 72}, {"url": "https://coursera.pxf.io/NKERRq", "anchor_text": "Deep Learning in Computer Vision", "paragraph_index": 72}, {"url": "https://medium.com/@rahul_agarwal?source=post_page---------------------------", "anchor_text": "Medium", "paragraph_index": 73}, {"url": "http://eepurl.com/dbQnuX?source=post_page---------------------------", "anchor_text": "blog", "paragraph_index": 73}, {"url": "https://twitter.com/MLWhiz?source=post_page---------------------------", "anchor_text": "@mlwhiz", "paragraph_index": 73}, {"url": "http://ko-fi.com/rahulagarwal", "anchor_text": "ko-fi.com/rahulagarwal", "paragraph_index": 76}], "all_paragraphs": ["PyTorch has sort of became one of the de facto standards for creating Neural Networks now, and I love its interface. Yet, it is somehow a little difficult for beginners to get a hold of.", "I remember picking PyTorch up only after some extensive experimentation a couple of years back. To tell you the truth, it took me a lot of time to pick it up but am I glad that I moved from Keras to PyTorch. With its high customizability and pythonic syntax, PyTorch is just a joy to work with, and I would recommend it to anyone who wants to do some heavy lifting with Deep Learning.", "So, in this PyTorch guide, I will try to ease some of the pain with PyTorch for starters and go through some of the most important classes and modules that you will require while creating any Neural Network with Pytorch.", "But, that is not to say that this is aimed at beginners only as I will also talk about the high customizability PyTorch provides and will talk about custom Layers, Datasets, Dataloaders, and Loss functions.", "If you would like to get the full power out of Pytorch, Exxact Corporation has a great line of AI-based workstations and servers starting at $3,700, with a couple of NVIDIA RTX 30 Series GPUs, 3-year warranty, and a deep learning software stack.", "So let\u2019s get some coffee \u2615 \ufe0fand start it up.", "Here is a Table of Contents if you want to look at a particular section.", "\u00b7 Tensors \u2218 1. Create a Tensor \u2218 2. Tensor Operations\u00b7 The nn.Module\u00b7 A word about Layers\u00b7 Datasets and DataLoaders \u2218 Understanding Custom Datasets \u2218 Understanding Custom DataLoaders\u00b7 Training a Neural Network\u00b7 Loss functions \u2218 Custom Loss Function\u00b7 Optimizers\u00b7 Using GPU/Multiple GPUs\u00b7 Conclusion", "Tensors are the basic building blocks in PyTorch and put very simply, they are NumPy arrays but on GPU. In this part, I will list down some of the most used operations we can use while working with Tensors. This is by no means an exhaustive list of operations you can do with Tensors, but it is helpful to understand what tensors are before going towards the more exciting parts.", "We can create a PyTorch tensor in multiple ways. This includes converting to tensor from a NumPy array. Below is just a small gist with some examples to start with, but you can do a whole lot of more things with tensors just like you can do with NumPy arrays.", "Again, there are a lot of operations you can do on these tensors. The full list of functions can be found here.", "Note: What are PyTorch Variables? In the previous versions of Pytorch, Tensor and Variables used to be different and provided different functionality, but now the Variable API is deprecated, and all methods for variables work with Tensors. So, if you don\u2019t know about them, it\u2019s fine as they re not needed, and if you know them, you can forget about them.", "Here comes the fun part as we are now going to talk about some of the most used constructs in Pytorch while creating deep learning projects. nn.Module lets you create your Deep Learning models as a class. You can inherit from nn.Moduleto define any model as a class. Every model class necessarily contains an __init__ procedure block and a block for the forward pass.", "So, put simply, any network we define will look like:", "Here we have defined a very simple Network that takes an input of size 784 and passes it through two linear layers in a sequential manner. But the thing to note is that we can define any sort of calculation while defining the forward pass, and that makes PyTorch highly customizable for research purposes. For example, in our crazy experimentation mode, we might have used the below network where we arbitrarily attach our layers. Here we send back the output from the second linear layer back again to the first one after adding the input to it(skip connection) back again(I honestly don\u2019t know what that will do).", "We can also check if the neural network forward pass works. I usually do that by first creating some random input and just passing that through the network I have created.", "Pytorch is pretty powerful, and you can actually create any new experimental layer by yourself using nn.Module. For example, rather than using the predefined Linear Layer nn.Linear from Pytorch above, we could have created our custom linear layer.", "You can see how we wrap our weights tensor in nn.Parameter. This is done to make the tensor to be considered as a model parameter. From PyTorch docs:", "Parameters are Tensor subclasses, that have a very special property when used with Module - when they\u2019re assigned as Module attributes they are automatically added to the list of its parameters, and will appear in parameters() iterator", "As you will later see, the model.parameters() iterator will be an input to the optimizer. But more on that later.", "Right now, we can now use this custom layer in any PyTorch network, just like any other layer.", "But then again, Pytorch would not be so widely used if it didn\u2019t provide a lot of ready to made layers used very frequently in wide varieties of Neural Network architectures. Some examples are:nn.Linear, nn.Conv2d, nn.MaxPool2d, nn.ReLU, nn.BatchNorm2d, nn.Dropout, nn.Embedding, nn.GRU/nn.LSTM, nn.Softmax, nn.LogSoftmax, nn.MultiheadAttention, nn.TransformerEncoder, nn.TransformerDecoder", "I have linked all the layers to their source where you could read all about them, but to show how I usually try to understand a layer and read the docs, I would try to look at a very simple convolutional layer here.", "So, a Conv2d Layer needs as input an Image of height H and width W, with Cin channels. Now, for the first layer in a convnet, the number of in_channels would be 3(RGB), and the number of out_channels can be defined by the user. The kernel_size mostly used is 3x3, and the stride normally used is 1.", "To check a new layer which I don\u2019t know much about, I usually try to see the input as well as output for the layer like below where I would first initialize the layer:", "And then pass some random input through it. Here 100 is the batch size.", "So, we get the output from the convolution operation as required, and I have sufficient information on how to use this layer in any Neural Network I design.", "How would we pass data to our Neural nets while training or while testing? We can definitely pass tensors as we have done above, but Pytorch also provides us with pre-built Datasets to make it easier for us to pass data to our neural nets. You can check out the complete list of datasets provided at torchvision.datasets and torchtext.datasets. But, to give a concrete example for datasets, let\u2019s say we had to pass images to an Image Neural net using a folder which has images in this structure:", "We can use torchvision.datasets.ImageFolder dataset to get an example image like below:", "This dataset has 847 images, and we can get an image and its label using an index. Now we can pass images one by one to any image neural network using a for loop:", "But that is not optimal. We want to do batching. We can actually write some more code to append images and labels in a batch and then pass it to the Neural network. But Pytorch provides us with a utility iterator torch.utils.data.DataLoader to do precisely that. Now we can simply wrap our train_dataset in the Dataloader, and we will get batches instead of individual examples.", "We can simply iterate with batches using:", "So actually, the whole process of using datasets and Dataloaders becomes:", "You can look at this particular example in action in my previous blogpost on Image classification using Deep Learning here.", "This is great, and Pytorch does provide a lot of functionality out of the box. But the main power of Pytorch comes with its immense customization. We can also create our own custom datasets if the datasets provided by PyTorch don\u2019t fit our use case.", "To write our custom datasets, we can make use of the abstract class torch.utils.data.Dataset provided by Pytorch. We need to inherit this Dataset class and need to define two methods to create a custom Dataset.", "For example, we can create a simple custom dataset that returns an image and a label from a folder. See that most of the tasks are happening in __init__ part where we use glob.glob to get image names and do some general preprocessing.", "Also, note that we open our images one at a time in the __getitem__ method and not while initializing. This is not done in __init__ because we don't want to load all our images in the memory and just need to load the required ones.", "We can now use this dataset with the utility Dataloader just like before. It works just like the previous dataset provided by PyTorch but without some utility functions.", "This particular section is a little advanced and can be skipped going through this post as it will not be needed in a lot of situations. But I am adding it for completeness here.", "So let\u2019s say you are looking to provide batches to a network that processes text input, and the network could take sequences with any sequence size as long as the size remains constant in the batch. For example, we can have a BiLSTM network that can process sequences of any length. It\u2019s alright if you don\u2019t understand the layers used in it right now; just know that it can process sequences with variable sizes.", "This network expects its input to be of shape (batch_size, seq_length) and works with any seq_length. We can check this by passing our model two random batches with different sequence lengths(10 and 25).", "Now, we want to provide tight batches to this model, such that each batch has the same sequence length based on the max sequence length in the batch to minimize padding. This has an added benefit of making the neural net run faster. It was, in fact, one of the methods used in the winning submission of the Quora Insincere challenge in Kaggle, where running time was of utmost importance.", "So, how do we do this? Let\u2019s write a very simple custom dataset class first.", "Also, let\u2019s generate some random data which we will use with this custom Dataset.", "We can use the custom dataset now using:", "If we now try to use the Dataloader on this dataset with batch_size>1, we will get an error. Why is that?", "This happens because the sequences have different lengths, and our data loader expects our sequences of the same length. Remember that in the previous image example, we resized all images to size 224 using the transforms, so we didn\u2019t face this error.", "So, how do we iterate through this dataset so that each batch has sequences with the same length, but different batches may have different sequence lengths?", "We can use collate_fn parameter in the DataLoader that lets us define how to stack sequences in a particular batch. To use this, we need to define a function that takes as input a batch and returns (x_batch, y_batch ) with padded sequence lengths based on max_sequence_length in the batch. The functions I have used in the below function are simple NumPy operations. Also, the function is properly commented so you can understand what is happening.", "We can now use this collate_fn with our Dataloader as:", "It will work this time as we have provided a custom collate_fn. And see that the batches have different sequence lengths now. Thus we would be able to train our BiLSTM using variable input sizes just like we wanted.", "We know how to create a neural network using nn.Module. But how to train it? Any neural network that has to be trained will have a training loop that will look something similar to below:", "In the above code, we are running five epochs and in each epoch:", "Till now, we have talked about how to use nn.Module to create networks and how to use Custom Datasets and Dataloaders with Pytorch. So let's talk about the various options available for Loss Functions and Optimizers.", "Pytorch provides us with a variety of loss functions for our most common tasks, like Classification and Regression. Some most used examples are nn.CrossEntropyLoss , nn.NLLLoss , nn.KLDivLoss and nn.MSELoss. You can read the documentation of each loss function, but to explain how to use these loss functions, I will go through the example of nn.NLLLoss", "The documentation for NLLLoss is pretty succinct. As in, this loss function is used for Multiclass classification, and based on the documentation:", "So, we can try to use this Loss function for a simple classification network. Please note the LogSoftmax layer after the final linear layer. If you don't want to use this LogSoftmax layer, you could have just used nn.CrossEntropyLoss", "Let\u2019s define a random input to pass to our network to test it:", "And pass it through the model to get predictions:", "We can now get the loss as:", "Defining your custom loss functions is again a piece of cake, and you should be okay as long as you use tensor operations in your loss function. For example, here is the customMseLoss", "You can use this custom loss just like before. But note that we don\u2019t instantiate the loss using criterion this time as we have defined it as a function.", "If we wanted, we could have also written it as a class using nn.Module , and then we would have been able to use it as an object. Here is an NLLLoss custom example:", "Once we get gradients using the loss.backward() call, we need to take an optimizer step to change the weights in the whole network. Pytorch provides a variety of different ready to use optimizers using the torch.optim module. For example: torch.optim.Adadelta , torch.optim.Adagrad , torch.optim.RMSprop and the most widely used torch.optim.Adam.", "To use the most used Adam optimizer from PyTorch, we can simply instantiate it with:", "And then use optimizer.zero_grad() and optimizer.step() while training the model.", "I am not discussing how to write custom optimizers as it is an infrequent use case, but if you want to have more optimizers, do check out the pytorch-optimizer library, which provides a lot of other optimizers used in research papers. Also, if you anyhow want to create your own optimizers, you can take inspiration using the source code of implemented optimizers in PyTorch or pytorch-optimizers.", "Till now, whatever we have done is on the CPU. If you want to use a GPU, you can put your model to GPU using model.to('cuda'). Or if you want to use multiple GPUs, you can use nn.DataParallel. Here is a utility function that checks the number of GPUs in the machine and sets up parallel training automatically using DataParallel if needed.", "The only thing that we will need to change is that we will load our data to GPU while training if we have GPUs. It\u2019s as simple as adding a few lines of code to our training loop.", "Pytorch provides a lot of customizability with minimal code. While at first, it might be hard to understand how the whole ecosystem is structured with classes, in the end, it is simple Python. In this post, I have tried to break down most of the parts you might need while using Pytorch, and I hope it makes a little more sense for you after reading this.", "You can find the code for this post here on my GitHub repo, where I keep codes for all my blogs.", "If you want to learn more about Pytorch using a course based structure, take a look at the Deep Neural Networks with PyTorch course by IBM on Coursera. Also, if you want to know more about Deep Learning, I would like to recommend this excellent course on Deep Learning in Computer Vision", "Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog to be informed about them. As always, I welcome feedback and constructive criticism and can be reached on Twitter @mlwhiz", "Also, a small disclaimer \u2014 There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "4M Views. Bridging the gap between Data Science and Intuition. MLE@FB, Ex-WalmartLabs, Citi. Connect on Twitter @mlwhiz \u2615\ufe0f ko-fi.com/rahulagarwal"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F8ccbd1ccba6b&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fminimal-pytorch-subset-for-deep-learning-for-data-scientists-8ccbd1ccba6b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fminimal-pytorch-subset-for-deep-learning-for-data-scientists-8ccbd1ccba6b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fminimal-pytorch-subset-for-deep-learning-for-data-scientists-8ccbd1ccba6b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fminimal-pytorch-subset-for-deep-learning-for-data-scientists-8ccbd1ccba6b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----8ccbd1ccba6b--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8ccbd1ccba6b--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://mlwhiz.medium.com/?source=post_page-----8ccbd1ccba6b--------------------------------", "anchor_text": ""}, {"url": "https://mlwhiz.medium.com/?source=post_page-----8ccbd1ccba6b--------------------------------", "anchor_text": "Rahul Agarwal"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe8cce06956c9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fminimal-pytorch-subset-for-deep-learning-for-data-scientists-8ccbd1ccba6b&user=Rahul+Agarwal&userId=e8cce06956c9&source=post_page-e8cce06956c9----8ccbd1ccba6b---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8ccbd1ccba6b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fminimal-pytorch-subset-for-deep-learning-for-data-scientists-8ccbd1ccba6b&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8ccbd1ccba6b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fminimal-pytorch-subset-for-deep-learning-for-data-scientists-8ccbd1ccba6b&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://pixabay.com/users/Manuchi-1728328/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=2462436", "anchor_text": "\u0414\u0435\u043d\u0438\u0441 \u041c\u0430\u0440\u0447\u0443\u043a"}, {"url": "https://pixabay.com/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=2462436", "anchor_text": "Pixabay"}, {"url": "https://towardsdatascience.com/moving-from-keras-to-pytorch-f0d4fff4ce79", "anchor_text": "Keras to PyTorch"}, {"url": "https://www.exxactcorp.com/Deep-Learning-NVIDIA-GPU-Workstations?utm_source=web%20referral&utm_medium=backlink&utm_campaign=Rahul%20Agarwal", "anchor_text": "AI-based workstations and servers"}, {"url": "https://pytorch.org/docs/stable/tensors.html", "anchor_text": "more things"}, {"url": "https://pytorch.org/docs/stable/torch.html?highlight=mm#math-operations", "anchor_text": "here"}, {"url": "https://pytorch.org/docs/stable/autograd.html#variable-deprecated", "anchor_text": "deprecated"}, {"url": "https://unsplash.com/@fernanddecanne?utm_source=medium&utm_medium=referral", "anchor_text": "Fernand De Canne"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#parameter", "anchor_text": "docs"}, {"url": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor", "anchor_text": "Tensor"}, {"url": "https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear", "anchor_text": "nn.Linear"}, {"url": "https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d", "anchor_text": "nn.Conv2d"}, {"url": "https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d", "anchor_text": "nn.MaxPool2d"}, {"url": "https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU", "anchor_text": "nn.ReLU"}, {"url": "https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html#torch.nn.BatchNorm2d", "anchor_text": "nn.BatchNorm2d"}, {"url": "https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#torch.nn.Dropout", "anchor_text": "nn.Dropout"}, {"url": "https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding", "anchor_text": "nn.Embedding"}, {"url": "https://pytorch.org/docs/stable/generated/torch.nn.GRU.html#torch.nn.GRU", "anchor_text": "nn.GRU"}, {"url": "https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM", "anchor_text": "nn.LSTM"}, {"url": "https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html#torch.nn.Softmax", "anchor_text": "nn.Softmax"}, {"url": "https://pytorch.org/docs/stable/generated/torch.nn.LogSoftmax.html#torch.nn.LogSoftmax", "anchor_text": "nn.LogSoftmax"}, {"url": "https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html#torch.nn.MultiheadAttention", "anchor_text": "nn.MultiheadAttention"}, {"url": "https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html#torch.nn.TransformerEncoder", "anchor_text": "nn.TransformerEncoder"}, {"url": "https://pytorch.org/docs/stable/generated/torch.nn.TransformerDecoder.html#torch.nn.TransformerDecoder", "anchor_text": "nn.TransformerDecoder"}, {"url": "https://pytorch.org/docs/stable/torchvision/datasets.html", "anchor_text": "torchvision.datasets"}, {"url": "https://pytorch.org/text/datasets.html", "anchor_text": "torchtext.datasets"}, {"url": "https://towardsdatascience.com/end-to-end-pipeline-for-setting-up-multiclass-image-classification-for-data-scientists-2e051081d41c", "anchor_text": "here"}, {"url": "https://pytorch.org/docs/stable/nn.html#loss-functions", "anchor_text": "loss functions"}, {"url": "https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss", "anchor_text": "nn.CrossEntropyLoss"}, {"url": "https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss", "anchor_text": "nn.NLLLoss"}, {"url": "https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html#torch.nn.KLDivLoss", "anchor_text": "nn.KLDivLoss"}, {"url": "https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss", "anchor_text": "nn.MSELoss"}, {"url": "https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss", "anchor_text": "nn.NLLLoss"}, {"url": "https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss", "anchor_text": "nn.CrossEntropyLoss"}, {"url": "https://pytorch.org/docs/stable/optim.html#torch.optim.Adadelta", "anchor_text": "torch.optim.Adadelta"}, {"url": "https://pytorch.org/docs/stable/optim.html#torch.optim.Adagrad", "anchor_text": "torch.optim.Adagrad"}, {"url": "https://pytorch.org/docs/stable/optim.html#torch.optim.RMSprop", "anchor_text": "torch.optim.RMSprop"}, {"url": "https://pytorch.org/docs/stable/optim.html#torch.optim.Adam", "anchor_text": "torch.optim.Adam"}, {"url": "https://pytorch-optimizer.readthedocs.io/en/latest/", "anchor_text": "pytorch-optimizer"}, {"url": "https://github.com/pytorch/pytorch/tree/master/torch/optim", "anchor_text": "PyTorch"}, {"url": "https://github.com/jettify/pytorch-optimizer/tree/master/torch_optimizer", "anchor_text": "pytorch-optimizers"}, {"url": "https://github.com/jettify/pytorch-optimizer", "anchor_text": "pytorch-optimizer"}, {"url": "https://github.com/MLWhiz/data_science_blogs/tree/master/pytorch_guide", "anchor_text": "GitHub"}, {"url": "https://coursera.pxf.io/jWG2Db", "anchor_text": "Deep Neural Networks with PyTorch"}, {"url": "https://coursera.pxf.io/NKERRq", "anchor_text": "Deep Learning in Computer Vision"}, {"url": "https://medium.com/@rahul_agarwal?source=post_page---------------------------", "anchor_text": "Medium"}, {"url": "http://eepurl.com/dbQnuX?source=post_page---------------------------", "anchor_text": "blog"}, {"url": "https://twitter.com/MLWhiz?source=post_page---------------------------", "anchor_text": "@mlwhiz"}, {"url": "https://mlwhiz.medium.com/membership", "anchor_text": "Join Medium with my referral link - Rahul AgarwalAs a Medium member, a portion of your membership fee goes to writers you read, and you get full access to every story on\u2026mlwhiz.medium.com"}, {"url": "https://medium.com/tag/programming?source=post_page-----8ccbd1ccba6b---------------programming-----------------", "anchor_text": "Programming"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----8ccbd1ccba6b---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----8ccbd1ccba6b---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----8ccbd1ccba6b---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----8ccbd1ccba6b---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8ccbd1ccba6b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fminimal-pytorch-subset-for-deep-learning-for-data-scientists-8ccbd1ccba6b&user=Rahul+Agarwal&userId=e8cce06956c9&source=-----8ccbd1ccba6b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8ccbd1ccba6b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fminimal-pytorch-subset-for-deep-learning-for-data-scientists-8ccbd1ccba6b&user=Rahul+Agarwal&userId=e8cce06956c9&source=-----8ccbd1ccba6b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8ccbd1ccba6b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fminimal-pytorch-subset-for-deep-learning-for-data-scientists-8ccbd1ccba6b&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8ccbd1ccba6b--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F8ccbd1ccba6b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fminimal-pytorch-subset-for-deep-learning-for-data-scientists-8ccbd1ccba6b&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----8ccbd1ccba6b---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----8ccbd1ccba6b--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----8ccbd1ccba6b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----8ccbd1ccba6b--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----8ccbd1ccba6b--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----8ccbd1ccba6b--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----8ccbd1ccba6b--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----8ccbd1ccba6b--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----8ccbd1ccba6b--------------------------------", "anchor_text": ""}, {"url": "https://mlwhiz.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://mlwhiz.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Rahul Agarwal"}, {"url": "https://mlwhiz.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "13.9K Followers"}, {"url": "http://ko-fi.com/rahulagarwal", "anchor_text": "ko-fi.com/rahulagarwal"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe8cce06956c9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fminimal-pytorch-subset-for-deep-learning-for-data-scientists-8ccbd1ccba6b&user=Rahul+Agarwal&userId=e8cce06956c9&source=post_page-e8cce06956c9--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Ff41165c9f72f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fminimal-pytorch-subset-for-deep-learning-for-data-scientists-8ccbd1ccba6b&newsletterV3=e8cce06956c9&newsletterV3Id=f41165c9f72f&user=Rahul+Agarwal&userId=e8cce06956c9&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}