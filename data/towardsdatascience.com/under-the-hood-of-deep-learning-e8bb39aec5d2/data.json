{"url": "https://towardsdatascience.com/under-the-hood-of-deep-learning-e8bb39aec5d2", "time": 1683002888.7795908, "path": "towardsdatascience.com/under-the-hood-of-deep-learning-e8bb39aec5d2/", "webpage": {"metadata": {"title": "Under the Hood of Deep Learning. A step-by-step tutorial to understand\u2026 | by Mohamed Gharibi | Towards Data Science", "h1": "Under the Hood of Deep Learning", "description": "The previous image is a simple architecture for a deep neural network. The goal of this post is to understand deep learning details and build your own network, rather than use the existing models as\u2026"}, "outgoing_paragraph_urls": [], "all_paragraphs": ["The previous image is a simple architecture for a deep neural network. The goal of this post is to understand deep learning details and build your own network, rather than use the existing models as a black box!", "In this post, we will go over a simple neural network that can learn to recognize hand-written digits (MNIST dataset). Currently, there are various types of neural networks, but for the sake of simplicity, we will start with the vanilla form (aka \u201cMultilayer Perceptron\u201d).", "Please note that the circles in the previous diagrams called neurons. Each neuron contains a number that ranges between 0 and 1 which is known as \u201cactivation\u201d. Also, note that the lines between these neurons called weights. Each weight is a randomly generated number that holds a value between -1 and 1.", "Each image in the MNIST dataset formed by black and white colors only and the dimension is (28 x 28 = 784 pixels). If you are still wondering why the range of neurons activation between 0 and 1, that is because it holds the greyscale of an image where 0 is completely black and 1 is completely white. Finally, to enter an image in our neural network, we need to convert it into a vector of pixels by taking each line of pixels and append it to the previous one, check the following example:", "As mentioned above, the input of the network will be an image that is converted to a vector of pixels (784 exactly) which is the first layer in our neural network.", "The number of output-layer neurons should be associated with the number of classes we are trying to predict. In our example, the number of classes is 10 (0, 1, 2, \u2026, 9). Therefore, the last layer (output layer) should consist of 10 neurons.", "The hidden layers are the layers in between. For now, let us just say that we chose the number of hidden layers and the number of neurons randomly.", "We want to build a model that is capable of predicting the digit of the input image (Image Classification). However, it is difficult to show a step by step example for the entire network. Therefore, we will start with a simpler neural network consists of three input nodes, a single hidden layer with a single neuron, and a single output. Our network will look like this", "Forward propagation is the process of starting with an input, going through the neural network and its calculations and ending up with making a single prediction (denoted by y^ and read as y-hat). Let us start with the following vector [0.6, 0.4, 0.2] and the actual output (y) is 1.", "Where did all these numbers come from? Well, the input nodes were already given, Z is the result for multiplying the input activations with the weights and add them together, y^ is the result for multiplying Z with its weight. Finally, y is also given which is the desired output. Generally, to compute any output, we multiply the activations with the weights and add them together. This the formula for the first layer.", "It is obvious that we are repeating the same multiplication across all the input and weights. Therefore, we just compute the summation since we may have more than 3 inputs.", "Each neuron will fire and light at some time. When a neuron fires it means this neuron detected a specific feature in the image. For example, every time an image with the digit 7 enters the network, almost the same neurons activate and fire (it means they triggered a similar event, similar angles, etc.). However, you do not want every neuron to fire when the activation is more than 0 (otherwise all the positive activations will keep firing). You want them to fire up after some threshold, say 10, this is known as the bias (b). We add the b to our summation to control when neurons fire.", "So far, our equation will produce good results. However, the results may be less than 0 or more than 1. As mentioned earlier, each activation should be in the range 0 to 1 since that is the greyscale of each image. Therefore, we need a function (f) to squash the result of y^ between 0 and 1. This function is known as the activation function, Sigmoid in particular. Calling Sigmoid on y^ will end up with the following", "Or you can write it as the following", "Note I named the left-hand size (Z) which is the convention name for the hidden layer output after applying an activation function. Also, note that I called f not Sigmoid since we have many different activation functions that we may apply. Here is a list of the commonly used activation functions:", "There are many other activation functions such as Leaky RELU, Parametric RELU, SQNL, ArcTan, etc.", "What is the loss? In simple words, the loss is how far the model from predicting the correct answer. As seen before, the output is 1, whereas the predicted output is 0.3, so the loss in our case is 0.7. If the model prediction is perfect, then the loss is 0. So it is possible to calculate the loss using the following equation (L denotes for Loss)", "Is that it? Basically, yes but there are few things that help to improve the loss in a way to be more beneficial for us in the future:", "This loss function is known as The Mean Square Error (MSE) and it is one of the most used loss functions. There are many other loss functions such as Cross-Entropy, Hinge, MAE, etc. But have you wondered what is the cost function? and what is the difference between loss and cost functions? Well, the difference is that the loss function is used for a single training example, whereas the cost function is the average loss over the entire training dataset.", "Congratulations! We are done with the forward propagation. However, the prediction that we just made may not be very accurate (consider the output 1, but the model predicted 0.7). How can we make a better prediction? Well, we cannot change the input value, but we can change the weight!! Viola. Now you know the secret of deep learning.", "We cannot change the input. However, we can increase the weight, then by multiplying it with the input will give us a larger predicted output, say 0.8. Keep repeating this process (adjusting the weights) and will get better results. Going in the opposite direction to change the weight knows as backpropagation! But how can this be done exactly? Well, this can be done using optimization algorithms. There are different optimizers such as Gradient Descent, Stochastic Gradient Descent, Adam Optimizer, etc.", "Gradient Descent is one of the optimization algorithms that aims to reduce the loss by adjusting the weights. Of course, changing weights manually is impossible (we have tens and hundreds of weights in a single neural network). So how can we automate this process? and how to tell the function which weight to adjust and when to stop?", "Let us start adjusting the weights, checking how does that affect the loss and plot all the results (check the bellow plot). As you can see, at a specific point (the red line) is the minimum loss. To the left of the red line, we have to increase the weight to decrease the loss, whereas to the right of the red line, we obviously need to decrease the weight in order to reduce the loss. The main questions remain: How do we know for a given point if it is to the left or to the right of the red line (in order to know if we should increase or decrease the weight)? And how much we should increase or decrease the weight in order to get closer to the minimum loss? Once we answer this question, then we can reduce the loss and get better accuracy.", "Please note in simple 2D dimensions it is easy to get to the minimum point quickly. However, most of the deep learning models deal with high dimensions.", "Luckily, there is a way in Math to answer these questions, derivatives (what you ignored in your high school :) ). Using the derivative, we can calculate the instantaneous rate of change for a tangent line on a graph using the derivative of Loss with respect to the Weight.", "If you are not familiar with derivatives and the previous sentence sounded like gibberish, then just think of it as a way to measure the slope and direction of a line that touches the graph at a specific point. Based on the slope direction for a given point, we can know if the point exists to the left or to the right of the red line.", "In the above figure, you can see that the right side of tangent line number 1 is pointing up which means it is a positive slope. Whereas the rest of the lines are pointing down, negative slopes. Also, note that the gradient of slope number 2 is bigger than the gradient of slope number 6. That is how we know how much we need to update the weight. The bigger the gradient is, the further the point is from the minimum point.", "After localizing the point to the left or right of the red line, we need to increase/decrease the weight in order to reduce the loss. However, let us say for a given point that exists to the left of the red line, how much should we increase the weight? Please note if you increase the weight significantly, then the point may pass the minimum loss to the other side. Then you have to reduce the weight, and so on. Adjusting the weight randomly is not efficient. Rather, we add a variable called \u201clearning rate\u201d denoted with \u201c\u03b7\u201d to control how the adjustment of weights. Generally, you start with a small learning rate to avoid passing the minimum loss. Why does it call the learning rate? Well, the process of reducing the loss and make better predictions is basically when the model learns. At that time, the learning rate is what controls how fast the model learns.", "Finally, we take the slope amount multiplied with the learning rate and we reduce this amount from the old weight in order to get a new weight. You will understand it better by looking at the bellow equation", "While gradient descent uses the entire dataset to compute the gradient, SGD uses a single example of the training dataset at each iteration. SGD typically reaches convergence much faster than batch or standard gradient descent. Batch Gradient Descent uses a batch of training examples each iteration.", "When a neural network is very deep, it has too many weights and biases. When that happens, neural networks tend to overfit their training data. In other words, the model will be so accurate to a specific classification task, without generalization. The model scores high scores against the training data, whereas it scores low against the test data. Dropout is one of the solutions.", "A simple, yet efficient way to avoid overfitting is to use dropout. For each layer, there is a dropout ratio which means deactivate a number of neurons associated with this ration. These neurons will be chosen randomly and will be turned off during that specific iteration. Next iteration, another set of randomly picked neurons will be deactivated, and so on. This helps in generalization the model rather than remembering specific features.", "I hope this post was helpful. Please let me know if you have any questions!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Ph.D. candidate interested in Machine Learning"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fe8bb39aec5d2&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funder-the-hood-of-deep-learning-e8bb39aec5d2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funder-the-hood-of-deep-learning-e8bb39aec5d2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funder-the-hood-of-deep-learning-e8bb39aec5d2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funder-the-hood-of-deep-learning-e8bb39aec5d2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----e8bb39aec5d2--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e8bb39aec5d2--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@gharibimo?source=post_page-----e8bb39aec5d2--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@gharibimo?source=post_page-----e8bb39aec5d2--------------------------------", "anchor_text": "Mohamed Gharibi"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F532ac9d36827&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funder-the-hood-of-deep-learning-e8bb39aec5d2&user=Mohamed+Gharibi&userId=532ac9d36827&source=post_page-532ac9d36827----e8bb39aec5d2---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe8bb39aec5d2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funder-the-hood-of-deep-learning-e8bb39aec5d2&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe8bb39aec5d2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funder-the-hood-of-deep-learning-e8bb39aec5d2&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "http://alexlenail.me/NN-SVG/index.html", "anchor_text": "Alexlenail"}, {"url": "http://mathworld.wolfram.com/SigmoidFunction.html", "anchor_text": "WolframMathWorld"}, {"url": "http://mathworld.wolfram.com/HyperbolicTangent.html", "anchor_text": "WolframMathWorld"}, {"url": "https://medium.com/@danqing?source=post_page-----b83ca804f1f7----------------------", "anchor_text": "Danqing Liu"}, {"url": "https://books.google.com/books/about/Deep_Learning.html?id=omivDQAAQBAJ&printsec=frontcover&source=kp_read_button&ppis=_c#v=onepage&q&f=false", "anchor_text": "Deep Learning. By Ian Goodfellow, Yoshua Bengio, and Aaron Courville."}, {"url": "https://www.amazon.com/Grokking-Deep-Learning-Andrew-Trask/dp/1617293709", "anchor_text": "Grokking Deep Learning. By Andrew Trask."}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----e8bb39aec5d2---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----e8bb39aec5d2---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----e8bb39aec5d2---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/image-classification?source=post_page-----e8bb39aec5d2---------------image_classification-----------------", "anchor_text": "Image Classification"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----e8bb39aec5d2---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe8bb39aec5d2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funder-the-hood-of-deep-learning-e8bb39aec5d2&user=Mohamed+Gharibi&userId=532ac9d36827&source=-----e8bb39aec5d2---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe8bb39aec5d2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funder-the-hood-of-deep-learning-e8bb39aec5d2&user=Mohamed+Gharibi&userId=532ac9d36827&source=-----e8bb39aec5d2---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe8bb39aec5d2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funder-the-hood-of-deep-learning-e8bb39aec5d2&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e8bb39aec5d2--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fe8bb39aec5d2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funder-the-hood-of-deep-learning-e8bb39aec5d2&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----e8bb39aec5d2---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----e8bb39aec5d2--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----e8bb39aec5d2--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----e8bb39aec5d2--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----e8bb39aec5d2--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----e8bb39aec5d2--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----e8bb39aec5d2--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----e8bb39aec5d2--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----e8bb39aec5d2--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@gharibimo?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@gharibimo?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Mohamed Gharibi"}, {"url": "https://medium.com/@gharibimo/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "98 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F532ac9d36827&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funder-the-hood-of-deep-learning-e8bb39aec5d2&user=Mohamed+Gharibi&userId=532ac9d36827&source=post_page-532ac9d36827--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F532ac9d36827%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funder-the-hood-of-deep-learning-e8bb39aec5d2&user=Mohamed+Gharibi&userId=532ac9d36827&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}