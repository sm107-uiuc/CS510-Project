{"url": "https://towardsdatascience.com/the-singular-value-decomposition-without-algebra-ae10147aab4c", "time": 1683009507.7162812, "path": "towardsdatascience.com/the-singular-value-decomposition-without-algebra-ae10147aab4c/", "webpage": {"metadata": {"title": "The Singular Value Decomposition without Algebra | by Ravi Charan | Towards Data Science", "h1": "The Singular Value Decomposition without Algebra", "description": "The Singular Value Decomposition (SVD) is the ultimate linear algebra tool. It can be understood geometrically, not with algebra for clearer understanding."}, "outgoing_paragraph_urls": [{"url": "https://medium.com/@rmcharan/regression-geometry-61fdd5515ab7", "anchor_text": "Understanding Regression with Geometry", "paragraph_index": 0}, {"url": "https://www.google.com/search?q=latent+semantic+analysis&oq=latent+semant&aqs=chrome.1.69i57j0l7.2743j0j7&sourceid=chrome&ie=UTF-8", "anchor_text": "Latent Semantic Analysis", "paragraph_index": 1}, {"url": "https://en.wikipedia.org/wiki/Collaborative_filtering", "anchor_text": "collaborative filtering", "paragraph_index": 1}, {"url": "https://en.wikipedia.org/wiki/Matrix_factorization_(recommender_systems)", "anchor_text": "matrix factorization", "paragraph_index": 1}, {"url": "https://en.wikipedia.org/wiki/Recommender_system", "anchor_text": "recommendation systems", "paragraph_index": 1}, {"url": "https://en.wikipedia.org/wiki/Basis_(linear_algebra)", "anchor_text": "basis", "paragraph_index": 10}, {"url": "https://en.wikipedia.org/wiki/Singular_value_decomposition", "anchor_text": "Wikipedia", "paragraph_index": 37}, {"url": "https://en.wikipedia.org/wiki/Spectral_theorem", "anchor_text": "Spectral Theorem", "paragraph_index": 42}, {"url": "https://en.wikipedia.org/wiki/Normal_matrix", "anchor_text": "normal", "paragraph_index": 43}, {"url": "https://en.wikipedia.org/wiki/Transpose", "anchor_text": "transpose", "paragraph_index": 44}, {"url": "https://en.wikipedia.org/wiki/Singular_value_decomposition#Based_on_the_spectral_theorem", "anchor_text": "Proving", "paragraph_index": 47}, {"url": "https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse#Singular_value_decomposition_(SVD)", "anchor_text": "Moore-Penrose Pseudoinverse", "paragraph_index": 49}, {"url": "https://en.wikipedia.org/wiki/Hilbert_space", "anchor_text": "Hilbert space", "paragraph_index": 51}, {"url": "https://en.wikipedia.org/wiki/Linear_independence", "anchor_text": "linearly independent", "paragraph_index": 54}, {"url": "https://en.wikipedia.org/wiki/Basis_(linear_algebra)", "anchor_text": "basis", "paragraph_index": 54}, {"url": "https://en.wikipedia.org/wiki/General_position", "anchor_text": "generic", "paragraph_index": 54}], "all_paragraphs": ["It is an unfortunate reality that many people try to learn linear algebra with algebra when what\u2019s going on is essentially a geometric matter (that means you can understand it with pictures). I previously described this point of view in Understanding Regression with Geometry and here I\u2019m going to attempt something similar with the Singular Value Decomposition.", "The Singular Value Decomposition is the ultimate linear algebra concept. With it you can understand, among many things, what a matrix really is, a variety of other related decompositions, eigenvalues and eigenvectors, and pseudo-inverses. On the computer science/data science side, the Singular Value Decomposition underlies techniques like Latent Semantic Analysis in NLP and is related to a variety of collaborative filtering/matrix factorization algorithms used in recommendation systems.", "To get to the Singular Value Decomposition we\u2019re first going to dispel the myth that a vector is a list of numbers and matrix is an array of them. Then we\u2019ll present and describe the decomposition.", "The background assumed here is that you know what a vector is (bonus if you think it is a list of numbers), and likewise a matrix. You should be familiar with:", "Technical Note: this article is restricted to considering real numbers and finite dimensions (our vectors are made up of a finite list of numbers, instead of an infinite list). Also, we add the technical assumption that we have an inner product structure, which you can just take to mean that the Pythagorean Theorem works.\u00b9 Everything we do will work with complex numbers or with infinite dimensions: the former requires adjusting a few words; the latter makes things extremely complicated.", "For starters, let\u2019s set up our geometric context: good old fashioned Euclidean space. You should picture either a perfectly flat infinite plane (2-dimensional space) or my personal favorite, 3 dimensional space like the one we live in.", "Here\u2019s the catch. You are probably used to drawing an x- and y-axis on the 2-dimensional space, or x-, y-, and z-axes on the 3-dimensional space. This is very misleading, so please erase them from your picture. Instead, place a dot somewhere in the space (the \u201cmiddle\u201d usually) and call it the origin.", "Please note that everything we are going to do works in n dimensions, where n could be a large integer. I am a mere mortal and I can only visualize in 3 dimensions and I draw best in 2. If you can think in more of them, more power to you.", "The diagram below shows on the left two vectors A and B, as well as their sum. On the right are two different ways of adding axes to the 2-dimensional space. Depending on how we choose the axes, we get different numbers! We\u2019re not going to worry about the relationship between those numbers (that is, of course, where the algebra comes in, and I promised we wouldn\u2019t do any of that).", "The essential point is that the vector is the vector is the vector: its existence is independent of the numbers used to describe it, if any. This is one of the essential insights underlying Einstein\u2019s theories of relativity: depending on your perspective, you might describe something differently; but that can\u2019t change what it is.", "A few fine points here. First of all, what a vector is includes its length (distance from the origin) and its angles relative to other vectors.\u00b9 That this is the essence of having a geometry. Second, we chose our axes to be orthogonal (perpendicular), something we didn\u2019t have to do. But things get very complicated if you don\u2019t choose them so and throughout this article we will always assume we have orthogonal axes.\u00b2 Third, we should technically pick a basis and not axes. However, this just makes things more complicated by adding algebra and subtracting intuition. If you are concerned, the basis is just made by choosing the vector of length 1 pointing in the positive direction of each axis.\u00b3", "Not much to say here. A vector space is a fancy word for\u2026 the space that the vectors live in. In a sense, it is just the collection of all the relevant vectors. And there is a special vector 0 (the origin). So in the diagram above, the vectors lived in a 2 dimensional vector space.", "The only reason I bring this up is because we will need to deal with more than one vector space at a time. For example, we might have a 2 dimensional vector space V and a 4 dimensional vector space W. The vectors in V have nothing to do with the vectors in W (for now). You can\u2019t add a vector in V to a vector in W or anything.", "\u201cIt is my experience that proofs involving matrices can be shortened by 50% if one throws the matrices out.\u201d \u2014 Emil Artin", "A lot of confusion here stems from the fact that, in a large number of computing contexts, a matrix is just an array of numbers and doesn\u2019t participate in the full mathematical structure involved. But, if you are reading this, you are hopefully interested in that full mathematical structure.", "A \u201cmap\u201d is just a function but with the connotation that it \u201cpreserves structure.\u201d A function (in math) is something that takes in inputs and gives out outputs. In the context of a vector space, the structure is \u201cthere is an origin\u201d and \u201cyou can add vectors\u201d as well as \u201cmultiply them by a constant.\u201d This structure is called linearity so in this context by \u201cmap\u201d we always implicitly mean \u201clinear map\u201d (hence the name: linear algebra).", "So a matrix is a function f that takes vectors from one vector space (\u201cthe source\u201d) and returns vectors in another vector space (\u201cthe target\u201d). Plus it has to satisfy three properties to make it linear:", "In a 2 dimensional source vector space, we can pick two general vectors like A and B as shown and the function can return any output for those two inputs. In an n-dimensional source, we need to pick n general vectors. The target can be any number of dimensions from 0 to 1 trillion or higher. Note by \u201cgeneral\u201d I\u2019m ruling out a few degenerate situations like (say) if A and B are the same vector, or B is twice A.\u2074 With this setup, we require:", "Note that, on occasion, the source and the target could be the same, but that doesn\u2019t change anything conceptually.", "Last thing about what a matrix is. Just like the numbers in a vector change when we re-assign our axes, so do the numbers in a matrix. They depend on the axes used in both the source and the target. So really, a matrix is a representation of a linear map with an implicit or explicit choice of the axes in the source and target spaces.", "In the following, we will assume every matrix comes with an implicit choice of axes; if we change the axes, the numbers in the matrix will change but the linear map itself won\u2019t. For this reason we will slightly abuse the term and use \u201cmatrix\u201d to mean \u201clinear map.\u201d The essential insight here is that you shouldn\u2019t think about arrays of numbers at all: you should think about linear maps.", "Okay, so now let\u2019s build up a stable of examples. The diagram below shows some possible things that a linear map between two 2-dimensional vector spaces can do. The images f(A) and f(B) in the target are just labeled A and B for simplicity.", "Items 1\u20134 show some things that a linear map can do. In addition, it can do any combination of these things, and an example is shown in the bottom right.", "The crucial thing to keep in mind is that in an n-dimensional space, a lot more combinations of these make sense. For example, with 5 dimensions and perpendicular vectors A, B, C, D, and E, we might have A, B, and C subject to a three dimensional rotation along with a reflection of C, D collapsed onto C, and E killed. (Yes, really, it\u2019s called \u201ckilling\u201d the vector; a violent metaphor but that\u2019s what mathematicians say). In some contexts, a lot more can happen in dimensions higher than 2 or 3, but this is not one of those contexts.", "We are now in a position to give an informal statement of the Singular Value Decomposition.", "Theorem (Singular Value Decomposition, Informal Statement). Every linear map is just a combination of rotation, reflection, scaling, and killing of vectors, provided the axes are chosen correctly.", "A couple points. \u201cKilling\u201d a vector is really just scaling it by zero. Notice also that I left out collapsing: collapsing is actually made up of the others, as we will see.", "So if you understand the diagram above (and how it would work in, say, 4 dimensions), then you understand the essence of everything there is to know about a linear map in this context (officially: real, finite dimensional vector spaces with an inner product structure).", "Here\u2019s an example. If you look at the diagram above, you won\u2019t see anything like a \u201csmush.\u201d In fact, the \u201csmush\u201d shown is really a scaling of a single vector; but instead of thinking about A and B, we need to consider the vectors x and y shown.", "Remember, we said that to describe a linear map from a 2-dimensional source vector space, we just need to pick two generic vectors and see where they go. But the point is that, if those two vectors are chosen carefully, the map will admit a simple description.", "Now, I said that we should pick the axes carefully, not these two vectors. But really doing one is doing the other. I have suggestively named the two vectors x and y because really, we are picking x- and y-axes, and then making x and y just vectors that point along those axes. Previously we had chosen A and B to point out our axes for us (the A-axis goes along A, the B-axis goes along B \u2013 we can name the axes whatever we want). So thinking about the two vectors is the same as thinking about the two axes. Do whichever comes naturally to you.", "The official statement of the Singular Value Decomposition necessarily involves just the slightest amount of Algebra, but I\u2019ll endeavor to explain it geometrically in English.", "Let A be a matrix with m rows and n columns (m\u2a09n), regarded as a linear map from an n-dimensional vector space V to an m-dimensional vector space W.", "Then A may be factored as the product of three matrices A = R*D*S, where:", "Recall that the linear map A implicitly comes with a choice of axes, specifically the usual x-, y-, z-, etc.-axes in both V and W.", "This means that the linear map A from V to W may be considered to happen in three steps. First, rotate vectors in V using S. Second, scale vectors along each axis by a constant (possibly 0 which kills them or also negative) and then put them into W by using the axes that implicitly come with A (using D). Finally, perform a rotation of the vectors in W (using R).", "Please note that the way matrix multiplication works, S acts first, then D, then R (since the vector goes on the right).", "Please also note that if you look up the Singular Value Decomposition on Wikipedia you will see a slightly different statement. My version is, I assure you, also true and much simpler to boot.", "The statement on Wikipedia uses a \u201ctranspose\u201d of S instead of S itself. This is to make explicit that the rotation matrices are effectively rejiggering the axes that we chose. The Wikipedia statement also allows S and R to include an extra reflection if they want, but we can accomplish that with a negative scaling in D instead.", "But, once we realize that a rotation matrix is equivalent to changing the axes (it is, I promise), then hopefully you will see why the informal statement of theorem above is true. If we were allowed to choose our axes appropriately in the first place, we could just write A as a single diagonal matrix which just scales the axes and moves them from the source V to the target W.", "Some terminology: the special axes that make A diagonal in the source and target are called left-singular vectors (in the target) and right-singular vectors (in the source). They are the columns of R and rows S are the left- and right-singular vectors. The values on the diagonal of D are called the singular values.", "I\u2019m only going to provide a construction of the matrices S, R, and D to add a little more geometric intuition to the Singular Value decomposition. This won\u2019t amount to a full proof.", "The Singular Value Decomposition generalizes the Spectral Theorem. The spectral theorem is so-called because the idea is that there is a ghost (\u201cspecter\u201d) of the \u201ctrue nature\u201d of the matrix that can be extracted. In the context of the Spectral Theorem, the \u201ctrue nature\u201d is that we can choose axes (using the singular vectors) so that the matrix is just a scaling matrix. \u201cEigen\u201d is the German word for \u201ctrue\u201d and so in the context of the Spectral Theorem, the singular vectors are instead called \u201ceigenvectors\u201d and the singular values are instead called \u201ceigenvalues.\u201d", "The Spectral Theorem is only for square matrices which have the same source and target. It also only works for some square matrices (\u201cnormal\u201d ones), not all. By comparison, the Singular Value Decomposition works for non-square matrices and it works for every matrix.", "You may have heard of the matrix transpose. Provided we have been nice and not naughty when choosing our axes (they need to be perpendicular), the matrix transpose is just flipping a matrix across the diagonal and onto its side.", "The transposes is a backwards map. If A maps the source V to the target W, then its transpose is backwards: the transpose instead has source W and target V.", "Let A be a matrix with source V and target W. Following it by its transpose results in a map from the source to itself.", "It turns out that the spectral theorem applies to this map. The eigenvectors provide the right-singular vectors. Likewise, if you first do A-transpose and then do A, you will get the left-singular vectors. Once you have these, all you have to do is extract the singular values as the square root of the eigenvalues. Proving that this will work is a little bit more complicated, but not crazy.", "I hope you have seen that treating a matrix as a linear map instead of an array of numbers is a very powerful means of understanding them. Each linear map can be described very simply as a diagonal scaling matrix provided the right set of axes is chosen for the source and target spaces. In applications, it is often those axes that are of interest as they in some sense provide the \u201ctrue\u201d and otherwise \u201clatent\u201d way of looking at the matrix.", "As an exercise, if you\u2019ve understood this you should be able to understand the Moore-Penrose Pseudoinverse which generalizes the inverse matrix to non-square matrices and non-invertible matrices.", "If you followed this, you should be able to take this understanding to any good linear algebra textbook. What we called \u201caxes\u201d becomes a \u201cbasis\u201d and most of the work of linear algebra comes from doing computations about bases as well as examining in greater detail and with more terminology what a matrix can do. But the vast majority can be understood through the lens of the Singular Value Decomposition.", "[1] Officially, we are working with real finite-dimensional Hilbert Spaces, and the theorem has an appropriate generalization to any Hilbert space. As stated, to keep things simple, this article will always assume we are working in a finite-dimensional real vector space with an inner product-structure (which automatically makes it a Hilbert Space since we have finite dimension).", "[2] In particular, if the axes aren\u2019t orthogonal the transpose of a matrix will no longer consist of flipping the numbers over the diagonal.", "[3] For those in the know, we are assuming we have an orthonormal basis but we\u2019ll avoid talking about bases as much as possible.", "[4] Officially, we require that the source vectors chosen be linearly independent and span the space, which makes them a basis. The official definition of \u201cgeneral\u201d is loosely borrowed from the concept of generic in algebraic geometry and roughly means that, in the space of choices of basis vectors we use to define the linear map, the unacceptable choices form a sub-variety of co-dimension at least 1.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Scientist, Mathematician. Formerly @MIT, @McKinsey, currently teaching computers to read"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fae10147aab4c&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-singular-value-decomposition-without-algebra-ae10147aab4c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-singular-value-decomposition-without-algebra-ae10147aab4c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-singular-value-decomposition-without-algebra-ae10147aab4c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-singular-value-decomposition-without-algebra-ae10147aab4c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----ae10147aab4c--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ae10147aab4c--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@rmcharan?source=post_page-----ae10147aab4c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@rmcharan?source=post_page-----ae10147aab4c--------------------------------", "anchor_text": "Ravi Charan"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F393ce2bbf82c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-singular-value-decomposition-without-algebra-ae10147aab4c&user=Ravi+Charan&userId=393ce2bbf82c&source=post_page-393ce2bbf82c----ae10147aab4c---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fae10147aab4c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-singular-value-decomposition-without-algebra-ae10147aab4c&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fae10147aab4c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-singular-value-decomposition-without-algebra-ae10147aab4c&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@badashphotos?utm_source=medium&utm_medium=referral", "anchor_text": "Ash Edmonds"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://medium.com/@rmcharan/regression-geometry-61fdd5515ab7", "anchor_text": "Understanding Regression with Geometry"}, {"url": "https://www.google.com/search?q=latent+semantic+analysis&oq=latent+semant&aqs=chrome.1.69i57j0l7.2743j0j7&sourceid=chrome&ie=UTF-8", "anchor_text": "Latent Semantic Analysis"}, {"url": "https://en.wikipedia.org/wiki/Collaborative_filtering", "anchor_text": "collaborative filtering"}, {"url": "https://en.wikipedia.org/wiki/Matrix_factorization_(recommender_systems)", "anchor_text": "matrix factorization"}, {"url": "https://en.wikipedia.org/wiki/Recommender_system", "anchor_text": "recommendation systems"}, {"url": "https://en.wikipedia.org/wiki/Basis_(linear_algebra)", "anchor_text": "basis"}, {"url": "https://en.wikipedia.org/wiki/Singular_value_decomposition", "anchor_text": "Wikipedia"}, {"url": "https://en.wikipedia.org/wiki/Spectral_theorem", "anchor_text": "Spectral Theorem"}, {"url": "https://en.wikipedia.org/wiki/Normal_matrix", "anchor_text": "normal"}, {"url": "https://en.wikipedia.org/wiki/Transpose", "anchor_text": "transpose"}, {"url": "https://en.wikipedia.org/wiki/Singular_value_decomposition#Based_on_the_spectral_theorem", "anchor_text": "Proving"}, {"url": "https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse#Singular_value_decomposition_(SVD)", "anchor_text": "Moore-Penrose Pseudoinverse"}, {"url": "https://en.wikipedia.org/wiki/Hilbert_space", "anchor_text": "Hilbert space"}, {"url": "https://en.wikipedia.org/wiki/Linear_independence", "anchor_text": "linearly independent"}, {"url": "https://en.wikipedia.org/wiki/Basis_(linear_algebra)", "anchor_text": "basis"}, {"url": "https://en.wikipedia.org/wiki/General_position", "anchor_text": "generic"}, {"url": "https://medium.com/tag/mathematics?source=post_page-----ae10147aab4c---------------mathematics-----------------", "anchor_text": "Mathematics"}, {"url": "https://medium.com/tag/data-science?source=post_page-----ae10147aab4c---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/linear-algebra?source=post_page-----ae10147aab4c---------------linear_algebra-----------------", "anchor_text": "Linear Algebra"}, {"url": "https://medium.com/tag/geometry?source=post_page-----ae10147aab4c---------------geometry-----------------", "anchor_text": "Geometry"}, {"url": "https://medium.com/tag/editors-pick?source=post_page-----ae10147aab4c---------------editors_pick-----------------", "anchor_text": "Editors Pick"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fae10147aab4c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-singular-value-decomposition-without-algebra-ae10147aab4c&user=Ravi+Charan&userId=393ce2bbf82c&source=-----ae10147aab4c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fae10147aab4c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-singular-value-decomposition-without-algebra-ae10147aab4c&user=Ravi+Charan&userId=393ce2bbf82c&source=-----ae10147aab4c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fae10147aab4c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-singular-value-decomposition-without-algebra-ae10147aab4c&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ae10147aab4c--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fae10147aab4c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-singular-value-decomposition-without-algebra-ae10147aab4c&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----ae10147aab4c---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----ae10147aab4c--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----ae10147aab4c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----ae10147aab4c--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----ae10147aab4c--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----ae10147aab4c--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----ae10147aab4c--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----ae10147aab4c--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----ae10147aab4c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@rmcharan?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@rmcharan?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ravi Charan"}, {"url": "https://medium.com/@rmcharan/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "599 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F393ce2bbf82c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-singular-value-decomposition-without-algebra-ae10147aab4c&user=Ravi+Charan&userId=393ce2bbf82c&source=post_page-393ce2bbf82c--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F6bca6dd641ca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-singular-value-decomposition-without-algebra-ae10147aab4c&newsletterV3=393ce2bbf82c&newsletterV3Id=6bca6dd641ca&user=Ravi+Charan&userId=393ce2bbf82c&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}