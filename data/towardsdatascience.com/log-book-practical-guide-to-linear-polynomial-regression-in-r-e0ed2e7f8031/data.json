{"url": "https://towardsdatascience.com/log-book-practical-guide-to-linear-polynomial-regression-in-r-e0ed2e7f8031", "time": 1682996852.081909, "path": "towardsdatascience.com/log-book-practical-guide-to-linear-polynomial-regression-in-r-e0ed2e7f8031/", "webpage": {"metadata": {"title": "Log Book \u2014 Practical guide to Linear & Polynomial Regression in R | by dearC | Towards Data Science", "h1": "Log Book \u2014 Practical guide to Linear & Polynomial Regression in R", "description": "I am relatively new in my journey within the Data Science realm and this article is sort of a note on a topic that I make. These might be of help to other fellow enthusiasts. Please note that as\u2026"}, "outgoing_paragraph_urls": [{"url": "https://drive.google.com/file/d/18bAZnwswJGxzalGmeJ_ou1we_ofMM-5k/view?usp=sharing", "anchor_text": "here", "paragraph_index": 9}, {"url": "http://www.sthda.com/english/articles/40-regression-analysis/163-regression-with-categorical-variables-dummy-coding-essentials-in-r/", "anchor_text": "link", "paragraph_index": 13}, {"url": "https://www.researchgate.net/post/What_is_the_acceptable_range_of_skewness_and_kurtosis_for_normal_distribution_of_data", "anchor_text": "here", "paragraph_index": 23}, {"url": "http://cran.r-project.org/web/packages/doParallel/index.html", "anchor_text": "doParallel", "paragraph_index": 57}, {"url": "https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/", "anchor_text": "excellent Analytics vidhya article,", "paragraph_index": 82}], "all_paragraphs": ["I am relatively new in my journey within the Data Science realm and this article is sort of a note on a topic that I make. These might be of help to other fellow enthusiasts.", "Please note that as these are my study notes there are sections which are referenced form other sources. I have given a detailed reference at the end of all the sources which have helped me.", "Simple linear regression estimates exactly how much Y will change when X changes by a certain amount. With the correlation coefficient, the variables X and Y are interchangeable. With regression, we are trying to predict the Y variable from X using a linear relationship (i.e., a line):", "The symbol b0 is known as the intercept (or constant), and b1 as the slope for X. Both appear in R output as coefficients. The Y variable is known as the response or dependent variable since it depends on X. The X variable is known as the predictor or independent variable. The machine learning community tends to use other terms, calling Y the target and X a feature vector.", "Important concepts in regression analysis are the fitted values and residuals. In general, the data doesn\u2019t fall exactly on a line, so the regression equation should include an explicit error term :", "The fitted values, also referred to as the predicted values, are typically denoted by (Y-hat). These are given by:", "where b0 & b1 indicates that the coefficients are estimated versus known.", "When there are multiple predictors, the equation is simply extended to accommodate them:", "Instead of a line, we now have a linear model \u2014 the relationship between each coefficient and its variable (feature) is linear. We will have a plane which will pass through the sample points right on the \u201cmiddle\u201d, something like the image below. In two dimensions, it\u2019s a line, in three a plane, in N, a hyperplane.", "Now that we have an idea on what is linear regression is, we will go through the details of different types of regression models to predict the price of the very popular King County housing data set. You can download the data set from here. The goal is that we use different models to define a plane which best fits our data with low error rate. This plane/line will then be used to predict outcomes as new data come in.", "Before beginning with the analysis, please make sure you have the below R packages installed and loaded:", "An example of using regression is in estimating the value of houses. County assessors must estimate the value of a house for the purposes of assessing taxes. Real estate consumers and professionals consult popular websites such as Zillow to ascertain a fair price. Here are a few rows of housing data from King County (Seattle), Washington, from the house data.frame:", "The data-set itself consists of 21,613 examples with 19 features[not considering the id and date columns]. The features are:", "Now if you notice the data, few values you see are given as categorical for example waterfront, view, condition, grade, etc. Now there is also a scope for you to group the zip code variable, as the zip code number on its own does not define anything, but the price of a house is location dependent. If you know that some zip codes belong to the posh area of the town then you can group them into zipgroup1, next such area as zipgroup2 and so on. We will not be doing that here as we have very little idea as to which zip code area belongs to which part of town. But we do have the lat and long values so we will be using those. But if you want to learn more about converting data into categorical values then you can refer to this link. It is usually done using the relevel command.", "You can check for missing data using the below command:", "as per this graph there are no missing values.", "Now we will remove the id column and the date column (all the information is gathered within one year only).", "In multiple regression, the predictor variables are often correlated with each other. Correlation is a problem because independent variables should be independent, and if two variables are correlated then it means that you are basically using almost same feature multiple times. This will give incorrect results. Once you find the correlated variables you can keep one and remove another.", "Many methods perform better if highly correlated attributes are removed. You can check the correlated variables using the below command:", "Now that we know which pairs of variables are correlated we can choose which one keep and which one to remove:", "So we are removing grade, sqft_above, sqft_living15, sqft_lot15 as the information carried by them is already provided by the other variables.", "If you want to reduce the dimensionality of your data-set, you apply the function nearZeroVar from the caret package. It diagnoses predictors that have either one or very few unique values relative to the number of samples and the ratio of the frequency of the most common value to the frequency of the second most common value is large. As of now we won\u2019t be doing that as we have 14 features left.", "Though I would remove the zipcode from the dataset as the location data is contained in lat and long.", "Next we will try to analyze the skewness of our numeric variables. Skewness check is done to make the data normal which is one of the assumptions of linear regression. Some people suggest here that an acceptable range of values for skewness lies between (-2,2). Consequently, we detect which variables are not within this range and they will be transformed using the log function[we will not transform the categorical data].", "Transforming the required data to log:", "The difference in the skewness before and after transforming can be checked in the below 2 histograms:", "Now that we have done our EDA, we will start with the goal to predict the sales price from the other variables. To begin with we will take all the variables/features to make our model; the argument na.action=na.omit causes the model to drop records that have missing values.", "The loaded dataset is separated into training and validation sets using a 70/30 split. Now, we have a training dataset that we will use to train our models and a validation set to use later to measure the performance of our models.", "As we can see running a summary of our linear model, the coefficient of determination (or R-Squared) is good. A 70.8% of the variance in the outcome is predictable from the linear regression model that we created.", "Next we will analyse the RMSE (or Root Mean Square Error) of our model.", "As we can see above, our RMSE is 0.28. It measures the differences between prices predicted by our model and the actual values. The lower the value, the better it is. Ours is close to 0 so it is a good indicator.", "Now that we have a basic model we need to understand the performance of the model:", "The most important performance metric from a data science perspective is root mean squared error, or RMSE. RMSE is the square root of the average squared error in the predicted values:", "You can calculate the RMSE in the above model by", "This measures the overall accuracy of the model, and is a basis for comparing it to other models (including models fit using machine learning techniques).", "Similar to RMSE is the residual standard error, or RSE. In this case we have p predictors, and the RSE is given by:", "The only difference is that the denominator is the degrees of freedom, as opposed to number of records. In practice, for linear regression, the difference between RMSE and RSE is very small, particularly for big data applications.", "In our case the observed price values deviate from the predicted price by approximately 202700 units in average. This corresponds to an error rate of 202700/mean(house$price) = 37.5%, which is high.", "Residual sum of squares (RSS) is the sum of the squared residuals:", "Residual standard error (RSE) is the square root of (RSS / degrees of freedom):", "Another useful metric that you will see in software output is the coefficient of determination, also called the R-squared statistic. R-squared ranges from 0 to 1 and measures the proportion of variation in the data that is accounted for in the model. It is useful mainly in explanatory uses of regression where you want to assess how well the model fits the data. The formula is:", "The denominator is proportional to the variance of Y. The output from R also reports an adjusted R-squared, which adjusts for the degrees of freedom; seldom is this significantly different in multiple regression.", "In our case we have R2 as 0.69 which is ok.", "Along with the estimated coefficients, R reports the standard error of the coefficients (SE) and a t-statistic:", "The t-statistic \u2014 and its mirror image, the p-value \u2014 measures the extent to which a coefficient is \u201cstatistically significant\u201d \u2014 that is, outside the range of what a random chance arrangement of predictor and target variable might produce. The higher the t-statistic (and the lower the p-value), the more significant the predictor.", "In addition to the t-statistic, R and other packages will often report a p-value (Pr(>|t|) in the R output) and F-statistic. Data scientists do not generally get too involved with the interpretation of these statistics, nor with the issue of statistical significance. Data scientists primarily focus on the t-statistic as a useful guide for whether to include a predictor in a model or not. High t-statistics (which go with p-values near 0) indicate a predictor should be retained in a model, while very low t-statistics indicate a predictor could be dropped.", "We can get some insights from the graphic representation of our linear model:", "Now how do we read these graphs?", "Some texts tell you that points for which Cook\u2019s distance is higher than 1 are to be considered as influential. Other texts give you a threshold of 4/N or 4/(N\u2212k\u22121), where N is the number of observations and k the number of explanatory variables. John Fox, in his booklet on regression diagnostics is rather cautious when it comes to giving numerical thresholds. He advises the use of graphics and to examine in closer details the points with \u201cvalues of D that are substantially larger than the rest\u201d. According to Fox, thresholds should just be used to enhance graphical displays. In our data we can see at least 2 such points which warrants out attention.", "With the above command and the graph we can get the row numbers of the outlier data, if you check these data manually you will see there are discrepancies in this data(one has got a bedroom count of 33), let us see what happens if we remove these:", "Again running the model, we get the below RMSE which has a minor improvement on the previous result:", "It is very difficult to guess which features to include in our model, as we did not know which features to select we took all features in our initial model. The importance of features can be estimated from data by building a model. Some methods like decision trees have a built in mechanism to report on variable importance. For other algorithms, the importance can be estimated using a ROC curve analysis conducted for each attribute. The varImp is then used to estimate the variable importance, which is printed and plotted.", "After running the above command we can see the features which the model thinks are important.", "Automatic feature selection methods can be used to build many models with different subsets of a dataset and identify those attributes that are and are not required to build an accurate model.", "A popular automatic method for feature selection provided by the caret R package is called Recursive Feature Elimination or RFE.", "The example below provides an example of the RFE method on the dataset. A Random Forest algorithm is used on each iteration to evaluate the model. The algorithm is configured to explore all possible subsets of the attributes. All 13 attributes are selected in this example, although in the plot showing the accuracy of the different attribute subset sizes, we can see that just 6 attributes gives almost comparable results.", "Now the problem with RFE algorithm is the huge amount of time it takes to run (I am talking about 10\u201312 hours), in order to reduce the time taken we can make use of a cluster (reduced to almost 2 hours, trying checking your task manager during this time).", "To tune a predictive model using multiple workers, a separate function is used to \u201cregister\u201d the parallel processing technique and specify the number of workers to use. For example, to use the doParallel package with five cores on the same machine, the package is loaded and them registered (check task manager once, you will notice the difference):", "As per the above model output the below 6 variables produce the lowest RMSE:", "If you notice that the feature selected in the RFE and the varIMP way are a bit different, different methods will select different subsets of features. There is likely no \u201cbest set\u201d of features just like there is no best model.", "Now if you have noticed we are using method=\u2019CV\u2019 so next we will see what this CV stands for. Classic statistical regression metrics (R2, F-statistics, and p-values) are all \u201cinsample\u201d metrics \u2014 they are applied to the same data that was used to fit the model. Intuitively, you can see that it would make a lot of sense to set aside some of the original data, not use it to fit the model, and then apply the model to the set aside (holdout) data to see how well it does. Normally, you would use a majority of the data to fit the model, and use a smaller portion to test the model. This idea of \u201cout-of-sample\u201d validation is not new, but it did not really take hold until larger data sets became more prevalent; with a small data set, analysts typically want to use all the data and fit the best possible model. Using a holdout sample, though, leaves you subject to some uncertainty that arises simply from variability in the small holdout sample. How different would the assessment be if you selected a different holdout sample?", "Cross-validation extends the idea of a holdout sample to multiple sequential holdout samples. The algorithm for basic k-fold cross-validation is as follows:", "The division of the data into the training sample and the holdout sample is also called a fold.", "In our problem, many variables could be used as predictors in a regression. There is a method called Stepwise regression which sequentially adds or removes features from a model and checks the performance of the model.", "Note that, the train() function [caret package] provides an easy workflow to perform stepwise selections using the leaps and the MASS packages. It has an option named method, which can take the following values:", "You also need to specify the tuning parameter nvmax, which corresponds to the maximum number of predictors to be incorporated in the model.", "For example, you can vary nvmax from 1 to 5. In this case, the function starts by searching different best models of different size, up to the best 5-variables model. That is, it searches the best 1-variable model, the best 2-variables model, \u2026, the best 5-variables models.", "We\u2019ll use 10-fold cross-validation to estimate the average prediction error (RMSE) of each of the 5 models. The RMSE statistical metric is used to compare the 5 models and to automatically choose the best one, where best is defined as the model that minimize the RMSE.", "Stepwise regression is very useful for high-dimensional data containing multiple predictor variables. Other alternatives are the penalized regression (ridge and lasso regression) and the principal components-based regression methods (PCR and PLS).", "The RMSE with 7 variables is almost similar to the one we found in the beginning with more variables.", "Adding more variables, does not necessarily mean we have a better model. Statisticians use the principle of Occam\u2019s razor to guide the choice of a model: all things being equal, a simpler model should be used in preference to a more complicated model.", "Including additional variables generally reduces RMSE and increases R2. Hence, these are not appropriate to help guide the model choice. In the 1970s, Hirotugu Akaike, the eminent Japanese statistician, deveoped a metric called AIC (Akaike\u2019s Information Criteria) that penalizes adding terms to a model. In the case of regression, AIC has the form:", "where p is the number of variables and n is the number of records. The goal is to find the model that minimizes AIC; models with k more extra variables are penalized by 2k.", "The formula for AIC may seem a bit mysterious, but in fact it is based on asymptotic results in information theory. There are several variants to AIC:", "AICc: a version of AIC corrected for small sample sizes.", "BIC or Bayesian information criteria: similar to AIC with a stronger penalty for including additional variables to the model.", "Mallows Cp: A variant of AIC developed by Colin Mallows.", "Data scientists generally do not need to worry about the differences among these in-sample metrics or the underlying theory behind them.", "How do we find the model that minimizes AIC? One approach is to search through all possible models, called all subset regression. This is computationally expensive and is not feasible for problems with large data and many variables. An attractive alternative is to use stepwise regression about which we learned above, this successively adds and drops predictors to find a model that lowers AIC. Simpler yet are forward selection and backward selection. In forward selection, you start with no predictors and add them one-by-one, at each step adding the predictor that has the largest contribution to , stopping when the contribution is no longer statistically significant. In backward selection, or backward elimination, you start with the full model and take away predictors that are not statistically significant until you are left with a model in which all predictors are statistically significant.", "The problem with stepwise methods is that they assess the fit of a variable based on the other variables that were in the model. Some people use the analogy of getting dressed to describe this problem. If a stepwise regression method was selecting your clothes, it would decide what clothes you should wear, based on the clothes it has already selected.", "Penalized regression is similar in spirit to AIC. Instead of explicitly searching through a discrete set of models, the model-fitting equation incorporates a constraint that penalizes the model for too many variables (parameters). Rather than eliminating predictor variables entirely \u2014 as with stepwise, forward, and backward selection \u2014 penalized regression applies the penalty by reducing coefficients, in some cases to near zero. Common penalized regression methods are ridge regression and lasso regression.", "Stepwise regression and all subset regression are in-sample methods to assess and tune models. This means the model selection is possibly subject to overfitting and may not perform as well when applied to new data. One common approach to avoid this is to use cross-validation to validate the models. In linear regression, overfitting is typically not a major issue, due to the simple (linear) global structure imposed on the data. For more sophisticated types of models, particularly iterative procedures that respond to local data structure, cross validation is a very important tool.", "This section is taken from this excellent Analytics vidhya article, to know more about the mathematics behind Ridge and Lasso Regression please do go through the link.", "Regularization helps to solve over fitting problem in machine learning. Simple model will be a very poor generalization of data. At the same time, complex model may not perform well in test data due to over fitting. We need to choose the right model in between simple and complex model. Regularization helps to choose preferred model complexity, so that the model is better at predicting. Regularization is nothing but adding a penalty term to the objective function and control the model complexity using that penalty term. It can be used for many machine learning algorithms. Both Ridge and Lasso regression uses L2 and L1 regularizations.", "Ridge and Lasso regression are powerful techniques generally used for creating parsimonious models in presence of a \u2018large\u2019 number of features. Here \u2018large\u2019 can typically mean either of two things:", "Though Ridge and Lasso might appear to work towards a common goal, the inherent properties and practical use cases differ substantially. If you\u2019ve heard of them before, you must know that they work by penalizing the magnitude of coefficients of features along with minimizing the error between predicted and actual observations. These are called \u2018regularization\u2019 techniques. The key difference is in how they assign penalty to the coefficients:", "Note that here \u2018LS Obj\u2019 refers to \u2018least squares objective\u2019, i.e. the linear regression objective without regularization.", "Now that we have a little idea of how ridge and lasso regression work, lets try to consolidate our understanding by comparing them and try to appreciate their specific use cases. I will also compare them with some alternate approaches. Lets analyze these under three buckets:", "Traditionally, techniques like stepwise regression were used to perform feature selection and make parsimonious models. But with advancements in Machine Learning, ridge and lasso regression provide very good alternatives as they give much better output, require fewer tuning parameters and can be automated to a large extent.", "Its not hard to see why the stepwise selection techniques become practically very cumbersome to implement in high dimensionality cases. Thus, lasso provides a significant advantage.", "3. Presence of Highly Correlated Features", "Along with Ridge and Lasso, Elastic Net is another useful techniques which combines both L1 and L2 regularization. It can be used to balance out the pros and cons of ridge and lasso regression. I encourage you to explore it further.", "Now let us try ridge and lasso regression on our dataset:", "Next we will try the Lasso regression and see if we can get any better output from that.", "Keep the RMSE score in mind which we will use to compare model performance further down the line.", "Weighted regression is used by statisticians for a variety of purposes; in particular, it is important for analysis of complex surveys. Data scientists may find weighted regression useful in two cases:", "For example, with the housing data, older sales are less reliable than more recent sales.", "Now that we have tried different modes of linear regression we can move to other methods and see if we can reduce the RMSE even further.", "In some cases, the true relationship between the outcome and a predictor variable might not be linear.", "There are different solutions extending the linear regression model for capturing these nonlinear effects, including:", "Suppose you suspect a nonlinear relationship between the response and a predictor variable, either by a priori knowledge or by examining the regression diagnostics. Polynomial terms may not flexible enough to capture the relationship, and spline terms require specifying the knots. Generalized additive models, or GAM, are a technique to automatically fit a spline regression. So we will try GAM on our data next.", "If you plot latitude against price then you will notice that a polynomial line will fit much better than a straight line, you can perform the same analysis for other variables and check their behavior.", "The below regression curve fits much better:", "Running GAM on our data, we have:", "The term s(SqFtTotLiving) tells the gam function to find the \u201cbest\u201d knots for a spline term.", "You can see that the RMSE score is better than normal regression.", "Random Forest is an algorithm capable of performing both regression and classification tasks. In the case of regression, it operates by constructing a multitude of decision trees at training time and outputting the class that is the mean prediction of the individual trees. We will study the Random Forest algorithm in details later.", "As we did before in the linear model, we split the dataset into train and validation sets. After that, we define the variables included in the model and we run it. Note we will be taking all the variables in this model:", "As we can see above, our RMSE for the Random Forest model is 0.18. The RMSE value is the lowest that we have seen so far.", "Gradient Boosting is one of the most powerful techniques for machine learning problems. It is an ensemble learning algorithm which combines the prediction of several base estimators in order to improve robustness over a single estimator. As Random Forest, it is also a tree classifier model. We will learn more about GBM later.", "The below code runs a GBM on our data:", "This is also good but not as good as the Random Forest results. To summarize the results of all the models that we have seen so far in this article:", "The result is clear Random Forest wins the day. But please note that it is not necessarily the fact that RF will perform better everytime, with different dataset different model will outperform RF and it is our job to find, tune and use the model which gives the best result.", "The article was long but hope you have learnt something from it. Please go through the below references they contain much more details on topics which I could not capture here. Until next time.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fe0ed2e7f8031&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flog-book-practical-guide-to-linear-polynomial-regression-in-r-e0ed2e7f8031&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flog-book-practical-guide-to-linear-polynomial-regression-in-r-e0ed2e7f8031&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flog-book-practical-guide-to-linear-polynomial-regression-in-r-e0ed2e7f8031&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flog-book-practical-guide-to-linear-polynomial-regression-in-r-e0ed2e7f8031&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----e0ed2e7f8031--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e0ed2e7f8031--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://dearc.medium.com/?source=post_page-----e0ed2e7f8031--------------------------------", "anchor_text": ""}, {"url": "https://dearc.medium.com/?source=post_page-----e0ed2e7f8031--------------------------------", "anchor_text": "dearC"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Feb63742fc872&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flog-book-practical-guide-to-linear-polynomial-regression-in-r-e0ed2e7f8031&user=dearC&userId=eb63742fc872&source=post_page-eb63742fc872----e0ed2e7f8031---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe0ed2e7f8031&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flog-book-practical-guide-to-linear-polynomial-regression-in-r-e0ed2e7f8031&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe0ed2e7f8031&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flog-book-practical-guide-to-linear-polynomial-regression-in-r-e0ed2e7f8031&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://drive.google.com/file/d/18bAZnwswJGxzalGmeJ_ou1we_ofMM-5k/view?usp=sharing", "anchor_text": "here"}, {"url": "http://www.sthda.com/english/articles/40-regression-analysis/163-regression-with-categorical-variables-dummy-coding-essentials-in-r/", "anchor_text": "link"}, {"url": "https://www.researchgate.net/post/What_is_the_acceptable_range_of_skewness_and_kurtosis_for_normal_distribution_of_data", "anchor_text": "here"}, {"url": "http://cran.r-project.org/web/packages/doParallel/index.html", "anchor_text": "doParallel"}, {"url": "https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/", "anchor_text": "excellent Analytics vidhya article,"}, {"url": "https://ccapella.github.io/post/predicting-king-county-usa-house-prices/", "anchor_text": "https://ccapella.github.io/post/predicting-king-county-usa-house-prices/"}, {"url": "https://www.kaggle.com/anwarmohammad/housing-prices-prediction-using-r-anwar/code", "anchor_text": "https://www.kaggle.com/anwarmohammad/housing-prices-prediction-using-r-anwar/code"}, {"url": "https://stats.stackexchange.com/questions/110999/r-confused-on-residual-terminology", "anchor_text": "https://stats.stackexchange.com/questions/110999/r-confused-on-residual-terminology"}, {"url": "https://stats.stackexchange.com/questions/22161/how-to-read-cooks-distance-plots", "anchor_text": "https://stats.stackexchange.com/questions/22161/how-to-read-cooks-distance-plots"}, {"url": "https://topepo.github.io/caret/parallel-processing.html", "anchor_text": "https://topepo.github.io/caret/parallel-processing.html"}, {"url": "http://www.sthda.com/english/articles/37-model-selection-essentials-in-r/154-stepwise-regression-essentials-in-r/", "anchor_text": "http://www.sthda.com/english/articles/37-model-selection-essentials-in-r/154-stepwise-regression-essentials-in-r/"}, {"url": "https://courses.analyticsvidhya.com/courses/introduction-to-data-science-2?utm_source=blog&utm_medium=RideandLassoRegressionarticle", "anchor_text": "https://courses.analyticsvidhya.com/courses/introduction-to-data-science-2?utm_source=blog&utm_medium=RideandLassoRegressionarticle"}, {"url": "http://www.sthda.com/english/articles/40-regression-analysis/162-nonlinear-regression-essentials-in-r-polynomial-and-spline-regression-models/", "anchor_text": "http://www.sthda.com/english/articles/40-regression-analysis/162-nonlinear-regression-essentials-in-r-polynomial-and-spline-regression-models/"}, {"url": "https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/", "anchor_text": "https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----e0ed2e7f8031---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/regression?source=post_page-----e0ed2e7f8031---------------regression-----------------", "anchor_text": "Regression"}, {"url": "https://medium.com/tag/data-science?source=post_page-----e0ed2e7f8031---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/r?source=post_page-----e0ed2e7f8031---------------r-----------------", "anchor_text": "R"}, {"url": "https://medium.com/tag/guides-and-tutorials?source=post_page-----e0ed2e7f8031---------------guides_and_tutorials-----------------", "anchor_text": "Guides And Tutorials"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe0ed2e7f8031&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flog-book-practical-guide-to-linear-polynomial-regression-in-r-e0ed2e7f8031&user=dearC&userId=eb63742fc872&source=-----e0ed2e7f8031---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe0ed2e7f8031&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flog-book-practical-guide-to-linear-polynomial-regression-in-r-e0ed2e7f8031&user=dearC&userId=eb63742fc872&source=-----e0ed2e7f8031---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe0ed2e7f8031&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flog-book-practical-guide-to-linear-polynomial-regression-in-r-e0ed2e7f8031&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e0ed2e7f8031--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fe0ed2e7f8031&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flog-book-practical-guide-to-linear-polynomial-regression-in-r-e0ed2e7f8031&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----e0ed2e7f8031---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----e0ed2e7f8031--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----e0ed2e7f8031--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----e0ed2e7f8031--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----e0ed2e7f8031--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----e0ed2e7f8031--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----e0ed2e7f8031--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----e0ed2e7f8031--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----e0ed2e7f8031--------------------------------", "anchor_text": ""}, {"url": "https://dearc.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://dearc.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "dearC"}, {"url": "https://dearc.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "151 Followers"}, {"url": "https://book.thedatascienceinterviewproject.com/", "anchor_text": "https://book.thedatascienceinterviewproject.com/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Feb63742fc872&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flog-book-practical-guide-to-linear-polynomial-regression-in-r-e0ed2e7f8031&user=dearC&userId=eb63742fc872&source=post_page-eb63742fc872--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F9d95fcc3fdb1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flog-book-practical-guide-to-linear-polynomial-regression-in-r-e0ed2e7f8031&newsletterV3=eb63742fc872&newsletterV3Id=9d95fcc3fdb1&user=dearC&userId=eb63742fc872&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}