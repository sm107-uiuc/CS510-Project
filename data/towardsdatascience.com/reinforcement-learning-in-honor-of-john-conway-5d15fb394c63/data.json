{"url": "https://towardsdatascience.com/reinforcement-learning-in-honor-of-john-conway-5d15fb394c63", "time": 1683008010.478074, "path": "towardsdatascience.com/reinforcement-learning-in-honor-of-john-conway-5d15fb394c63/", "webpage": {"metadata": {"title": "Reinforcement Learning in Honor of John Conway | by Ravi Charan | Towards Data Science", "h1": "Reinforcement Learning in Honor of John Conway", "description": "Philosopher\u2019s Football is a board game invented by the late and legendary mathematician John Conway (another victim of the virus). In his honor, I built a website where you can play and learn more\u2026"}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/John_Horton_Conway", "anchor_text": "John Conway", "paragraph_index": 0}, {"url": "http://philosophers.football/", "anchor_text": "philosophers.football", "paragraph_index": 0}, {"url": "https://github.com/rcharan/phutball", "anchor_text": "Github", "paragraph_index": 3}, {"url": "https://arxiv.org/abs/1512.03385", "anchor_text": "ResNet", "paragraph_index": 4}, {"url": "http://philosophers.football", "anchor_text": "philosophers.football", "paragraph_index": 6}, {"url": "https://en.wikipedia.org/wiki/Winning_Ways_for_your_Mathematical_Plays", "anchor_text": "Winning Ways for your Mathematical Plays", "paragraph_index": 8}, {"url": "https://www.youtube.com/watch?v=LfduUFF_i1A", "anchor_text": "Monty Python skit", "paragraph_index": 8}, {"url": "https://en.wikipedia.org/wiki/Conway%27s_Game_of_Life", "anchor_text": "Game of Life", "paragraph_index": 8}, {"url": "http://philosophers.football", "anchor_text": "philosophers.football", "paragraph_index": 62}, {"url": "http://incompleteideas.net/book/the-book-2nd.html", "anchor_text": "Sutton and Barto", "paragraph_index": 63}], "all_paragraphs": ["Philosopher\u2019s Football is a board game invented by the late and legendary mathematician John Conway (another victim of the virus). In his honor, I built a website where you can play and learn more about the game at philosophers.football.", "You can find someone else to play with online or locally and you can play against an AI player to get a feel for the game. The AI is still training as of this writing though \u2014 it may take a few weeks to get any good.", "My goal today is to explain the game quickly and cover just enough reinforcement learning basics to explain the custom modification of the TD(\u03bb) algorithm I am using, which I call Alternating TD(\u03bb).\u00b9 By the way, the TD means \u201cTemporal Difference (learning)\u201d and the \u03bb is a hyperparameter setting the decay for the \u201celigibility trace\u201d (similar to momentum). These will be explained below.", "The implementation is from scratch in PyTorch and you can find the code on Github. I opted against using reinforcement-learning frameworks like OpenAI Gym because the algorithm is relatively simple. For a custom \u201cenvironment\u201d (i.e. the game) and modified algorithm, the frameworks add more complications than they solve.", "For background, I\u2019m assuming you\u2019re familiar with PyTorch and some basics of Neural Networks, specifically what they are and backpropagation used in Stochastic Gradient Descent (SGD) (with momentum). The current implementation uses a Residual (Convolutional) Neural Network, essentially a smaller version of ResNet, but that isn\u2019t important to understanding this article. No prior knowledge of reinforcement learning is assumed.", "The rest of this article briefly covers the game then gets into basics of Reinforcement Learning. From there it covers the basics of Temporal Difference Learning and eligibility traces. Finally, it covers the modification I used for a symmetric 2-player board game, which I call Alternating TD(\u03bb).\u00b9", "I built a website at philosophers.football for the game so you can experience it yourself. Words can\u2019t do everything justice. To get a feel for it, you can read the rules and then play in sandbox mode (against yourself) or against a baseline bot like RandoTron, who always plays randomly.", "Otherwise, here is a condensed version of the rules, shorn of some of the details. On a 19x15 board like a Go board (often used for the game), stones are placed. There is only ever a single white stone, the ball. The black stones are \u201cplayers\u201d. The two opponents are called X and O by tradition. Both X and O can either place a player or jump the ball over some players on their move. X wins if the ball ends up on or over the right-most column; O wins on the left.", "The game was played at Cambridge by John Conway and described in a book, Winning Ways for your Mathematical Plays. It is named after a hilarious Monty Python skit. Conway may be most famous for his Game of Life and work on the classification of the finite simple groups, but he did a number of other interesting things in his lifetime!", "I chose to work on this game when I read about Conway\u2019s viral demise and before thinking about the reinforcement learning problems. But the game later turned out to have a very useful symmetry property that will substantially simplify the reinforcement learning discussed below.", "Consider the case where you are playing as X and it is your move. You will want to consider O\u2019s response to your move. After your move, it is O\u2019s turn. Now, if we turn the board around, it again appears to be your turn and you are playing towards the right (as X). Because the two players use exactly the same pieces, understanding how to evaluate the position you face as X is exactly the same as understanding the position you face as O.", "This is slightly different than games like Tic-Tac-Toe, Chess, or Go, in which, for example, you may want to use your evaluation of a position after you move to inform how much you like a position before you move (or vice versa). There are other ways to solve this issue, but Philosopher\u2019s Football has this nice symmetry which eliminates it.", "My goal in this section is not to write a complete introduction to reinforcement learning. Instead, my goal is to make this \u201cas simple as possible but no simpler\u201d for our limited purposes of explaining the (Alternating) TD(\u03bb) algorithm.\u00b2 In particular (for those in the know), a 2 player win/loss game doesn\u2019t need rewards (R) or a discount-rate (\u03b3), so I will pretend these don\u2019t exist. The game is not stochastic, so only the deterministic case will be treated. We also won\u2019t talk about Q-learning or policy-gradients.", "Here is the setup. There are states S. For us, each is a board position assuming X is to move. We can assume it is X to move by the symmetry discussed above. Counting only states where the game is not over, there are substantially more possible states (#S \u2248 10\u2078\u2078) than atoms in the universe. Of course, most states are unlikely to occur in actual gameplay.\u00b3", "There are also two \u201cterminal\u201d states where the game is over. One for the case where X has won and one for O. All positions resulting in one of the two are (respectively) the same.", "Now a game consists of a series of states at various times. Denote the time as t with t=0 the beginning of the game, t=1 after the first move is made, and so on. Furthermore, let t=\u221e denote the end of the game (though it does not in fact take infinite time).", "We will denote the state at various times with a subscript. For example S\u2092 is the state at the beginning of the game. Likewise with subscripts 1, t, or \u221e.", "Given a state S, there is a set of possible actions (moves) that can be taken by the player whose turn it is (again, we will always consider X to play). Denote this set of actions A(S), since it depends on the state. An individual action will be denoted in lower case as a.", "In context, an action is either a placement of a piece (maximum of 19\u2a0915\u20131=284 possible) or one of the available jumps (there could be a lot of possible jumps if you arrange the pieces properly). For a terminal state (the game is over), the set of actions will be treated as empty.", "As before, we will use subscripts to denote the action set at a given time, or the action taken at a given time. For example, A\u2092 = A(S\u2092) is the set of possible actions at the beginning (284 possible placements and no possible jumps). Likewise, a\u2092 is the action taken at time 0, namely the first move.", "The policy \u03c0 is the core of what it means to play a game. Given a state S, you have to choose an action to take!", "Now, even in a deterministic, perfect information game like Philosopher\u2019s Football, a policy could be stochastic (somewhat random). Consider the policy \u201cwhat John Conway would play.\u201d Namely, given a state S with X to move, what move would John Conway make? Whatever it is, play that.", "Now, this Conway policy isn\u2019t simple (good luck evaluating it on a computer). But it is certainly a policy.\u2074 It is also not deterministic. (Most) people do not always make the same move given the same position. This would first of all be hard to remember and second be very boring.", "So we will consider the policy to be a function \u03c0(a|S) giving the probability of taking action a given the state S. Note that we must have that the action a is a legal move (a\u2208 A(S)). Also, the policy function won\u2019t be defined for the terminal (game-over) states.", "Now making things, non-deterministic tends to make them more complicated without being much more enlightening. So through, feel free to consider the case where the policy function just takes as input a state and gives as output an action.", "There is a value function v(S) taking as input a state S and returning the \u201cvalue.\u201d In our case, the value will be the probability of the player whose move it is winning. It is valued between 0 and 1.", "We must also add the restriction that the value function evaluated at the two terminal (game-over) states is always 0 when O won and 1 when X won. (This is because we can assume we are always evaluating for X, by symmetry).", "Now, the value function comes in a few flavors (at left). With perfect play in a deterministic perfect-information game, the game must certainly end with a win for X, a loss, or an infinite loop which we shall call a draw with value \u00bd. For our purposes, we may assume the third never happens.", "With a given policy \u03c0 and assuming that the opponent is playing the same policy, there is an \u201cideal\u201d function that determines exactly the probability of winning, a number between 0 and 1. If the policy is deterministic, the probability of winning is either 0 or 1 (ignoring the draw case).", "Unfortunately, if we knew how to play perfectly, there would be no need to train a computer to play. Also, the ideal function is computationally intractable:", "So instead, we will use the approximate value function, which we will henceforth just refer to as v. Most commonly, the approximate value function can be implemented with a neural network, which is what we shall do.", "So far all we have done is set up a context and some notation. We haven\u2019t described how our AI player is going to learn to play a better policy. There are (of course) a variety of algorithms to do this. Roughly speaking, a learning algorithm could try to learn a policy function \u03c0 directly (policy-gradients). It could try to learn to decide which actions are best (Q-learning). Or it can just learn the value function \u2013 simply learning how good a position is. We are only going to discuss the last case.", "In this context, given a value function v we can make our policy \u201cchoose the move that results in the resulting position that we think is best for us.\u201d In particular, since our game is deterministic, choosing an action a is equivalent to choosing the next state S at time t+1. So, abusing notation slightly and treating \u03c0 as a simple function from states to actions:", "In the \u03c0(a|S) notation, \u03c0 is 1 if a is the best move, and 0 otherwise.", "Note that we have elided for now the issue of dealing with the fact that it is O to play after X, and we have said all states assume X is to play. For the time being, the value function is conceptually X\u2019s estimate of their likelihood of winning given the state S including the information about who is to move.", "Our AI player pursues some policy \u03c0 which takes a state S at time t and determines an action (move) a \u2013 either giving a single move or probabilities of each move. There is also a value function v that estimates our probability of winning given a state and when following policy \u03c0. Conversely, if \u03c0 is \u201cpick the best move\u201d (or a probability distribution over a few of the best ones), then we can treat v as a first-class object and derive \u03c0 from it. Our learning problem is reduced to a good estimate v.", "Here\u2019s a recap of the symbols we defined and their brief interpretation.", "So we have set up the basic context of reinforcement learning. Our goal is to estimate a value function v. At every step, the AI player just chooses the move that results in the best resulting value (i.e. probability of winning). This is a deterministic strategy, but we can make it random (less boring) by giving a probability distribution over the top 3 or 5 moves or something.", "Now how are we going to learn v? For the time being, let\u2019s consider the case where v is simply a giant table with all the possible states and their values. Computationally impossible here but conceptually straightforward.", "Here is a list of upgrades we can make to our learning algorithm, starting from the obvious and proceeding towards the clever.", "Option 3, temporal difference learning aka TD(0), is where the learning really come into play. We go through a series of states and at each step we update our estimate of v(S) to match our estimate of v(S\u02b9). Generally we start with a table initialized with either random numbers or zeros, and we update with a learning rate \u03b1. We introduce a temporal difference \u03b4 and make our updates as follows:", "Notice the subscripts carefully. Our value function v now changes at each time step, so we have to keep track of which time step we are evaluating at.", "All of this would be quite boring if our bot just kept playing the same moves over and over again. It needs to try different moves to see if they\u2019re any good. This is called the exploration-exploitation tradeoff and gets covered a lot, so I won\u2019t talk about it much.", "One of the benefits of our TD(0) algorithm is that, while we are playing the bot against itself and learning the approximate value function v, we can just make a random move and not make our update. This makes it very simple to try out new moves. Monte-Carlo methods run into issues because you have to simulate an entire game under the policy to learn about any of them. If you make a random (most likely bad) move in the middle, you have to try to figure out how that affects your estimates of earlier moves. This is possible, but comes with downsides (mostly high variance).", "Above we considered the case where the function v was just a table of values. To update v(S) we just change the value in the table. As discussed above, this tabular approach is computationally impossible for Philosopher\u2019s Football. So we need to upgrade v to a neural network. It will take as input some representation of the state as a tensor or list of tensors. And it will produce as output a number between 0 and 1. We need to add a set of parameters (the network weights) w at each time t. So we clarify of our definition of v to now look like:", "And we need to modify our update algorithms to use backpropogation:", "Eligibility traces are essentially momentum as in Stochastic Gradient Descent, but in reinforcement learning they are motivated differently and have a different interpretation. They allow us to keep the simplicity of the TD(0) algorithm but gain the benefits of using Monte-Carlo methods.", "Specifically, TD(0) suffers from the issue that we are learning the whole value function at the same time. So v(S\u02b9) may not be a good estimate of v(S). In particular, after moving to S\u02b9, the AI player moves to another state, call it S\u02b9\u02b9. If v(S\u02b9) isn\u2019t a good estimate of v(S\u02b9\u02b9), then it isn\u2019t a good estimate to use for v(S) either. Monte-Carlo methods solve this by doing their updates after an entire game has been played, but in turn suffer when it comes to dealing with off-policy moves.", "Eligibility traces are the solution. Similar to momentum in Stochastic Gradient Descent we add a trace z (initialized to 0) to our algorithm and with decay parameter \u03bb < 1. We get the update at time t:", "The trace matches the parameters w component for component. So if w is a vector, then z is a vector of the same length. Likewise substituting \u201ctensor\u201d for vector and \u201cshape\u201d for length. Likewise with \u201clist of tensors\u201d and \u201ccorresponding list of shapes.\u201d", "It\u2019s worth considering what happens step-by-step. Let\u2019s start at time t=0 and make the update. With the careful understanding that our derivatives are evaluated based on the current state", "We get the following update at our first step (z is initialized to 0):", "with v(S) updated to match v(S\u02b9) just as before. The difference comes in at the next step:", "It\u2019s as if we went back in time and made the update at time 0 account for the future, not-yet-known, difference between v(S\u02b9) and v(S\u02b9\u02b9)! Suitably discounted by a factor of \u03bb.", "Note that the two derivatives were evaluated at different times with different parameters and inputs. This means we can\u2019t write down any enlightening expression for the resulting v(S\u2092).", "The final note for our algorithm is that, if we make an off policy action, we should just zero out the trace. The value of future positions is no longer informative for the moves we made in the past.", "We\u2019ve introduced more new notation. With a brief reprise of the standard stuff, we have the following summary:", "The whole point of this article is that a slight modification is needed to the TD(\u03bb) to account for the fact that the game is a 2 player game. There are a variety of ways to deal with this, but the symmetry of Philosopher\u2019s Football makes it very clean.", "In particular, let\u2019s interpret a state S as representing an arrangement of pieces with the underlying assumption that X is to play. Then v evaluates the probability of X winning. Use a \u2020 to indicate that the board has been turned around. Ignoring parameter updates for now, we have the following transformations when the AI player moves", "In particular, after playing, if we turn the board around (\u2020) then the probability of winning has changed to 1-v. This will necessitate some changes in the algorithm.", "Another change is necessary to take into the account that it\u2019s desirable to compute derivatives at the same time as evaluation is done. Implicitly in the textbook TD(\u03bb) algorithm, we first do a forward pass evaluation of states to choose a move and then compute the derivative with respect to the old state. This is obviously wasteful. Our complete algorithm for a step now becomes:", "The tricky part is the minus sign in step 6! This is why I call it Alternating TD(\u03bb). We make an update at time t based on the difference in values between times t+1 and t. If, at time t+2 we realize our t+1 value was an underestimate (so \u03b4 is negative), we need to increase our value-estimate for the state S at time t. Hence the alternating sign of the gradient to account for the alternating players.", "Hopefully, you\u2019ve learned a good bit about reinforcement learning as well as some of the extra complexity from a 2-player game. And if you haven\u2019t yet, play the game at philosophers.football. The AI players are still training, but you can play against another human.", "[1] I\u2019m not sure if the algorithm is original. I based it off the textbook description in Sutton and Barto.", "[3] First of all because it is unlikely enough games will ever be played to achieve all of those states. Second, because any remotely competent player would have won long before those states were achieved.", "[4] Technically to make this policy well defined, we should include all the information necessary to a human in the state variable: the time of day, whether they are hungry, whether Mercury is in retrograde, how many cats are in the room, whether their opponent is wearing a black hat, whether they believe the axiom of choice at the moment, etc. But this technicality won\u2019t affect the main argument.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Scientist, Mathematician. Formerly @MIT, @McKinsey, currently teaching computers to read"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F5d15fb394c63&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-in-honor-of-john-conway-5d15fb394c63&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-in-honor-of-john-conway-5d15fb394c63&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-in-honor-of-john-conway-5d15fb394c63&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-in-honor-of-john-conway-5d15fb394c63&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----5d15fb394c63--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5d15fb394c63--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@rmcharan?source=post_page-----5d15fb394c63--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@rmcharan?source=post_page-----5d15fb394c63--------------------------------", "anchor_text": "Ravi Charan"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F393ce2bbf82c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-in-honor-of-john-conway-5d15fb394c63&user=Ravi+Charan&userId=393ce2bbf82c&source=post_page-393ce2bbf82c----5d15fb394c63---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5d15fb394c63&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-in-honor-of-john-conway-5d15fb394c63&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5d15fb394c63&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-in-honor-of-john-conway-5d15fb394c63&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@kennyluoping?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Kenny Luo"}, {"url": "https://unsplash.com/collections/9524181/go-players?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://en.wikipedia.org/wiki/John_Horton_Conway", "anchor_text": "John Conway"}, {"url": "http://philosophers.football/", "anchor_text": "philosophers.football"}, {"url": "https://github.com/rcharan/phutball", "anchor_text": "Github"}, {"url": "https://arxiv.org/abs/1512.03385", "anchor_text": "ResNet"}, {"url": "http://philosophers.football", "anchor_text": "philosophers.football"}, {"url": "https://en.wikipedia.org/wiki/Winning_Ways_for_your_Mathematical_Plays", "anchor_text": "Winning Ways for your Mathematical Plays"}, {"url": "https://www.youtube.com/watch?v=LfduUFF_i1A", "anchor_text": "Monty Python skit"}, {"url": "https://en.wikipedia.org/wiki/Conway%27s_Game_of_Life", "anchor_text": "Game of Life"}, {"url": "http://philosophers.football", "anchor_text": "philosophers.football"}, {"url": "http://incompleteideas.net/book/the-book-2nd.html", "anchor_text": "Sutton and Barto"}, {"url": "https://medium.com/tag/data-science?source=post_page-----5d15fb394c63---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----5d15fb394c63---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/board-games?source=post_page-----5d15fb394c63---------------board_games-----------------", "anchor_text": "Board Games"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----5d15fb394c63---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----5d15fb394c63---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5d15fb394c63&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-in-honor-of-john-conway-5d15fb394c63&user=Ravi+Charan&userId=393ce2bbf82c&source=-----5d15fb394c63---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5d15fb394c63&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-in-honor-of-john-conway-5d15fb394c63&user=Ravi+Charan&userId=393ce2bbf82c&source=-----5d15fb394c63---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5d15fb394c63&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-in-honor-of-john-conway-5d15fb394c63&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5d15fb394c63--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F5d15fb394c63&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-in-honor-of-john-conway-5d15fb394c63&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----5d15fb394c63---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----5d15fb394c63--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----5d15fb394c63--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----5d15fb394c63--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----5d15fb394c63--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----5d15fb394c63--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----5d15fb394c63--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----5d15fb394c63--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----5d15fb394c63--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@rmcharan?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@rmcharan?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ravi Charan"}, {"url": "https://medium.com/@rmcharan/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "599 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F393ce2bbf82c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-in-honor-of-john-conway-5d15fb394c63&user=Ravi+Charan&userId=393ce2bbf82c&source=post_page-393ce2bbf82c--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F6bca6dd641ca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-in-honor-of-john-conway-5d15fb394c63&newsletterV3=393ce2bbf82c&newsletterV3Id=6bca6dd641ca&user=Ravi+Charan&userId=393ce2bbf82c&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}