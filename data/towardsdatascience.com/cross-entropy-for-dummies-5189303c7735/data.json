{"url": "https://towardsdatascience.com/cross-entropy-for-dummies-5189303c7735", "time": 1683002452.668283, "path": "towardsdatascience.com/cross-entropy-for-dummies-5189303c7735/", "webpage": {"metadata": {"title": "Cross-Entropy for Dummies. A simple and intuitive explanation of\u2026 | by Viraj Kulkarni | Towards Data Science", "h1": "Cross-Entropy for Dummies", "description": "Cross-entropy is commonly used as a loss function for classification problems, but due to historical reasons, most explanations of cross-entropy are based on communication theory which data\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/@diningphilosopher", "anchor_text": "Medium", "paragraph_index": 19}, {"url": "https://www.linkedin.com/in/kulkarniviraj/", "anchor_text": "LinkedIn", "paragraph_index": 19}, {"url": "https://twitter.com/VirajZero", "anchor_text": "Twitter", "paragraph_index": 19}, {"url": "https://virajkulkarni.org/", "anchor_text": "personal webpage", "paragraph_index": 19}], "all_paragraphs": ["Cross-entropy is commonly used as a loss function for classification problems, but due to historical reasons, most explanations of cross-entropy are based on communication theory which data scientists may not be familiar with. You cannot understand cross-entropy without understanding entropy, and you cannot understand entropy without knowing what information is. This article builds the concept of cross-entropy in an easy-to-understand manner without relying on its communication theory background.", "(1) I toss a coin, and I give you a message that the coin came up heads. The message conveyed some information to you. How much information? Let\u2019s say you got one bit of information from this message.", "(2) I woke you up in the morning and told you that the sun rose. You shrugged and angrily went back to sleep, because this message was obvious and gave you no new information.", "(3) I woke you up in the morning and told you the sun did not rise. You came out shocked and saw the world had gone crazy. This was breaking news \u2014 lots of information.", "From the above examples, you might have deduced that the information contained in a message about an event is related to the uncertainty and surprise-value of the event. An occurrence of an unlikely event gives you more information than the occurrence of a likely event. Claude Shannon formalised this intuition behind information in his seminal work on Information Theory. He defined information as:", "Information quantifies the uncertainty in one single event.", "What if you are interested not in one single event but in a sequence of events? Consider the following example. A bin contains 2 red, 3 green, and 4 blue balls. Now, instead of tossing a coin, I pick out one ball at random and give it to you. What is the expected amount of information you will receive every time I pick one ball? We model picking out a ball as a stochastic process represented by a random variable X. The entropy of X is then defined as the expected value of the information conveyed by an outcome in X. Using our above definition of information, we get:", "* Expected Value or Expectation of random variable X, written as E[X], is the average of all values of X weighted by the probability of their occurrence.", "In other words, you can expect to get 1.53 bits of information on average every time I pick out a ball from the bin.", "Formally, the entropy H of a probability distribution of a random variable is defined as:", "The x~P in the above equation means that the values x takes are from the distribution P. In our example, P = (2 red, 3 green, 4 blue).", "Cross-entropy measures the relative entropy between two probability distributions over the same set of events. Intuitively, to calculate cross-entropy between P and Q, you simply calculate entropy for Q using probability weights from P. Formally:", "Let\u2019s consider the same bin example with two bins.", "Instead of the contrived example above, let\u2019s take a machine learning example where we use cross-entropy as a loss function. Suppose we build a classifier that predicts samples in three classes: A, B, C.", "Let P be the true label distribution and Q be the predicted label distribution. Suppose the true label of one particular sample is B and our classifier predicts probabilities for A, B, C as (0.15, 0.60, 0.25)", "On the other hand, if our classifier is more confident and predicts probabilities as (0.05, 0.90, 0.05), we would get cross-entropy as 0.15, which is lower than the above example.", "For classification problems, using cross-entropy as a loss function is equivalent to maximising log likelihood. Consider the below case of binary classification where a, b, c, d represent probabilities:", "This is the same equation for maximum likelihood estimation.", "Due to historical reasons, you will often find cross-entropy defined in the context of communication theory such as: cross-entropy is the average number of bits needed to encode data coming from a source with distribution p when we use model q. Most data scientists have never studied communication theory. I hope they find the above article useful.", "If you liked the article, check out my other pieces on Medium, follow me on LinkedIn or Twitter, view my personal webpage, or email me at viraj@berkeley.edu.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Equal superposition of quantum computing, machine learning, and philosophy."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F5189303c7735&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcross-entropy-for-dummies-5189303c7735&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcross-entropy-for-dummies-5189303c7735&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcross-entropy-for-dummies-5189303c7735&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcross-entropy-for-dummies-5189303c7735&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----5189303c7735--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5189303c7735--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@diningphilosopher?source=post_page-----5189303c7735--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@diningphilosopher?source=post_page-----5189303c7735--------------------------------", "anchor_text": "Viraj Kulkarni"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2eb0926f5807&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcross-entropy-for-dummies-5189303c7735&user=Viraj+Kulkarni&userId=2eb0926f5807&source=post_page-2eb0926f5807----5189303c7735---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5189303c7735&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcross-entropy-for-dummies-5189303c7735&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5189303c7735&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcross-entropy-for-dummies-5189303c7735&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/@diningphilosopher", "anchor_text": "Medium"}, {"url": "https://www.linkedin.com/in/kulkarniviraj/", "anchor_text": "LinkedIn"}, {"url": "https://twitter.com/VirajZero", "anchor_text": "Twitter"}, {"url": "https://virajkulkarni.org/", "anchor_text": "personal webpage"}, {"url": "https://medium.com/tag/data-science?source=post_page-----5189303c7735---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/information-theory?source=post_page-----5189303c7735---------------information_theory-----------------", "anchor_text": "Information Theory"}, {"url": "https://medium.com/tag/loss-function?source=post_page-----5189303c7735---------------loss_function-----------------", "anchor_text": "Loss Function"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----5189303c7735---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----5189303c7735---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5189303c7735&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcross-entropy-for-dummies-5189303c7735&user=Viraj+Kulkarni&userId=2eb0926f5807&source=-----5189303c7735---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5189303c7735&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcross-entropy-for-dummies-5189303c7735&user=Viraj+Kulkarni&userId=2eb0926f5807&source=-----5189303c7735---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5189303c7735&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcross-entropy-for-dummies-5189303c7735&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5189303c7735--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F5189303c7735&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcross-entropy-for-dummies-5189303c7735&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----5189303c7735---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----5189303c7735--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----5189303c7735--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----5189303c7735--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----5189303c7735--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----5189303c7735--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----5189303c7735--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----5189303c7735--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----5189303c7735--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@diningphilosopher?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@diningphilosopher?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Viraj Kulkarni"}, {"url": "https://medium.com/@diningphilosopher/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "291 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2eb0926f5807&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcross-entropy-for-dummies-5189303c7735&user=Viraj+Kulkarni&userId=2eb0926f5807&source=post_page-2eb0926f5807--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fa20bd7667b96&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcross-entropy-for-dummies-5189303c7735&newsletterV3=2eb0926f5807&newsletterV3Id=a20bd7667b96&user=Viraj+Kulkarni&userId=2eb0926f5807&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}