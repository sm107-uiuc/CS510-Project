{"url": "https://towardsdatascience.com/is-random-forest-better-than-logistic-regression-a-comparison-7a0f068963e4", "time": 1682997005.278316, "path": "towardsdatascience.com/is-random-forest-better-than-logistic-regression-a-comparison-7a0f068963e4/", "webpage": {"metadata": {"title": "Is Random Forest better than Logistic Regression? (a comparison) | by Andrew Hershy | Towards Data Science", "h1": "Is Random Forest better than Logistic Regression? (a comparison)", "description": "Random Forests are another way to extract information from a set of data. The appeals of this type of model are: In this analysis we will classify the data with random forest, compare the results\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6050737/", "anchor_text": "compare", "paragraph_index": 1}, {"url": "https://towardsdatascience.com/predicting-cancer-with-logistic-regression-in-python-7b203ace16bc", "anchor_text": "logistic regression analysis", "paragraph_index": 1}, {"url": "https://medium.com/u/4e564d7496d6?source=post_page-----7a0f068963e4--------------------------------", "anchor_text": "@", "paragraph_index": 5}, {"url": "https://towardsdatascience.com/predicting-cancer-with-logistic-regression-in-python-7b203ace16bc", "anchor_text": "logistic regression analysis", "paragraph_index": 6}, {"url": "https://towardsdatascience.com/predicting-cancer-with-logistic-regression-in-python-7b203ace16bc", "anchor_text": "Predicting Cancer with Logistic Regression in Python", "paragraph_index": 38}, {"url": "https://towardsdatascience.com/r-squared-recipe-5814995fa39a", "anchor_text": "Calculating R-squared from scratch (using python)", "paragraph_index": 39}, {"url": "https://medium.com/better-programming/risk-board-game-battle-probability-grid-program-f3073fb34e5c", "anchor_text": "Risk Board Game \u2014 Battle Probability Grid Program", "paragraph_index": 40}, {"url": "https://neptune.ai/blog/random-forest-regression-when-does-it-fail-and-why", "anchor_text": "Random Forest Regression: When Does It Fail and Why?", "paragraph_index": 41}], "all_paragraphs": ["Random Forests are another way to extract information from a set of data. The appeals of this type of model are:", "In this analysis we will classify the data with random forest, compare the results with logistic regression, and discuss the differences. Take a look at the previous logistic regression analysis to see what we\u2018ll be comparing it to.", "We have a sample of 255 patients and would like to measure the relationship between 4 proteins levels and cancer growth.", "To predict whether future patients have cancer by extracting information from the relationship between protein levels and cancer in our sample.", "The 4 proteins we\u2019ll be looking at:", "I received this data set to use for educational purposes from the MBA program @UAB.", "Again, take a look at the logistic regression analysis to get a more in-depth understanding. Below are the essentials:", "To refresh on the logistic regression output:", "CEA and CA125 were the most predictive, with their pvalues below alpha at 5% and their coefficients being higher than the others. We took out AFP and CA50 from the logistic regression due to their high pvalue.", "However, we will keep them in for the random forest model. The whole purpose of this exercise is to compare the 2 models, not combine them.", "We will build the decision tree and visualize what it looks like:", "That\u2019s an intimidating tree for new-comers. Let\u2019s break it down a bit:", "The root node shows the gini index of the whole data set, prior to branching. The lower the gini score, the more pure the data is. The worst-mixed data would give a gini index of 0.5.", "To refresh, there are 144 noncancerous and 111 cancerous patients in our data.The Gini Index for this would be 0.492 which means it is very mixed. But don\u2019t worry, the tree will lower the gini indices as new branches and nodes are formed.", "The regression model told us CEA is the most predictive feature with the highest coefficient and the lowest pvalue. The decision tree agrees with this by placing CEA on the root node. Every other node is derivative of the root node\u2019s split.The algorithm chose to split at CEA level 3.25 because that point splits the target variable into cancerous and noncancerous more purely than any other point in any other attribute. The instances of CEA values lower than 3.25 (180 samples) are more likely to be non cancerous; the instances above 3.25 (75 samples) are more likely to be cancerous. Refer to the connecting internal nodes below the root to see how the instances are further divided.", "The tree\u2019s second layer analyzes both new buckets of data (the 180 samples below CEA 3.25 and the 75 samples above) in the same way it did the root node:", "It runs the ID3 algorithm, finds the attribute which divides the target variable to its maximum purity, determines the optimal cut-off point, and splits.", "The second layer node of CEA level above 3.25 split is based on CA125 levels above 38.6. This split results in another internal node of 72 samples and our first leaf node of 3 samples. This leaf node has a gini index of 0 because all 3 samples in this node are classified as being noncancerous. The way the algorithm will think about classifying future data based on this particular leaf node would be:", "The process continues until the tree ends in all leaf nodes and there is a decision for every series of splits.", "Instead of stopping there and basing our model off of the tree\u2019s leaves, we will be implementing a random forest: taking random samples, forming many decision trees and taking the average of those decisions to form a more refined model. We are taking the averages of 1000 tree samples in this model.", "This 71% accuracy compares to the 74% accuracy of the logistic model.", "Edit: I was talking with a friend in biostats about my analysis, and the convention in that field is that the disease is attributed as being positive. I arbitrarily set cancer as negative because I didn\u2019t know that at the time.", "Match the matrix above to Figure 4 to learn what it is saying:", "30% of our total data went to testing group, that leaves 255(.3) = 77 instances that were tested. The sum of the matrix is 77. Divide the \u201cTrue\u201d numbers by the total and that will give the accuracy of our model: 55/77 = 71%.", "Forming new DataFrame for Accuracy plot and ROC curve:", "To understand what is going on in the dataframe below, let\u2019s analyze it, row by row.", "The rest of the columns are based solely on \u201cy_test\u201d, not our model\u2019s predictions. Think of these values as their own confusion matrices. These will help us determine where the optimal cut off point will be later.", "After looking over the confusion matrices within the dataframe, try to find the highest accuracy percentage. If you can locate that, you can match it to the corresponding model_probability to discover the optimal cut-off point for classification.", "The random forest model sets the cut-off point at 60% model probability, which is at 75% accuracy.", "It may seem counter-intuitive, but this means if we use 60% instead of 50% when classifying a patient as cancerous, it will actually be more accurate using this particular model.", "For comparison, the logistic model set it\u2019s optimal cut-off point at 54% probability with the same accuracy at 75%.", "Lastly, let\u2019s graph the ROC curve and find AUC:", "The black ROC curve is showing the trade-off between our testing data\u2019s true positive rate and false positive rate. The dotted red line cutting through the center of the graph is to provide a sense of what the worst possible model would look like as an ROC curve.", "The closer the ROC line can get to the top-left side, the more predictive our model is. The closer it resembles the dotted red line, the less predictive it is.", "That\u2019s where the area under curve (AUC) comes in. AUC is the area of space that lies under the ROC curve. Intuitively, the closer this is to 1, the better our classification model is. The AUC of the dotted line is 0.5. The AUC of a perfect model would be 1. Our random forest has an AUC at 0.71.", "For comparison, provided is the logistic ROC curve with an AUC of 0.82:", "Comparing the accuracy and AUC between the models, logistic regression wins this time. Both models regardless have their pros and cons.", "Please subscribe if you found this helpful. If you enjoy my content, please check out a few other projects:", "Predicting Cancer with Logistic Regression in Python", "Calculating R-squared from scratch (using python)", "Risk Board Game \u2014 Battle Probability Grid Program", "Random Forest Regression: When Does It Fail and Why?", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F7a0f068963e4&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-random-forest-better-than-logistic-regression-a-comparison-7a0f068963e4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-random-forest-better-than-logistic-regression-a-comparison-7a0f068963e4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-random-forest-better-than-logistic-regression-a-comparison-7a0f068963e4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-random-forest-better-than-logistic-regression-a-comparison-7a0f068963e4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----7a0f068963e4--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----7a0f068963e4--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://andrewhershy.medium.com/?source=post_page-----7a0f068963e4--------------------------------", "anchor_text": ""}, {"url": "https://andrewhershy.medium.com/?source=post_page-----7a0f068963e4--------------------------------", "anchor_text": "Andrew Hershy"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff2b7b4b5294a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-random-forest-better-than-logistic-regression-a-comparison-7a0f068963e4&user=Andrew+Hershy&userId=f2b7b4b5294a&source=post_page-f2b7b4b5294a----7a0f068963e4---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7a0f068963e4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-random-forest-better-than-logistic-regression-a-comparison-7a0f068963e4&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7a0f068963e4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-random-forest-better-than-logistic-regression-a-comparison-7a0f068963e4&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/photos/sp-p7uuT0tw", "anchor_text": "Source"}, {"url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6050737/", "anchor_text": "compare"}, {"url": "https://towardsdatascience.com/predicting-cancer-with-logistic-regression-in-python-7b203ace16bc", "anchor_text": "logistic regression analysis"}, {"url": "https://medium.com/u/4e564d7496d6?source=post_page-----7a0f068963e4--------------------------------", "anchor_text": "@"}, {"url": "https://towardsdatascience.com/predicting-cancer-with-logistic-regression-in-python-7b203ace16bc", "anchor_text": "logistic regression analysis"}, {"url": "https://towardsdatascience.com/linear-vs-polynomial-regression-walk-through-83ca4f2363a3", "anchor_text": "Simple Linear vs Polynomial Regression"}, {"url": "https://towardsdatascience.com/predicting-cancer-with-logistic-regression-in-python-7b203ace16bc", "anchor_text": "Predicting Cancer with Logistic Regression in Python"}, {"url": "https://towardsdatascience.com/univariate-logistic-regression-example-in-python-acbefde8cc14", "anchor_text": "Bivariate Logistic Regression Example (python)"}, {"url": "https://towardsdatascience.com/r-squared-recipe-5814995fa39a", "anchor_text": "Calculating R-squared from scratch (using python)"}, {"url": "https://medium.com/better-programming/risk-board-game-battle-automation-5e2d955cc9b3", "anchor_text": "Risk Board Game Battle Automation"}, {"url": "https://medium.com/better-programming/risk-board-game-battle-probability-grid-program-f3073fb34e5c", "anchor_text": "Risk Board Game \u2014 Battle Probability Grid Program"}, {"url": "https://neptune.ai/blog/random-forest-regression-when-does-it-fail-and-why", "anchor_text": "Random Forest Regression: When Does It Fail and Why?"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----7a0f068963e4---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/modeling?source=post_page-----7a0f068963e4---------------modeling-----------------", "anchor_text": "Modeling"}, {"url": "https://medium.com/tag/data-science?source=post_page-----7a0f068963e4---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/decision-tree?source=post_page-----7a0f068963e4---------------decision_tree-----------------", "anchor_text": "Decision Tree"}, {"url": "https://medium.com/tag/analysis?source=post_page-----7a0f068963e4---------------analysis-----------------", "anchor_text": "Analysis"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7a0f068963e4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-random-forest-better-than-logistic-regression-a-comparison-7a0f068963e4&user=Andrew+Hershy&userId=f2b7b4b5294a&source=-----7a0f068963e4---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7a0f068963e4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-random-forest-better-than-logistic-regression-a-comparison-7a0f068963e4&user=Andrew+Hershy&userId=f2b7b4b5294a&source=-----7a0f068963e4---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7a0f068963e4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-random-forest-better-than-logistic-regression-a-comparison-7a0f068963e4&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----7a0f068963e4--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F7a0f068963e4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-random-forest-better-than-logistic-regression-a-comparison-7a0f068963e4&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----7a0f068963e4---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----7a0f068963e4--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----7a0f068963e4--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----7a0f068963e4--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----7a0f068963e4--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----7a0f068963e4--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----7a0f068963e4--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----7a0f068963e4--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----7a0f068963e4--------------------------------", "anchor_text": ""}, {"url": "https://andrewhershy.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://andrewhershy.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Andrew Hershy"}, {"url": "https://andrewhershy.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "496 Followers"}, {"url": "https://www.linkedin.com/in/andrew-hershy-a7779199/", "anchor_text": "https://www.linkedin.com/in/andrew-hershy-a7779199/"}, {"url": "https://andrewhershy.medium.com/membership", "anchor_text": "https://andrewhershy.medium.com/membership"}, {"url": "https://github.com/ahershy", "anchor_text": "https://github.com/ahershy"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff2b7b4b5294a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-random-forest-better-than-logistic-regression-a-comparison-7a0f068963e4&user=Andrew+Hershy&userId=f2b7b4b5294a&source=post_page-f2b7b4b5294a--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F4d03304fb3bb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-random-forest-better-than-logistic-regression-a-comparison-7a0f068963e4&newsletterV3=f2b7b4b5294a&newsletterV3Id=4d03304fb3bb&user=Andrew+Hershy&userId=f2b7b4b5294a&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}