{"url": "https://towardsdatascience.com/mlmachine-hyperparameter-tuning-with-bayesian-optimization-2de81472e6d", "time": 1683012199.9150121, "path": "towardsdatascience.com/mlmachine-hyperparameter-tuning-with-bayesian-optimization-2de81472e6d/", "webpage": {"metadata": {"title": "mlmachine - Hyperparameter Tuning with Bayesian Optimization | by Tyler Peterson | Towards Data Science", "h1": "mlmachine - Hyperparameter Tuning with Bayesian Optimization", "description": "mlmachine is a Python library that organizes and accelerates notebook-based machine learning experiments. In this article, we use mlmachine to accomplish actions that would otherwise take\u2026"}, "outgoing_paragraph_urls": [{"url": "https://github.com/petersontylerd/mlmachine/blob/master/notebooks/mlmachine_part_4.ipynb", "anchor_text": "Jupyter Notebook", "paragraph_index": 2}, {"url": "https://github.com/petersontylerd/mlmachine", "anchor_text": "project", "paragraph_index": 3}, {"url": "https://towardsdatascience.com/an-introductory-example-of-bayesian-optimization-in-python-with-hyperopt-aae40fff4ff0", "anchor_text": "Will Koehrsen\u2019s article on Medium", "paragraph_index": 5}, {"url": "https://github.com/petersontylerd/mlmachine", "anchor_text": "GitHub repository", "paragraph_index": 46}], "all_paragraphs": ["mlmachine is a Python library that organizes and accelerates notebook-based machine learning experiments.", "In this article, we use mlmachine to accomplish actions that would otherwise take considerable coding and effort, including:", "Check out the Jupyter Notebook for this article.", "Check out the project on GitHub.", "And check out past mlmachine articles:", "Bayesian optimization is typically described as an advancement beyond exhaustive grid searches, and rightfully so. This hyperparameter tuning strategy succeeds by using prior information to inform future parameter selection for a given estimator. Check out Will Koehrsen\u2019s article on Medium for an excellent overview of the package.", "mlmachine uses hyperopt as a foundation for performing Bayesian optimization, and takes the functionality of hyperopt a step further through a simplified workflow that allows for optimization of multiple models in single process execution. In this article, we are going to optimize four classifiers:", "First, we apply data preprocessing techniques to clean up our data. We\u2019ll start by creating two Machine() objects \u2014 one for the training data and a second for the validation data:", "Now we process the data by imputing nulls and applying various binning, feature engineering and encoding techniques:", "Here is the output, still in a DataFrame:", "As a second preparatory step, we want to perform feature selection for each of our classifiers:", "For our final preparatory step, we use this feature selection summary to perform iterative cross-validation on smaller and smaller subsets of features for each of our estimators:", "From this result, we extract our dictionary of optimum feature sets for each estimator:", "The keys are estimator names, and the associated values are lists containing the column names of the best performing feature subset for each estimator. Here are the key/value pairs for XGBClassifier(), which used only 10 of the available 43 features to achieve the best average cross-validation accuracy on the validation dataset:", "With our processed dataset and optimum feature subsets in hand, it\u2019s time to use Bayesian optimization to tune the hyperparameters of our 4 estimators.", "First, we need to establish our feature space for each parameter for each estimator:", "The outermost keys of the dictionary are names of classifiers, represented by strings. The associated values are also dictionaries, where the keys are parameter names, represented as strings, and the values are hyperopt sampling distributions from which parameter values will be chosen.", "Now we\u2019re ready to run our Bayesian optimization hyperparameter tuning job. We will use a built-in method belonging to our Machine() object called exec_bayes_optim_search(). Let\u2019s see mlmachine in action:", "Anyone familiar with hyperopt will be wondering where the objective function is. mlmachine abstracts away this complexity.", "The process runtime depends on several attributes, including hardware, the number and type of estimators used, the number of folds, feature selection, and the number of sampling iterations. Runtimes can be quite lengthy. For this reason, exec_bayes_optim_search() automatically saves the result of each iteration to a CSV.", "Let\u2019s start by loading and reviewing the results:", "Our Bayesian optimization log maintains key information about each iteration:", "This log provides an immense amount of data for us to analyze and evaluate the effectiveness of the Bayesian optimization process.", "First and foremost, we want to see how if performance improved over the iterations.", "Let\u2019s visualize the XGBClassifier() loss by iteration:", "Each dot represents the performance of one of our 200 experiments. The key detail to notice is that the line of best fit has a clear downward slope - exactly what we want. This means that with each iteration, model performance tends to improve compared to the previous iterations.", "One of the coolest parts of Bayesian optimization is seeing how parameter selection is optimized.", "For each model and for each model\u2019s parameters, we can generate a two-panel visual.", "For numeric parameters, such as n_estimators or learning_rate, the two-visual panel includes:", "For categorical parameters, such as loss function, the two-visual panel includes:", "Let\u2019s review the parameter selection panels for KNeighborsClassifier():", "The built-in method model_param_plot() cycles through of the estimator\u2019s parameters and presents the appropriate panel given each parameter\u2019s type. Let\u2019s look at a numeric parameter and categorical parameter separately.", "First, we\u2019ll review the panel for the numeric parameter n_neighbors:", "On the left, we can see two overlapping kernel density plots summarizing the actual parameter selections and the theoretical parameter distribution. The purple line corresponds to the theoretical distribution, and, as expected, this curve is smooth and evenly distributed. The teal line corresponds to the actual parameter selections, and it\u2019s clearly evident that hyperopt prefers values between 5 and 10.", "On the right, the scatter plot visualizes the n_neighbors value selections over the iterations. There is a slight downward slope to the line of best fit, as the Bayesian optimization process hones in on values around 7.", "Next, we\u2019ll review the panel for the categorical parameter algorithm:", "On the left, we see a bar chart displaying the counts of parameter selections, faceted by actual parameter selections and selections from the theoretical distribution . The purple bars, representing selections from the theoretical distribution, are more even than the teal bars, representing the actual selection.", "On the right, the scatter plot again visualizes the algorithm value selection over the iterations. There is a clear decrease in selection of \u201cball_tree\u201d and \u201cauto\u201d in favor of \u201ckd_tree\u201d and \u201cbrute\u201d over the the iterations.", "Our Machine() object has a built-in method called top_bayes_optim_models(), which identifies the best model for each estimator type based on the results in our Bayesian optimization log.", "With this method, we can identify the top N models for each estimator based on mean cross-validation score. In this experiment, top_bayes_optim_models() returns the dictionary below, which tells us that LogisticRegression() identified its top model on iteration 30, XGBClassifier() on iteration 61, RandomForestClassifier() on iteration 46, and KNeighborsClassifier() on iteration 109.", "To reinstantiate a model, we leverage our Machine() object\u2019s built-in method BayesOptimClassifierBuilder(). To use this method, we pass in our results log, specify an estimator class and iteration number. This will instantiate a model object with the parameters stored on that record of the log:", "Here we see the model parameters:", "The models instantiated with BayesOptimClassifierBuilder() use .fit() and .predict() in a way that should feel quite familiar.", "Let\u2019s finish this article with a very basic model performance evaluation. We will fit this RandomForestClassifier() on the training data and labels, generate predictions with the training data, and evaluate the model\u2019s performance by comparing these predictions to the ground-truth:", "Anyone familiar with Scikit-learn should feel right at home.", "mlmachine makes it easy to efficiently optimize the hyperparameters for multiple estimators in one shot, and facilitates the visual inspection of model improvement and parameter selection.", "Check out the GitHub repository, and stay tuned for additional column entries.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Senior Director, Data Intelligence & Interoperability at Vizient, Inc"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F2de81472e6d&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmlmachine-hyperparameter-tuning-with-bayesian-optimization-2de81472e6d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmlmachine-hyperparameter-tuning-with-bayesian-optimization-2de81472e6d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmlmachine-hyperparameter-tuning-with-bayesian-optimization-2de81472e6d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmlmachine-hyperparameter-tuning-with-bayesian-optimization-2de81472e6d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----2de81472e6d--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2de81472e6d--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@petersontylerd?source=post_page-----2de81472e6d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@petersontylerd?source=post_page-----2de81472e6d--------------------------------", "anchor_text": "Tyler Peterson"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd34516b410ae&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmlmachine-hyperparameter-tuning-with-bayesian-optimization-2de81472e6d&user=Tyler+Peterson&userId=d34516b410ae&source=post_page-d34516b410ae----2de81472e6d---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2de81472e6d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmlmachine-hyperparameter-tuning-with-bayesian-optimization-2de81472e6d&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2de81472e6d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmlmachine-hyperparameter-tuning-with-bayesian-optimization-2de81472e6d&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://pixabay.com/users/ThorstenF-7677369/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=4197733", "anchor_text": "Thorsten Frenzel"}, {"url": "https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=4197733", "anchor_text": "Pixabay"}, {"url": "https://github.com/petersontylerd/mlmachine/blob/master/notebooks/mlmachine_part_4.ipynb", "anchor_text": "Jupyter Notebook"}, {"url": "https://github.com/petersontylerd/mlmachine", "anchor_text": "project"}, {"url": "https://towardsdatascience.com/mlmachine-clean-ml-experiments-elegant-eda-pandas-pipelines-daba951dde0a", "anchor_text": "mlmachine - Clean ML Experiments, Elegant EDA & Pandas PipelinesThis new Python package accelerates notebook-based machine learning experimentationtowardsdatascience.com"}, {"url": "https://towardsdatascience.com/mlmachine-groupbyimputer-kfoldencoder-and-skew-correction-357f202d2212", "anchor_text": "mlmachine - GroupbyImputer, KFoldEncoder, and Skew CorrectionThis new Python package accelerates notebook-based machine learning experimentationtowardsdatascience.com"}, {"url": "https://towardsdatascience.com/mlmachine-crowd-sourced-feature-selection-50cd2bbda1b7", "anchor_text": "mlmachine - Crowd-Sourced Feature SelectionThis new Python package accelerates notebook-based machine learning experimentationtowardsdatascience.com"}, {"url": "https://towardsdatascience.com/an-introductory-example-of-bayesian-optimization-in-python-with-hyperopt-aae40fff4ff0", "anchor_text": "Will Koehrsen\u2019s article on Medium"}, {"url": "https://github.com/petersontylerd/mlmachine", "anchor_text": "GitHub repository"}, {"url": "https://medium.com/tag/python?source=post_page-----2de81472e6d---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/data-science?source=post_page-----2de81472e6d---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----2de81472e6d---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/mlmachine?source=post_page-----2de81472e6d---------------mlmachine-----------------", "anchor_text": "Mlmachine"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2de81472e6d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmlmachine-hyperparameter-tuning-with-bayesian-optimization-2de81472e6d&user=Tyler+Peterson&userId=d34516b410ae&source=-----2de81472e6d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2de81472e6d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmlmachine-hyperparameter-tuning-with-bayesian-optimization-2de81472e6d&user=Tyler+Peterson&userId=d34516b410ae&source=-----2de81472e6d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2de81472e6d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmlmachine-hyperparameter-tuning-with-bayesian-optimization-2de81472e6d&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2de81472e6d--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F2de81472e6d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmlmachine-hyperparameter-tuning-with-bayesian-optimization-2de81472e6d&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----2de81472e6d---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----2de81472e6d--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----2de81472e6d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----2de81472e6d--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----2de81472e6d--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----2de81472e6d--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----2de81472e6d--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----2de81472e6d--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----2de81472e6d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@petersontylerd?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@petersontylerd?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Tyler Peterson"}, {"url": "https://medium.com/@petersontylerd/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "40 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd34516b410ae&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmlmachine-hyperparameter-tuning-with-bayesian-optimization-2de81472e6d&user=Tyler+Peterson&userId=d34516b410ae&source=post_page-d34516b410ae--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Fd34516b410ae%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmlmachine-hyperparameter-tuning-with-bayesian-optimization-2de81472e6d&user=Tyler+Peterson&userId=d34516b410ae&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}