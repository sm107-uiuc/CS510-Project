{"url": "https://towardsdatascience.com/why-deep-learning-works-289f17cab01a", "time": 1682997294.719468, "path": "towardsdatascience.com/why-deep-learning-works-289f17cab01a/", "webpage": {"metadata": {"title": "Why Deep Learning Works \u2014 Step by Step Tutorial | by Michel Kana, Ph.D | Towards Data Science", "h1": "Why Deep Learning Works \u2014 Step by Step Tutorial", "description": "Narrative & visual aids to understand neural networks, gradient descent, back propagation. Hands-on Keras code for linear & logistic regression, autoencoder, convolutional neural network, VGG16."}, "outgoing_paragraph_urls": [{"url": "https://blog.statsbot.co/deep-learning-achievements-4c563e034257", "anchor_text": "this article", "paragraph_index": 8}, {"url": "https://keras.io/", "anchor_text": "Keras", "paragraph_index": 18}, {"url": "https://towardsdatascience.com/wild-wide-ai-responsible-data-science-16b860e1efe9", "anchor_text": "responsible data science", "paragraph_index": 26}, {"url": "https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6", "anchor_text": "here", "paragraph_index": 40}, {"url": "http://playground.tensorflow.org", "anchor_text": "http://playground.tensorflow.org", "paragraph_index": 45}, {"url": "http://yann.lecun.com/exdb/mnist/", "anchor_text": "MNIST digits", "paragraph_index": 53}], "all_paragraphs": ["This article is for those who miss a painless journey through the black box of deep learning. We motivate and engage the readers with a story about a farmer who cared a bit too much about the future. We start looking at his problems by solving a simple linear and logistic regression using an artificial neural network. We gradually build up our understanding by looking at gradient descent, chain rule and back propagation. We later get deeper by building an autoencoder and adding up convolutional layers to denoise MNIST hand-written digits. We further extract features from the pre-trained VGG16 network after explaining why such networks are so successful in computer vision.", "By reading this article, you will finally crack the mystery of deep learning.", "You will also have meaningful open-ended questions. That\u2019s all the point of (deep) learning, right?", "This is the story of an old man who wanted to know about his future. He was a farmer. It was his good fortune to have fields producing an increasing amount of crops each year. He had too much of it in his granaries and had to offer the overflow to his friends and neighbors. Still, he was worried about the future. He did not know how much to give to charity and how much to keep. Actually, this depended on how much he will harvest the next years. He needed an accurate prediction of the production in the following years.", "So, what\u2019s the connection to deep learning? Please keep on reading and you will be delighted. The oldest son of the farmer came up with the following experiment. He took samples of grains and put a quantity corresponding approximately to the magnitude of harvest for each of the previous year in a bag. Each bag represented a year, in ascending order of time. He then ask his youngest sibling to put a stick on the top of the bags so that the stick does not fall down and is well balanced, as shown on the picture featuring this article.", "This visual gave the old man an idea of the harvest he might expect in the upcoming years assuming that the trend kept the same pace. Had his son just invented linear regression, hundreds of years before Francis Galton (sorry, Sir)?", "If we can do linear regression, why do we need artificial neural networks?", "Keep on reading, we will tackle deep learning soon. People are obsessed about the future. We want to know if it will rain tomorrow, if we will get married next year, if and when we will finally get rich. In the past these were the favorite questions asked to oracles (not the software company Oracle). Oracles were presumably specially skilled people who would predict future based on real-word data such as lines in the hand, stars constellation or taste of blood.", "Being able to predict the future is not always a good thing. Cassandra of Troy had the gift of foreseeing. To her own bad, she was cursed by Apollo that no one would believe her predictions. Therefore, her warnings of the destruction of Troy were ignored and things didn\u2019t turn out great for her. Today, companies, organizations and governments seek their own Cassandra of Troy, by hiring Data Scientists, oracles of modern time. Because some data scientists and machine learners want to be as obscure as possible, in order to gain more importance, like oracles of the past, they choose to use the so-called Deep Learning technology, all the time. Also, because it works. Not only for predicting the future but also for understanding the present, making inferences and classifying all sort of things. You can look at this article to see for yourself, what amazing things we can do with deep learning.", "Why does deep learning work? Why would our farmer\u2019s son need deep learning, although he has already invented linear regression? These are questions we will try to approach in the following sections.", "We will first have a look at the farmer\u2019s son\u2019s linear model. He used the year as predictor of the harvest quantity (its magnitude). By drawing the best possible matching line which follows the trend of quantity over years, he was able to predict the harvest quantity for the following years. Mathematically, he found a line \u0176 over years X with intercept \u03b20 and slope \u03b21.", "Given real world measurements for Y, and the corresponding predictor X, we are able to compute the slope and the intercept as follows.", "An image is worth a thousand words, so is Python code. In order to make beautiful plots and explore the model, we will recreate the data used by the farmer\u2019s son. Let\u2019s create 100 artificial set of random measurements along the straight line in a plane represented by the linear equation. We distribute those points along the line uniformly in the interval between -2 and 3. We then add a random normally distributed value with standard deviation of 1 and mean 0 to each Y value. We finally use the formula above to compute the intercept and slope.", "As we can see in the image above, the farmer\u2019s son idea is a winner. We can predict Y for any value of X, since we know the regression line that dictates Y behavior. So, can we do the same with an artificial neural network? First, we need to define what is actually an artificial neural network.", "What came first? Neuroscience or Artificial Neural Networks? Anyway, the idea of artificial neuron seems to come from neurons in our head. Our brain is made of 100 billion neurons. As electrical signals flow through the dendrites into the nucleus of the neuron, electric charge builds up. When the cell reaches a certain level of charge, a threshold, it activates, sending an electrical signal out through the axon. Neurons weight different incoming signals differently when deciding when to fire. Some believe that artificial neural networks mimic the process of assigning different weights to different dendrites (inputs).", "A single artificial neuron takes an input X, applies a (linear) affine transformation using weights W to produce a temporary value h. It then pass that value through an activation function to produce an output. In our linear regression example, X can be a number, let\u2019s say 2. The affine transformation will use the intercept and slope stored in the vector W to transform the input, in order to return, let\u2019s say h=5. The later value will go through an activation, which is a linear function in our case, to output, let\u2019s say Y=h=5.", "We saw previously that the intercept and the slope of a regression line can be directly calculated using an analytical formula. Can an artificial neuron, as we have just described, find those values without using any formula? We could use the following strategy: we can pick random values for intercept and slope, let\u2019s say W=(-1, 3), then, the neuron can calculate a corresponding regression line and compare it with the true Y values. We will then be able to calculate the difference between the output from the neuron and the true Y, i.e. the loss. In our case the mean-squared error is a good loss function, we call L. We would repeat the process of picking random W values until the loss is small enough or until we are tired trying. This strategy could work fine for one neuron. However if we have two or thousands neurons, we can spend an entire life searching. This is why that strategy sound so crazy that people came up with a better idea called gradient descent. It is like descending down a snowed alps mountain in freestyle ski from the top through the shortest path down to the valley. Without getting killed.", "Gradient descent is a tool for finding the values for W which minimize the loss function L by looking at the derivative of that function with respect to W. Let\u2019s say we have chosen W=(-1, 3) and we have calculated the loss for these values, now we have to decide which next values of W to try in order to have a decreasing loss. The idea is to calculate the derivative of the loss, let\u2019s say with respect to the intercept, and to check its signs. If the derivative is positive, it means that the loss is increasing, so we need to try a smaller intercept. If the derivative is negative, the loss is decreasing, so we can keep trying larger values of the slope. If the derivative is zero, we have reached a minima of the loss, hopefully a global one. We can stop and consider W as the optimal weight that minimizes the loss. Gradient descent requires calculating derivatives, which can become a nightmare if we don\u2019t have a good tool like Matlab. Thanks to back propagation, we don\u2019t need such tools even, as we will see later.", "Let\u2019s have a practical look at how our neuron will perform on the linear regression task. We use Keras, a Python library and good companion for deep learning experimentation. We create a model with a single neuron, a scalar input, a scalar output, an affine transformation with a weight and bias, and a linear output. Next, we train our neuron on the data points while using stochastic gradient descent optimizer. The loss function is the mean-squared error. The optimizer will perform 5 iterations (epochs). For the sake of simplicity we do not scale the data.", "Voil\u00e0! A single neuron was able to predict almost the same regression line (green), which we previously calculated analytically (orange). The predicted values for slope and intercept are very close, as displayed in the image above. The R2 scores, a measure of fitness, look also similar in both cases. It seems that Artificial Neural Network really works! Well, what if we make it a bit harder?", "Our farmer realized soon that only few things in life are linear. The trend in harvest was likely to reach a plateau and to undergo a decline. Below, we visualize a new dataset that exhibits a so-called \u201cone-hump\u201d trend.", "Obviously, a single neuron with linear activation will not be able to capture the behavior in this new data. A first thought is to change the linear activation function and to use a S-like shape function instead. The sigmoid function is a good candidate that is pretty much used in deep learning. A sigmoid would perfectly capture the increasing trend observed in our data. The decreasing trend can also be captured by a mirrored sigmoid. Thus, we need two sigmoids, therefore two neurons, and a third neuron that will sum up both sigmoids. The corresponding architecture is shown in the image below.", "The first neuron will take a scalar X as input, then apply an affine transformation using weight W1 and a bias. The result will go through a sigmoid activation to produce h1. Something similar will happen in the second neuron to produce h2. Now, both numbers h1 and h2 will be fed into the third neuron, which will perform an affine transformation by applying the weights W31 to h1 and W32 to h2, finally adding a bias. The resulting q will go through a linear activation to produce the one-hump shape.", "Let\u2019s construct this network in Python, from scratch, without Keras. The network has a total of 7 weights (and bias) as we saw in the previous section. We give values to the 7 weights manually, perform a forward pass using the data for the hump function and plot the output from the network, in the same plot as the true Y values. We adjust the weights manually until the plots matched as closely as possible. By doing so, we are simulating the so-called forward-pass, which is performed by Keras when training a network.", "Like Cassandra of Troy, we were able to guess the perfect values for W, so that the output of the network overfits the data perfectly. Although most data scientists can do such magic, not all people have this gift. Luckily, there is another way to do this, for the common people. This is to use a library, such as Keras to run a gradient descent optimizer that tries its best to find those 7 weights for us. This is what we are illustrating in the code below.", "After 50000 epochs of training, the network achieves 96.69% R2 score. The plot above shows its prediction compared to the one we previously obtained through manual weights tuning. It looks quite amazing, doesn\u2019t it? Given 3 neurons, X and Y data points, the network is able to fine tune itself to fit the data. It has learned the shape of the data without any further insight. Below, we can visually see how the search for the best W has converged towards the lowest loss. If we let the optimizer run for additional epochs, we would eventually reach 100% fit. Some readers of this article are already grinding their teeth and thinking about overfitting and scaling. Well, let\u2019s be happy for now and come back to this hot topic later.", "This result is a further evidence supporting the claim that artificial neural networks have the capability to capture non-linear dependencies in data. Is this why deep learning works? Well, we cheated a bit here. We designed the network based on our prior knowledge of the double-sigmoidal shape of the data, and therefore, by choosing the sigmoid as activation function. Data from real-word scenario does not always have such a clear shape and we will end up playing around to find the right network architecture. Sooner or later, we will introduce our view of the world into the network architecture, turning it into a biased model. You can look at my other article about responsible data science. For now, let\u2019s assume for now that we are all good guys. The question why does deep learning work, remains however open.", "Our farmer was not quite happy with the hump model, especially its second part, telling him to expect a decrease in harvest. He wanted rather to know which years to expect good harvest. Below we create a dataset that would reflect the situation. Years below 0 are classified as years with bad harvest, while years after 0 offered good harvest.", "Let\u2019s now see if a simple neuron is able to classify a given year as a good year for harvesting or not. This is a logistic regression problem. Below we feed our data into a Keras model, using stochastic gradient descent as optimizer for a binary crossentropy loss (too many keywords to amaze you). We achieve 81% classification accuracy, as you can see in the plot below, the sigmoid activation function did a good job.", "The classification problem we have just solved is a good example for getting into the dark world of gradient descent and back-propagation. We recall that, in our neuron above, the input went through an affine transformation and the result was fed into a sigmoid activation. The output is therefore a value between 0 or 1, the likelihood of having a good harvest. Below we see the expression of this output P(Y=1). Note that the affine transformation on X, depends on two parameters \u03b20 and \u03b21.", "During training, for any input Xi, the neuron is able to compute the likelihood Pi and compare it to the true value Yi. The error is typically calculated using binary cross-entropy. It does the same job as the mean squared error we introduce previously. Binary cross-entropy measures how far away from the true value Yi (which is either 0 or 1) is the prediction Pi. The formula used to calculate that binary cross-entropy Li is given below.", "As you can see, the loss depends on the weights \u03b20 and \u03b21. After the gradient descent optimizer has chosen random weights, it picks a random Xi, then does a forward pass to calculate the loss. The optimizer repeats this calculation for all input data points if we are using the basic gradient descent, or just for small batches of data if we are using the stochastic version. The full loss is obtained by summing up all individual losses. After the optimizer has calculated the loss, it will compute its derivative with respect to the weights. Based on the sign of the derivative, it will update the weight either positively or negatively.", "We know that we want to go in the opposite direction of the derivative and we know we want to be making a step proportionally to the derivative. The learning rate \u03bb is what controls that proportion for every weight W (\u03b20 or \u03b21).", "As you can see in the image above, updating the weights requires calculating the partial derivatives of the loss with respect to each weight. How do we do that? Well, we can use high school math to do this. In the image below we try to compute the derivation of the cross-entropy loss function with respect to \u03b20 and \u03b21.", "For a network with one neuron, this is a great solution. But imagine, a network with hundreds of neurons as we usually encounter in deep learning. I bet, you don\u2019t want to calculate the resulting derivative. Even if you succeed in doing so, you will have to update your formulas every time the architecture of the network changes, even just a little bit. Here is where backpropagation comes into play. The back propagation algorithm was originally introduced in the 1970s, but its importance wasn\u2019t fully appreciated until a 1986 paper by Geoffrey Hinton.", "Back propagation uses the chain rule, which is convenient mnemonics for writing derivatives of nested functions. For example if we have a network with one neuron that feeds into a second and finally into a third in order to get an output. The total loss function f is a function of the loss function g of the first two neurons, similarly g is a function of the loss function h of the first neuron.", "Basically, while we go forward from the inputs calculating the outputs of each neuron up to the last neuron, we also evaluate tiny components of the derivative already. In the example above, we are able to calculate dh/dx already when going forward through the first neuron. Next, we are able to calculate dg/dh when going forward through the second neuron. Finally, we start calculating df/dg going backwards through the neurons and by reusing all of the elements already calculated. That\u2019s the origin of the name back propagation. There are several implementations and flavors of this technique. For the sake of simplicity, we keep it simple here.", "In order to illustrate how the chain rule and back propagation work, let\u2019s return to the loss function for our 1-neuron network with sigmoid activation. The loss function was defined as binary cross-entropy, which can be separated into two parts A and B, as shown below.", "Let\u2019s have a closer look at part A of the loss function. It can be divided into blocks highlighted with red boxes on the image below.", "Back propagation requires to compute the derivative of that function at any given data point X for any given weight W. This is done by calculating the derivative of each block and putting all together using the chain rule. Below we see how this would work for X=3 and W=3.", "All we need is to be able to calculate derivatives of the small blocks (called variables above). Such blocks are known because activation functions are usually known. They can be sigmoid, linear, ReLu, \u2026etc. These are differentiable functions with known derivatives. You can find the full list of trendy activation functions here. Therefore the calculations above can be built up during run time using a computational graph. Keras is able to look at your network architecture and the activation functions used at each neuron, in order to build a computational graph during model compilation. That graph is used during training to perform forward pass and backpropagation. An example of computational graph for the cross-entropy loss function is presented below.", "Understanding how gradient descent and backpropagation work is a great step in understanding why deep learning actually works. Well, we have not gone that deep yet. Till now, we used only 3 neurons. Our farmer can argue that the relationship between years and harvest quantity is too complex to be represented only by two classes: good or bad. In this section we will explore a clustering problem with artificial neural network. Let\u2019s simulate some cloud of data distributed in three groups representing harvesting quality. We consider three points in (x,y) plane with coordinates (-2,0), (0,1.7) and (2.1,0). Around each of those three centers we create a cloud of 100 randomly generated points. For the radial distance of any one of those points from its center we use a random normal distribution. For the angular coordinate of any one of \u201ccloud\u201d points we use the uniform distribution.", "We will now create a two-layer artificial neural network that can predict whether a randomly generated point in the plane belongs to cloud 0, 1 or 2. The input layer has 50 neurons (we are getting deep)! It receives two values (x, y). A ReLU activation function is applied to the affine transformation of the input. The result is fed into an output layer with 3 neurons, which use the softmax activation function to predict one of the three classes of cloud. The stochastic gradient optimizer is used to solve the classification problem of estimating 303 weights and biases by minimizing the categorical cross-entropy used as loss function. We split the data into 50 data points for training the classifier and 50 for validation. We train in batch of 5 points during 20 epochs. After training we also perform prediction on test sets. We use argmax to convert the predicted categorical vector into original labels prior to display.", "As shown above, the neural network does a good job in separating the classes in the test set (right plot). Some outliers found in the original data (left) are assigned a different label by the network (right) in order to produce clear boundaries. We achieve 95.33% classification accuracy on test data.", "Below we randomly generate brand new and unseen coordinates in the plane. We then use the trained network to predict the class (cloud). As we can see in the plot, the network clearly separate all three clouds, although there are few points very close to boundaries, which are good candidates for false positive/negative. Overall, it seems that we are starting to do deep learning.", "Now, we got our farmer\u2019s grandson\u2019s grandson\u2019s grandson excited about deep learning! Head straight to http://playground.tensorflow.org to build your own artificial neural network by visually tinker layers, neurons, activation function, training and testing right in your browser. When you are done, come back here for more exciting stuff to come.", "Readers, who have not been impressed by now, already know why deep learning works. For the remaining of us, we will now have a look at an example of network architecture that gets kind of deep. An Autoencoder is made of a pair of two connected networks: an encoder model and a decoder model. The goal of an autoencoder is to find a way to encode the input into a compressed form (latent space) in such a way that the decoded version is as close as possible to the input. The encoder model turns the input into a small dense representation. The decoder model can be seen as a generative model which is able to generate specific features. Both encoder and decoder are usually trained as a whole. The loss function penalizes the network for creating output which differ from input. Therefore the encoder learns to preserve as much of the relevant information needed in the limitation of the latent space, and cleverly discard irrelevant parts, e.g. noise. The decoder learns to take the compressed latent information and reconstruct it into a full error-free input. An autoencoder can be useful for dimensionality reduction and denoising images, but can also be successful in unsupervised machine translation (wow!).", "Below we define the simplest architecture of an autoencoder that processes images of noisy hand-written digits. We provide the network with original images x, as well as their noisy version x~. The network tries to reconstruct its output x\u2019 to be as close as possible to the original image. By doing so, it learns how to denoise any hand-written digit.", "In the next few paragraphs, we see how such a network would work by implementing a small version of it using Keras.", "Input: a 28x28 grey scaled image. Therefore the input is a 784-elements vector.", "Encoder: a single dense layer with 64 neurons. Therefore the latent space will have dimension 64. The activation function used is rectified units (ReLu), chosen from best practices. The activation function is attached to each neuron in the layer, and determines whether it should be activated (\u201cfired\u201d) or not, based on whether each neuron\u2019s input is relevant for the autoencoder\u2019s prediction. The activation function also helps normalize the output of each neuron to a range between 1 and 0.", "Decoder: a single dense layer with 784 neurons, corresponding to a 28x28 greyscaled image. The sigmoid activation function is used, as a best practice for the classification task (input to encoder vs output from decoder).", "Optimizer: we use binary cross-entropy as loss function and Adadelta as optimizer for minimizing the loss function.", "Data: we\u2019re using MNIST digits to generate synthetic noisy digits by applying a Gaussian noise matrix and clip the images between 0 and 1. Below we display some of the noisy digits.", "We can still recognize digits, but barely. Therefore, we want to use our autoencoder to learn to recover the original digits. We do this by fitting the autoencoder over 100 epoch while using the noisy digits as input and original denoised digits as target. Thus, the autoencoder will minimize the difference between noisy and clean images. By doing this it will learn how to remove noise from any unseen hand-written digit, that was produced with similar noise.", "Below we plot noisy input images against their cleaned version.", "Hopefully, this denoising example has illustrated the opportunities in deep learning. The deeper we get by adding layers, the more complex the model becomes, the more magic it can achieve. However complexity comes at the price of overfitting to training data and less generalization power. This is where regularization and other tricks come into place. In the next section, we will upgrade our autoencoder by adding convolutional layers.", "The previous sections gently introduced the idea of using neurons to approximate linear and non-linear phenomenon. We also demonstrated how two layers of neurons are enough to learn the correct features in hand-written digits and to remove noisy features. In this section, we explore how to stack several layers with hundreds of neurons that learn low-level features in noisy images in order to reduce information loss in the latent space and increase the quality of generated digits.", "Now, let\u2019s build a deep network. We start with a first convolutional layer Conv2D that has 16 filters, each with dimension 3x3. Padding is the same, stride is 1, therefore the output features map will have shape 28x28x16. We apply a ReLu activation function to this output, after adding a bias to it. Convolution causes repetition of information coming from neighboring areas of the image, leading to high dimensionality. Pooling takes an area of the output of a convolution layer and returns an aggregated value: usually max, but average, min or any function can be used.", "We add a MaxPooling2D layer of size 2x2, stride 1 and padding same. This means that we will take the max value of 2x2 chunks, and move by 1 step to the next chunk. The output of pooling will be padded with zeros to have the same dimension as the input. Pooling is always done after applying activation function to the convolution.", "CNNs work the best when considering multiple scales. Therefore, we duplicate the convolutional and max pooling layers one more time. The resulting encoder outputs a representation of the image in the shape 7x7x32. This is because the input image 28x28x1 is convolved by each of the 32 3x3 filters in the first Conv2D layer, returning a features space of dimension 28x28x32. MaxPooling reduces that representation to the half 14x14x32. The second Conv2D layer convolves each of the 14x14x32 representation using a set of 32 3x3 filters. This results into a feature space of dimension 14x14x32. The last MaxPooling layer reduces the representation by its half size, resulting in the shape 7x7x32.", "The decoder has the same architecture as the encoder with layers aligned in reverse order. We use a UpSampling layer to restore the dimensionality reduction that was performed by MaxPooling in the encoder. Finally we add a convolutional layer which restore the number of channels in the last representation to 1. The Sigmoid activation function is used to solve a classification problem, where the binary outcome reflects how similar the decoded image compares to the input one. We can observe how the decoding part of the autoencoder has restored the original image shape (28x28x1).", "The full network has 28,353 weights and bias to train. We use binary cross-entropy as loss function and Adadelta as optimizer for minimizing the loss function. After 20 epochs, the CNN autoencoder has a train/test loss value of about 0.098, compared to the higher loss 0.1235 we had with the dense autoencoder. Our autoencoder with CNNs performs quite well on unseen images. Compared to the simple autoencoder with 1 dense hidden layer from the previous section, denoised images look a lot better.", "The way to stack convolutional layers together is more an art than a science. Don\u2019t be afraid to look at the VGG network below, it was proposed in 2014 for classifying 14 million images into to 1000 classes with 92.7% test accuracy. It was trained for 2 weeks to estimate its 14,714,688 weights. Horrible! There are more of such supermassive networks: GoogLeNet, ResNet, DenseNet, MobileNet, Xception, ResNeXt, just to name a few.", "A recommended and highly effective approach to deep learning is to leverage one of those pre-trained networks. A pre-trained network is simply a saved network previously trained on a large dataset, and can effectively act as a generic model of the real world. Feature extraction is one way of using the representations learned by pre-trained network by taking the convolutional base of a pre-trained network, running the new data through it, and training a new simple classifier on top of the output using a new small dataset, as illustrated below. An alternative technique consists of freezing the base of the pre-trained network, appending a simple classifier and training the whole. Another alternative, called fine-tuning, consists of unfreezing specific layers in the base before training.", "In this article, we assumed a neural network architecture approach to deep learning. Instead of directly explain why Deep Learning algorithms learn progressively the representation of complex inputs as it goes through each neural network layer, we started small, with a single neuron. Through hands-on examples and Python code we solved basic regression tasks with artificial neural networks and gradually built up to create an autoencoder with convolutional layers. We showcased the learning process through gradient descent and backpropagation. We shortly introduced transfer learning using pre-trained supermassive artificial neural networks such as VGG16.", "We did not touch topics like style transfer, where textures, not object shapes, seem to be the most important aspects of an object for object recognition. The concepts we explained, will work slightly differently for recurrent neural networks, sequence-to-sequence networks, long-short-term-memory networks, and many other fancy deep learning extraterrestrial inventions.", "Covering Deep Learning in a single article is a joke, we did not attempt to make. The classic books \u201cDeep Learning\u201d by Ian Goodfellow and \u201cDeep Learning with Python\u201d by Francois Chollet are excellent starting points.", "Why Deep Learning Works? This is an open-ended question. Thank Goodness.", "Thank you for reading. Look at my article about Bayesian approach in machine learning below.", "Husband & Dad. Mental health advocate. Top Medium Writer. 20 years in IT. AI Expert @Harvard. Empowering human-centered organizations with high-tech."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F289f17cab01a&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-deep-learning-works-289f17cab01a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-deep-learning-works-289f17cab01a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-deep-learning-works-289f17cab01a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-deep-learning-works-289f17cab01a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://michel-kana.medium.com/?source=post_page-----289f17cab01a--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----289f17cab01a--------------------------------", "anchor_text": ""}, {"url": "https://michel-kana.medium.com/?source=post_page-----289f17cab01a--------------------------------", "anchor_text": "Michel Kana, Ph.D"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcb0b01fe20d2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-deep-learning-works-289f17cab01a&user=Michel+Kana%2C+Ph.D&userId=cb0b01fe20d2&source=post_page-cb0b01fe20d2----289f17cab01a---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----289f17cab01a--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F289f17cab01a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-deep-learning-works-289f17cab01a&user=Michel+Kana%2C+Ph.D&userId=cb0b01fe20d2&source=-----289f17cab01a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F289f17cab01a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-deep-learning-works-289f17cab01a&source=-----289f17cab01a---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://blog.statsbot.co/deep-learning-achievements-4c563e034257", "anchor_text": "this article"}, {"url": "https://keras.io/", "anchor_text": "Keras"}, {"url": "https://towardsdatascience.com/wild-wide-ai-responsible-data-science-16b860e1efe9", "anchor_text": "responsible data science"}, {"url": "https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6", "anchor_text": "here"}, {"url": "http://playground.tensorflow.org", "anchor_text": "http://playground.tensorflow.org"}, {"url": "https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html", "anchor_text": "source"}, {"url": "http://yann.lecun.com/exdb/mnist/", "anchor_text": "MNIST digits"}, {"url": "https://towardsdatascience.com/bayesian-nightmare-how-to-start-loving-bayes-1622741fa960", "anchor_text": "Bayesian nightmare. Solved!Gentle introduction to Bayesian data analysis by examples and cod in Python PyMC3.towardsdatascience.com"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----289f17cab01a---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/tag/keras?source=post_page-----289f17cab01a---------------keras-----------------", "anchor_text": "Keras"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----289f17cab01a---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----289f17cab01a---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----289f17cab01a---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F289f17cab01a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-deep-learning-works-289f17cab01a&user=Michel+Kana%2C+Ph.D&userId=cb0b01fe20d2&source=-----289f17cab01a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F289f17cab01a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-deep-learning-works-289f17cab01a&user=Michel+Kana%2C+Ph.D&userId=cb0b01fe20d2&source=-----289f17cab01a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F289f17cab01a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-deep-learning-works-289f17cab01a&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://michel-kana.medium.com/?source=post_page-----289f17cab01a--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----289f17cab01a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcb0b01fe20d2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-deep-learning-works-289f17cab01a&user=Michel+Kana%2C+Ph.D&userId=cb0b01fe20d2&source=post_page-cb0b01fe20d2----289f17cab01a---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F69e95067d2a1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-deep-learning-works-289f17cab01a&newsletterV3=cb0b01fe20d2&newsletterV3Id=69e95067d2a1&user=Michel+Kana%2C+Ph.D&userId=cb0b01fe20d2&source=-----289f17cab01a---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://michel-kana.medium.com/?source=post_page-----289f17cab01a--------------------------------", "anchor_text": "Written by Michel Kana, Ph.D"}, {"url": "https://michel-kana.medium.com/followers?source=post_page-----289f17cab01a--------------------------------", "anchor_text": "5.4K Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----289f17cab01a--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcb0b01fe20d2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-deep-learning-works-289f17cab01a&user=Michel+Kana%2C+Ph.D&userId=cb0b01fe20d2&source=post_page-cb0b01fe20d2----289f17cab01a---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F69e95067d2a1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-deep-learning-works-289f17cab01a&newsletterV3=cb0b01fe20d2&newsletterV3Id=69e95067d2a1&user=Michel+Kana%2C+Ph.D&userId=cb0b01fe20d2&source=-----289f17cab01a---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/bert-for-dummies-step-by-step-tutorial-fb90890ffe03?source=author_recirc-----289f17cab01a----0---------------------bee55aee_880c_4e66_abf5_78bc5f40abbf-------", "anchor_text": ""}, {"url": "https://michel-kana.medium.com/?source=author_recirc-----289f17cab01a----0---------------------bee55aee_880c_4e66_abf5_78bc5f40abbf-------", "anchor_text": ""}, {"url": "https://michel-kana.medium.com/?source=author_recirc-----289f17cab01a----0---------------------bee55aee_880c_4e66_abf5_78bc5f40abbf-------", "anchor_text": "Michel Kana, Ph.D"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----289f17cab01a----0---------------------bee55aee_880c_4e66_abf5_78bc5f40abbf-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/bert-for-dummies-step-by-step-tutorial-fb90890ffe03?source=author_recirc-----289f17cab01a----0---------------------bee55aee_880c_4e66_abf5_78bc5f40abbf-------", "anchor_text": "BERT for dummies \u2014 Step by Step TutorialDIY Practical guide on Transformer. Hands-on proven PyTorch code for Intent Classification with BERT fine-tuned."}, {"url": "https://towardsdatascience.com/bert-for-dummies-step-by-step-tutorial-fb90890ffe03?source=author_recirc-----289f17cab01a----0---------------------bee55aee_880c_4e66_abf5_78bc5f40abbf-------", "anchor_text": "\u00b711 min read\u00b7Sep 15, 2019"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffb90890ffe03&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbert-for-dummies-step-by-step-tutorial-fb90890ffe03&user=Michel+Kana%2C+Ph.D&userId=cb0b01fe20d2&source=-----fb90890ffe03----0-----------------clap_footer----bee55aee_880c_4e66_abf5_78bc5f40abbf-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/bert-for-dummies-step-by-step-tutorial-fb90890ffe03?source=author_recirc-----289f17cab01a----0---------------------bee55aee_880c_4e66_abf5_78bc5f40abbf-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffb90890ffe03&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbert-for-dummies-step-by-step-tutorial-fb90890ffe03&source=-----289f17cab01a----0-----------------bookmark_preview----bee55aee_880c_4e66_abf5_78bc5f40abbf-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----289f17cab01a----1---------------------bee55aee_880c_4e66_abf5_78bc5f40abbf-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----289f17cab01a----1---------------------bee55aee_880c_4e66_abf5_78bc5f40abbf-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----289f17cab01a----1---------------------bee55aee_880c_4e66_abf5_78bc5f40abbf-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----289f17cab01a----1---------------------bee55aee_880c_4e66_abf5_78bc5f40abbf-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----289f17cab01a----1---------------------bee55aee_880c_4e66_abf5_78bc5f40abbf-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----289f17cab01a----1---------------------bee55aee_880c_4e66_abf5_78bc5f40abbf-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----bee55aee_880c_4e66_abf5_78bc5f40abbf-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----289f17cab01a----1---------------------bee55aee_880c_4e66_abf5_78bc5f40abbf-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----289f17cab01a----1-----------------bookmark_preview----bee55aee_880c_4e66_abf5_78bc5f40abbf-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----289f17cab01a----2---------------------bee55aee_880c_4e66_abf5_78bc5f40abbf-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----289f17cab01a----2---------------------bee55aee_880c_4e66_abf5_78bc5f40abbf-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----289f17cab01a----2---------------------bee55aee_880c_4e66_abf5_78bc5f40abbf-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----289f17cab01a----2---------------------bee55aee_880c_4e66_abf5_78bc5f40abbf-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----289f17cab01a----2---------------------bee55aee_880c_4e66_abf5_78bc5f40abbf-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----289f17cab01a----2---------------------bee55aee_880c_4e66_abf5_78bc5f40abbf-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----2-----------------clap_footer----bee55aee_880c_4e66_abf5_78bc5f40abbf-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----289f17cab01a----2---------------------bee55aee_880c_4e66_abf5_78bc5f40abbf-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----289f17cab01a----2-----------------bookmark_preview----bee55aee_880c_4e66_abf5_78bc5f40abbf-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/generative-adversarial-network-gan-for-dummies-a-step-by-step-tutorial-fdefff170391?source=author_recirc-----289f17cab01a----3---------------------bee55aee_880c_4e66_abf5_78bc5f40abbf-------", "anchor_text": ""}, {"url": "https://michel-kana.medium.com/?source=author_recirc-----289f17cab01a----3---------------------bee55aee_880c_4e66_abf5_78bc5f40abbf-------", "anchor_text": ""}, {"url": "https://michel-kana.medium.com/?source=author_recirc-----289f17cab01a----3---------------------bee55aee_880c_4e66_abf5_78bc5f40abbf-------", "anchor_text": "Michel Kana, Ph.D"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----289f17cab01a----3---------------------bee55aee_880c_4e66_abf5_78bc5f40abbf-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/generative-adversarial-network-gan-for-dummies-a-step-by-step-tutorial-fdefff170391?source=author_recirc-----289f17cab01a----3---------------------bee55aee_880c_4e66_abf5_78bc5f40abbf-------", "anchor_text": "Generative Adversarial Network (GAN) for Dummies \u2014 A Step By Step TutorialThe ultimate beginner guide for understanding, building and training GANs with bulletproof Python code."}, {"url": "https://towardsdatascience.com/generative-adversarial-network-gan-for-dummies-a-step-by-step-tutorial-fdefff170391?source=author_recirc-----289f17cab01a----3---------------------bee55aee_880c_4e66_abf5_78bc5f40abbf-------", "anchor_text": "\u00b710 min read\u00b7Apr 20, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffdefff170391&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerative-adversarial-network-gan-for-dummies-a-step-by-step-tutorial-fdefff170391&user=Michel+Kana%2C+Ph.D&userId=cb0b01fe20d2&source=-----fdefff170391----3-----------------clap_footer----bee55aee_880c_4e66_abf5_78bc5f40abbf-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/generative-adversarial-network-gan-for-dummies-a-step-by-step-tutorial-fdefff170391?source=author_recirc-----289f17cab01a----3---------------------bee55aee_880c_4e66_abf5_78bc5f40abbf-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "5"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffdefff170391&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerative-adversarial-network-gan-for-dummies-a-step-by-step-tutorial-fdefff170391&source=-----289f17cab01a----3-----------------bookmark_preview----bee55aee_880c_4e66_abf5_78bc5f40abbf-------", "anchor_text": ""}, {"url": "https://michel-kana.medium.com/?source=post_page-----289f17cab01a--------------------------------", "anchor_text": "See all from Michel Kana, Ph.D"}, {"url": "https://towardsdatascience.com/?source=post_page-----289f17cab01a--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----289f17cab01a----0---------------------90cb0480_55f1_40aa_9cc6_55b1c6905a18-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----289f17cab01a----0---------------------90cb0480_55f1_40aa_9cc6_55b1c6905a18-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----289f17cab01a----0---------------------90cb0480_55f1_40aa_9cc6_55b1c6905a18-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----289f17cab01a----0---------------------90cb0480_55f1_40aa_9cc6_55b1c6905a18-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----289f17cab01a----0---------------------90cb0480_55f1_40aa_9cc6_55b1c6905a18-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----289f17cab01a----0---------------------90cb0480_55f1_40aa_9cc6_55b1c6905a18-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----0-----------------clap_footer----90cb0480_55f1_40aa_9cc6_55b1c6905a18-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----289f17cab01a----0---------------------90cb0480_55f1_40aa_9cc6_55b1c6905a18-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----289f17cab01a----0-----------------bookmark_preview----90cb0480_55f1_40aa_9cc6_55b1c6905a18-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----289f17cab01a----1---------------------90cb0480_55f1_40aa_9cc6_55b1c6905a18-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----289f17cab01a----1---------------------90cb0480_55f1_40aa_9cc6_55b1c6905a18-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----289f17cab01a----1---------------------90cb0480_55f1_40aa_9cc6_55b1c6905a18-------", "anchor_text": "The PyCoach"}, {"url": "https://artificialcorner.com/?source=read_next_recirc-----289f17cab01a----1---------------------90cb0480_55f1_40aa_9cc6_55b1c6905a18-------", "anchor_text": "Artificial Corner"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----289f17cab01a----1---------------------90cb0480_55f1_40aa_9cc6_55b1c6905a18-------", "anchor_text": "You\u2019re Using ChatGPT Wrong! Here\u2019s How to Be Ahead of 99% of ChatGPT UsersMaster ChatGPT by learning prompt engineering."}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----289f17cab01a----1---------------------90cb0480_55f1_40aa_9cc6_55b1c6905a18-------", "anchor_text": "\u00b77 min read\u00b7Mar 17"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fartificial-corner%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&user=The+PyCoach&userId=fb44e21903f3&source=-----886a50dabc54----1-----------------clap_footer----90cb0480_55f1_40aa_9cc6_55b1c6905a18-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----289f17cab01a----1---------------------90cb0480_55f1_40aa_9cc6_55b1c6905a18-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "275"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&source=-----289f17cab01a----1-----------------bookmark_preview----90cb0480_55f1_40aa_9cc6_55b1c6905a18-------", "anchor_text": ""}, {"url": "https://jehillparikh.medium.com/u-nets-with-attention-c8d7e9bf2416?source=read_next_recirc-----289f17cab01a----0---------------------90cb0480_55f1_40aa_9cc6_55b1c6905a18-------", "anchor_text": ""}, {"url": "https://jehillparikh.medium.com/?source=read_next_recirc-----289f17cab01a----0---------------------90cb0480_55f1_40aa_9cc6_55b1c6905a18-------", "anchor_text": ""}, {"url": "https://jehillparikh.medium.com/?source=read_next_recirc-----289f17cab01a----0---------------------90cb0480_55f1_40aa_9cc6_55b1c6905a18-------", "anchor_text": "Jehill Parikh"}, {"url": "https://jehillparikh.medium.com/u-nets-with-attention-c8d7e9bf2416?source=read_next_recirc-----289f17cab01a----0---------------------90cb0480_55f1_40aa_9cc6_55b1c6905a18-------", "anchor_text": "U-Nets with attentionU-Net are popular NN architecture which are employed for many applications and were initially developed for medical image segmentation."}, {"url": "https://jehillparikh.medium.com/u-nets-with-attention-c8d7e9bf2416?source=read_next_recirc-----289f17cab01a----0---------------------90cb0480_55f1_40aa_9cc6_55b1c6905a18-------", "anchor_text": "\u00b72 min read\u00b7Nov 15, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2Fc8d7e9bf2416&operation=register&redirect=https%3A%2F%2Fjehillparikh.medium.com%2Fu-nets-with-attention-c8d7e9bf2416&user=Jehill+Parikh&userId=c972081b627e&source=-----c8d7e9bf2416----0-----------------clap_footer----90cb0480_55f1_40aa_9cc6_55b1c6905a18-------", "anchor_text": ""}, {"url": "https://jehillparikh.medium.com/u-nets-with-attention-c8d7e9bf2416?source=read_next_recirc-----289f17cab01a----0---------------------90cb0480_55f1_40aa_9cc6_55b1c6905a18-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc8d7e9bf2416&operation=register&redirect=https%3A%2F%2Fjehillparikh.medium.com%2Fu-nets-with-attention-c8d7e9bf2416&source=-----289f17cab01a----0-----------------bookmark_preview----90cb0480_55f1_40aa_9cc6_55b1c6905a18-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/using-transformers-for-computer-vision-6f764c5a078b?source=read_next_recirc-----289f17cab01a----1---------------------90cb0480_55f1_40aa_9cc6_55b1c6905a18-------", "anchor_text": ""}, {"url": "https://wolfecameron.medium.com/?source=read_next_recirc-----289f17cab01a----1---------------------90cb0480_55f1_40aa_9cc6_55b1c6905a18-------", "anchor_text": ""}, {"url": "https://wolfecameron.medium.com/?source=read_next_recirc-----289f17cab01a----1---------------------90cb0480_55f1_40aa_9cc6_55b1c6905a18-------", "anchor_text": "Cameron R. Wolfe"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----289f17cab01a----1---------------------90cb0480_55f1_40aa_9cc6_55b1c6905a18-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/using-transformers-for-computer-vision-6f764c5a078b?source=read_next_recirc-----289f17cab01a----1---------------------90cb0480_55f1_40aa_9cc6_55b1c6905a18-------", "anchor_text": "Using Transformers for Computer VisionAre Vision Transformers actually useful?"}, {"url": "https://towardsdatascience.com/using-transformers-for-computer-vision-6f764c5a078b?source=read_next_recirc-----289f17cab01a----1---------------------90cb0480_55f1_40aa_9cc6_55b1c6905a18-------", "anchor_text": "\u00b713 min read\u00b7Oct 5, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6f764c5a078b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-transformers-for-computer-vision-6f764c5a078b&user=Cameron+R.+Wolfe&userId=28aa6026c553&source=-----6f764c5a078b----1-----------------clap_footer----90cb0480_55f1_40aa_9cc6_55b1c6905a18-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/using-transformers-for-computer-vision-6f764c5a078b?source=read_next_recirc-----289f17cab01a----1---------------------90cb0480_55f1_40aa_9cc6_55b1c6905a18-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "4"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6f764c5a078b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-transformers-for-computer-vision-6f764c5a078b&source=-----289f17cab01a----1-----------------bookmark_preview----90cb0480_55f1_40aa_9cc6_55b1c6905a18-------", "anchor_text": ""}, {"url": "https://medium.com/mlearning-ai/understanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f?source=read_next_recirc-----289f17cab01a----2---------------------90cb0480_55f1_40aa_9cc6_55b1c6905a18-------", "anchor_text": ""}, {"url": "https://medium.com/@martin-thissen?source=read_next_recirc-----289f17cab01a----2---------------------90cb0480_55f1_40aa_9cc6_55b1c6905a18-------", "anchor_text": ""}, {"url": "https://medium.com/@martin-thissen?source=read_next_recirc-----289f17cab01a----2---------------------90cb0480_55f1_40aa_9cc6_55b1c6905a18-------", "anchor_text": "Martin Thissen"}, {"url": "https://medium.com/mlearning-ai?source=read_next_recirc-----289f17cab01a----2---------------------90cb0480_55f1_40aa_9cc6_55b1c6905a18-------", "anchor_text": "MLearning.ai"}, {"url": "https://medium.com/mlearning-ai/understanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f?source=read_next_recirc-----289f17cab01a----2---------------------90cb0480_55f1_40aa_9cc6_55b1c6905a18-------", "anchor_text": "Understanding and Coding the Attention Mechanism \u2014 The Magic Behind TransformersIn this article, I\u2019ll give you an introduction to the attention mechanism and show you how to code the attention mechanism yourself."}, {"url": "https://medium.com/mlearning-ai/understanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f?source=read_next_recirc-----289f17cab01a----2---------------------90cb0480_55f1_40aa_9cc6_55b1c6905a18-------", "anchor_text": "\u00b712 min read\u00b7Dec 6, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fmlearning-ai%2Ffe707a85cc3f&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fmlearning-ai%2Funderstanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f&user=Martin+Thissen&userId=f99c73950195&source=-----fe707a85cc3f----2-----------------clap_footer----90cb0480_55f1_40aa_9cc6_55b1c6905a18-------", "anchor_text": ""}, {"url": "https://medium.com/mlearning-ai/understanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f?source=read_next_recirc-----289f17cab01a----2---------------------90cb0480_55f1_40aa_9cc6_55b1c6905a18-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffe707a85cc3f&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fmlearning-ai%2Funderstanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f&source=-----289f17cab01a----2-----------------bookmark_preview----90cb0480_55f1_40aa_9cc6_55b1c6905a18-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----289f17cab01a----3---------------------90cb0480_55f1_40aa_9cc6_55b1c6905a18-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----289f17cab01a----3---------------------90cb0480_55f1_40aa_9cc6_55b1c6905a18-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----289f17cab01a----3---------------------90cb0480_55f1_40aa_9cc6_55b1c6905a18-------", "anchor_text": "Alexander Nguyen"}, {"url": "https://levelup.gitconnected.com/?source=read_next_recirc-----289f17cab01a----3---------------------90cb0480_55f1_40aa_9cc6_55b1c6905a18-------", "anchor_text": "Level Up Coding"}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----289f17cab01a----3---------------------90cb0480_55f1_40aa_9cc6_55b1c6905a18-------", "anchor_text": "Why I Keep Failing Candidates During Google Interviews\u2026They don\u2019t meet the bar."}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----289f17cab01a----3---------------------90cb0480_55f1_40aa_9cc6_55b1c6905a18-------", "anchor_text": "\u00b74 min read\u00b7Apr 13"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fgitconnected%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&user=Alexander+Nguyen&userId=a148fd75c2e9&source=-----dc8f865b2c19----3-----------------clap_footer----90cb0480_55f1_40aa_9cc6_55b1c6905a18-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----289f17cab01a----3---------------------90cb0480_55f1_40aa_9cc6_55b1c6905a18-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "89"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&source=-----289f17cab01a----3-----------------bookmark_preview----90cb0480_55f1_40aa_9cc6_55b1c6905a18-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----289f17cab01a--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----289f17cab01a--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----289f17cab01a--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----289f17cab01a--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----289f17cab01a--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----289f17cab01a--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----289f17cab01a--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----289f17cab01a--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----289f17cab01a--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----289f17cab01a--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}