{"url": "https://towardsdatascience.com/the-fascinating-no-gradient-approach-to-neural-net-optimization-abb287f88c97", "time": 1683011595.842454, "path": "towardsdatascience.com/the-fascinating-no-gradient-approach-to-neural-net-optimization-abb287f88c97/", "webpage": {"metadata": {"title": "The Fascinating No-Gradient Approach to Neural Net Optimization | by Andre Ye | Towards Data Science", "h1": "The Fascinating No-Gradient Approach to Neural Net Optimization", "description": "Gradient descent is one of the most important ideas in machine learning: given some cost function to minimize, the algorithm iteratively takes steps of the greatest downward slope, theoretically\u2026"}, "outgoing_paragraph_urls": [{"url": "https://andre-ye.medium.com/membership", "anchor_text": "https://andre-ye.medium.com/membership", "paragraph_index": 22}], "all_paragraphs": ["Gradient descent is one of the most important ideas in machine learning: given some cost function to minimize, the algorithm iteratively takes steps of the greatest downward slope, theoretically landing in a minima after a sufficient number of iterations. First discovered by Cauchy in 1847 but expanded upon in Haskell Curry for non-linear optimization problems in 1944, gradient descent has been used for all sorts of algorithms, from linear regression to deep neural networks.", "While gradient descent and its repurposing in the form of backpropagation has been one of the greatest breakthroughs in machine learning, the optimization of neural networks remain an unsolved problem. Many on the Internet are willing to go as far as to declare that \u2018gradient descent sucks\u2019, and while that may be a bit far, it is true that gradient descent has many issues.", "Of course, gradient descent has been extensively studied, and there are many proposed solutions \u2014 some which are GD variations and others that are network architecture-based \u2014 that work in some cases. Just because gradient descent is overrated doesn\u2019t mean it\u2019s not the best possible solution available currently. Commonplace knowledge like using batch normalization to smooth the error space or picking a sophisticated optimizer like Adam or Adagrad are not the focus of this article, even if they generally perform better.", "Instead, the purpose of this writing is to shed some well-deserved light onto more obscure and certainty interesting optimization methods that don\u2019t fit the standard bill of being gradient-based, which, like any other technique used to improve the performance of the neural network, work exceptionally well in some scenarios and not so well in others. Regardless of how well they perform on a specific task, however, they are fascinating, creative, and a promising area of research for the future of machine learning.", "Particle Swarm Optimization is a population-based method that defines a set of \u2018particles\u2019 that explore the search space, attempting to find a minimum. PSO iteratively improved a candidate solution with respect to a certain quality metric. It solves the problem by having a population of potential solutions (\u2018particles\u2019) and moving them around according to simple mathematical rules like the particle\u2019s position and velocity. Each particle\u2019s movement is influenced by the local position it believes is best, but is also attracted by the best known positions in the search-place (found by other particles). In theory, the swarm moves over several iterations towards the best solutions.", "PSO is a fascinating idea \u2014 it is much less sensitive to initialization than neural networks, and the communication between particles on certain findings could prove to be a very efficient method of searching sparse and large areas.", "Because Particle Swarm Optimization is not gradient-based (gasp!), it does not require the optimization problem to be differentiable; hence using PSO to optimize a neural network or any other algorithm would allow more freedom and less sensitivity on the choice of activation function or equivalent role in other algorithms. Additionally, it makes little to no assumptions about the problem being optimized and can search very large spaces.", "As one may imagine, population-based methods can be a fair bit more computationally expensive than gradient-based optimizers, but not necessarily so. Because the algorithm is so open and non-rigid \u2014 as evolution-based algorithms often are, one can control the number of particles, the speed at which they move, the amount of information that is shared globally, and so on; just like one might tune learning rates in a neural network.", "Surrogate optimization is a method of optimization that attempts to model the loss function with another well-established function to find the minima. The technique samples \u2018data points\u2019 from the loss function, meaning it tries different values for parameters (the x) and stores the value of the loss function (the y). After a sufficient number of data points have been collected, a surrogate function (in this case, a 7th-degree polynomial) is fitted to the collected data.", "Because finding the minima of polynomials is a very well-studied subject and there exist a host of very efficient methods to find the global minima of a polynomial using derivatives, we can assume that the global minima of the surrogate function is the same for the loss function.", "Surrogate optimization is technically a non-iterative method, although the training of the surrogate function is often iterative; additionally, it is technically a no-gradient method, although often effective mathematical methods to find the global minima of the modelling function are based on derivatives. However, because both the iterative and gradient-based properties are \u2018secondary\u2019 to surrogate optimization, it can handle large data and non-differentiable optimization problems.", "Optimization using a surrogate function is quite clever in a few ways:", "Surrogate optimization is almost always faster than gradient descent methods, but often at the cost of accuracy. Using surrogate optimization may only be able to pinpoint the rough location of a global minima, but this can still be tremendously beneficial.", "An alternative is a hybrid model; a surrogate optimization is used to bring the neural network parameters to the rough location, from which gradient descent can be used to find the exact global minima. Another is to use the surrogate model to guide the optimizer\u2019s decisions, since the surrogate function can a) \u2018see ahead\u2019 and b) is less sensitive to specific ups and downs of the loss function.", "Simulated Annealing is a concept based on annealing in metallurgy, in which a material can be heated above its recrystallization temperature to reduce its hardness and alter other physical and occasionally chemical properties, then allowing the material to gradually cool and become rigid again.", "Using the notion of slow cooling, simulated annealing slowly decreases the probability of accepting worse solutions as the solution space is explored. Because accepting worse solutions allows for a more broad search for the global minima (think \u2014 cross the hill for a deeper valley), simulated annealing assumes that the possibilities are properly represented and explored in the first iterations. As time progresses, the algorithm moves away from exploration and towards exploitations.", "The following is a rough outline of how simulated annealing algorithms work:", "The simulation can be performed with kinetic equations or with stochastic sampling methods. Simulated Annealing was used to solve the travelling salesman problem, which tries to find the shortest distance between hundreds of locations, represented by data points. Obviously, the combinations are endless, but simulated annealing \u2014 with its reminiscence of reinforcement learning \u2014 performs very well.", "Simulated annealing performs especially well in scenarios where an approximate solution is required in a short period of time, outperforming the slow pace of gradient descent. Like surrogate optimization, it can be used in hybrid with gradient descent for the benefits of both: the speed of simulated annealing and the accuracy of gradient descent.", "This was a small sampling of no-gradient methods; there exist many additional algorithms like pattern search and multi-objective optimization to be explored. Genetic and population-based algorithms like particle swarm optimization are extremely promising for creating truly \u2018intelligent\u2019 agents, given that we, humans, are evidence of its success.", "Non-gradient methods for optimization are fascinating because of the creativity many of them utilize, not being restricted by the mathematical chains of gradients. No one expects no-gradient methods to go mainstream ever because gradient-based optimization performs so well even considering its many problems. However, harnessing the power of no-gradient and gradient-based methods with hybrid optimizers demonstrates extremely high potential, especially in an era where we are reaching a computational limit.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "ML enthusiast. Join Medium through my referral link: https://andre-ye.medium.com/membership."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fabb287f88c97&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-fascinating-no-gradient-approach-to-neural-net-optimization-abb287f88c97&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-fascinating-no-gradient-approach-to-neural-net-optimization-abb287f88c97&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-fascinating-no-gradient-approach-to-neural-net-optimization-abb287f88c97&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-fascinating-no-gradient-approach-to-neural-net-optimization-abb287f88c97&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----abb287f88c97--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----abb287f88c97--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://andre-ye.medium.com/?source=post_page-----abb287f88c97--------------------------------", "anchor_text": ""}, {"url": "https://andre-ye.medium.com/?source=post_page-----abb287f88c97--------------------------------", "anchor_text": "Andre Ye"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fbe743a65b006&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-fascinating-no-gradient-approach-to-neural-net-optimization-abb287f88c97&user=Andre+Ye&userId=be743a65b006&source=post_page-be743a65b006----abb287f88c97---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fabb287f88c97&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-fascinating-no-gradient-approach-to-neural-net-optimization-abb287f88c97&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fabb287f88c97&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-fascinating-no-gradient-approach-to-neural-net-optimization-abb287f88c97&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/some-call-it-genius-others-call-it-stupid-the-most-controversial-neural-network-ever-created-2224ed22795a", "anchor_text": "Some Call it Genius, Others Call it Stupid: The Most Controversial Neural Network Ever CreatedThe Extreme Learning Machinetowardsdatascience.com"}, {"url": "https://towardsdatascience.com/youre-not-measuring-covid-19-correctly-80398c251a1", "anchor_text": "You\u2019re Not Measuring COVID-19 CorrectlyThe Metric Problemtowardsdatascience.com"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----abb287f88c97---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/ai?source=post_page-----abb287f88c97---------------ai-----------------", "anchor_text": "AI"}, {"url": "https://medium.com/tag/data-science?source=post_page-----abb287f88c97---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----abb287f88c97---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----abb287f88c97---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fabb287f88c97&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-fascinating-no-gradient-approach-to-neural-net-optimization-abb287f88c97&user=Andre+Ye&userId=be743a65b006&source=-----abb287f88c97---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fabb287f88c97&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-fascinating-no-gradient-approach-to-neural-net-optimization-abb287f88c97&user=Andre+Ye&userId=be743a65b006&source=-----abb287f88c97---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fabb287f88c97&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-fascinating-no-gradient-approach-to-neural-net-optimization-abb287f88c97&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----abb287f88c97--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fabb287f88c97&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-fascinating-no-gradient-approach-to-neural-net-optimization-abb287f88c97&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----abb287f88c97---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----abb287f88c97--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----abb287f88c97--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----abb287f88c97--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----abb287f88c97--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----abb287f88c97--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----abb287f88c97--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----abb287f88c97--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----abb287f88c97--------------------------------", "anchor_text": ""}, {"url": "https://andre-ye.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://andre-ye.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Andre Ye"}, {"url": "https://andre-ye.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "9.8K Followers"}, {"url": "https://andre-ye.medium.com/membership", "anchor_text": "https://andre-ye.medium.com/membership"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fbe743a65b006&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-fascinating-no-gradient-approach-to-neural-net-optimization-abb287f88c97&user=Andre+Ye&userId=be743a65b006&source=post_page-be743a65b006--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Ff44a966e4ff1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-fascinating-no-gradient-approach-to-neural-net-optimization-abb287f88c97&newsletterV3=be743a65b006&newsletterV3Id=f44a966e4ff1&user=Andre+Ye&userId=be743a65b006&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}