{"url": "https://towardsdatascience.com/advanced-spark-tuning-optimization-and-performance-techniques-54f858c92e", "time": 1683016189.149987, "path": "towardsdatascience.com/advanced-spark-tuning-optimization-and-performance-techniques-54f858c92e/", "webpage": {"metadata": {"title": "Advanced Spark Tuning, Optimization, and Performance Techniques | by Garrett R Peternel | Towards Data Science", "h1": "Advanced Spark Tuning, Optimization, and Performance Techniques", "description": "Apache Spark is a distributed computing big data analytics framework designed to transform, engineer, and process massive amounts of data (think terabytes and petabytes) across a cluster of machines\u2026"}, "outgoing_paragraph_urls": [{"url": "https://spark.apache.org/docs/latest/quick-start.html#self-contained-applications", "anchor_text": "Apache Spark Documentation", "paragraph_index": 11}, {"url": "https://spark.apache.org/docs/latest/configuration.html", "anchor_text": "Apache Spark Documentation", "paragraph_index": 18}], "all_paragraphs": ["Apache Spark is a distributed computing big data analytics framework designed to transform, engineer, and process massive amounts of data (think terabytes and petabytes) across a cluster of machines. It has a plethora of embedded components for specific tasks including Spark SQL\u2019s Structured DataFrame and Structured Streaming APIs, both of which will be discussed in this blog. One of the challenges with Spark is appending new data to a data lake thus producing \u2018small and skewed files\u2019 on write. It can be tricky to solve these challenges completely, which consequently have a negative impact on users performing additional downstream Spark layers, Data Science analysis, and SQL queries consuming the \u2018small and skewed files\u2019. Fairly new frameworks Delta Lake and Apache Hudi help address these issues.", "However, in this blog using the native Scala API I will walk you through two Spark problem solving techniques of 1.) how to include a transient timer in your Spark Structured Streaming job for gracefully auto-terminating periodic data processing appends of new source data, and 2.) how to control the number of output files and the size of the partitions produced by your Spark jobs. Problem solve #1 capability avoids always paying for a long-running (sometimes idle) \u201824/7\u2019 cluster (i.e. in Amazon EMR). For example, short-lived streaming jobs are a solid option for processing only new available source data (i.e. in Amazon S3) that does not have a consistent cadence arrival; perhaps landing every hour or so as mini-batches. Problem solve #2 capability is really important for improving the I/O performance of downstream processes such as next layer Spark jobs, SQL queries, Data Science analysis, and overall data lake metadata management.", "Disclaimer: The public datasets used in this blog contain very small data volumes and are used for demonstration purposes only. These Spark techniques are best applied on real-world big data volumes (i.e. terabytes & petabytes). Hence, size, configure, and tune Spark clusters & applications accordingly.", "1a.) First, let\u2019s view some sample files and define the schema for the public IoT device event dataset retrieved from Databricks Community Edition stored at dbfs:/databricks-datasets/structured-streaming/events/.", "1b.) Next, we will read the dataset as a streaming dataframe with the schema defined, as well as, include function arguments:", "1c.) Now, we execute the streaming query as parquet file sink format and append mode to ensure only new data is periodically written incrementally, as well as, include function arguments:", "1d.) A Scala sleep function (in milliseconds) will be used to shutdown the streaming job on a graceful transient timer.", "1e.) Lastly, the streaming job Spark Session will be executed after the timer expires thus terminating the short-lived application.", "1f.) Apply the functions to Scala values, and optionally set additional Spark properties if needed:", "1g.) View the job\u2019s output location", "In summary, the streaming job will continuously process, convert, and append micro-batches of unprocessed data only from the source json location to the target parquet location. After the timer runs out (ex: 5 min) a graceful shutdown of the Spark application occurs. For Spark application deployment, best practices include defining a Scala object with a main() method including args: Array[String] as command line arguments. Then create a required directory structure to compile the <appName>.scala (application code) file with a build.sbt (library dependencies) file all via SBT build tool to create a JAR file, which will be used to run the application via spark-submit.", "Here is official Apache Spark Documentation explaining the steps.", "In AWS, via Amazon EMR you can submit applications as job steps and auto-terminate the cluster\u2019s infrastructure when all steps complete. This can be fully orchestrated, automated, and scheduled via services like AWS Step Functions, AWS Lambda, and Amazon CloudWatch.", "Sometimes the output file size of a streaming job will be rather \u2018skewed\u2019 due to a sporadic cadence arrival of the source data, as well as, the timing challenge of always syncing it with the trigger of the streaming job. Example 2 will help address and optimize the \u2018small and skewed files\u2019 dilemma.", "2a.) First, let\u2019s view some sample files and read our public airlines input dataset (retrieved from Databricks Community Edition stored at dbfs:/databricks-datasets/airlines/ and converted to small parquet files for demo purposes) and identify the number of partitions in the dataframe.", "2b.) In order to calculate the desired output partition (file) size you need to estimate the size (i.e. megabytes) of the input dataframe by persisting it in memory. This can be determined ad hoc beforehand via executing df.cache() or df.persist(), call an action like df.count() or df.foreach(x => println(x)) to cache the entire dataframe, and then search for the dataframe's RAM size in the Spark UI under the Storage tab.", "2c.) The Spark property spark.default.parallelism can help with determining the initial partitioning of a dataframe, as well as, be used to increase Spark parallelism. Generally it is recommended to set this parameter to the number of available cores in your cluster times 2 or 3. For example, in Databricks Community Edition the spark.default.parallelism is only 8 ( Local Mode single machine with 1 Spark executor and 8 total cores). For real-world scenarios, I recommend you avoid trying to set this application parameter at runtime or in a notebook. In Amazon EMR, you can attach a configuration file when creating the Spark cluster's infrastructure and thus achieve more parallelism using this formula spark.default.parallelism = spark.executor.instances * spark.executors.cores * 2 (or 3). For review, the spark.executor.instances property is the total number of JVM containers across worker nodes. Each executor has a universal fixed amount of allocated internal cores set via the spark.executor.cores property.", "\u2018Cores\u2019 are also known as \u2018slots\u2019 or \u2018threads\u2019 and are responsible for executing Spark \u2018tasks\u2019 in parallel, which are mapped to Spark \u2018partitions\u2019 also known as a \u2018chunk of data in a file\u2019.", "Here is official Apache Spark Documentation explaining the many properties.", "2d.) The new dataframe\u2019s partition value will be determined on which integer value is larger: (defaultParallelism times multiplier) or (approx. dataframe memory size divided by approx. desired partition size).", "2e.) For demonstration, the cached dataframe is approximately 3,000 mb and a desired partition size is 128 mb. In this example, the calculated partition size (3,000 divided by 128=~23) is greater than the default parallelism multiplier (8 times 2=16) hence why the value of 23 was chosen as the repartitioned dataframe\u2019s new partition count to split on.", "2f.) Lastly, we view some sample output partitions and can see there are exactly 23 files ( part-00000 to part-00022) approximately 127 mb (~127,000,000 bytes=~127 mb) each in size, which is close to the set 128 mb target size, as well as, within the optimized 50 to 200 mb recommendation. Having the same optimized file size across all partitions solves the \u2018small and skewed files\u2019 problem that harms data lake management, storage costs, and analytics I/O performance. Alternatives include partitioning the data by columns too. For example, a folder hierarchy (i.e. year / month / day) containing 1 merged partition per day. Specific best practices will vary and depend on use case requirements, data volume, and data structure though.", "In perspective, hopefully, you can see that Spark properties like spark.sql.shuffle.partitions and spark.default.parallelism have a significant impact on the performance of your Spark applications. It is critical these kinds of Spark properties are tuned accordingly to optimize the output number and size of the partitions when processing large datasets across many Spark worker nodes.", "In summary, these kind of Spark techniques have worked for me on many occasions when building out highly available and fault tolerant data lakes, resilient machine learning pipelines, cost-effective cloud compute and storage savings, and optimal I/O for generating a reusable curated feature engineering repository. However, they may or may not be official best practices within the Spark community. The benefits will likely depend on your use case. In addition, exploring these various types of tuning, optimization, and performance techniques have tremendous value and will help you better understand the internals of Spark. Creativity is one of the best things about open source software and cloud computing for continuous learning, solving real-world problems, and delivering solutions. Thank you for reading this blog.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Engineering | Machine Learning | Solutions Architecture"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F54f858c92e&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadvanced-spark-tuning-optimization-and-performance-techniques-54f858c92e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadvanced-spark-tuning-optimization-and-performance-techniques-54f858c92e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadvanced-spark-tuning-optimization-and-performance-techniques-54f858c92e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadvanced-spark-tuning-optimization-and-performance-techniques-54f858c92e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----54f858c92e--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----54f858c92e--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@garrett.r.peternel?source=post_page-----54f858c92e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@garrett.r.peternel?source=post_page-----54f858c92e--------------------------------", "anchor_text": "Garrett R Peternel"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6abb8b9bd26&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadvanced-spark-tuning-optimization-and-performance-techniques-54f858c92e&user=Garrett+R+Peternel&userId=6abb8b9bd26&source=post_page-6abb8b9bd26----54f858c92e---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F54f858c92e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadvanced-spark-tuning-optimization-and-performance-techniques-54f858c92e&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F54f858c92e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadvanced-spark-tuning-optimization-and-performance-techniques-54f858c92e&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@chuttersnap?utm_source=medium&utm_medium=referral", "anchor_text": "CHUTTERSNAP"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://spark.apache.org/docs/latest/quick-start.html#self-contained-applications", "anchor_text": "Apache Spark Documentation"}, {"url": "https://spark.apache.org/docs/latest/configuration.html", "anchor_text": "Apache Spark Documentation"}, {"url": "https://medium.com/tag/spark?source=post_page-----54f858c92e---------------spark-----------------", "anchor_text": "Spark"}, {"url": "https://medium.com/tag/aws?source=post_page-----54f858c92e---------------aws-----------------", "anchor_text": "AWS"}, {"url": "https://medium.com/tag/big-data?source=post_page-----54f858c92e---------------big_data-----------------", "anchor_text": "Big Data"}, {"url": "https://medium.com/tag/data-science?source=post_page-----54f858c92e---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/data-engineering?source=post_page-----54f858c92e---------------data_engineering-----------------", "anchor_text": "Data Engineering"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F54f858c92e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadvanced-spark-tuning-optimization-and-performance-techniques-54f858c92e&user=Garrett+R+Peternel&userId=6abb8b9bd26&source=-----54f858c92e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F54f858c92e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadvanced-spark-tuning-optimization-and-performance-techniques-54f858c92e&user=Garrett+R+Peternel&userId=6abb8b9bd26&source=-----54f858c92e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F54f858c92e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadvanced-spark-tuning-optimization-and-performance-techniques-54f858c92e&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----54f858c92e--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F54f858c92e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadvanced-spark-tuning-optimization-and-performance-techniques-54f858c92e&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----54f858c92e---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----54f858c92e--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----54f858c92e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----54f858c92e--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----54f858c92e--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----54f858c92e--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----54f858c92e--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----54f858c92e--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----54f858c92e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@garrett.r.peternel?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@garrett.r.peternel?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Garrett R Peternel"}, {"url": "https://medium.com/@garrett.r.peternel/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "96 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6abb8b9bd26&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadvanced-spark-tuning-optimization-and-performance-techniques-54f858c92e&user=Garrett+R+Peternel&userId=6abb8b9bd26&source=post_page-6abb8b9bd26--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fce17f72c880b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadvanced-spark-tuning-optimization-and-performance-techniques-54f858c92e&newsletterV3=6abb8b9bd26&newsletterV3Id=ce17f72c880b&user=Garrett+R+Peternel&userId=6abb8b9bd26&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}