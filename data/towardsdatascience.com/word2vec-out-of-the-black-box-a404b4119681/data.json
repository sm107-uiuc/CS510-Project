{"url": "https://towardsdatascience.com/word2vec-out-of-the-black-box-a404b4119681", "time": 1683017779.461385, "path": "towardsdatascience.com/word2vec-out-of-the-black-box-a404b4119681/", "webpage": {"metadata": {"title": "Word2Vec: Out of the Black Box | Towards Data Science", "h1": "Word2Vec: Out of the Black Box", "description": "An exploration of Word2Vec: We review how the algorithm works and provide a step-by-step practical example in an easy to understand style."}, "outgoing_paragraph_urls": [{"url": "https://medium.com/u/98ef37157d28?source=post_page-----a404b4119681--------------------------------", "anchor_text": "Nick Paine", "paragraph_index": 0}], "all_paragraphs": ["By: Jenna Jones and Nick Paine", "How does a computer learn to interpret or even understand human language? This is what we set out to understand when we began researching Natural Language Processing (NLP). One simple but effective tool is Word2Vec, an algorithm created by Mikolov et al in 2013.", "Word2Vec takes a large body of text, which can be in any language, and creates a vector representation of words and the context of those words in relation to each other. Once in vector form, the computer processes the word data numerically, allowing it to capture semantic and syntactic characteristics of words and word combinations.", "Trained models can answer queries like:", "\u2018Best\u2019 is to \u2018Worst\u2019 as \u2018Catan\u2019 is to ? and the network answers \u2018Monopoly\u2019", "and the \u2018hello world\u2019 of NLP:", "\u2018Man\u2019 is to \u2018King\u2019 as \u2018Woman\u2019 is to ? and the network answers \u2018Queen\u2019", "We consider these examples to be an amazing feat of textual analysis, especially as the analysis is probabilistic.", "Word2Vec can use either of two algorithms: CBOW (continuous bag of words) or Skip-Gram. We will only use the Skip-Gram neural network algorithm in this article.", "This algorithm uses a center word to predict the probability of each word in the vocabulary V being a context word within a chosen window size.", "To train the network, we apply each word from the vocabulary to the input layer (a series of one hot vectors) and adjust the weights in the two weight matrices to get an output prediction from the network as close as possible to the expected output.", "To make sense of all of this, we will work through an example together but to summarise, the steps are as follows:", "We are going to show how Word2Vec works by processing a short example corpus:", "\u201cEvery rose has its thorn just like every night has its dawn\u201d", "This is a corpus of 12 words and 9 unique words (\u201cevery\u201d, \u201chas\u201d and \u201cits\u201d appear more than once). Therefore, our vocabulary size, V = 9.", "Before we get started with our algorithm, we need to do some preprocessing.", "We collect information about which words occur next to each other- this is called co-occurrence data and we hold it in a co-occurrence matrix. To obtain this information, we use a context window.", "In our example, we use a context window of 2; however, the choice of window size is arbitrary and is usually determined by trial and error. We will run over the entire corpus and use each word in the vocabulary as a center word in turn, recording every word that is within the context window (either in front of or behind the center word). This is done by adding a 1 when the context word appears in its appropriate dimension in the co-occurrence vector.", "As in the above diagram, suppose we want to record the words that occur closely to \u2018thorn\u2019. We will record 4 words: \u2018has\u2019, \u2018its\u2019, \u2018just\u2019, \u2018like\u2019. We will then do this same process for the next word, \u2018just\u2019. You should be able to see that the words at the beginning and end of the corpus will have only 2 context words.", "After completing this process, each word will be associated with a co-occurrence vector (combined below to form a 9x9 matrix), and this data will be used to train the network.", "b. Initialize two randomly weighted matrices", "Here, we will initially set random values for each of the two weight matrices in the network to start it off. These weights will change over time and become meaningful as the network \u2018learns\u2019. The weights themselves do not mean anything in the human language- they don\u2019t represent a characteristic of the word. They are the means by which the whole network passes information from layer to layer and will change as the network learns relationships between inputs and outputs.", "The dimensions of the matrices are based on the number of unique words in our vocabulary and the number of hidden layer neurons. For our example, we have 9 unique words and have chosen 3 hidden layer neurons (this is a hyper parameter meaning we just choose what we think is an appropriate figure); therefore, our input and output weight matrices will be of dimensions 9x3 and 3x9, respectively.", "Since the computer does not understand actual words, we need to convert each word into a mathematical representation. This is referred to as one-hot encoding. We create a V dimensional vector with each dimension representing each unique word in our vocabulary. The order of the words is insignificant and does not represent any of their characteristics. All dimensions are set to 0 except for the center word that is the input into the network, in this case, \u2018thorn\u2019, which is set to 1.", "To calculate the forward pass, we take the dot product of the one hot vector and input weight matrix (recall this weight matrix from the Preprocessing section and that we have chosen to use 3 neurons in the hidden layer). This dot product gives the hidden layer (called hidden because it is not part of what we see in the output). Since the input layer is all zeros except for the center word, the multiplication will yield only one row related to the center word. An important note to understand is that each row in the input weight matrix is actually the word vector used for each word! So if we were on our last epoch and the weight matrices were finalised, the word \u2018thorn\u2019 would be located in the 3-dimensional vector space at (-0.41, -0.63, 0.84).", "We then take the dot product of the hidden layer and the output weight matrix (the other weight matrix from the Preprocessing section) to calculate the output layer with 9 neurons. The output of the network is a single vector including every word in the vocabulary.", "The end goal is to use the output layer to train the network (shown below in Part C) and use the input weight matrix to create word vectors.", "Once the network has been trained, word vectors will place similar words close to each other in the vector space. Similar ranges from \u2018dog\u2019 and \u2018dogs\u2019 to \u2018good\u2019 and \u2018great\u2019 to apple and pear.", "If you are familiar with neural networks, you may be wondering why we haven\u2019t applied an activation function like the sigmoid, tanh or ReLU functions to the hidden layer. For Word2Vec, this isn\u2019t necessary as the point of the hidden layer (apart from the ability it gives to a network to process more complicated factors such as the exclusive or XOR problem) is to map the input word vectors into a lower dimensionality while maintaining separation between dissimilar words. Most activation functions invoke some \u201csquashing\u201d of the space in one region and expanding of the space in another. This clearly reduces the amount of word mapping space available. On the other hand, we usually find that the value of the hidden layer is compromised if there is a linear activation function (as there is in Word2Vec) as the combination of two or more linear functions is itself linear. This is not a problem in Word2Vec as since the input to the network is a one hot vector, there is only ever one input to each neuron in the hidden layer.", "The output layer of the network consists of numbers but we want to obtain a set of probabilities (ie the sum of the outputs needs to equal 1). In order to do this, the output layer must be passed through a final process. There are a number of methods that can be used, but one of the most popular is softmax. Each value in the outer layer is processed and turned into a probability:", "So to calculate the probability of the word \u2018just\u2019 occurring within a window of two of \u2018thorn\u2019, we take:", "The prediction vector shows, for each word in the vocabulary, the probability that it is within a window size of x of the center word. In our example, we hope that the prediction vector will show that the words \u2018has\u2019, \u2018its\u2019, \u2018just\u2019 and \u2018like\u2019 have a high probability of being within two words of \u2018thorn\u2019. Obviously, since the network is not yet trained, the predictions will be wrong at this stage but with training, we will see them converge to the correct figures.", "In this section, we outline the theory and math involved in backpropagation. If you would like to skip this and continue with the example, click here.", "Backpropagation (or backprop in machine learning language) is an algorithm that calculates the error between the prediction and the actual figures known from the co-occurence matrix for each input word, and then adjusts the weights in the two weight matrices to minimise this error. When the weights have been adjusted, the loss function is recalculated and the process is done again and again until the loss function is smaller than a pre-determined value or after a given number of iterations have been carried out.", "To do this, we calculate a loss function, L, which measures the error between the predicted and actual figures for a known single fixed input/output pair of vectors. When completed, a new fixed input/output pair is then used for the calculation.", "We then calculate a gradient of the loss function with respect to the elements in each of the weight matrices and use these gradients to follow the loss function down to a minimum as we changes the weights.", "The loss function we will use here is:", "There are a number of different ways to use the gradients, depending on the application to solve and the degree of computing overhead needed: gradient descent, stochastic gradient descent, negative sampling, etc. We will use gradient descent in our example.", "Backpropogation calculates the gradient of the loss function with respect to the weights efficiently rather than looking at a direct computation of each individual weight and does it in a way where the gradients do not have to be continuously recomputed. We show you how to do this in our example below.", "This gives the change to the weights as:", "A larger alpha value will make the network \u2018learn\u2019 more quickly but will also make it susceptible to overshooting the minimum. On the other hand, a lower alpha value will make the network \u2018learn\u2019 more slowly as the weights are not moved as much.", "To carry out the gradient descent (and indeed any of the other techniques to change the weights in the weight matrices) we need to compute the partial derivative of the loss function with respect to w as discussed above. Recall,", "A quick note on notation- i represents neuron i, N_Hj is the output of the jth neuron in the hidden layer, and w_Oji is the weight in the output weight matrix between the jth neuron in the hidden layer and the ith neuron in the outer layer.", "We will use the chain rule of calculus to find the gradient of the loss function with respect to the output weight matrix:", "We are going to break down each component in the equation:", "For classification tasks, the cross entropy or log loss function is generally chosen and for regression or multi-class classification tasks, the squared error loss function is generally chosen. We have used the squared error loss function.", "Therefore, we can now update the output weight matrix:", "Here, we just follow the same procedure as for the output weight matrix:", "which can be expanded out to:", "We already have the first two derivatives from the output matrix gradient descent; therefore, we only need to calculate the remaining two.", "Therefore, we can now update the input weight matrix:", "Now it is time to apply the gradient descent and use the formulae above to update the weight matrices- we start with the output weight matrix. We calculated the prediction O\u2019 in Step 3 and we can obtain Y from the co-occurrence vector for \u2018thorn\u2019 and pass it through the softmax function to achieve a probability. In our example, 4 words are within a context window of 2 of \u2018thorn\u2019 so we attribute a probability of 0.25 to each of them. From this, we are able to calculate the gradient based on the formulae above.", "We then use this gradient matrix to update the output matrix as:", "The same process is replicated using the input weight matrix formula.", "We have now shown one full epoch. After running through many epochs, the model will become trained. This is when we can begin to have some fun and do linear algebra arithmetic with words. Some examples of analysis include computing the degree of similarity between words, completing analogies, and finding a word in a list that doesn\u2019t belong. Of course with our example network, the corpus is too small to do any meaningful analysis, so we use Google\u2019s pre-trained dataset with 3 million words and 300-dimension hidden layer. We will end where we began and calculate: (Worst-Best)+Catan = ?", "The network assigns \u2018Monopoly\u2019 as the answer with the highest probability at: 0.77! We can also create a 2-dimensional Principle Component Analysis model to plot the word vectors in a scatter plot.", "Additionally, we can look at similar words and see how they are grouped. We see that words with similar semantic meanings (such as the list of countries or the list of drinks) have successfully been grouped together.", "We have taken the Word2Vec algorithm out of the box and shown how it works, giving you the mechanics to build your own analysis network.", "This is an extremely powerful and surprisingly successful algorithm for analysing language and can also be a lot of fun with many applications.", "A large corpus can be analyzed fairly quickly with a modern laptop and programming environment like Python. We have only touched the surface on Word2Vec so with this introduction, there are fascinating areas where you can continue your research such as: faster algorithms like negative sampling and stochastic gradient descent and different types of networks like LSTM neural networks or RNNs.", "We have enjoyed exploring this fascinating science and hope that you have too.", "All non-referenced images were created by the authors.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Favorite sport: Rubik\u2019s Cube, Passions: Finance and Machine Learning, Best Known For: being within the 0.11% of population that uses a Blackberry."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fa404b4119681&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword2vec-out-of-the-black-box-a404b4119681&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword2vec-out-of-the-black-box-a404b4119681&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword2vec-out-of-the-black-box-a404b4119681&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword2vec-out-of-the-black-box-a404b4119681&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----a404b4119681--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a404b4119681--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@jlj314159?source=post_page-----a404b4119681--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@jlj314159?source=post_page-----a404b4119681--------------------------------", "anchor_text": "Jenna Jones"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F21a461c0b583&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword2vec-out-of-the-black-box-a404b4119681&user=Jenna+Jones&userId=21a461c0b583&source=post_page-21a461c0b583----a404b4119681---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa404b4119681&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword2vec-out-of-the-black-box-a404b4119681&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa404b4119681&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword2vec-out-of-the-black-box-a404b4119681&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/tagged/getting-started", "anchor_text": "Getting Started"}, {"url": "https://medium.com/u/98ef37157d28?source=post_page-----a404b4119681--------------------------------", "anchor_text": "Nick Paine"}, {"url": "https://unsplash.com/@urielsc26?utm_source=medium&utm_medium=referral", "anchor_text": "Uriel SC"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://arxiv.org/abs/1301.3781", "anchor_text": "arXiv:1301.3781"}, {"url": "http://www.mccormickml.com/", "anchor_text": "http://www.mccormickml.com"}, {"url": "https://medium.com/tag/nlp?source=post_page-----a404b4119681---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----a404b4119681---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----a404b4119681---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/editors-pick?source=post_page-----a404b4119681---------------editors_pick-----------------", "anchor_text": "Editors Pick"}, {"url": "https://medium.com/tag/getting-started?source=post_page-----a404b4119681---------------getting_started-----------------", "anchor_text": "Getting Started"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa404b4119681&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword2vec-out-of-the-black-box-a404b4119681&user=Jenna+Jones&userId=21a461c0b583&source=-----a404b4119681---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa404b4119681&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword2vec-out-of-the-black-box-a404b4119681&user=Jenna+Jones&userId=21a461c0b583&source=-----a404b4119681---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa404b4119681&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword2vec-out-of-the-black-box-a404b4119681&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a404b4119681--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fa404b4119681&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword2vec-out-of-the-black-box-a404b4119681&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----a404b4119681---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----a404b4119681--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----a404b4119681--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----a404b4119681--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----a404b4119681--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----a404b4119681--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----a404b4119681--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----a404b4119681--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----a404b4119681--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@jlj314159?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@jlj314159?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Jenna Jones"}, {"url": "https://medium.com/@jlj314159/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "96 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F21a461c0b583&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword2vec-out-of-the-black-box-a404b4119681&user=Jenna+Jones&userId=21a461c0b583&source=post_page-21a461c0b583--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fb467325baa14&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword2vec-out-of-the-black-box-a404b4119681&newsletterV3=21a461c0b583&newsletterV3Id=b467325baa14&user=Jenna+Jones&userId=21a461c0b583&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}