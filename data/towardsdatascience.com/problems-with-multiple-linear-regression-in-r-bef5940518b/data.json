{"url": "https://towardsdatascience.com/problems-with-multiple-linear-regression-in-r-bef5940518b", "time": 1683006160.345119, "path": "towardsdatascience.com/problems-with-multiple-linear-regression-in-r-bef5940518b/", "webpage": {"metadata": {"title": "Problems with Multiple Linear Regression, in R | by Flaviu Vadan | Towards Data Science", "h1": "Problems with Multiple Linear Regression, in R", "description": "Linear regression is a popular, old, and thoroughly developed method for estimating the relationship between a measured outcome and one or more explanatory (independent) variables. For instance\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.census.gov/data.html", "anchor_text": "US Census Bureau", "paragraph_index": 12}, {"url": "https://en.wikipedia.org/wiki/Stepwise_regression", "anchor_text": "stepwise regression", "paragraph_index": 16}, {"url": "https://en.wikipedia.org/wiki/Multicollinearity", "anchor_text": "multicollinearity", "paragraph_index": 22}, {"url": "https://en.wikipedia.org/wiki/Invertible_matrix", "anchor_text": "inverse of a matrix", "paragraph_index": 23}], "all_paragraphs": ["Linear regression is a popular, old, and thoroughly developed method for estimating the relationship between a measured outcome and one or more explanatory (independent) variables. For instance, linear regression can help us build a model that represents the relationship between heart rate (measured outcome), body weight (first predictor), and smoking status (second predictor). In this blog, we will see how parameter estimation is performed, explore how to perform multiple linear regression using a dataset created based on data from the US Census Bureau, and discuss some problems that arise as a consequence of removing \u201cbad predictors\u201d as we attempt to simplify our model.", "The typical way a linear model is represented is the potentially familiar:", "Here, y represents the outcome of a measurement estimated by a line with slope m and intercept b. We can rearrange the equation to have:", "and we can further change the variables to be represented as betas:", "which represents the typical way a linear regression model is represented as. Notice that we are using y_hat to denote an estimate, not an observation. We can extend this model to include more than one predictor variable:", "where x_1, x_2, \u2026, x_p are the predictors (there are p of them). There is a notion from linear algebra that can be invoked in this instance \u2014 linear combinations! Notice that the betas, and the predictors x_i (i is the index of the predictor) can be represented as individual vectors, giving us a general matrix form for the model:", "Imagine we have N outcomes and we want to find the relationship between the outcome and a single predictor variable. In addition to N outcomes, we will have N observations of a single predictor. Further, imagine that both the outcomes and the observations are stored in matrices. Since the outcome is a single number and there are N of them, we will have an N x 1 matrix representing the outcomes \u2014 Y (a vector in this case). In addition, the observations will be stored in an N x (p + 1) matrix, where p is the number of predictors (one in our case). We add a column of 1s to the observations matrix as it will help us estimate the parameter that corresponds to the intercept of the model \u2014 the matrix X. Notice that we have added an error term \u2014 epsilon \u2014 that represents the difference between the prediction (Y_hat) and the actual observation (Y).", "The term \u03b2 is a (p + 1) x 1 vector containing the parameters/coefficients of the linear model. The additional term, \u03b5, is an n x 1 vector that represents the errors of the measurements.", "Similar to most, if not all, Statistics tools, linear regression has several assumptions that have to be satisfied in order to model a problem using its principles:", "When fitting a model, the aim is to minimize the difference between a measured observation and the predicted value of that observation. In linear regression, we are typically attempting to minimize the mean squared error \u2014 the mean of the summed squared differences between independent observations and their predictions:", "We can minimize the MSE by taking the gradient with respect to beta (parameters) and setting it equal to 0 to get a formulation for beta:", "With awareness of how \u03b2 is derived, we can start the exercise.", "We will be using a dataset created by aggregating different types of metrics collected by the US Census Bureau. We are going to try and predict life expectancy in years based on 7 predictors \u2014 population estimate, illiteracy (population percentage), murder and non-negligent manslaughter rate per 100k members of the population, percent high-school graduates, mean number of days with temperature < 32 degrees Fahrenheit, and land area in square miles grouped by state. Note that the dataset is from ~1975, is not representative of current trends, and it is exclusively used for the purpose of exercising how to create a linear model:", "R is a great tool, among many (Python is also great), for statistics, so we are going to take advantage of it here. To get started, we can create a simple regression model and inspect the significance of each predictor variable:", "The syntax is interesting, so let\u2019s go through it:", "We get the following summary (only displaying coefficients\u2019 significance):", "When a model is created, R performs significance testing for us and reports the p-values associated with the respective tests of each predictor. If we assume a p-value cutoff of 0.01, we notice that most predictors are\u2026 useless, given the other predictors included in the model. In this case, we can perform something akin to manual dimensionality reduction by creating a model that uses only a subset of the predictors (stepwise regression). We can drop predictors in descending p-value order, from most useless, to least useless:", "I highly recommend performing a summary call after each model update \u2014 the significance test of each coefficient estimate is performed again after one of the features is dropped, which influences the resulting p-values, which can determine whether we continue removing features or not. I have skipped it here in the interest of saving space. A final summary of the model gives us:", "We managed to reduce the number of features to only 3! However, there are problems with this approach.", "Performing backwards elimination of variables, similar to how we did in this exercise, only helps us simplify our model for computation purposes and, potentially, improve performance as measured by metrics such as the sum of squares of residuals. A variable that is eliminated from the model does not suggest the variable is not significant in real life. For example, we have eliminated income, which is possibly a \u201csignificant\u201d factor in a person\u2019s life expectancy. The world is very complex, and a simple model, such as the one we created, has several drawbacks:", "However, note that adding an insignificant variable will always increase the R\u00b2 value and decrease MSE. Albeit insignificant, the addition of the variable can still explain a small percentage of the variation in the response variable, which causes R\u00b2 to be higher and MSE to be lower;", "3. As already alluded to, models such as this one can be over-simplifications of the real world. There are many factors that can influence a person\u2019s life overall and, therefore, expectancy. We have to be mindful of those factors and always interpret these models with skepticism. This pertains specifically to this model as it attempt to model a factor that represents people\u2019s livelihoods. However, this can be extended to any general model we build; be it modelling the climate, yield of chemicals in a manufacturing process, etc. In addition, the principle of skepticism applies to any model architecture, not only regression.", "Recall how we mentioned linear combinations at the beginning \u2014 they play a role in multicollinearity as well. When a dataset showcases multicollinearity, one, or more, of the measured features can be expressed in terms of the other ones in the same dataset. This means that information about a feature (a column vector) is encoded by other features. Technically, the matrix does not have full rank, which means not all columns are linearly independent. The consequence of this is numerical instability and potentially inflated coefficients \u2014 that is, \u03b2! Let us focus on the inverse of the matrix obtained by multiplying the transpose of X with X itself.", "Finding the inverse of a matrix A involves computing the determinant of the matrix. The inverse of the determinant is then multiplied by another term to obtain the inverse. Notice I mentioned \u201cthe inverse of the determinant\u201d; that is, 1/determinant(A). When a matrix is not full rank, the determinants will, generally, be a value much smaller than 1, resulting in the inverse of the determinant being a huge value. This is the element of finding the inverse that ends up inflating the coefficients! In R, we can check whether the determinant is smaller than 1 by writing out the matrix multiplication ourselves. Given the dataset we used in the exercise, we can write:", "It turns out that for the dataset we have used in the example, the determinant is approximately 3e+41, so we get TRUE as the output! It is an important element to check when performing multiple linear regression as it not only helps better understand the dataset, but it also suggests that a step back should be taken in order to: (1) better understand the data; (2) potentially collect more data; (3) or perform dimensionality reduction using principle component analysis or Ridge regression.", "Linear regression is, still, a very popular method for modelling. Like with any Statistics tool, care should be taken to: (1) understand data in order to avoid spurious parameter estimations; (2) develop awareness of how the parameter estimates are performed in order to be able to diagnose potential problems before they occur; (3) explain why a coefficient is significant, whereas another may not be, and how this reflects something about the world phenomenon we are attempting to model. Here\u2019s the final code sample:", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fbef5940518b&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fproblems-with-multiple-linear-regression-in-r-bef5940518b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fproblems-with-multiple-linear-regression-in-r-bef5940518b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fproblems-with-multiple-linear-regression-in-r-bef5940518b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fproblems-with-multiple-linear-regression-in-r-bef5940518b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----bef5940518b--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----bef5940518b--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@flaviuvadan?source=post_page-----bef5940518b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@flaviuvadan?source=post_page-----bef5940518b--------------------------------", "anchor_text": "Flaviu Vadan"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F950c0cb06c7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fproblems-with-multiple-linear-regression-in-r-bef5940518b&user=Flaviu+Vadan&userId=950c0cb06c7&source=post_page-950c0cb06c7----bef5940518b---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbef5940518b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fproblems-with-multiple-linear-regression-in-r-bef5940518b&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbef5940518b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fproblems-with-multiple-linear-regression-in-r-bef5940518b&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@atharva_tulsi", "anchor_text": "Atharva Tulsi"}, {"url": "https://unsplash.com/photos/tpRGKt2IdHY", "anchor_text": "Unsplash"}, {"url": "https://www.census.gov/data.html", "anchor_text": "US Census Bureau"}, {"url": "https://en.wikipedia.org/wiki/Stepwise_regression", "anchor_text": "stepwise regression"}, {"url": "https://en.wikipedia.org/wiki/Multicollinearity", "anchor_text": "multicollinearity"}, {"url": "https://en.wikipedia.org/wiki/Invertible_matrix", "anchor_text": "inverse of a matrix"}, {"url": "https://medium.com/tag/linear-regression?source=post_page-----bef5940518b---------------linear_regression-----------------", "anchor_text": "Linear Regression"}, {"url": "https://medium.com/tag/statistics?source=post_page-----bef5940518b---------------statistics-----------------", "anchor_text": "Statistics"}, {"url": "https://medium.com/tag/gradient?source=post_page-----bef5940518b---------------gradient-----------------", "anchor_text": "Gradient"}, {"url": "https://medium.com/tag/r?source=post_page-----bef5940518b---------------r-----------------", "anchor_text": "R"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbef5940518b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fproblems-with-multiple-linear-regression-in-r-bef5940518b&user=Flaviu+Vadan&userId=950c0cb06c7&source=-----bef5940518b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbef5940518b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fproblems-with-multiple-linear-regression-in-r-bef5940518b&user=Flaviu+Vadan&userId=950c0cb06c7&source=-----bef5940518b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbef5940518b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fproblems-with-multiple-linear-regression-in-r-bef5940518b&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----bef5940518b--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fbef5940518b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fproblems-with-multiple-linear-regression-in-r-bef5940518b&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----bef5940518b---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----bef5940518b--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----bef5940518b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----bef5940518b--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----bef5940518b--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----bef5940518b--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----bef5940518b--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----bef5940518b--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----bef5940518b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@flaviuvadan?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@flaviuvadan?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Flaviu Vadan"}, {"url": "https://medium.com/@flaviuvadan/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "30 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F950c0cb06c7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fproblems-with-multiple-linear-regression-in-r-bef5940518b&user=Flaviu+Vadan&userId=950c0cb06c7&source=post_page-950c0cb06c7--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F950c0cb06c7%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fproblems-with-multiple-linear-regression-in-r-bef5940518b&user=Flaviu+Vadan&userId=950c0cb06c7&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}