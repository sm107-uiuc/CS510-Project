{"url": "https://towardsdatascience.com/spectral-encoding-of-categorical-features-b4faebdf4a", "time": 1682996708.809202, "path": "towardsdatascience.com/spectral-encoding-of-categorical-features-b4faebdf4a/", "webpage": {"metadata": {"title": "Spectral encoding of categorical features | by Michael Larionov, PhD | Towards Data Science", "h1": "Spectral encoding of categorical features", "description": "About a year ago I was working on a regression model, which had over a million features. Needless to say, the training was super slow, and the model was overfitting a lot. After investigating this\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.kaggle.com/residentmario/iowa-liquor-sales/downloads/iowa-liquor-sales.zip/1", "anchor_text": "liquor sales data set", "paragraph_index": 23}, {"url": "https://amethix.com/entropy-in-machine-learning/", "anchor_text": "this blog", "paragraph_index": 27}, {"url": "https://github.com/mlarionov/machine_learning_POC/blob/master/spectral-analysis/spectral-encoding-of-categorical-features.ipynb", "anchor_text": "my github repository", "paragraph_index": 33}], "all_paragraphs": ["About a year ago I was working on a regression model, which had over a million features. Needless to say, the training was super slow, and the model was overfitting a lot. After investigating this issue, I realized that most of the features were created using 1-hot encoding of the categorical features, and some of them had tens of thousands of unique values.", "The problem of mapping categorical features to lower-dimensional space is not new. Recently one of the popular way to deal with it is using entity embedding layers of a neural network. However that method assumes that neural networks are used. What if we decided to use tree-based algorithms instead? In tis case we can use Spectral Graph Theory methods to create low dimensional embedding of the categorical features.", "The idea came from spectral word embedding, spectral clustering and spectral dimensionality reduction algorithms. If you can define a similarity measure between different values of the categorical features, we can use spectral analysis methods to find the low dimensional representation of the categorical feature.", "From the similarity function (or kernel function) we can construct an Adjacency matrix, which is a symmetric matrix, where the ij element is the value of the kernel function between category values i and j:", "It is very important that I only need a Kernel function, not a high-dimensional representation. This means that 1-hot encoding step is not necessary here. Also for the kernel-base machine learning methods, the categorical variable encoding step is not necessary as well, because what matters is the kernel function between two points, which can be constructed using the individual kernel functions.", "Once the adjacency matrix is constructed, we can construct a degree matrix:", "Here \u03b4 is the Kronecker delta symbol. The Laplacian matrix is the difference between the two:", "And the normalize Laplacian matrix is defined as:", "Following the Spectral Graph theory, we proceed with eigendecomposition of the normalized Laplacian matrix. The number of zero eigenvalues correspond to the number of connected components. In our case, let\u2019s assume that our categorical feature has two sets of values that are completely dissimilar. This means that the kernel function K(i,j) is zero if i and j belong to different groups. In this case we will have two zero eigenvalues of the normalized Laplacian matrix.", "If there is only one connected component, we will have only one zero eigenvalue. Normally it is uninformative and is dropped to prevent multicollinearity of features. However we can keep it if we are planning to use tree-based models.", "The lower eigenvalues correspond to \u201csmooth\u201d eigenvectors (or modes), that are following the similarity function more closely. We want to keep only these eigenvectors and drop the eigenvectors with higher eigenvalues, because they are more likely represent noise. It is very common to look for a gap in the matrix spectrum and pick the eigenvalues below the gap. The resulting truncated eigenvectors can be normalized and represent embeddings of the categorical feature values.", "As an example, let\u2019s consider the Day of Week. 1-hot encoding assumes every day is similar to any other day (K(i,j)=1 ). This is not a likely assumption, because we know that days of the week are different. For example, the bar attendance spikes on Fridays and Saturdays (at least in USA) because the following day is a weekend. Label encoding is also incorrect, because it will make the \u201cdistance\u201d between Monday and Wednesday twice higher than between Monday and Tuesday. And the \u201cdistance\u201d between Sunday and Monday will be six times higher, even though the days are next to each other. By the way, the label encoding corresponds to the kernel K(i,j)=exp(\u2212\u03b3|i\u2212j|)", "We will consider an example, where weekdays are similar to each other, but differ a lot from the weekends.", "Notice, that the eigenvalues are not ordered here. Let\u2019s plot the eigenvalues, ignoring the uninformative zero.", "We can see a pretty substantial gap between the first eigenvalue and the rest of the eigenvalues. If this does not give enough model performance, you can include the second eigenvalue, because the gap between it and the higher eigenvalues is also quite substantial.", "Look at the second eigenvector. The weekend values have a different size than the weekdays and Friday is close to zero. This proves the transitional role of Friday, that, being a day of the week, is also the beginning of the weekend.", "If we are going to pick two lowest non-zero eigenvalues, our categorical feature encoding will result in these category vectors:", "In the plot above we see that Monday and Tuesday, and also Saturday and Sunday are clustered close together, while Wednesday, Thursday and Friday are far apart.", "In the previous example we assumed that the similarity function is given. Sometimes this is the case, where it can be defined based on the business rules. However it may be possible to learn it from data.", "One of the ways to compute the Kernel is using Kullback-Leibler Divergence:", "Where D is Symmetrised KL divergence:", "Here pi is a probability of the data given the category value i:", "The idea is to estimate the data distribution (including the target variable, but excluding the categorical variable) for each value of the categorical variable. If for two values the distributions are similar, then the divergence will be small and the similarity value will be large. Note that \u03b3 is a hyperparameter and will have to be tuned", "To try this approach will will use liquor sales data set. To keep the file small I removed some columns and aggregated the data.", "Since we care about sales, let\u2019s encode the day of week using the information from the sales column Let\u2019s check the histogram first:", "We see that the distribution is very skewed, so let\u2019s try to use log of sales columns instead", "This is much better. So we will use a log for our distribution", "Here we will follow this blog for computation of the Kullback-Leibler divergence. Also note, that since there are no liquor sales on Sunday, we consider only six days in a week", "As we already mentioned, the hyperparameter \u03b3 has to be tuned. Here we just pick the value that will give a plausible result", "Ignoring the zero eigenvalue, we can see that there is a bigger gap between the first eigenvalue and the rest of the eigenvalues, even though the values are all in the range between 1 and 1.3.", "Ultimately the number of eigenvectors to use is another hyperparameter, that should be optimized on a supervised learning task. The Category field is another candidate to do spectral analysis, and is, probably, a better choice since it has more unique values", "We can see, that a lot of eigenvalues are grouped around the 1.1 mark. The eigenvalues that are below that cluster can be used for encoding the Category feature. Please also note that this method is highly sensitive on selection of hyperparameter \u03b3 . For illustration let me pick a higher and a lower gamma", "We presented a way to encode the categorical features as a low dimensional vector that preserves most of the feature similarity information. For this we use methods of Spectral analysis on the values of the categorical feature. In order to find the kernel function we can either use heuristics, or learn it using a variety of methods, for example, using Kullback\u2013Leibler divergence of the data distribution conditional on the category value. To select the subset of the eigenvectors we used gap analysis, but what we really need is to validate this methods by analyzing a variety of data sets and both classification and regression problems. We also need to compare it with other encoding methods, for example, entity embedding using Neural Networks. The kernel function we used can also include the information about category frequency, which will help us deal with high information, but low frequency values.", "Update 07/04/2019: A better way to calculate the similarity function is to use Wasserstein distance instead of symmetric Kullback\u2013Leibler divergence. The updated code can be found in my github repository.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fb4faebdf4a&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspectral-encoding-of-categorical-features-b4faebdf4a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspectral-encoding-of-categorical-features-b4faebdf4a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspectral-encoding-of-categorical-features-b4faebdf4a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspectral-encoding-of-categorical-features-b4faebdf4a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://me.dm/@michaellarionov", "anchor_text": "Mastodon"}, {"url": "https://towardsdatascience.com/?source=post_page-----b4faebdf4a--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b4faebdf4a--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://michaellarionov.medium.com/?source=post_page-----b4faebdf4a--------------------------------", "anchor_text": ""}, {"url": "https://michaellarionov.medium.com/?source=post_page-----b4faebdf4a--------------------------------", "anchor_text": "Michael Larionov, PhD"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff9158ca11a43&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspectral-encoding-of-categorical-features-b4faebdf4a&user=Michael+Larionov%2C+PhD&userId=f9158ca11a43&source=post_page-f9158ca11a43----b4faebdf4a---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb4faebdf4a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspectral-encoding-of-categorical-features-b4faebdf4a&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb4faebdf4a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspectral-encoding-of-categorical-features-b4faebdf4a&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@kellysikkema?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Kelly Sikkema"}, {"url": "https://unsplash.com/search/photos/list?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://www.kaggle.com/residentmario/iowa-liquor-sales/downloads/iowa-liquor-sales.zip/1", "anchor_text": "liquor sales data set"}, {"url": "https://amethix.com/entropy-in-machine-learning/", "anchor_text": "this blog"}, {"url": "https://github.com/mlarionov/machine_learning_POC/blob/master/spectral-analysis/spectral-encoding-of-categorical-features.ipynb", "anchor_text": "my github repository"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----b4faebdf4a---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/graph-theory?source=post_page-----b4faebdf4a---------------graph_theory-----------------", "anchor_text": "Graph Theory"}, {"url": "https://medium.com/tag/feature-engineering?source=post_page-----b4faebdf4a---------------feature_engineering-----------------", "anchor_text": "Feature Engineering"}, {"url": "https://medium.com/tag/data-science?source=post_page-----b4faebdf4a---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/kernel-trick?source=post_page-----b4faebdf4a---------------kernel_trick-----------------", "anchor_text": "Kernel Trick"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb4faebdf4a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspectral-encoding-of-categorical-features-b4faebdf4a&user=Michael+Larionov%2C+PhD&userId=f9158ca11a43&source=-----b4faebdf4a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb4faebdf4a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspectral-encoding-of-categorical-features-b4faebdf4a&user=Michael+Larionov%2C+PhD&userId=f9158ca11a43&source=-----b4faebdf4a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb4faebdf4a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspectral-encoding-of-categorical-features-b4faebdf4a&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b4faebdf4a--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fb4faebdf4a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspectral-encoding-of-categorical-features-b4faebdf4a&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----b4faebdf4a---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----b4faebdf4a--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----b4faebdf4a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----b4faebdf4a--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----b4faebdf4a--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----b4faebdf4a--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----b4faebdf4a--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----b4faebdf4a--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----b4faebdf4a--------------------------------", "anchor_text": ""}, {"url": "https://michaellarionov.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://michaellarionov.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Michael Larionov, PhD"}, {"url": "https://michaellarionov.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "611 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff9158ca11a43&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspectral-encoding-of-categorical-features-b4faebdf4a&user=Michael+Larionov%2C+PhD&userId=f9158ca11a43&source=post_page-f9158ca11a43--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fcf000f0c5fdd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspectral-encoding-of-categorical-features-b4faebdf4a&newsletterV3=f9158ca11a43&newsletterV3Id=cf000f0c5fdd&user=Michael+Larionov%2C+PhD&userId=f9158ca11a43&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}