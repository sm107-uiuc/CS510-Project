{"url": "https://towardsdatascience.com/want-to-cluster-text-try-custom-word-embeddings-615526cbef7a", "time": 1682994224.2159202, "path": "towardsdatascience.com/want-to-cluster-text-try-custom-word-embeddings-615526cbef7a/", "webpage": {"metadata": {"title": "Want to Cluster Text? Try Custom Word-Embeddings! | by Ashok Chilakapati | Towards Data Science", "h1": "Want to Cluster Text? Try Custom Word-Embeddings!", "description": "That is welcome news after our ho-hum results for text classification when using word-embeddings. In the context of classification we concluded that keeping it simple with naive bayes and tf-idf\u2026"}, "outgoing_paragraph_urls": [{"url": "http://xplordat.com/2018/10/09/word-embeddings-and-document-vectors-part-2-classification/", "anchor_text": "ho-hum results for text classification", "paragraph_index": 0}, {"url": "http://xplordat.com/2018/11/05/want-clusters-how-many-will-you-have/", "anchor_text": "Want Clusters? How Many Will You Have?", "paragraph_index": 0}, {"url": "http://xplordat.com/2018/11/26/clustering-text-with-transformed-document-vectors/", "anchor_text": "Clustering Text with Transformed Document Vectors.", "paragraph_index": 0}, {"url": "http://scikit-learn.org/stable/datasets/twenty_newsgroups.html", "anchor_text": "20-news", "paragraph_index": 0}, {"url": "http://ai.stanford.edu/~amaas/data/sentiment/", "anchor_text": "movie reviews", "paragraph_index": 0}, {"url": "http://xplordat.com/2018/10/09/word-embeddings-and-document-vectors-part-2-classification/", "anchor_text": "classification exercise", "paragraph_index": 0}, {"url": "https://github.com/ashokc/Evaluating-Document-Transformations-for-Clustering-Text", "anchor_text": "github", "paragraph_index": 0}, {"url": "https://pdfs.semanticscholar.org/4008/d78a584102086f2641bcb0dab51aff0d353b.pdf", "anchor_text": "VSM", "paragraph_index": 3}, {"url": "http://xplordat.com/2018/10/09/word-embeddings-and-document-vectors-part-2-classification/", "anchor_text": "Word Embeddings and Document Vectors: Part 2. Classification", "paragraph_index": 5}, {"url": "http://xplordat.com/2018/10/09/word-embeddings-and-document-vectors-part-2-classification/", "anchor_text": "classification", "paragraph_index": 6}, {"url": "https://github.com/ashokc/Word-Embeddings-and-Document-Vectors", "anchor_text": "github", "paragraph_index": 6}, {"url": "http://xplordat.com/2018/11/26/clustering-text-with-transformed-document-vectors/", "anchor_text": "Clustering Text with Transformed Document Vectors", "paragraph_index": 9}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html", "anchor_text": "tf-idf vectorizer", "paragraph_index": 11}, {"url": "http://scikit-learn.org/stable/datasets/twenty_newsgroups.html", "anchor_text": "20-news", "paragraph_index": 12}, {"url": "http://ai.stanford.edu/~amaas/data/sentiment/", "anchor_text": "movie reviews", "paragraph_index": 12}, {"url": "http://xplordat.com/2018/09/27/word-embeddings-and-document-vectors-part-1-similarity/", "anchor_text": "xplordat.com", "paragraph_index": 23}], "all_paragraphs": ["That is welcome news after our ho-hum results for text classification when using word-embeddings. In the context of classification we concluded that keeping it simple with naive bayes and tf-idf vectors is a great starting point. While we cannot generalize, all that extra work put into constructing document+word vectors did not yield commensurate gains for classification quality. In this post we pick up on the foundations we laid earlier for clustering text, specifically the two posts \u2014 Want Clusters? How Many Will You Have? and Clustering Text with Transformed Document Vectors. We choose the 20-news and the movie reviews text corpus we are familiar with from the classification exercise. We evaluate the different order reducing transformations for their clustering effectiveness. The code to reproduce these results can be downloaded from github.", "Why should order reducing transformations help with clustering?", "Before we plunge into the details, let us stop and consider whether there is any reason to expect some benefit from applying word-embeddings. Well, besides the obvious benefit of compute performance which is of course useless if we do not get good quality clusters!", "The document vectors arising from the VSM model are long and sparse. The centroid of every document cluster would unfortunately be close to the origin. That is, all document clusters pretty much have a common centroid! Plus, since we normalize the document vectors to have unit length, every document in a cluster would be at about the same distance from this common centroid. We have a situation where the intercluster distances are close to zero and intracluster distances are close to unity! Not a good situation to work with for demarcating the clusters.", "If we can reduce the order of the document vectors without destroying their content/meaning in the process, we can certainly expect better quality results. That precisely is the point of this post and here is the plan.", "The prep work for building document vectors from the text corpus with/without word-embeddings is already done in the earlier post \u2014 Word Embeddings and Document Vectors: Part 2. Classification. We have the tokenized 20-news and movie-reviews text corpus in an elasticsearch index. The pre-trained and custom word-vectors built from different algorithms (FastText, Word2Vec, and Glove) are in the index as well. Applying K-Means to the reduced order document vectors is straightforward.", "The following diagram illustrates the mechanics. The set up is similar to the one we had for the classification except that we are clustering the vectors here. The earlier article on classification has a detailed description of each of the steps in the diagram and the code is at its github as well.", "To illustrate the typical effect of an order reducing transformation on intra & intercluster distances let us pick the \u2018alt.atheism\u2019 group from the 20-news corpus and compute the following.", "The box-whisker plots (whiskers at 5 & 95% percentile) in Figure 2 show the distribution of distances before & after the transformation. With the sparse and long (n = 44870, the size of the stopped vocabulary for 20-news) raw document vectors, the distances within the cluster are all around 1.0 as expected, and the intercluster distances are close to 0.1. With the custom fasttext word-embeddings (with p = 300, i.e the transformed vectors have a length of 300) we get a favorable distribution of distances where the cluster itself got crunched (median intracluster distance decreased to 0.27 from 1.0) while different clusters got pushed apart (median distance from other clusters increased to 0.28 from 0.1 or so).", "We know from the previous article Clustering Text with Transformed Document Vectors that,", "Further we have outlined the following procedure for computing this ratio.", "For better clusterability we want B/A to be large. That is, the transformation that maximizes B/A should do better at clustering. Figure 3 below gives us a glimpse of what at to expect. The documents in all cases have been vectorized with Scikit\u2019s tf-idf vectorizer using stopped tokens (but no stemming). Different order reducing transformations are then applied to these vectors. We choose p as 300, i.e. the transformed document vectors have a length of 300 in all cases. The attempted transformations include the following.", "The document vectors obtained in each transformation are processed as per the steps 1\u20134, to compute the intercluster/intracluster distance ratio B/A. Figure 3A shows this ratio for 20-news document vectors considering all the 20 groups/clusters. Figure 3B is for the movie reviews dataset.", "The main takeaway from Figure 3 is that order reducing transformations and in particular the custom word-embeddings seem to yield large values for B/A and hence can help improve clusterability. Time to verify if this indeed is the case.", "Now we get down to the actual clustering task outlined in Figure 1 in order to verify what Figure 3 is indicating. We will employ the same list of transformations and check how pure the obtained clusters are. That is, in any obtained cluster we hope to find articles of a single group/class. Our position is that those transformations with larger values for B/A should yield purer clusters.", "We give the known number of clusters to the K-Means simulation in every case. Given that we are helping K-Means here by giving the exact K, we hope that it would at least be able separate out the articles by group and place them in distinct clusters. But that does not always happen unfortunately and in some cases the articles get lopsidedly bunched up in just one or two clusters. That is a failed clustering exercise from our view point.", "If the clustering task is successful at all, we should end up with clusters each having a dominant fraction of articles from just one of the groups so we can identify that cluster with that group.", "The \u2018purity\u2019 of an obtained cluster is then measured by number of articles that do not belong in that cluster/group.", "In the case of 20-news dataset we pick articles from 3 groups for this exercise \u2014 just so we can plot the results easily. Note however that the results in Figure 3A are obtained by considering all 20 groups/clusters. A word-cloud plot gives a bird\u2019s eye-view of the content we are dealing with.", "Figure 5A below shows that fewest number of articles were misplaced overall when using custom word-embeddings, whereas the raw document vectors fared the worst. Document vectors with custom word2vec embeddings yield the most pure clusters, each identifying with a specific group. The performance of different transformations is in good match with what Figure 3A has indicated.", "The movie-reviews dataset has only 2 classes so easy enough to plot up and we consider all the documents. Here is a word-cloud of the two groups once again.", "Figure 7A identifies only two successful transformations and they coincide with the two that have the largest B/A ratio in Figure 3B. Clearly as the word-clouds in Figure 6 have indicated, the movie reviews are more difficult to cluster. Overall, compared to 20-news we can make the following observations.", "With that we wrap up another post. We have shown here that order reducing transformations can help with clustering documents. Within those transformations, the custom word-embeddings seem to have an edge \u2014 hence it made its way into the title. As to why custom word-embeddings have done better than other other order reducing transformations and if that conclusion has legs beyond the text repositories studied here is open to further study.", "Originally published at xplordat.com on December 14, 2018.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F615526cbef7a&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwant-to-cluster-text-try-custom-word-embeddings-615526cbef7a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwant-to-cluster-text-try-custom-word-embeddings-615526cbef7a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwant-to-cluster-text-try-custom-word-embeddings-615526cbef7a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwant-to-cluster-text-try-custom-word-embeddings-615526cbef7a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----615526cbef7a--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----615526cbef7a--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@ashok.chilakapati?source=post_page-----615526cbef7a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ashok.chilakapati?source=post_page-----615526cbef7a--------------------------------", "anchor_text": "Ashok Chilakapati"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcc37b40eae29&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwant-to-cluster-text-try-custom-word-embeddings-615526cbef7a&user=Ashok+Chilakapati&userId=cc37b40eae29&source=post_page-cc37b40eae29----615526cbef7a---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F615526cbef7a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwant-to-cluster-text-try-custom-word-embeddings-615526cbef7a&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F615526cbef7a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwant-to-cluster-text-try-custom-word-embeddings-615526cbef7a&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "http://xplordat.com/2018/10/09/word-embeddings-and-document-vectors-part-2-classification/", "anchor_text": "ho-hum results for text classification"}, {"url": "http://xplordat.com/2018/11/05/want-clusters-how-many-will-you-have/", "anchor_text": "Want Clusters? How Many Will You Have?"}, {"url": "http://xplordat.com/2018/11/26/clustering-text-with-transformed-document-vectors/", "anchor_text": "Clustering Text with Transformed Document Vectors."}, {"url": "http://scikit-learn.org/stable/datasets/twenty_newsgroups.html", "anchor_text": "20-news"}, {"url": "http://ai.stanford.edu/~amaas/data/sentiment/", "anchor_text": "movie reviews"}, {"url": "http://xplordat.com/2018/10/09/word-embeddings-and-document-vectors-part-2-classification/", "anchor_text": "classification exercise"}, {"url": "https://github.com/ashokc/Evaluating-Document-Transformations-for-Clustering-Text", "anchor_text": "github"}, {"url": "https://pdfs.semanticscholar.org/4008/d78a584102086f2641bcb0dab51aff0d353b.pdf", "anchor_text": "VSM"}, {"url": "http://xplordat.com/2018/10/09/word-embeddings-and-document-vectors-part-2-classification/", "anchor_text": "Word Embeddings and Document Vectors: Part 2. Classification"}, {"url": "http://xplordat.com/2018/10/09/word-embeddings-and-document-vectors-part-2-classification/", "anchor_text": "classification"}, {"url": "https://github.com/ashokc/Word-Embeddings-and-Document-Vectors", "anchor_text": "github"}, {"url": "http://xplordat.com/2018/11/26/clustering-text-with-transformed-document-vectors/", "anchor_text": "Clustering Text with Transformed Document Vectors"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html", "anchor_text": "tf-idf vectorizer"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html", "anchor_text": "Singular value decomposition"}, {"url": "https://github.com/stanfordnlp/GloVe", "anchor_text": "Glove"}, {"url": "http://nlp.stanford.edu/data/wordvecs/glove.840B.300d.zip", "anchor_text": "glove.840B.300d"}, {"url": "https://fasttext.cc/", "anchor_text": "FastText"}, {"url": "https://s3-us-west-1.amazonaws.com/fasttext-vectors/crawl-300d-2M-subword.zip", "anchor_text": "crawl-300d-2M-subword.vec"}, {"url": "https://arxiv.org/abs/1301.3781", "anchor_text": "Word2Vec"}, {"url": "https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing", "anchor_text": "GoogleNews-vectors-negative300.bin"}, {"url": "https://radimrehurek.com/gensim/index.html", "anchor_text": "Gensim"}, {"url": "https://radimrehurek.com/gensim/index.html", "anchor_text": "Gensim"}, {"url": "http://scikit-learn.org/stable/datasets/twenty_newsgroups.html", "anchor_text": "20-news"}, {"url": "http://ai.stanford.edu/~amaas/data/sentiment/", "anchor_text": "movie reviews"}, {"url": "http://xplordat.com/2018/09/27/word-embeddings-and-document-vectors-part-1-similarity/", "anchor_text": "xplordat.com"}, {"url": "https://medium.com/tag/k-means?source=post_page-----615526cbef7a---------------k_means-----------------", "anchor_text": "K Means"}, {"url": "https://medium.com/tag/word-embeddings?source=post_page-----615526cbef7a---------------word_embeddings-----------------", "anchor_text": "Word Embeddings"}, {"url": "https://medium.com/tag/text-analytics?source=post_page-----615526cbef7a---------------text_analytics-----------------", "anchor_text": "Text Analytics"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F615526cbef7a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwant-to-cluster-text-try-custom-word-embeddings-615526cbef7a&user=Ashok+Chilakapati&userId=cc37b40eae29&source=-----615526cbef7a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F615526cbef7a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwant-to-cluster-text-try-custom-word-embeddings-615526cbef7a&user=Ashok+Chilakapati&userId=cc37b40eae29&source=-----615526cbef7a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F615526cbef7a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwant-to-cluster-text-try-custom-word-embeddings-615526cbef7a&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----615526cbef7a--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F615526cbef7a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwant-to-cluster-text-try-custom-word-embeddings-615526cbef7a&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----615526cbef7a---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----615526cbef7a--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----615526cbef7a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----615526cbef7a--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----615526cbef7a--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----615526cbef7a--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----615526cbef7a--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----615526cbef7a--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----615526cbef7a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ashok.chilakapati?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ashok.chilakapati?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ashok Chilakapati"}, {"url": "https://medium.com/@ashok.chilakapati/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "244 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcc37b40eae29&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwant-to-cluster-text-try-custom-word-embeddings-615526cbef7a&user=Ashok+Chilakapati&userId=cc37b40eae29&source=post_page-cc37b40eae29--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F5ab4b71672c9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwant-to-cluster-text-try-custom-word-embeddings-615526cbef7a&newsletterV3=cc37b40eae29&newsletterV3Id=5ab4b71672c9&user=Ashok+Chilakapati&userId=cc37b40eae29&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}