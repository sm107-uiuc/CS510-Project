{"url": "https://towardsdatascience.com/hyperparameter-tuning-with-keras-and-ray-tune-1353e6586fda", "time": 1683013721.82218, "path": "towardsdatascience.com/hyperparameter-tuning-with-keras-and-ray-tune-1353e6586fda/", "webpage": {"metadata": {"title": "Hyperparameter tuning with Keras and Ray Tune | by Himanshu Rawlani | Towards Data Science", "h1": "Hyperparameter tuning with Keras and Ray Tune", "description": "A practical tutorial on using HyperOpt's Bayesian optimization with HyperBand scheduler to choose the best hyperparameters for machine learning models."}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/implementing-a-fully-convolutional-network-fcn-in-tensorflow-2-3c46fb61de3b", "anchor_text": "article", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/implementing-a-fully-convolutional-network-fcn-in-tensorflow-2-3c46fb61de3b", "anchor_text": "article", "paragraph_index": 2}, {"url": "https://pablormier.github.io/2017/09/05/a-tutorial-on-differential-evolution-with-python/", "anchor_text": "here", "paragraph_index": 10}, {"url": "https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f", "anchor_text": "here", "paragraph_index": 11}, {"url": "https://github.com/himanshurawlani/hyper_fcn", "anchor_text": "this GitHub link", "paragraph_index": 12}, {"url": "https://ray.readthedocs.io/en/latest/tune.html", "anchor_text": "Ray Tune", "paragraph_index": 13}, {"url": "https://github.com/ray-project/ray", "anchor_text": "Ray", "paragraph_index": 13}, {"url": "https://github.com/hyperopt/hyperopt", "anchor_text": "HyperOpt", "paragraph_index": 14}, {"url": "http://ax.dev/", "anchor_text": "Ax", "paragraph_index": 14}, {"url": "https://keras.io/guides/preprocessing_layers/", "anchor_text": "here", "paragraph_index": 18}, {"url": "https://github.com/hyperopt/hyperopt/wiki/FMin#2-defining-a-search-space", "anchor_text": "here", "paragraph_index": 21}, {"url": "https://github.com/hyperopt/hyperopt/wiki/FMin#21-parameter-expressions", "anchor_text": "here", "paragraph_index": 21}, {"url": "https://docs.ray.io/en/latest/tune/api_docs/suggestion.html#hyperopt-tune-suggest-hyperopt-hyperoptsearch", "anchor_text": "HyperOpt", "paragraph_index": 22}, {"url": "https://docs.ray.io/en/latest/tune/api_docs/schedulers.html#asha-tune-schedulers-ashascheduler", "anchor_text": "AsyncSuccessiveHalvingAlgorithm", "paragraph_index": 23}, {"url": "https://docs.ray.io/en/latest/tune/api_docs/schedulers.html#hyperband-tune-schedulers-hyperbandscheduler", "anchor_text": "HyperBand", "paragraph_index": 23}, {"url": "https://github.com/himanshurawlani/hyper_fcn/blob/master/callbacks.py#L8", "anchor_text": "code on GitHub", "paragraph_index": 26}, {"url": "https://docs.ray.io/en/latest/tune/api_docs/trainable.html#tune-function-api", "anchor_text": "function-based", "paragraph_index": 27}, {"url": "https://docs.ray.io/en/latest/tune/api_docs/trainable.html#tune-class-api", "anchor_text": "class-based", "paragraph_index": 27}, {"url": "https://www.tensorflow.org/tensorboard/hyperparameter_tuning_with_hparams", "anchor_text": "TensorBoard HParams", "paragraph_index": 29}, {"url": "https://github.com/himanshurawlani/hyper_fcn", "anchor_text": "GitHub project", "paragraph_index": 34}, {"url": "https://twitter.com/raw_himanshu", "anchor_text": "@raw_himanshu", "paragraph_index": 34}, {"url": "https://www.linkedin.com/in/himanshurawlani/", "anchor_text": "himanshurawlani", "paragraph_index": 34}], "all_paragraphs": ["In my previous article, I had explained how to build a small and nimble image classifier and what are the advantages of having variable input dimensions in a convolutional neural network. However, after going through the model building code and training routine, one can ask questions such as:", "Building or training a neural network involves figuring out the answers to the above questions. You may have an intuition for CNNs, for example, as we go deeper the number of filters in each layer should increase as the neural network learns to extract more and more complex features built on simpler features extracted in the earlier layers. However, there might be a more optimal model (for your dataset) with a lesser number of parameters that might outperform the model that you have designed based on your intuition.", "In this article, I\u2019ll explain what these parameters are and how do they affect the training of a machine learning model. I\u2019ll explain how do machine learning engineers choose these parameters and how can we automate this process using a simple mathematical concept. I\u2019ll be starting with the same model architecture from my previous article and will be modifying it to make most of the training and architectural parameters tunable.", "A hyperparameter is a training parameter set by a machine learning engineer before training the model. These parameters are not learned by the machine learning model during the training process. Examples include batch size, learning rate, number of layers and corresponding units, etc. The parameters that are learned by the machine learning model, from the data, during the training process are called model parameters.", "When training a machine learning model the main goal is to get the best performing model that has the best performance on the validation set. We focus on the validation set as it represents how well the model generalizes (performance on unseen data). Hyperparameters form the premise of the training process. For eg., if the learning rate is set too high then the model might never converge to the minima as it will take too large steps after every iteration. On the other hand, if the learning rate is set too low it will take a long time for the model to reach the minima.", "Why is it difficult to choose hyperparameters?", "Finding the right learning rate involves choosing a value, training a model, evaluating it, and trying again. Every dataset is unique on its own and with so many parameters to choose from, a beginner can easily get confused. Machine learning engineers who have had many failed training attempts eventually develop an intuition of how a hyperparameter affects a given training process. However, that intuition doesn\u2019t generalize to all the datasets and a new use case usually needs some experiments before settling with convincing hyperparameters. And yet, it\u2019s possible to miss out on best or optimal parameters.", "We want to choose the hyperparameters such that, after the training process is complete, we have a model that is both precise and generalized. When dealing with neural networks evaluating the objective function can be very expensive as training takes a long time and trying out different hyperparameters manually may take days. This becomes a difficult task to do by hand.", "Hyperparameter tuning can be considered as a black-box optimization problem where we try to find a minimum of a function f(x) without knowing its analytical form. It is also called derivative-free optimization as we do not know its analytical form and no derivatives can be computed to minimize f(x), and hence techniques like gradient descent cannot be used.", "Few well-known techniques for hyperparameter tuning include grid search, random search, differential evolution, and bayesian optimization. Grid search and random search perform slightly better than manual tuning as we set up a grid of hyperparameters and run training and evaluation cycles on the parameters which are systematically or randomly chosen from the grid respectively.", "However, grid and random search are relatively inefficient as they do not choose the next set of hyperparameters based on previous results. On the other hand, Differential evolution is a type of evolutionary algorithm where the initial set of best performing hyperparameter configurations (which are one of the randomly initialized individuals) are chosen to produce more hyperparameters (offsprings). The new generation of hyperparameters (offsprings) are more likely to perform better as they inherit good traits of their parents and the population improves over time (generation after generation). Read more about this concept in this beautiful and practical tutorial here.", "Though differential evolution works, it takes a long time and still doesn\u2019t quite take informed steps or it isn\u2019t aware of what are we trying to achieve/optimize. Bayesian optimization approach keeps track of the past evaluation results and uses it to create a probabilistic model of the actual training objective function which is to be optimized. This probabilistic model is called a \u201csurrogate\u201d for the objective function which forms a mapping of the hyperparameters to the probability score of how well the objective function will perform. The next set of hyperparameters, to be evaluated, are chosen on the basis of their performance on the surrogate. This makes bayesian optimization efficient as it chooses the next set of hyperparameters in an informed manner. Read more about this concept in this detailed article here. The article explains Tree Parzen Estimators (TPE) surrogate model which will be used internally in our implementation below.", "As always, you can get all the code used in this tutorial in this GitHub link. I would suggest the reader to clone the project and follow the tutorial step by step for better understanding. Note: The code snippets in this article highlight only a part of the actual script, please refer to the GitHub link for complete code.", "Ray Tune is a Python library that accelerates hyperparameter tuning by allowing you to leverage cutting edge optimization algorithms at scale. It is built on Ray designed to remove the friction from scaling and setting up experiment execution.", "Ray Tune integrates seamlessly with experiment management tools such as MLFlow, TensorBoard, weights and biases, etc. and provides a flexible interface for many cutting edge optimization algorithms and libraries such as HyperOpt (implemented below) and Ax.", "A hyper model is a model whose hyperparameters can be optimized, using optimization algorithms, in order to give the best performance on a metric (validation loss in this case). These hyperparameters include a number of layers, number of units in each layer, type of the layer to be used, type of activation function, etc. Let\u2019s create a simple hyper model which performs image classification task.", "In the above model building code, we are passing a config dictionary which contains values like the number of filters, dropout rate, whether to use a specific data augmentation layer, etc. A new config dictionary will be created for every run of hyperparameter tuning. The config corresponding to the best run will be chosen as the best configuration which will include optimal parameters for data augmentation, model, and training routine. An example config is explained in the hyperparameter search space section below.", "Choosing a data augmentation is very sensitive to the nature of the application for which the model is being trained for. A face recognition system might come across faces in different brightness, orientation, partially cropped, etc. However, an OCR based text extraction system used to extract text from a system-generated PDF will most certainly come across a text with very little orientation and brightness variations.", "If we have huge datasets, it might not be possible to go through each and every image to decide on the data augmentations to be used. We can leave this task, as one of the hyperparameters, for our optimization algorithm. In TensorFlow 2, it becomes easier than ever to add data augmentation using Keras preprocessing layers as a part of our model code. These preprocessing layers are active only in training mode and are disabled during inference or evaluation. Read more about it here.", "In order to define a hyperparameter search space, we first need to understand what are the possible valid configurations using which we can create our model. Let\u2019s consider one valid config dictionary below:", "If we consider batch_size, then we can choose any value between 1 to 100 or even higher. However, most common batch sizes are a power of 2 and lie somewhere between 8 and 64. So, we can either define our search space to a be any integer value between 1 and 100 or we can lessen the burden of the optimization algorithm by providing a list of most common values like [8, 16, 32, 64]. Similarly, if we consider learning rate (lr), we can choose any floating-point value between 0.0001 and 0.1. We can go lower or higher but that\u2019s usually not needed. Rather than going for an exhaustive search space of 0.0001 to 0.1 we can specify the most common values that are usually in powers of 10 like [0.1, 0.01, 0.001, 0.0001].", "In HyperOpt, a search space consists of nested function expressions, including stochastic expressions. The stochastic expressions are the hyperparameters on which the optimization algorithms work by replacing normal \u201csampling\u201d logic with adaptive exploration strategies. Read more about it here. We can define a stochastic expression which consists of a list of batch_size values as hp.choice(\u2018batch_size\u2019, [8, 16, 32, 64]). Similarly, for learning rate, we can define an expression as hp.choice(\u2018lr\u2019, [0.0001, 0.001, 0.01, 0.1]). If you want to define a continuous space constrained by a two-sided interval we can modify the expression to be hp.uniform(\u2018lr\u2019, 0.0001, 0.1). You can refer to the complete list of parameter expressions here. Our final hyperparameter search space will look something like this:", "A search algorithm is an \u201coptimization algorithm\u201d that optimizes the hyperparameters of a training process by suggesting better hyperparameters with every subsequent trial. Tune\u2019s Search Algorithms are wrappers around open-source optimization libraries for efficient hyperparameter selection. Each library has a specific way of defining the search space like the above search space is defined for HyperOpt. To use this search algorithm, we need to install it separately using pip install -U hyperopt.", "A trial scheduler is also an optimization algorithm implemented as a \u201cscheduling algorithm\u201d that makes the hyperparameter tuning process more efficient. A trial scheduler can early terminate bad trials, pause trials, clone trials, and alter hyperparameters of a running trial making the hyperparameter tuning process much faster. NOTE: Unlike search algorithm, a trial scheduler does not select the hyperparameter configuration to evaluate for each run. We will use AsyncSuccessiveHalvingAlgorithm (ASHA) scheduler which gives similar theoretical performance as HyperBand (SHA) but provides better parallelism and avoids straggler issues during eliminations. We don\u2019t need to install AsyncSuccessiveHalvingAlgorithm scheduler separately.", "All trial schedulers and search algorithms take in metric which is maximized or minimized according to the mode. Trial schedulers also take in grace_period which is similar to patience used in EarlyStopping callback in Keras. For search algorithm, we can provide an initial config (search space) which is usually the best config found through manual tuning, or we can skip it if we don\u2019t have one.", "To start hyperparameter tuning we need to specify an objective function to optimize which is passed to tune.run(). The loss function to train (update model parameters) our image classifier will be categorical cross-entropy. Training and validation losses are a more accurate representation of how our model is performing. However, during overfitting, our training loss will decrease but our validation loss will increase. Hence, validation loss will be to right metric to monitor for hyperparameter tuning.", "Training an image classifier is a lengthy process and waiting for the training to complete and then reporting the validation loss metric is not a good idea as our trial scheduler will have no clue as to how the training is progressing and is there a need to early stop it. To overcome this issue, we will make use of Keras callbacks where validation loss is calculated at the end of every epoch, so we can send the scores to Tune using tune.report(). A minimal code for Keras callback is given below, please refer to the complete code on GitHub.", "In Ray Tune, we can specify an objective function using a function-based API or a class-based API, in this tutorial we\u2019ll be using function-based API. After the hyperparameter tuning is completed, we get the best config which is used to train our final model that will be saved on the disk. We\u2019ll wrap our objective function inside a class to store a few directory paths and a boolean variable which tells whether the given run is the final run or not.", "If TensorBoard is installed, Tune automatically outputs Tensorboard files during tune.run(). After you run an experiment, you can visualize your results with TensorBoard by specifying the output directory of your results: $ tensorboard --logdir=~/ray_results/my_experiment", "In TF2, Tune also automatically generates TensorBoard HParams output, as shown below:", "There are many use cases where we use standard architectures (like ResNet50) instead of building one from scratch. These architectures are huge and performing hyperparameter tuning might not be practical or maybe you want to make use of pre-trained ImageNet weights so changing the model architecture is not an option. In such cases, we can look for hyperparameters outside the model architecture for eg., data augmentation, batch size, learning rate, optimizers, etc.", "Let\u2019s consider object detection as one such use case, where we make use of anchor boxes for bounding box prediction which are not learned during the training process. Every object detection dataset has unique aspect ratios of objects to be detected and the default anchor configuration might not suitable for detecting objects in your dataset. For example, if your objects are smaller than the size of the smallest anchors or your objects have a higher aspect ratio. In such cases, it might be suitable to modify the anchor configuration. This can be done automatically by setting the anchor parameters as the hyperparameters to be tuned.", "I hope this blogpost gave you an insight into different hyperparameters involved in the training of a machine learning model. Manually tuning these parameters is tedious and unintuitive but with the help of Bayesian optimization, we can keep track of the past evaluation results and use it to create a probabilistic model of the actual training objective function. This not only automates the tuning process but also results in an optimal model that we might not have found via manual tuning.", "Combining HyperOpt search algorithm with HyperBand trial scheduler significantly reduces our hyperparameter tuning search time and compute resources. And, being able to find the optimal the data augmentation steps for a given dataset is just icing on the cake. I have linked some great resources while discussing various topics in the article, but I\u2019ll re-link them below so that you do not miss out on any of it. Keep learning!", "I would love to hear your feedback and improvements to this article and the GitHub project. I can be reached on Twitter (@raw_himanshu) and LinkedIn (himanshurawlani)", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F1353e6586fda&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameter-tuning-with-keras-and-ray-tune-1353e6586fda&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameter-tuning-with-keras-and-ray-tune-1353e6586fda&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameter-tuning-with-keras-and-ray-tune-1353e6586fda&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameter-tuning-with-keras-and-ray-tune-1353e6586fda&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----1353e6586fda--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1353e6586fda--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@himanshurawlani?source=post_page-----1353e6586fda--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@himanshurawlani?source=post_page-----1353e6586fda--------------------------------", "anchor_text": "Himanshu Rawlani"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F52b8b739a77b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameter-tuning-with-keras-and-ray-tune-1353e6586fda&user=Himanshu+Rawlani&userId=52b8b739a77b&source=post_page-52b8b739a77b----1353e6586fda---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1353e6586fda&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameter-tuning-with-keras-and-ray-tune-1353e6586fda&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1353e6586fda&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameter-tuning-with-keras-and-ray-tune-1353e6586fda&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@alexisbaydoun?utm_source=medium&utm_medium=referral", "anchor_text": "Alexis Baydoun"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://towardsdatascience.com/implementing-a-fully-convolutional-network-fcn-in-tensorflow-2-3c46fb61de3b", "anchor_text": "article"}, {"url": "https://towardsdatascience.com/implementing-a-fully-convolutional-network-fcn-in-tensorflow-2-3c46fb61de3b", "anchor_text": "article"}, {"url": "https://pablormier.github.io/2017/09/05/a-tutorial-on-differential-evolution-with-python/", "anchor_text": "here"}, {"url": "https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f", "anchor_text": "here"}, {"url": "https://github.com/himanshurawlani/hyper_fcn", "anchor_text": "this GitHub link"}, {"url": "https://github.com/himanshurawlani/hyper_fcn", "anchor_text": "himanshurawlani/hyper_fcnThis project uses HyperOpt's Bayesian optimization and Ray Tune to perform hyperparameter tuning for a simple image\u2026github.com"}, {"url": "https://ray.readthedocs.io/en/latest/tune.html", "anchor_text": "Ray Tune"}, {"url": "https://github.com/ray-project/ray", "anchor_text": "Ray"}, {"url": "https://medium.com/riselab/cutting-edge-hyperparameter-tuning-with-ray-tune-be6c0447afdf", "anchor_text": "Source"}, {"url": "https://github.com/hyperopt/hyperopt", "anchor_text": "HyperOpt"}, {"url": "http://ax.dev/", "anchor_text": "Ax"}, {"url": "https://keras.io/guides/preprocessing_layers/", "anchor_text": "here"}, {"url": "https://github.com/hyperopt/hyperopt/wiki/FMin#2-defining-a-search-space", "anchor_text": "here"}, {"url": "https://github.com/hyperopt/hyperopt/wiki/FMin#21-parameter-expressions", "anchor_text": "here"}, {"url": "https://docs.ray.io/en/latest/tune/api_docs/suggestion.html#hyperopt-tune-suggest-hyperopt-hyperoptsearch", "anchor_text": "HyperOpt"}, {"url": "https://docs.ray.io/en/latest/tune/api_docs/schedulers.html#asha-tune-schedulers-ashascheduler", "anchor_text": "AsyncSuccessiveHalvingAlgorithm"}, {"url": "https://docs.ray.io/en/latest/tune/api_docs/schedulers.html#hyperband-tune-schedulers-hyperbandscheduler", "anchor_text": "HyperBand"}, {"url": "https://wood-b.github.io/post/a-novices-guide-to-hyperparameter-optimization-at-scale/", "anchor_text": "Source"}, {"url": "https://github.com/himanshurawlani/hyper_fcn/blob/master/callbacks.py#L8", "anchor_text": "code on GitHub"}, {"url": "https://docs.ray.io/en/latest/tune/api_docs/trainable.html#tune-function-api", "anchor_text": "function-based"}, {"url": "https://docs.ray.io/en/latest/tune/api_docs/trainable.html#tune-class-api", "anchor_text": "class-based"}, {"url": "https://www.tensorflow.org/tensorboard/hyperparameter_tuning_with_hparams", "anchor_text": "TensorBoard HParams"}, {"url": "https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f", "anchor_text": "A Conceptual Explanation of Bayesian Hyperparameter Optimization for Machine Learning"}, {"url": "https://wood-b.github.io/post/a-novices-guide-to-hyperparameter-optimization-at-scale/", "anchor_text": "A Novice\u2019s Guide to Hyperparameter Optimization at Scale"}, {"url": "https://www.automl.org/blog_bohb/", "anchor_text": "BOHB: Robust and efficient hyperparameter optimization at scale"}, {"url": "https://cloud.google.com/blog/products/gcp/hyperparameter-tuning-cloud-machine-learning-engine-using-bayesian-optimization", "anchor_text": "Hyperparameter tuning in Cloud Machine Learning Engine using Bayesian Optimization"}, {"url": "https://sigopt.com/static/pdf/SigOpt_Bayesian_Optimization_Primer.pdf", "anchor_text": "Bayesian Optimization Primer"}, {"url": "https://pablormier.github.io/2017/09/05/a-tutorial-on-differential-evolution-with-python/", "anchor_text": "A tutorial on Differential Evolution with Python"}, {"url": "https://neptune.ai/blog/hyperparameter-tuning-in-python-a-complete-guide-2020", "anchor_text": "Hyperparameter Tuning in Python: a Complete Guide 2020"}, {"url": "https://github.com/himanshurawlani/hyper_fcn", "anchor_text": "GitHub project"}, {"url": "https://twitter.com/raw_himanshu", "anchor_text": "@raw_himanshu"}, {"url": "https://www.linkedin.com/in/himanshurawlani/", "anchor_text": "himanshurawlani"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----1353e6586fda---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----1353e6586fda---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/hyperparameter-tuning?source=post_page-----1353e6586fda---------------hyperparameter_tuning-----------------", "anchor_text": "Hyperparameter Tuning"}, {"url": "https://medium.com/tag/bayesian-optimization?source=post_page-----1353e6586fda---------------bayesian_optimization-----------------", "anchor_text": "Bayesian Optimization"}, {"url": "https://medium.com/tag/data-science?source=post_page-----1353e6586fda---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1353e6586fda&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameter-tuning-with-keras-and-ray-tune-1353e6586fda&user=Himanshu+Rawlani&userId=52b8b739a77b&source=-----1353e6586fda---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1353e6586fda&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameter-tuning-with-keras-and-ray-tune-1353e6586fda&user=Himanshu+Rawlani&userId=52b8b739a77b&source=-----1353e6586fda---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1353e6586fda&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameter-tuning-with-keras-and-ray-tune-1353e6586fda&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1353e6586fda--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F1353e6586fda&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameter-tuning-with-keras-and-ray-tune-1353e6586fda&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----1353e6586fda---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----1353e6586fda--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----1353e6586fda--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----1353e6586fda--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----1353e6586fda--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----1353e6586fda--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----1353e6586fda--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----1353e6586fda--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----1353e6586fda--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@himanshurawlani?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@himanshurawlani?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Himanshu Rawlani"}, {"url": "https://medium.com/@himanshurawlani/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "349 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F52b8b739a77b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameter-tuning-with-keras-and-ray-tune-1353e6586fda&user=Himanshu+Rawlani&userId=52b8b739a77b&source=post_page-52b8b739a77b--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F1bc332dbc854&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameter-tuning-with-keras-and-ray-tune-1353e6586fda&newsletterV3=52b8b739a77b&newsletterV3Id=1bc332dbc854&user=Himanshu+Rawlani&userId=52b8b739a77b&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}