{"url": "https://towardsdatascience.com/pipeline-for-text-data-pre-processing-a9887b4e2db3", "time": 1683016405.8119922, "path": "towardsdatascience.com/pipeline-for-text-data-pre-processing-a9887b4e2db3/", "webpage": {"metadata": {"title": "Pipeline For Text Data Pre-processing | by Casey Whorton | Towards Data Science", "h1": "Pipeline For Text Data Pre-processing", "description": "Skip ahead to the actual Pipeline section if you are more interested in that than learning about the quick motivation behind it: Text Pre Process Pipeline (halfway through the blog). I\u2019ve been\u2026"}, "outgoing_paragraph_urls": [{"url": "https://pythonprogramming.net/tokenizing-words-sentences-nltk-tutorial/", "anchor_text": "this website will get you up and running", "paragraph_index": 2}, {"url": "http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html", "anchor_text": "This is the package you need to download and utilize if your machine learning projects start dealing with several transformations", "paragraph_index": 14}], "all_paragraphs": ["Skip ahead to the actual Pipeline section if you are more interested in that than learning about the quick motivation behind it: Text Pre Process Pipeline (halfway through the blog).", "I\u2019ve been looking at performing machine learning on text data but there are some data preprocessing steps that are unique to text data that I was not used to. Because of that, my Python code included a lot of transformation steps where I would wrangle with the data, fit a transformation, then transform the training data, transform the testing data, and then repeat this process for every type of transformation I wanted to do. I remembered reading that Python had a convenient way to wrap up transformations but never had a reason to look into it before now. Usually, I would perform something like a standardization scaling to numeric data or some dummy variable creation and that would be it. Now that I\u2019m in a territory a little new to me, I wanted to keep these transformations straight and learn something new.", "If you are new to using NLTK as I was about a year ago, this website will get you up and running, and is saved to my favorites because I use it as a reference pretty frequently when using NLTK.", "The \u2018Movie Reviews\u2019 data has 2000 movie reviews, each with a corresponding category (label) for whether it was negative (\u2018neg\u2019) or positive (\u2018pos\u2019).", "Just changing the \u2018docs\u2019 to a Pandas dataframe now, mostly because I am used to this format. Pipelines of data transformations can apply to more than just Pandas dataframes.", "The \u2018category\u2019 of a movie review is initially \u2018neg\u2019 or \u2018pos\u2019, changing here to 0 and 1, respectively:", "At this point, the reviews are in a dataframe, with the X column as the review text and the y column as the target variable, review category. This is more in the style of what I\u2019m currently used to with machine learning projects. Here is a view of the first 5 observations:", "X y 0 plot : two teen couples go to a church party ,\u2026 0 1 the happy bastard\u2019s quick movie review \\ndamn \u2026 0 2 it is movies like these that make a jaded movi\u2026 0 3 \u201c quest for camelot \u201c is warner bros . \u2018 firs\u2026 0 4 synopsis : a mentally unstable man undergoing \u2026 0", "What needs to happen is the X column needs to be tokenized, the stop-words removed, and for each observation to be vectorized in order for the machine to learn how to distinguish positive from negative reviews.", "Both NLTK and the Scikit-Learn function CountVectorizer have built-in sets or lists of stopwords which basically serve as a bunch of words that we don\u2019t really want hanging around in our data. Words like \u2018a\u2019, \u2018of\u2019, and \u2018the\u2019 are usually not useful and dominate other words in terms of how often they show up in a sentence or paragraph. Sometimes though, you want to add some custom stopwords of your own, so I do that here with the creation of my list, custom_stopwords:", "Now that the movie reviews are in a dataframe and there is a list of stopwords that can be removed from the movie reviews, I\u2019ll show the text pre process pipeline I used before and the one I made using Sci-kit Learn\u2019s Pipeline.", "The idea is to put all of the sentences into a big matrix with columns representing individual words and either a term-frequency or term-frequency-inverse-document-frequency as the observation\u2019s values under a corresponding word.Usually, in a machine learning project, you\u2019ll need to repeat the transformations that you do on the training data to the testing data, so I was explicit in saving the transformations. That is why I had a line to save the TfidfTransform and another line to actually transform the training data. Later, I used that TfidfTransform on the test dataset. I did the same with the CountVectorizer.", "Similarly, for selecting the top k features (words) of the dataset, I made sure to save the transform and apply it to the training and test datasets.", "Not only was this a hassle, but if I had to repeat this process at any point in time then I would have to make new variables and be sure to keep all of these steps in order and not overwrite anything. It just wasn\u2019t good coding. That is where the Pipeline comes in and cleans everything up.", "This is the package you need to download and utilize if your machine learning projects start dealing with several transformations. For each of the transformations in Python that has a \u2018fit_transform()\u2019 method (and all of the ones I use here do), we can wrap them up in an actual pipeline that executes them all in order and even go back and view attributes of each of the transformations. Additionally, you can set parameters for each transformation and the syntax for that is in the link I just shared. If anything, using this pipeline has just cleaned up my code a lot and organized my thinking better.", "Now, once this is fit to the training data, the text_preprocessor pipeline has the transform method that does all three of the included transformations in order to the data.", "Like any other transformation with a fit_transform() method, the text_processor pipeline\u2019s transformations are fit and the data is transformed. The proc_fit can be used to transform testing data in the same way.", "Returning the original words that ended up in the final 1000 words for a particular comment can still be accomplished by the following two steps:", "I picked the number 616 from the first transformed movie reviews words. Turns out that this corresponds to feature number 23078 from the original features. (Only 1000 features were left after the \u2018chi2score\u2019 transformation, and the numbers of the features were changed.)", "What is returned: The word \u2018music\u2019 was the word corresponding to feature number 23078 from the original movie review text data that was transformed into count vectors. It is now feature number 616 in the top 1,000 features. We can always just print the entire movie review and see if that word appears there.", "Examining the first review, we can see that the word \u2018music\u2019 was indeed there, and this word is in the top 1,000 selected features from the entire corpus of movie reviews.", "\u2026 the studio took this film away from its director and chopped it up themselves , and it shows . there might\u2019ve been a pretty decent teen mind-fuck movie in here somewhere , but i guess \u201c the suits \u201c decided that turning it into a music video with little edge , would make more sense . the actors are pretty good for the most part , although wes bentley just seemed to be playing the exact same character that he did in american beauty \u2026", "Usually, there is no need to find individual words in the original data and make sure it has a feature in the new, transformed data. Here, I took the time to go through those steps to make sure that the pipeline I constructed works as intended and so show how all parts of the pipeline can have their attributes displayed.", "Unstructured text data can be useful in machine learning projects. That text data needs to be changed into a more structured format that the machine can learn from, usually using pre-built transformations like removing stopwords and tf-idf transformations. Pipelines of transformations that the data needs to go through can be built to simplify code and provide consistency to the text pre-processing stage. Utilizing the Pipeline function can help to wrap up several transformations into one.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Scientist | British Bake-Off Connoisseur| Recovering Insomniac | Heavy Metal Music Advocate"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fa9887b4e2db3&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpipeline-for-text-data-pre-processing-a9887b4e2db3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpipeline-for-text-data-pre-processing-a9887b4e2db3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpipeline-for-text-data-pre-processing-a9887b4e2db3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpipeline-for-text-data-pre-processing-a9887b4e2db3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----a9887b4e2db3--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a9887b4e2db3--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://casey-whorton.medium.com/?source=post_page-----a9887b4e2db3--------------------------------", "anchor_text": ""}, {"url": "https://casey-whorton.medium.com/?source=post_page-----a9887b4e2db3--------------------------------", "anchor_text": "Casey Whorton"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd0549046d684&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpipeline-for-text-data-pre-processing-a9887b4e2db3&user=Casey+Whorton&userId=d0549046d684&source=post_page-d0549046d684----a9887b4e2db3---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa9887b4e2db3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpipeline-for-text-data-pre-processing-a9887b4e2db3&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa9887b4e2db3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpipeline-for-text-data-pre-processing-a9887b4e2db3&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@mbenna?utm_source=medium&utm_medium=referral", "anchor_text": "Mike Benna"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://github.com/caseywhorton/Text-Pre-Processing", "anchor_text": "link"}, {"url": "https://pythonprogramming.net/tokenizing-words-sentences-nltk-tutorial/", "anchor_text": "this website will get you up and running"}, {"url": "http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html", "anchor_text": "This is the package you need to download and utilize if your machine learning projects start dealing with several transformations"}, {"url": "http://www.nltk.org/", "anchor_text": "Natural Language Toolkit - NLTK 3.5 documentationNLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use\u2026www.nltk.org"}, {"url": "https://pythonprogramming.net/tokenizing-words-sentences-nltk-tutorial/", "anchor_text": "Python Programming TutorialsWelcome to a Natural Language Processing tutorial series, using the Natural Language Toolkit, or NLTK, module with\u2026pythonprogramming.net"}, {"url": "http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html", "anchor_text": "http://scikitlearn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html"}, {"url": "https://medium.com/tag/datascience-training?source=post_page-----a9887b4e2db3---------------datascience_training-----------------", "anchor_text": "Datascience Training"}, {"url": "https://medium.com/tag/data-preprocessing?source=post_page-----a9887b4e2db3---------------data_preprocessing-----------------", "anchor_text": "Data Preprocessing"}, {"url": "https://medium.com/tag/text-data?source=post_page-----a9887b4e2db3---------------text_data-----------------", "anchor_text": "Text Data"}, {"url": "https://medium.com/tag/hands-on-tutorials?source=post_page-----a9887b4e2db3---------------hands_on_tutorials-----------------", "anchor_text": "Hands On Tutorials"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----a9887b4e2db3---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa9887b4e2db3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpipeline-for-text-data-pre-processing-a9887b4e2db3&user=Casey+Whorton&userId=d0549046d684&source=-----a9887b4e2db3---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa9887b4e2db3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpipeline-for-text-data-pre-processing-a9887b4e2db3&user=Casey+Whorton&userId=d0549046d684&source=-----a9887b4e2db3---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa9887b4e2db3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpipeline-for-text-data-pre-processing-a9887b4e2db3&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a9887b4e2db3--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fa9887b4e2db3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpipeline-for-text-data-pre-processing-a9887b4e2db3&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----a9887b4e2db3---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----a9887b4e2db3--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----a9887b4e2db3--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----a9887b4e2db3--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----a9887b4e2db3--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----a9887b4e2db3--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----a9887b4e2db3--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----a9887b4e2db3--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----a9887b4e2db3--------------------------------", "anchor_text": ""}, {"url": "https://casey-whorton.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://casey-whorton.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Casey Whorton"}, {"url": "https://casey-whorton.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "103 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd0549046d684&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpipeline-for-text-data-pre-processing-a9887b4e2db3&user=Casey+Whorton&userId=d0549046d684&source=post_page-d0549046d684--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F49a7b8e78c3f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpipeline-for-text-data-pre-processing-a9887b4e2db3&newsletterV3=d0549046d684&newsletterV3Id=49a7b8e78c3f&user=Casey+Whorton&userId=d0549046d684&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}