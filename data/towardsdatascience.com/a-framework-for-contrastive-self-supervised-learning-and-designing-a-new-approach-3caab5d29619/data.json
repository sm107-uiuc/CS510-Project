{"url": "https://towardsdatascience.com/a-framework-for-contrastive-self-supervised-learning-and-designing-a-new-approach-3caab5d29619", "time": 1683013269.9090219, "path": "towardsdatascience.com/a-framework-for-contrastive-self-supervised-learning-and-designing-a-new-approach-3caab5d29619/", "webpage": {"metadata": {"title": "A Framework For Contrastive Self-Supervised Learning And Designing A New Approach | by William Falcon | Towards Data Science", "h1": "A Framework For Contrastive Self-Supervised Learning And Designing A New Approach", "description": "This is the partner blog matching our new paper: A Framework For Contrastive Self-Supervised Learning And Designing A New Approach (by William Falcon and Kyunghyun Cho). In the last year, a stream of\u2026"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/abs/2009.00104", "anchor_text": "A Framework For Contrastive Self-Supervised Learning And Designing A New Approach", "paragraph_index": 0}, {"url": "https://arxiv.org/abs/2009.00104", "anchor_text": "our recent paper", "paragraph_index": 2}, {"url": "https://ai.facebook.com/", "anchor_text": "Facebook AI Research", "paragraph_index": 3}, {"url": "https://github.com/PyTorchLightning/pytorch-lightning", "anchor_text": "PyTorch Lightning", "paragraph_index": 4}, {"url": "https://pytorch-lightning-bolts.readthedocs.io/en/latest/self_supervised_models.html#cpc-v2", "anchor_text": "CPC V2", "paragraph_index": 5}, {"url": "https://twitter.com/ylecun/status/1123235709802905600?s=20", "anchor_text": "reference", "paragraph_index": 7}, {"url": "https://arxiv.org/abs/1505.05192", "anchor_text": "predicting relative locations of two patches", "paragraph_index": 8}, {"url": "https://arxiv.org/abs/1603.09246", "anchor_text": "solving a jigsaw puzzle", "paragraph_index": 8}, {"url": "https://richzhang.github.io/colorization/", "anchor_text": "colorizing an image", "paragraph_index": 8}, {"url": "http://yann.lecun.com/exdb/publis/pdf/chopra-05.pdf", "anchor_text": "Learning a Similarity Metric Discriminatively, with Application to Face Verification", "paragraph_index": 11}, {"url": "https://papers.nips.cc/paper/5548-discriminative-unsupervised-feature-learning-with-convolutional-neural-networks.pdf", "anchor_text": "this paper", "paragraph_index": 24}, {"url": "https://www.quora.com/What-is-representation-learning-in-deep-learning", "anchor_text": "This Quora post", "paragraph_index": 35}, {"url": "https://arxiv.org/pdf/2006.09882.pdf", "anchor_text": "reference", "paragraph_index": 53}, {"url": "https://arxiv.org/abs/1704.05310", "anchor_text": "Noise as Targets.", "paragraph_index": 54}, {"url": "https://untitled-ai.github.io/understanding-self-supervised-contrastive-learning.html", "anchor_text": "recent ablations", "paragraph_index": 60}, {"url": "https://github.com/PyTorchLightning/pytorch-lightning", "anchor_text": "PyTorch Lightning", "paragraph_index": 78}], "all_paragraphs": ["This is the partner blog matching our new paper: A Framework For Contrastive Self-Supervised Learning And Designing A New Approach (by William Falcon and Kyunghyun Cho).", "In the last year, a stream of \u201cnovel\u201d self-supervised learning algorithms have set new state-of-the-art results in AI research: AMDIM, CPC, SimCLR, BYOL, Swav, etc\u2026", "In our recent paper, we formulate a conceptual framework for characterizing contrastive self-supervised learning approaches. We used our framework to analyze three examples of these leading approaches, SimCLR, CPC, AMDIM, and show that although these approaches seem different on the surface, they are all in fact slight tweaks of one another.", "The majority of this work was conducted while at Facebook AI Research.", "You can find all the augmentations and approaches we described in this article implemented in PyTorch Lightning which will allow you to train on arbitrary hardware and makes the side-by-side comparison of each approach much easier.", "CPC V2 (only verified implementation outside of DeepMind to our knowledge).", "Recall that in supervised learning, a system is given input (x) and a label (y),", "In self-supervised learning, the system is only given (x). Instead of a (y), the system \u201clearns to predict part of its input from other parts of its input\u201d [reference].", "In fact, this formulation is so generic that you can get creative about ways of \u201csplitting\u201d up the input. These strategies are called pretext tasks and researchers have tried all sorts of approaches. Here are three examples: (1) predicting relative locations of two patches, (2) solving a jigsaw puzzle, (3) colorizing an image.", "Although the approaches above are full of creativity, they don\u2019t actually work well in practice. However, a more recent stream of approaches that use contrastive learning has actually started to dramatically close the gap between supervised learning on ImageNet.", "A fundamental idea behind most machine learning algorithms is that similar examples should be grouped together and far from other clusters of related examples.", "This idea is what\u2019s behind one of the earliest works on contrastive learning, Learning a Similarity Metric Discriminatively, with Application to Face Verification By Chopra et al in 2004.", "The animation below illustrates this main idea:", "Contrastive learning achieves this by using three key ingredients, a positive, anchor, and negative(s) representation. To create a positive pair, we need two examples that are similar, and for a negative pair, we use a third example that is not similar.", "But in self-supervised learning, we don\u2019t know the labels of the examples. So, there\u2019s no way to know whether two images are similar or not.", "However, if we assume that each image is its own class, then we can come up with all sorts of ways of forming these triplets (the positive and negative pair). This means that in a dataset of size N, we now have N labels!", "Now that we know the labels (kind of) for each image, we can use data augmentations to generate these triplets.", "The first way we can characterize a contrastive self-supervised learning approach is by defining a data augmentation pipeline.", "A data augmentation pipeline A(x) applies a sequence of stochastic transformations to the same input.", "In deep learning, a data augmentation aims to build representations that are invariant to noise in the raw input. For example, the network should recognize the above pig as a pig even if it\u2019s rotated, or if the colors are gone or even if the pixels are \u201cjittered\u201d around.", "In contrastive learning, the data augmentation pipeline has a secondary goal which is to generate the anchor, positive and negative examples that will be fed to the encoder and will be used for extracting representations.", "CPC introduced a pipeline that applies transforms like color jitter, random greyscale, random flip, etc\u2026 but it also introduced a special transform that splits an image into overlaying sub patches.", "Using this pipeline, CPC can generate many sets of positive and negative samples. In practice, this process is applied to a batch of examples where we can use the rest of the examples in the batch as the negative samples.", "AMDIM takes a slightly different approach. After it performs the standard transforms (jitter, flip, etc\u2026), it generates two versions of an image by applying the data augmentation pipeline twice to the same image.", "This idea was actually proposed in 2014 via this paper by Dosovitski et al. The idea is to use a \u201cseed\u201d image to generate many versions of the same image.", "The pipeline in AMDIM worked so well that every approach that has followed uses the same pipeline but makes slight tweaks to the transforms that happen beforehand (some add jitter, some add gaussian blur, etc\u2026). However, most of these transforms are inconsequential compared with the main idea introduced in AMDIM.", "In our paper, we ran ablations on the impact of these transforms and found that the choice of transforms is critical to the performance of the approach. In fact, we believe that the success of these approaches is mostly driven by the particular choice of transforms.", "These findings are in line with similar results posted by SimCLR and BYOL.", "The video below illustrates the SimCLR pipeline in more detail.", "The second way we characterize these methods is by the choice of encoder. Most of the approaches above use ResNets of various widths and depths.", "When these methods began to come out, CPC and AMDIM actually designed custom encoders. Our ablations found that AMDIM did not generalize well while CPC suffered less from a change in the encoder.", "Every approach since CPC has settled on a ResNet-50. And while there may be more optimal architectures that we\u2019ve yet to invent, standardizing on the ResNet-50 means we can focus on improving the other characteristics to drive improvements as a result of better training methods and not better architectures.", "One finding did hold true for every ablation, wider encoders perform much better in contrastive learning.", "The third way to characterize these methods is by the strategy they employ to extract representations. This is arguably where the \u201cmagic\u201d happens in all of these methods and where they differ the most.", "To understand why this is important, let\u2019s first define what we mean by representations. A representation is the set of unique characteristics that allow a system (and humans) to understand what makes that object, that object, and not a different one.", "This Quora post uses an example of trying to classify a shape. To successfully classify the shapes a good representation might be the number of corners detected in this shape.", "In this collection of methods for contrastive learning, these representations are extracted in various ways.", "CPC introduces the idea of learning representations by predicting the \u201cfuture\u201d in latent space. In practice this means two things:", "1) Treat an image as a timeline with the past at the top left and the future at the bottom right.", "2) The predictions don\u2019t happen at the pixel level, but instead, they use the outputs of the encoder (ie: the latent space)", "Finally, the representation extraction happens by formulating a prediction task using the output of the encoder (H) as targets to the context vectors generated by a projection head (which the authors call a context encoder).", "In our paper, we find that this prediction task is unnecessary as long as the data augmentation pipeline is strong enough. And while there are a lot of hypotheses about what makes a good pipeline, we suggest that a strong pipeline creates positive pairs that share a similar global structure but have a different local structure.", "AMDIM, on the other hand, uses the idea of comparing representations across views from feature maps extracted from intermediate layers of a convolutional neural network (CNN). Let\u2019s unpack this into two parts, 1) multiple views of an image, 2) intermediate layers of a CNN.", "1) Recall that the data augmentation pipeline of AMDIM generates two versions of the same image.", "2) Each version is passed into the same encoder to extract feature maps for each image. AMDIM does not discard the intermediate feature maps generated by the encoder but instead uses them to make comparisons across spatial scales. Recall that as an input makes its way through the layers of a CNN, the receptive fields encode information for different scales of an input.", "AMDIM leverages these ideas by making the comparisons across the intermediate outputs of a CNN. The following animation illustrates how these comparisons are made across the three feature maps generated by the encoder.", "The rest of these methods make slight tweaks to the idea proposed by AMDIM.", "SimCLR uses the same idea as AMDIM but makes 2 tweaks.", "A) Use only the last feature map", "B) Run that feature map through a projection head and compare both vectors (similar to the CPC context projection).", "As we mentioned earlier, contrastive learning needs negative samples to work. Normally this is done by comparing an image in a batch against the other images in a batch.", "Moco does the same thing as AMDIM (with the last feature map only) but keeps a history of all the batches it has seen and increases the number of negative samples. The effect is that the number of negative samples used to provide a contrastive signal increases beyond a single batch size.", "Using the same main ideas as AMDIM (but with the last feature map only), but with two changes.", "Frames their representation extraction task as one of \u201conline clustering\u201d where they enforce \u201cconsistency between codes from different augmentations of the same image.\u201d [reference]. So, it\u2019s the same approach as AMDIM (using only the last feature map), but instead of comparing the vectors directly against each other, they compute the similarity against a set of K precomputed codes.", "In practice, this means that Swav generates K clusters and for each encoded vector it compares against those clusters to learn new representations. This work can be viewed as mixing the ideas of AMDIM and Noise as Targets.", "The representation extraction strategies is where these approaches all differ. However, the changes are very subtle and without rigorous ablations, it\u2019s hard to tell what actually drives results or not.", "From our experiments, we found that the CPC and AMDIM strategies have a negligible effect on the results but instead add complexity. The primary driver that makes these approaches work is the data augmentation pipeline.", "The fourth characteristic we can use to compare these approaches is on the similarity measure that they use. All of the approaches above use a dot product or cosine similarity. Although our paper does not list these ablations, our experiments show that the choice of similarity is largely inconsequential.", "The fifth characteristic we use to compare these approaches is by the choice of the loss function. All of these approaches (except BYOL) have converged on using an NCE loss. The NCE loss has two parts, a numerator, and denominator. The numerator encourages similar vectors close together and the denominator pushes all other vectors far apart.", "Without the denominator, the loss can trivially become a constant, and thus the representations learned will not be useful.", "BYOL however, drops the need for the denominator and instead relies on the weighted updates to the second encoder to provide the contrastive signal. However, as mentioned earlier, recent ablations show that this in fact may not actually be the driver of the contrastive signal.", "In this video, I give a full explanation on the NCE loss using SimCLR as an example.", "We wanted to show the usefulness of our framework by generating a new approach to self-supervised learning without pretext motivations or involved representation extraction strategies. We call this new approach Yet Another DIM (YADIM).", "YADIM can be characterized as follows:", "For YADIM we merge the pipelines of CPC and AMDIM.", "We use the encoder from AMDIM, although any encoder such as a ResNet-50 can also work", "The YADIM strategy is simple. Encode the multiple versions of an image and use the last feature map to make a comparison. There is no projection head or other complicated comparison strategy", "We stick to dot product for YADIM", "We also use the NCE loss.", "Even though our only meaningful choice was to merge the pipelines of AMDIM and CPC YADIM still manages to do really well compared with other approaches.", "Unlike all the related approaches, we generate the results above by actually implementing each approach ourselves. In fact, our implementation of CPC V2 is, to our knowledge, the first public implementation outside of DeepMind.", "More importantly, we use PyTorch Lightning to standardize all implementations so we can objectively distill the main drivers of the above results.", "The methods above are trained using huge amounts of computing resources. The prohibitive costs mean that we did not conduct a rigorous hyperparameter search but simply used the hyperparameters from STL-10 to train on ImageNet.", "Using PyTorch Lightning to efficiently distribute the computations we were able to get an epoch through ImageNet down to about 3 minutes per epoch using 16-bit precision.", "These are the compute resources we used for each approach", "As noted in our paper, I\u2019d like to thank some of the authors of CPC, AMDIM, and BYOL for helpful discussions.", "Most of this work was conducted while at Facebook AI Research. The ablations and long training times would not have been possible without the FAIR compute resources.", "I\u2019d also like to thank colleagues at FAIR and NYU CILVR for helpful discussions, Stephen Roller, Margaret Li, Tullie Murrell, Cinjon Resnick, Ethan Perez, Shubho Sengupta and Soumith Chintala.", "In addition, this happens to have been one of the main reasons for creating PyTorch Lightning, rapid iteration of ideas using massive computing resources without getting caught up in all the engineering details required to train models at this scale.", "Finally, I\u2019d like to thank my advisors Kyunghyun Cho and Yann LeCun for patience while working on this research while building PyTorch Lightning in parallel.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "\u26a1\ufe0fPyTorch Lightning Creator \u2022 PhD Student, AI (NYU, Facebook AI research)."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F3caab5d29619&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-framework-for-contrastive-self-supervised-learning-and-designing-a-new-approach-3caab5d29619&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-framework-for-contrastive-self-supervised-learning-and-designing-a-new-approach-3caab5d29619&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-framework-for-contrastive-self-supervised-learning-and-designing-a-new-approach-3caab5d29619&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-framework-for-contrastive-self-supervised-learning-and-designing-a-new-approach-3caab5d29619&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----3caab5d29619--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3caab5d29619--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://william-falcon.medium.com/?source=post_page-----3caab5d29619--------------------------------", "anchor_text": ""}, {"url": "https://william-falcon.medium.com/?source=post_page-----3caab5d29619--------------------------------", "anchor_text": "William Falcon"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8536ebfbc90b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-framework-for-contrastive-self-supervised-learning-and-designing-a-new-approach-3caab5d29619&user=William+Falcon&userId=8536ebfbc90b&source=post_page-8536ebfbc90b----3caab5d29619---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3caab5d29619&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-framework-for-contrastive-self-supervised-learning-and-designing-a-new-approach-3caab5d29619&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3caab5d29619&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-framework-for-contrastive-self-supervised-learning-and-designing-a-new-approach-3caab5d29619&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://arxiv.org/abs/2009.00104", "anchor_text": "A Framework For Contrastive Self-Supervised Learning And Designing A New Approach"}, {"url": "https://arxiv.org/abs/2009.00104", "anchor_text": "our recent paper"}, {"url": "https://ai.facebook.com/", "anchor_text": "Facebook AI Research"}, {"url": "https://github.com/PyTorchLightning/pytorch-lightning", "anchor_text": "PyTorch Lightning"}, {"url": "https://pytorch-lightning-bolts.readthedocs.io/en/latest/self_supervised_models.html#amdim", "anchor_text": "AMDIM"}, {"url": "https://pytorch-lightning-bolts.readthedocs.io/en/latest/self_supervised_models.html#byol", "anchor_text": "BYOL"}, {"url": "https://pytorch-lightning-bolts.readthedocs.io/en/latest/self_supervised_models.html#cpc-v2", "anchor_text": "CPC V2"}, {"url": "https://pytorch-lightning-bolts.readthedocs.io/en/latest/self_supervised_models.html#moco-v2", "anchor_text": "Moco V2"}, {"url": "https://pytorch-lightning-bolts.readthedocs.io/en/latest/self_supervised_models.html#simclr", "anchor_text": "SimCLR"}, {"url": "https://twitter.com/ylecun/status/1123235709802905600?s=20", "anchor_text": "reference"}, {"url": "https://arxiv.org/abs/1505.05192", "anchor_text": "predicting relative locations of two patches"}, {"url": "https://arxiv.org/abs/1603.09246", "anchor_text": "solving a jigsaw puzzle"}, {"url": "https://richzhang.github.io/colorization/", "anchor_text": "colorizing an image"}, {"url": "https://arxiv.org/pdf/2006.09882.pdf", "anchor_text": "credit: Swav authors"}, {"url": "http://yann.lecun.com/exdb/publis/pdf/chopra-05.pdf", "anchor_text": "Learning a Similarity Metric Discriminatively, with Application to Face Verification"}, {"url": "https://papers.nips.cc/paper/5548-discriminative-unsupervised-feature-learning-with-convolutional-neural-networks.pdf", "anchor_text": "this paper"}, {"url": "https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf", "anchor_text": "Original ResNet authors"}, {"url": "https://www.quora.com/What-is-representation-learning-in-deep-learning", "anchor_text": "This Quora post"}, {"url": "https://arxiv.org/pdf/1311.2901.pdf", "anchor_text": "[1]"}, {"url": "https://www.researchgate.net/figure/Hierarchical-representation-learning-by-a-Convolutional-Neural-Network-where-the-initial_fig4_317558591", "anchor_text": "[2]"}, {"url": "https://github.com/facebookresearch/moco", "anchor_text": "Source"}, {"url": "https://twitter.com/deepmind/status/1272810643222126594", "anchor_text": "source"}, {"url": "https://untitled-ai.github.io/understanding-self-supervised-contrastive-learning.html", "anchor_text": "a recent ablation"}, {"url": "https://arxiv.org/pdf/2006.09882.pdf", "anchor_text": "reference"}, {"url": "https://ai.facebook.com/blog/high-performance-self-supervised-image-classification-with-contrastive-clustering/", "anchor_text": "source"}, {"url": "https://arxiv.org/abs/1704.05310", "anchor_text": "Noise as Targets."}, {"url": "https://untitled-ai.github.io/understanding-self-supervised-contrastive-learning.html", "anchor_text": "recent ablations"}, {"url": "https://github.com/PyTorchLightning/pytorch-lightning", "anchor_text": "PyTorch Lightning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----3caab5d29619---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----3caab5d29619---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----3caab5d29619---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/ai?source=post_page-----3caab5d29619---------------ai-----------------", "anchor_text": "AI"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3caab5d29619&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-framework-for-contrastive-self-supervised-learning-and-designing-a-new-approach-3caab5d29619&user=William+Falcon&userId=8536ebfbc90b&source=-----3caab5d29619---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3caab5d29619&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-framework-for-contrastive-self-supervised-learning-and-designing-a-new-approach-3caab5d29619&user=William+Falcon&userId=8536ebfbc90b&source=-----3caab5d29619---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3caab5d29619&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-framework-for-contrastive-self-supervised-learning-and-designing-a-new-approach-3caab5d29619&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3caab5d29619--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F3caab5d29619&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-framework-for-contrastive-self-supervised-learning-and-designing-a-new-approach-3caab5d29619&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----3caab5d29619---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----3caab5d29619--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----3caab5d29619--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----3caab5d29619--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----3caab5d29619--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----3caab5d29619--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----3caab5d29619--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----3caab5d29619--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----3caab5d29619--------------------------------", "anchor_text": ""}, {"url": "https://william-falcon.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://william-falcon.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "William Falcon"}, {"url": "https://william-falcon.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "3K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8536ebfbc90b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-framework-for-contrastive-self-supervised-learning-and-designing-a-new-approach-3caab5d29619&user=William+Falcon&userId=8536ebfbc90b&source=post_page-8536ebfbc90b--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F9ea6a21cf951&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-framework-for-contrastive-self-supervised-learning-and-designing-a-new-approach-3caab5d29619&newsletterV3=8536ebfbc90b&newsletterV3Id=9ea6a21cf951&user=William+Falcon&userId=8536ebfbc90b&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}