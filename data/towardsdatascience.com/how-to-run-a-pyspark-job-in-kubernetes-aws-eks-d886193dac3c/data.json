{"url": "https://towardsdatascience.com/how-to-run-a-pyspark-job-in-kubernetes-aws-eks-d886193dac3c", "time": 1683011026.314334, "path": "towardsdatascience.com/how-to-run-a-pyspark-job-in-kubernetes-aws-eks-d886193dac3c/", "webpage": {"metadata": {"title": "How to run a PySpark job in Kubernetes (AWS EKS) | by Bogdan Cojocar | Towards Data Science", "h1": "How to run a PySpark job in Kubernetes (AWS EKS)", "description": "In this tutorial, we will focus on deploying a Spark application on AWS EKS end to end. We will do the following steps: A VPC is an isolated network where we can have different infrastructure\u2026"}, "outgoing_paragraph_urls": [{"url": "https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/setup-credentials.html", "anchor_text": "set up our AWS credentials", "paragraph_index": 10}, {"url": "https://github.com/GoogleCloudPlatform/spark-on-k8s-operator", "anchor_text": "Spark Operator", "paragraph_index": 18}], "all_paragraphs": ["In this tutorial, we will focus on deploying a Spark application on AWS EKS end to end. We will do the following steps:", "To deploy Kubernetes on AWS we will need at a minimum to deploy :", "Let\u2019s dive into the Terraform code. First, let\u2019s see the VPC:", "A VPC is an isolated network where we can have different infrastructure components. We can break down this network into smaller blocks and on AWS we call them subnets. Some subnets can have access to the internet and that is why we call them public subnets and some don\u2019t have any access to the internet and they are called private subnets. Another terminology that we will use in context to the network traffic is egress and ingress. Egress means traffic from inside the network to the outside world and ingress traffic from the outside world to the network. As you can expect these rules can be different depending on the use case. We also use security groups, which are traffic rules inside the VPC, that define how the EC2 instances \u201ctalk\u201d with each other, basically on which network ports.", "For the Spark EKS cluster see will use private subnets for the workers. All the data processing is done in total isolation. But we need egress traffic to the internet, to do updates or install open source libraries. To enable traffic to the internet we use NAT gateways into our VPC. We have to add them to public subnets. In the Terraform code, this is done using the flag enable_nat_gateway.", "Another thing we can notice is that we are using three public and private subnets. This is because we want to have network fault tolerance. The subnets are deployed in different availability zones in a region.", "The tags are created as per the requirements from AWS. They are needed for the Control plane to recognize the worker nodes. We can go into more detail about the networking, but it is outside of the scope of this tutorial, so if you need more details please have a look into the Github code where you can find the full example.", "And let\u2019s see the EKS cluster setup as well:", "Again in this snippet, we can see that we declare the cluster inside private subnets. We enable the Clowdwatch logs for all the components of the Control plane. We set the EC2 instance types and number for a config varmodule and as defaults, we use m5.xlarge as the instance type and 3 nodes. We set an EC2 key eks_key if we need to ssh into the worker nodes.", "To be able to run the code in this tutorial we need to install a couple of tools. On Mac we can use brew:", "And to reach AWS we need to also set up our AWS credentials.", "Now we can start to initialize Terraform in order to get all the dependencies needed to deploy the infrastructure:", "If everything runs successfully you should be able to see something similar to the image:", "We are ready to deploy the infrastructure. To do so run:", "It will take some time until the deployment is done, so we can sit back and relax for a bit.", "Once done you should see the message Apply complete! Resources: 55 added, 0 changed, 0 destroyed. and the names of the resources deployed.", "One additional step we can do to check if the deployment was correct is to see if the worker nodes have been attached to the cluster. For that we setup kubectl:", "We should be able to see three nodes when we run the following command:", "Usually, we deploy spark jobs using the spark-submit , but in Kubernetes, we have a better option, more integrated with the environment called the Spark Operator. Some of the improvements that it brings are automatic application re-submission, automatic restarts with a custom restart policy, automatic retries of failed submissions, and easy integration with monitoring tools such as Prometheus.", "We can install it via helm:", "If we run helm list in the terminal the spark-op chart should be available. Also, we should have a running pod for the spark operator. We can watch what pods are running in the default namespace with the command kubectl get pods.", "Now we can finally run python spark apps in K8s. The first thing we need to do is to create a spark user, in order to give the spark jobs, access to the Kubernetes resources. We create a service account and a cluster role binding for this purpose:", "To execute the creation of the role:", "We define our spark run parameters in a yml file, similar to any other resource declarations on Kubernetes. Basically we are defining that we are running a Python 3 spark app and we are the image uprush/apache-spark-pyspark:2.4.5. I recommend using this image because it comes with a newer version of yarn that handles writes to s3a more efficiently. We have a retry policy if the job fails, it will be restarted. Some resource allocations for the driver and the executors. As the job is very simple, we use just an executor. Another thing we can notice is that we use the spark service account that we defined earlier. To code we are using is a classic example of computing the pi number.", "To submit the code we are using kubectl again:", "Upon completion, if we inspect the pods again we should have a similar result:", "And if we check the logs by running kubectl logs spark-job-driver we should find one line in the logs giving an approximate value of pi Pi is roughly 3.142020.", "That was all folks. I hope you enjoyed this tutorial. We\u2019ve seen how we can create our own AWS EKS cluster using terraform, to easily re-deploy it in different environments and how we can submit PySpark jobs using a more Kubernetes friendly syntax.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Big data consultant. I write about the wonderful world of data."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fd886193dac3c&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-run-a-pyspark-job-in-kubernetes-aws-eks-d886193dac3c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-run-a-pyspark-job-in-kubernetes-aws-eks-d886193dac3c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-run-a-pyspark-job-in-kubernetes-aws-eks-d886193dac3c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-run-a-pyspark-job-in-kubernetes-aws-eks-d886193dac3c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----d886193dac3c--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d886193dac3c--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@bogdan.cojocar?source=post_page-----d886193dac3c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@bogdan.cojocar?source=post_page-----d886193dac3c--------------------------------", "anchor_text": "Bogdan Cojocar"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa5d35fbaadc8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-run-a-pyspark-job-in-kubernetes-aws-eks-d886193dac3c&user=Bogdan+Cojocar&userId=a5d35fbaadc8&source=post_page-a5d35fbaadc8----d886193dac3c---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd886193dac3c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-run-a-pyspark-job-in-kubernetes-aws-eks-d886193dac3c&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd886193dac3c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-run-a-pyspark-job-in-kubernetes-aws-eks-d886193dac3c&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://github.com/BogdanCojocar/medium-articles/tree/master/terraform_eks_spark", "anchor_text": "Github code repo"}, {"url": "https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/setup-credentials.html", "anchor_text": "set up our AWS credentials"}, {"url": "https://github.com/GoogleCloudPlatform/spark-on-k8s-operator", "anchor_text": "Spark Operator"}, {"url": "https://medium.com/tag/kubernetes?source=post_page-----d886193dac3c---------------kubernetes-----------------", "anchor_text": "Kubernetes"}, {"url": "https://medium.com/tag/spark?source=post_page-----d886193dac3c---------------spark-----------------", "anchor_text": "Spark"}, {"url": "https://medium.com/tag/aws?source=post_page-----d886193dac3c---------------aws-----------------", "anchor_text": "AWS"}, {"url": "https://medium.com/tag/python?source=post_page-----d886193dac3c---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd886193dac3c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-run-a-pyspark-job-in-kubernetes-aws-eks-d886193dac3c&user=Bogdan+Cojocar&userId=a5d35fbaadc8&source=-----d886193dac3c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd886193dac3c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-run-a-pyspark-job-in-kubernetes-aws-eks-d886193dac3c&user=Bogdan+Cojocar&userId=a5d35fbaadc8&source=-----d886193dac3c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd886193dac3c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-run-a-pyspark-job-in-kubernetes-aws-eks-d886193dac3c&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d886193dac3c--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fd886193dac3c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-run-a-pyspark-job-in-kubernetes-aws-eks-d886193dac3c&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----d886193dac3c---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----d886193dac3c--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----d886193dac3c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----d886193dac3c--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----d886193dac3c--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----d886193dac3c--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----d886193dac3c--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----d886193dac3c--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----d886193dac3c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@bogdan.cojocar?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@bogdan.cojocar?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Bogdan Cojocar"}, {"url": "https://medium.com/@bogdan.cojocar/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "566 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa5d35fbaadc8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-run-a-pyspark-job-in-kubernetes-aws-eks-d886193dac3c&user=Bogdan+Cojocar&userId=a5d35fbaadc8&source=post_page-a5d35fbaadc8--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Ffaedcd759ecc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-run-a-pyspark-job-in-kubernetes-aws-eks-d886193dac3c&newsletterV3=a5d35fbaadc8&newsletterV3Id=faedcd759ecc&user=Bogdan+Cojocar&userId=a5d35fbaadc8&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}