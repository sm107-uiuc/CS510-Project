{"url": "https://towardsdatascience.com/introduction-to-generative-networks-e33c18a660dd", "time": 1683018424.226474, "path": "towardsdatascience.com/introduction-to-generative-networks-e33c18a660dd/", "webpage": {"metadata": {"title": "Introduction To Generative Networks | by Abhijit Roy | Towards Data Science", "h1": "Introduction To Generative Networks", "description": "GANs are the most exciting discovery in the field of machine learning in the last decade according to several prominent experts of the domain. The idea of GANs was first introduced in 2014 by an\u2026"}, "outgoing_paragraph_urls": [{"url": "http://www.gatsby.ucl.ac.uk/~gretton/papers/testing_workshop.pdf", "anchor_text": "her", "paragraph_index": 16}, {"url": "https://towardsdatascience.com/how-a-chess-playing-computer-thinks-about-its-next-move-8f028bd0e7b1", "anchor_text": "here", "paragraph_index": 30}, {"url": "https://en.wikipedia.org/wiki/Transportation_problem", "anchor_text": "transportation problem", "paragraph_index": 37}, {"url": "https://abhijitroy1998.wixsite.com/abhijitcv", "anchor_text": "https://abhijitroy1998.wixsite.com/abhijitcv", "paragraph_index": 42}], "all_paragraphs": ["GANs are the most exciting discovery in the field of machine learning in the last decade according to several prominent experts of the domain. The idea of GANs was first introduced in 2014 by an article by Ian J Goodfellow and has been very popular since its discovery.", "GANs have been a large improvement in the field of generative networks. Previously, the generative work was done using variational autoencoders, which worked on a policy of reparameterization trick and was based on a random number generation from feature distributions. This caused VAEs to create non-realistic images. GANs work much better in that aspect.", "In this article, we will see the idea behind generative networks, types of generative networks, and their workings", "Let\u2019s understand the idea with a simple example. Let\u2019s say we have RGB images of puppies of dimension 100 x 100. So, we will have 100x100x3= 30000 different pixels. we multiply 3 as an RGB has 3 channels in the image. Now, if we flatten the image, we will get a vector of 30000 dimensions.", "The image can be represented as a point vector in a 30000-dimensional vector space. Similarly, if we plot all the images in the puppy images dataset as point vectors on that vector space, we will get an entire distribution of the images. So, from the distribution, we can get a clear idea, which points in the 30000-dimensional vector space can represent a puppy.", "From the above diagram, we can get an idea that the blue points distribution is the distribution of the puppy images of the dataset. Now, if we can pick any point vector from this vector space, we will be able to obtain a probability of that point to be an image of a puppy using the given distribution. As the blue points represent all the points in the dataset that has been used to create the distribution, the sum of the probabilities of all the points to represent a puppy must be equal to 1. In simpler words, we are creating a probability density distribution using the samples in the dataset.", "Now, if we consider point 1 and point 2, point 1 has a much higher probability to represent a puppy compared to point 2, which is pretty evident from the diagram. So, to generate an image that is not present in our original dataset all we have to do is randomly sample a vector point from the probability distribution created by the given dataset.", "The problem with the above approach is that the distribution given is too complex to sample from. The above image is a simplified representation but in the actual cases, we have a huge huge number of dimensions and the distribution becomes too complex. The reason for not being able to sample is that we can\u2019t really obtain the distribution function of the given distribution and it is impossible to generate such a complex random variable.", "To solve this issue, we use transformations. In this method, we generate a random variable to create a simple uniform random distribution. We then transform this simple distribution using a complex function to convert our simple distribution into our required complex distribution. Here we know the distribution function and can randomly sample from it.", "We have found a way to create the complex distribution given by the dataset, now comes the implementational challenges. To transform the signal we actually need the complex distribution function which of course we don't have. What we have, are the data samples that we can plot to obtain the distribution. So, we work with two distributions, one is the N-dimensional complex one obtained by plotting the samples we have, and another, we create, by generating N uniform random variables (one for each dimension). We have two distributions now, both of N dimensions.", "We can generate simple random uniform variables using a pseudorandom number generator. It generates a sequence of numbers that approximates a random distribution between 0 and 1.", "As we can see above, we created the N variable uniform distribution and transform it using a complex function to be similar to the given complex distribution. So, we can just plot the points in the dataset as samples to obtain the distribution and bring two distributions closer to make them similar as shown in the last diagram. Thus we can create a distribution and transform it as the given complex distribution and sample from it to generate the required outputs. One thing to note is, after creating the new uniform random distribution, we upsample, so that we can compare and transform to approximate the original complex distribution with the newly created one.", "So, we have learned till now that our actual task is to formulate a complex function that we can use to transform our created simple normal uniform distribution into a given complex distribution. We know we can use neural networks for the formulation of the function of the transformation. Neural networks use a degree of non-linearity which makes it possible for neural networks to devise any required complex function. So, the Neural Network acts as the transforming function.", "Now, there are two types of generative network architectures possible depending on the procedure they use to perform the task.", "Generative Matching networks is a direct approach to this problem. It simply tries to minimize the distance between the generated distribution and the actual complex distribution. It picks some random samples, generates the distribution calculates the difference between the generated distribution and the actual distribution after each iteration. The difference serves as the error which is backpropagated through the model and the model parameters are updated using gradient descent. Again, we obtain the generated distribution and continue the above process.", "The above diagram represents GMN. Now, as at every iteration we match the real distribution with the generated distribution at each stage to generate the error, we call it the Generative Matching Network. For every neural network to work, we need a loss function, which the network minimizes. From our discussion above, it is pretty evident for us that the loss function, in this case, is the difference or distance between the generated distribution and the real distribution.", "Now, for this purpose, we may have used any loss functions, like KL divergence which calculates the difference between two given distributions, but Maximum Mean Discrepancy (MMD) is used as the loss function in the case of GMT. The MMD defines a distance between two probability distributions that can be computed (estimated) based on samples of these distributions. The details about MMD can be found here. The main target of our Neural Network is to minimize the MMD error or loss function. One thing to note is that the input to the network is randomly generated N-Dimensional point vectors regarded as noise in these circumstances.", "We do not use GMNs that much because they are hard to set up and train.", "Let\u2019s uncover the most used generative network GANs and dive into the concepts.", "General Adversarial Networks is said to be an indirect approach to the problem. GANs train the generator network to do a task that in turn reduces the difference between the original and generated distributions. Here the task is to increase the error of a discriminator model. So, as we are not directly working on the actual motive, we call them \u201cAdversarial\u201d Networks.", "Now, let\u2019s elaborate on the workings of GANs. GANs have two neural networks, a generator, and a discriminator. The generator generates vector points and the function of the discriminator is to discriminate between the generated points and the real data points. So, the discriminator tries to decrease the classification error by identifying the generated data correctly while the generator tries to increase the classification error by generating better data points.", "The idea behind this network is based on a concept of equilibrium in game theory. This is called the Nash equilibrium.", "In game theory, the Nash equilibrium, named after the mathematician John Forbes Nash Jr., is the most common way to define the solution of a non-cooperative game involving two or more players. In a Nash equilibrium, each player is assumed to know the equilibrium strategies of the other players and no player has anything to gain by changing only his own strategy.-Wikipedia.", "Two models, the discriminator, and generator are trained simultaneously to beat each other and attain Nash Equilibrium in a two-player game. We can understand, in real-world two-player games, the action of one player affects another player's moves. Similarly, here the target of the generator shifts according to the discriminator\u2019s generated error. It will be very hard for the generator to do its work if the target or the discriminator error is varying or moving. So, to avoid this situation, the discriminator is not trained during the training of the generator and vice versa. But it is important for the two models to learn together for them to converge. So, first, we train the discriminator and then we move on to train the generator.", "The above image shows a GAN model. It is a design based on the original by Google Developers. As we can see the generator is dependent on the discriminator.", "Now, the discriminator loss is only backpropagated to the Discriminator network. The Generator loss is backpropagated through the Discriminator to reach the Generator. The weights of the discriminator are frozen during the training of the generator so that they do not get updated due to the backpropagation during generator training.", "The above steps show the procedure for the training of a GAN in one epoch. Initially, the discriminator performs in a superb fashion as the generator is not at all trained and the generated output is not anywhere close to the actual instances.", "For any network to train, we know, we need a loss function, which will be minimized by the Discriminator network and the Generator network to learn.", "The loss function for GAN is proposed to be Minimax loss in the introductory paper. The minimax loss is again developed from the minimax algorithm from game theory. I will try to give an example.", "Say, two players play a game and compete. The game theory states the whole game and every possible move of both the players at every stage of the game can be clearly represented by an n-ary tree structure. The value of n depends on the type of game and move. This is called a game tree. Now, we can score a player move\u2019s according to the potential of the move to win the game. This is called utility. So, the one who makes the best moves has the maximum utility and he/she wins. The utility is nothing but a way to interpret the probable chance of winning.", "We can see that, to win, a player must maximize his own utility and minimize the opponent\u2019s utility. But for this we must find out the best moves the opponent can make, so we will need to find their future maximum utility moves also. Thus, the algorithm needs us to make recursive calls to maximize and minimize. So, the algorithm is called the Minimax algorithm. You can read more about this algorithm here.", "The above equation shows the Discriminator Loss Function. E(x) is the expectation of a probabilistic sum. D(x) is the probability of the discriminator to predict the instance x as real. G(x) is the generated output, for random noise x. z is the given random noise.", "In this equation, the first part corresponds to the real instances. x is real, so, here D(x) needs to be 1 or closer to 1 for the discriminator to work better. We need the value of the first part to increase. Again, G(z) is fake, so we need D(G(z)) to be low, so, we try to increase (1- D(G(z))). The loss is similar to a cross-entropy loss. This is the discriminator loss. The Discriminator tries to maximize the above-given loss function, conversely, it tries to minimize the negative of it.", "The generator works in the opposite direction. So, it tries to minimize the given loss function. The first part is not affected by the generator. So, it only affects the second part (1-D(G(z))). The generator minimizes (1-D(G(z))) or it maximizes, D(G(z)). So, it wants the discriminator to predict a high probability of being a real instance, for the generated instances.", "GANs face some challenges in their applications and design. Let\u2019s talk about them.", "The above problems proved that we must modify the loss function in order to remove the vanishing gradient challenge. Let\u2019s look at the modified loss functions:", "In statistics, the earth mover\u2019s distance (EMD) is a measure of the distance between two probability distributions over a region D. In mathematics, this is known as the Wasserstein metric. Informally, if the distributions are interpreted as two different ways of piling up a certain amount of dirt over the region D, the EMD is the minimum cost of turning one pile into the other; where the cost is assumed to be amount of dirt moved times the distance by which it is moved.", "Earth Mover\u2019s Distance can be formulated and solved as a transportation problem. Suppose that several suppliers, each with a given amount of goods, are required to supply several consumers, each with a given limited capacity. For each supplier-consumer pair, the cost of transporting a single unit of goods is given. The transportation problem is then to find a least-expensive flow of goods from the suppliers to the consumers that satisfies the consumers\u2019 demand. \u2014 Wikipedia", "So, the earth mover\u2019s distance basically gives the least cost required to move the points of one distribution to match another distribution. The GAN using this loss is called Wasserstein GAN or WGAN. Here, the discriminator does not try to classify an instance as generated or real. The discriminator assigns a score. The discriminator tries to assign a larger score for the real instances, than the fake instances. There are no cutoffs as such. Just the number assigned to real instances are much larger. Now, as the discriminator doesn\u2019t classify, it is called a critic. The loss is called critic loss. The generator tries to increase the score for fake instances.", "The D(x) is the critic\u2019s output value for x instance. As we can see from the critic\u2019s loss the critic tries to increase the difference between the values output for real and fake instances. So, it maximizes this function or minimizes its negation. Similarly, the generator tries to increase or maximize the output value given by the discriminator to the generated instances. The values are not bound between 0 and 1, and has large values, so free from the vanishing gradient problem.", "In this article, we have seen an introduction to generative networks and focussed on GAN.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "I am a Computer Science and Technology Graduate from NIT, Durgapur. Find Me at https://abhijitroy1998.wixsite.com/abhijitcv"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fe33c18a660dd&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-generative-networks-e33c18a660dd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-generative-networks-e33c18a660dd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-generative-networks-e33c18a660dd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-generative-networks-e33c18a660dd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----e33c18a660dd--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e33c18a660dd--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@myac.abhijit?source=post_page-----e33c18a660dd--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@myac.abhijit?source=post_page-----e33c18a660dd--------------------------------", "anchor_text": "Abhijit Roy"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4c235a4f4b95&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-generative-networks-e33c18a660dd&user=Abhijit+Roy&userId=4c235a4f4b95&source=post_page-4c235a4f4b95----e33c18a660dd---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe33c18a660dd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-generative-networks-e33c18a660dd&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe33c18a660dd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-generative-networks-e33c18a660dd&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@pietrozj?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Pietro Jeng"}, {"url": "https://unsplash.com/s/photos/deep-learning?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "http://www.gatsby.ucl.ac.uk/~gretton/papers/testing_workshop.pdf", "anchor_text": "her"}, {"url": "https://towardsdatascience.com/how-a-chess-playing-computer-thinks-about-its-next-move-8f028bd0e7b1", "anchor_text": "here"}, {"url": "https://en.wikipedia.org/wiki/Transportation_problem", "anchor_text": "transportation problem"}, {"url": "https://medium.com/tag/generative-adversarial?source=post_page-----e33c18a660dd---------------generative_adversarial-----------------", "anchor_text": "Generative Adversarial"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe33c18a660dd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-generative-networks-e33c18a660dd&user=Abhijit+Roy&userId=4c235a4f4b95&source=-----e33c18a660dd---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe33c18a660dd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-generative-networks-e33c18a660dd&user=Abhijit+Roy&userId=4c235a4f4b95&source=-----e33c18a660dd---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe33c18a660dd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-generative-networks-e33c18a660dd&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e33c18a660dd--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fe33c18a660dd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-generative-networks-e33c18a660dd&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----e33c18a660dd---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----e33c18a660dd--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----e33c18a660dd--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----e33c18a660dd--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----e33c18a660dd--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----e33c18a660dd--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----e33c18a660dd--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----e33c18a660dd--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----e33c18a660dd--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@myac.abhijit?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@myac.abhijit?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Abhijit Roy"}, {"url": "https://medium.com/@myac.abhijit/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "458 Followers"}, {"url": "https://abhijitroy1998.wixsite.com/abhijitcv", "anchor_text": "https://abhijitroy1998.wixsite.com/abhijitcv"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4c235a4f4b95&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-generative-networks-e33c18a660dd&user=Abhijit+Roy&userId=4c235a4f4b95&source=post_page-4c235a4f4b95--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F2ba8066c30a7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-generative-networks-e33c18a660dd&newsletterV3=4c235a4f4b95&newsletterV3Id=2ba8066c30a7&user=Abhijit+Roy&userId=4c235a4f4b95&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}