{"url": "https://towardsdatascience.com/trade-smarter-w-reinforcement-learning-a5e91163f315", "time": 1683000897.033538, "path": "towardsdatascience.com/trade-smarter-w-reinforcement-learning-a5e91163f315/", "webpage": {"metadata": {"title": "Trade and Invest Smarter \u2014 The Reinforcement Learning Way | by Adam King | Towards Data Science", "h1": "Trade and Invest Smarter \u2014 The Reinforcement Learning Way", "description": "Winning high stakes poker tournaments, out-playing world-class StarCraft players, and autonomously driving Tesla\u2019s futuristic sports cars. What do they all have in common? Each of these extremely\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/questions-96667b06af5", "anchor_text": "rules and guidelines", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/readers-terms-b5d780a700a4", "anchor_text": "Reader Terms", "paragraph_index": 0}, {"url": "https://www.nature.com/articles/d41586-019-02156-9", "anchor_text": "Winning high stakes poker tournaments", "paragraph_index": 1}, {"url": "https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/", "anchor_text": "out-playing world-class StarCraft players", "paragraph_index": 1}, {"url": "https://www.tesla.com/autopilot", "anchor_text": "autonomously driving Tesla\u2019s futuristic sports cars", "paragraph_index": 1}, {"url": "https://towardsdatascience.com/creating-bitcoin-trading-bots-that-dont-lose-money-2e7165fb0b29", "anchor_text": "progress on that front", "paragraph_index": 3}, {"url": "https://discord.gg/ZZ7BGWh", "anchor_text": "RL trading Discord", "paragraph_index": 4}, {"url": "http://tensortrade.org", "anchor_text": "TensorTrade", "paragraph_index": 7}, {"url": "https://en.wikipedia.org/wiki/Machine_learning", "anchor_text": "machine learning", "paragraph_index": 11}, {"url": "https://en.wikipedia.org/wiki/Software_agent", "anchor_text": "software agents", "paragraph_index": 11}, {"url": "https://en.wikipedia.org/wiki/Action_selection", "anchor_text": "actions", "paragraph_index": 11}, {"url": "https://colab.research.google.com/drive/1hzbugXnkGWO6l3vpQ0bSqnJJxBGiogar", "anchor_text": "Google Colab", "paragraph_index": 17}, {"url": "https://github.com/notadamking/tensortrade/blob/master/examples/TensorTrade_Tutorial.ipynb", "anchor_text": "Github", "paragraph_index": 17}, {"url": "https://www.binance.com/en/register?ref=PDOJ9XB8", "anchor_text": "Binance", "paragraph_index": 31}, {"url": "http://coinbase-consumer.sjv.io/c/1949163/626313/9251", "anchor_text": "Coinbase", "paragraph_index": 31}, {"url": "https://stable-baselines.readthedocs.io/en/master", "anchor_text": "Stable Baselines", "paragraph_index": 50}, {"url": "https://tensorforce.readthedocs.io/en/0.4.4", "anchor_text": "Tensorforce", "paragraph_index": 50}, {"url": "https://ray.readthedocs.io/en/latest/rllib.html", "anchor_text": "Ray\u2019s RLLib", "paragraph_index": 50}, {"url": "https://github.com/openai/baselines", "anchor_text": "OpenAI\u2019s Baselines", "paragraph_index": 50}, {"url": "https://github.com/NervanaSystems/coach", "anchor_text": "Intel\u2019s Coach", "paragraph_index": 50}, {"url": "https://github.com/tensorflow/agents", "anchor_text": "TF Agents", "paragraph_index": 50}, {"url": "https://stable-baselines.readthedocs.io/en/master/", "anchor_text": "Documentation", "paragraph_index": 53}, {"url": "https://tensorforce.readthedocs.io/en/0.4.4", "anchor_text": "Tensorforce", "paragraph_index": 54}, {"url": "https://tensorforce.readthedocs.io/en/0.4.4", "anchor_text": "Documentation", "paragraph_index": 55}, {"url": "https://stable-baselines.readthedocs.io/en/master", "anchor_text": "Documentation", "paragraph_index": 64}, {"url": "https://github.com/notadamking/tensortrade", "anchor_text": "Github", "paragraph_index": 85}, {"url": "https://tensortrade.org", "anchor_text": "tensortrade.org", "paragraph_index": 85}, {"url": "https://discord.gg/ZZ7BGWh", "anchor_text": "Discord community", "paragraph_index": 85}, {"url": "https://github.com/notadamking/tensortrade", "anchor_text": "GitHub", "paragraph_index": 88}, {"url": "https://twitter.com/notadamking", "anchor_text": "Twitter", "paragraph_index": 88}, {"url": "https://github.com/users/notadamking/sponsorship", "anchor_text": "Github Sponsors", "paragraph_index": 89}, {"url": "https://www.patreon.com/join/notadamking", "anchor_text": "Patreon", "paragraph_index": 89}, {"url": "https://www.patreon.com/notadamking", "anchor_text": "https://www.patreon.com/notadamking", "paragraph_index": 91}], "all_paragraphs": ["Note from Towards Data Science\u2019s editors: While we allow independent authors to publish articles in accordance with our rules and guidelines, we do not endorse each author\u2019s contribution. You should not rely on an author\u2019s works without seeking professional advice. See our Reader Terms for details.", "Winning high stakes poker tournaments, out-playing world-class StarCraft players, and autonomously driving Tesla\u2019s futuristic sports cars. What do they all have in common? Each of these extremely complex tasks were long thought to be impossible by machines, until recent advancements in deep reinforcement learning showed they were possible, today.", "Reinforcement learning is beginning to take over the world.", "A little over two months ago, I decided I wanted to take part in the revolution, so I set out on a journey to create a profitable Bitcoin trading strategy using state-of-the-art deep reinforcement learning algorithms. While I made quite a bit of progress on that front, I realized that the tooling for this sort of project can be quite daunting to wrap your head around, and as such, it is very easy to get lost in the details.", "In between optimizing my previous project for distributed high-performance computing (HPC) systems; getting lost in endless pipelines of data and feature optimizations; and running my head in circles around efficient model set-up, tuning, training, and evaluation; I realized that there had to be a better way of doing things. After countless hours of researching existing projects, spending endless nights watching PyData conference talks, and having many back-and-forth conversations with the hundreds of members of the RL trading Discord community, I realized there weren\u2019t any existing solutions that were all that good.", "There were many bits and pieces of great reinforcement learning trading systems spread across the inter-webs, but nothing solid and complete. For this reason, I\u2019ve decided to create an open source Python framework for getting any trading strategy from idea to production, efficiently, using deep reinforcement learning.", "Enter TensorTrade. The idea was to create a highly modular framework for building efficient reinforcement learning trading strategies in a composable, maintainable way. Sounds like a mouthful of buzz-words if you ask me, so let\u2019s get into the meat.", "TensorTrade is an open source Python framework for training, evaluating, and deploying robust trading strategies using deep reinforcement learning. The framework focuses on being highly composable and extensible, to allow the system to scale from simple trading strategies on a single CPU, to complex investment strategies run on a distribution of HPC machines.", "Under the hood, the framework uses many of the APIs from existing machine learning libraries to maintain high quality data pipelines and learning models. One of the main goals of TensorTrade is to enable fast experimentation with algorithmic trading strategies, by leveraging the existing tools and pipelines provided by numpy, pandas, gym, keras, and tensorflow.", "Every piece of the framework is split up into re-usable components, allowing you to take advantage of the general use components built by the community, while keeping your proprietary features private. The aim is to simplify the process of testing and deploying robust trading agents using deep reinforcement learning, to allow you and I to focus on creating profitable strategies.", "In case your reinforcement learning chops are a bit rusty, let\u2019s quickly go over the basic concepts.", "Reinforcement learning (RL) is an area of machine learning concerned with how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward.", "Every reinforcement learning problem starts out with an environment and one or more agents that can interact with the environment.", "The agent will first observe the environment, then build a model of the current state and the expected value of actions within that environment. Based on that model, the agent will then take the action it has deemed as having the highest expected value.", "Based on the effects of the chosen action within the environment, the agent will be rewarded by an amount corresponding to the actual value of that action. The reinforcement learning agent can then, through the process of trial and error (i.e. learning through reinforcement), improve its underlying model and learn to take more rewarding actions over time.", "If you still need a bit of refreshment on the subject, there is a link to an article titled Introduction to Deep Reinforcement Learning in the references for this article, which goes much more in-depth into the details. Let\u2019s move on.", "The following tutorial should provide enough examples to get you started with creating simple trading strategies using TensorTrade, although you will quickly see the framework is capable of handling much more complex configurations.", "You can follow along with Google Colab or the tutorial on Github.", "TensorTrade requires Python 3.6 or later, so make sure you\u2019re using a valid version before pip installing the framework.", "To follow this entire tutorial, you will need to install some extra dependencies, such as tensorflow, tensorforce, stable-baselines, ccxt, ta, and stochastic.", "That\u2019s all the installation necessary! Let\u2019s get into the code.", "TensorTrade is built around modular components that together make up a trading strategy. Trading strategies combine reinforcement learning agents with composable trading logic in the form of a gym environment. A trading environment is made up of a set of modular components that can be mixed and matched to create highly diverse trading and investment strategies. I will explain this in further detail later, but for now it is enough to know the basics.", "Just like electrical components, the purpose of TensorTrade components is to be able to mix and match them as necessary.", "The code snippets in this section should serve as guidelines for creating new strategies and components. There will likely be missing implementation details that will become more clear in a later section, as more components are defined.", "A trading environment is a reinforcement learning environment that follows OpenAI\u2019s gym.Env specification. This allows us to leverage many of the existing reinforcement learning models in our trading agent, if we\u2019d like.", "Trading environments are fully configurable gym environments with highly composable Exchange, FeaturePipeline, ActionScheme, and RewardScheme components.", "If it seems a bit complicated now, it\u2019s really not. That\u2019s all there is to it, now it\u2019s just a matter of composing each of these components into a complete environment.", "When the reset method of a TradingEnvironment is called, all of the child components will also be reset. The internal state of each exchange, feature pipeline, transformer, action scheme, and reward scheme will be set back to their default values, ready for the next episode.", "Let\u2019s begin with an example environment. As mentioned before, initializing a TradingEnvironment requires an exchange, an action scheme, and a reward scheme, the feature pipeline is optional.", "While the recommended use case is to plug a trading environment into a trading strategy, you can obviously use the trading environment separately, any way you\u2019d otherwise use a gym environment.", "Exchanges determine the universe of tradable instruments within a trading environment, return observations to the environment on each time step, and execute trades made within the environment. There are two types of exchanges: live and simulated.", "Live exchanges are implementations of Exchange backed by live pricing data and a live trade execution engine. For example, CCXTExchange is a live exchange, which is capable of returning pricing data and executing trades on hundreds of live cryptocurrency exchanges, such as Binance and Coinbase.", "There are also exchanges for stock and ETF trading, such as RobinhoodExchange and InteractiveBrokersExchange, but these are still works in progress.", "Simulated exchanges, on the other hand, are implementations of Exchange backed by simulated pricing data and trade execution.", "For example, FBMExchange is a simulated exchange, which generates pricing and volume data using fractional brownian motion (FBM). Since its price is simulated, the trades it executes must be simulated as well. The exchange uses a simple slippage model to simulate price and volume slippage on trades, though like almost everything in TensorTrade, this slippage model can easily be swapped out for something more complex.", "Though the FBMExchange generates fake price and volume data using a stochastic model, it is simply an implementation of SimulatedExchange. Under the hood, SimulatedExchange only requires a data_frame of price history to generate its simulations. This data_frame can either be provided by a coded implementation such as FBMExchange, or at runtime such as in the following example.", "Feature pipelines are meant for transforming observations from the environment into meaningful features for an agent to learn from. If a pipeline has been added to a particular exchange, then observations will be passed through the FeaturePipeline before being output to the environment. For example, a feature pipeline could normalize all price values, make a time series stationary, add a moving average column, and remove an unnecessary column, all before the observation is returned to the agent.", "Feature pipelines can be initialized with an arbitrary number of comma-separated transformers. Each FeatureTransformer needs to be initialized with the set of columns to transform, or if nothing is passed, all input columns will be transformed.", "Each feature transformer has a transform method, which will transform a single observation (a pandas.DataFrame) from a larger data set, keeping any necessary state in memory to transform the next frame. For this reason, it is often necessary to reset the FeatureTransformer periodically. This is done automatically each time the parent FeaturePipeline or Exchange is reset.", "Let\u2019s create an example pipeline and add it to our existing exchange.", "This feature pipeline normalizes the price values between 0 and 1, before adding some moving average columns and making the entire time series stationary by fractionally differencing consecutive values.", "Action schemes define the action space of the environment and convert an agent\u2019s actions into executable trades. For example, if we were using a discrete action space of 3 actions (0 = hold, 1 = buy 100%, 2 = sell 100%), our learning agent does not need to know that returning an action of 1 is equivalent to buying an instrument. Rather, our agent needs to know the reward for returning an action of 1 in specific circumstances, and can leave the implementation details of converting actions to trades to the ActionScheme.", "Each action scheme has a get_trade method, which will transform the agent\u2019s specified action into an executable Trade. It is often necessary to store additional state within the scheme, for example to keep track of the currently traded position. This state should be reset each time the action scheme\u2019s reset method is called, which is done automatically when the parent TradingEnvironment is reset.", "Reward schemes receive the trade taken at each time step and return a float, corresponding to the benefit of that specific action. For example, if the action taken this step was a sell that resulted in positive profits, our RewardScheme could return a positive number to encourage more trades like this. On the other hand, if the action was a sell that resulted in a loss, the scheme could return a negative reward to teach the agent not to make similar actions in the future.", "A version of this example algorithm is implemented in the SimpleProfit component, however more complex strategies can obviously be used instead.", "Each reward scheme has a get_reward method, which takes in the trade executed at each time step and returns a float corresponding to the value of that action. As with action schemes, it is often necessary to store additional state within a reward scheme for various reasons. This state should be reset each time the reward scheme\u2019s reset method is called, which is done automatically when the parent TradingEnvironment is reset.", "The simple profit scheme returns a reward of -1 for not holding a trade, 1 for holding a trade, 2 for purchasing an instrument, and a value corresponding to the (positive/negative) profit earned by a trade if an instrument was sold.", "Up until this point, we haven\u2019t seen the \u201cdeep\u201d part of the deep reinforcement learning framework. This is where learning agents come in. Learning agents are where the math (read: magic) happens.", "At each time step, the agent takes the observation from the environment as input, runs it through its underlying model (a neural network most of the time), and outputs the action to take. For example, the observation might be the previous open, high, low, and close price from the exchange. The learning model would take these values as input and output a value corresponding to the action to take, such as buy, sell, or hold.", "It is important to remember the learning model has no intuition of the prices or trades being represented by these values. Rather, the model is simply learning which values to output for specific input values or sequences of input values, to earn the highest reward.", "In this example, we will be using the Stable Baselines library to provide learning agents to our trading strategy, however, the TensorTrade framework is compatible with many reinforcement learning libraries such as Tensorforce, Ray\u2019s RLLib, OpenAI\u2019s Baselines, Intel\u2019s Coach, or anything from the TensorFlow line such as TF Agents.", "It is possible that custom TensorTrade learning agents will be added to this framework in the future, though it will always be a goal of the framework to be interoperable with as many existing reinforcement learning libraries as possible, since there is so much concurrent growth in the space.", "But for now, Stable Baselines is simple and powerful enough for our needs.", "Note: Stable Baselines is not required to use TensorTrade though it is required for this tutorial. This example uses a GPU-enabled Proximal Policy Optimization model with a layer-normalized LSTM perceptron network. If you would like to know more about Stable Baselines, you can view the Documentation.", "I will also quickly cover the Tensorforce library to show how simple it is to switch between reinforcement learning frameworks.", "If you would like to know more about Tensorforce agents, you can view the Documentation.", "A TradingStrategy consists of a learning agent and one or more trading environments to tune, train, and evaluate on. If only one environment is provided, it will be used for tuning, training, and evaluating. Otherwise, a separate environment may be provided at each step.", "Don\u2019t worry if you don\u2019t understand the strategy initialization just yet, it will be explained in more detail later.", "Now that we know about each component that makes up a TradingStrategy, let\u2019s build and evaluate one.", "For a quick recap, a TradingStrategy is made up of a TradingEnvironment and a learning agent. A TradingEnvironment is a gym environment that takes an Exchange, an ActionScheme, a RewardScheme, and an optional FeaturePipeline, and returns observations and rewards that the learning agent can be trained and evaluated on.", "The first step is to create a TradingEnvironment using the components outlined above.", "Simple enough, now environment is a gym environment that can be used by any compatible trading strategy or learning agent.", "Now that the environment is set up, it\u2019s time to create our learning agent. Again, we will be using Stable Baselines for this, but feel free to drop in any other reinforcement learning agent here.", "Since we are using StableBaselinesTradingStrategy, all we need to do is provide a model type and a policy type for the underlying neural network to be trained. For this example, we will be using a simple proximal policy optimization (PPO) model and a layer-normalized LSTM policy network.", "For more examples of model and policy specifications, see the Stable Baselines Documentation.", "Creating our trading strategy is as simple as plugging in our agent and the environment.", "Then to train the strategy (i.e. train the agent on the current environment), all we need to do is call strategy.run() with the total number of steps or episodes you\u2019d like to run.", "And voila! Three hours and thousands of print statements later, you will see the results of how your agent has done!", "If this feedback loop is a bit slow for you, you can pass a callback function to run, which will be called at the end of each episode. The callback function will pass in a data frame containing the agent\u2019s performance that episode, and expects a bool in return. If True, the agent will continue training, otherwise, the agent will stop and return its overall performance.", "All trading strategies are capable of saving their agent to a file, for later restoring. The environment is not saved, as it does not have state that we care about preserving. To save our TensorflowTradingStrategy to a file, we just need to provide the path of the file to our strategy.", "To restore the agent from the file, we first need to instantiate our strategy, before calling restore_agent.", "Our strategy is now restored back to its previous state, and ready to be used again.", "Sometimes a trading strategy will require tuning a set of hyper-parameters, or features, on an environment to achieve maximum performance. In this case, each TradingStrategy provides an optionally implementable tune method.", "Tuning a model is similar to training a model, however in addition to adjusting and saving the weights and biases of the best performing model, the strategy also adjusts and persists the hyper-parameters that produced that model.", "In this case, the agent will be trained for 10 episodes, with a different set of hyper-parameters each episode. The best set will be saved within the strategy, and used any time strategy.run() is called thereafter.", "Now that we\u2019ve tuned and trained our agent, it\u2019s time to see how well it performs. To evaluate our strategy\u2019s performance on unseen data, we will need to run it on a new environment backed by such data.", "When complete, strategy.run returns a Pandas data frame of the agent\u2019s performance, including the net worth and balance of the agent at each time step.", "Once you\u2019ve built a profitable trading strategy, trained an agent to trade it properly, and ensured its \u201cgeneralize-ability\u201d to new data sets, all there is left to do is profit. Using a live exchange such as CCXTExchange, you can plug your strategy in and let it run!", "While the gambler in you may enjoy starting a strategy and letting it run without bounds, the more risk averse of you can use a trade_callback, which will be called each time the strategy makes a trade. This callback function, similar to the episode callback, will pass in a data frame containing the agent\u2019s overall performance, and expects a bool in return. If True, the agent will continue trading, otherwise, the agent will stop and return its performance over the session.", "Passing steps=0 instructs the strategy to run until otherwise stopped.", "That\u2019s all there is to it! As you can see, it is quite simple to build complex trading strategies using simple components and deep reinforcement learning. So what are you waiting for? Dive in, get your hands dirty, and see what\u2019s possible using TensorTrade.", "Currently, the framework is in its early stages. The focus so far has been to get a working prototype, with all of the necessary building blocks to create highly profitable strategies. The next step is to build a roadmap for the future, and decide which upcoming building blocks are important to the community.", "Soon, we will see highly informative visualizations of the environments added to the framework, as well as much more in-depth strategies on more exchanges, trading more instruments.", "The sky is the limit. The groundwork (i.e. framework) has been laid, it\u2019s now up to the community to decide what\u2019s next. I hope that you will be a part of it.", "TensorTrade is a powerful framework capable of building highly modular, high performance trading systems. It is fairly simple and easy to experiment with new trading and investment strategies, while allowing you to leverage components from one strategy in another. But don\u2019t take my word for it, create a strategy of your own and start teaching your robots to take over the world!", "While this tutorial should be enough to get you started, there is still quite a lot more to learn if you want to create a profitable trading strategy. I encourage you to head over to the Github and dive into the codebase, or take a look at our documentation at tensortrade.org. There is also quite an active Discord community with nearly 1000 total members, so if you have questions, feedback, or feature requests, feel free to drop them there!", "I\u2019ve gotten the project to a highly usable state. Though, my time is limited, and I believe there are many of you out there who could make valuable contributions to the open source codebase. So if you are a developer or data scientist with an interest in building state-of-the-art trading systems, I\u2019d love to see you open a pull request, even if its just a simple test case!", "Others have asked how they can contribute to the project without writing code. There are currently three ways that you can do that.", "Thanks for reading! As always, all of the code for this tutorial can be found on my GitHub. Leave a comment below if you have any questions or feedback, I\u2019d love to hear from you! I can also be reached on Twitter at @notadamking.", "You can also sponsor me on Github Sponsors or Patreon via the links below.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Co-Founder, Software Architect, and Deep Learning Enthusiast \u2014 Judge me by my age, I dare you https://www.patreon.com/notadamking"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fa5e91163f315&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrade-smarter-w-reinforcement-learning-a5e91163f315&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrade-smarter-w-reinforcement-learning-a5e91163f315&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrade-smarter-w-reinforcement-learning-a5e91163f315&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrade-smarter-w-reinforcement-learning-a5e91163f315&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----a5e91163f315--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a5e91163f315--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@notadamking?source=post_page-----a5e91163f315--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@notadamking?source=post_page-----a5e91163f315--------------------------------", "anchor_text": "Adam King"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6e3a5234f1cc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrade-smarter-w-reinforcement-learning-a5e91163f315&user=Adam+King&userId=6e3a5234f1cc&source=post_page-6e3a5234f1cc----a5e91163f315---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa5e91163f315&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrade-smarter-w-reinforcement-learning-a5e91163f315&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa5e91163f315&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrade-smarter-w-reinforcement-learning-a5e91163f315&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/questions-96667b06af5", "anchor_text": "rules and guidelines"}, {"url": "https://towardsdatascience.com/readers-terms-b5d780a700a4", "anchor_text": "Reader Terms"}, {"url": "https://www.nature.com/articles/d41586-019-02156-9", "anchor_text": "Winning high stakes poker tournaments"}, {"url": "https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/", "anchor_text": "out-playing world-class StarCraft players"}, {"url": "https://www.tesla.com/autopilot", "anchor_text": "autonomously driving Tesla\u2019s futuristic sports cars"}, {"url": "https://deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii", "anchor_text": "https://deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii"}, {"url": "https://towardsdatascience.com/creating-bitcoin-trading-bots-that-dont-lose-money-2e7165fb0b29", "anchor_text": "progress on that front"}, {"url": "https://discord.gg/ZZ7BGWh", "anchor_text": "RL trading Discord"}, {"url": "http://tensortrade.org", "anchor_text": "TensorTrade"}, {"url": "https://en.wikipedia.org/wiki/Machine_learning", "anchor_text": "machine learning"}, {"url": "https://en.wikipedia.org/wiki/Software_agent", "anchor_text": "software agents"}, {"url": "https://en.wikipedia.org/wiki/Action_selection", "anchor_text": "actions"}, {"url": "https://www.researchgate.net/publication/319121340_Enabling_Cognitive_Smart_Cities_Using_Big_Data_and_Machine_Learning_Approaches_and_Challenges", "anchor_text": ""}, {"url": "https://colab.research.google.com/drive/1hzbugXnkGWO6l3vpQ0bSqnJJxBGiogar", "anchor_text": "Google Colab"}, {"url": "https://github.com/notadamking/tensortrade/blob/master/examples/TensorTrade_Tutorial.ipynb", "anchor_text": "Github"}, {"url": "https://github.com/notadamking/tensortrade.git#egg=tensortrade[tf,tensorforce,baselines,ccxt,fbm", "anchor_text": "https://github.com/notadamking/tensortrade.git#egg=tensortrade[tf,tensorforce,baselines,ccxt,ta,fbm"}, {"url": "https://www.binance.com/en/register?ref=PDOJ9XB8", "anchor_text": "Binance"}, {"url": "http://coinbase-consumer.sjv.io/c/1949163/626313/9251", "anchor_text": "Coinbase"}, {"url": "https://stable-baselines.readthedocs.io/en/master", "anchor_text": "Stable Baselines"}, {"url": "https://stable-baselines.readthedocs.io/en/master", "anchor_text": "Stable Baselines"}, {"url": "https://tensorforce.readthedocs.io/en/0.4.4", "anchor_text": "Tensorforce"}, {"url": "https://ray.readthedocs.io/en/latest/rllib.html", "anchor_text": "Ray\u2019s RLLib"}, {"url": "https://github.com/openai/baselines", "anchor_text": "OpenAI\u2019s Baselines"}, {"url": "https://github.com/NervanaSystems/coach", "anchor_text": "Intel\u2019s Coach"}, {"url": "https://github.com/tensorflow/agents", "anchor_text": "TF Agents"}, {"url": "https://stable-baselines.readthedocs.io/en/master/", "anchor_text": "Documentation"}, {"url": "https://tensorforce.readthedocs.io/en/0.4.4", "anchor_text": "Tensorforce"}, {"url": "https://tensorforce.readthedocs.io/en/0.4.4", "anchor_text": "Tensorforce"}, {"url": "https://tensorforce.readthedocs.io/en/0.4.4", "anchor_text": "Documentation"}, {"url": "https://stable-baselines.readthedocs.io/en/master", "anchor_text": "Documentation"}, {"url": "https://towardsdatascience.com/using-reinforcement-learning-to-trade-bitcoin-for-massive-profit-b69d0e8f583b", "anchor_text": "previous article"}, {"url": "https://github.com/notadamking/tensortrade", "anchor_text": "Github"}, {"url": "https://tensortrade.org", "anchor_text": "tensortrade.org"}, {"url": "https://discord.gg/ZZ7BGWh", "anchor_text": "Discord community"}, {"url": "https://github.com/notadamking/tensortrade", "anchor_text": "Github"}, {"url": "https://gitcoin.co/profile/notadamking/active", "anchor_text": "Gitcoin smart contracts"}, {"url": "https://www.blockchain.com/btc/address/1Lc47bhYvdyKGk1qN8oBHdYQTkbFLL3PFw", "anchor_text": "Bitcoin"}, {"url": "https://www.blockchain.com/eth/address/0x9907A0cF64Ec9Fbf6Ed8FD4971090DE88222a9aC", "anchor_text": "Ethereum"}, {"url": "https://gitcoin.co/profile/notadamking/active", "anchor_text": "Gitcoin smart contracts"}, {"url": "https://github.com/users/notadamking/sponsorship", "anchor_text": "Github"}, {"url": "https://github.com/notadamking/tensortrade", "anchor_text": "GitHub"}, {"url": "https://twitter.com/notadamking", "anchor_text": "Twitter"}, {"url": "https://github.com/users/notadamking/sponsorship", "anchor_text": "Github Sponsors"}, {"url": "https://www.patreon.com/join/notadamking", "anchor_text": "Patreon"}, {"url": "https://github.com/users/notadamking/sponsorship", "anchor_text": "Sponsor @notadamking on GitHub SponsorsHi, I'm Adam. I'm a developer, writer, and entrepreneur, specifically interested in financial applications of deep\u2026github.com"}, {"url": "https://patreon.com/notadamking", "anchor_text": "Adam King is creating World Changing Content | PatreonHi, I\u2019m Adam. I\u2019m a developer, writer, and entrepreneur, specifically interested in financial applications of deep\u2026patreon.com"}, {"url": "https://medium.com/@jonathan_hui/rl-introduction-to-deep-reinforcement-learning-35c25e04c199.", "anchor_text": "https://medium.com/@jonathan_hui/rl-introduction-to-deep-reinforcement-learning-35c25e04c199."}, {"url": "https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html#reinforce.", "anchor_text": "https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html#reinforce."}, {"url": "https://amzn.to/2XANX1X", "anchor_text": "Martin, Robert C. Clean Code: a Handbook of Agile Software Craftsmanship. Prentice Hall, 2010"}, {"url": "https://amzn.to/2J6YCrW", "anchor_text": "Prado Marcos Lo\u0301pez de. Advances in Financial Machine Learning. Wiley, 2018"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----a5e91163f315---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----a5e91163f315---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/programming?source=post_page-----a5e91163f315---------------programming-----------------", "anchor_text": "Programming"}, {"url": "https://medium.com/tag/money?source=post_page-----a5e91163f315---------------money-----------------", "anchor_text": "Money"}, {"url": "https://medium.com/tag/investing?source=post_page-----a5e91163f315---------------investing-----------------", "anchor_text": "Investing"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa5e91163f315&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrade-smarter-w-reinforcement-learning-a5e91163f315&user=Adam+King&userId=6e3a5234f1cc&source=-----a5e91163f315---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa5e91163f315&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrade-smarter-w-reinforcement-learning-a5e91163f315&user=Adam+King&userId=6e3a5234f1cc&source=-----a5e91163f315---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa5e91163f315&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrade-smarter-w-reinforcement-learning-a5e91163f315&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a5e91163f315--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fa5e91163f315&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrade-smarter-w-reinforcement-learning-a5e91163f315&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----a5e91163f315---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----a5e91163f315--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----a5e91163f315--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----a5e91163f315--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----a5e91163f315--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----a5e91163f315--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----a5e91163f315--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----a5e91163f315--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----a5e91163f315--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@notadamking?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@notadamking?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Adam King"}, {"url": "https://medium.com/@notadamking/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "2.7K Followers"}, {"url": "https://www.patreon.com/notadamking", "anchor_text": "https://www.patreon.com/notadamking"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6e3a5234f1cc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrade-smarter-w-reinforcement-learning-a5e91163f315&user=Adam+King&userId=6e3a5234f1cc&source=post_page-6e3a5234f1cc--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F77c80e888452&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrade-smarter-w-reinforcement-learning-a5e91163f315&newsletterV3=6e3a5234f1cc&newsletterV3Id=77c80e888452&user=Adam+King&userId=6e3a5234f1cc&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}