{"url": "https://towardsdatascience.com/building-a-vocal-emotion-sensor-with-deep-learning-bedd3de8a4a9", "time": 1682997691.660788, "path": "towardsdatascience.com/building-a-vocal-emotion-sensor-with-deep-learning-bedd3de8a4a9/", "webpage": {"metadata": {"title": "Building a Vocal Emotion Sensor with Deep Learning | by Alex Muhr | Towards Data Science", "h1": "Building a Vocal Emotion Sensor with Deep Learning", "description": "Human expression is multi-faceted and complex. For example, a speaker not only communicates through words, but also through cadence, intonation, facial expressions, and body language. It\u2019s why we\u2026"}, "outgoing_paragraph_urls": [{"url": "https://smartlaboratory.org/ravdess", "anchor_text": "RAVDESS", "paragraph_index": 3}, {"url": "https://tspace.library.utoronto.ca/handle/1807/24487", "anchor_text": "TESS", "paragraph_index": 3}, {"url": "http://kahlan.eps.surrey.ac.uk/savee/Download.html", "anchor_text": "SAVEE", "paragraph_index": 3}, {"url": "http://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/", "anchor_text": "here", "paragraph_index": 5}], "all_paragraphs": ["Human expression is multi-faceted and complex. For example, a speaker not only communicates through words, but also through cadence, intonation, facial expressions, and body language. It\u2019s why we prefer to hold business meetings in person rather than over conference calls, and why conference calls are preferred over emails or texting. The closer we are the more communication bandwidth exists.", "Voice recognition software has advanced greatly in recent years. This technology now does an excellent job of recognizing phonetic sounds and piecing these together to reproduce spoken words and sentences. However, simply translating speech to text does not fully encapsulate a speaker\u2019s message. Facial expressions and body language aside, text is highly limited in its capacity to capture emotional intent compared to audio.", "Initially I chose to build a vocal emotion sensor because it seemed like a fun and interesting project to work on. Thinking more about the problem though, I realized emotion sensing via audio has some really interesting applications. Imagine if your smart-home device could play songs to match your emotions, for example by playing uplifting songs when you\u2019re sad. Customer service departments could use emotion detection to train employees, or measure whether customers become more happy over the course of a service call.", "The datasets I used to build my emotion classifier were the RAVDESS, TESS, and SAVEE which are all freely available to the public (SAVEE requires a very simple registration). These datasets contain audio files across seven common categories: neutral, happy, sad, angry, fearful, disgusted, and surprised. Combined I had access to over 160 minutes of audio across 4,500 labeled audio files produced by 30 actors and actresses. The files generally consist of the actor or actress speaking a short simple phrase with a specific emotional intent.", "Next I had to find useful features that could be extracted from the audio. Initially I thought to use short-time Fourier transforms to extract frequency information. However, some research into the topic revealed that Fourier transforms are quite flawed when it comes to voice recognition applications. The reason why is that Fourier transforms, although an excellent physical representation of sound, do not represent how human\u2019s perceive sound.", "A better way to extract features from audio is to use Mel Frequency Cepstral Coefficients, or MFCCs for short. A nice explanation of how MFCCs are derived from audio is provided here. MFCCs attempt to represent audio in a way that is better aligned to human perception.", "To derive MFCCs from audio requires decisions on how many frequency bins to use and how wide of a temporal step to segment by. These decisions determine the granularity of the output MFCC data. A standard practice for voice recognition applications is to apply 26 frequency bins between 20Hz \u2014 20kHz and only use the first 13 for classification. Most useful information is in lower frequency ranges, and including higher frequency ranges often leads to worse performance. For temporal step size, values between 10 to 100 milliseconds are common. I chose to use 25 milliseconds.", "Once derived MFCCs can be plotted on a heatmap and used to visualize audio. Doing so does not reveal any obvious differences between emotional categories. This isn\u2019t so much due to a lack of patterns, it\u2019s because humans are not trained to recognize these subtle emotional differences visually. However, from these heatmaps it is quite easy to see differences between male and female speakers.", "By deriving MFCCs, audio classification problems are essentially translated to image recognition problems. As such, the tools, algorithms, and techniques that are highly effective in the realm of image recognition are also highly effective in audio classification. To tackle the emotion classification problem, I chose to use a convolutional neural network (CNN) as these have been shown to be effective at both image and audio recognition.", "Before training my CNN, I randomly assigned files in my dataset to training or test sets with an 80/20 split. I then performed a number of preprocessing steps on the training files. The process for each file was:", "Upon completion of preprocessing I had generated 75,000 labeled 0.4s windows for training, each window represented by a 13x16 array. I then trained my CNN on this data for 25 epochs.", "To benchmark the model on the test set, I applied a process workflow similar to that used to create training data. The process for each file in the test set was:", "Applying this process to all 889 files in the test set produced an overall accuracy score of 83%. I highly doubt that I would be able to label these files myself with anything close to 83% accuracy. Accuracy for each specific emotion is shown on the barchart below.", "This blog post may make it seem as though building, training, and testing the model was simple and straightforward. I can assure you that this was very much not the case. Before achieving 83% accuracy, there were many versions of the model that performed quite poorly. In one iteration I did not scale my inputs correctly which led to predicting nearly every file in the test set as \u2018surprised\u2019. So what did I learn from this experience?", "First off, this project was a great demonstration of how simply collecting more data can greatly improve results. My first successful iteration of the model used only the RAVDESS dataset, about 1400 audio files. The best accuracy I could achieve with this dataset alone was 67%. To get to 83% accuracy all I did was increase the size of my dataset to 4500 files.", "Second, I learned that for audio classification data preprocessing is critical. Raw audio, and even short-time fourier transforms, are almost completely useless. As I learned the hard way, proper scaling can make or break a model. Failure to remove silence is another simple pitfall. Once audio has been properly transformed into informative features, building and training a deep learning model is comparatively easy.", "To wrap things up, building a classification model for voice emotion detection was a challenging but rewarding experience. In the near future I\u2019ll likely revisit this project to expand upon it. Some things I\u2019d like to do include: test the model against a wider range of inputs, adapt the model to a greater range of emotions, and deploy a model for to the cloud for live emotion detection.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Using data science to uncover new perspectives, solve practical problems, and have some fun along the way."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fbedd3de8a4a9&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-vocal-emotion-sensor-with-deep-learning-bedd3de8a4a9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-vocal-emotion-sensor-with-deep-learning-bedd3de8a4a9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-vocal-emotion-sensor-with-deep-learning-bedd3de8a4a9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-vocal-emotion-sensor-with-deep-learning-bedd3de8a4a9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----bedd3de8a4a9--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----bedd3de8a4a9--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@acmuhr?source=post_page-----bedd3de8a4a9--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@acmuhr?source=post_page-----bedd3de8a4a9--------------------------------", "anchor_text": "Alex Muhr"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd932be500623&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-vocal-emotion-sensor-with-deep-learning-bedd3de8a4a9&user=Alex+Muhr&userId=d932be500623&source=post_page-d932be500623----bedd3de8a4a9---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbedd3de8a4a9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-vocal-emotion-sensor-with-deep-learning-bedd3de8a4a9&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbedd3de8a4a9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-vocal-emotion-sensor-with-deep-learning-bedd3de8a4a9&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://smartlaboratory.org/ravdess", "anchor_text": "RAVDESS"}, {"url": "https://tspace.library.utoronto.ca/handle/1807/24487", "anchor_text": "TESS"}, {"url": "http://kahlan.eps.surrey.ac.uk/savee/Download.html", "anchor_text": "SAVEE"}, {"url": "http://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/", "anchor_text": "here"}, {"url": "https://github.com/alexmuhr/Voice_Emotion", "anchor_text": "Github"}, {"url": "https://www.linkedin.com/in/alexander-muhr/", "anchor_text": "LinkedIn"}, {"url": "https://www.datascienceodyssey.com", "anchor_text": "Personal Blog"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----bedd3de8a4a9---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----bedd3de8a4a9---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----bedd3de8a4a9---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/voice-recognition?source=post_page-----bedd3de8a4a9---------------voice_recognition-----------------", "anchor_text": "Voice Recognition"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----bedd3de8a4a9---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbedd3de8a4a9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-vocal-emotion-sensor-with-deep-learning-bedd3de8a4a9&user=Alex+Muhr&userId=d932be500623&source=-----bedd3de8a4a9---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbedd3de8a4a9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-vocal-emotion-sensor-with-deep-learning-bedd3de8a4a9&user=Alex+Muhr&userId=d932be500623&source=-----bedd3de8a4a9---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbedd3de8a4a9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-vocal-emotion-sensor-with-deep-learning-bedd3de8a4a9&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----bedd3de8a4a9--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fbedd3de8a4a9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-vocal-emotion-sensor-with-deep-learning-bedd3de8a4a9&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----bedd3de8a4a9---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----bedd3de8a4a9--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----bedd3de8a4a9--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----bedd3de8a4a9--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----bedd3de8a4a9--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----bedd3de8a4a9--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----bedd3de8a4a9--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----bedd3de8a4a9--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----bedd3de8a4a9--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@acmuhr?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@acmuhr?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Alex Muhr"}, {"url": "https://medium.com/@acmuhr/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "179 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd932be500623&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-vocal-emotion-sensor-with-deep-learning-bedd3de8a4a9&user=Alex+Muhr&userId=d932be500623&source=post_page-d932be500623--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F459ab5d3c363&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-vocal-emotion-sensor-with-deep-learning-bedd3de8a4a9&newsletterV3=d932be500623&newsletterV3Id=459ab5d3c363&user=Alex+Muhr&userId=d932be500623&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}