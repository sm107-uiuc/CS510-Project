{"url": "https://towardsdatascience.com/the-math-behind-gradient-descent-and-backpropagation-code-example-in-java-using-deeplearning4j-f7340f137ca5", "time": 1683016047.545564, "path": "towardsdatascience.com/the-math-behind-gradient-descent-and-backpropagation-code-example-in-java-using-deeplearning4j-f7340f137ca5/", "webpage": {"metadata": {"title": "The math behind Gradient Descent and Backpropagation | by Enghin Omer | Towards Data Science", "h1": "The math behind Gradient Descent and Backpropagation", "description": "In this article I give an introduction of two algorithms: the Gradient Descent and Backpropagation. I give an intuition on how they work but also a detailed presentation of the math behind them. At\u2026"}, "outgoing_paragraph_urls": [{"url": "https://github.com/enghinomer/deep-learning/tree/master/NN", "anchor_text": "here", "paragraph_index": 52}], "all_paragraphs": ["In this article I give an introduction of two algorithms: the Gradient Descent and Backpropagation. I give an intuition on how they work but also a detailed presentation of the math behind them. At the end of this article I present an implementation of this algorithms in Java using Deeplearning4J. However, its purpose is not to introduce the Deeplearning4J framework but to implement all the concepts presented in the article. After reading this article you should have a clear understanding of why and how those two algorithms work and also be able to implement them by yourself.", "There are often cases when we want to minimize a function, let\u2019s say C. One way of doing this is to compute the derivatives and try to find the extremum point of C. If C has a small number of variables this is a viable solution, but if C has a large number of variables, as it is often the case for machine learning algorithms, this is not a practical solution anymore and we need another approach.", "Let\u2019s assume that our function looks like in Fig.1 and a random point is chosen the red point. Our goal is to reach the minimum, or the bottom, of that function. So, from our initial position, which direction shall we go to in order to reach the bottom of the valley? Intuitively, the direction is the one that points steepest down. If we go that direction a little bit, then look around and find again the direction that points steepest down and take again a little step into that direction and follow this procedure many times we\u2019ll eventually reach the bottom of the valley. But how can we find the direction of steepest descent?", "To find our desired direction, the notion of directional derivative will be helpful. Let\u2019s assume that our function C has 2 variables x and y and we want to know the instantaneous rate of change of C when moving in the direction of", "The instantaneous rate of change of C along the x axis is", "and similarly for y. The vector v can be thought of as v\u2081 units to the x direction and v\u2082 units to the y direction. Therefore, the directional derivative of C along the direction of vector v is", "However, this is not the slope of C in the direction of v. To get the slope the norm of vector v must be 1, since v can be scaled up and that will reflect in the value of EQ(1).", "Let\u2019s come back to our initial question: if we are at the point w=(a,b), what is vector v so that it points to the steepest descent direction? Well, the instantaneous rate of change of C when moving in the direction of v has to be negative and maximum in absolute terms, it is the steepest descent. So, the problem reduces to finding:", "We have a dot product between two vectors, \u2207 C(a,b) and v, and this is computed by multiplying the length of the projection of vector v onto \u2207 C(a,b) and the length of \u2207 C(a,b) (if the projection points to the opposite direction of \u2207C(a, b) then its length has negative value). So, what should vector v be to minimize the product? If you swing v in all directions, it\u2019s easy to see that the projection of v is maximum when v points to the same direction as \u2207C(a, b) and the product is minimum when v points in the opposite direction of \u2207C(a,b). So, the direction of steepest descent is the opposite direction of \u2207C(a, b). This is the vector v we were looking for!", "All left to do to approach the minimum is to update until convergence our position w as follows:", "where \u03b7 is called the learning rate. This is the Gradient Descent algorithm. This works not only for two variate functions but for any number of variables. To make gradient descent work correctly, \u03b7 has to be carefully chosen. If \u03b7 is too large the steps we take when updating w will be large and we might \u201cmiss\u201d the minimum. On the other hand if \u03b7 is too small the gradient descent algorithm will work slowly. A limitation of gradient descent algorithm is that it is susceptible to local minimum but many machine learning cost functions are convex and there\u2019s no local minimum.", "In machine learning to measure the accuracy of a model a cost function is used. One of those functions is the Mean Squared Error or MSE:", "In this equation, w and b denotes the set of all weights and biases in the model, m is the number of training observations, \u0177(x) is the value outputted by the model and y(x) is the true prediction of x. C is a sum of squared terms and we want to minimize the value of it, i.e., C(w,b) \u2248 0.", "To minimize the value of C(w,b) we can apply gradient descent algorithm. To do so, we can start with some initial values for w and b and then repeatedly update them until it hopefully converges to a value that minimizes C(w,b) as described above in EQ(3). However, there\u2019s a drawback with this approach. At each iteration we have to compute the gradient \u2207C. But to compute the \u2207C we have to compute the gradient \u2207C\u2093 for each training example and then average them. When there is a large number of training observation the algorithm may be slow. To speed up things, another approach called stochastic gradient descent can be used.", "The idea of stochastic gradient descent is to use only one training observation at a time to update the parameters value. It works as follows:", "This algorithm doesn\u2019t scan all the training observations to update the model parameters, but it tries to fit one training example at a time. Stochastic gradient descent will probably not converge at the global minimum but it can get close enough to be a good approximation.", "Another approach is instead of using one training example, as in stochastic gradient descent, or all training examples, as in batch gradient descent, to use some in-between number of observation. This is called mini-batch gradient descent.", "Let\u2019s say we randomly chose k training examples, x\u00b9, x\u00b2 \u2026 x\u1d4f. This a mini- batch. Having a large enough mini-batch, the average value of gradients in the mini-batch will approximate the one over the entire set of training examples, that is:", "The algorithm works by randomly picking a mini-batch and updating the parameters using them:", "This is repeated until all the training examples are used, thus an epoch of training is completed. After that we can start with a new training epoch following the same procedure.", "A common approach for classification problems is to use the sigmoid function.", "It has weights for each input and an overall bias, b. A very nice property of this function is that \u03c3(w\u1d57 \u00b7 x + b) takes values in the interval (0,1), so when its output is greater than 0.5 we can classify the input as belonging to class 1 or 0 otherwise.", "However, if we want to use three features including all the quadratic terms we have to compute", "which has six features. Here we take all two-element combinations of features. More generally, to compute the number of polynomial terms we can use the combination function with repetition:", "Neural networks is another option when creating complex models with many features. The architecture of a neural network looks like following:", "A neural network has many layers each layer having a bunch of neurons. The first layer is called the input layer and the neurons within this layer are called input neurons. The last layer is called the output layer and the neurons output neurons. The layers in the middle are called hidden layers. The output of one layer is used as the input for the next layer. These networks are called feedforward neural networks.", "The sigmoid neuron works as follows. Each input x1 , x2 \u2026xn to the neuron has a corresponding weight w1 , w2 \u2026wn . The neuron has also an overall bias b. The output of the neuron is \u03c3(w T \u00b7 x + b) and this is the activation of the neuron. This output value will serve as input for the next layer.", "The weights are denoted w\u02e1\u2c7c\u2096 meaning the weight from the k\u1d57\u02b0 neuron in the layer (l-1) to the j\u1d57\u02b0 neuron in the layer l. The b\u02e1\u2c7c is the bias of the j\u1d57\u02b0 neuron in the l\u1d57\u02b0 layer. The activation of the j\u1d57\u02b0 neuron in the l\u1d57\u02b0 layer is a\u02e1\u2c7c. For the values corresponding to the last layer superscript L is used.", "For example, in a problem of classifying pictures, the input layer may consist of all pixels grayscale taking values between 0.0 and 1.0. To compute the activation value of the last neuron in layer 2 as in figure 4, we compute:", "How can we train a neural network? We can use gradient descent to minimize the cost function. But to perform gradient descent we have to compute \u2202C/\u2202w and \u2202C/\u2202b. Next, an algorithm that computes these partial derivatives is presented.", "Backpropagation is the algorithm used to compute the gradient of the cost function, that is the partial derivatives \u2202C/\u2202w\u02e1\u2c7c\u2096 and \u2202C/\u2202b\u02e1\u2c7c. To define the cost function we can use EQ(4):", "where the second term is is the vector of activation values for input x. We know that", "where the sum is over the k neurons in the (l \u2212 1)\u1d57\u02b0 layer.", "We can define a weight matrix W\u02e1 which are the weights connecting to the l\u1d57\u02b0 layer and the weight w\u02e1\u2c7c\u2096 corresponds to the entry in W\u02e1 with row j and column k. Similarly, we can define a bias vector b\u02e1 containing all the biases in layer l, and the vector of activations:", "It is also useful to compute the intermediate value", "We also define the error of neuron j in layer l, as \u03b4\u02e1\u2c7c . We know from gradient descent algorithm that when the partial derivative is 0 or close to zero we approached the minimum otherwise the steeper the slope the more incorrect we are. With this consideration we can define", "Being equipped with all these tools we can define the error in the output layer as follows:", "and if we are using the quadratic function defined in EQ (9) for one example, then", "We can define \u2207\u2090C the vector containing partial derivatives of C with respect to each activation in the last layer then we can write the vectorized form of (16):", "Let\u2019s now try to compute the error of any neuron. In particular", "So, using this formula, if we know \u03b4 for layer l we can compute \u03b4 for l-1, l-2 etc. That is we backpropagate the error, thus the name of the algorithm. We already saw in EQ(17) how to compute \u03b4 L .", "This is great news since we already saw how to compute \u03b4\u02e1\u2c7c for any l. Similarly, we can compute", "Equations (23) and (24) shows how to compute the gradient of the cost function.", "In the previous section I described the backpropagation algorithm using the quadratic cost function (9). Another cost function used for classification problems is the Cross-entropy function.", "Let\u2019s break this function into pieces and see why it makes sense. If y=1 then the second term cancels out and \u2212 ln a remains. If we look at the graph of this function we see that as a approaches 1 the value of the function approaches 0 and closer a is to 0 the function goes to infinity. That is the closer a is to the true value of y the smaller is the cost. Similarly for y=0. The remaining term is \u2212 ln(1 \u2212 a). As a approaches 1 the value of the function approaches infinity and closer a is to 0 the function goes toward zero. The sum over j in the Cross entropy function means the sum is over all neurons in the output layer.", "In practice, the cross-entropy cost seems to be most often used instead of the quadratic cost function. We\u2019ll see in a moment why that\u2019s the case. We presented earlier the backpropagation algorithm. How does it change for this new function? Equation (16) gave us a way of computing the error in output layer and we also saw how it would look like for the quadratic cost function. Let\u2019s now calculate it for the cross-entropy function.", "It seems that the only difference with the quadratic function is that we got rid of the second term in EQ(16), the sigma prime. How important is this? If we look at Fig.3 we can notice that when the sigmoid function approaches 0 or 1 the graph is flattening, thus \u03c3\u2019(x) is getting very close to 0. For example if an output layer neuron will output a value very close to 0 then \u03c3\u2019 will be close to 0. But if the true value is 1, the value of \u03b4 being close to 0 will make the partial derivatives of the corresponding weights very small and so the learning will be very slow and many iterations will be required. But when using the cross entropy function the \u03c3\u2019 term disappears and the learning can be faster. The way of computing partial derivatives with respect to the weights and biases remains the same as described in the previous section, the only difference being the way of computing EQ(26).", "Regularization is a way of overcoming overfitting. There are many techniques of achieving regularization but here I\u2019ll describe only the L2 regularization. The idea of L2 regularization is to penalize large value of weights by adding a regularization term. Then, the regularized cross-entropy becomes:", "The regularization parameter is \u03b1. The biases are not included in the regularization term, so the partial derivatives with respect to the biases do not change and the gradient descent update rule doesn\u2019t change:", "The update rule for the weights becomes:", "As you can notice, the first term shrinks the weight. This is also called weight decay.", "Using the notions described above we\u2019ll try to recognize handwritten digits using the MNIST handwritten digits databse.", "The full code can be found here.", "First, the network is constructed specifying the layer sizes", "To train the network, I implemented the stochastic gradient descent algorithm.", "The SGD method basically takes batches from the training set and calls updateMiniBatch method. In this method the backpropagation method is called and it returns the gradient computed for that mini batch and the biases and weights arrays are updated according to the rule presented in Stochastic Gradient Descent section.", "The backpropagation method computes the equations described above as follows:", "Here are computed EQ (13) and (14) and then using (22) compute the delta terms. Having the deltas, we can compute the partial deivatives using (23)and (24). Here is the code:", "In this article we saw how Gradient Descent and Backpropagation work. I presented the mathematical concepts that describe them and an implementation of those algorithms. You can use the code and try tuning the parameter to see how accurate your classification will be.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Software Developer, passionate about programming, Java, Machine Learning and Math"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Ff7340f137ca5&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-math-behind-gradient-descent-and-backpropagation-code-example-in-java-using-deeplearning4j-f7340f137ca5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-math-behind-gradient-descent-and-backpropagation-code-example-in-java-using-deeplearning4j-f7340f137ca5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-math-behind-gradient-descent-and-backpropagation-code-example-in-java-using-deeplearning4j-f7340f137ca5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-math-behind-gradient-descent-and-backpropagation-code-example-in-java-using-deeplearning4j-f7340f137ca5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----f7340f137ca5--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f7340f137ca5--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://omer-enghin.medium.com/?source=post_page-----f7340f137ca5--------------------------------", "anchor_text": ""}, {"url": "https://omer-enghin.medium.com/?source=post_page-----f7340f137ca5--------------------------------", "anchor_text": "Enghin Omer"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F314dfb35eea0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-math-behind-gradient-descent-and-backpropagation-code-example-in-java-using-deeplearning4j-f7340f137ca5&user=Enghin+Omer&userId=314dfb35eea0&source=post_page-314dfb35eea0----f7340f137ca5---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff7340f137ca5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-math-behind-gradient-descent-and-backpropagation-code-example-in-java-using-deeplearning4j-f7340f137ca5&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff7340f137ca5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-math-behind-gradient-descent-and-backpropagation-code-example-in-java-using-deeplearning4j-f7340f137ca5&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/tagged/getting-started", "anchor_text": "Getting Started"}, {"url": "https://github.com/enghinomer/deep-learning/tree/master/NN", "anchor_text": "here"}, {"url": "https://medium.com/tag/gradient-descent?source=post_page-----f7340f137ca5---------------gradient_descent-----------------", "anchor_text": "Gradient Descent"}, {"url": "https://medium.com/tag/backpropagation?source=post_page-----f7340f137ca5---------------backpropagation-----------------", "anchor_text": "Backpropagation"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----f7340f137ca5---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/java?source=post_page-----f7340f137ca5---------------java-----------------", "anchor_text": "Java"}, {"url": "https://medium.com/tag/getting-started?source=post_page-----f7340f137ca5---------------getting_started-----------------", "anchor_text": "Getting Started"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff7340f137ca5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-math-behind-gradient-descent-and-backpropagation-code-example-in-java-using-deeplearning4j-f7340f137ca5&user=Enghin+Omer&userId=314dfb35eea0&source=-----f7340f137ca5---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff7340f137ca5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-math-behind-gradient-descent-and-backpropagation-code-example-in-java-using-deeplearning4j-f7340f137ca5&user=Enghin+Omer&userId=314dfb35eea0&source=-----f7340f137ca5---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff7340f137ca5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-math-behind-gradient-descent-and-backpropagation-code-example-in-java-using-deeplearning4j-f7340f137ca5&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f7340f137ca5--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Ff7340f137ca5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-math-behind-gradient-descent-and-backpropagation-code-example-in-java-using-deeplearning4j-f7340f137ca5&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----f7340f137ca5---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----f7340f137ca5--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----f7340f137ca5--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----f7340f137ca5--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----f7340f137ca5--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----f7340f137ca5--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----f7340f137ca5--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----f7340f137ca5--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----f7340f137ca5--------------------------------", "anchor_text": ""}, {"url": "https://omer-enghin.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://omer-enghin.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Enghin Omer"}, {"url": "https://omer-enghin.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "4 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F314dfb35eea0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-math-behind-gradient-descent-and-backpropagation-code-example-in-java-using-deeplearning4j-f7340f137ca5&user=Enghin+Omer&userId=314dfb35eea0&source=post_page-314dfb35eea0--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F314dfb35eea0%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-math-behind-gradient-descent-and-backpropagation-code-example-in-java-using-deeplearning4j-f7340f137ca5&user=Enghin+Omer&userId=314dfb35eea0&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}