{"url": "https://towardsdatascience.com/whats-new-for-transformers-at-the-iclr-2020-conference-4285a4294792", "time": 1683006428.634818, "path": "towardsdatascience.com/whats-new-for-transformers-at-the-iclr-2020-conference-4285a4294792/", "webpage": {"metadata": {"title": "What\u2019s new for Transformers at ICLR 2020? | Towards Data Science", "h1": "What\u2019s new for Transformers at the ICLR 2020 Conference?", "description": "ICLR 2020 has a healthy dose of Transformers, so here's a curated collection of related publications that will help you navigate them."}, "outgoing_paragraph_urls": [{"url": "https://www.zeta-alpha.com", "anchor_text": "Zeta Alpha", "paragraph_index": 1}, {"url": "https://openreview.net/pdf?id=rkxoh24FPH", "anchor_text": "A Mutual Information Maximization Perspective of Language Representation Learning", "paragraph_index": 15}, {"url": "https://openreview.net/pdf?id=ByxY8CNtvr", "anchor_text": "Improving Neural Language Generation with Spectrum Control", "paragraph_index": 15}, {"url": "https://openreview.net/pdf?id=Syx4wnEtvH", "anchor_text": "Large Batch Optimization for Deep Learning: Training BERT in 76 minutes", "paragraph_index": 15}, {"url": "https://openreview.net/pdf?id=S1eZYeHFDS", "anchor_text": "Deep Learning For Symbolic Mathematics", "paragraph_index": 26}, {"url": "http://LOGIC AND THE 2-SIMPLICIAL TRANSFORMER", "anchor_text": "Logic and the 2-Simplicial Transformer", "paragraph_index": 26}, {"url": "https://twitter.com/ZetaVector", "anchor_text": "@zetavector", "paragraph_index": 27}, {"url": "http://bit.ly/3ITDNNE", "anchor_text": "bit.ly/3ITDNNE", "paragraph_index": 29}, {"url": "http://linkedin.com/in/sergicastella", "anchor_text": "linkedin.com/in/sergicastella", "paragraph_index": 29}], "all_paragraphs": ["The International Conference on Learning Representations (ICLR) is one of the most beloved stages for the Machine Learning community. Nowadays, conferences in the field often serve as a quality trademark and a spotlight for publications that already exist in pre-print servers. Still, the volume of work presented is growingly overwhelming, which makes it hard to keep up.", "At Zeta Alpha, we keep a close eye at the forefront of Natural Language Processing (NLP) and Information Retrieval (IR) research. In this spirit, and with the help of our semantic search engine, we\u2019ve curated a selection of 9 papers \u2014 out of more than 40! \u2014 related to Transformers that are making an appearance at ICLR 2020 from 3 main angles: architecural revisions, innovations for training and spin-off applications. Enjoy!", "Meet the latest reincarnations of Transformer models.", "Transformers have largely become overparametrized, as this has been a successful recipe for achieving state-of-the-art in several NLP tasks. Alternatively, ALBERT is an already influential example about how BERT can be made way less resource hungry while maintaining the impressive performance that made it famous.", "The result? 18x fewer parameters compared to BERT-large at comparable performance and slightly faster inference.", "One of the limitations of early Transformers is the computational complexity of the attention mechanism scaling quadratically with sequence length. This work introduces some tricks to allow for more efficient computation, which enables modeling attention for longer sequences (up from 512 to 64k!). To do so, the backbone of the model includes:", "Here\u2019s another proposal to overcome long range dependencies and high resource demands in Transformers by imposing what they call \u201cmobile constraints\u201d. This time, using convolutions for short term dependencies and selective attention for long range ones, they create a new transformer LSRA building block that\u2019s more efficient.", "Although results are not competitive with other full-fledged flagship Transformers, the principled architectural design and thoughtful motivations behind it make this a worthy mention.", "How models learn is just as crucial as how models look, so here are some refreshing publications pushing the boundaries of how Transformers do so.", "Masked Language Modeling (MLM) has been the learning foundation of pre-training objectives for models since the introduction of BERT. This paper proposes an alternative that could be cheaper and faster: Replaced Token Detection.", "The main idea is very simple: instead of making the model guess masked tokens, it needs to discriminate which ones were replaced by a small generator network that proposes plausible but wrong tokens. The authors claim that this objective is more sample-efficient than MLM, as the task is defined over all the sequence instead of only the masked tokens. If these results prove themselves to be easily reproducible, this task has the potential to become a new standard for unsupervised pre-training.", "Many of the classical NLP datasets are becoming obsolete as modern Transformers close the gap with human performance, which means new more challenging benchmarks need to be created to stimulate progress. In this case, a new dataset is proposed to tackle the problem of modeling fact-based information expressed on natural language.", "It consists of 16k tables from wikipedia and 118k human anotated statements with ENTAILMENT or REFUTED labels that refer to the factual data. Performance of the baselines is still mediocre, so it\u2019s an exciting time to innovate solving this task!", "Instead of applying the vanilla MLM objective, this work explores the power of self-supervised training from slightly more structured data: Wikipedia and its entities. They replace entities in text with other similar-type entities (a la ELECTRA) and the model learns to discern the replaced instances by context. Using this method, the model is forced to learn information about real world entities and their relationships.", "When this task is combined with classical MLM in pre-training, it results in a substantial increase in performance for zero-shot fact completion and improved performance in entity-centric tasks such as Question Answering and Entity Typing.", "Honorable mentions: A Mutual Information Maximization Perspective of Language Representation Learning; Improving Neural Language Generation with Spectrum Control; Large Batch Optimization for Deep Learning: Training BERT in 76 minutes.", "Transformers are not only about Language Modelling. Here are some works that cleverly use the power of these models to solve related problems.", "Objectively measuring quality under loosely defined environments (i.e. generation of coherent text) is intrinsically challenging. In language, BLUE score is widely used as a proxy for text similarity that correlates fairly well with human judgement for text generation tasks such as Translation or Question Answering, but it\u2019s still far from perfect.", "This work addresses this problem and shows how a Bert-based scoring function for sequence pairs can be designed for text generation that correlates better with human judgement. The process is very straightforward and does not involve any fine-tuning: only pre-trained contextualized embeddings, cosine similarities and frequency-based importance weighting.", "Despite some loss of explainability, could this kind of learned scoring become a new standard? Only time will tell.", "The field of Information Retrieval has been late to the neural revolution, given how strong and hard to beat simple baselines like BM25 are. Currently, most neural-enhanced SOTA approaches require two main steps: a first fast filtering over the whole document set \u2014 based on BM25-like algorithms \u2014 and a re-ranking step where the query and a small subset of documents are processed through a Neural Network. This approach presents many limitations because any documents missed in the first step will not be processed further and the computational cost of fully processing query and document pairs at inference time severely limits real-world applicability.", "This work explores instead the constrained problem where the inference can only be done by an embedding similarity score of pre-calculated document representations, enabling large-scale end-to-end Transformer-based retrieval.", "The key insight to be drawn is that pre-training with paragraph-level self-supervised tasks is essential, while token-level Masked Language Modeling has a negligible impact for this particular task. In the results section they show how BM25 can be beaten for Question Answering tasks even under relatively scarce supervised training data.", "How can the pre-training and fine-tuning framework be leveraged to jointly learn generic language and visual representations? Here we find a great example: Visual-Linguistic BERT takes the Transformer architecture as a backbone along with R-CNNs. Although this is not the first of its kind, it\u2019s a refreshing improvement over existing models and set a new state-of-the-art for the Visual Commonsense Reasoning (VCR) benchmark (well, at the time of publication). The pre-training procedure relies on two main objectives:", "This unconventional paper presents a compelling analysis of what the attention mechanism and the convolution might have in common. Interestingly, they find more overlap than one might expect a priori: as their evidence suggests, attention layers often learn to attend \u201cpixel-grid patterns\u201d similarly to CNNs.", "Using Computer Vision as a case study, along with detailed mathematical derivations they conclude that Transformer architectures might be a generalization of CNNs as they often learn equivalent patterns and might even present advantages thanks to the ability of learning local and global information simultaneously.", "Honorable mentions: Deep Learning For Symbolic Mathematics; Logic and the 2-Simplicial Transformer (for Deep RL).", "This year\u2019s ICLR perfectly reflects how a vibrant active branch of Machine Learning matures: models, training techniques, datasets and applications get more refined and the understanding around them solidifies. Our Transformer-focused journey for this post ends here, but there\u2019s still a lot more to explore for the conference. The team and I will be following relevant talks and workshops closely and reporting interesting insights live from our company twitter feed at @zetavector, so tune in if you don\u2019t want to miss a thing!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "\ud83d\udd0e Working on search technology at Zeta Alpha // \ud83c\udf99 Neural IR Podcast: bit.ly/3ITDNNE // \ud83d\udc54 Linkedin: linkedin.com/in/sergicastella"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F4285a4294792&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhats-new-for-transformers-at-the-iclr-2020-conference-4285a4294792&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhats-new-for-transformers-at-the-iclr-2020-conference-4285a4294792&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhats-new-for-transformers-at-the-iclr-2020-conference-4285a4294792&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhats-new-for-transformers-at-the-iclr-2020-conference-4285a4294792&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----4285a4294792--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4285a4294792--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@sergicastella?source=post_page-----4285a4294792--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@sergicastella?source=post_page-----4285a4294792--------------------------------", "anchor_text": "Sergi Castella i Sap\u00e9"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1e27e64320ad&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhats-new-for-transformers-at-the-iclr-2020-conference-4285a4294792&user=Sergi+Castella+i+Sap%C3%A9&userId=1e27e64320ad&source=post_page-1e27e64320ad----4285a4294792---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4285a4294792&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhats-new-for-transformers-at-the-iclr-2020-conference-4285a4294792&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4285a4294792&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhats-new-for-transformers-at-the-iclr-2020-conference-4285a4294792&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.zeta-alpha.com", "anchor_text": "Zeta Alpha"}, {"url": "https://openreview.net/pdf?id=H1eA7AEtvS", "anchor_text": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"}, {"url": "https://iclr.cc/virtual/poster_H1eA7AEtvS.html", "anchor_text": "ICLR session"}, {"url": "https://openreview.net/pdf?id=rkgNKkHtvB", "anchor_text": "Reformer: The Efficient Transformer"}, {"url": "https://iclr.cc/virtual/poster_rkgNKkHtvB.html", "anchor_text": "ICLR session"}, {"url": "https://openreview.net/pdf?id=rkgNKkHtvB", "anchor_text": "Reformer: The Efficient Transformer"}, {"url": "https://openreview.net/pdf?id=ByeMPlHKPH", "anchor_text": "Lite Transformer with Long-Short Range Attention (LSRA)"}, {"url": "https://iclr.cc/virtual/poster_ByeMPlHKPH.html", "anchor_text": "ICLR session"}, {"url": "https://openreview.net/pdf?id=ByeMPlHKPH", "anchor_text": "Lite Transformer with Long-Short Range Attention (LSRA)"}, {"url": "https://openreview.net/pdf?id=r1eIiCNYwS", "anchor_text": "Transformer-XH"}, {"url": "https://openreview.net/pdf?id=SJg7KhVKPH", "anchor_text": "Depth-Adaptive Transformer"}, {"url": "https://openreview.net/pdf?id=SylKikSYDH", "anchor_text": "Compressive Transformer"}, {"url": "https://openreview.net/pdf?id=r1xMH1BtvB", "anchor_text": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators"}, {"url": "https://iclr.cc/virtual/poster_r1xMH1BtvB.html", "anchor_text": "ICLR session"}, {"url": "https://openreview.net/pdf?id=r1xMH1BtvB", "anchor_text": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators"}, {"url": "https://openreview.net/pdf?id=rkeJRhNYDH", "anchor_text": "TabFact: A Large-scale Dataset for Table-based Fact Verification"}, {"url": "https://iclr.cc/virtual/poster_rkeJRhNYDH.html", "anchor_text": "ICLR session"}, {"url": "https://openreview.net/pdf?id=rkeJRhNYDH", "anchor_text": "TabFact: A Large-scale Dataset for Table-based Fact Verification"}, {"url": "https://openreview.net/pdf?id=BJlzm64tDH", "anchor_text": "Pretrained Encyclopedia: Weakly Supervised Knowledge-Pretrained Language Model"}, {"url": "https://iclr.cc/virtual/poster_BJlzm64tDH.html", "anchor_text": "ICLR session"}, {"url": "https://openreview.net/pdf?id=rkxoh24FPH", "anchor_text": "A Mutual Information Maximization Perspective of Language Representation Learning"}, {"url": "https://openreview.net/pdf?id=ByxY8CNtvr", "anchor_text": "Improving Neural Language Generation with Spectrum Control"}, {"url": "https://openreview.net/pdf?id=Syx4wnEtvH", "anchor_text": "Large Batch Optimization for Deep Learning: Training BERT in 76 minutes"}, {"url": "https://openreview.net/pdf?id=SkeHuCVFDr", "anchor_text": "BERTScore: Evaluating Text Generation with BERT"}, {"url": "https://iclr.cc/virtual/poster_SkeHuCVFDr.html", "anchor_text": "ICLR session"}, {"url": "https://openreview.net/pdf?id=SkeHuCVFDr", "anchor_text": "BERTScore: Evaluating Text Generation with BERT"}, {"url": "https://openreview.net/pdf?id=rkg-mA4FDr", "anchor_text": "Pre-training Tasks for Embedding-based Large-scale Retrieval"}, {"url": "https://iclr.cc/virtual/poster_rkg-mA4FDr.html", "anchor_text": "ICLR session"}, {"url": "https://openreview.net/pdf?id=rkg-mA4FDr", "anchor_text": "Pre-training Tasks for Embedding-based Large-scale Retrieval"}, {"url": "https://openreview.net/pdf?id=SygXPaEYvH", "anchor_text": "VL-BERT: Pre-training of Generic Visual-Linguistic Representations"}, {"url": "https://iclr.cc/virtual/poster_SygXPaEYvH.html", "anchor_text": "ICLR session"}, {"url": "https://openreview.net/pdf?id=SygXPaEYvH", "anchor_text": "VL-BERT: Pre-training of Generic Visual-Linguistic Representations"}, {"url": "https://openreview.net/pdf?id=HJlnC1rKPB", "anchor_text": "On the Relationship Between Self-Attention and Convolutional Layers"}, {"url": "https://iclr.cc/virtual/poster_HJlnC1rKPB.html", "anchor_text": "ICLR session"}, {"url": "https://openreview.net/pdf?id=HJlnC1rKPB", "anchor_text": "On the Relationship Between Self-Attention and Convolutional Layers"}, {"url": "https://openreview.net/pdf?id=S1eZYeHFDS", "anchor_text": "Deep Learning For Symbolic Mathematics"}, {"url": "http://LOGIC AND THE 2-SIMPLICIAL TRANSFORMER", "anchor_text": "Logic and the 2-Simplicial Transformer"}, {"url": "https://twitter.com/ZetaVector", "anchor_text": "@zetavector"}, {"url": "https://medium.com/tag/transformers?source=post_page-----4285a4294792---------------transformers-----------------", "anchor_text": "Transformers"}, {"url": "https://medium.com/tag/iclr?source=post_page-----4285a4294792---------------iclr-----------------", "anchor_text": "Iclr"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----4285a4294792---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/nlp?source=post_page-----4285a4294792---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----4285a4294792---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4285a4294792&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhats-new-for-transformers-at-the-iclr-2020-conference-4285a4294792&user=Sergi+Castella+i+Sap%C3%A9&userId=1e27e64320ad&source=-----4285a4294792---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4285a4294792&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhats-new-for-transformers-at-the-iclr-2020-conference-4285a4294792&user=Sergi+Castella+i+Sap%C3%A9&userId=1e27e64320ad&source=-----4285a4294792---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4285a4294792&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhats-new-for-transformers-at-the-iclr-2020-conference-4285a4294792&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4285a4294792--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F4285a4294792&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhats-new-for-transformers-at-the-iclr-2020-conference-4285a4294792&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----4285a4294792---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----4285a4294792--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----4285a4294792--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----4285a4294792--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----4285a4294792--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----4285a4294792--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----4285a4294792--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----4285a4294792--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----4285a4294792--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@sergicastella?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@sergicastella?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Sergi Castella i Sap\u00e9"}, {"url": "https://medium.com/@sergicastella/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "3.2K Followers"}, {"url": "http://bit.ly/3ITDNNE", "anchor_text": "bit.ly/3ITDNNE"}, {"url": "http://linkedin.com/in/sergicastella", "anchor_text": "linkedin.com/in/sergicastella"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1e27e64320ad&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhats-new-for-transformers-at-the-iclr-2020-conference-4285a4294792&user=Sergi+Castella+i+Sap%C3%A9&userId=1e27e64320ad&source=post_page-1e27e64320ad--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fce123c69a0ef&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhats-new-for-transformers-at-the-iclr-2020-conference-4285a4294792&newsletterV3=1e27e64320ad&newsletterV3Id=ce123c69a0ef&user=Sergi+Castella+i+Sap%C3%A9&userId=1e27e64320ad&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}