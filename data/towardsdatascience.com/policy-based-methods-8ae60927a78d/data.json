{"url": "https://towardsdatascience.com/policy-based-methods-8ae60927a78d", "time": 1683013561.57643, "path": "towardsdatascience.com/policy-based-methods-8ae60927a78d/", "webpage": {"metadata": {"title": "Policy-Based Methods. Hill Climbing algorithm | by Jordi TORRES.AI | Towards Data Science", "h1": "Policy-Based Methods", "description": "This is a new post devoted to Policy-Based Methods, in the \u201cDeep Reinforcement Learning Explained\u201d series. Here we will introduce a class of algorithms that allow us to approximate the policy\u2026"}, "outgoing_paragraph_urls": [{"url": "https://torres.ai/deep-reinforcement-learning-explained-series/", "anchor_text": "Deep Reinforcement Learning Explained", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/solving-a-reinforcement-learning-problem-using-cross-entropy-method-23d9726a737", "anchor_text": "in Post 6", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/the-value-iteration-algorithm-4714f113f7c5", "anchor_text": "Value Iteration", "paragraph_index": 1}, {"url": "https://towardsdatascience.com/deep-q-network-dqn-i-bce08bdf2af", "anchor_text": "Q-learning", "paragraph_index": 1}, {"url": "https://towardsdatascience.com/the-bellman-equation-59258a0d3fa7", "anchor_text": "Bellman equation", "paragraph_index": 1}, {"url": "https://towardsdatascience.com/drl-01-a-gentle-introduction-to-deep-reinforcement-learning-405b79866bf4", "anchor_text": "Post 1", "paragraph_index": 3}, {"url": "https://towardsdatascience.com/solving-a-reinforcement-learning-problem-using-cross-entropy-method-23d9726a737", "anchor_text": "in Post 6", "paragraph_index": 8}, {"url": "https://towardsdatascience.com/deep-q-network-dqn-i-bce08bdf2af", "anchor_text": "previous three Posts", "paragraph_index": 9}, {"url": "https://gym.openai.com/envs/CartPole-v0", "anchor_text": "Cart-Pole balancing problem", "paragraph_index": 10}, {"url": "https://github.com/openai/gym/wiki/CartPole-v0", "anchor_text": "this document", "paragraph_index": 13}, {"url": "https://en.wikipedia.org/wiki/Hill_climbing", "anchor_text": "Hill Climbing", "paragraph_index": 21}, {"url": "https://towardsdatascience.com/a-pair-of-interrelated-neural-networks-in-dqn-f0f58e09b3c4", "anchor_text": "Cartpole Environment", "paragraph_index": 29}, {"url": "https://github.com/jorditorresBCN/Deep-Reinforcement-Learning-Explained/blob/master/DRL_18_Policy_Based_Methods.ipynb", "anchor_text": "found on GitHub", "paragraph_index": 30}, {"url": "https://colab.research.google.com/github/jorditorresBCN/Deep-Reinforcement-Learning-Explained/blob/master/DRL_18_Policy_Based_Methods.ipynb", "anchor_text": "can be run as a Colab google notebook using this link", "paragraph_index": 30}, {"url": "https://towardsdatascience.com/solving-a-reinforcement-learning-problem-using-cross-entropy-method-23d9726a737", "anchor_text": "in Post 6", "paragraph_index": 42}, {"url": "https://towardsdatascience.com/policy-gradient-methods-104c783251e0", "anchor_text": "next post", "paragraph_index": 43}, {"url": "https://towardsdatascience.com/policy-gradient-methods-104c783251e0", "anchor_text": "the next post", "paragraph_index": 45}, {"url": "https://www.upc.edu/en", "anchor_text": "UPC Barcelona Tech", "paragraph_index": 46}, {"url": "https://www.bsc.es/", "anchor_text": "Barcelona Supercomputing Center", "paragraph_index": 46}, {"url": "https://torres.ai/deep-reinforcement-learning-explained-series/", "anchor_text": "series", "paragraph_index": 47}, {"url": "https://twitter.com/hashtag/StayAtHome?src=hashtag_click", "anchor_text": "#StayAtHome", "paragraph_index": 48}, {"url": "https://torres.ai", "anchor_text": "https://torres.ai", "paragraph_index": 50}], "all_paragraphs": ["This is a new post devoted to Policy-Based Methods, in the \u201cDeep Reinforcement Learning Explained\u201d series. Here we will introduce a class of algorithms that allow us to approximate the policy function, \u03c0, instead of the values functions (V, or Q). Remember that we defined policy as the entity that tells us what to do in every state. That is, instead of training a network that outputs action values, we will train a network to output (the probability of) actions, as we advanced with one example in Post 6 .", "The central topic in Value Iteration and Q-learning, introduced in previous Posts, is the value of the state (denoted by V) or value of the state-action (denoted by Q). Remember that Value is defined as the discounted total reward that we can gather from a state or by issuing a particular action from the state. If the value is known, the decision on every step becomes simple and obvious: greedily in terms of value, and that guarantees a good total reward at the end of the episode. To obtain these values, we have used the Bellman equation, which expresses the value on the current step via the values on the next step (it makes a prediction from a prediction).", "Reinforcement learning is ultimately about learning an optimal policy, denoted by \u03c0*, from interacting with the Environment. So far, we\u2019ve been learning at value-based methods, where we first find an estimate of the optimal action-value function q* from which we obtain the optimal policy \u03c0*.", "For small state spaces, like the Frozen-Lake example introduced in Post 1, this optimal value function q* can be represented in a table, the Q-table, with one row for each state and one column for each action. At each time step, for a given state, we only need to pull its corresponding row from the table, and the optimal action is just the action with the largest value entry.", "But what about environments with much larger state spaces, like the Pong Environment introduced in the previous Posts? There\u2019s a vast number of possible states, and that would make the table way too big to be useful in practice. So, we presented how to represent the optimal action-value function q* with a neural network. In this case, the neural network is fed with the Environment state as input, and it returns as output the value of each possible action.", "But it is important to note that in both cases, whether we used a table or a neural network, we had to first estimate the optimal action-value function before we could tackle the optimal policy \u03c0*. Then, an interesting question arises: can we directly find the optimal policy without first having to deal with a value function? The answer is yes, and the class of algorithms to accomplish this are known as policy-based methods.", "With value-based methods, the Agent uses its experience with the Environment to maintain an estimate of the optimal action-value function. The optimal policy is then obtained from the optimal action-value function estimate (e.g., using e-greedy).", "Instead, Policy-based methods directly learn the optimal policy from the interactions with the environment, without having to maintain a separate value function estimate.", "An example of a policy-based method, was already introduced at the beginning of this series when the cross-entropy method was presented in Post 6 . We introduced that a policy, denoted by \ud835\udf0b(\ud835\udc4e|\ud835\udc60), says which action the Agent should take for every state observed. In practice, the policy is usually represented as a probability distribution over actions (that the Agent can take at a given state) with the number of classes equal to the number of actions we can carry out. We refer to it as a stochastic policy because it returns a probability distribution over actions rather than returning a single deterministic action.", "Policy-based methods offer a few advantages over value-prediction methods like DQN presented in the previous three Posts. One is that, as we already discussed, we no longer have to worry about devising an action-selection strategy like \u03f5-greedy policy; instead, we directly sample actions from the policy. And this is important; remember that we wasted a lot of time fixing up methods to improve the stability of training our DQN. For instance, we had to use experience replay and target networks, and there are several other methods in the academic literature that helps. A policy network tends to simplify some of that complexity.", "In Deep Reinforcement Learning, it is common to represent the policy with a neural network (as we did for the first time in Post 6). Let\u00b4s consider the Cart-Pole balancing problem from Post 12 of this series as an example to introduce how we can represent a policy with a neural network.", "Remember that a cart is positioned on a frictionless track along the horizontal axis in this example, and a pole is anchored to the top of the cart. The objective is to keep the pole from falling over by moving the cart either left or right, and without falling off the track.", "The system is controlled by applying a force of +1 (left) or -1 (right) to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every time-step that the pole remains upright, including the episode\u2019s final step. The episode ends when the pole is more than 15 degrees from the vertical, or the cart moves more than 2.4 units from the center.", "The observation space for this Environment at each time point is an array of 4 numbers. At every time step, you can observe its position, velocity, angle, and angular velocity. These are the observable states of this world. You can look up what each of these numbers represents in this document. Notice the minimum (-Inf) and maximum (Inf) values for both Cart Velocity and the Pole Velocity at Tip. Since the entry in the array corresponding to each of these indices can be any real number, that means the state space is infinite!", "At any state, the cart only has two possible actions: move to the left or move to the right. In other words, the state-space of the Cart-Pole has four dimensions of continuous values, and the action-space has one dimension of two discrete values.", "We can construct a neural network that approximates the policy that takes a state as input. In this example, the output layer will have two nodes that return, respectively, the probability for each action. In general, if the Environment has discrete action space, as in this example, the output layer has a node for each possible action and contains the probability that the Agent should select each possible action.", "The way to use the network is that the Agent feeds the current Environment state, and then the Agent samples from the probabilities of actions (left or right in this case) to select its next action.", "Then, the objective is to determine appropriate values for the network weights represented by \u03b8 (Theta). \u03b8 encodes the policy that for each state that we pass into the network, it returns action probabilities where the optimal action is most likely to be selected. The actions chosen influences the Rewards obtained that are used to get the return.", "Remember that the Agent\u2019s goal is always to maximize expected return. In our case, let\u2019s denote the expected return as J. The main idea is that it is possible to write the expected return J as a function of \u03b8. Later we will see how we can express this relationship, J(\u03b8), in a more \u201cmathematical\u201d way to find the values for the weights that maximize the expected return.", "In the previous section, we have seen how a neural network can represent a policy. The weights in this neural network are initially set to random values. Then, the Agent updates the weights as it interacts with the Environment. This section will give an overview of approaches that we can take towards optimizing these weights, derivative-free methods, also known as zero-order methods.", "Derivative-free methods directly search in parameter space for the vector of weights that maximizes the returns obtained by a policy; by evaluating only some positions of the parameter space, without derivatives that compute the gradients. Let\u2019s explain the most straightforward algorithm in this category that will help us to understand later how policy gradient methods work, the Hill Climbing.", "Hill Climbing is an iterative algorithm that can be used to find the weights \u03b8 for an optimal policy. It is a relatively simple algorithm that the Agent can use to gradually improve the weights \u03b8 in its policy network while interacting with the Environment.", "As the name indicates, intuitively, we can visualize that the algorithm draws up a strategy to reach the highest point of a hill, where \u03b8 indicates the coordinates of where we are at a given moment and G indicates the altitude at which we are at that point:", "This visual example represents a function of two parameters, but the same idea extends to more than two parameters. The algorithm begins with an initial guess for the value of \u03b8 (random set of weights). We collect a single episode with the policy that corresponds to those weights \u03b8 and then record the return G.", "This return is an estimate of what the surface looks like at that value of Theta. It will not be a perfect estimate because the return we just collected is unlikely to be equal to the expected return. This is because due to randomness in the Environment (and the policy, if it is stochastic), it is highly likely that if we collect a second episode with the same values for \u03b8, we\u2019ll likely get a different value for the return G. But in practice, even though the (sampled) return is not a perfect estimate for the expected return estimates, it often turns out to be good enough.", "At each iteration, we slightly perturb the values (add a little bit of random noise) of the current best estimate for the weights \u03b8\u200b, to yield a new set of candidate weights we can try. These new weights are then used to collect an episode. To see how good those new weights are, we\u2019ll use the policy that they give us to again interact with the Environment for an episode and add up the return.", "In up the new weights, give us more return than our current best estimate, we focus our attention on that new value, and then we just repeat by iteratively proposing new policies in the hope that they outperform the existing policy. In the event that they don\u2019t well, we go back to our last best guess for the optimal policy and iterate until we end up with the optimal policy.", "Now that we have an intuitive understanding of how the hill climbing algorithm should work, we can summarize it in the following pseudocode:", "In our example, we assumed a surface with only one maximum in which the algorithm Hill-climbers are well-suited. Note that it\u2019s not guaranteed to always yield the weights of the optimal policy on a surface with more than one local maxima. This is because if the algorithm begins in a poor location, it may converge to the lower maximum.", "This section will explore an implementation of Hill-Climbing applied to the Cartpole Environment based in the previous pseudocode. The neural network model here is so simple, uses only the simplest matrix of shape[4x2] (state_space x action_space), that does not use tensors (no PyTorch required nor even GPU).", "The code presented in this section can be found on GitHub (and can be run as a Colab google notebook using this link).", "From this post, since it repeats many of the things that we have been using, we will not go into describing the code in detail; I think this is quite self-explanatory.", "As always, we will start by importing the required packages and create the Environment:", "The policy \u03c0 (and its initialization with random weights \u03b8) can be coded as:", "To visualize the training\u2019s effect, we plot the weights \u03b8 before and after the training and render how the Agent apply the policy:", "The following code defines the function that trains the Agent:", "The code does not require too much explanation as it is quite explicit, and it is annotated with the corresponding pseudocode steps. Maybe note some details. For instance, the algorithm seeks to maximize the cumulative discounted reward, and it looks in Python as follows:", "Remember that Hill Climbing is a simple gradient-free algorithm (i.e., we do not use the gradient ascent/gradient descent methods). We try to climb to the top of the curve by only changing the arguments of the target function G, the weight matrix \u03b8 determining the neural network that underlies in our mode, using a certain noise:", "As in some previous examples, we try to exceed a certain threshold to consider the Environment solved. For Cartpole-v0, this threshold score is 195, indicated by env.spec.reward_threshold. In the example that we used to write this post, we only needed 215 episodes to solve the Environment:", "With the following code, we can plot the scores obtained in each episode during training:", "Now we plot the weights \u03b8 again, after the training and also how the Agent applies this policy and seems smarter:", "Although in this example, we have coded a deterministic policy for simplicity, Policy-based methods can learn either stochastic or deterministic policies, and they can be used to solve Environments with either finite or continuous action spaces.", "Hill Climbing algorithm does not need to be differentiable or even continuous, but because it is taking random steps, this may not result in the most efficient path up the hill. There are in the literature many improvements to this approach: adaptive noise scaling, Steepest ascent hill climbing, random restarts, simulated annealing, evolution strategies, or cross-entropy method (presented in Post 6 ) .", "However, the usual solutions to this problem consider Policy Gradient Methods that estimate an optimal policy\u2019s weights through gradient ascent. Policy gradient methods are a subclass of policy-based methods that we will present in the next post.", "In this post, we introduced the concept of Policy-Based Methods. There are several reasons why we consider policy-based methods instead of value-based methods that seem to work so well as we saw in the previous post. Primarily because Policy-based methods directly get to the problem at hand (estimating the optimal policy) without having to store additional data, i.e., the action values that may not be useful. A further advantage over value-based methods, Policy-based methods are well-suited for continuous action spaces. As we will see in future posts, unlike value-based methods, policy-based methods can learn true stochastic policies.", "See you in the next post!", "by UPC Barcelona Tech and Barcelona Supercomputing Center", "A relaxed introductory series that gradually and with a practical approach introduces the reader to this exciting technology that is the real enabler of the latest disruptive advances in the field of Artificial Intelligence.", "I started to write this series in May, during the period of lockdown in Barcelona. Honestly, writing these posts in my spare time helped me to #StayAtHome because of the lockdown. Thank you for reading this publication in those days; it justifies the effort I made.", "Disclaimers \u2014 These posts were written during this period of lockdown in Barcelona as a personal distraction and dissemination of scientific knowledge, in case it could be of help to someone, but without the purpose of being an academic reference document in the DRL area. If the reader needs a more rigorous document, the last post in the series offers an extensive list of academic resources and books that the reader can consult. The author is aware that this series of posts may contain some errors and suffers from a revision of the English text to improve it if the purpose were an academic document. But although the author would like to improve the content in quantity and quality, his professional commitments do not leave him free time to do so. However, the author agrees to refine all those errors that readers can report as soon as he can.", "Professor at UPC Barcelona Tech & Barcelona Supercomputing Center. Research focuses on Supercomputing & Artificial Intelligence https://torres.ai @JordiTorresAI"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F8ae60927a78d&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpolicy-based-methods-8ae60927a78d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpolicy-based-methods-8ae60927a78d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpolicy-based-methods-8ae60927a78d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpolicy-based-methods-8ae60927a78d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/tagged/deep-r-l-explained", "anchor_text": "DEEP REINFORCEMENT LEARNING EXPLAINED \u2014 18"}, {"url": "https://torres-ai.medium.com/?source=post_page-----8ae60927a78d--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8ae60927a78d--------------------------------", "anchor_text": ""}, {"url": "https://torres-ai.medium.com/?source=post_page-----8ae60927a78d--------------------------------", "anchor_text": "Jordi TORRES.AI"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F497013a3c715&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpolicy-based-methods-8ae60927a78d&user=Jordi+TORRES.AI&userId=497013a3c715&source=post_page-497013a3c715----8ae60927a78d---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----8ae60927a78d--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8ae60927a78d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpolicy-based-methods-8ae60927a78d&user=Jordi+TORRES.AI&userId=497013a3c715&source=-----8ae60927a78d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8ae60927a78d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpolicy-based-methods-8ae60927a78d&source=-----8ae60927a78d---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://torres.ai/deep-reinforcement-learning-explained-series/", "anchor_text": "Deep Reinforcement Learning Explained"}, {"url": "https://towardsdatascience.com/solving-a-reinforcement-learning-problem-using-cross-entropy-method-23d9726a737", "anchor_text": "in Post 6"}, {"url": "https://towardsdatascience.com/the-value-iteration-algorithm-4714f113f7c5", "anchor_text": "Value Iteration"}, {"url": "https://towardsdatascience.com/deep-q-network-dqn-i-bce08bdf2af", "anchor_text": "Q-learning"}, {"url": "https://towardsdatascience.com/the-bellman-equation-59258a0d3fa7", "anchor_text": "Bellman equation"}, {"url": "https://towardsdatascience.com/drl-01-a-gentle-introduction-to-deep-reinforcement-learning-405b79866bf4", "anchor_text": "Post 1"}, {"url": "https://towardsdatascience.com/solving-a-reinforcement-learning-problem-using-cross-entropy-method-23d9726a737", "anchor_text": "in Post 6"}, {"url": "https://towardsdatascience.com/deep-q-network-dqn-i-bce08bdf2af", "anchor_text": "previous three Posts"}, {"url": "https://gym.openai.com/envs/CartPole-v0", "anchor_text": "Cart-Pole balancing problem"}, {"url": "https://github.com/openai/gym/wiki/CartPole-v0", "anchor_text": "this document"}, {"url": "https://torres.ai", "anchor_text": "torres.ai"}, {"url": "https://en.wikipedia.org/wiki/Hill_climbing", "anchor_text": "Hill Climbing"}, {"url": "https://towardsdatascience.com/a-pair-of-interrelated-neural-networks-in-dqn-f0f58e09b3c4", "anchor_text": "Cartpole Environment"}, {"url": "https://github.com/jorditorresBCN/Deep-Reinforcement-Learning-Explained/blob/master/DRL_18_Policy_Based_Methods.ipynb", "anchor_text": "found on GitHub"}, {"url": "https://colab.research.google.com/github/jorditorresBCN/Deep-Reinforcement-Learning-Explained/blob/master/DRL_18_Policy_Based_Methods.ipynb", "anchor_text": "can be run as a Colab google notebook using this link"}, {"url": "https://towardsdatascience.com/solving-a-reinforcement-learning-problem-using-cross-entropy-method-23d9726a737", "anchor_text": "in Post 6"}, {"url": "https://towardsdatascience.com/policy-gradient-methods-104c783251e0", "anchor_text": "next post"}, {"url": "https://towardsdatascience.com/policy-gradient-methods-104c783251e0", "anchor_text": "the next post"}, {"url": "https://www.upc.edu/en", "anchor_text": "UPC Barcelona Tech"}, {"url": "https://www.bsc.es/", "anchor_text": "Barcelona Supercomputing Center"}, {"url": "https://torres.ai/deep-reinforcement-learning-explained-series/", "anchor_text": "series"}, {"url": "https://torres.ai/deep-reinforcement-learning-explained-series/", "anchor_text": "Deep Reinforcement Learning Explained \u2014 Jordi TORRES.AIContent of this series"}, {"url": "https://twitter.com/hashtag/StayAtHome?src=hashtag_click", "anchor_text": "#StayAtHome"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----8ae60927a78d---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----8ae60927a78d---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----8ae60927a78d---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/deep-r-l-explained?source=post_page-----8ae60927a78d---------------deep_r_l_explained-----------------", "anchor_text": "Deep R L Explained"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----8ae60927a78d---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8ae60927a78d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpolicy-based-methods-8ae60927a78d&user=Jordi+TORRES.AI&userId=497013a3c715&source=-----8ae60927a78d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8ae60927a78d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpolicy-based-methods-8ae60927a78d&user=Jordi+TORRES.AI&userId=497013a3c715&source=-----8ae60927a78d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8ae60927a78d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpolicy-based-methods-8ae60927a78d&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://torres-ai.medium.com/?source=post_page-----8ae60927a78d--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8ae60927a78d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F497013a3c715&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpolicy-based-methods-8ae60927a78d&user=Jordi+TORRES.AI&userId=497013a3c715&source=post_page-497013a3c715----8ae60927a78d---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F9fb911e344f9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpolicy-based-methods-8ae60927a78d&newsletterV3=497013a3c715&newsletterV3Id=9fb911e344f9&user=Jordi+TORRES.AI&userId=497013a3c715&source=-----8ae60927a78d---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://torres-ai.medium.com/?source=post_page-----8ae60927a78d--------------------------------", "anchor_text": "Written by Jordi TORRES.AI"}, {"url": "https://torres-ai.medium.com/followers?source=post_page-----8ae60927a78d--------------------------------", "anchor_text": "2.1K Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----8ae60927a78d--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://torres.ai", "anchor_text": "https://torres.ai"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F497013a3c715&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpolicy-based-methods-8ae60927a78d&user=Jordi+TORRES.AI&userId=497013a3c715&source=post_page-497013a3c715----8ae60927a78d---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F9fb911e344f9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpolicy-based-methods-8ae60927a78d&newsletterV3=497013a3c715&newsletterV3Id=9fb911e344f9&user=Jordi+TORRES.AI&userId=497013a3c715&source=-----8ae60927a78d---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-bellman-equation-59258a0d3fa7?source=author_recirc-----8ae60927a78d----0---------------------b4e3a86b_7d64_49c5_b7c1_576c628b9356-------", "anchor_text": ""}, {"url": "https://torres-ai.medium.com/?source=author_recirc-----8ae60927a78d----0---------------------b4e3a86b_7d64_49c5_b7c1_576c628b9356-------", "anchor_text": ""}, {"url": "https://torres-ai.medium.com/?source=author_recirc-----8ae60927a78d----0---------------------b4e3a86b_7d64_49c5_b7c1_576c628b9356-------", "anchor_text": "Jordi TORRES.AI"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----8ae60927a78d----0---------------------b4e3a86b_7d64_49c5_b7c1_576c628b9356-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-bellman-equation-59258a0d3fa7?source=author_recirc-----8ae60927a78d----0---------------------b4e3a86b_7d64_49c5_b7c1_576c628b9356-------", "anchor_text": "The Bellman EquationV-function and Q-function Explained"}, {"url": "https://towardsdatascience.com/the-bellman-equation-59258a0d3fa7?source=author_recirc-----8ae60927a78d----0---------------------b4e3a86b_7d64_49c5_b7c1_576c628b9356-------", "anchor_text": "\u00b712 min read\u00b7Jun 11, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F59258a0d3fa7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-bellman-equation-59258a0d3fa7&user=Jordi+TORRES.AI&userId=497013a3c715&source=-----59258a0d3fa7----0-----------------clap_footer----b4e3a86b_7d64_49c5_b7c1_576c628b9356-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-bellman-equation-59258a0d3fa7?source=author_recirc-----8ae60927a78d----0---------------------b4e3a86b_7d64_49c5_b7c1_576c628b9356-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "5"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F59258a0d3fa7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-bellman-equation-59258a0d3fa7&source=-----8ae60927a78d----0-----------------bookmark_preview----b4e3a86b_7d64_49c5_b7c1_576c628b9356-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----8ae60927a78d----1---------------------b4e3a86b_7d64_49c5_b7c1_576c628b9356-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----8ae60927a78d----1---------------------b4e3a86b_7d64_49c5_b7c1_576c628b9356-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----8ae60927a78d----1---------------------b4e3a86b_7d64_49c5_b7c1_576c628b9356-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----8ae60927a78d----1---------------------b4e3a86b_7d64_49c5_b7c1_576c628b9356-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----8ae60927a78d----1---------------------b4e3a86b_7d64_49c5_b7c1_576c628b9356-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----8ae60927a78d----1---------------------b4e3a86b_7d64_49c5_b7c1_576c628b9356-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----b4e3a86b_7d64_49c5_b7c1_576c628b9356-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----8ae60927a78d----1---------------------b4e3a86b_7d64_49c5_b7c1_576c628b9356-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----8ae60927a78d----1-----------------bookmark_preview----b4e3a86b_7d64_49c5_b7c1_576c628b9356-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----8ae60927a78d----2---------------------b4e3a86b_7d64_49c5_b7c1_576c628b9356-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----8ae60927a78d----2---------------------b4e3a86b_7d64_49c5_b7c1_576c628b9356-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----8ae60927a78d----2---------------------b4e3a86b_7d64_49c5_b7c1_576c628b9356-------", "anchor_text": "Jacob Marks, Ph.D."}, {"url": "https://towardsdatascience.com/?source=author_recirc-----8ae60927a78d----2---------------------b4e3a86b_7d64_49c5_b7c1_576c628b9356-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----8ae60927a78d----2---------------------b4e3a86b_7d64_49c5_b7c1_576c628b9356-------", "anchor_text": "How I Turned My Company\u2019s Docs into a Searchable Database with OpenAIAnd how you can do the same with your docs"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----8ae60927a78d----2---------------------b4e3a86b_7d64_49c5_b7c1_576c628b9356-------", "anchor_text": "15 min read\u00b7Apr 25"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&user=Jacob+Marks%2C+Ph.D.&userId=f7dc0c0eae92&source=-----4f2d34bd8736----2-----------------clap_footer----b4e3a86b_7d64_49c5_b7c1_576c628b9356-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----8ae60927a78d----2---------------------b4e3a86b_7d64_49c5_b7c1_576c628b9356-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "20"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&source=-----8ae60927a78d----2-----------------bookmark_preview----b4e3a86b_7d64_49c5_b7c1_576c628b9356-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/deep-q-network-dqn-ii-b6bf911b6b2c?source=author_recirc-----8ae60927a78d----3---------------------b4e3a86b_7d64_49c5_b7c1_576c628b9356-------", "anchor_text": ""}, {"url": "https://torres-ai.medium.com/?source=author_recirc-----8ae60927a78d----3---------------------b4e3a86b_7d64_49c5_b7c1_576c628b9356-------", "anchor_text": ""}, {"url": "https://torres-ai.medium.com/?source=author_recirc-----8ae60927a78d----3---------------------b4e3a86b_7d64_49c5_b7c1_576c628b9356-------", "anchor_text": "Jordi TORRES.AI"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----8ae60927a78d----3---------------------b4e3a86b_7d64_49c5_b7c1_576c628b9356-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/deep-q-network-dqn-ii-b6bf911b6b2c?source=author_recirc-----8ae60927a78d----3---------------------b4e3a86b_7d64_49c5_b7c1_576c628b9356-------", "anchor_text": "Deep Q-Network (DQN)-IIExperience Replay and Target Networks"}, {"url": "https://towardsdatascience.com/deep-q-network-dqn-ii-b6bf911b6b2c?source=author_recirc-----8ae60927a78d----3---------------------b4e3a86b_7d64_49c5_b7c1_576c628b9356-------", "anchor_text": "\u00b714 min read\u00b7Aug 15, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb6bf911b6b2c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-q-network-dqn-ii-b6bf911b6b2c&user=Jordi+TORRES.AI&userId=497013a3c715&source=-----b6bf911b6b2c----3-----------------clap_footer----b4e3a86b_7d64_49c5_b7c1_576c628b9356-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/deep-q-network-dqn-ii-b6bf911b6b2c?source=author_recirc-----8ae60927a78d----3---------------------b4e3a86b_7d64_49c5_b7c1_576c628b9356-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb6bf911b6b2c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-q-network-dqn-ii-b6bf911b6b2c&source=-----8ae60927a78d----3-----------------bookmark_preview----b4e3a86b_7d64_49c5_b7c1_576c628b9356-------", "anchor_text": ""}, {"url": "https://torres-ai.medium.com/?source=post_page-----8ae60927a78d--------------------------------", "anchor_text": "See all from Jordi TORRES.AI"}, {"url": "https://towardsdatascience.com/?source=post_page-----8ae60927a78d--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----8ae60927a78d----0---------------------79469714_79b5_4fa4_a6b5_02353cd8cfa4-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----8ae60927a78d----0---------------------79469714_79b5_4fa4_a6b5_02353cd8cfa4-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----8ae60927a78d----0---------------------79469714_79b5_4fa4_a6b5_02353cd8cfa4-------", "anchor_text": "The PyCoach"}, {"url": "https://artificialcorner.com/?source=read_next_recirc-----8ae60927a78d----0---------------------79469714_79b5_4fa4_a6b5_02353cd8cfa4-------", "anchor_text": "Artificial Corner"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----8ae60927a78d----0---------------------79469714_79b5_4fa4_a6b5_02353cd8cfa4-------", "anchor_text": "You\u2019re Using ChatGPT Wrong! Here\u2019s How to Be Ahead of 99% of ChatGPT UsersMaster ChatGPT by learning prompt engineering."}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----8ae60927a78d----0---------------------79469714_79b5_4fa4_a6b5_02353cd8cfa4-------", "anchor_text": "\u00b77 min read\u00b7Mar 17"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fartificial-corner%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&user=The+PyCoach&userId=fb44e21903f3&source=-----886a50dabc54----0-----------------clap_footer----79469714_79b5_4fa4_a6b5_02353cd8cfa4-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----8ae60927a78d----0---------------------79469714_79b5_4fa4_a6b5_02353cd8cfa4-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "276"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&source=-----8ae60927a78d----0-----------------bookmark_preview----79469714_79b5_4fa4_a6b5_02353cd8cfa4-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----8ae60927a78d----1---------------------79469714_79b5_4fa4_a6b5_02353cd8cfa4-------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=read_next_recirc-----8ae60927a78d----1---------------------79469714_79b5_4fa4_a6b5_02353cd8cfa4-------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=read_next_recirc-----8ae60927a78d----1---------------------79469714_79b5_4fa4_a6b5_02353cd8cfa4-------", "anchor_text": "Wouter van Heeswijk, PhD"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----8ae60927a78d----1---------------------79469714_79b5_4fa4_a6b5_02353cd8cfa4-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----8ae60927a78d----1---------------------79469714_79b5_4fa4_a6b5_02353cd8cfa4-------", "anchor_text": "Proximal Policy Optimization (PPO) ExplainedThe journey from REINFORCE to the go-to algorithm in continuous control"}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----8ae60927a78d----1---------------------79469714_79b5_4fa4_a6b5_02353cd8cfa4-------", "anchor_text": "\u00b713 min read\u00b7Nov 29, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fabed1952457b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fproximal-policy-optimization-ppo-explained-abed1952457b&user=Wouter+van+Heeswijk%2C+PhD&userId=33f45c9ab481&source=-----abed1952457b----1-----------------clap_footer----79469714_79b5_4fa4_a6b5_02353cd8cfa4-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----8ae60927a78d----1---------------------79469714_79b5_4fa4_a6b5_02353cd8cfa4-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fabed1952457b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fproximal-policy-optimization-ppo-explained-abed1952457b&source=-----8ae60927a78d----1-----------------bookmark_preview----79469714_79b5_4fa4_a6b5_02353cd8cfa4-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----8ae60927a78d----0---------------------79469714_79b5_4fa4_a6b5_02353cd8cfa4-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----8ae60927a78d----0---------------------79469714_79b5_4fa4_a6b5_02353cd8cfa4-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----8ae60927a78d----0---------------------79469714_79b5_4fa4_a6b5_02353cd8cfa4-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----8ae60927a78d----0---------------------79469714_79b5_4fa4_a6b5_02353cd8cfa4-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----8ae60927a78d----0---------------------79469714_79b5_4fa4_a6b5_02353cd8cfa4-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----8ae60927a78d----0---------------------79469714_79b5_4fa4_a6b5_02353cd8cfa4-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----0-----------------clap_footer----79469714_79b5_4fa4_a6b5_02353cd8cfa4-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----8ae60927a78d----0---------------------79469714_79b5_4fa4_a6b5_02353cd8cfa4-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----8ae60927a78d----0-----------------bookmark_preview----79469714_79b5_4fa4_a6b5_02353cd8cfa4-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----8ae60927a78d----1---------------------79469714_79b5_4fa4_a6b5_02353cd8cfa4-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----8ae60927a78d----1---------------------79469714_79b5_4fa4_a6b5_02353cd8cfa4-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----8ae60927a78d----1---------------------79469714_79b5_4fa4_a6b5_02353cd8cfa4-------", "anchor_text": "Alexander Nguyen"}, {"url": "https://levelup.gitconnected.com/?source=read_next_recirc-----8ae60927a78d----1---------------------79469714_79b5_4fa4_a6b5_02353cd8cfa4-------", "anchor_text": "Level Up Coding"}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----8ae60927a78d----1---------------------79469714_79b5_4fa4_a6b5_02353cd8cfa4-------", "anchor_text": "Why I Keep Failing Candidates During Google Interviews\u2026They don\u2019t meet the bar."}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----8ae60927a78d----1---------------------79469714_79b5_4fa4_a6b5_02353cd8cfa4-------", "anchor_text": "\u00b74 min read\u00b7Apr 13"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fgitconnected%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&user=Alexander+Nguyen&userId=a148fd75c2e9&source=-----dc8f865b2c19----1-----------------clap_footer----79469714_79b5_4fa4_a6b5_02353cd8cfa4-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----8ae60927a78d----1---------------------79469714_79b5_4fa4_a6b5_02353cd8cfa4-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "91"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&source=-----8ae60927a78d----1-----------------bookmark_preview----79469714_79b5_4fa4_a6b5_02353cd8cfa4-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/trust-region-policy-optimization-trpo-explained-4b56bd206fc2?source=read_next_recirc-----8ae60927a78d----2---------------------79469714_79b5_4fa4_a6b5_02353cd8cfa4-------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=read_next_recirc-----8ae60927a78d----2---------------------79469714_79b5_4fa4_a6b5_02353cd8cfa4-------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=read_next_recirc-----8ae60927a78d----2---------------------79469714_79b5_4fa4_a6b5_02353cd8cfa4-------", "anchor_text": "Wouter van Heeswijk, PhD"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----8ae60927a78d----2---------------------79469714_79b5_4fa4_a6b5_02353cd8cfa4-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/trust-region-policy-optimization-trpo-explained-4b56bd206fc2?source=read_next_recirc-----8ae60927a78d----2---------------------79469714_79b5_4fa4_a6b5_02353cd8cfa4-------", "anchor_text": "Trust Region Policy Optimization (TRPO) ExplainedThe Reinforcement Learning algorithm TRPO builds upon natural policy gradient algorithms, ensuring updates remain within \u2018trustworthy\u2019\u2026"}, {"url": "https://towardsdatascience.com/trust-region-policy-optimization-trpo-explained-4b56bd206fc2?source=read_next_recirc-----8ae60927a78d----2---------------------79469714_79b5_4fa4_a6b5_02353cd8cfa4-------", "anchor_text": "\u00b712 min read\u00b7Oct 12, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4b56bd206fc2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrust-region-policy-optimization-trpo-explained-4b56bd206fc2&user=Wouter+van+Heeswijk%2C+PhD&userId=33f45c9ab481&source=-----4b56bd206fc2----2-----------------clap_footer----79469714_79b5_4fa4_a6b5_02353cd8cfa4-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/trust-region-policy-optimization-trpo-explained-4b56bd206fc2?source=read_next_recirc-----8ae60927a78d----2---------------------79469714_79b5_4fa4_a6b5_02353cd8cfa4-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4b56bd206fc2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftrust-region-policy-optimization-trpo-explained-4b56bd206fc2&source=-----8ae60927a78d----2-----------------bookmark_preview----79469714_79b5_4fa4_a6b5_02353cd8cfa4-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----8ae60927a78d----3---------------------79469714_79b5_4fa4_a6b5_02353cd8cfa4-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData?source=read_next_recirc-----8ae60927a78d----3---------------------79469714_79b5_4fa4_a6b5_02353cd8cfa4-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData?source=read_next_recirc-----8ae60927a78d----3---------------------79469714_79b5_4fa4_a6b5_02353cd8cfa4-------", "anchor_text": "Andrew Austin"}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----8ae60927a78d----3---------------------79469714_79b5_4fa4_a6b5_02353cd8cfa4-------", "anchor_text": "AI Anyone Can Understand Part 1: Reinforcement LearningReinforcement learning is a way for machines to learn by trying different things and seeing what works best. For example, a robot could\u2026"}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----8ae60927a78d----3---------------------79469714_79b5_4fa4_a6b5_02353cd8cfa4-------", "anchor_text": "\u00b74 min read\u00b7Dec 11, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F6c3b3d623a2d&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40MoneyAndData%2Fai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d&user=Andrew+Austin&userId=42d388912d13&source=-----6c3b3d623a2d----3-----------------clap_footer----79469714_79b5_4fa4_a6b5_02353cd8cfa4-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----8ae60927a78d----3---------------------79469714_79b5_4fa4_a6b5_02353cd8cfa4-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6c3b3d623a2d&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40MoneyAndData%2Fai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d&source=-----8ae60927a78d----3-----------------bookmark_preview----79469714_79b5_4fa4_a6b5_02353cd8cfa4-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----8ae60927a78d--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----8ae60927a78d--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----8ae60927a78d--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----8ae60927a78d--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----8ae60927a78d--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----8ae60927a78d--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----8ae60927a78d--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----8ae60927a78d--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----8ae60927a78d--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----8ae60927a78d--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}