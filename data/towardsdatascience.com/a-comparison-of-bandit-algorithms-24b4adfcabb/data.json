{"url": "https://towardsdatascience.com/a-comparison-of-bandit-algorithms-24b4adfcabb", "time": 1683016249.93932, "path": "towardsdatascience.com/a-comparison-of-bandit-algorithms-24b4adfcabb/", "webpage": {"metadata": {"title": "A Comparison of Bandit Algorithms | by Steve Roberts | Towards Data Science", "h1": "A Comparison of Bandit Algorithms", "description": "Over the course of this series we\u2019ve looked at the framework and terminology that are used to define Multi-Armed Bandits. We then turned our attention to some of the algorithms that can be used to\u2026"}, "outgoing_paragraph_urls": [{"url": "https://github.com/WhatIThinkAbout/BabyRobot/tree/master/Multi_Armed_Bandits", "anchor_text": "Multi_Armed_Bandits", "paragraph_index": 2}, {"url": "https://github.com/WhatIThinkAbout/BabyRobot/blob/master/Multi_Armed_Bandits/Part%206%20-%20A%20Comparison%20of%20Bandit%20Algorithms.ipynb", "anchor_text": "github notebook", "paragraph_index": 10}, {"url": "https://arxiv.org/pdf/1904.10040.pdf", "anchor_text": "A Survey on Practical Applications of Multi-Armed and Contextual Bandits", "paragraph_index": 40}, {"url": "https://tor-lattimore.com/downloads/book/book.pdf", "anchor_text": "Bandit Book", "paragraph_index": 44}, {"url": "https://arxiv.org/pdf/1904.10040.pdf", "anchor_text": "A Survey on Practical Applications of Multi-Armed and Contextual Bandits", "paragraph_index": 45}], "all_paragraphs": ["Over the course of this series we\u2019ve looked at the framework and terminology that are used to define Multi-Armed Bandits. We then turned our attention to some of the algorithms that can be used to solve this problem, from the simple Greedy algorithms, up to the complex Bayesian approach of Thompson Sampling. Now there\u2019s only one question left to answer. Which of these approaches is the best at solving the bandit problem?", "For those not yet familiar with the Multi-Armed Bandit problem, or wishing to refresh their knowledge of any of the particular areas, the other parts in this series are as follows:", "All code for the bandit algorithms and testing framework can be found on github: Multi_Armed_Bandits", "Baby Robot is lost in the mall. Using Reinforcement Learning we want to help him find his way back to his mum. However, before he can even begin looking for her, he needs to recharge, from a set of power sockets that each give a slightly different amount of charge.", "Using the strategies from the multi-armed bandit problem we need to find the best socket, in the shortest amount of time, to allow Baby Robot to get charged up and on his way.", "Baby Robot has entered a charging room containing 5 different power sockets. Each of these sockets returns a slightly different amount of charge. We want to get Baby Robot charged up in the minimum amount of time, so we need to locate the best socket and then use it until charging is complete.", "This is identical to the Multi-Armed Bandit problem except that, instead of looking for a slot machine that gives the best payout, we\u2019re looking for a power socket that gives the most charge.", "To answer the question of which is the best bandit algorithm (in terms of the ones we\u2019ve looked at), we can re-frame the question in terms of our own problem: which algorithm will let Baby Robot get fully charged in the shortest amount of time?", "To run this experiment we first need to define exactly what we mean by being fully charged. For this we\u2019ll just arbitrarily define that Baby Robot is fully charged when he has enough charge to run for an hour (3600 seconds).", "With this definition we can now run each of our bandit algorithms for a maximum of 500 time steps, which yields the following results:", "(Note: The full test system used to generate these results is available in the github notebook.)", "From the graph above we can see that:", "So, from the point of view of charging Baby Robot, any of the Optimistic Greedy, UCB or Thompson Sampling algorithms would do the job. However, it should be noted that both Optimistic Greedy and UCB require a parameter to be set (these parameters are the initial values for Optimistic Greedy and the confidence value for UCB). Making a bad choice for these parameters could lead to a degraded performance of the algorithm.", "Since Thompson Sampling doesn\u2019t require a parameter to be set, this isn\u2019t an issue, and so this may be the deciding factor when choosing which algorithm to use.", "The socket problem we\u2019ve used up to now was deliberately made very simple, to allow the exploration and exploitation mechanisms of each algorithm to be illustrated. However, in terms of finding which algorithm is best, it has a couple of major drawbacks.", "To overcome these deficiencies in our original experiment, let\u2019s double the number of sockets and decrease the difference in the amount of charge the sockets can return, reducing this from 2 seconds of charge down to 0.2. With these values the output of the sockets now looks as follows:", "Now we have 10 sockets, with 0.2 seconds of charge difference between the mean reward of a socket and the next best socket. This means that there\u2019s a much greater overlap in the rewards that are returned.", "A couple of other points to note are:", "Running with this new setup generates the following results:", "Now we can see some separation in the performance of the algorithms:", "This difference in the algorithms becomes even more distinct as the number of sockets is increased and the spread of mean reward values is decreased, as shown below, for 100 sockets with a spread of 0.1 (note that now the maximum socket output is once again 12 (=0.1*100 +2):", "With 100 sockets it\u2019s interesting to note how both the UCB and Optimistic Greedy algorithms perform worse than the Epsilon Greedy algorithm. In fact, by the time that Thompson Sampling has reached maximum charge, neither of these other algorithms has yet surpassed the total mean reward of Epsilon Greedy.", "This can be seen to be caused by the priming rounds of the UCB and Optimistic Greedy algorithms, during which each socket is tested exactly once (note the lower gradient up to the 100th time step). So, for the first 100 time steps, UCB and Optimistic Greedy are working their way through each of the 100 sockets, whereas Epsilon Greedy and Thompson Sampling have already started to exploit the best sockets that they\u2019ve found.", "If we left the experiment to run for more time-steps, then UCB and Optimistic Greedy would probably exceed the mean total reward of Epsilon Greedy and reach full charge before it. However, if Baby Robot has chosen his sockets using the Thompson Sampling algorithm he\u2019ll already be fully charged and on his way.", "One major drawback of the basic bandit algorithms we\u2019ve seen is that they take no account of any available context information.", "For example, imagine that the power sockets were colour coded, with the colour giving an indication of the amount of charge that would be returned. If, after completing our trials, we found that blue sockets gave a lot of charge and yellow sockets gave very little, then it would make sense the next time we entered a charging station to favour blue sockets over yellow ones.", "The basic bandit algorithms simply take an action, collect a reward, and pay no attention to their current state. Therefore potentially helpful information from the current state, that could assist in choosing the best action, is simply ignored.", "Contextual bandits\u00b3 (also known as \u201cassociative bandits\u201d) address this limitation by using information from the current state to help guide their choice of action. As a result they can either be thought of as sophisticated bandit algorithms or as a simplified version of reinforcement learning.", "In the full reinforcement learning problem the action that is taken can lead to a change in state and therefore to new contextual information. As a result, the chosen action can have an impact on the future rewards that can be obtained. For example, in a game of chess, a move that looks good and that gives a large immediate reward (such as taking the opposition\u2019s queen) may in fact lead to you losing the game. It would therefore actually be considered to be a bad move. The true reward is delayed and isn\u2019t received until the game ends.", "In contextual bandits neither of these conditions exist; actions don\u2019t change the state and rewards are instant, not delayed.", "So, apart from charging Baby Robots or winning your fortune on the slot machines the next time you\u2019re in Vegas\u00b9, what exactly can Multi-Armed Bandit algorithms be used for?", "\u00b9Disclaimer: Vegas casino owners are notoriously touchy about people using algorithms in their casinos. I therefore accept no responsibility for any damages, physical or financial, incurred during the use of these algorithms!", "There are probably two main areas of use for Multi-Armed Bandits:", "The first is how we\u2019ve used them, as a stepping stone to full Reinforcement Learning. Many of the concepts, such as actions and rewards, from Multi-Armed Bandits are directly applicable to the full Reinforcement Learning (RL) problem. Effectively a Multi-Armed Bandit can be thought of as representing a single state in full Reinforcement Learning. Additionally, the greedy selection of actions, although maybe not the best approach to solving the bandit problem, is often used to choose between different actions in RL.", "The second main area of use for bandit algorithms is during real world testing. This can be in any field but is particularly prevalent in online commerce, healthcare and finance.", "For example, when assessing how changes to a web page affect its performance, where this can be measured in a variety of ways such as the sales generated from the page or the click through rate, a standard approach is to use A/B Testing. This takes two or more different variations of the page and then typically presents each of these equally to the site users to see how they perform. After a predefined period of time the statistics from each of the pages are compared and the one that has performed the best is then adopted as the winning page.", "Quite clearly there is one major drawback to this form of testing: for the duration of the trial an equal number of customers are being sent to an under performing web page. If this page is performing particularly badly it could have a potentially large negative impact on your site\u2019s overall performance. By having a period solely dedicated to exploration A/B Testing continues to investigate badly performing options.", "In contrast, in a Multi-Armed Bandit approach, pages are presented to users according to their measured relative performances. Pages that are performing well will increasingly be shown and those that are under-performing will be shown less often. In this way changes to a website may be tested. Good features, that improve performance, are instantly promoted to give a positive impact and bad features, that in A/B testing would continue to give a negative impact for the duration of the test period, are quickly dropped.", "Multi-Armed Bandits are used in a similar way in clinical trials. Obviously, during a drug trial, it would be a bad idea to continue to administer a drug that is causing patients to exhibit negative side effects. Therefore Multi-Armed Bandit algorithms have been employed to decide on the drugs and dosages to give to patients to maximise positive outcomes.", "For a more comprehensive look at the practical uses of Bandit Algorithms I recommend checking out the following paper:", "\u201cA Survey on Practical Applications of Multi-Armed and Contextual Bandits\u201d, Djallel Bouneffouf, Irina Rish (2019)", "We\u2019ve seen that, when faced with the problem of having to choose between various different options, where the reward for selecting any of those options is initially unknown, that algorithms such as Thompson Sampling or Upper Confidence Bounds (UCB) perform much better than simply choosing from the options at random. With these algorithms we can minimise the number of times we try bad actions and maximise the number of times we take the best action.", "By employing these algorithms in the socket selection problem we were able to quickly locate and exploit the best socket and get Baby Robot charged up and on his way.", "In subsequent articles we\u2019ll look at more advanced Reinforcement Learning (RL) techniques, some of which make use of these bandit algorithms. Using these we\u2019ll help Baby Robot to find his way back to his mum!", "Note: Although we\u2019ve examined quite a few methods to solve the Multi-Armed Bandit problem, we\u2019ve really just scratched the surface in terms of all available algorithms. Take a look at the Bandit Book if you\u2019d like to see a whole lot more.", "[4] \u201cA Survey on Practical Applications of Multi-Armed and Contextual Bandits\u201d, Djallel Bouneffouf, Irina Rish (2019)", "All code for the bandit algorithms and testing framework can be found on github:", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Ph.D., \"The evolution of artificial neural networks\""], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F24b4adfcabb&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-comparison-of-bandit-algorithms-24b4adfcabb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-comparison-of-bandit-algorithms-24b4adfcabb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-comparison-of-bandit-algorithms-24b4adfcabb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-comparison-of-bandit-algorithms-24b4adfcabb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----24b4adfcabb--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----24b4adfcabb--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@tinkertytonk?source=post_page-----24b4adfcabb--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk?source=post_page-----24b4adfcabb--------------------------------", "anchor_text": "Steve Roberts"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6b6735266652&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-comparison-of-bandit-algorithms-24b4adfcabb&user=Steve+Roberts&userId=6b6735266652&source=post_page-6b6735266652----24b4adfcabb---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F24b4adfcabb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-comparison-of-bandit-algorithms-24b4adfcabb&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F24b4adfcabb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-comparison-of-bandit-algorithms-24b4adfcabb&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/tagged/baby-robot-guide", "anchor_text": "A Baby Robot\u2019s Guide To Reinforcement Learning"}, {"url": "https://unsplash.com/@jdent?utm_source=medium&utm_medium=referral", "anchor_text": "Jason Dent"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://towardsdatascience.com/multi-armed-bandits-part-1-b8d33ab80697", "anchor_text": "Part 1: Mathematical Framework and Terminology"}, {"url": "https://towardsdatascience.com/multi-armed-bandits-part-2-5834cb7aba4b", "anchor_text": "Part 2: The Bandit Framework"}, {"url": "https://towardsdatascience.com/bandit-algorithms-34fd7890cb18", "anchor_text": "Part 3: Bandit Algorithms"}, {"url": "https://towardsdatascience.com/bandit-algorithms-34fd7890cb18", "anchor_text": "The Greedy Algorithm"}, {"url": "https://towardsdatascience.com/bandit-algorithms-34fd7890cb18", "anchor_text": "The Optimistic-Greedy Algorithm"}, {"url": "https://towardsdatascience.com/bandit-algorithms-34fd7890cb18", "anchor_text": "The Epsilon-Greedy Algorithm (\u03b5-Greedy)"}, {"url": "https://towardsdatascience.com/bandit-algorithms-34fd7890cb18", "anchor_text": "Regret"}, {"url": "https://towardsdatascience.com/the-upper-confidence-bound-ucb-bandit-algorithm-c05c2bf4c13f", "anchor_text": "Part 4: The Upper Confidence Bound (UCB) Bandit Algorithm"}, {"url": "https://towardsdatascience.com/thompson-sampling-fc28817eacb8", "anchor_text": "Part 5: Thompson Sampling"}, {"url": "https://towardsdatascience.com/thompson-sampling-fc28817eacb8#f535", "anchor_text": "Bernoulli Thompson Sampling"}, {"url": "https://towardsdatascience.com/thompson-sampling-fc28817eacb8#16db", "anchor_text": "Gaussian Thompson Sampling"}, {"url": "https://towardsdatascience.com/thompson-sampling-using-conjugate-priors-e0a18348ea2d", "anchor_text": "Part 5b: Thompson Sampling using Conjugate Priors"}, {"url": "https://github.com/WhatIThinkAbout/BabyRobot/tree/master/Multi_Armed_Bandits", "anchor_text": "Multi_Armed_Bandits"}, {"url": "https://github.com/WhatIThinkAbout/BabyRobot/blob/master/Multi_Armed_Bandits/Part%206%20-%20A%20Comparison%20of%20Bandit%20Algorithms.ipynb", "anchor_text": "github notebook"}, {"url": "https://towardsdatascience.com/bandit-algorithms-34fd7890cb18", "anchor_text": "regret"}, {"url": "https://arxiv.org/pdf/1904.10040.pdf", "anchor_text": "A Survey on Practical Applications of Multi-Armed and Contextual Bandits"}, {"url": "https://tor-lattimore.com/downloads/book/book.pdf", "anchor_text": "Bandit Book"}, {"url": "http://www.incompleteideas.net/book/RLbook2020.pdf", "anchor_text": "Reinforcement Learning: An Introduction"}, {"url": "https://arxiv.org/abs/1707.02038", "anchor_text": "A Tutorial on Thompson Sampling"}, {"url": "https://arxiv.org/abs/1802.04064", "anchor_text": "A Contextual Bandit Bake-off"}, {"url": "https://arxiv.org/pdf/1904.10040.pdf", "anchor_text": "A Survey on Practical Applications of Multi-Armed and Contextual Bandits"}, {"url": "https://github.com/WhatIThinkAbout/BabyRobot", "anchor_text": "WhatIThinkAbout/BabyRobotA Baby Robot's Guide to Reinforcement Learning. Contribute to WhatIThinkAbout/BabyRobot development by creating an\u2026github.com"}, {"url": "https://towardsdatascience.com/state-values-and-policy-evaluation-ceefdd8c2369", "anchor_text": "State Values and Policy EvaluationAn Introduction to Reinforcement Learning: Part 1towardsdatascience.com"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----24b4adfcabb---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/multi-armed-bandits?source=post_page-----24b4adfcabb---------------multi_armed_bandits-----------------", "anchor_text": "Multi Armed Bandits"}, {"url": "https://medium.com/tag/baby-robot-guide?source=post_page-----24b4adfcabb---------------baby_robot_guide-----------------", "anchor_text": "Baby Robot Guide"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F24b4adfcabb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-comparison-of-bandit-algorithms-24b4adfcabb&user=Steve+Roberts&userId=6b6735266652&source=-----24b4adfcabb---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F24b4adfcabb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-comparison-of-bandit-algorithms-24b4adfcabb&user=Steve+Roberts&userId=6b6735266652&source=-----24b4adfcabb---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F24b4adfcabb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-comparison-of-bandit-algorithms-24b4adfcabb&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----24b4adfcabb--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F24b4adfcabb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-comparison-of-bandit-algorithms-24b4adfcabb&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----24b4adfcabb---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----24b4adfcabb--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----24b4adfcabb--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----24b4adfcabb--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----24b4adfcabb--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----24b4adfcabb--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----24b4adfcabb--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----24b4adfcabb--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----24b4adfcabb--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Steve Roberts"}, {"url": "https://medium.com/@tinkertytonk/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "593 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6b6735266652&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-comparison-of-bandit-algorithms-24b4adfcabb&user=Steve+Roberts&userId=6b6735266652&source=post_page-6b6735266652--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F1d5f26d16450&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-comparison-of-bandit-algorithms-24b4adfcabb&newsletterV3=6b6735266652&newsletterV3Id=1d5f26d16450&user=Steve+Roberts&userId=6b6735266652&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}