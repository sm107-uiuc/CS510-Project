{"url": "https://towardsdatascience.com/bfgs-in-a-nutshell-an-introduction-to-quasi-newton-methods-21b0e13ee504", "time": 1683017026.284214, "path": "towardsdatascience.com/bfgs-in-a-nutshell-an-introduction-to-quasi-newton-methods-21b0e13ee504/", "webpage": {"metadata": {"title": "BFGS in a Nutshell: An Introduction to Quasi-Newton Methods | by Adrian Lam | Towards Data Science", "h1": "BFGS in a Nutshell: An Introduction to Quasi-Newton Methods", "description": "Surely anyone who has dabbled in machine learning is familiar with gradient descent, and possibly even its close counterpart, stochastic gradient descent. If you have more than dabbled, then you\u2019re\u2026"}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/Woodbury_matrix_identity", "anchor_text": "Woodbury formula", "paragraph_index": 46}, {"url": "https://en.wikipedia.org/wiki/Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno_algorithm", "anchor_text": "1] BFGS on Wikipedia", "paragraph_index": 54}, {"url": "https://en.wikipedia.org/wiki/Quasi-Newton_method", "anchor_text": "2] Quasi-Newton methods on Wikipedia", "paragraph_index": 54}, {"url": "https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization", "anchor_text": "3] Newton\u2019s method on Wikipedia", "paragraph_index": 54}, {"url": "https://link.springer.com/book/10.1007/978-0-387-40065-5", "anchor_text": "4] J. Nocedal and S. Wright. Numerical Optimization (Chapter 6). Springer, 2nd edition, 2006.", "paragraph_index": 55}, {"url": "http://www.numerical.rl.ac.uk/people/nimg/oupartc/lectures/raphael/lectures/lecture4.pdf", "anchor_text": "5] Oxford University lecture notes by R. Hauser", "paragraph_index": 55}, {"url": "https://ocw.mit.edu/courses/mathematics/18-335j-introduction-to-numerical-methods-spring-2019/week-11/MIT18_335JS19_lec30.pdf", "anchor_text": "6] MIT 18.335 lecture notes by S. G. Johnson", "paragraph_index": 55}], "all_paragraphs": ["Surely anyone who has dabbled in machine learning is familiar with gradient descent, and possibly even its close counterpart, stochastic gradient descent. If you have more than dabbled, then you\u2019re surely also aware of the fancier extensions like gradient descent with momentum and Adam optimization.", "Perhaps less well-known are a class of optimization algorithms known as quasi-Newton methods. Though these optimization methods are less fervently advertised in popular accounts of machine learning, they hold an important place in the arsenal of machine learning practitioners.", "The goal of this article is to provide an introduction to the mathematical formulation of BFGS optimization, by far the most widely used quasi-Newton method. As such, the focus will be on the mathematical derivation of results, rather than the application of BFGS in code. It is my hope that by the end of this article, you will have gained an appreciation for what BFGS is, how it works, and why it was developed. A top priority of mine when crafting this article was to make it as accessible as possible. To this end, any non-trivial derivation will be explicitly shown, and the dreaded phrase \u201cit is straightforward to show\u201d will never make an appearance.", "Instead of jumping right into quasi-Newton methods and BFGS, my strategy is to start off by doing a run-through of a few of the more basic optimization methods first, and explore their deficiencies. This would then provide a natural segue to quasi-Newton methods and how they aim to address these deficiencies. This might seem like a long-winded way of getting to the main topic, but I believe it is well-justified if we are to truly appreciate the motivation behind developing BFGS, and to get a sense of where it stands within the landscape of optimization methods.", "So without further ado, let\u2019s start from the very beginning, with gradient descent.", "We begin with a lightning quick review of gradient descent, which is an iterative method for finding a local minimum of a real-valued, differentiable objective function f(x).", "To find a local minimum, we start off at a random initial point and iteratively take steps proportional to the negative gradient of the function f at the current point. Since the gradient of a function points in the direction of steepest ascent, the negative gradient points in the direction of steepest descent, and thus at each step of gradient descent, we are moving in the direction where f(x) decreases the fastest. Symbolically, an iteration of gradient descent is written as", "where \ud835\udefc is a positive real number known as the learning rate, which controls the step size taken at each iteration. Too large of a learning rate and our model diverges out of control; too small of a learning rate and our model becomes highly inefficient, taking unnecessarily long to converge to the minimum. There is no a priori way of choosing a good learning rate that lies in the sweet spot between these two extremes, and in practice the optimal learning rate is typically found empirically, by looping through various values and seeing how they perform.", "Looking at equation (1), we see that gradient descent is a first-order optimization method, as it uses first-order information (ie. the gradient) to find the minimum. While this often reliably gets the job done, its main disadvantage lies in the fact that it is quite inefficient, even for a suitably chosen learning rate. By approximating our objective function linearly at each step, we are only working with very limited local information, and we thus have to be cautious and restrain ourselves to small step sizes at each iteration.", "Perhaps we can do better, by obtaining more local information of the objective function at each iteration, in hopes that we can make more well-informed steps. A natural extension would be to look at the second-order behavior of the objective function.", "For simplicity, let\u2019s first consider a function f of one variable. Consulting Taylor, we know that the second order approximation of f about a point x\u1d0b+\u03f5 is given by", "Our Taylor approximation is minimized when", "which corresponds to a step size of", "We can thus try a new iteration scheme", "which also takes into account the second-order behavior of the objective function.", "Generalizing to n dimensions, the first derivative is replaced by the gradient \u2207f and the second derivative is replaced by the Hessian H.", "In n dimensions, our new iterative scheme is thus written as:", "This method of optimization, where we take into account the objective function\u2019s second order behavior in addition to its first order behavior, is known as Newton\u2019s method. In each iteration k, Newton\u2019s method approximates the function f at the point x\u1d0b with a paraboloid, and then proceeds to minimize that approximation by stepping to the minimum of that paraboloid (there is actually a caveat here that we will get to later). Notice that in contrast to regular gradient descent, there is no longer a need to set a learning rate parameter, because now our step size is determined exactly by the distance to the minimum of the fitted parabola at that point.To see how much we stand to gain from computing the Hessian, consider figure 1 below where we want to minimize our loss function, starting from the point (-5, 5). For a suitably chosen learning rate, gradient descent takes 229 steps to converge to the minimum. On the other hand, Newton\u2019s method converges to the minimum in only six steps!", "If this is your first time learning about Newton\u2019s method, you\u2019re probably just as shocked as I was when it was my first time. \u201cSurely this is too good to be true!\u201d you might exclaim. Why ever use gradient descent if Newton\u2019s method converges to the minimum so much faster?", "It turns out that the benefits we gain from Newton\u2019s method comes at a cost. There are two main issues with Newton\u2019s method:", "Despite its limited practical use, Newton\u2019s method is still of great theoretical value. We did see how efficient the second-order optimization can be if used correctly. What if we could somehow leverage the efficiency gained from considering second-order behavior, but avoid the computational cost of calculating the inverse Hessian? In other words, can we get a sort of hybrid between gradient descent and Newton\u2019s method, where we can have faster convergence than gradient descent, but lower operational cost per iteration than Newton\u2019s method?", "It turns out there\u2019s a class of optimization methods, called quasi-Newton methods, that does just that.", "We went through Newton\u2019s method for optimization, which, in contrast to vanilla gradient descent, leverages second-order behavior in addition to first-order behavior at each step, making for a much faster convergence to the minimum. However, we also saw the downsides to Newton\u2019s method \u2014 one of which is how computationally expensive it is to both calculate the Hessian and to invert it, especially when dimensions get large. Quasi-Newton methods are a class of optimization methods that attempt to address this issue.", "Recall that in Newton\u2019s method, we make the following update at each iteration:", "where the Hessian is computed and inverted at each step. In quasi-Newton methods, instead of computing the actual Hessian, we just approximate it with a positive-definite matrix B, which is updated from iteration to iteration using information computed from previous steps (we require B to be positive-definite because we are optimizing a convex function, and this automatically takes care of the symmetry requirement of the Hessian).We immediately see that this scheme would yield a much less costly algorithm compared to Newton\u2019s method, because instead of computing a large amount of new quantities at each iteration, we\u2019re largely making use of previously computed quantities.", "At this stage the nature of the update scheme for B has been left vague, because the specific update for B is given by the specific quasi-Newton method used. There is however one condition that all quasi-Newton methods must share, and that is the Hessian approximation B must satisfy the quasi-Newton condition (or secant equation):", "which is obtained from the first order Taylor expansion of \u2207 f(x\u1d0b\u208a\u2081) about \u2207f(x\u1d0b) (we can also view this as sort of a finite difference equation of the gradient itself). We can rewrite the quasi-Newton condition more succinctly by letting y\u1d0b = \u2207 f(x\u1d0b\u208a\u2081) \u2212\u2207f(x\u1d0b) and \u0394x\u1d0b = x\u1d0b\u208a\u2081\u2212x\u1d0b, so that we have", "Furthermore, we can verify the positive-definiteness of our Hessian approximation B by pre-multiplying the quasi-Newton condition with \u0394x\u1d0b\u1d40, so our requirement for positive-definiteness (ie. the curvature condition) can be expressed as \u0394x\u1d0b\u1d40 y\u1d0b>0.Before we go further however, let\u2019s take a step back and consider only one dimension, where our intuition is strong. The secant equation, in one dimension, is", "and the curvature condition is satisfied by requiring (f\u2032\u1d0b\u208a\u2081\u2212f\u2032\u1d0b)/(x\u1d0b\u208a\u2081\u2212x\u1d0b) > 0. Solving for f\u2033 and substituting into Newton\u2019s method in one dimension, we get", "We thus have here an optimization method that leverages the (approximate) second-order behavior of the objective function in order to converge faster, without actually taking any second derivatives. Instead, at each iteration k+1, we construct an approximate inverse second derivative using only quantities from previous steps \u2014 in this case the first derivative from the previous two iterations k and k\u22121.", "This method \u2014 approximating Newton\u2019s method in one dimension by replacing the second derivative with its finite difference approximation, is known as the secant method, a subclass of quasi-Newton methods.", "Now back to our n-dimensional quasi-Newton condition (equation 4). At first glance, it looks like we could just work analogously to our one-dimensional case: we can simply solve for B\u1d0b\u208a\u2081 directly, and substitute it into Newton\u2019s iterative step (2). Job done, right?", "Not quite, actually. Despite looking deceptively similar to our one-dimensional case, remember that B is in general a symmetric n \u00d7 n matrix, with n(n+1)/2 components, whereas our equation only has n components. This means that we\u2019re trying to solve for n(n+1)/2 unknowns with only n equations, making this an underdetermined system. In fact, we were only able to find a unique solution to the secant equation in one dimension because the unknown components of the Hessian, being 1(1+1)/2 = 1, coincide with the one equation that we have. In general, there are n(n+1)/2 \u2212 n= n(n\u22121)/2 unknowns that we cannot solve for.", "This is where quasi-Newton methods come in, where the secant method is generalized to multidimensional objective functions. Instead of approximating the second derivative merely by using the finite difference like in the secant method, quasi-Newton methods have to impose additional constraints. The common theme still runs through though \u2014 at each iteration k+1, the new Hessian approximation B\u1d0b\u208a\u2081 is obtained using only previous gradient information.", "Various quasi-Newton methods have been developed over the years, and they differ in how the approximate Hessian B is updated at each iteration. As of today, the most widely used quasi-Newton method is the BFGS method, and this will be our focus for the remaining of this article.", "It\u2019s been somewhat of a long trek so far, so let\u2019s pause for moment and do a quick recap before moving on. Our objective is to find the minimum of a (twice-differentiable) convex function. A simple approach to this is gradient descent \u2014 starting from some initial point, we slowly move downhill by taking iterative steps proportional to the negative gradient of the function at each point. Newton then taught us that we can take far less steps and converge much quicker to the minimum if we also consider the second-order behavior of the function, by computing the (inverse) Hessian at each step. This comes at a cost, however, as calculating (and inverting) the Hessian takes a lot of resources. A compromise would be to instead just approximate the Hessian at each step, subject to the quasi-Newton condition. In one dimension, this just amounts to approximating the second derivative by replacing it with a finite difference approximation; this method of optimization is called the secant method. In more than one dimension, the quasi-Newton condition does not uniquely specify the Hessian estimate B, and we need to impose further constraints on B to solve for it. Different quasi-Newton methods offer their own method for constraining the solution. Here, we will focus on one of the most popular methods, known as the BFGS method. The name is an acronym of the algorithm\u2019s creators: Broyden, Fletcher, Goldfarb, and Shanno, who each came up with the algorithm independently in 1970 [7\u201310].", "We saw previously that in n>1 dimensions, the quasi-Newton condition (4) is underdetermined. To determine an update scheme for B then, we will need to impose additional constraints. One such constrain that we\u2019ve already mentioned is the symmetry and positive-definiteness of B \u2014 these properties should be preserved in each update. Another desirable property we want is for B\u1d0b\u208a\u2081 to be sufficiently close to B\u1d0b at each update k+1. A formal way to characterize the notion of \u201ccloseness\u201d for matrices is the matrix norm. Thus, we should look to minimize the quantity ||B\u1d0b\u208a\u2081\u2212B\u1d0b||. Putting all our conditions together, we can formulate our problem as", "Referring back to Newton\u2019s method, we recall that it\u2019s actually the Hessian\u2019s inverse, instead of the Hessian itself, that makes an appearance. So instead of computing the approximate Hessian B, we can just as well compute the approximate inverse B\u207b\u00b9 directly. We can thus alternatively formulate (5) as", "(Notice the inverted quasi-Newton condition.) To repeat, we minimize the change in B\u207b\u00b9 at each iteration, subject to the (inverted) quasi-Newton condition and the requirement that it be symmetric. We have also previously mentioned many times the requirement that B (and by extension B\u207b\u00b9) be positive-definite; it turns out this property comes along for the ride when we solve this problem. We will show this eventually; in the meantime, I\u2019ll have to ask you to hold your breath.", "To solve for B\u1d0b\u208a\u2081, we still have to specify the particular matrix norm we\u2019re using in (6). Different choices of norms result in different quasi-Newton methods, characterized by a different resulting update scheme. In the BFGS method, the norm is chosen to be the Frobenius norm:", "which is just the square root of the sum of the absolute value squared of the matrix elements.", "Solving (6) from scratch is no easy feat, and we will not go through it here. For the highly mathematically inclined reader, refer to references [4-6] for detailed derivations. For our purposes, it suffices to know that the problem ends up being equivalent to updating our approximate Hessian at each iteration by adding two symmetric, rank-one matrices U and V:", "To fulfill the aforementioned conditions, the update matrices can then be chosen to be of the form U = a u u\u1d40 and V = b v v\u1d40, where u and v are linearly independent non-zero vectors, and a and b are constants. The outer product of any two non-zero vectors is always rank one, and since U and V are both the outer product of a vector with itself, they are also symmetric, taking care of the requirement that the Hessian remains symmetric upon updates. We then have", "Since U\u1d0b and V\u1d0b are rank-one (and u and v are linearly independent), their sum is rank-two, and an update of this form is known as a rank-two update. Thus if we start out with a symmetric matrix B\u2080, then B\u1d0b will remain symmetric for all k. The rank-two condition guarantees the \u201ccloseness\u201d of B\u1d0b and B\u1d0b\u208a\u2081at each iteration.Furthermore, we have to impose the quasi-Newton condition B\u1d0b\u208a\u2081 \u0394x = y\u1d0b :", "At this point, a natural choice for u and v would be u = y\u1d0b and v = B\u1d0b \u0394x\u1d0b. We then have", "Substituting a and b back into (7), we have the BFGS update in all its glory:", "As advertised, we are updating the approximate Hessian at each iteration using only previous gradient information. Note, however, that in practice we actually want to compute B\u207b\u00b9 directly, because it\u2019s the inverse Hessian that appears in a Newton update. To invert (8), we make use of the Woodbury formula, which tells us how to invert the sum of an invertible matrix A and a rank-k correction:", "We can thus obtain the formula for a BFGS update for the inverse of B. We first rewrite the update in a more suitable form for applying the Woodbury formula (to avoid clutter, we\u2019ll suppress the subscripts k+1 and k, instead denoting the update as B\u208a):", "So while equation (8) is cleaner and allows for more straightforward analysis, equation (9) is what we actually want to compute in practice. Again, each update is made only requiring previous gradient information. At each iteration, we update the value of B\u207b\u00b9 using only the values of \u0394x\u1d0b = x\u1d0b\u208a\u2081\u2212x\u1d0b and y\u1d0b = \u2207 f(x\u1d0b\u208a\u2081) \u2212\u2207f(x\u1d0b) of the previous steps, in accordance to equation (9). By directly estimating the inverse Hessian at each step, we are completely doing away with those laborious O(n\u00b3) operations of inverting the Hessian, as in Newton\u2019s method.", "We are now also in a position to show that our rank-two update preserves positive-definiteness. From (9), we can calculate the quantity", "for a non-zero vector z. If B\u1d0b\u207b\u00b9 is positive-definite, then both terms are non-negative, with the second term being zero only if \u0394x\u1d40 z = 0. Positive-definiteness is thus preserved by BFGS\u2019s rank-two update. If we were to go back to (7) and repeat the development but with a rank-one update instead, we wouldn\u2019t actually end up with a positive-definite preserving update [4, 5]. This explains why BFGS uses a rank-two update: it is the update with the lowest rank that preserves positive-definiteness.", "One final implementation detail that we previously glossed over: since an update of B\u207b\u00b9 depends on its previous value, we have to initialize B\u2080\u207b\u00b9 to kick off the algorithm. There are two natural ways to do this. The first approach is to set B\u2080\u207b\u00b9 to the identity matrix, in which case the first step will be equivalent to vanilla gradient descent, and subsequent updates will incrementally refine it to get closer to the inverse Hessian. Another approach would be to compute and invert the true Hessian at the initial point. This would start the algorithm off with more efficient steps, but comes at an initial cost of computing the true Hessian and inverting it.", "Being an introductory piece, the aim of this discussion was to present quasi-Newton methods and BFGS in a manner that is as accessible as possible. As such, we\u2019ve really only just skimmed the surface of all the mathematical intricacies and performance considerations that underlie BFGS and other quasi-Newton methods. There is a wealth of resources out there on this subject, a very small selection of which I have included in the references below. I encourage readers who are interested in diving deeper to seek out these resources. Be forewarned though \u2014 some of these references aren\u2019t easy!", "Thank you for making it all the way to the end! Hopefully you\u2019ve gained an appreciation for how BFGS works under the hood, so that next time you call scipy.optimize.minimize(method=\u2018BFGS'), instead of viewing it as a complete black box, you\u2019ll smile to yourself knowingly, fully aware that BFGS is simply iteratively making positive-definite preserving rank-two updates to the loss function\u2019s approximate inverse Hessian, to efficiently find you its minimum.", "Cursory run-throughs:[1] BFGS on Wikipedia[2] Quasi-Newton methods on Wikipedia[3] Newton\u2019s method on Wikipedia", "Advanced references on BFGS:[4] J. Nocedal and S. Wright. Numerical Optimization (Chapter 6). Springer, 2nd edition, 2006.[5] Oxford University lecture notes by R. Hauser[6] MIT 18.335 lecture notes by S. G. Johnson", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data scientist, former astrophysics researcher | Brown University"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F21b0e13ee504&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbfgs-in-a-nutshell-an-introduction-to-quasi-newton-methods-21b0e13ee504&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbfgs-in-a-nutshell-an-introduction-to-quasi-newton-methods-21b0e13ee504&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbfgs-in-a-nutshell-an-introduction-to-quasi-newton-methods-21b0e13ee504&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbfgs-in-a-nutshell-an-introduction-to-quasi-newton-methods-21b0e13ee504&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----21b0e13ee504--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----21b0e13ee504--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://adrian-lam.medium.com/?source=post_page-----21b0e13ee504--------------------------------", "anchor_text": ""}, {"url": "https://adrian-lam.medium.com/?source=post_page-----21b0e13ee504--------------------------------", "anchor_text": "Adrian Lam"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F19b24be5cab4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbfgs-in-a-nutshell-an-introduction-to-quasi-newton-methods-21b0e13ee504&user=Adrian+Lam&userId=19b24be5cab4&source=post_page-19b24be5cab4----21b0e13ee504---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F21b0e13ee504&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbfgs-in-a-nutshell-an-introduction-to-quasi-newton-methods-21b0e13ee504&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F21b0e13ee504&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbfgs-in-a-nutshell-an-introduction-to-quasi-newton-methods-21b0e13ee504&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/tagged/getting-started", "anchor_text": "Getting Started"}, {"url": "https://en.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations#Matrix_algebra", "anchor_text": "inverting it scales as O(n\u00b3)"}, {"url": "https://en.wikipedia.org/wiki/Woodbury_matrix_identity", "anchor_text": "Woodbury formula"}, {"url": "https://en.wikipedia.org/wiki/Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno_algorithm", "anchor_text": "1] BFGS on Wikipedia"}, {"url": "https://en.wikipedia.org/wiki/Quasi-Newton_method", "anchor_text": "2] Quasi-Newton methods on Wikipedia"}, {"url": "https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization", "anchor_text": "3] Newton\u2019s method on Wikipedia"}, {"url": "https://link.springer.com/book/10.1007/978-0-387-40065-5", "anchor_text": "4] J. Nocedal and S. Wright. Numerical Optimization (Chapter 6). Springer, 2nd edition, 2006."}, {"url": "http://www.numerical.rl.ac.uk/people/nimg/oupartc/lectures/raphael/lectures/lecture4.pdf", "anchor_text": "5] Oxford University lecture notes by R. Hauser"}, {"url": "https://ocw.mit.edu/courses/mathematics/18-335j-introduction-to-numerical-methods-spring-2019/week-11/MIT18_335JS19_lec30.pdf", "anchor_text": "6] MIT 18.335 lecture notes by S. G. Johnson"}, {"url": "https://academic.oup.com/imamat/article-abstract/6/1/76/746016?redirectedFrom=fulltext", "anchor_text": "[7] C. G. Broyden, \u201cThe convergence of a class of double-rank minimization algorithms\u201d, J. Inst. Math. Appl. 6, 76\u201390 (1970)"}, {"url": "https://academic.oup.com/comjnl/article/13/3/317/345520", "anchor_text": "[8] R. Fletcher, \u201cA new approach to variable metric algorithms\u201d, Comp. J. 13, 317\u2013322 (1970)"}, {"url": "https://www.ams.org/journals/mcom/1970-24-109/S0025-5718-1970-0258249-6/home.html", "anchor_text": "[9] D. F. Goldfarb, \u201cA family of variable-metric methods derived by variational means\u201d, Math. Comp. 24, 23\u201326 (1970)"}, {"url": "https://www.ams.org/journals/mcom/1970-24-111/S0025-5718-1970-0274029-X/home.html", "anchor_text": "[10] D. Shanno, \u201cConditioning of quasi-Newton methods for function minimization\u201d, Math. Comp. 24, 647\u2013656 (1970)"}, {"url": "https://medium.com/tag/optimization?source=post_page-----21b0e13ee504---------------optimization-----------------", "anchor_text": "Optimization"}, {"url": "https://medium.com/tag/mathematics?source=post_page-----21b0e13ee504---------------mathematics-----------------", "anchor_text": "Mathematics"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----21b0e13ee504---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/editors-pick?source=post_page-----21b0e13ee504---------------editors_pick-----------------", "anchor_text": "Editors Pick"}, {"url": "https://medium.com/tag/getting-started?source=post_page-----21b0e13ee504---------------getting_started-----------------", "anchor_text": "Getting Started"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F21b0e13ee504&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbfgs-in-a-nutshell-an-introduction-to-quasi-newton-methods-21b0e13ee504&user=Adrian+Lam&userId=19b24be5cab4&source=-----21b0e13ee504---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F21b0e13ee504&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbfgs-in-a-nutshell-an-introduction-to-quasi-newton-methods-21b0e13ee504&user=Adrian+Lam&userId=19b24be5cab4&source=-----21b0e13ee504---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F21b0e13ee504&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbfgs-in-a-nutshell-an-introduction-to-quasi-newton-methods-21b0e13ee504&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----21b0e13ee504--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F21b0e13ee504&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbfgs-in-a-nutshell-an-introduction-to-quasi-newton-methods-21b0e13ee504&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----21b0e13ee504---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----21b0e13ee504--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----21b0e13ee504--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----21b0e13ee504--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----21b0e13ee504--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----21b0e13ee504--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----21b0e13ee504--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----21b0e13ee504--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----21b0e13ee504--------------------------------", "anchor_text": ""}, {"url": "https://adrian-lam.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://adrian-lam.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Adrian Lam"}, {"url": "https://adrian-lam.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "93 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F19b24be5cab4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbfgs-in-a-nutshell-an-introduction-to-quasi-newton-methods-21b0e13ee504&user=Adrian+Lam&userId=19b24be5cab4&source=post_page-19b24be5cab4--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F52af4bd8c9d0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbfgs-in-a-nutshell-an-introduction-to-quasi-newton-methods-21b0e13ee504&newsletterV3=19b24be5cab4&newsletterV3Id=52af4bd8c9d0&user=Adrian+Lam&userId=19b24be5cab4&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}