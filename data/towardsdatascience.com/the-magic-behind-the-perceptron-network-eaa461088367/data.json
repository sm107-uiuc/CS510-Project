{"url": "https://towardsdatascience.com/the-magic-behind-the-perceptron-network-eaa461088367", "time": 1682994975.4992452, "path": "towardsdatascience.com/the-magic-behind-the-perceptron-network-eaa461088367/", "webpage": {"metadata": {"title": "The magic behind the perceptron network | by Adriano Vinhas | Towards Data Science", "h1": "The magic behind the perceptron network", "description": "This story is part of a series I am creating about neural networks. This chapter is dedicated to a very simple type of neural network whose creation is attributed to Frank Rosenblatt after his\u2026"}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/Frank_Rosenblatt", "anchor_text": "Frank Rosenblatt", "paragraph_index": 0}, {"url": "https://en.wikipedia.org/wiki/Perceptron", "anchor_text": "perceptron", "paragraph_index": 0}, {"url": "https://medium.com/@adrianovinhas/a-gentle-introduction-to-neural-networks-14e5c02bafe", "anchor_text": "last article", "paragraph_index": 1}, {"url": "https://medium.com/@adrianovinhas/a-gentle-introduction-to-neural-networks-14e5c02bafe", "anchor_text": "chapter 0", "paragraph_index": 6}, {"url": "https://medium.com/@jayeshbahire/the-xor-problem-in-neural-networks-50006411840b", "anchor_text": "XOR problem", "paragraph_index": 19}, {"url": "https://twitter.com/adrianovinhas", "anchor_text": "Twitter", "paragraph_index": 23}, {"url": "https://www.linkedin.com/in/adrianovinhas/", "anchor_text": "LinkedIn", "paragraph_index": 23}], "all_paragraphs": ["This story is part of a series I am creating about neural networks. This chapter is dedicated to a very simple type of neural network whose creation is attributed to Frank Rosenblatt after his research during the 50s and 60s. I am going to talk about the perceptron.", "The perceptron is an enhancement of the McCulloch and Pitts neuron I mentioned in my last article. Similarly to the McCulloch and Pitts model, it is a single neuron that uses the threshold function as the activation function, and a set of connections with different strengths which connect several inputs to the neuron. However, the major breakthrough introduced by Rosenblatt was suggesting a way that a neural network could learn the optimal weights values by itself. These weights could now be numerical and determined by the learning algorithm rather than handcrafted and limited to a boolean fashion (positive/negative). The way these weight values were optimised was by using a learning strategy called supervised learning.", "Supervised learning is a type of learning that implies the existence of a supervisor/teacher to guide the model during the learning process. In practice, it means that the network is presented with several examples and their correct answers to provide some guidance on what can be improved. Then, the network adjusts its weights based on the answers that it got wrong, so that in a future iteration the network can increase the number of examples that were predicted correctly.", "The perceptron was designed for a specific type of Machine Learning problems: binary classification problems. A classification problem is one where our set of possible answers is finite and limited to a set of categories/classes (e.g. detecting if an email is spam or not). In our perceptron case, the reason why it solves solely binary classification problems is because of the nature of its activation function. I mentioned in the last article that the output y of a neural network is the output of the activation function f(z). Given that the perceptron uses the threshold function as activation and this function has two possible outputs, 0 or 1, the output will be then conditioned to distinguish solely between two different classes. You can imagine this as a neuron trying to decide between turning a light off or on.", "Let\u2019s spice things up. A possible way to make our network architecture a bit more complex is by placing several perceptrons and making sure that all the inputs are connected to each one of the perceptrons that we placed. The illustration of what I have just described is depicted on figure 1. In Machine Learning terminology one considers this group of neurons as a fully-connected layer or dense layer because each one of the inputs has a connection to each one of the neurons. Neural networks can have several layers and this concept will be explored in detail in one of my next articles, let\u2019s keep it simple for now.", "In figure 1 there are 3 inputs, each one of them is connected to 5 different neurons. The green square represents the plot of the threshold activation function.", "As there is a connection from each one of the inputs to each one of the neurons one can say that these neurons are independent from each other: the computation of their outputs does not depend on the others. It depends on the inputs (which will naturally be the same for every neuron), their weights (each neuron has their own set) and their activation functions (each neuron has its own). This forces us to redefine the weight\u2019s vector definition done in chapter 0.", "From now on, each connection is going to be identified as w(i,j), where i identifies the input where the connection comes from and j is the neuron where the connection goes to. The formal representation is done as follows: let a neural network have m inputs and n neurons. Each connection is represented by w(i,j) where 0 \u2266 i <m and 0 \u2266 j <n, which in turn form together the weight matrix W", "This matrix W is the one that describes the connections between all the inputs to all the neurons from a perceptron\u2019s layer. For the example described in figure 1 we have m=3 because that is the number of inputs and n=5 because these are the number of neurons, which means that the matrix W will have 3 rows and 5 columns.", "The training process proposed by Rosenblatt for perceptron networks is quite simple. It consists in showing to the network a set of examples that it should learn. Each example has an expected output and a set of metrics/characteristics associated to it, these ones are known as features. Each example\u2019s features are fed into the network through the input nodes of the network so that the predicted output can be calculated. Then, the weights are adjusted depending on any divergences between the expected output (t) and the predicted output (y). This process is iterative as the same examples are shown to the network several times. But how do we change these weights?", "We want the weights to change in such a way that they allow the network\u2019s prediction to be closer to the actual result when the same example is shown to the network in the next iteration. The logic behind it is to decrease the weights if a neuron fired but it should not have fired (y=1 and t=0) or increase the weights if a neuron did not fire but it should have fired (y=0 and t=1). Formally speaking, we want to add a delta \u0394w(i,j) to the current weight w(i,j) and create a new weight that replaces the existing one.", "So the next question is how much should this delta be? The update formula relies on 3 factors:", "Given these three factors, this is the final formula on how to train a perceptron:", "where w(i,j) is a weight that connects the input i to the neuron j, y(j) identifies the perceptron\u2019s output, t(j) identifies the perceptron\u2019s expected result, x(i) identifies the input and \u00b5 is the learning rate.", "We have seen before that the output of a perceptron neuron is the dot product between the inputs (x) and weights (w) subjected to the threshold activation function. But what happens if that input is 0? Let\u2019s imagine a single neuron with a single input. We know that our output y is determined by y=f(x\u2080 \u00d7 w\u2080). If the input is x\u2080=0 then y=f(0)=1.", "In this case the output of the neuron will be the same regardless the weight value due to the zero-product property. This implies that there is no learning occurring because as much as the weight value is changed in the learning process, the neuron will still output the same result. The way this problem is solved is by adding an extra node connected to the neuron called the bias node, in order to make learning possible. The amended perceptron architecture is described in figure 2.", "Considering the example described in figure 2, the formula to get the output y will then be y = f(x\u2080 \u00d7 w\u2080\ufe62x\u2081 \u00d7 w\u2081\ufe62x\u2082 \u00d7 w\u2082 + b\u2080).", "The perceptron is a powerful problem solver given its simplicity. If a problem has a linearly separable solution, then it is proved that the perceptron can always converge towards an optimal solution.", "Two points come up from my last sentence:", "However, the problem with the perceptron is that many of the real-world problems are not linearly separable, so it is very likely that using a layer of perceptrons will not be the best solution for your problem because it will be unable to find the optimal boundary that separates your classes. This limitation is illustrated in figure 4 through what is known as the XOR problem.", "In the figure above there is a graphical representation of the logical OR function and the XOR function (XOR stands for \u201cexclusive or\u201d). The OR function is one that based on two inputs returns 1 if any of the inputs is 1 (otherwise the result is 0), whereas the XOR function outputs 1 if only one of the inputs is 1.", "If we were to use a perceptron with two inputs to teach these functions, we can see in the leftmost plot in figure 4 (OR function) that there is a line that separates the crosses from circles whereas in the rightmost plot (XOR function) the only way to separate both classes is by using a non-linear boundary. This means that a single perceptron can learn the OR function but it cannot learn the XOR function.", "In this article I have explained you what is the perceptron, what kinds of problems does it solve, how it can be trained to learn patterns by separating classes and what are its limitations. However, there is now a need to dive into code and make this happen in practice, so in my next article I am going to show you how to create a perceptron, train it and use it to. Stay tuned!", "Thanks for reading! Did you like this article? All your feedback \ud83d\udde3 is greatly appreciated. Feel free to reach me out on Twitter or LinkedIn, or just follow me on Medium if you are interested in being up-to-date about the next chapters \ud83d\ude00.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Machine Learning Engineer, Captify. Interested in Evolutionary Computation, Machine Learning and Data Processing"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Feaa461088367&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-magic-behind-the-perceptron-network-eaa461088367&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-magic-behind-the-perceptron-network-eaa461088367&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-magic-behind-the-perceptron-network-eaa461088367&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-magic-behind-the-perceptron-network-eaa461088367&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----eaa461088367--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----eaa461088367--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://adrianovinhas.medium.com/?source=post_page-----eaa461088367--------------------------------", "anchor_text": ""}, {"url": "https://adrianovinhas.medium.com/?source=post_page-----eaa461088367--------------------------------", "anchor_text": "Adriano Vinhas"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fef905c4b57df&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-magic-behind-the-perceptron-network-eaa461088367&user=Adriano+Vinhas&userId=ef905c4b57df&source=post_page-ef905c4b57df----eaa461088367---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Feaa461088367&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-magic-behind-the-perceptron-network-eaa461088367&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Feaa461088367&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-magic-behind-the-perceptron-network-eaa461088367&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/tagged/neural-networks-series", "anchor_text": "Neural networks series"}, {"url": "https://en.wikipedia.org/wiki/Frank_Rosenblatt", "anchor_text": "Frank Rosenblatt"}, {"url": "https://en.wikipedia.org/wiki/Perceptron", "anchor_text": "perceptron"}, {"url": "https://medium.com/@adrianovinhas/a-gentle-introduction-to-neural-networks-14e5c02bafe", "anchor_text": "last article"}, {"url": "https://medium.com/@adrianovinhas/a-gentle-introduction-to-neural-networks-14e5c02bafe", "anchor_text": "chapter 0"}, {"url": "https://leimao.github.io/blog/Perceptron-Convergence-Theorem/", "anchor_text": "check this link"}, {"url": "https://medium.com/@jayeshbahire/the-xor-problem-in-neural-networks-50006411840b", "anchor_text": "XOR problem"}, {"url": "https://twitter.com/adrianovinhas", "anchor_text": "Twitter"}, {"url": "https://www.linkedin.com/in/adrianovinhas/", "anchor_text": "LinkedIn"}, {"url": "https://towardsdatascience.com/what-the-hell-is-perceptron-626217814f53", "anchor_text": "What the hell is perceptron?"}, {"url": "https://medium.com/u/165370addbb5?source=post_page-----eaa461088367--------------------------------", "anchor_text": "SAGAR SHARMA"}, {"url": "https://medium.com/@nikhilc3013/simple-perceptron-training-algorithm-explained-7bbfdff2c57d", "anchor_text": "Simple perceptron training algorithm: explained"}, {"url": "https://medium.com/u/a7dad0bdd4ac?source=post_page-----eaa461088367--------------------------------", "anchor_text": "Nikhil Chigali"}, {"url": "https://medium.com/@thomascountz/perceptrons-in-neural-networks-dc41f3e4c1b9", "anchor_text": "Perceptrons in real networks"}, {"url": "https://medium.com/u/5deb80af2cc0?source=post_page-----eaa461088367--------------------------------", "anchor_text": "Thomas Countz"}, {"url": "https://medium.com/anubhav-shrimal/perceptron-algorithm-1b387058ecfb", "anchor_text": "Perceptron algorithm"}, {"url": "https://medium.com/u/c8ac91815c3d?source=post_page-----eaa461088367--------------------------------", "anchor_text": "Anubhav Shrimal"}, {"url": "https://medium.com/@jayeshbahire/the-xor-problem-in-neural-networks-50006411840b", "anchor_text": "The XOR problem in neural networks"}, {"url": "https://medium.com/u/4d137af1d608?source=post_page-----eaa461088367--------------------------------", "anchor_text": "Jayesh Bapu Ahire"}, {"url": "https://towardsdatascience.com/perceptrons-logical-functions-and-the-xor-problem-37ca5025790a", "anchor_text": "Perceptrons, Logical Functions, and the XOR problem"}, {"url": "https://medium.com/u/e217ceca2278?source=post_page-----eaa461088367--------------------------------", "anchor_text": "Francesco Cicala"}, {"url": "https://towardsdatascience.com/perceptron-learning-algorithm-d5db0deab975", "anchor_text": "Perceptron Learning Algorithm: A Graphical Explanation of Why It Works"}, {"url": "https://medium.com/u/202534492f47?source=post_page-----eaa461088367--------------------------------", "anchor_text": "Akshay Chandra Lagandula"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----eaa461088367---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----eaa461088367---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----eaa461088367---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----eaa461088367---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/neural-networks-series?source=post_page-----eaa461088367---------------neural_networks_series-----------------", "anchor_text": "Neural Networks Series"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Feaa461088367&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-magic-behind-the-perceptron-network-eaa461088367&user=Adriano+Vinhas&userId=ef905c4b57df&source=-----eaa461088367---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Feaa461088367&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-magic-behind-the-perceptron-network-eaa461088367&user=Adriano+Vinhas&userId=ef905c4b57df&source=-----eaa461088367---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Feaa461088367&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-magic-behind-the-perceptron-network-eaa461088367&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----eaa461088367--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Feaa461088367&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-magic-behind-the-perceptron-network-eaa461088367&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----eaa461088367---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----eaa461088367--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----eaa461088367--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----eaa461088367--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----eaa461088367--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----eaa461088367--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----eaa461088367--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----eaa461088367--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----eaa461088367--------------------------------", "anchor_text": ""}, {"url": "https://adrianovinhas.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://adrianovinhas.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Adriano Vinhas"}, {"url": "https://adrianovinhas.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "112 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fef905c4b57df&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-magic-behind-the-perceptron-network-eaa461088367&user=Adriano+Vinhas&userId=ef905c4b57df&source=post_page-ef905c4b57df--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fc645e74d6fd5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-magic-behind-the-perceptron-network-eaa461088367&newsletterV3=ef905c4b57df&newsletterV3Id=c645e74d6fd5&user=Adriano+Vinhas&userId=ef905c4b57df&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}