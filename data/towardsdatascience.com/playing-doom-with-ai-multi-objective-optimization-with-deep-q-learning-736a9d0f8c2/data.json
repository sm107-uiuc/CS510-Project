{"url": "https://towardsdatascience.com/playing-doom-with-ai-multi-objective-optimization-with-deep-q-learning-736a9d0f8c2", "time": 1683011683.6108952, "path": "towardsdatascience.com/playing-doom-with-ai-multi-objective-optimization-with-deep-q-learning-736a9d0f8c2/", "webpage": {"metadata": {"title": "Playing Doom with AI: Multi-objective optimization with Deep Q-learning | by Adrian Yijie Xu | Towards Data Science", "h1": "Playing Doom with AI: Multi-objective optimization with Deep Q-learning", "description": "Online learning methods are a dynamic family of algorithms powering many of the latest achievements in reinforcement learning over the past decade. Belonging to the sample-based learning class of\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-automating-pong-in-using-a-policy-model-an-implementation-b71f64c158ff", "anchor_text": "sample-based learning", "paragraph_index": 0}, {"url": "https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-understanding-blackjack-strategy-through-monte-carlo-88c9b85194ed", "anchor_text": "offline counterparts", "paragraph_index": 0}, {"url": "https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-navigating-cliffworld-with-sarsa-and-q-learning-cc3c36eb5830", "anchor_text": "theory", "paragraph_index": 1}, {"url": "https://towardsdatascience.com/automating-pac-man-with-deep-q-learning-an-implementation-in-tensorflow-ca08e9891d9c", "anchor_text": "practical implementations", "paragraph_index": 1}, {"url": "https://towardsdatascience.com/automating-pac-man-with-deep-q-learning-an-implementation-in-tensorflow-ca08e9891d9c", "anchor_text": "past articles", "paragraph_index": 1}, {"url": "https://towardsdatascience.com/automating-doom-with-deep-q-learning-an-implementation-in-tensorflow-db03c1b03a9c", "anchor_text": "previous article", "paragraph_index": 2}, {"url": "https://github.com/shakenes/vizdoomgym", "anchor_text": "Vizdoomgym", "paragraph_index": 2}, {"url": "https://towardsdatascience.com/automating-doom-with-deep-q-learning-an-implementation-in-tensorflow-db03c1b03a9c", "anchor_text": "contrast to our previous Doom article", "paragraph_index": 4}, {"url": "https://github.com/EXJUSTICE/GradientCrescent", "anchor_text": "GradientCrescent Github.", "paragraph_index": 5}, {"url": "https://www.manning.com/livevideo/reinforcement-learning-in-motion", "anchor_text": "course", "paragraph_index": 5}, {"url": "https://medium.com/@adrianitsaxu", "anchor_text": "GradientCrescent", "paragraph_index": 23}, {"url": "https://github.com/EXJUSTICE/GradientCrescent", "anchor_text": "Github", "paragraph_index": 23}], "all_paragraphs": ["Online learning methods are a dynamic family of algorithms powering many of the latest achievements in reinforcement learning over the past decade. Belonging to the sample-based learning class of reinforcement learning approaches, online learning methods allow for the determination of state values simply through repeated observations, eliminating the need for explicit transition dynamics. Unlike their offline counterparts, online learning approaches such as Temporal Difference learning (TD), allow for the incremental updates of the values of states and actions during episode of agent-environment interaction, allowing for constant, incremental performance improvements to be observed.", "Beyond TD we\u2019ve discussed the theory and practical implementations of Q-learning, an evolution of TD designed to allow for incrementally more precise estimations state-action values in an environment. Q-learning has been made famous as becoming the backbone of reinforcement learning approaches to simulated game environments, such as those observed in OpenAI\u2019s gyms. As we\u2019ve already covered theoretical aspects of Q-learning in past articles, they will not be repeated here.", "In our previous article, we explored how Q-learning can be applied to training an agent to play a basic scenario in the classic FPS game Doom, through the use of the open-source OpenAI gym wrapper library Vizdoomgym. We\u2019ll build upon that article by introducing a more complex Vizdoomgym scenario, and build our solution in Pytorch. This is the first in a series of articles investigating various RL algorithms for Doom, serving as our baseline.", "The environment we\u2019ll be exploring is the Defend The Line-scenario of Vizdoomgym. The environment has the agent at one end of a hallway, with demons spawning at the other end. Some characteristics of the environment include:", "Implicitly, success in this environment requires balancing the multiple objectives: the ideal player must learn prioritize the brown monsters, which are able to damage the player upon spawning, while the pink monsters can be safely ignored for a period of time due to their travel time. This setup is in contrast to our previous Doom article, where single objectives were presented.", "Our Google Colaboratory implementation is written in Python utilizing Pytorch, and can be found on the GradientCrescent Github. Our approach is based on the approach detailed in Tabor\u2019s excellent Reinforcement Learning course. As the implementation for this approach is quite convoluted, let\u2019s summarize the order of actions required:", "Let\u2019s start by importing all of the necessary packages, including the OpenAI and Vizdoomgym environments. We\u2019ll also install the AV package necessary for Torchvision, which we\u2019ll use for visualization. Note that the runtime must be restarted after installation is complete.", "Next, we initialize our environment scenario, inspect the observation space and action space, and visualize our environment..", "Next, we\u2019ll define our preprocessing wrappers. These are classes that inherit from the OpenAI gym base class, overriding their methods and variables in order to implicitly provide all of our necessary preprocessing. We\u2019ll start defining a wrapper to repeat every action for a number of frames, and perform an element-wise maxima in order to increase the intensity of any actions. You\u2019ll notice a few tertiary arguments such as fire_first and no_ops \u2014 these are environment-specific, and of no consequence to us in Vizdoomgym.", "Next, we define the preprocessing function for our observations. We\u2019ll make our environment symmetrical by converting it into the Box space, swapping the channel integer to the front of our tensor, and resizing it to an area of (84,84) from its original (320,480) resolution. We\u2019ll also greyscale our environment, and normalize the entire image by dividing by a constant.", "Next, we create a wrapper to handle frame-stacking. The objective here is to help capture motion and direction from stacking frames, by stacking several frames together as a single batch. In this way, we can capture position, translation, velocity, and acceleration of the elements in the environment. With stacking, our input adopts a shape of (4,84,84,1).", "Finally, we tie all of our wrappers together into a single make_env() method, before returning the final environment for use.", "Next, let\u2019s define our model, a deep Q-network. This is essentially a three layer convolutional network that takes preprocessed input observations, with the generated flattened output fed to a fully-connected layer, generating state-action values in the game space as an output. Note there are no activation layers here, as the presence of one would result in a binary output distribution. Our loss is the squared difference of our calculated state-action value versus our predicted state-action value. We\u2019ll use the RMSProp optimizer to minimize our loss during training.", "Recall that the update function for Q-learning requires the following:", "To supply these parameters in meaningful quantities, we need to evaluate our current policy following a set of parameters and store all of the variables in a buffer, from which we\u2019ll draw data in minibatches during training. Hence, we need a replay memory buffer from which to store and draw observations from.", "Next, we\u2019ll define our agent. Our agent be using an epsilon greedy policy with a decaying exploration rate, in order to maximize exploitation over time. To learn to predict state-action-values that maximize our cumulative reward, our agent will be using the discounted future rewards obtained by sampling the memory.", "You\u2019\u2019ll notice that we initialize two copies of our DQN as part of our agent, with methods to copy weight parameters of our original network into a target network. This dual-network approach allows us to generate data during the training process using an existing policy while still optimizing our parameters for the next policy iteration, reducing loss oscillations.", "With all of supporting code defined, let\u2019s run our main training loop. We\u2019ve defined most of this in the initial summary, but let\u2019s recall for posterity.", "We\u2019ve graphed the average score of our agents together with our epsilon rate, across 500, 1000, and 2000 episodes below.", "Looking at the results, you\u2019ll notice a few patterns.", "Interestingly, we can observe some of these points in the gameplay. Below are clips of gameplay for our agents trained at 500, 1000, and 2000 episodes, respectively.", "All of the agents exhibit continuous firing \u2014 understandable given the lack of a penalty regarding ammo expenditure. Notice how the agent trained at 500 episodes exhibits much larger turn arcs, while the better trained agents seem to stick to specific sectors of the map. This behavior may be in anticipation of the spawning of the brown monsters, a tactic relying on the pink monsters to walk up closer to cross the line of fire. Note that this environment is still relatively simple in order to facilitate relatively facile training \u2014 introducing a penalty to ammo use, or increasing the action space to include strafing, would result in significantly different behaviour.", "That wraps up this implementation on Q-learning. In our next article, we\u2019ll move on to examining the performance of our agent in these environments with more advanced Q-learning approaches.", "We hope you enjoyed this article, and hope you check out the many other articles on GradientCrescent, covering applied and theoretical aspects of AI. To stay up to date with the latest updates on GradientCrescent, please consider following the publication and following our Github repository.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F736a9d0f8c2&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplaying-doom-with-ai-multi-objective-optimization-with-deep-q-learning-736a9d0f8c2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplaying-doom-with-ai-multi-objective-optimization-with-deep-q-learning-736a9d0f8c2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplaying-doom-with-ai-multi-objective-optimization-with-deep-q-learning-736a9d0f8c2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplaying-doom-with-ai-multi-objective-optimization-with-deep-q-learning-736a9d0f8c2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----736a9d0f8c2--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----736a9d0f8c2--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@adrianitsaxu?source=post_page-----736a9d0f8c2--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@adrianitsaxu?source=post_page-----736a9d0f8c2--------------------------------", "anchor_text": "Adrian Yijie Xu"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc834a59b6354&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplaying-doom-with-ai-multi-objective-optimization-with-deep-q-learning-736a9d0f8c2&user=Adrian+Yijie+Xu&userId=c834a59b6354&source=post_page-c834a59b6354----736a9d0f8c2---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F736a9d0f8c2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplaying-doom-with-ai-multi-objective-optimization-with-deep-q-learning-736a9d0f8c2&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F736a9d0f8c2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplaying-doom-with-ai-multi-objective-optimization-with-deep-q-learning-736a9d0f8c2&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-automating-pong-in-using-a-policy-model-an-implementation-b71f64c158ff", "anchor_text": "sample-based learning"}, {"url": "https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-understanding-blackjack-strategy-through-monte-carlo-88c9b85194ed", "anchor_text": "offline counterparts"}, {"url": "https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-navigating-cliffworld-with-sarsa-and-q-learning-cc3c36eb5830", "anchor_text": "theory"}, {"url": "https://towardsdatascience.com/automating-pac-man-with-deep-q-learning-an-implementation-in-tensorflow-ca08e9891d9c", "anchor_text": "practical implementations"}, {"url": "https://towardsdatascience.com/automating-pac-man-with-deep-q-learning-an-implementation-in-tensorflow-ca08e9891d9c", "anchor_text": "past articles"}, {"url": "https://towardsdatascience.com/automating-doom-with-deep-q-learning-an-implementation-in-tensorflow-db03c1b03a9c", "anchor_text": "previous article"}, {"url": "https://github.com/shakenes/vizdoomgym", "anchor_text": "Vizdoomgym"}, {"url": "https://towardsdatascience.com/automating-doom-with-deep-q-learning-an-implementation-in-tensorflow-db03c1b03a9c", "anchor_text": "contrast to our previous Doom article"}, {"url": "https://github.com/EXJUSTICE/GradientCrescent", "anchor_text": "GradientCrescent Github."}, {"url": "https://www.manning.com/livevideo/reinforcement-learning-in-motion", "anchor_text": "course"}, {"url": "https://github.com/shakenes/vizdoomgym.git", "anchor_text": "https://github.com/shakenes/vizdoomgym.git"}, {"url": "https://medium.com/@adrianitsaxu", "anchor_text": "GradientCrescent"}, {"url": "https://github.com/EXJUSTICE/GradientCrescent", "anchor_text": "Github"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----736a9d0f8c2---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/ai?source=post_page-----736a9d0f8c2---------------ai-----------------", "anchor_text": "AI"}, {"url": "https://medium.com/tag/doom?source=post_page-----736a9d0f8c2---------------doom-----------------", "anchor_text": "Doom"}, {"url": "https://medium.com/tag/games?source=post_page-----736a9d0f8c2---------------games-----------------", "anchor_text": "Games"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----736a9d0f8c2---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F736a9d0f8c2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplaying-doom-with-ai-multi-objective-optimization-with-deep-q-learning-736a9d0f8c2&user=Adrian+Yijie+Xu&userId=c834a59b6354&source=-----736a9d0f8c2---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F736a9d0f8c2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplaying-doom-with-ai-multi-objective-optimization-with-deep-q-learning-736a9d0f8c2&user=Adrian+Yijie+Xu&userId=c834a59b6354&source=-----736a9d0f8c2---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F736a9d0f8c2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplaying-doom-with-ai-multi-objective-optimization-with-deep-q-learning-736a9d0f8c2&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----736a9d0f8c2--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F736a9d0f8c2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplaying-doom-with-ai-multi-objective-optimization-with-deep-q-learning-736a9d0f8c2&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----736a9d0f8c2---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----736a9d0f8c2--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----736a9d0f8c2--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----736a9d0f8c2--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----736a9d0f8c2--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----736a9d0f8c2--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----736a9d0f8c2--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----736a9d0f8c2--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----736a9d0f8c2--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@adrianitsaxu?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@adrianitsaxu?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Adrian Yijie Xu"}, {"url": "https://medium.com/@adrianitsaxu/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "604 Followers"}, {"url": "https://github.com/EXJUSTICE/", "anchor_text": "https://github.com/EXJUSTICE/"}, {"url": "https://www.linkedin.com/in/yijie-xu-0174a325/", "anchor_text": "https://www.linkedin.com/in/yijie-xu-0174a325/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc834a59b6354&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplaying-doom-with-ai-multi-objective-optimization-with-deep-q-learning-736a9d0f8c2&user=Adrian+Yijie+Xu&userId=c834a59b6354&source=post_page-c834a59b6354--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F362de3a1de04&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplaying-doom-with-ai-multi-objective-optimization-with-deep-q-learning-736a9d0f8c2&newsletterV3=c834a59b6354&newsletterV3Id=362de3a1de04&user=Adrian+Yijie+Xu&userId=c834a59b6354&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}