{"url": "https://towardsdatascience.com/curse-of-batch-normalization-8e6dd20bc304", "time": 1683007411.1789818, "path": "towardsdatascience.com/curse-of-batch-normalization-8e6dd20bc304/", "webpage": {"metadata": {"title": "Curse of Batch Normalization. Batch Normalization is Indeed one of\u2026 | by Sahil Uppal | Towards Data Science", "h1": "Curse of Batch Normalization", "description": "Batch Normalization is Indeed one of the major breakthrough in the field of Deep Learning and is one of the hot topics for discussion among researchers in the past few years. Batch Normalization is a\u2026"}, "outgoing_paragraph_urls": [], "all_paragraphs": ["Batch Normalization is Indeed one of the major breakthrough in the field of Deep Learning and is one of the hot topics for discussion among researchers in the past few years. Batch Normalization is a widely adopted technique that enables faster and more stable training and has become one of the most influential methods. However, despite its versatility, there are still some points holding this method back as we are going to discuss in this article, which shows that there\u2019s still room for improvement for normalization methods.", "Before discussing anything, first, we should know what batch normalization is, how it works, and discuss it\u2019s use cases.", "During training, the output distribution of each intermediate activation layer shifts at each iteration as we update the previous weights. This phenomenon is referred to as an internal covariant shift (ICS). So a natural thing to do, if I want to prevent this from happening, is to fix all the distributions. In simple words, if I had some problem that my distributions are shifting around, ill just clamp them and not let them shift around to help gradient optimization and prevent vanishing gradients, and this will help my neural network train faster. So reducing this internal covariant shift was the key principle driving the development of batch normalization.", "Batch Normalization normalizes the output of the previous output layer by subtracting the empirical mean over the batch divided by the empirical standard deviation. This will help the data look like Gaussian distribution.", "Where mu and sigma_square are the batch mean and batch variance respectively.", "And, we learn a new mean and covariance in terms of two learnable parameters \u03b3 and \u03b2. So in short, you can think of batch normalization is something that helps you control the first and second moments of the distribution of the batch.", "I\u2019ll enlist some of the benefits of using batch normalization but I won\u2019t get into much detail, as there are tonnes of articles already covering that.", "So, getting back to the motive of the article, there are many situations under which batch normalization starts to hurt performance or doesn\u2019t work at all.", "As discussed above, the batch normalization layer has to calculate mean and variance to normalize the previous outputs across the batch. This statistical estimation will be pretty accurate if the batch size is fairly large while keeps on decreasing as the batch size decreases.", "Above is ResNet-50\u2019s validation error plot. As one can infer, if the batch size is kept 32, it\u2019s final validation error is around 23 and the error keeps on decreasing with smaller batch sizes (Batch size can\u2019t be 1 for batch normalization because it will be mean of itself). And there\u2019s a huge difference in the loss (around 10%).", "If the small batch size is a problem, why don\u2019t we use a higher batch size? Well, we can\u2019t use a higher batch size in every situation. Consider fine-tuning, we can\u2019t use high batch size to not hurt our model with high gradients. Consider distributive training, your high batch size will eventually be distributed among instances as a set of small batch sizes.", "As a result of experiments conducted by NVIDIA and Carnegie Mellon University, they claim that \u201ceven though Batch Normalization is not the computationally intensive and total number of iterations needed for convergence are decreased. The per-iteration time could be noticeably increased.\u201d, and it can further be increased with an increase in batch size.", "As you can see, batch normalization consumed 1/4 of total training time. The reason is that because batch norm requires double iteration through input data, one for computing batch statistics and another for normalizing the output.", "For Instance, consider the real-world application \u201cobject detection\u201d. While training an object detector, we usually go with a large batch size (YOLOv4 and Faster-RCNN both are trained on batch size = 64 by default). But after putting these models into production, these models don\u2019t work as good as they were while training. This is because they are trained with large batch size, while in real-time they are getting a batch size equal to one because it has to process each frame subsequently. Considering this limitation, some implementations tends to use pre-computed mean and variances based on the activations on the training set. Another potential is to compute the mean and variation values based on your test-set activation distribution, but still not batch-wise.", "In contrast to batch learning, online learning is a type of learning technique in which the system is trained incrementally by feeding it data instances sequentially, either individually or by small groups called mini-batches. Each learning step is fast and cheap, so the system can learn about new data on the fly, as it arrives.", "As it depends on an external source of data, data may arrive individually or in batches. Due to the change of batch size in every iteration, it poorly generalizes the scale and shift of input data, which eventually hurts performance.", "Although batch normalization speeds-up training and generalization significantly in convolution neural networks, they are proven to be difficult to apply on recurrent architectures. Batch normalization can be applied in between stacks of RNN, where normalization is applied \u201cvertically\u201d i.e. the output of each RNN. But it cannot be applied \u201chorizontally\u201d i.e. between timesteps, as it hurts training because of exploding gradients due to repeated rescaling.", "[NOTE]: Some research experiments claim that batch normalization makes neural networks prone to adversarial vulnerability. But we didn\u2019t include this point due to lack of study and proof.", "So these were some drawbacks of using batch normalization. There are several alternatives used in situations where batch normalization can\u2019t hold.", "So training deep neural networks is simple but I don\u2019t think it\u2019s quite easy yet. In the sense that, there are few architectures I can choose between, there\u2019s a fixed learning rate that everyone uses and a fixed optimizer and a fixed set of tricks. These tricks have been chosen via natural selection almost like someone comes up with some trick they introduce it if it works, it stays, if it doesn\u2019t work, people eventually forget about it and no one uses it again. Apart from this, batch normalization is a milestone technique in the development of deep learning. However, normalizing along the batch dimension introduces some problems as discussed, which suggests that there\u2019s still room for improvement in normalization techniques.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F8e6dd20bc304&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcurse-of-batch-normalization-8e6dd20bc304&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcurse-of-batch-normalization-8e6dd20bc304&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcurse-of-batch-normalization-8e6dd20bc304&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcurse-of-batch-normalization-8e6dd20bc304&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----8e6dd20bc304--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8e6dd20bc304--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@sahiluppal?source=post_page-----8e6dd20bc304--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@sahiluppal?source=post_page-----8e6dd20bc304--------------------------------", "anchor_text": "Sahil Uppal"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fba6589a0bb01&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcurse-of-batch-normalization-8e6dd20bc304&user=Sahil+Uppal&userId=ba6589a0bb01&source=post_page-ba6589a0bb01----8e6dd20bc304---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8e6dd20bc304&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcurse-of-batch-normalization-8e6dd20bc304&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8e6dd20bc304&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcurse-of-batch-normalization-8e6dd20bc304&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@visuals_by_fred?utm_source=medium&utm_medium=referral", "anchor_text": "Freddie Collins"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----8e6dd20bc304---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----8e6dd20bc304---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----8e6dd20bc304---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----8e6dd20bc304---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8e6dd20bc304&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcurse-of-batch-normalization-8e6dd20bc304&user=Sahil+Uppal&userId=ba6589a0bb01&source=-----8e6dd20bc304---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8e6dd20bc304&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcurse-of-batch-normalization-8e6dd20bc304&user=Sahil+Uppal&userId=ba6589a0bb01&source=-----8e6dd20bc304---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8e6dd20bc304&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcurse-of-batch-normalization-8e6dd20bc304&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8e6dd20bc304--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F8e6dd20bc304&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcurse-of-batch-normalization-8e6dd20bc304&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----8e6dd20bc304---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----8e6dd20bc304--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----8e6dd20bc304--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----8e6dd20bc304--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----8e6dd20bc304--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----8e6dd20bc304--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----8e6dd20bc304--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----8e6dd20bc304--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----8e6dd20bc304--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@sahiluppal?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@sahiluppal?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Sahil Uppal"}, {"url": "https://medium.com/@sahiluppal/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "43 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fba6589a0bb01&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcurse-of-batch-normalization-8e6dd20bc304&user=Sahil+Uppal&userId=ba6589a0bb01&source=post_page-ba6589a0bb01--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Fba6589a0bb01%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcurse-of-batch-normalization-8e6dd20bc304&user=Sahil+Uppal&userId=ba6589a0bb01&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}