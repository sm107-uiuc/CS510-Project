{"url": "https://towardsdatascience.com/tweet-generation-with-neural-networks-lstm-and-gpt-2-e163bfd3fbd8", "time": 1683006905.26519, "path": "towardsdatascience.com/tweet-generation-with-neural-networks-lstm-and-gpt-2-e163bfd3fbd8/", "webpage": {"metadata": {"title": "Tweet generation with Neural Networks: LSTM and GPT-2 | by Sarah Tam | Towards Data Science", "h1": "Tweet generation with Neural Networks: LSTM and GPT-2", "description": "Twitter is the place where millions go to express their opinions on any topic, and it has also become a place for interactions between celebrities and their fans. This inspired us to explore the\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/@jmorris5", "anchor_text": "Jarlai Morris", "paragraph_index": 0}, {"url": "https://medium.com/@nataliianevinchana", "anchor_text": "Nataliia Nevinchana", "paragraph_index": 0}, {"url": "https://github.com/marquisvictor/Optimized-Modified-GetOldTweets3-OMGOT", "anchor_text": "this", "paragraph_index": 2}, {"url": "https://ai.googleblog.com/2019/01/transformer-xl-unleashing-potential-of.html", "anchor_text": "Transformer-XL: Unleashing the Potential of Attention Models", "paragraph_index": 23}, {"url": "https://medium.com/dair-ai/a-light-introduction-to-transformer-xl-be5737feb13", "anchor_text": "A Light Introduction to Transformer-XL \u2014 dair.ai", "paragraph_index": 23}, {"url": "https://github.com/CR-Gjx/LeakGAN", "anchor_text": "LeakGAN", "paragraph_index": 24}, {"url": "https://github.com/LantaoYu/SeqGAN", "anchor_text": "SeqGAN", "paragraph_index": 24}, {"url": "https://arxiv.org/pdf/1909.05858.pdf", "anchor_text": "CTRL: A CONDITIONAL TRANSFORMER LANGUAGE MODEL FOR CONTROLLABLE GENERATION", "paragraph_index": 25}, {"url": "https://github.com/salesforce/ctrl#generations", "anchor_text": "its GitHub repo", "paragraph_index": 25}, {"url": "https://openai.com/blog/better-language-models/", "anchor_text": "Better Language Models and Their Implications", "paragraph_index": 26}, {"url": "http://b-gat.es/2vwmyaw", "anchor_text": "http://b-gat.es/2vwmyaw", "paragraph_index": 33}, {"url": "https://twitter.com/smazurgop/status", "anchor_text": "https://twitter.com/smazurgop/status", "paragraph_index": 35}, {"url": "https://minimaxir.com/2019/09/howto-gpt2/", "anchor_text": "How To Make Custom AI-Generated Text With GPT-2", "paragraph_index": 38}, {"url": "https://b-gat.es/2JsNJoh", "anchor_text": "https://b-gat.es/2JsNJoh", "paragraph_index": 59}, {"url": "https://www.instagram.com/p/BRiT3YZrH6R/", "anchor_text": "https://www.instagram.com/p/BRiT3YZrH6R/", "paragraph_index": 64}, {"url": "http://b-gat.es/1o5YKV7", "anchor_text": "http://b-gat.es/1o5YKV7", "paragraph_index": 68}, {"url": "https://twitter.com/realDonaldTrump/status/11668362477942539", "anchor_text": "https://twitter.com/realDonaldTrump/status/11668362477942539", "paragraph_index": 69}], "all_paragraphs": ["By Jarlai Morris, Nataliia Nevinchana, and Sarah Tam", "Twitter is the place where millions go to express their opinions on any topic, and it has also become a place for interactions between celebrities and their fans. This inspired us to explore the limits of recurrent neural networks and see if they can be used for automatic text generation for social networks like Twitter. As our final project for our Artificial Neural Networks and Deep Learning course, our goal was to generate tweets for 10 popular accounts on Twitter, analyze them using NLP techniques, and compare the results. In addition, we look at different networks and assess the range of possibilities. We decided to look at politicians and entertainers and build one network ourselves and find another pre-trained one to compare the results.", "We used this library we found on Github to mine tweets from the accounts of 10 public figures. Interested in group differences and their effect on training our networks, we chose five political figures and five entertainers:", "Political Figures: Donald Trump (@realDonaldTrump ), Bernie Sanders (@BernieSanders). Elizabeth Warren (@ewarren), Alexandria Ocasio-Cortez (@AOC), Bill Gates (@BillGates)", "Entertainers: Justin Bieber (@justinbieber), Ariana Grande (@arianagrande), Kevin Hart (@KevinHart4real), LeBron James (@KingJames), Chrissy Teigen (@chrissyteigen)", "For each public figure, we mined 10,000 of their most recent tweets (note that for a few of the figures, their complete tweet history totalled less than 10,000, so we just used what was available). For figures that had additional tweets, we collected another 10,000 to train the LSTM network.", "Data cleaning: For our training data, we removed any posts consisting only of \u2018pics.twitter\u2019 links (on Twitter, these appear as photos and videos). We also wanted to analyze our training data and identify any patterns in tweets, which required additional cleaning. For word frequency analysis, we removed punctuation and stop words (as defined by the nltk library). In addition, we replaced remaining links with the keyword \u2018http\u2019 to track their frequency.", "Across all ten people, \u2018http\u2019 was among the most frequently used terms, indicating that they all post links relatively often in their tweets. Since we only used the keyword \u2018http\u2019 as an indicator of the frequency of links, we left it out in our visualizations.", "The word that all five political figures had in their top 3 was \u201cpeople.\u201d These political figures are deeply involved with society\u2019s issues and making decisions that affect a large body of people. Their work revolves around people, so it makes sense that \u201cpeople\u201d is commonly used by all of them.", "The ten most frequently used words are emblematic of each individual\u2019s rhetoric and style of speech. For example, people have come to associate certain words with Trump\u2019s grandiose style \u2014 \u201cgreat,\u201d (c.f. \u201cMake American Great Again), \u201cbig,\u201d and \u201cmany.\u201d In the list are also \u201cfake\u201d and \u201cnews\u201d \u2014 another iconic term Trump coined. For Sanders, it is \u201chealth\u201d + \u201ccare\u201d were 5th and 6th most common. For Warren, both \u201cfight\u201d and \u201cfighting\u201d were in the top ten.", "Interestingly, Trump\u2019s fourth most used word was \u201cdemocrats,\u201d the opposing political party. This speaks to how much time Trump spends tweeting about polarizing, partisan matters. Similarly, Sanders and Warren both have \u201ctrump\u201d in their top ten words.", "AOC\u2019s top ten words were more generic than the first three. Interestingly, her third most common word was \u201cthank\u201d which (as we\u2019ll see next) is quite characteristic of entertainers\u2019 tweets \u2014 perhaps because she is by far the youngest and socially-media-savvy of the political figures. Though Bill Gates isn\u2019t directly involved in politics also included Gates because he is involved in shaping our society and communities. His humanitarian view comes through in his top ten words, including \u201cworld,\u201d \u201cone,\u201d \u201chelp,\u201d and \u201cprogress.\u201d", "Entertainers use social media as a platform to interact with fans, and as evidenced by their heavy usage of the terms \u201cthank\u201d and \u201clove.\u201d Like political figures, their commonly used terms manifest their personal style and vernacular. Hart is the clearest example with two terms he coined \u2014 \u201cdopepic\u201d (as in, #DopeEpic) and \u201ccomedicrockstarshit\u201d \u2014 ranking 6th and 8th in usage. Generally, entertainers\u2019 word clouds provided less insight than political figures\u2019 because they often use Twitter for promo and brief fan interactions. Whereas, political figures use Twitter to convey more complex messages on their stances, policies, current events, etc.", "Political Figures vs. Entertainers: Average Tweet Length", "We found that, on average, entertainers\u2019 tweets were about half the length of political figures\u2019 tweets. We can see how differences between political figures\u2019 training data and entertainers\u2019 training data affect the tweets generated by our networks. With longer tweets to train on, the quality and resemblance of tweets our networks generated for political figures were higher.", "We performed sentiment analysis on training data using the VADER (Valence Aware Dictionary for sEntiment Reasoning) tool. We took the compound score \u2014 a metric that calculates the sum of all the lexicon ratings which have been normalized between -1 (most extreme negative) and +1 (most extreme positive) \u2014 to compare sentiment across groups.", "Compared to political figures, a far greater number of entertainers\u2019 tweets fall near 0.0 (neutral). From the bar plots, entertainers have far more light-tailed distributions. Many entertainers use social media platforms for promotions (neutral) and fan interaction (positive) \u2014 most evident with Bieber and Grande. James showed a significantly higher number of positive-sentiment tweets, which may be linked to his motivational messages. Teigen has more negative-sentiment tweets of all the entertainers, which may be attributed to her outspoken nature.", "On the other hand, the IQR of political figures\u2019 box plots is generally wider than that of entertainers, i.e. the middle 50% of data fall within a greater range. For political figures, there may be a need to appeal to the public by expressing criticisms (negative sentiment) or rallying support (positive).", "Trump has the greatest IQR of the bunch, ranging from -0.50 to 0.75. The bar plot representation shows a bimodal distribution sentiment in his tweets. The sentiment pattern corroborates his flair for drama and showmanship; he tends towards either strong criticism or high praise. Warren and Gates had the most skewed-right distributions, which speaks to their reputations for being highly optimistic and driven towards positive social change.", "We wanted to create our own LSTM network, in addition one of the latest state-of-the-art networks to compare the results with and learn the specificities of how they work. We also considered developing a GAN network for text generation coming from the idea that since we are trying to generate tweets that look like tweets by specific users, creating a discriminator and doing a backprop that helps the generator generate better outputs that look like our initial training data would produce better results. Here is a short overview of the pros and cons that we found for each network we considered (Transformer XL, CTRL, and GPT-2) and explanation of why we ended up choosing LSTM and GPT-2.", "LSTM (Long Short Term Memory) is a type of Recurrent Neural Network (RNN). We chose to create an LSTM model to generate new tweets for the 10 politicians and entertainers because this particular model learns and picks up on style, language and grammar because an LSTM is able to detect and learn long-term dependencies. In learning the sequences of the input text, the model is then able to generate new text.", "LSTMs use a process of learning that can be described as a looping effect, meaning that the network interprets an input and generates an output. This output and its information is then passed to the next step within the network. This is essential because it allows the model to pick up on patterns within the text in order to generate better outputs similar to the style of text in question.", "The main reason why we considered implementing the Transformer-XL network was its efficiency. In contrast to other wide-spread language models that are unable to capture long-term dependencies in the text, Transformer-XL has an ability to identify context connections both on word level and character level. It achieves that goal by combining the concept of transformer networks with relative positional encodings. The traditional transformers are able to capture the context of the segment of a fixed length. Therefore, the context becomes fractured. The relative positional encodings help Transformer-XL (which stands for \u201cextra-long\u201d) capture context within much longer segments, making the model more effective at generating text.", "As we wanted to generate tweets in the style of specific entertainers and politicians, we wanted the network to learn the dependencies between different tweets and not just within one. However, as Transformer-XL was developed not long ago,, even though its code is open-source, we weren\u2019t able to find sufficient code walkthroughs that would help us implement that model. More information about the model can be found in these articles: Transformer-XL: Unleashing the Potential of Attention Models by Google AI who are authors of the model and A Light Introduction to Transformer-XL \u2014 dair.ai from one of the blogs on Medium.", "When we were considering using GAN for text-generation, we were thinking of the examples of images that were generated by GANs, mentioned in related lectures. As the results of those were impressive, we were wondering that it would be interesting to try to use the same concept for text generation. However, as we started to do more research, we figured out that it is very problematic to use GAN for text generation, because if we use RNN for text generation, we would face the problem with backprop, as the operation of RNN \u201cpicking\u201d the next word is non-differentiable. Since there are ways to get around that problem, we looked for some examples of GANs already tested for text generation. We found a couple of examples on GitHub including LeakGAN and SeqGAN, but we didn\u2019t find evidence that these networks would work better than a regular LSTM network or some advanced ones like GPT-2 or CTRL. Under Further Reading, you can check out some interesting reads we found on how GANs perform in text generation.", "CTRL is a large-scale text generation network that was presented by Salesforce in 2019. It stands for Conditional Transformer Language Model. This model was created to give users more control over generated text, which other large scale models usually lack. This model is open-source and is available on GitHub. Out of all the models we looked at, CTRL had the most documentation provided by creators. However, the use of CTRL is mostly aimed at producing stories from the prompts that come from links, text, etc. Because our goal was to create tweets and not stories, we decided to move on with GPT-2, even though it would be interesting to test CTRL with links to users\u2019 profiles. Here is the original article about CTRL CTRL: A CONDITIONAL TRANSFORMER LANGUAGE MODEL FOR CONTROLLABLE GENERATION by Nitish Shirish Keskar, Bryan McCann, Lav R. Varshney, Caiming Xiong, Richard Socher from Salesforce Research and its GitHub repo with documentation.", "GPT-2 is the most recent text-generating network released by Open AI in 2019. It is based on a transformer network and was trained on 40GB of text from the internet. Because the creators were concerned about the possibility of using this model to generate fake news, they decided to release a much smaller model. This model generates text that can be easily mistaken for human and the grammar and spelling are almost perfect. Out of all the models we looked at, this one was the most adopted by researchers and therefore had many more instructions and explanations on its use. For our text generation, we used the adoption of the model by Max Woolf GPT-2 Simple and used his respective Colab Notebook to check the model. The results were impressive from the first tries so we decided to stop on this one as it had optimal training time and the best quality of results. More information about the model can be found in the article Better Language Models and Their Implications by Open AI.", "In order to generate tweets using an LSTM network, the 10,000 tweets from the CSV were transferred into a pandas file. There were disparities within the tweets, so we did our best to clean the data without lowering the amount of data that would be used in training the model. Because of this, we decided to attempt to train the model to learn links. We only removed tweets whose lengths were less than 60 characters. By removing tweets that fall under a certain length, it removes tweets that are one word and the empty tweets that made it in the CSV file. Next, tweets were divided into sequences of equal lengths and placed into a list called sentences. These sentences will be used as training examples for the model.", "Vectorization is the next step. Here, we split the tweets into x and y evenly. x is a 3D matrix that has the shape of the total number of sentences, length of steps(40), and the number of unique characters. y is a 2D matrix that has the total length of sentences and unique characters and the purpose of this vector is to retrieve the next character that\u2019s after a sentence given by x. Now that the data has been scraped and prepared for training, it\u2019s time to build the LSTM model.", "After spending days trying to find the perfect model that was suitable and efficient, we stumbled upon the following model. The model, in particular, worked best out of the 10 different models that we trained. The difference between this model and the others is that the extra neurons, the activation function, SELU, and regular batch normalization was suitable for the size of the training data used.", "During the training of the model, each epoch took around 30 minutes and after 10 epoch the model stopped learning with a loss of around 1.5 for each of the datasets. During training, the model outputs the next character based on the probability distribution. This is known as diversity. Diversity is a value that determines how random the generated sentence is. For training purposes, we set the diversity to 0.2, 0.5, 0.7, 1.0, and 1.2 in order to visualize which diversity generated the most meaningful tweets.", "The final step is to generate new tweets for the user. The generate_tweets function takes a random seed, which is a random tweet from the lists of tweets, and a set diversity. We chose a diversity of 0.7 because it formulated the most comprehensible tweets compared to the others. After receiving a random seed and a diversity value, the function cuts the seed length down to 40 characters. This is done because it would be benign to use an already formulated tweet to create another and because once a new tweet is generated, it would exceed the max length of characters that are permitted on twitter.", "After setting sentences to the first 40 characters of the random seed, we create the generated variable that will store the beginning of the generated tweet, the first 40 characters of the random seed, and the 120 other generated characters that will be provided by the model\u2019s predictions. The time needed to generate 2,000 tweets ranged from 45 minutes to 2 hours. Below, I\u2019ve provided 1 of the 2,000 generated tweets for the four people below:", "good news you might have missed in 2018. thanking about the world\u2019s commitments we\u2019re done thing consider: http://b-gat.es/2vwmyaw pic.twitter.com/eyfhwch8gpngw", "i have more breaking news twitches! due with the cangiran cinewark now!!!! #hustlehart #hustlehart #makinghistory #comedicrockstar \u2026 #thinklikeamanston in the", "my album is out but so is my big bro\u2019s @jesicnolker @showmybirthdi @americcaem @dankanter and @itsbeliebers @spendwer @changel https://twitter.com/smazurgop/status", "It took days in order to find a model suitable for raising the accuracy above 40%. As far as accuracy with this particular model, the politicians, Bernie Sanders and Bill Gates, share the same tweeting style and grammar throughout their tweets that were used for training the model. This consistency provided accuracy results of 62% and higher. Because entertainers tweet frequently and their tweets range from topic to topic, this affected the accuracy of their training. This resulted in the model being able to generate better tweets for Bernie Sanders and Bill Gates overall.", "There are a few improvements that could\u2019ve been made overall. For starters, we could\u2019ve excluded links from the tweets, but the problem with this is that because all of the people that were selected promote themselves using a lot of links. This would cause the size of the dataset to drop tremendously. But, by gathering around fifty thousand tweets per user, it would\u2019ve given the model more sentences to train off of and learn. However, this would take weeks to implement and generate per person.", "To generate tweets with GPT-2 we used the tutorial by Max Woolf How To Make Custom AI-Generated Text With GPT-2. As for this part of the tweet generation, we used the network that cannot be customized, the main things we could experiment with were the learning rate and the number of epochs. The best quality of generated tweets was achieved when eta was set at 0.0001 and the number of epochs was 1000. We also were able to choose between the sizes of the versions of the GPT-2 model \u2014 124M, 355M, and 774M models. We decided to run with the smallest one, as we needed to train the model 10 times for 1000 epochs and then generate 20000 tweets (2000 per user), so time was our concern. We used Google Colab notebook to run the code to use Google\u2019s GPU and make it faster, but even the smallest model took around 1 h to be trained for each user and around 20 minutes to produce tweets. As Google Colab would frequently disconnect runtime we decided not to risk running heavier models, so we can generate tweets before the deadline. Even with the smallest model used, the quality of the tweets was impressive.", "We noticed some big disparities in the quality of tweets between entertainers and politicians. Overall, politicians have much longer tweets that have more substance to them, therefore, for the network, it was much easier to learn, as when feeding 10000 tweets, it would get more words. In the meanwhile, entertainers would have a lot of promos and short messages that aren\u2019t very unique like \u201clove you\u201d etc. So when the network was processing entertainers\u2019 tweets, the results were of lower quality and contained more empty tweets with just spaces. When we standardized the length of the tweets we augmented them with spaces, so the shorter the tweets \u2014 the higher the proportion of spaces to the written text.", "The makers of GPT-2 also mentioned that one of the faults of the network was repetition. We saw that happening as in a number of tweets for each of the users, the network would stop at a certain word and generate it 10 times or more. An example of a tweet like that would be one generated for Justin Bieber: \u201cXXL is back. U know what to do. @scooterbraun @scrappy @ludacris @ludacris @ludacris @ludacris @ludacris @ludacris @ludacris @ludacris @ludacris @ludacris @ludacris @ludacris @ludacris\u201d.", "Another problem with repetition that we saw was when the network would generate a couple of variations of the same tweet and just change the time, the ending of the link, a couple of words at the end, etc. For example, the network generated the tweets for Gates that just changed the time of \u201can update\u201d:", "Update, 19:45 a.m.: This article has been updated to clarify that the polio vaccines I got recently were not a \u201cpolio vaccine.\u201d It is.", "Update, 2:58: This article has been updated to clarify that the polio vaccines I got recently were not a \u201cpolio vaccine.\u201d It is.", "Update, 3:58 a.m.: This article has been updated to clarify that the polio vaccines I got recently were not a \u201cpolio vaccine.\u201d It is.", "Update, 4:55 a.m.: This article has been updated to clarify that the polio vaccines I got recently were not a \u201cpolio vaccine.\u201d It is.", "Update, 5:45 a.m.: This article has been updated to clarify that the polio vaccines I got recently were not a \u201cpolio vaccine.\u201d It is.", "Update, 6:30 a.m.: This article has been updated to clarify that the polio vaccines I got recently were not a \u201cpolio vaccine.\u201d It is.", "Update, 7:45 a.m.: This article has been updated to clarify that the polio vaccines I got recently was not a \u201cpolio vaccine.\u201d It is.", "Another example is with the 10 variations of the ending of the link:", "The world is better than it\u2019s ever been, but I\u2019m struck by how few people know this.https://b-gat.es/2BZcNFk", "The world is better than it\u2019s ever been, but I\u2019m struck by how few people know this.https://b-gat.es/2E3zEMw", "The world is better than it\u2019s ever been, but I\u2019m struck by how few people know this.https://b-gat.es/2EJzX1V", "The world is better than it\u2019s ever been, but I\u2019m struck by how few people know this.https://b-gat.es/2Ft7tBS", "The world is better than it\u2019s ever been, but I\u2019m struck by how few people know this.https://b-gat.es/2FWfEE1", "The world is better than it\u2019s ever been, but I\u2019m struck by how few people know this.https://b-gat.es/2HfkiQi", "The world is better than it\u2019s ever been, but I\u2019m struck by how few people know this.https://b-gat.es/2I0xWEWT", "The world is better than it\u2019s ever been, but I\u2019m struck by how few people know this.https://b-gat.es/2I4m0JT", "The world is better than it\u2019s ever been, but I\u2019m struck by how few people know this.https://b-gat.es/2I9cVMr", "The world is better than it\u2019s ever been, but I\u2019m struck by how few people know this.https://b-gat.es/2JsNJoh", "Also, when the tweets were pulled, some of the symbols like apostrophe have not been read well, so in generated tweets, we can see some weird symbols or combinations of symbols instead of apostrophes, at the end of the sentences, etc. Here are some examples of those:\u201dI\u0420\u0406\u0420\u201a\u0432\u201e\u045em\u201d instead of \u201cI\u2019m\u201d, \u201cI\u0432\u0402\u2122m\u201d, \u201c\u0420\u00a9\u0432\u0402\u00a6\u0420\u0401\u0420\u2021\u0420\u0401\u043f\u0457\u0405\u201d, etc.", "The positive aspects of GPT-2 text generation were that the network was able to grasp the main topics all of the users were talking about, including their signature hashtags, websites they share links from, projects they work on, and personal style of conversations. For example, for Trump\u2019s fake tweets, the network identified that Trump often reposts from Fox News, while for Alexandra Ocasio-Cortes the generated tweets were from The New York Times. Bieber\u2019s generated tweets had a lot of announcements of the new songs and albums, while Sanders\u2019 tweets had a lot about universal healthcare. Gates generated tweets contained a lot of comments on articles about economic development, fight against polio, malaria, etc as well as mentions of Melinda Gates\u2019 books and comments.", "These are some of our best generated tweets:", "You can preorder #AsLongAsYouLoveMe by texting SUMMER to 22837 and the app will let you know when we do an exclusive acoustic album special. #AsLongAsYouLoveMe and #LoveYourself will be up on ITUNES and in OUR TOP TEN too. #BIEBERBLAST", "Tonight is the final night of the #DangerousWomanTour and we\u2019re so honored to be performing it at Barclays Center. Thank you so much for all your love, energy, excitement and love tonight. https://www.instagram.com/p/BRiT3YZrH6R/", "The NBA playoffs are finally here! I was looking forward to tonight\u2019s game and I\u2019ll be looking forward to the next one.", "The full text of \u201cLOOKING FORWARD\u201d is out tomorrow! Preorder in my bio!! #CravingsCollection #ChrissysCourt #ChrissysCourt #ChrissysBliss pic.twitter.com/aI1oIOyCEI", "Congress and the GOP have been working to dismantle the Affordable Care Act (HC) for 14 years. They can\u2019t repeal ACA without killing the law, which means cutting the insurance market in half. We need to replace ACA with something more.", "A lot of people think of Africa as a continent of starvation. It\u2019s actually one of the largest economies in the world. Here are 5 amazing statistics: http://b-gat.es/1o5YKV7 pic.twitter.com/wZ8EFmYJFl", "A few weeks ago President Trump signed an executive order to cut funding for certain programs and activities, including Planned Parenthood. Trump\u2019s actions must end and immediate halt all funding of Planned Parenthood and all supplies of breast cancer screenings.https://twitter.com/realDonaldTrump/status/11668362477942539", "Do you think the people who run our Country, and our Economy, will allow this ridiculous Impeachment Hoax to proceed? Our economy is the best it has ever been, and the Dems are coming after you!", "A year ago today, President Obama signed an executive order that forced private prisons to close. Today, as I write, 46 states have implemented their own version of my plan, and Congress has signed on, many thanks to @KamalaHarris and @AyannaPressley.", "Overall, GPT-2 Simple did a great job of generating tweets for users that we chose. The text was of the same style, in the same topics and included details that are characteristic to each user. We also saw that results highly depended on the quality of input data. For almost all entertainers, the network generated much lower quality of tweets, compared to politicians\u2019 ones, and with much more repetition and empty tweets. For politicians, as the network had much more data to work on, the resulting tweets made sense, had much fewer mistakes, and looked more recognizable for each user. To sum up, GTP-2 is a really good way to quickly generate high-quality text with relatively little data. For those users that we weren\u2019t able to get all 10000 tweets at first, the results were impressive even with 5000 tweets. With Max Woolf\u2019s tutorial that we used, almost anyone can use this model to generate high-quality text quickly.", "The LSTM only generated tweets for four figures (Bieber, Hart, Gates, Sanders), while GPT-2 generated tweets for all ten public figures. Thus we did an analysis for GPT-2 vs. training data for all ten people first, then we compared both networks against the training data.", "Political figures generated tweets\u2019 ten most common words matched 6\u20138 of those from the training data, while the number of matches for entertainers was half that (3\u20134).", "For most public figures, the GPT-2 network produced tweets with similar to higher frequencies of links than the training data. Across the GPT-2 generated twitter data, \u2018http\u2019 was among the 10 most common terms. We can observe the difference in link usage by comparing the in the distribution of commonly used words for Ariana Grande:", "Comparing the word clouds for entertainers\u2019 actual tweets and generated tweets, we can see that the network still generates words that match the general tone of the individual. As an example, here\u2019s a comparison of Ariana Grande\u2019s word clouds:", "We also plotted tweet length and sentiment of training data and GPT-2 generated tweets side-by-side. Interestingly, the GPT-2 network consistently generates tweets that are half the length of the tweets it was trained on, if not less. In addition, their sentiment scores were far more neutral than the training data. We can see from the light-tailed distribution of sentiment scores for each individual that there is much less variation in sentiment in generated tweets, we\u2019ll get back to this in the next section.", "When it comes to tweet length, the LSTM network performed similarly to the GPT-2 network. The one outlier was Warren, for which the LSTM network actually generated slightly longer tweets on average.", "Compared to the GPT-2 network, the LSTM produced tweets with more positive sentiment scores, closer to the actual Twitter data. However, there is still a marked difference in sentiment distribution between the generated tweets and actual tweets, as we can see with Sanders. For both networks, the distribution of generated tweets are very light-tailed, and scores are concentrated around 0.0 (neutral). Clearly, networks can convincingly imitate tweeting styles, but getting the sentiment right involves more nuance. The combination of shorter tweet lengths and more links in generated tweets may contribute to higher neutral sentiment.", "Overall, we learned a lot throughout this project. Both of our networks generated tweets with better quality and resemblance for politicians than for entertainers. This may be attributed to differences in the way these two groups tweet. Politicians tend to post longer, more complex messages that reflect their platforms while most entertainers use Twitter to post promos and interact with fans.", "In regards to our networks, our main conclusion is that GPT-2 Simple is much easier and quicker to use than the LSTM network. It produces decent tweets that very well pass for the real thing. As an extension of this project, it would be interesting to compare the results of the model we used with larger versions of GPT-2. In addition, we observed that the quality of the generated text is directly correlated with the quality of the data that the network was trained on. Therefore, if you want to produce better results, spend more time cleaning and adjusting your data.", "From our networks and generated tweets, we saw evidence for OpenAI concerns about releasing the fully trained network \u2014 if it\u2019s that easy to create tweets that look almost exactly like tweets of the actual politicians, the threat of misuse of this model for the generation of fake news is alarming. We noticed, however, that our networks weren\u2019t even close in mimicking the varied sentiments of both political figures\u2019 and entertainers\u2019 tweets. Perhaps there are still patterns to human speech that these networks have yet to pick up on."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fe163bfd3fbd8&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftweet-generation-with-neural-networks-lstm-and-gpt-2-e163bfd3fbd8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftweet-generation-with-neural-networks-lstm-and-gpt-2-e163bfd3fbd8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftweet-generation-with-neural-networks-lstm-and-gpt-2-e163bfd3fbd8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftweet-generation-with-neural-networks-lstm-and-gpt-2-e163bfd3fbd8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/@sarah.tam17?source=post_page-----e163bfd3fbd8--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e163bfd3fbd8--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@sarah.tam17?source=post_page-----e163bfd3fbd8--------------------------------", "anchor_text": "Sarah Tam"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F247542c108e3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftweet-generation-with-neural-networks-lstm-and-gpt-2-e163bfd3fbd8&user=Sarah+Tam&userId=247542c108e3&source=post_page-247542c108e3----e163bfd3fbd8---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----e163bfd3fbd8--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe163bfd3fbd8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftweet-generation-with-neural-networks-lstm-and-gpt-2-e163bfd3fbd8&user=Sarah+Tam&userId=247542c108e3&source=-----e163bfd3fbd8---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe163bfd3fbd8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftweet-generation-with-neural-networks-lstm-and-gpt-2-e163bfd3fbd8&source=-----e163bfd3fbd8---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/@jmorris5", "anchor_text": "Jarlai Morris"}, {"url": "https://medium.com/@nataliianevinchana", "anchor_text": "Nataliia Nevinchana"}, {"url": "https://pixabay.com/users/photomix-company-1546875/", "anchor_text": "PhotoMIX-Company"}, {"url": "https://pixabay.com/photos/twitter-screen-social-phone-1795652/", "anchor_text": "pixabay"}, {"url": "https://github.com/marquisvictor/Optimized-Modified-GetOldTweets3-OMGOT", "anchor_text": "this"}, {"url": "https://ai.googleblog.com/2019/01/transformer-xl-unleashing-potential-of.html", "anchor_text": "Transformer-XL: Unleashing the Potential of Attention Models"}, {"url": "https://medium.com/dair-ai/a-light-introduction-to-transformer-xl-be5737feb13", "anchor_text": "A Light Introduction to Transformer-XL \u2014 dair.ai"}, {"url": "https://github.com/CR-Gjx/LeakGAN", "anchor_text": "LeakGAN"}, {"url": "https://github.com/LantaoYu/SeqGAN", "anchor_text": "SeqGAN"}, {"url": "https://arxiv.org/pdf/1909.05858.pdf", "anchor_text": "CTRL: A CONDITIONAL TRANSFORMER LANGUAGE MODEL FOR CONTROLLABLE GENERATION"}, {"url": "https://github.com/salesforce/ctrl#generations", "anchor_text": "its GitHub repo"}, {"url": "https://openai.com/blog/better-language-models/", "anchor_text": "Better Language Models and Their Implications"}, {"url": "http://b-gat.es/2vwmyaw", "anchor_text": "http://b-gat.es/2vwmyaw"}, {"url": "http://b-gat.es/1u6yecf", "anchor_text": "http://b-gat.es/1u6yecf"}, {"url": "http://b-gat.es/2dp", "anchor_text": "http://b-gat.es/2dp"}, {"url": "https://twitter.com/smazurgop/status", "anchor_text": "https://twitter.com/smazurgop/status"}, {"url": "https://minimaxir.com/2019/09/howto-gpt2/", "anchor_text": "How To Make Custom AI-Generated Text With GPT-2"}, {"url": "https://b-gat.es/2JsNJoh", "anchor_text": "https://b-gat.es/2JsNJoh"}, {"url": "https://www.instagram.com/p/BRiT3YZrH6R/", "anchor_text": "https://www.instagram.com/p/BRiT3YZrH6R/"}, {"url": "https://twitter.com/originalcinema/status/924093577757350208", "anchor_text": "https://twitter.com/originalcinema/status/924093577757350208"}, {"url": "http://b-gat.es/1o5YKV7", "anchor_text": "http://b-gat.es/1o5YKV7"}, {"url": "https://twitter.com/realDonaldTrump/status/11668362477942539", "anchor_text": "https://twitter.com/realDonaldTrump/status/11668362477942539"}, {"url": "https://ai.googleblog.com/2019/01/transformer-xl-unleashing-potential-of.html", "anchor_text": "Transformer-XL: Unleashing the Potential of Attention ModelsTo correctly understand an article, sometimes one will need to refer to a word or a sentence that occurs a few thousand\u2026ai.googleblog.com"}, {"url": "https://medium.com/dair-ai/a-light-introduction-to-transformer-xl-be5737feb13", "anchor_text": "A Light Introduction to Transformer-XLSummary of a novel technique for attentive language modeling that supports longer-term dependency.medium.com"}, {"url": "https://arxiv.org/abs/1909.05858", "anchor_text": "CTRL: A Conditional Transformer Language Model for Controllable GenerationLarge-scale language models show promising text generation capabilities, but users cannot easily control particular\u2026arxiv.org"}, {"url": "https://github.com/salesforce/ctrl#generations", "anchor_text": "salesforce/ctrlAuthors: Nitish Shirish Keskar, Bryan McCann, Lav Varshney, Caiming Xiong, and Richard Socher Apr 20, 2020 We are\u2026github.com"}, {"url": "https://openai.com/blog/better-language-models/", "anchor_text": "Better Language Models and Their ImplicationsWe've trained a large-scale unsupervised language model which generates coherent paragraphs of text, achieves\u2026openai.com\\"}, {"url": "https://medium.com/@IrekponorVictor/twitter-data-mining-mining-twitter-data-without-api-keys-a2a2bd3f11c", "anchor_text": "Twitter Data Mining: Mining Twitter Data without API keys.Getting Old Twitter Big Data for Analysis with a Single Line of Command.medium.com"}, {"url": "https://www.semanticscholar.org/paper/Evaluating-Generative-Models-for-Text-Generation-Kawthekar/0a4e66bd2ba65a53b11684250f2adc76eb768222", "anchor_text": "[PDF] Evaluating Generative Models for Text Generation | Semantic ScholarGenerating human quality text is a challenging problem because of ambiguity of meaning and difficulty in modeling long\u2026www.semanticscholar.org"}, {"url": "https://www.semanticscholar.org/paper/TextKD-GAN%3A-Text-Generation-Using-Knowledge-and-Haidar-Rezagholizadeh/abcbfa04b14ee6cc76afe8c5e504daf815d14fa2", "anchor_text": "[PDF] TextKD-GAN: Text Generation Using Knowledge Distillation and Generative Adversarial Networks\u2026Text generation is of particular interest in many NLP applications such as machine translation, language modeling, and\u2026www.semanticscholar.org"}, {"url": "https://becominghuman.ai/generative-adversarial-networks-for-text-generation-part-1-2b886c8cab10", "anchor_text": "Generative Adversarial Networks for Text Generation \u2014 Part 1The issues with GANs for text generation and the methods being used to combat thembecominghuman.ai"}, {"url": "https://becominghuman.ai/generative-adversarial-networks-for-text-generation-part-2-rl-1bc18a2b8c60", "anchor_text": "Generative Adversarial Networks for Text Generation \u2014 Part 2: RLUsing Reinforcement Learning to combat the non-differentiability issue in text GANs.becominghuman.ai"}, {"url": "https://medium.com/tag/twitter?source=post_page-----e163bfd3fbd8---------------twitter-----------------", "anchor_text": "Twitter"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----e163bfd3fbd8---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/data-science?source=post_page-----e163bfd3fbd8---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----e163bfd3fbd8---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe163bfd3fbd8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftweet-generation-with-neural-networks-lstm-and-gpt-2-e163bfd3fbd8&user=Sarah+Tam&userId=247542c108e3&source=-----e163bfd3fbd8---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe163bfd3fbd8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftweet-generation-with-neural-networks-lstm-and-gpt-2-e163bfd3fbd8&user=Sarah+Tam&userId=247542c108e3&source=-----e163bfd3fbd8---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe163bfd3fbd8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftweet-generation-with-neural-networks-lstm-and-gpt-2-e163bfd3fbd8&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/@sarah.tam17?source=post_page-----e163bfd3fbd8--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e163bfd3fbd8--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F247542c108e3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftweet-generation-with-neural-networks-lstm-and-gpt-2-e163bfd3fbd8&user=Sarah+Tam&userId=247542c108e3&source=post_page-247542c108e3----e163bfd3fbd8---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F3026512800d2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftweet-generation-with-neural-networks-lstm-and-gpt-2-e163bfd3fbd8&newsletterV3=247542c108e3&newsletterV3Id=3026512800d2&user=Sarah+Tam&userId=247542c108e3&source=-----e163bfd3fbd8---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/@sarah.tam17?source=post_page-----e163bfd3fbd8--------------------------------", "anchor_text": "Written by Sarah Tam"}, {"url": "https://medium.com/@sarah.tam17/followers?source=post_page-----e163bfd3fbd8--------------------------------", "anchor_text": "11 Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----e163bfd3fbd8--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F247542c108e3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftweet-generation-with-neural-networks-lstm-and-gpt-2-e163bfd3fbd8&user=Sarah+Tam&userId=247542c108e3&source=post_page-247542c108e3----e163bfd3fbd8---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F3026512800d2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftweet-generation-with-neural-networks-lstm-and-gpt-2-e163bfd3fbd8&newsletterV3=247542c108e3&newsletterV3Id=3026512800d2&user=Sarah+Tam&userId=247542c108e3&source=-----e163bfd3fbd8---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/coronavirus-twitter-trends-d32fed5a027e?source=author_recirc-----e163bfd3fbd8----0---------------------d56027b7_5e32_4218_92ca_da18d0c8b47f-------", "anchor_text": ""}, {"url": "https://medium.com/@sarah.tam17?source=author_recirc-----e163bfd3fbd8----0---------------------d56027b7_5e32_4218_92ca_da18d0c8b47f-------", "anchor_text": ""}, {"url": "https://medium.com/@sarah.tam17?source=author_recirc-----e163bfd3fbd8----0---------------------d56027b7_5e32_4218_92ca_da18d0c8b47f-------", "anchor_text": "Sarah Tam"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----e163bfd3fbd8----0---------------------d56027b7_5e32_4218_92ca_da18d0c8b47f-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/coronavirus-twitter-trends-d32fed5a027e?source=author_recirc-----e163bfd3fbd8----0---------------------d56027b7_5e32_4218_92ca_da18d0c8b47f-------", "anchor_text": "Coronavirus Twitter TrendsThe COVID-19 pandemic has changed the world. How has it changed our tweets?"}, {"url": "https://towardsdatascience.com/coronavirus-twitter-trends-d32fed5a027e?source=author_recirc-----e163bfd3fbd8----0---------------------d56027b7_5e32_4218_92ca_da18d0c8b47f-------", "anchor_text": "11 min read\u00b7May 6, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd32fed5a027e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcoronavirus-twitter-trends-d32fed5a027e&user=Sarah+Tam&userId=247542c108e3&source=-----d32fed5a027e----0-----------------clap_footer----d56027b7_5e32_4218_92ca_da18d0c8b47f-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/coronavirus-twitter-trends-d32fed5a027e?source=author_recirc-----e163bfd3fbd8----0---------------------d56027b7_5e32_4218_92ca_da18d0c8b47f-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd32fed5a027e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcoronavirus-twitter-trends-d32fed5a027e&source=-----e163bfd3fbd8----0-----------------bookmark_preview----d56027b7_5e32_4218_92ca_da18d0c8b47f-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----e163bfd3fbd8----1---------------------d56027b7_5e32_4218_92ca_da18d0c8b47f-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----e163bfd3fbd8----1---------------------d56027b7_5e32_4218_92ca_da18d0c8b47f-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----e163bfd3fbd8----1---------------------d56027b7_5e32_4218_92ca_da18d0c8b47f-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----e163bfd3fbd8----1---------------------d56027b7_5e32_4218_92ca_da18d0c8b47f-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----e163bfd3fbd8----1---------------------d56027b7_5e32_4218_92ca_da18d0c8b47f-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----e163bfd3fbd8----1---------------------d56027b7_5e32_4218_92ca_da18d0c8b47f-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----d56027b7_5e32_4218_92ca_da18d0c8b47f-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----e163bfd3fbd8----1---------------------d56027b7_5e32_4218_92ca_da18d0c8b47f-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----e163bfd3fbd8----1-----------------bookmark_preview----d56027b7_5e32_4218_92ca_da18d0c8b47f-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----e163bfd3fbd8----2---------------------d56027b7_5e32_4218_92ca_da18d0c8b47f-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----e163bfd3fbd8----2---------------------d56027b7_5e32_4218_92ca_da18d0c8b47f-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----e163bfd3fbd8----2---------------------d56027b7_5e32_4218_92ca_da18d0c8b47f-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----e163bfd3fbd8----2---------------------d56027b7_5e32_4218_92ca_da18d0c8b47f-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----e163bfd3fbd8----2---------------------d56027b7_5e32_4218_92ca_da18d0c8b47f-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----e163bfd3fbd8----2---------------------d56027b7_5e32_4218_92ca_da18d0c8b47f-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----2-----------------clap_footer----d56027b7_5e32_4218_92ca_da18d0c8b47f-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----e163bfd3fbd8----2---------------------d56027b7_5e32_4218_92ca_da18d0c8b47f-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----e163bfd3fbd8----2-----------------bookmark_preview----d56027b7_5e32_4218_92ca_da18d0c8b47f-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----e163bfd3fbd8----3---------------------d56027b7_5e32_4218_92ca_da18d0c8b47f-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----e163bfd3fbd8----3---------------------d56027b7_5e32_4218_92ca_da18d0c8b47f-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----e163bfd3fbd8----3---------------------d56027b7_5e32_4218_92ca_da18d0c8b47f-------", "anchor_text": "Jacob Marks, Ph.D."}, {"url": "https://towardsdatascience.com/?source=author_recirc-----e163bfd3fbd8----3---------------------d56027b7_5e32_4218_92ca_da18d0c8b47f-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----e163bfd3fbd8----3---------------------d56027b7_5e32_4218_92ca_da18d0c8b47f-------", "anchor_text": "How I Turned My Company\u2019s Docs into a Searchable Database with OpenAIAnd how you can do the same with your docs"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----e163bfd3fbd8----3---------------------d56027b7_5e32_4218_92ca_da18d0c8b47f-------", "anchor_text": "15 min read\u00b76 days ago"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&user=Jacob+Marks%2C+Ph.D.&userId=f7dc0c0eae92&source=-----4f2d34bd8736----3-----------------clap_footer----d56027b7_5e32_4218_92ca_da18d0c8b47f-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----e163bfd3fbd8----3---------------------d56027b7_5e32_4218_92ca_da18d0c8b47f-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "20"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&source=-----e163bfd3fbd8----3-----------------bookmark_preview----d56027b7_5e32_4218_92ca_da18d0c8b47f-------", "anchor_text": ""}, {"url": "https://medium.com/@sarah.tam17?source=post_page-----e163bfd3fbd8--------------------------------", "anchor_text": "See all from Sarah Tam"}, {"url": "https://towardsdatascience.com/?source=post_page-----e163bfd3fbd8--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://medium.com/codex/time-series-prediction-using-lstm-in-python-19b1187f580f?source=read_next_recirc-----e163bfd3fbd8----0---------------------a69eccde_c2ad_499a_9529_68373caaa1dd-------", "anchor_text": ""}, {"url": "https://medium.com/@coucoucamille?source=read_next_recirc-----e163bfd3fbd8----0---------------------a69eccde_c2ad_499a_9529_68373caaa1dd-------", "anchor_text": ""}, {"url": "https://medium.com/@coucoucamille?source=read_next_recirc-----e163bfd3fbd8----0---------------------a69eccde_c2ad_499a_9529_68373caaa1dd-------", "anchor_text": "Coucou Camille"}, {"url": "https://medium.com/codex?source=read_next_recirc-----e163bfd3fbd8----0---------------------a69eccde_c2ad_499a_9529_68373caaa1dd-------", "anchor_text": "CodeX"}, {"url": "https://medium.com/codex/time-series-prediction-using-lstm-in-python-19b1187f580f?source=read_next_recirc-----e163bfd3fbd8----0---------------------a69eccde_c2ad_499a_9529_68373caaa1dd-------", "anchor_text": "Time Series Prediction Using LSTM in PythonImplementation of Machine Learning Algorithm for Time Series Data Prediction."}, {"url": "https://medium.com/codex/time-series-prediction-using-lstm-in-python-19b1187f580f?source=read_next_recirc-----e163bfd3fbd8----0---------------------a69eccde_c2ad_499a_9529_68373caaa1dd-------", "anchor_text": "\u00b76 min read\u00b7Feb 10"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fcodex%2F19b1187f580f&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fcodex%2Ftime-series-prediction-using-lstm-in-python-19b1187f580f&user=Coucou+Camille&userId=d796c2fbb274&source=-----19b1187f580f----0-----------------clap_footer----a69eccde_c2ad_499a_9529_68373caaa1dd-------", "anchor_text": ""}, {"url": "https://medium.com/codex/time-series-prediction-using-lstm-in-python-19b1187f580f?source=read_next_recirc-----e163bfd3fbd8----0---------------------a69eccde_c2ad_499a_9529_68373caaa1dd-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F19b1187f580f&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fcodex%2Ftime-series-prediction-using-lstm-in-python-19b1187f580f&source=-----e163bfd3fbd8----0-----------------bookmark_preview----a69eccde_c2ad_499a_9529_68373caaa1dd-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----e163bfd3fbd8----1---------------------a69eccde_c2ad_499a_9529_68373caaa1dd-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----e163bfd3fbd8----1---------------------a69eccde_c2ad_499a_9529_68373caaa1dd-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----e163bfd3fbd8----1---------------------a69eccde_c2ad_499a_9529_68373caaa1dd-------", "anchor_text": "The PyCoach"}, {"url": "https://artificialcorner.com/?source=read_next_recirc-----e163bfd3fbd8----1---------------------a69eccde_c2ad_499a_9529_68373caaa1dd-------", "anchor_text": "Artificial Corner"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----e163bfd3fbd8----1---------------------a69eccde_c2ad_499a_9529_68373caaa1dd-------", "anchor_text": "You\u2019re Using ChatGPT Wrong! Here\u2019s How to Be Ahead of 99% of ChatGPT UsersMaster ChatGPT by learning prompt engineering."}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----e163bfd3fbd8----1---------------------a69eccde_c2ad_499a_9529_68373caaa1dd-------", "anchor_text": "\u00b77 min read\u00b7Mar 17"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fartificial-corner%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&user=The+PyCoach&userId=fb44e21903f3&source=-----886a50dabc54----1-----------------clap_footer----a69eccde_c2ad_499a_9529_68373caaa1dd-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----e163bfd3fbd8----1---------------------a69eccde_c2ad_499a_9529_68373caaa1dd-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "276"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&source=-----e163bfd3fbd8----1-----------------bookmark_preview----a69eccde_c2ad_499a_9529_68373caaa1dd-------", "anchor_text": ""}, {"url": "https://medium.com/@prateekgaurav/nlp-zero-to-hero-part-2-vanilla-rnn-lstm-gru-bi-directional-lstm-77fd60fc0b44?source=read_next_recirc-----e163bfd3fbd8----0---------------------a69eccde_c2ad_499a_9529_68373caaa1dd-------", "anchor_text": ""}, {"url": "https://medium.com/@prateekgaurav?source=read_next_recirc-----e163bfd3fbd8----0---------------------a69eccde_c2ad_499a_9529_68373caaa1dd-------", "anchor_text": ""}, {"url": "https://medium.com/@prateekgaurav?source=read_next_recirc-----e163bfd3fbd8----0---------------------a69eccde_c2ad_499a_9529_68373caaa1dd-------", "anchor_text": "Prateek Gaurav"}, {"url": "https://medium.com/@prateekgaurav/nlp-zero-to-hero-part-2-vanilla-rnn-lstm-gru-bi-directional-lstm-77fd60fc0b44?source=read_next_recirc-----e163bfd3fbd8----0---------------------a69eccde_c2ad_499a_9529_68373caaa1dd-------", "anchor_text": "NLP: Zero To Hero [Part 2: Vanilla RNN, LSTM, GRU & Bi-Directional LSTM]Link to Part 1of this article: NLP: Zero To Hero [Part 1: Introduction, BOW, TF-IDF & Word2Vec] Link to Part 3 of this article: NLP: Zero\u2026"}, {"url": "https://medium.com/@prateekgaurav/nlp-zero-to-hero-part-2-vanilla-rnn-lstm-gru-bi-directional-lstm-77fd60fc0b44?source=read_next_recirc-----e163bfd3fbd8----0---------------------a69eccde_c2ad_499a_9529_68373caaa1dd-------", "anchor_text": "\u00b78 min read\u00b7Mar 23"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F77fd60fc0b44&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40prateekgaurav%2Fnlp-zero-to-hero-part-2-vanilla-rnn-lstm-gru-bi-directional-lstm-77fd60fc0b44&user=Prateek+Gaurav&userId=966fe9bb6729&source=-----77fd60fc0b44----0-----------------clap_footer----a69eccde_c2ad_499a_9529_68373caaa1dd-------", "anchor_text": ""}, {"url": "https://medium.com/@prateekgaurav/nlp-zero-to-hero-part-2-vanilla-rnn-lstm-gru-bi-directional-lstm-77fd60fc0b44?source=read_next_recirc-----e163bfd3fbd8----0---------------------a69eccde_c2ad_499a_9529_68373caaa1dd-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F77fd60fc0b44&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40prateekgaurav%2Fnlp-zero-to-hero-part-2-vanilla-rnn-lstm-gru-bi-directional-lstm-77fd60fc0b44&source=-----e163bfd3fbd8----0-----------------bookmark_preview----a69eccde_c2ad_499a_9529_68373caaa1dd-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----e163bfd3fbd8----1---------------------a69eccde_c2ad_499a_9529_68373caaa1dd-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----e163bfd3fbd8----1---------------------a69eccde_c2ad_499a_9529_68373caaa1dd-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----e163bfd3fbd8----1---------------------a69eccde_c2ad_499a_9529_68373caaa1dd-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----e163bfd3fbd8----1---------------------a69eccde_c2ad_499a_9529_68373caaa1dd-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----e163bfd3fbd8----1---------------------a69eccde_c2ad_499a_9529_68373caaa1dd-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----e163bfd3fbd8----1---------------------a69eccde_c2ad_499a_9529_68373caaa1dd-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----1-----------------clap_footer----a69eccde_c2ad_499a_9529_68373caaa1dd-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----e163bfd3fbd8----1---------------------a69eccde_c2ad_499a_9529_68373caaa1dd-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----e163bfd3fbd8----1-----------------bookmark_preview----a69eccde_c2ad_499a_9529_68373caaa1dd-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/time-series-forecasting-with-deep-learning-in-pytorch-lstm-rnn-1ba339885f0c?source=read_next_recirc-----e163bfd3fbd8----2---------------------a69eccde_c2ad_499a_9529_68373caaa1dd-------", "anchor_text": ""}, {"url": "https://zainbaq.medium.com/?source=read_next_recirc-----e163bfd3fbd8----2---------------------a69eccde_c2ad_499a_9529_68373caaa1dd-------", "anchor_text": ""}, {"url": "https://zainbaq.medium.com/?source=read_next_recirc-----e163bfd3fbd8----2---------------------a69eccde_c2ad_499a_9529_68373caaa1dd-------", "anchor_text": "Zain Baquar"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----e163bfd3fbd8----2---------------------a69eccde_c2ad_499a_9529_68373caaa1dd-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/time-series-forecasting-with-deep-learning-in-pytorch-lstm-rnn-1ba339885f0c?source=read_next_recirc-----e163bfd3fbd8----2---------------------a69eccde_c2ad_499a_9529_68373caaa1dd-------", "anchor_text": "Time Series Forecasting with Deep Learning in PyTorch (LSTM-RNN)An in depth tutorial on forecasting a univariate time series using deep learning with PyTorch"}, {"url": "https://towardsdatascience.com/time-series-forecasting-with-deep-learning-in-pytorch-lstm-rnn-1ba339885f0c?source=read_next_recirc-----e163bfd3fbd8----2---------------------a69eccde_c2ad_499a_9529_68373caaa1dd-------", "anchor_text": "\u00b712 min read\u00b7Feb 9"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1ba339885f0c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftime-series-forecasting-with-deep-learning-in-pytorch-lstm-rnn-1ba339885f0c&user=Zain+Baquar&userId=d16fc4a70186&source=-----1ba339885f0c----2-----------------clap_footer----a69eccde_c2ad_499a_9529_68373caaa1dd-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/time-series-forecasting-with-deep-learning-in-pytorch-lstm-rnn-1ba339885f0c?source=read_next_recirc-----e163bfd3fbd8----2---------------------a69eccde_c2ad_499a_9529_68373caaa1dd-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "5"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1ba339885f0c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftime-series-forecasting-with-deep-learning-in-pytorch-lstm-rnn-1ba339885f0c&source=-----e163bfd3fbd8----2-----------------bookmark_preview----a69eccde_c2ad_499a_9529_68373caaa1dd-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/language-models-gpt-and-gpt-2-8bdb9867c50a?source=read_next_recirc-----e163bfd3fbd8----3---------------------a69eccde_c2ad_499a_9529_68373caaa1dd-------", "anchor_text": ""}, {"url": "https://wolfecameron.medium.com/?source=read_next_recirc-----e163bfd3fbd8----3---------------------a69eccde_c2ad_499a_9529_68373caaa1dd-------", "anchor_text": ""}, {"url": "https://wolfecameron.medium.com/?source=read_next_recirc-----e163bfd3fbd8----3---------------------a69eccde_c2ad_499a_9529_68373caaa1dd-------", "anchor_text": "Cameron R. Wolfe"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----e163bfd3fbd8----3---------------------a69eccde_c2ad_499a_9529_68373caaa1dd-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/language-models-gpt-and-gpt-2-8bdb9867c50a?source=read_next_recirc-----e163bfd3fbd8----3---------------------a69eccde_c2ad_499a_9529_68373caaa1dd-------", "anchor_text": "Language Models: GPT and GPT-2How smaller language models inspired modern breakthroughs"}, {"url": "https://towardsdatascience.com/language-models-gpt-and-gpt-2-8bdb9867c50a?source=read_next_recirc-----e163bfd3fbd8----3---------------------a69eccde_c2ad_499a_9529_68373caaa1dd-------", "anchor_text": "\u00b713 min read\u00b7Nov 24, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8bdb9867c50a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flanguage-models-gpt-and-gpt-2-8bdb9867c50a&user=Cameron+R.+Wolfe&userId=28aa6026c553&source=-----8bdb9867c50a----3-----------------clap_footer----a69eccde_c2ad_499a_9529_68373caaa1dd-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/language-models-gpt-and-gpt-2-8bdb9867c50a?source=read_next_recirc-----e163bfd3fbd8----3---------------------a69eccde_c2ad_499a_9529_68373caaa1dd-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8bdb9867c50a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flanguage-models-gpt-and-gpt-2-8bdb9867c50a&source=-----e163bfd3fbd8----3-----------------bookmark_preview----a69eccde_c2ad_499a_9529_68373caaa1dd-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----e163bfd3fbd8--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----e163bfd3fbd8--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----e163bfd3fbd8--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----e163bfd3fbd8--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----e163bfd3fbd8--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----e163bfd3fbd8--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----e163bfd3fbd8--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----e163bfd3fbd8--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----e163bfd3fbd8--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----e163bfd3fbd8--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}