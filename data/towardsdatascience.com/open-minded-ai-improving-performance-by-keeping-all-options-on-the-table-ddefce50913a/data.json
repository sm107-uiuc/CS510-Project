{"url": "https://towardsdatascience.com/open-minded-ai-improving-performance-by-keeping-all-options-on-the-table-ddefce50913a", "time": 1682995980.8654778, "path": "towardsdatascience.com/open-minded-ai-improving-performance-by-keeping-all-options-on-the-table-ddefce50913a/", "webpage": {"metadata": {"title": "Open Minded AI: Improving Performance by Keeping All Options on the Table | by Shaked Zychlinski | Towards Data Science", "h1": "Open Minded AI: Improving Performance by Keeping All Options on the Table", "description": "Following the highest reward is what lies underneath pretty much all Reinforcement Learning models, But it\u2019s quite naively narrow-minded when examining this approach from a higher perspective."}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/open-minded-ai-improving-performance-by-keeping-all-options-on-the-table-ddefce50913a?sk=38b2b6fce052dba76e442aa51f376e00", "anchor_text": "Friends Link!", "paragraph_index": 0}, {"url": "https://github.com/shakedzy/tic_tac_toe", "anchor_text": "tic_tac_toe", "paragraph_index": 1}, {"url": "https://github.com/shakedzy", "anchor_text": "my GitHub page", "paragraph_index": 1}, {"url": "https://bair.berkeley.edu/blog/2017/10/06/soft-q-learning/", "anchor_text": "this blogpost", "paragraph_index": 4}, {"url": "https://bair.berkeley.edu/", "anchor_text": "Berkeley AI Research Center", "paragraph_index": 4}, {"url": "https://arxiv.org/abs/1702.08165", "anchor_text": "this paper", "paragraph_index": 4}, {"url": "https://en.wikipedia.org/wiki/Entropy_(information_theory)", "anchor_text": "entropy", "paragraph_index": 4}, {"url": "https://meduim.com/@shakedzy/qrash-course-deep-q-networks-from-the-ground-up-1bbda41d3677", "anchor_text": "Q-Learning", "paragraph_index": 5}, {"url": "https://www.cs.uic.edu/pub/Ziebart/Publications/thesis-bziebart.pdf", "anchor_text": "this 236 pages Ph.D thesis", "paragraph_index": 11}, {"url": "https://www.cs.uic.edu/Ziebart", "anchor_text": "Brian Ziebart", "paragraph_index": 11}, {"url": "https://en.wikipedia.org/wiki/Entropy", "anchor_text": "entropy", "paragraph_index": 11}, {"url": "https://medium.com/@shakedzy/lessons-learned-from-tic-tac-toe-practical-reinforcement-learning-tips-5cac654a45a8", "anchor_text": "Tic-Tac-Toe agent I trained", "paragraph_index": 12}, {"url": "https://github.com/shakedzy/tic_tac_toe", "anchor_text": "run this experiment", "paragraph_index": 13}, {"url": "http://shakedzy.xyz", "anchor_text": "shakedzy.xyz", "paragraph_index": 16}], "all_paragraphs": ["Read this on Medium without being a Medium member using this Friends Link!", "The Tic-Tac-Toe game described in this post, as well as all algorithms and pre-trained models can be found on the tic_tac_toe repository on my GitHub page.", "When I\u2019m being asked to describe what fascinates me so much about Reinforcement Learning, I usually explain that I see it as if I train my computer in the same way I trained my dog \u2014 using nothing but rewards. My dog learned to sit, wait, come over, stand, lie down and pretend to be shot at (kudos to my wife), all in the exact same way \u2014 I rewarded her every time she did what I asked her to. She had no idea what we wanted from her every time we tried to teach her something new, but after enough trial-and-error, and some trial-and-success, she figured it out. The exact same thing happens when a Reinforcement Learning model is being taught something new. And that\u2019s absolutely amazing.", "Following the highest probable reward is the fundamental engine that lies underneath pretty much all Reinforcement Learning models \u2014 each one simply has its own method to accomplish it. But when examining this approach from a higher perspective, following naively after rewards is quite narrow-minded. Following such strategy will prevent us from being able to quickly adapt to unexpected changes, as we never maintain a Plan-B. Our entire exploration phase is performed only for the sake of finding the best possible way, with far less attention to other good options. Can we teach our models to open up their minds? And should we?", "Not too long ago I stumbled upon this blogpost by the Berkeley AI Research Center (which summarizes very briefly this paper), which suggests a novel method for learning \u2014 instead of learning the path which provides the highest reward, follow the one that provides the most positive options to choose from. In other words, teach your model to increase its action-selection entropy.", "Here\u2019s an example: let\u2019s consider a simple Q-Learning algorithm, and examine the following scenario:", "Assume we are standing in state s, and can choose from one of two actions: action X and action Y. From there we will reach state s\u2019, where three options will be possible: 1, 2 and 3, which will take us to the terminal state and we receive a reward. We see that if we stand in s\u2019 after selecting X, our policy will be to choose action 2 no matter what. On the other hand, if we reached s\u2019 after selecting Y, we can be a bit more flexible \u2014 though action 2 is still the best. In other words, the policy\u2019s entropy of s\u2019 after X is very low \u2014 as it focus solely on a single action, while after Y it\u2019s higher \u2014 as it can afford trying all states with a reasonable probability.", "But why bother? It\u2019s clear from the diagram that the optimal option will be action 2 from state s\u2019. True \u2014 but what will happen if there\u2019s a sudden change in the environment? A bug, a modification, or an unexpected action by an opponent? If such a change will suddenly prevent us from taking action 2, then action X becomes the wrong decision.", "But there\u2019s more to that than just paranoia. Just a few lines ago we agreed that after choosing action Y we can be more flexible in the next action selection. While there\u2019s still the best option, the others are not too far off of it, and this can allow the model to explore these other actions more, as the price payed for not choosing the optimal one is low. That cannot be said about the same scenario after choosing action X, and as we know \u2014 sufficient exploration is vital and crucial for a robust Reinforcement Learning agent.", "How to design a general policy that encourages an agent to maximize entropy is presented in the paper I linked to above. Here I\u2019d like to focus on the Soft Bellman Equation (discussed in the blogpost I referred to). Let\u2019s first refresh our memories with the regular Bellman Equation:", "The Soft Bellman Equation will try to maximize entropy rather than future reward. Therefore, it shall replace the last term, where we maximize over the future Q Value, with an entropy-maximization term. And so, in the case of a finite number of actions, the Soft Bellman Equation is:", "If you\u2019d like to see the mathematical proof for how this new term relates to the entropy, the blogpost authors claim it is found in this 236 pages Ph.D thesis by Brian Ziebart. If you\u2019ve taken a Thermodynamics class once, you can get the general idea if you recall that the thermodynamic entropy of a gas is defined as S=k\u22c5ln\u03a9, where the number of configurations, \u03a9, at equilibrium, is \u03a9\u2248exp(N), where N is the number of particles. If you have no idea what you\u2019ve just read, you\u2019ll just have to trust me (or read the thesis).", "If you\u2019ve read some of my blogposts so far, you might have noticed that I like to test things myself, and this case isn\u2019t different. I\u2019ve wrote once about a Tic-Tac-Toe agent I trained using Deep Q-Networks, so I decided to edit it and allow it to learn using the Soft Bellman Equation too. I then trained two agents: a regular Q-Network agent and a Max-Entropy-Q-Network agent. I trained two such agents playing against each other and then two other agents playing separately against an external player \u2014 and repeated this process 3 times, ending up with 6 different trained models of each type. I then matched all regular Q-Network agents with all Max-Entropy-Q-Network agents to see which type of agent wins the most games. I also forced the agents to select a different first move each game (to cover all possible game-options), and made sure they both get to play both X and O.", "The results are very clear: of the 648 games played, the Soft Bellman agents won 36.1% of the games (234), 33.5% ended in a tie (217) and only 30.4% of the games (197) were won by the regular Q-Network agents. When considering only the games played without me forcing the agents to make a specific first action, but rather letting them play as they wish, the results were even more in favor of the Soft Bellman agents: of 72 games played, 40.3% (29) were won by the Max-Entropy-Q-Network, 33.3% (24) were won by the regular agents, and the rest 26.4% (19) ended with no winner. I encourage you to run this experiment yourself too!", "This experiment have demonstrated that while learning complex systems, and even not so complex systems, following broader objectives other than the highest reward can be quite beneficial. As I see it, teaching a model such a broader policy, is as if we no longer treat the agent as a pet we wish to train \u2014 but a human we\u2019re trying to teach. I\u2019m excited to see what more can we teach it in the future!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Head of Recommendations at Lightricks. Lives in Tel-Aviv, Israel. See me on shakedzy.xyz"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fddefce50913a&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fopen-minded-ai-improving-performance-by-keeping-all-options-on-the-table-ddefce50913a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fopen-minded-ai-improving-performance-by-keeping-all-options-on-the-table-ddefce50913a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fopen-minded-ai-improving-performance-by-keeping-all-options-on-the-table-ddefce50913a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fopen-minded-ai-improving-performance-by-keeping-all-options-on-the-table-ddefce50913a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----ddefce50913a--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ddefce50913a--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://shakedzy.medium.com/?source=post_page-----ddefce50913a--------------------------------", "anchor_text": ""}, {"url": "https://shakedzy.medium.com/?source=post_page-----ddefce50913a--------------------------------", "anchor_text": "Shaked Zychlinski"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F43218078e688&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fopen-minded-ai-improving-performance-by-keeping-all-options-on-the-table-ddefce50913a&user=Shaked+Zychlinski&userId=43218078e688&source=post_page-43218078e688----ddefce50913a---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fddefce50913a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fopen-minded-ai-improving-performance-by-keeping-all-options-on-the-table-ddefce50913a&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fddefce50913a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fopen-minded-ai-improving-performance-by-keeping-all-options-on-the-table-ddefce50913a&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/open-minded-ai-improving-performance-by-keeping-all-options-on-the-table-ddefce50913a?sk=38b2b6fce052dba76e442aa51f376e00", "anchor_text": "Friends Link!"}, {"url": "https://github.com/shakedzy/tic_tac_toe", "anchor_text": "tic_tac_toe"}, {"url": "https://github.com/shakedzy", "anchor_text": "my GitHub page"}, {"url": "https://bair.berkeley.edu/blog/2017/10/06/soft-q-learning/", "anchor_text": "this blogpost"}, {"url": "https://bair.berkeley.edu/", "anchor_text": "Berkeley AI Research Center"}, {"url": "https://arxiv.org/abs/1702.08165", "anchor_text": "this paper"}, {"url": "https://en.wikipedia.org/wiki/Entropy_(information_theory)", "anchor_text": "entropy"}, {"url": "https://meduim.com/@shakedzy/qrash-course-deep-q-networks-from-the-ground-up-1bbda41d3677", "anchor_text": "Q-Learning"}, {"url": "https://www.cs.uic.edu/pub/Ziebart/Publications/thesis-bziebart.pdf", "anchor_text": "this 236 pages Ph.D thesis"}, {"url": "https://www.cs.uic.edu/Ziebart", "anchor_text": "Brian Ziebart"}, {"url": "https://en.wikipedia.org/wiki/Entropy", "anchor_text": "entropy"}, {"url": "https://medium.com/@shakedzy/lessons-learned-from-tic-tac-toe-practical-reinforcement-learning-tips-5cac654a45a8", "anchor_text": "Tic-Tac-Toe agent I trained"}, {"url": "https://github.com/shakedzy/tic_tac_toe", "anchor_text": "run this experiment"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----ddefce50913a---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----ddefce50913a---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----ddefce50913a---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----ddefce50913a---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/algorithms?source=post_page-----ddefce50913a---------------algorithms-----------------", "anchor_text": "Algorithms"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fddefce50913a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fopen-minded-ai-improving-performance-by-keeping-all-options-on-the-table-ddefce50913a&user=Shaked+Zychlinski&userId=43218078e688&source=-----ddefce50913a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fddefce50913a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fopen-minded-ai-improving-performance-by-keeping-all-options-on-the-table-ddefce50913a&user=Shaked+Zychlinski&userId=43218078e688&source=-----ddefce50913a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fddefce50913a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fopen-minded-ai-improving-performance-by-keeping-all-options-on-the-table-ddefce50913a&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ddefce50913a--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fddefce50913a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fopen-minded-ai-improving-performance-by-keeping-all-options-on-the-table-ddefce50913a&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----ddefce50913a---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----ddefce50913a--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----ddefce50913a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----ddefce50913a--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----ddefce50913a--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----ddefce50913a--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----ddefce50913a--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----ddefce50913a--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----ddefce50913a--------------------------------", "anchor_text": ""}, {"url": "https://shakedzy.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://shakedzy.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Shaked Zychlinski"}, {"url": "https://shakedzy.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.3K Followers"}, {"url": "http://shakedzy.xyz", "anchor_text": "shakedzy.xyz"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F43218078e688&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fopen-minded-ai-improving-performance-by-keeping-all-options-on-the-table-ddefce50913a&user=Shaked+Zychlinski&userId=43218078e688&source=post_page-43218078e688--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F4123ceb9438d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fopen-minded-ai-improving-performance-by-keeping-all-options-on-the-table-ddefce50913a&newsletterV3=43218078e688&newsletterV3Id=4123ceb9438d&user=Shaked+Zychlinski&userId=43218078e688&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}