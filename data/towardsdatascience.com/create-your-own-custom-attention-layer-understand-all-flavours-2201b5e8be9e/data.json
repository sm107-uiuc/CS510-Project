{"url": "https://towardsdatascience.com/create-your-own-custom-attention-layer-understand-all-flavours-2201b5e8be9e", "time": 1683017231.558239, "path": "towardsdatascience.com/create-your-own-custom-attention-layer-understand-all-flavours-2201b5e8be9e/", "webpage": {"metadata": {"title": "Write your own custom Attention layer: Easy, intuitive guide | Towards Data Science", "h1": "Craft your own Attention layer in 6 lines \u2014 Story of how the code evolved", "description": "Understand differences between Bahdanau, Luong, Raffel, Yang, self-attention & create your own sequence classification or Seq to Seq NMT with a custom Attention layer in barely 6 lines"}, "outgoing_paragraph_urls": [{"url": "https://www.kaggle.com/c/stanford-covid-vaccine/overview", "anchor_text": "link", "paragraph_index": 3}, {"url": "https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/", "anchor_text": "here", "paragraph_index": 8}, {"url": "https://www.youtube.com/watch?v=WCUNPb-5EYI", "anchor_text": "Here", "paragraph_index": 9}, {"url": "https://www.youtube.com/watch?v=4-QoMdSqG_I", "anchor_text": "This", "paragraph_index": 10}, {"url": "https://www.youtube.com/watch?v=aircAruvnKk", "anchor_text": "here", "paragraph_index": 11}, {"url": "https://arxiv.org/pdf/1512.08756.pdf", "anchor_text": "here", "paragraph_index": 41}, {"url": "https://www.kaggle.com/qqgeogor/keras-lstm-attention-glove840b-lb-0-043", "anchor_text": "Toxic comment classification challenge", "paragraph_index": 41}, {"url": "https://www.kaggle.com/sermakarevich/hierarchical-attention-network", "anchor_text": "here", "paragraph_index": 42}, {"url": "https://www.cs.cmu.edu/~./hovy/papers/16HLT-hierarchical-attention-networks.pdf", "anchor_text": "here", "paragraph_index": 45}, {"url": "https://arxiv.org/pdf/1409.0473.pdf", "anchor_text": "here", "paragraph_index": 51}, {"url": "https://arxiv.org/pdf/1312.6026.pdf", "anchor_text": "this", "paragraph_index": 53}, {"url": "https://arxiv.org/pdf/1508.04025.pdf", "anchor_text": "https://arxiv.org/pdf/1508.04025.pdf", "paragraph_index": 60}, {"url": "https://www.tensorflow.org/api_docs/python/tf/keras/layers/AdditiveAttention", "anchor_text": "here", "paragraph_index": 64}, {"url": "https://github.com/tensorflow/nmt", "anchor_text": "here", "paragraph_index": 65}, {"url": "https://arxiv.org/abs/1503.08895", "anchor_text": "End-to-End Memory Networks", "paragraph_index": 69}, {"url": "https://arxiv.org/pdf/1601.06733.pdf", "anchor_text": "here", "paragraph_index": 70}, {"url": "https://arxiv.org/pdf/1502.03044.pdf", "anchor_text": "this", "paragraph_index": 72}], "all_paragraphs": ["Sometimes when the Dalai Lama addresses large audiences, he gets caught up in the narrative and speaks in longer and longer chunks. This complicates the job of his interpreter, who has to wait for a pause to translate the speech to English ensuring that every essence of the message \u2014 every intent, pun, sarcasm, irony, humor \u2014 is passed on undiluted to the audience. This is an extraordinary job requiring extraordinary skills. There are occasions where the interpreter has to wait up to 20 mins to speak and this challenges even his razor-sharp memory honed by years of meditation. The tension in the air is palatable as the audience looks at the interpreter in sympathy, wishing him to somehow succeed. And succeed he does\u2026 much to the appreciation of the audience. If someone were to hack into the interpreter\u2019s brain to find out how he does this, they might conclude that the interpreter would perhaps have made a mental note of all the important words to which to pay \u2018attention\u2019 during translation.", "By now, the reader would have made the connection with the LSTMs of the machine learning world and how the problem of very long sequences of text could cause the ML model to falter, were it not for an innovation called \u2018Attention\u2019 that gained fame in 2014. Today, many state-of-the-art models use Attention in some way or another. Thankfully, there is a huge abundance of information on Attention that we can refer to. Unfortunately, it is this abundance of information that sometimes leads to a deficit of attention (no pun intended). Let me explain\u2026", "I remember sitting in the library of my first company in \u201899, going thru\u2019 the voluminous mainframe manuals. They were fairly dry and not easy to comprehend as the material was crouched in technical jargon. It almost looked as if a focussed effort had been put to make the material difficult to understand (Jay Alammar would not have been accepted as an author in those times but S.Tharoor would probably have been given the red carpet). But I had one distinct advantage! There was only ONE source of information \u2014 take it or leave it. So I ploughed on, until solutions finally presented themselves.", "Circa 2020. Me trying to contribute to a Kaggle competition which involved using AI/ML to identify the right mRNA structure which could serve as a stable Covid vaccine (link). I had in the recent past, dabbled with a few old-style ML programs (SVMs, Random forests etc) and was just beginning my first neural net program. Thanks to the basic pseudocode shared by some of the seniors, I was off to a decent start. We were convinced that the stability depended on the sequence. LSTMs were used liberally. I wanted to add an Attention mechanism on top of that because I felt LSTM was not capturing the complete sequence info. And then all my problems began!", "Attention is one of those topics that is easy to understand intuitively. There are several great tutorials which bring out this \u2018 intuitive\u2019 explanation very well. Yet, when it comes to a custom practical implementation, it can be pretty confusing to understand. For starters \u2014 there are several variations and this lack of a \u2018standard approach\u2019 can sometimes be confusing. Even if we take a single standard approach, there could be several variations in the implementation. Even if we take a single standard approach and a single standard implementation, there could be minor variations in code based on the framework used and further variations within versions of the same framework itself. Moreover, some prefer using an off-the-shelf attention layer, others prefer custom layers while yet others use regular layers to achieve attention-like functionality. All of this could be confusing to the novice who is trying to peek under the hood.", "I am sure you too will nod your head as I repeat the words of economist Herbert Simon who warned of an information overdose as early as 50 years back with his beautiful words \u2014", "\u201cIn an information-rich world, the wealth of information means a dearth of something else: a scarcity of whatever it is that information consumes. What information consumes is rather obvious: it consumes the attention of its recipients. Hence a wealth of information creates a poverty of attention\u2026\u201d", "This article aims to introduce attention in its simplest essence possible. We then build upon this simple model to add various flavours in an easy-to-understand way. Attention can be used in a variety of use-cases and while its most common use is in sequence-sequence language translators which are reasonably well explained, they are several other use-cases as well. A de-linking of the \u2018Attention\u2019 concept from translators would help when we try to peek under the hood.", "We will not spend much time on the intuition behind attention \u2014 this is covered well by Jay Allamar here \u2014 this is, in my view, one of the best introductions to \u2018Attention\u2019.", "It is also assumed that you have a basic idea of RNN\u2019s. Here is one of the best videos covering this topic \u2014 Thank you Brandon Rohrer!", "This captivating 50-minute video by Jay Allamar gives you an instant intuitive overview of embeddings. Jay is indeed a master story-teller!", "Last but not the least, if you are totally new to neural nets, here is a link to 3Blue1Brown\u2019s excellent video. This introduces neural nets in the softest way possible. This 30 min lecture is equivalent to many weeks of classes. I include this link in nearly every article that I write, as a tribute to its creator Grant Sanderson. But there are other reasons why I admire Grant. Anybody who can get 3.2 Million viewers to subscribe for a set of \u2018Maths\u2019 courses must be an illusionist of the highest order.", "Let us look at using \u2018Attention\u2019 in a classification problem instead of the regular translator example that we typically see. You want to take a tweet and tell whether the sentiment is positive or negative. This is a sequence classification exercise. Let us look at this hypothetical tweet:\u2018Attention is so hard to understand. There are so many blogs and implementation and tend to confuse the novice\u2019", "Overall, the sentiment is somewhat negative. A typical classification exercise (without Attention) might involve feeding the above tweet to an LSTM which retains a memory of the sequence and then uses the final hidden state (the output after the last word) as an input to a feedforward dense neural net layer which learns and then classifies. It is assumed you already have a fair understanding of the below steps:", "The results seem good but now you want to bring in an \u2018Attention\u2019 mechanism to further improve the score. You feel that instead of using just the last hidden state of the LSTM in step 3 as an input to the feedforward layer in step 4, you need to pay attention to every single hidden state generated, decide which ones are important and then take a weighted sum and feed this as input to step 4. You decide to plunge headlong into the code. The LSTM layer at Step 3 has the option of returning a hidden state for every word with the \u2018return_sequences=true\u2019 option and you decide to make full use of it. So far so good.", "Now, after step 3, you have 19 hidden states. Intuitively, you know that all that is needed now is a set of weights. You need 19 weights overall \u2014 one for each hidden state, i.e basically, one weight for each of the 19 words. You intuitively know that words like \u2018hard\u2019, \u2018confuse\u2019 etc should be given higher weight and if it were in your hands, you would probably give 49.5% weight to each of these two words and distribute the remaining 1% to each of the other 17 words. Unfortunately, that kind of hard-coding won't work and you need the machine to output these weights. Your intuition tells you that when you don't know something, you can use neural nets to guess those values. So you create an \u2018ADDITIONAL\u2019 simple feedforward layer between steps 3 & 4 to give you those 19 weights!", "Before we get into the code, let us first try to understand a little more about this so-called \u2018simple additional feedforward layer\u2019. Maybe, because it is so simple, it is not typically explained in existing blogs.", "Intuitively, you may want to define the layer weights as (?,19,1). If you train this model to generate 19 weights, you feel this will give you a weight vector where the words number 4 (hard) and number 17 (confusing) have higher weights. Can you guess what could be the problem with this?", "The problem is \u2014 You are training the layer to assign weights based on the position of the words instead of training the model to assign weights based on the meaning of the words. So if after the training (i.e., during production launch), another tweet were to come up for classification which read something like:\u2018I understood attention after reading Jay Allamar's brilliantly written blog. It is too good and all misunderstanding was cleared\u2019", "Guess what happens now? The weights of the model you have trained are such that the 4th and the 17th word get importance. So \u2018after\u2019 and \u2018misunderstanding\u2019 gets all the attention and most likely the sentiment would be rated negative and Jay might feel sad.", "So instead of training the model on the \u2018position\u2019 of the words, we need to train the model on the \u2018meaning\u2019 of the words. Since the \u2018meaning\u2019 of the word is encoded in its \u2018embedding\u2019, this basically means that the model needs to consider all 256 dimensions of each word during training and create weights for each one of those. So this means that the number of weights generated by the layer after training should be 256. So the weights in the additional layer we are defining should be (?, 256, 1) \u2014 one weighted connection each for the 256 inputs connected to the solitary neuron in this layer. This also makes sense mathematically speaking, during training, we matrix multiply each word embedding by this weight. So (?, 19, 256) matrix is multiplied by (?, 256, 1) and this yields us a set of weights (?, 19,1) which are our attention weights (well almost\u2026we have to use an activation and add a softmax layer to ensure that all these weights add up to 1). The rest is a simple multiplication to get the weighted sum. This is the \u2018attention adjusted output\u2019 which is fed to the next layer.", "With this, we have defined the basics of what our \u2018Attention\u2019 layer looks like. If you have understood the above \u2014 particularly the difference between the \u2018attention weights\u2019 which is of shape (19,1) and \u2018layer weights\u2019 which is of shape (256,1), then implementation is going to be a piece of cake irrespective of the framework you are using. There will also be 1 bias for every word, so the bias shape is going to be (19,1). We will do a custom implementation which takes less than half a dozen lines of code. Of course, this is the bare essence after peeling away all the layers of the onion skins. Once you have understood the bare essence, it is always better to add back the peels which help in keeping the code more structured/readable and maintainable.", "By the way, which type of attention is this \u2014 you may ask \u2014 xyz or abc or def? To be frank, at this point in time, the answer is \u2014 I don't know. In fact, this reminds me of a funny story read in the Tinkle comic book more than 3 decades back \u2014 The owner of a house has a lot of fancy gadgets and likes to show off a bit. When a guest comes and requests to look at his latest acquisition, his assistant gets that object from the other room. After the guest leaves, the owner reprimands the assistant saying, he should always ask whether he needs to bring the American model or the Asian one or the European one etc\u2026 the idea being to show off his richness. The assistant, of course, gets miffed and when another guest comes the next day and the owner requests his assistant to call his father from the other room to make introductions, the assistant asks \u2014 Shall I get your American father or European father or your Asian father?", "Well, humour never got me anywhere and may well get this article rejected from publications, nonetheless, I am going to keep this little joke. The fact is that it does not matter whether we are dealing with \u2018this\u2019 Attention or \u2018that\u2019 Attention or some \u2018other\u2019 Attention. We will take the intuitive approach for the problem at hand, write the code and then later reflect upon which attention model our code closely resembles. Most versions are minor tweaks of each other\u2026 I am sure it would resemble one of the many models present today and if it doesn't \u2014 hey, we got a new IEEE paper on our hands!", "Adding a custom layer to any neural net is easy. In general, it follows the same pattern irrespective of whether you are using TF, Keras, Pytorch or any other framework. Let us use Keras. We start by sub-classing the base \u2018layer\u2019 class & create our own custom layer. Next, define the weights in the build() method. Put the logic in the call() method and we are pretty much done. Oh, we also need to define the init() layer. There are a couple of optional methods we can choose to define e.g. to save and load the layer later but we will stick to basic usage for now.", "So this is it! Exactly half a dozen lines of code and we have created an attention layer. We invoke this layer between steps 3 & 4 \u2014 let us call it step 3.5", "We need to add some peels back though. For one thing \u2014 all the hardcoding business is not good. We have the input shape which comes to the layer class build() method as a param \u2018input_shape\u2019. Let us derive the layer weights and bias shapes from that. We discussed the input shape would be: (?,19,256). The layer-weight is currently hardcoded as (256, 1). The last index of the input_shape is nothing but the number of dimensions for each word. This should be the number of weights in the layer. The last-but-one index of the input_shape is the number of timesteps or words in the tweet. Note that I use reverse indexing instead of forward which eliminates hassles with presence or absence of batch size. Lastly, we have one solitary neuron in this layer. This explains the hard-coding of 1. Let us refine the build method slightly as:", "Better, but we can do further improvements. There are different types of initializers, so you may want to soft-code that. Also, it makes more sense to define the num_units in the init() method as a method parameter instead of hardcoding it to 1 in build(). You can then reference it in the build() method by using self.units. Lastly, avoid specific variable names like num_dim_perword or words_pertweet. Rename it to inp_dimensions and seq_length etc which is more generic.", "Let us turn our attention to the call() method now. Actually, it has only 3 lines and is as straightforward as can be:", "We multiply the inputs \u2018x\u2019 of shape (19 * 256) by the layer weights \u2018w\u2019 of shape (256 * 1) and obtain a (19 * 1) matrix. We add the bias (19 * 1) and pass the output thru\u2019 any activation layer. So we now have 19 * 1 values (I would not call them attention weights yet). We take a softmax of these values. Softmax squashes these into values in the range between 0, and 1 whose sum is 1. These are the 19 attention weights. We multiply each attention weight by the respective word and sum up and we are done. We now have the \u2018attention adjusted output\u2019 state ready to be fed to the next layer.", "It is a good idea to also look at the minor changes needed to get this code working across frameworks. For example, based on the framework you are using, you may need to explicitly \u2018squeeze\u2019 the (19 * 1) attention vector that you got above into a 1-D array of (19) before computing the softmax. Some frameworks do it automatically for you and some don't. After calculating the softmax, you need to \u2018expand\u2019 back the attention weights from (19) to (19 * 1)", "But the best approach I like is the functional one \u2014 something akin to the below. This helps us visualise the code as we go along and make it a little more intuitive and easy to understand.", "The first 3 lines are easily understood. We now get \u2018a\u2019 which is a set of 19 weights \u2014 a0, a1\u2026a19. Let us say it is 0.49999 for the 4th and the 17th word and a tiny 0.000005 or something for the rest of the 17 words.", "We now need to multiply the first word (by word, I mean all of its 256 dimensions) with a0, 2nd word with a1 and so on for all 19 words. This is done by the below 3 lines.", "We take each weight-vector of 19 values and repeat it 256 times. We get 256 rows of 19 attention weights. What we actually need for the matrix multiplication is 19 rows of 256 (same valued) attention weights. So we swap the indices slightly (changing the stride). This gives us 19 vectors. Each vector has 256 dimensions. Each of the 256 dimensions has the same value which is nothing but the attention weight for the word in question. So in the 4th and the 17th row, we have a vector which has 256 values all of them [.49999, .499999, .49999,\u2026] and for the rest of the words it is [.00005, .00005, .00005, .00005,\u2026]. The rest of the logic is straightforward to understand. The Lambda layer wraps up any arbitrary function and gives it a \u2018layer-like\u2019 look.", "All the code snippets above achieve the same thing. In fact, if you look carefully enough, we haven't done anything \u2018grand\u2019 in our new custom layer. Any layer of a feedforward network, by default, multiplies the input with the weights to arrive at the output. The only additional customization we have done is to do the softmax calculation and the weighted summation. So the next logical question is \u2014 Do we need a custom layer at all? Why don't we use the \u2018regular layer\u2019 command itself to create a layer and then do all this softmax business after we get the output from the layer? Why bother writing a custom layer? This is a very pertinent question and hence the below type of approach is also correct:", "Take good note of this approach for this is the style that will serve us best going ahead. Note that we don't need to define any custom layer here and the above code directly comes after the LSTM layer at step 3. Also, notice how we don't need to define layer weights and just need to give the number of units. The framework does all the shape-calculations by itself.", "In the above case, the framework sees that the input \u2018lstm_out\u2019 has the shape (?,19,256) and the number of units we have given to this layer is 1 and therefore the framework itself defines the layer weight shape as (?,256,1) and the output of this layer has a shape (?,19,1). We will use this style when creating more serious implementations of Attention.", "There are other interesting approaches as well. For e.g. in one implementation, I found attention added in the LSTM layer \u2014 step 3 itself by overriding the necessary methods in the appropriate classes used by the LSTM layer (without harming the LSTM flow).", "We discussed about half-a-dozen different possible implementations of a single flavour of Attention. While we haven't coded each one in entirety, we have examined the bare essence of each. Now you can safely navigate any \u2018attention\u2019 code and clearly know what is happening at any point in time. Now let us start tweaking this code to bring in the various flavours of attention. Before we do that, it may help to summarise intuitively what we have done so far. I find it easy to do so by putting in plain English what we expect the layer to do. In the above case, we would make a plain English request as follows \u2014 Layer, Layer in the model, Given a tweet, could you highlight all the words carrying sentiment in them?", "or to be more specific \u2014 Could you give me a set of 256 weights that transform any input word such that the words involving sentiment generate a higher output signal. Oh! by the way, since this is a straightforward linear problem, I have given you only 1 neuron to help you with the calculations.", "Which paper is the above simple model based on? The closest match I could find was the work done by Raffel et al. here. How about a working code? There are many implementations of this type of simple Attention for NLP in Kaggle, most inspired directly by GrandMaster qianqian solving the Toxic comment classification challenge.", "Now let us turn our attention to different \u2018flavours\u2019 of adding attention. A new variable here or a minor code tweak there and hey presto \u2014 you get a new approach. For e.g. one paper defines a new variable \u2014 the \u2018context\u2019, and this \u2018context\u2019 is supposed to best summarize the sentiment of the sentence or the tweet in one word. Who determines the value of this \u2018context\u2019? Well, the authors say \u2014 let the additional feedforward layer determine the value of this context along with the weights and bias during training. So after the training, you have the weights, bias AND a \u2018context\u2019 vector that best summarizes the sentiment of the tweet. Now, we calculate the attention weights based on the similarity of each of the 19 words with the \u2018context\u2019 vector as is done here. Other than an additional line or 2 of code (one line in build method to define the context \u2018u\u2019, another in call method to do the dot-product of the output with the self.u), the rest of the processing is the same as before.", "Note that the product of 2 vectors gives a measure of their similarity. Softmaxing it returns a set of 19 probabilities adding up to 1. Each probability indicates how close the word is to the context vector. The rest of the processing is the same and we finally end up with the \u2018attention adjusted output\u2019.", "Intuitively, this \u2018Context\u2019 approach may help the model return a higher score as compared to the plain-vanilla \u2018no-context\u2019 attention. Instead of blindly shooting in the dark, this \u2018context\u2019 provides a reference point for the layer \u2014 it asks the layer to select weights such that after transformation, the words nearer to this \u2018context\u2019 should have a better say in determining the label.", "But this Attention implementation is just a side effect of the actual intent of the authors which is to show how attention could help in document classification. Texts tend to have a hierarchical structure and the importance of words and sentences are highly context-dependent. To include this insight, Yang et al. proposed the hierarchical model here, which uses two levels of attention \u2014 one at the word level and one at the sentence level. This makes it an extremely powerful model to analyse text corpora and hence the importance of this paper cannot be emphasised enough. There is another pleasant side effect of this approach. The attention mechanism here can also function as a pure \u2018reduction\u2019 operation, which could be used in place of any pooling step. This is because the \u2018context\u2019 that is derived, is 1 word and it best summarises the sentiment of the 19-word tweet \u2014 a classic \u2018reduction\u2019.", "There are several other variations of attention. For instance, Parikh et al. (2016) introduce an attention mechanism that takes two sentences and outputs a single vector. Another take on this is Attention-over-Attention \u2014 Cui et al, 2016. The idea here is to place another attention layer over the primary attentions, to indicate the relative importance of each attention. We can go on and on\u2026", "In fact, this brings us back to a point where we can couple back the \u2018language translator\u2019 aspect to the above \u2018attention\u2019 code. Let us say we want to convert the tweets from English to German. This is a sequence to sequence translation. Some call it an NMT \u2014 a neural machine translator. The plain vanilla translator without Attention is straightforward to design. There is a decoder at the fourth step and this is another LSTM layer. By default, this takes the value of the last hidden state from the encoder and starts spitting out the translation for the first word (there is a Dense final layer on top which actually does this). To translate the second word, it uses the hidden state generated by the decoder in the previous time-step and also the just-translated word# 1 and generates the second word and a second hidden state. Similarly, the third word is generated with two inputs 1) hidden state generated from the decoder after time-step two & 2) the second word just translated. Note: I am giving a quick intuitive overview here and not covering specifics like end of sentence placeholders etc. Before proceeding further, you may want to recheck the reference links above to get a clear and intuitive understanding of how a plain decoder (without attention) works. The discussion henceforth focusses on the part where we add \u2018Attention\u2019 to a translator.", "We now want to add \u2018attention\u2019 to this process. So instead of taking just the last hidden state as input, the decoder needs to take all hidden states generated by the encoder and pay attention to the ones that matter most. This is similar to the approach we have seen earlier in Yang\u2019s paper \u2014 with one exception \u2014 instead of determining a \u2018context\u2019 through training, the \u2018context\u2019 (or more specifically \u2018an indicator to the context\u2019 which I call the pseudo-context) in this case is nothing but the last hidden state. The layer has to match all the hidden states generated by the encoder to this \u2018pseudo-context\u2019 to generate the attention weights. The \u2018final context\u2019 is the weighted average of all the hidden states. Thus, every step of the decoder now has 3 inputs instead of 2. The semantics can get a bit confusing here. So let us spend a couple of minutes to get a crystal-clear idea on the role of each player. We will use these semantics for the rest of the article.", "Player 4, Player 1 are concatenated and fed to the next time-step along with the translated word of the previous step as input. The output is the next translated word and a new hidden state (which becomes player 1 for the next step). This process repeats 19 times. This is a raw essence of Attention in sequence to sequence models and both Bahdanau et al. and Luong et al. have variations over this essence.", "We can see how a trail is built from Raffel (generate a set of attention weights, use it to calculate the \u2018attention-adjusted\u2019 state and use this to predict) to a version of Yang (generate a context, using that context generate a set of attention weights and use this to calculate the \u2018attention-adjusted\u2019 state and predict) to the above model (use the prev hidden state as a pseudo-context, using that pseudo-context, generate a set of attention weights and use this to calculate the \u2018attention-adjusted\u2019 state. Now concatenate the prev hidden state, the attention-adjusted state and the translated word and use this to predict). Surprisingly it transpires that the papers came in exactly the reverse timeline order!!!", "We will spend a short time on Bahdanau & Luong variations as these are important papers and I didn't find a satisfying explanation of these approaches anywhere on the web. The best option is to go thru\u2019 the papers themselves. Bahdanau et al. introduced \u2018Attention\u2019 for the first time here. They also tried to increase non-linearity in the model. The final output in Bahdanau continues to be a function of 3 variables listed below with one minor change:", "The next surprise is in the logic for calculation of decoder hidden state at time-step \u2018t\u2019. They calculate this value somewhat indirectly. They use another simple function which takes 3 inputs:", "So basically Bahdanau has an additional step of calculating \u2018decoder hidden state at time-step t\u2019. Why do we need a separate function for this? I am not very sure, but when I tried going thru\u2019 this paper by Pascanu et al., it seems that this kind of approach makes the simple RNN (which is sort of shallow, let us admit) deeper. So possibly, it may perform better.", "There are a couple of other minor changes to the way the context vector is calculated. Like before, we have 2 key inputs to the function determining the context: the decoder hidden state at t-1 (s_tminus1) as the pseudo context and all the encoder hidden state outputs (let us call it hj instead of lstm_out to be more generic). The first change is that they recommend s_tminus1 value to be uni-directional, not bi-directional. hj values continue to be bi-directional. The second change is that there is no dot product between them involved here. They train and find 3 separate weights u,v,w such that the attention weights are the softmax of:", "They continue to use single layer MLP\u2019s to generate 3 weights \u2014 a weight w for the prev hidden state S_tminus1, a weight u for all the encoder hidden states(hj) and another mystery weight \u2018v\u2019. Giving a separate set of weights w, u sounds like a good idea, but I felt that there should be a dot product somewhere between the prev hidden state S_tminus1 and hj while calculating attention. There isn't one here. This is one aspect that I couldn't yet understand intuitively. It is just an addition of weighted values multiplied with yet another derived weight\u2026hence the name additive attention. Maybe it has some other benefit that I am overlooking. Anyway, let us see how u,v,w are derived.", "We discussed earlier that the problem of getting attention weights was a simple linear one and needs just 1 unit single MLP in the new layer. However, in Bahdanau we have a choice to use more than one unit to determine w and u \u2014 the weights that are applied individually on the decoder hidden state at t-1 and the encoder hidden states. Having done that, we need to massage the tensor shape back to (?,19,1) and hence there is a need for another weight v. Determining v is a simple linear transformation and needs just 1 unit. So you may see Bahdanau implementation in the following way to determine context:", "So we have layers inside layers. Keras provides this kind of nice functionality. w, u are determined using a single layer MLP of \u2018n\u2019 units. \u2018v\u2019 needs a single unit-single layer MLP as discussed earlier. All the layer weights for these 3 feed forward layers are calculated based on the respective input shapes. All three feedforward layers do the default layer processing \u2014 it multiples the layer weights by the input and adds a bias if any. All of that need not be explicitly coded as it is the default behaviour in a Dense layer. Once u,v,w are determined, the context is calculated in call() and the rest of the code above is more or less straightforward.", "Assume our old 19-word tweet conversion problem where we have an encoder which is bidirectional LSTM of 128 units and a decoder LSTM of 128 units. Assume n=10 units for the alignment layer to determine w,u. Then: the shapes for stminus1 and hj would be (?,128) and (?,19,256). Note that stminus1 is the single decoder hidden state at t-1 and hj are the 19 hidden states of the bi-directional encoder. We have to expand stminus1 to (?,1,128) for the addition that follows later along the time axis. The layer weights for w,u,v will be automatically determined by the framework as (?,128,10), (?,256,10) and (?,10,1) respectively. Notice how self.w(stminus1) works out to (?,1,10). This is added to each of the self.u(hj) to give a shape of (?,19,10). The result is fed to self.v and the output is (?,19,1) which is the shape we want. Softmaxing this gives the attention weights.", "Multiplying this attention weight with each encoder hidden state and summing up returns the weighted context. This is the essence of Bahdanau. Of course, as discussed, we now need to use this freshly minted context, the last translated word and the decoder hidden state at t-1 to generate the decoder hidden state at t. Then we use this hidden state at t, along with context and the last translated word to finally generate our translated word. Phew!!", "Luong et al (https://arxiv.org/pdf/1508.04025.pdf) introduce local attention. The idea of global attention (or soft attention) is to use all the hidden states of the encoder when computing each context vector which is what we have done in all the models explained so far. The downside of a global attention model is that it has to attend to all words on the source side for each target word, which is computationally costly. Local attention is an interesting mix of hard and soft attention. It first chooses a position in the source sentence. This position will determine a window of words that the model attends to. Calculating Local attention during training is slightly more complicated and requires techniques such as reinforcement learning to train. So there are some tradeoffs.", "Luong also gives not one but 4 different ways to calculate context (hope you are keeping track of the number of flavours of attention) All four are minor variations of each other and similar to the approaches we have discussed earlier. In fact, I was able to relate to these approaches better. Most involve taking a dot product of hj (all encoder hidden states) with the decoder hidden state (s), hence these are usually called multiplicative attention. There are several other simplifications as well. They don\u2019t have separate weights for encoder hidden states and decoder hidden state. Luong just adds them up and uses one weight to calculate the context. Luong also uses the hidden state of current time-step directly from the decoder and does not calculate it using a separate function based on <context, decoder hidden state t-1 and last translated word> like how Bahdanau does. So there is one function less. They also don't see why encoder needs to be uni-directional. They simply take the bi-directional RNN output from both encoder and decoder. They also suggest using only the topmost LSTM layer outputs and not worry about the others. These simplifications by Luong show that it is the concept of \u2018Attention\u2019 itself that matters and not the other adornments. But they made one other change which proved to generate excellent results and must be used when we write any Attention model.", "The context generated by them is concatenated with the decoder hidden state at time t and then a new entity called attentional vector is calculated and this is percolated to the next step\u2026. the idea is to inform the model about past alignment decisions. Intuitively, this strikes me as a very good idea irrespective of the Attention approach you are using and this is what is the crux. Based on the problem context, it becomes necessary to experiment with hybrid models of Attention inspired by various papers and this is the reason why we need to understand Attention in all its flavours!", "Of course, you could always choose to implement the pre-written versions of Attention layers that come with the latest version of most frameworks\u2026 it then boils down to 1 line of code (well at least this is the way one would expect it to work):", "But there is a catch here. I was wondering where to specify the \u2019n\u2019 units needed for the alignment layer in Bahdanau and tried digging thru\u2019 the Keras source code for the above class. The first line I could see was: This class is suitable for Dense or CNN networks, and not for RNN networks. You would have imagined this type of disclaimer would have been given here but unfortunately, that is not the case. So use it cautiously. This code also uses the post 2018-Attention terminology and can be confusing to someone trying to map the Bahdanau or Luong papers directly to the above code.", "In general, there are many hybrid versions of Attention on the web, claiming to implement Bahdanau or Luong. Most of these work because at the end of the day, there are only a couple of things that matter \u2014 the direct connection between the target and source (basic Attention in any form) and passing the Attention vector along to the next time step at the decoder (Luong). Pay attention to the scoring function as well. Surprisingly, the remaining nuts and bolts of the implementation don\u2019t matter at all. One reliable guide to Attention that I found I could blindly trust is here.", "Well, we are pretty much done now with all the key essences, but there is one last thing\u2026", "If you have arrived at this page from Google, then you may have typed a query in the search box, the Google search engine would have mapped your query against a set of keys indexing all the available literature on the Internet, then present you the best-matched values (web-pages) and hopefully, this page-link would have been somewhere at the top. This is the query-key-value semantic that you may associate with easily.", "Now, let us turn our attention back to \u2018Attention\u2019. There is another interesting way of looking at this whole \u2018Attention\u2019 business. Attention is nothing but simply giving the model access to a memory. In our case, memory is nothing but all the hidden states of the encoder. The model chooses what to retrieve from this memory. All this is not very different from traditional memory systems\u2026 the databases you may have used in your software programs. A major difference is that the memory access mechanism used here is \u2018soft\u2019. This basically means that the network retrieves a weighted combination of all memory locations when you query something, not a value from a single discrete location unlike your traditional databases. We can say that there is no single key to open the lock (to retrieve information). There are many keys and the lock is opened only when the relevant bunch of keys are used correctly. Looking back at the Attention models we have studied, we can deduce that query was the last hidden state of the decoder, the keys are the various hidden states of the encoder and the \u2018attention adjusted output\u2019 is the final response. In order to respond to a query, the model must grab a key from the memory. However as discussed, there is no perfect key here. Nonetheless, some keys help unlock the output better than the others. The model knows that the product between \u2018query\u2019 and \u2018keys\u2019 give the \u2018match\u2019 probability (remember when similar vectors are multiplied, we get higher scores). The sum of \u2018value\u2019 vectors weighted by this probability gives the final response.", "End-to-End Memory Networks by Sukhbaatar et al. is an important paper in which the model uses such kind of a soft memory, making multiple hops and is trained end to end, over multiple sentences. Multiple hops mean that the network reads the input sequence multiple times before producing an output, and updates the memory contents at each step.", "How else can \u2018Attention\u2019 be leveraged? Is there a benefit in \u2018attending\u2019 to the input sentence itself to create a more efficient memory database? This is an interesting question because we are not even talking of the decoder or the output sentence at this juncture. What we are doing here is to carefully pay attention to all parts of the input sentence itself, trying to capture how the words in the sentence best relate to one another. Maybe \u2018Attention\u2019 helps capture different possible nuances, meanings and relationships of each word in the input sentence with every other word in the input sentence in a better way and gives a better representation for each word as compared to LSTMs? This sort of self-introspection benefits humans and models alike and is called self-attention and if this step precedes all the rest of the decoder business, immense benefits can be seen. Cheng et al probably came out with the first version of self-attention saying \u201cIn our model, memory and attention are added within a sequence encoder allowing the network to uncover lexical relations between tokens\u201d here. Interestingly enough they use the term intra-attention, not self-attention.", "By the way, all this key-quey-value stuff is the same ol\u2019 Attention we have been discussing all along. But this different view-point and new terminology will serve us better here-onwards, as we move on from our pitstop at Attention towards our next destination \u2014 BERT \u2014 the queen of all Embeddings.", "When text-processing was reaping such immense benefits from \u2018Attention\u2019, would image processing remain behind? Definitely not and so we had a series of papers showing the relevance of \u2018Attention\u2019 in image processing as well. For e.g. this paper \u2014 Show, Attend, Tell by Xu et al. inspired by Bahdanau\u2019s original paper, applies \u2018attention\u2019 to image captioning. Instead of using the whole image embedding(equivalent to the final LSTM hidden state in the NLP world) to come out with a caption using an RNN, they use the features obtained from the last but one layer of the CNN that has processed the image. These features represent various objects in the image (equivalent to all the encoder hidden states in NLP world). Now we apply attention using the last hidden state of the RNN generating the caption and \u2018attend\u2019 to all the above features, find out which one needs to be given more weightage and then sum them up to generate a context. This context now contains data on which all features (objects) in the image to pay attention to when generating the next word. This context along with the last generated word and the prev RNN hidden state is used to generate the next word of the caption.", "Attention is important even if it occasionally doesn't produce better results because it always has the advantage of making the AI model more Explainable. Having said that, Attention does work well most of the time. In fact, it works so darn well that the RNN piece is now decoupled and the \u2018attention\u2019 piece (which was a decorator when originally conceived) becomes the centre-piece. Bahdanu\u2019s paper in 2014, set in motion a chain of events that was to culminate in 2018 with a landmark moment for NLP\u2026 the likes of which probably come once in a decade. The stage is set in 2018 for the announcement of a revolutionary paper by a young team at Google. The name of the paper is \u2018Attention is all you need\u2019 by Vaswani et al. and its bare essence is as simple as its name. Attention made it possible for the rise of the transformers and it is now possible for a simple device in your pocket to translate the Dalai Lama\u2019s live speech into any language that you want without the need of an interpreter!", "We will peel the layers of these interesting innovations in the follow-up article.", "PS \u2014 A well-meaning friend of mine, who has seen me sit multiple late nights and weekends to write these articles, advised me to keep it short. \u2018People like 10-12 minute stories at max. For every minute you go beyond that, you lose a quarter of your potential readers\u2019, he said. Well, I have gone on for a good 24 additional minutes and assuming 1000 readers to start out with, I have now brought down my readership to a grand total of one. I will await the weary Internet traveller who may someday land on this page, brimming with more questions than what I had when I started out, and hopefully, this article retains her/his \u2018attention\u2019 till the very end.", "I would be more than satisfied with that one view.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Joker in the pack \u2014 Have played many roles with equal (dis?) passion. Current loves: to code, read, play piano, eat, jog \u2014 though not necessarily in that order"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F2201b5e8be9e&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreate-your-own-custom-attention-layer-understand-all-flavours-2201b5e8be9e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreate-your-own-custom-attention-layer-understand-all-flavours-2201b5e8be9e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreate-your-own-custom-attention-layer-understand-all-flavours-2201b5e8be9e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreate-your-own-custom-attention-layer-understand-all-flavours-2201b5e8be9e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----2201b5e8be9e--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2201b5e8be9e--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@allohvk?source=post_page-----2201b5e8be9e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@allohvk?source=post_page-----2201b5e8be9e--------------------------------", "anchor_text": "Allohvk"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fef5173b64279&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreate-your-own-custom-attention-layer-understand-all-flavours-2201b5e8be9e&user=Allohvk&userId=ef5173b64279&source=post_page-ef5173b64279----2201b5e8be9e---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2201b5e8be9e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreate-your-own-custom-attention-layer-understand-all-flavours-2201b5e8be9e&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2201b5e8be9e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreate-your-own-custom-attention-layer-understand-all-flavours-2201b5e8be9e&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.kaggle.com/c/stanford-covid-vaccine/overview", "anchor_text": "link"}, {"url": "https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/", "anchor_text": "here"}, {"url": "https://www.youtube.com/watch?v=WCUNPb-5EYI", "anchor_text": "Here"}, {"url": "https://www.youtube.com/watch?v=4-QoMdSqG_I", "anchor_text": "This"}, {"url": "https://www.youtube.com/watch?v=aircAruvnKk", "anchor_text": "here"}, {"url": "https://arxiv.org/pdf/1512.08756.pdf", "anchor_text": "here"}, {"url": "https://www.kaggle.com/qqgeogor/keras-lstm-attention-glove840b-lb-0-043", "anchor_text": "Toxic comment classification challenge"}, {"url": "https://www.kaggle.com/sermakarevich/hierarchical-attention-network", "anchor_text": "here"}, {"url": "https://www.cs.cmu.edu/~./hovy/papers/16HLT-hierarchical-attention-networks.pdf", "anchor_text": "here"}, {"url": "https://arxiv.org/pdf/1409.0473.pdf", "anchor_text": "here"}, {"url": "https://arxiv.org/pdf/1312.6026.pdf", "anchor_text": "this"}, {"url": "https://arxiv.org/pdf/1508.04025.pdf", "anchor_text": "https://arxiv.org/pdf/1508.04025.pdf"}, {"url": "https://www.tensorflow.org/api_docs/python/tf/keras/layers/AdditiveAttention", "anchor_text": "here"}, {"url": "https://github.com/tensorflow/nmt", "anchor_text": "here"}, {"url": "https://arxiv.org/abs/1503.08895", "anchor_text": "End-to-End Memory Networks"}, {"url": "https://arxiv.org/pdf/1601.06733.pdf", "anchor_text": "here"}, {"url": "https://arxiv.org/pdf/1502.03044.pdf", "anchor_text": "this"}, {"url": "https://medium.com/tag/attention-layer?source=post_page-----2201b5e8be9e---------------attention_layer-----------------", "anchor_text": "Attention Layer"}, {"url": "https://medium.com/tag/additiveattention?source=post_page-----2201b5e8be9e---------------additiveattention-----------------", "anchor_text": "Additiveattention"}, {"url": "https://medium.com/tag/luong?source=post_page-----2201b5e8be9e---------------luong-----------------", "anchor_text": "Luong"}, {"url": "https://medium.com/tag/neural-machine-translator?source=post_page-----2201b5e8be9e---------------neural_machine_translator-----------------", "anchor_text": "Neural Machine Translator"}, {"url": "https://medium.com/tag/sequence-classifier?source=post_page-----2201b5e8be9e---------------sequence_classifier-----------------", "anchor_text": "Sequence Classifier"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2201b5e8be9e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreate-your-own-custom-attention-layer-understand-all-flavours-2201b5e8be9e&user=Allohvk&userId=ef5173b64279&source=-----2201b5e8be9e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2201b5e8be9e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreate-your-own-custom-attention-layer-understand-all-flavours-2201b5e8be9e&user=Allohvk&userId=ef5173b64279&source=-----2201b5e8be9e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2201b5e8be9e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreate-your-own-custom-attention-layer-understand-all-flavours-2201b5e8be9e&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2201b5e8be9e--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F2201b5e8be9e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreate-your-own-custom-attention-layer-understand-all-flavours-2201b5e8be9e&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----2201b5e8be9e---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----2201b5e8be9e--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----2201b5e8be9e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----2201b5e8be9e--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----2201b5e8be9e--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----2201b5e8be9e--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----2201b5e8be9e--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----2201b5e8be9e--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----2201b5e8be9e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@allohvk?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@allohvk?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Allohvk"}, {"url": "https://medium.com/@allohvk/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "96 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fef5173b64279&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreate-your-own-custom-attention-layer-understand-all-flavours-2201b5e8be9e&user=Allohvk&userId=ef5173b64279&source=post_page-ef5173b64279--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F2b553ccff02f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreate-your-own-custom-attention-layer-understand-all-flavours-2201b5e8be9e&newsletterV3=ef5173b64279&newsletterV3Id=2b553ccff02f&user=Allohvk&userId=ef5173b64279&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}