{"url": "https://towardsdatascience.com/neural-networks-overview-c3e6ab3e366b", "time": 1683006573.2111108, "path": "towardsdatascience.com/neural-networks-overview-c3e6ab3e366b/", "webpage": {"metadata": {"title": "Neural Networks Overview. Math, Code, Drawings, Plots, Analogies\u2026 | by Mehdi Amine | Towards Data Science", "h1": "Neural Networks Overview", "description": "My intent is to walk with you through the main concepts of Neural Networks using analogies, math, code, plots, drawings, and mind maps. We focus on the building block of Neural Networks: Perceptrons\u2026"}, "outgoing_paragraph_urls": [{"url": "https://gist.github.com/Mehdi-Amine/61ec3ae19e7df511eeabc7b2235345a6", "anchor_text": "a full implementation of a Perceptron", "paragraph_index": 23}, {"url": "https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/", "anchor_text": "Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow", "paragraph_index": 40}, {"url": "http://neuralnetworksanddeeplearning.com/index.html", "anchor_text": "Neural Networks and Deep Learning", "paragraph_index": 41}, {"url": "https://github.com/Mehdi-Amine/neural-networks-overview-of-my-understanding/blob/gh-pages/perceptron-overview.ipynb", "anchor_text": "Jupyter notebook and playground", "paragraph_index": 42}, {"url": "https://gist.github.com/Mehdi-Amine", "anchor_text": "Code snippets", "paragraph_index": 43}, {"url": "https://www.linkedin.com/in/mehdi-amine", "anchor_text": "https://www.linkedin.com/in/mehdi-amine", "paragraph_index": 45}], "all_paragraphs": ["My intent is to walk with you through the main concepts of Neural Networks using analogies, math, code, plots, drawings, and mind maps. We focus on the building block of Neural Networks: Perceptrons.", "Throughout the article, we will confront the intimidating math and implement it using Python code with Numpy. We will also look at the equivalent implementation using Scikit-learn. Our results will be visualized using Matplotlib and Plotly. And at the end of each concept, we will structure our understanding using mind maps.", "The input layer of a perceptron is a placeholder. It contains as many nodes as there are features in the training dataset. Each of these nodes is connected to the output node by an edge. We attribute weights to the edges and a bias to the output node.", "A good analogy is to think of a perceptron as a squid. It has an input layer with many arms. The number of arms is equal to the number of input it needs to feed from. In this analogy let\u2019s think of our dataset containing three types of ingredients: salty, sour, and spicy. Our squid needs three arms to grab one ingredient from each type. The arms are connected to the head, which is the output node where the squid mixes the ingredients and gives a score for how good they taste.", "Having lived all its life in the sea, the squid can hardly notice the salty ingredients, so they don\u2019t impact the overall taste. Towards sourness and spiciness however, it can be a real snob. The weights in a perceptron can be understood as representing how much our types of ingredients contribute towards the final taste. The bias can be understood as a factor that influences the squid\u2019s palate, like its mood or appetite.", "The input is multiplied by the corresponding weights, then summed together with the bias. This mixing of the ingredients with their respective weights and the addition with the bias is an affine function: z=\ud835\udc64x+\ud835\udc4f", "After the mixing, the squid outputs a score for its impression on the taste. This score is referred to as an activation and calculated using an activation function. The activation could simply be the result z as it is, in this case, we can use the identity function. It could be a number between -1 and 1, in this case, we can use the hyperbolic tangent function. It could also be a number between 0 and 1, in this case, we can use the sigmoid function. Or a number between 0 and \u221e, in this case, we can use the rectifier linear unit (ReLU) function. Finally, Squid may also be asked to give multiple scores for the same input, each score between 0 and 1 based on different criteria. In this final case, we may be interested in making all the scores add up to 1, the softmax function would be ideal for this task.", "The choice is made depending on the task and the interval of output that serves you best. An example calculating the sigmoid activation \ud835\udc4e\u2032 from the input vector \ud835\udc4e with the weights \ud835\udc64 and bias \ud835\udc4f:", "Note on the activation functions: since affine functions are linear, they are unable to represent nonlinear datasets. Neural Networks are considered universal function approximators thanks to the nonlinearity introduced by the activation functions.", "We are unsatisfied with the output of our friend Squid. It seems that the parameters on which it is operating are random. Sure enough, the bias and weights have been initialized as such:", "We would like to train Squid into acquiring better taste. Our standard for accurate taste is a vector y containing the actual score for each row in our ingredients dataset. The evaluation of the performance of Squid will be with respect to the scores in y.", "The purpose of evaluating the performance of Squid is to measure its error with respect to the targets y. There are different functions to calculate this error:", "To evaluate our perceptron we will use the mean squared error function:", "Training is about minimizing the error \ud835\udc36 by tweaking the parameters w and b. This is best visualized with the analogy of being in a mountain trying to descend back home while it is too dark to see. Home down below represents the error \ud835\udc36 at its minimum. Calculating the square root of the MSE gives you the distance of the straight line between you and home. Knowing this distance, however, is of no help to you in the dark. What you want to know instead is the direction to take for your next step.", "Directions in a 3D world account for three coordinates x, y, and z. Hence the question \u201cwhere is home?\u201d has to be answered with respect to x, y, and z. Similarly, the question \u201cwhere is the minimum error \ud835\udc36?\u201d has to be answered with respect to the parameters w and b. The mathematical representation of these directions is the gradient of \ud835\udc36. More specifically, the negative gradient of \ud835\udc36: -\u2207\ud835\udc36.", "\u2207\ud835\udc36 is a vector containing all the partial derivatives of C with respect to each parameter. For MSE we start by deriving Equation 3:", "We have merely derived MSE, we have some more work to do before we get the partial derivatives of \ud835\udc36 with respect to the parameters. In equation 4, \ud835\udc4e depends on the output of the activation function. In the case of the sigmoid activation, \ud835\udc4e is equivalent to \ud835\udc4e\u2032 in equation 1. Next comes the gradient of \ud835\udc36 with respect to z (Recall that z=\ud835\udc64x+\ud835\udc4f):", "We have the gradient of \ud835\udc36 with respect to z. Since the bias in z is multiplied by 1, the partial derivatives of C with respect to the bias is:", "And since the weights are multiplied by the input x, the partial derivatives of C with respect to the weights is:", "With the partial derivatives of the cost \ud835\udc36 with respect to the parameters, we can now have the direction to take for the next step towards home. Now we need to know how wide should we make the step. Choosing a good step size is important. If your step is too narrow, you won\u2019t be able to jump over obstacles in your way. If your step is too wide, you could overshoot your whole town down below and end up in another mountain. A good step size is somewhere in between and can be calculated by multiplying the partial derivatives (equations 6 and 7) with a chosen value called the learning rate or eta: \ud835\udf02.", "Now we can step down the slope of the mountain. This is equivalent to updating our coordinates/parameters:", "This concludes Gradient Descent: the process of calculating the direction and size of the next step before updating the parameters. With Gradient Descent we can train Squid to acquire better taste. We do this by making Squid feed on some input and output a score using equation 1: this is referred to as Feedforward. The score is plugged as \ud835\udc4e into equation 4, the result of which is plugged as the gradient of \ud835\udc36 with respect to \ud835\udc4e into equation 5. We then compute the gradient of \ud835\udc36 with respect to z in equation 6. Finally, we compute the gradient of \ud835\udc36 with respect to the parameters and we update the initially random parameters of Squid. This process is referred to as Back-propagation as it propagates the error backwards from the output layer to the input layer.", "Gradient Descent is iterative. It stops when one of these conditions is met:", "A full implementation of a Perceptron can be built from the pieces of code we have looked at. For the sake of not submerging this article with code, here is a link to a full implementation of a Perceptron.", "To see our Perceptron at work, let\u2019s make a very simple dataset. We will randomly generate two columns with a hundred rows of integers. Then we will make a third column to store our labels. The labels will be equal to the first column added to half the values in the second column.", "We know that in order to reach the targets, our perceptron will have to start with random parameters and optimize them to have a bias equal to 0, the first weight equal to 1, and the second weight equal to 0.5. Let\u2019s put it to the test:", "We started from the most basic perceptron. As it is performing regression, it does not need an activation function. All it does so far is stochastic gradient descent. In Scikit-learn this can be achieved using the SGDRegressor class. While Scikit-learn includes a Perceptron class, it does not serve our current purpose as it is a classifier and not a regressor.", "We can plot the steps taken by our Perceptron to see the path it took to reach the ideal parameters. Here is the code to plot our Gradient Descent using Matplotlib:", "Here is the same plot from a different angle and using Plotly this time:", "I wanted you to see that our Perceptron\u2019s descent led it home. But it was not the straightest of paths, far from that. We introduce improvements with feature scaling.", "It is generally the case that Machine Learning algorithms perform better with scaled numerical input. Without scaling, Gradient Descent takes longer to converge. In a 2D world where you are still trying to descend from a mountain in the dark to reach home, you need to reduce the vertical and horizontal distances separating you from home. If the two distances are on different ranges, you will spend more time reducing the distance with the larger range.", "For example, if the vertical distance you need to reduce is in the thousands while the horizontal distance is in the ones, your descent will mainly be about climbing down. By the time you get close to the minimum horizontal distance, you will still need to reduce the vertical one.", "Scaling the two distances to have equal ranges makes your steps affect both distances simultaneously, which enables you to travel in a straight path directly towards home.", "The two most common ways to scale data are Normalization and Standardization. We are going to implement both of them and visualize their effect on Gradient Descent.", "Also referred to as min-max scaling, is a method that constricts data to be between 0 and 1:", "Also referred to as z-score normalization, is a method that centers the data around 0 with a standard deviation equal to 1. \ud835\udf07 is the mean, \ud835\udf0e is the standard deviation:", "To investigate the effect of feature scaling, we are going to train two more Perceptrons. The purpose is to compare the convergence of the parameters in Gradient Descent with and without scaling.", "Our first Perceptron was training on an unscaled dataset. The second one will be trained on normalized data. And the third one will be trained on standardized data.", "We can now visualize the paths taken by our three Perceptrons. The code below uses Plotly:", "The Perceptrons trained on scaled data have taken more direct paths to converge. Direct paths enabled their descents to be faster, with wider steps (possible by increasing the learning rate eta) and a lower number of steps (possible by decreasing the iterations epochs).", "A. G\u00e9ron, Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow (2019)", "M. Nielsen, Neural Networks and Deep Learning (2019)", "M. Amine, Neural Networks Overview: Jupyter notebook and playground", "M. Amine, Neural Networks Overview: Code snippets", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Writer. AI Researcher. MSc Advanced Computer Science Intelligent Systems. BSc Computer Science and Mathematics. https://www.linkedin.com/in/mehdi-amine"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fc3e6ab3e366b&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-overview-c3e6ab3e366b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-overview-c3e6ab3e366b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-overview-c3e6ab3e366b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-overview-c3e6ab3e366b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----c3e6ab3e366b--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c3e6ab3e366b--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@m.e.mehdi.amine?source=post_page-----c3e6ab3e366b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@m.e.mehdi.amine?source=post_page-----c3e6ab3e366b--------------------------------", "anchor_text": "Mehdi Amine"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff6182caa5a35&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-overview-c3e6ab3e366b&user=Mehdi+Amine&userId=f6182caa5a35&source=post_page-f6182caa5a35----c3e6ab3e366b---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc3e6ab3e366b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-overview-c3e6ab3e366b&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc3e6ab3e366b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-overview-c3e6ab3e366b&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://en.wikipedia.org/wiki/De_gustibus_non_est_disputandum", "anchor_text": "De gustibus non est disputandum"}, {"url": "https://gist.github.com/Mehdi-Amine/61ec3ae19e7df511eeabc7b2235345a6", "anchor_text": "a full implementation of a Perceptron"}, {"url": "https://gist.github.com/Mehdi-Amine/41e04f8faa8808ac655640f7c3ea55e2", "anchor_text": "Generating our dataset"}, {"url": "https://gist.github.com/Mehdi-Amine/7032ccfd615eca0e4752e572a4db852c", "anchor_text": "Great! Our perceptron has successfully optimized all the parameters."}, {"url": "https://gist.github.com/Mehdi-Amine/4c74f9f7baa7b053dc9a97d498fd4088", "anchor_text": "Training before inspecting the optimized parameters of an SGDRegressor"}, {"url": "https://gist.github.com/Mehdi-Amine/d0889b465d4a32b8e4f474fc74b10ad7", "anchor_text": "Gradient Descent visualized using Matplotlib"}, {"url": "https://gist.github.com/Mehdi-Amine/905c695c17af1c44752a305a99b5dfac", "anchor_text": "Gradient Descent visualized using Plotly"}, {"url": "https://gist.github.com/Mehdi-Amine/dc78714366a3aed1d1f9204e0b08e80a", "anchor_text": "Normalizing our training dataset using Numpy"}, {"url": "https://gist.github.com/Mehdi-Amine/4cc3780057f37b63b4218863a514241c", "anchor_text": "Normalizing our training dataset using Scikit-learn"}, {"url": "https://gist.github.com/Mehdi-Amine/dee046527cb8a9ae117b37e5cbdd5478", "anchor_text": "Standardizing our training dataset using Numpy"}, {"url": "https://gist.github.com/Mehdi-Amine/f19234372b93f1aa486bd79aa2f68eb0", "anchor_text": "Standardizing our training dataset using Scikit-learn"}, {"url": "https://gist.github.com/Mehdi-Amine/f1577cfda558b38934738488c321f091", "anchor_text": "Training two perceptrons with normalized and standardized data"}, {"url": "https://gist.github.com/Mehdi-Amine/6ef489a0afda130c13cc51cff45aa185", "anchor_text": "Visualizing the three Gradient Descents"}, {"url": "https://mehdi-amine.github.io/sgd-plot/sgdplot.html", "anchor_text": "Output of the previous code: Click here to interact with the figure"}, {"url": "https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/", "anchor_text": "Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow"}, {"url": "http://www.deeplearningbook.org/", "anchor_text": "Deep Learning"}, {"url": "http://neuralnetworksanddeeplearning.com/index.html", "anchor_text": "Neural Networks and Deep Learning"}, {"url": "https://github.com/Mehdi-Amine/neural-networks-overview-of-my-understanding/blob/gh-pages/perceptron-overview.ipynb", "anchor_text": "Jupyter notebook and playground"}, {"url": "https://gist.github.com/Mehdi-Amine", "anchor_text": "Code snippets"}, {"url": "https://www.xmind.net/", "anchor_text": "Mind Mapping Software"}, {"url": "https://medium.com/tag/data-science?source=post_page-----c3e6ab3e366b---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----c3e6ab3e366b---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----c3e6ab3e366b---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/python?source=post_page-----c3e6ab3e366b---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----c3e6ab3e366b---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc3e6ab3e366b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-overview-c3e6ab3e366b&user=Mehdi+Amine&userId=f6182caa5a35&source=-----c3e6ab3e366b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc3e6ab3e366b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-overview-c3e6ab3e366b&user=Mehdi+Amine&userId=f6182caa5a35&source=-----c3e6ab3e366b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc3e6ab3e366b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-overview-c3e6ab3e366b&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c3e6ab3e366b--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fc3e6ab3e366b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-overview-c3e6ab3e366b&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----c3e6ab3e366b---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----c3e6ab3e366b--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----c3e6ab3e366b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----c3e6ab3e366b--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----c3e6ab3e366b--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----c3e6ab3e366b--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----c3e6ab3e366b--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----c3e6ab3e366b--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----c3e6ab3e366b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@m.e.mehdi.amine?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@m.e.mehdi.amine?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Mehdi Amine"}, {"url": "https://medium.com/@m.e.mehdi.amine/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "70 Followers"}, {"url": "https://www.linkedin.com/in/mehdi-amine", "anchor_text": "https://www.linkedin.com/in/mehdi-amine"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff6182caa5a35&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-overview-c3e6ab3e366b&user=Mehdi+Amine&userId=f6182caa5a35&source=post_page-f6182caa5a35--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F32a2916fde12&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-overview-c3e6ab3e366b&newsletterV3=f6182caa5a35&newsletterV3Id=32a2916fde12&user=Mehdi+Amine&userId=f6182caa5a35&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}