{"url": "https://towardsdatascience.com/policy-gradient-in-multi-task-meta-learning-2aeeaf23817d", "time": 1683007521.168642, "path": "towardsdatascience.com/policy-gradient-in-multi-task-meta-learning-2aeeaf23817d/", "webpage": {"metadata": {"title": "Policy gradient in multi-task/meta-learning | by Qiurui Chen | Towards Data Science", "h1": "Policy gradient in multi-task/meta-learning", "description": "When do you not need sequential decision making? when your system is making a single isolated decision (such as classification, regression), and when that decision does not affect future inputs or\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.youtube.com/watch?v=UPT4Rndftc8&list=PLoROMvodv4rMC6zfYmnD7UG3LVvwaITY5&index=6", "anchor_text": "Stanford CS330: Multi-Task and Meta-Learning, 2019 | Lecture 6 \u2014 Reinforcement Learning Primer\u2019", "paragraph_index": 1}, {"url": "https://images.nvidia.com/content/tegra/automotive/images/2016/solutions/pdf/end-to-end-dl-using-px.pdf", "anchor_text": "Maybe you could collect a bunch of driving data", "paragraph_index": 5}], "all_paragraphs": ["When do you not need sequential decision making? when your system is making a single isolated decision (such as classification, regression), and when that decision does not affect future inputs or decision, you do not need sequential decision making. The common application areas include robotics, language and dialog, autonomous driving, business operations and finance.", "The second part of this story will cover Q-learning and Multi-task Q-learning. This story is a short summary of the course \u2018Stanford CS330: Multi-Task and Meta-Learning, 2019 | Lecture 6 \u2014 Reinforcement Learning Primer\u2019.", "In supervised learning, data is iid and we need a large labeled, curated dataset, while in sequential decision making, the action affects the next state instead of iid, and for the dataset, how to collect data? what are the labels?", "For supervised learning, we have a function \u03c0 which is parameterized by \u03b8 that takes o in and generates output a. For example, the input could be imaged while the output could be the image class (s.t. tiger). In RL, we use policy that will be taking actions and the actions will affect the next state. These will be this feedback loop that goes from the action back to the observations, and our class (a) will be the actions, such as \u201crunaway\u201d,\u201d ignore\u201d or \u201cpet the tiger\u201d. O denotes the observations that the agent (system) receives as input, a denotes as action, \u03c0 is denoting the policy which is parameterized by \u03b8. Typically we assume that there is some underlying state of the world s. in the fully observed situation, s is observed. in the partially observed setting, o is observed.", "What\u2019s the concrete difference between o and s? for example, you try to chase a hyena, if you are given an image, then the images would be an observation o, whereas in contrast if you are given the pose of the respective animals, then these poses would state s. You basically be able to fully observe the system underlying state of the system and the things that matter for making decisions in the world.", "One basic approach to the sequential decision-making problem is to treat it as supervised learning(SL) problem. For example, you want to imitate some experts. Maybe you could collect a bunch of driving data, collect the observation that the person sees and collect the action that they took in those states, put this into some big training dataset, and then sample iid from this training datasets during supervised learning to train your policy to predict actions from observations. This approach works well in some context if you have a lot of expert data on performing the right actions. But these systems don\u2019t reason about outcomes in any way instead of just mimicking what the data is doing.", "In RL, reward functions are needed. These reward function should capture what states and actions are better or worse for the system. It typically takes in both a state and an action( r(s,a) ), and tells us which states and actions are better. For example, if we are driving, we might have a very high reward if we drive smoothly and we will have a low reward is we have a car crash.", "In aggregate the states s, the actions a, and the rewards r(s,a), as well as the dynamics of the system p(s'|s,a) define a Markov decision process, because this is encapsulating the notion of a sequential decision-making problem.", "So the goal of reinforcement learning is typically to learn parameters of the policies that take as inputs. In this case, we will look at the fully observed setting takes as input some state, and make predictions about actions. The goal is to learn the parameters of the policy. In a deep RL setting, your policy will probably be parameterized as a neural network where the states are being processed as input, actions are outputs. The actions are fed into the world and then the world gives you the next state that\u2019s feeding back into your policy.", "We can actually characterize a system as the graphical model here where we have a policy that takes the observation and produce an action (partially observed setting), the dynamics take in the current state, and the current action and produce a distribution over the next state. The dynamic function is independent of the previous state, and this is known as the Markov property. Basically the definition of a state in a Markov decision process is that you can fully define the reward function and the dynamics from the information in that state variable independent of previous states. If you look at the diagrams here, it only depends on St and At and doesn\u2019t depend on st-1.", "The concrete objective for RL is to maximize the expected reward under the policy. And in the infinite horizon case, we want to maximize the reward function under that stationary distribution the stationary distribution over states and actions arising from the policy. In the finite horizon case, we might have some horizon capital T and you want to maximize the rewards of the states and actions when rolling out our policy.", "We\u2019ve talked RL problem, then what is the RL tasks? If you compare the RL to the supervised learning setting, the initial state distribution and the dynamics basically are the same as the data generating distributions. The reward function corresponds to the loss function, and the state and action space are just kind of telling you what is the general set that your states and actions lie within. So this is just as a Markov decision process. But if the different MDPs are different tasks, then this is much more than just the semantic meaning of a task. Because different tasks could have the same exact reward function but have different action spaces or different dynamics. So we use the term task loosely to describe these different Markov decision processes.", "How to apply RL in meta-learning? One application is personalized recommendations, where different people are different tasks. The dynamics correspond to how that person will react to a particular action that you take and the reward function corresponds to whether or not you recommend something to the results in a state that is good. In some contexts, the initial state distribution may also vary for different people, it depends on how you formulate your problem. Another application is across maneuvers, which is animating different characters in computer graphics across different maneuvers, for example. If you treat this as a multitask learning problem, different tasks would have different reward functions in the setting but the dynamics would be the same, as well as the state and action space.", "One alternative way to view multitask RL is as follows. We typically have some sort of task identifier that\u2019s part of the state and this is required to make it a fully observable setting or a fully observable MDP. S bar denotes as original state, and zi denotes the task identifier. Basically tasks viewed as a standard Markov decision process, where the state space and the action space are the unions of the state spaces and action spaces in the original tasks. The initial state distribution just corresponds to a mixture distribution over the initial state distribution for each of those tasks, the dynamics and the reward function are a single dynamics and single reward function that takes as input the task identifier and produces either the next state or the reward. So you can basically apply standard single task RL algorithms to the multitask problem with this view on multitasking RL.", "Multi-task RL is the same as the single task RL problem, except a task identifier is part of the sate ( s = (s bar, zi)). For example, a task identifier could be a one-hot task ID, a language description of the task, or the desired goal state that you want to reach. This goal state would be known as goal-conditioned RL where you conditioned it on a particular state that you want to be able to reach in the future. The reward function could be the same as before where it takes as input the task id and outputs the reward function corresponds to that task for that state, or for things like goal-conditioned RL, it can correspond to simply the negative distance between your (original) current state and the goal state. Some example of distance function might be Euclidean distance in some latent space or a sparse 0/1 where 1 indicates s bar equals sg and 0 indicate not equal. If it\u2019s still a standard Markov decision process, then why not apply standard RL algorithms? You can, but it will be more challenging than the individual single tasks because you will have a wider distribution of things in general.", "You can generally review RL algorithms in the following flow graph where we first generate samples in the environment, this is just running the policy forward typically, then we fit some model to estimate the return, then we use that model to improve the policy. Different algorithms typically correspond to just differences in this green box and in this blue box. So for example, one example of fitting a model might be estimating the empirical return such as using the Monte-Carlo policy gradient. Another example of estimating the return might be to try to fit a Q function, using, for example, dynamic programming algorithms. Another example of fitting a model would be to estimate a function that models the dynamics. Once we have any of these models we can then, for example, apply the policy gradient to our policy parameters, we can improve the policy by taking the max over Q values for our current Q-function, or in the case of model-based algorithms, we can optimize a policy by for example backpropagating through a model into our policy.", "So let\u2019s start with policy gradients. Above is our objective in RL, so we want to be able to sample trajectories from our policy and estimate the return. We will refer to this objective as J of \u03b8. We could rewrite J of \u03b8 . You can view this or estimate this as rolling out and trajectories, for example, shown in the graph right, which is computing the reward for each of those trajectories, and maybe the first trajectory has a high reward, the middle trajectory has a medium reward and the last trajectory has a bad reward. And so this first sum is the sum over the samples from our policy and the second sum is a sum over time.", "Can we differentiate through this objective directly into our policy? So if our objective is the expected reward and we can estimate this with the reward of a trajectory. You can view this as this expectation as an integral over \u03c0 \u03b8 because the expectation is with respect to \u03c0 \u03b8 of \u03c4. If you want to compute the gradient of this objective with respect to our policy parameters, you can move the gradient inside the integral because it\u2019s a linear operation and then you basically have the integral of the gradient of the policy times reward function integrated over trajectories. How do we actually go about evaluating this gradient? We don\u2019t have to integrate over all possible trajectories. So we are going to use the likelihood ratio trick. If we are looking at the policy probability for a trajectory time the gradient of the log of the policy, this is basically we just differentiate to the log which is equal to the policy times the gradient of \u03c0 divided by \u03c0. The two \u03c0s are canceled and it equals the gradient of \u03c0 or the gradient of the policy w.r.t the policy parameters. After applying the convenient identity, we can simply evaluate the gradient or estimate the gradient by taking an expectation over a trajectory sampled from our policy and using those samples to evaluate the gradients of the log probability of our policy weighted by the reward of that trajectory. The \u03c0 of the full trajectory which can be broken down into the initial state density times a product over time of the policy probability and the dynamics probability. After calculation, the final gradient is something we can evaluate.", "We can basically rule out our policy to get trajectories, then estimate the policy gradient by averaging over those trajectories over time of log \u03c0 times the reward function and then apply the gradient to our policy parameters. If we go back to the diagram, collecting data corresponds to the orange box (corresponds to orange underline part in the right formulas) evaluating the return corresponds to the green box(corresponds to green underline part), and actually using that to improve the policy in the last step corresponds to the blue box(corresponds to blue underline part). The reinforce algorithm is explicitly sampling trajectories from your policy and then computing the gradient, using those trajectories, and then using that estimated gradient to update your policy parameters. And then you can repeat this step to iteratively improve your policy.", "How does this compare to something like imitation learning like the maximum likelihood of expert actions? The maximum likelihood objective looks pretty similar to the policy form. And in particular, the difference is that just the reward term on the right. So basically policy gradient will correspond to maximizing the probability of actions that have a high reward. If they have a low reward, then you will try to maximize it less essentially. Because it is just basically a gradient descent algorithm, it\u2019s very easy to apply to multitask learning algorithms to it.", "If we do maximum likelihood imitation learning, we are just trying to imitate the best trajectories whereas in policy gradient, we have some distribution over these trajectories, and then we are going to try to increase the probability of the actions that had a high reward and place less probability mass on the actions that had a low reward. So as a result, we will basically just be making the good stuff more likely, and making the stuff gets bad reward less likely, and formalizing this notion of \u201ctrial and error\u201d.", "So that\u2019s policy gradient. It\u2019s pretty easy to combine with things like multitask learning. It\u2019s also pretty easy to combine with things like meta-learning. The meta-learning algorithms (such as MAML and black-box approaches) assume that you can get some gradient of your objective. And so we can readily apply these algorithms in combination with policy gradient algorithms.", "For example, let\u2019s combine MAML and policy gradient. There were 2 tasks: running forward and backward. We are not evaluating generalization in any way. We are just gonna look at whether or not it can learn to adapt its policy with a single gradient step for one of these two tasks.", "The above example shows that there exists a representation under which RL is fast and efficient.", "We could also combine the black-box meta-learning and policy gradient. Here is the maze example. This experiment is learning to navigate a maze, it is trained on 1000 small mazes and test on held-out small mazes and large mazes.", "MAML is very expressive in the supervised learning setting, but it\u2019s actually not very expressive in RL because of the policy gradient. Basically, if the reward function is 0 for all of your trajectories, then your gradient will always be 0. And so even if it gets lots of rich experience about the environment with 0 rewards, it can NOT actually incorporate that experience to update the policy. As a result, MAML with policy gradients isn\u2019t actually very expressive. In general, these meta-learning algorithms to the RL setting is pretty easy to combine with policy gradients, combining them with methods like Q-learning and actor-critic algorithms is a lot more challenging.", "The policy gradient approach is simple and easy to combine with existing multitask and meta-learning algorithms. But it produces a high-variance gradient, which can be mitigated with baseline or trust regions. Another downside of the policy gradient is that it requires on-policy data. You can see the expectation in respect to \u03c0\u03b8, and \u03c0\u03b8 is your current policy. So in order to improve your policy, you need data from your current policy. And this is really important because this means you cant reuse any data from your previous policies to try to improve your policy. As a result, these algorithms tend to be less sample efficient than other algorithms that are able to reuse data from previous policies and other experiences, etc. Things like importance weighting can help with this. So you can basically add a weight that corresponds to the ratio between your current policy and the policy that you collected data with. But these importance weights also give you high variance especially when those two policies are very different.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "MSc in Computer Science at UT."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F2aeeaf23817d&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpolicy-gradient-in-multi-task-meta-learning-2aeeaf23817d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpolicy-gradient-in-multi-task-meta-learning-2aeeaf23817d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpolicy-gradient-in-multi-task-meta-learning-2aeeaf23817d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpolicy-gradient-in-multi-task-meta-learning-2aeeaf23817d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----2aeeaf23817d--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2aeeaf23817d--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@rachel_95942?source=post_page-----2aeeaf23817d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@rachel_95942?source=post_page-----2aeeaf23817d--------------------------------", "anchor_text": "Qiurui Chen"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcbd32a3f303d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpolicy-gradient-in-multi-task-meta-learning-2aeeaf23817d&user=Qiurui+Chen&userId=cbd32a3f303d&source=post_page-cbd32a3f303d----2aeeaf23817d---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2aeeaf23817d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpolicy-gradient-in-multi-task-meta-learning-2aeeaf23817d&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2aeeaf23817d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpolicy-gradient-in-multi-task-meta-learning-2aeeaf23817d&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.youtube.com/watch?v=UPT4Rndftc8&list=PLoROMvodv4rMC6zfYmnD7UG3LVvwaITY5&index=6", "anchor_text": "Stanford CS330: Multi-Task and Meta-Learning, 2019 | Lecture 6 \u2014 Reinforcement Learning Primer\u2019"}, {"url": "https://wordart.com/", "anchor_text": "WordArt"}, {"url": "https://www.youtube.com/watch?v=UPT4Rndftc8&list=PLoROMvodv4rMC6zfYmnD7UG3LVvwaITY5&index=6", "anchor_text": "Stanford CS330: Multi-Task and Meta-Learning, 2019 | Lecture 6 \u2014 Reinforcement Learning Primer\u2019"}, {"url": "http://cs330.stanford.edu/slides/cs330_mtrl.pdf", "anchor_text": "the course slide"}, {"url": "http://cs330.stanford.edu/slides/cs330_mtrl.pdf", "anchor_text": "the course slide"}, {"url": "https://images.nvidia.com/content/tegra/automotive/images/2016/solutions/pdf/end-to-end-dl-using-px.pdf", "anchor_text": "Maybe you could collect a bunch of driving data"}, {"url": "http://cs330.stanford.edu/slides/cs330_mtrl.pdf", "anchor_text": "the course slide"}, {"url": "http://cs330.stanford.edu/slides/cs330_mtrl.pdf", "anchor_text": "the course slide"}, {"url": "http://cs330.stanford.edu/slides/cs330_mtrl.pdf", "anchor_text": "the course slide"}, {"url": "http://cs330.stanford.edu/slides/cs330_mtrl.pdf", "anchor_text": "the course slide"}, {"url": "http://cs330.stanford.edu/slides/cs330_mtrl.pdf", "anchor_text": "the course slide"}, {"url": "http://cs330.stanford.edu/slides/cs330_mtrl.pdf", "anchor_text": "the course slide"}, {"url": "http://cs330.stanford.edu/slides/cs330_mtrl.pdf", "anchor_text": "the course slide"}, {"url": "http://cs330.stanford.edu/slides/cs330_mtrl.pdf", "anchor_text": "the course slide"}, {"url": "http://cs330.stanford.edu/slides/cs330_mtrl.pdf", "anchor_text": "the course slide"}, {"url": "http://cs330.stanford.edu/slides/cs330_mtrl.pdf", "anchor_text": "the course slide"}, {"url": "http://cs330.stanford.edu/slides/cs330_mtrl.pdf", "anchor_text": "the course slide"}, {"url": "http://cs330.stanford.edu/slides/cs330_mtrl.pdf", "anchor_text": "the course slide"}, {"url": "http://cs330.stanford.edu/slides/cs330_mtrl.pdf", "anchor_text": "the course slide"}, {"url": "http://cs330.stanford.edu/slides/cs330_mtrl.pdf", "anchor_text": "the course slide"}, {"url": "http://cs330.stanford.edu/slides/cs330_mtrl.pdf", "anchor_text": "the course slide"}, {"url": "https://www.youtube.com/watch?v=UPT4Rndftc8&list=PLoROMvodv4rMC6zfYmnD7UG3LVvwaITY5&index=6", "anchor_text": "Stanford CS330: Multi-Task and Meta-Learning, 2019 | Lecture 6 \u2014 Reinforcement Learning Primer\u2019"}, {"url": "http://cs330.stanford.edu/slides/cs330_mtrl.pdf", "anchor_text": "he course slide"}, {"url": "https://images.nvidia.com/content/tegra/automotive/images/2016/solutions/pdf/end-to-end-dl-using-px.pdf", "anchor_text": "End to End Learning for Self-Driving Cars"}, {"url": "https://arxiv.org/abs/1703.03400", "anchor_text": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks"}, {"url": "https://arxiv.org/abs/1707.03141", "anchor_text": "A Simple Neural Attentive Meta-Learner"}, {"url": "https://medium.com/tag/meta-learning?source=post_page-----2aeeaf23817d---------------meta_learning-----------------", "anchor_text": "Meta Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----2aeeaf23817d---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----2aeeaf23817d---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2aeeaf23817d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpolicy-gradient-in-multi-task-meta-learning-2aeeaf23817d&user=Qiurui+Chen&userId=cbd32a3f303d&source=-----2aeeaf23817d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2aeeaf23817d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpolicy-gradient-in-multi-task-meta-learning-2aeeaf23817d&user=Qiurui+Chen&userId=cbd32a3f303d&source=-----2aeeaf23817d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2aeeaf23817d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpolicy-gradient-in-multi-task-meta-learning-2aeeaf23817d&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2aeeaf23817d--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F2aeeaf23817d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpolicy-gradient-in-multi-task-meta-learning-2aeeaf23817d&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----2aeeaf23817d---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----2aeeaf23817d--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----2aeeaf23817d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----2aeeaf23817d--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----2aeeaf23817d--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----2aeeaf23817d--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----2aeeaf23817d--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----2aeeaf23817d--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----2aeeaf23817d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@rachel_95942?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@rachel_95942?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Qiurui Chen"}, {"url": "https://medium.com/@rachel_95942/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "77 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcbd32a3f303d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpolicy-gradient-in-multi-task-meta-learning-2aeeaf23817d&user=Qiurui+Chen&userId=cbd32a3f303d&source=post_page-cbd32a3f303d--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F5d77130b77c3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpolicy-gradient-in-multi-task-meta-learning-2aeeaf23817d&newsletterV3=cbd32a3f303d&newsletterV3Id=5d77130b77c3&user=Qiurui+Chen&userId=cbd32a3f303d&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}