{"url": "https://towardsdatascience.com/part-2-a-new-tool-to-your-toolkit-kl-divergence-736c134baa3d", "time": 1682996683.327884, "path": "towardsdatascience.com/part-2-a-new-tool-to-your-toolkit-kl-divergence-736c134baa3d/", "webpage": {"metadata": {"title": "A new Tool to your Toolkit, KL Divergence at Work | by Abhishek Mungoli | Towards Data Science", "h1": "A new Tool to your Toolkit, KL Divergence at Work", "description": "In my previous post, we got a thorough understanding of Entropy, Cross-Entropy, and KL-Divergence in an intuitive way and also by calculating their values through examples. In case you missed it\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/part-i-a-new-tool-to-your-toolkit-kl-divergence-5b887b5b420e", "anchor_text": "previous post", "paragraph_index": 0}, {"url": "https://en.wikipedia.org/wiki/Entropy", "anchor_text": "Entropy", "paragraph_index": 0}, {"url": "https://en.wikipedia.org/wiki/Cross_entropy", "anchor_text": "Cross-Entropy", "paragraph_index": 0}, {"url": "https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence", "anchor_text": "KL-Divergence", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/part-i-a-new-tool-to-your-toolkit-kl-divergence-5b887b5b420e", "anchor_text": "go through", "paragraph_index": 0}, {"url": "https://en.wikipedia.org/wiki/Probability_density_function", "anchor_text": "Probability Density Function", "paragraph_index": 4}, {"url": "http://www.aishack.in/tutorials/expectation-maximization-gaussian-mixture-model-mixtures/", "anchor_text": "about it here", "paragraph_index": 6}, {"url": "https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm", "anchor_text": "Expectation-Maximisation", "paragraph_index": 6}, {"url": "https://en.wikipedia.org/wiki/Bayes%27_theorem", "anchor_text": "Bayes theorem", "paragraph_index": 13}, {"url": "https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence", "anchor_text": "KL divergence", "paragraph_index": 16}, {"url": "https://medium.com/@mungoliabhishek81", "anchor_text": "Medium", "paragraph_index": 25}, {"url": "https://github.com/samread81/KL-Divergence", "anchor_text": "here", "paragraph_index": 25}, {"url": "https://www.youtube.com/channel/UCg0PxC9ThQrbD9nM_FU1vWA", "anchor_text": "https://www.youtube.com/channel/UCg0PxC9ThQrbD9nM_FU1vWA", "paragraph_index": 27}], "all_paragraphs": ["In my previous post, we got a thorough understanding of Entropy, Cross-Entropy, and KL-Divergence in an intuitive way and also by calculating their values through examples. In case you missed it, please go through it once before proceeding to the finale.", "In this post, we will apply these concepts and check the results in a real dataset. Also, it will give us good intuition on how to use these concepts in modeling various day-to-day machine learning problems. So, let\u2019s get started.", "The Dataset consists of two latent features \u2018f1\u2019 and \u2018f2\u2019 and the class to which the data-point belongs to, i.e. the positive class or the negative class.", "So, we have data points having two latent features, \u2018f1\u2019 and \u2018f2\u2019. The data points belong to \u2018+\u2019 class (red in color) and \u2018-\u2019 class (blue in color).", "Our purpose is to define an ideal distribution for both positive and negative class of the dataset. As of now, we have no idea what will it look like, how it\u2019s Probability Density Function will be, but we can define some good to have properties for it.", "Now the fun begins. How to estimate that ideal distribution. It\u2019s an open-ended question and many techniques can be tried upon. But for this blog\u2019s post, I will keep things simple and won\u2019t deviate too much from the original topic, applying KL Divergence to day-to-day machine learning problems.", "One way to estimate distribution is by using a Gaussian Distribution. We will try to fit a Gaussian for positive class and another Gaussian for negative class. There are available packages which will find us the appropriate parameters of these fitted Gaussians. But, if you are interested in understanding, how it does that then you can read more about it here. An algorithm called Expectation-Maximisation is used for it. Maybe, I will write about it in some another blog post. Let\u2019s fit the distribution using GaussianMixture package available in python.", "Visually the distribution looks good in doing the task assigned. One Gaussian is fitted to positive class and another to negative class. Next, we will calculate for each data-point, it\u2019s probability to belong to the positive and negative class distributions.", "In this part, we will see, how the final probability for each data point is calculated once the fitting of multivariate Gaussian for the positive and the negative class is done. It will be a little more mathematics intensive and optional. It can be used as black-box, to get the final probability. But in case, you are interested in understanding the mathematics behind, you can follow the section else skip to the next one.", "For any datapoint \u2018x\u2019, probability belonging to distribution is given by", "Using the above formula we can find the likelihood,", "Next, we can find the class probabilities or the priors using,", "where n is the total number of data points.", "Once we have the likelihoods and priors, the last step is to just find the posterior, i.e. probability of datapoint. We can use Bayes theorem to calculate that.", "We can use the above posterior to find the probability of each data point belonging to +ve or -ve distribution.", "Now, once we have fitted the distribution and also calculated the probability of each data point belonging to the positive and negative distribution. We can see how much this fitted distribution differs from our ideal distribution.", "How can we check that? Of course, using our favorite metric, KL divergence (Kullback\u2013Leibler divergence).", "Just to reiterate, KL Divergence is just the difference between a fitted distribution and actual distribution, i.e. the difference between cross-entropy and entropy. It can also be looked as to how much the two distributions differ.", "where H(p,q) is the cross-entropy and H(p) is the entropy of the system, where p\u1d62 is the actual probability of the i-th event and q\u1d62 is the estimated probability of the i-th event.", "p\u1d62 is the actual probability of the event which is coming from the properties of ideal distribution. q\u1d62 is the estimated probability of the event, calculated using the fitted/estimated distribution. We use these probabilities to find the KL Divergence.", "The KL divergence comes out to be 5.74, which denotes the fitted distribution is pretty close to the ideal. But can we do better?", "One Gaussian curve per class may not be enough to mimic the whole distribution. We can fit a mixture of Gaussians and see the results. How many Gaussians? Till our KL divergence approaches to 0, i.e no or minimal difference between ideal and fitted distribution. Let\u2019s try that.", "Four Gaussians per class are enough and closely mimic the ideal distribution with almost 0 KL-Divergence. The below plot also makes that clear.", "We took a proper dataset with two different classes. We wanted to find the underlying distribution for the two classes. So, we first defined what is good to have properties for an ideal distribution and were able to mimic that ideal distribution very closely. In this way, we can always try to find the underlying distribution for data and see the goodness of fit using KL Divergence. Hope it brings the required clarity for the topic and arises new horizons for its application to your day-to-day machine learning works.", "My Youtube channel for more content:", "It takes a lot of effort to write a good post with clarity and easy understandability for the audience. I will keep trying to do justice with my work. Follow me up at Medium and check out my previous posts. I welcome feedback and constructive criticism. The complete code of the assignment can be obtained from here.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Lead Data Scientist at Meesho | Ex-Walmart | IIIT-Hyderabad | NERIST | Insta: simplyspartanx | Youtube: https://www.youtube.com/channel/UCg0PxC9ThQrbD9nM_FU1vWA"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F736c134baa3d&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpart-2-a-new-tool-to-your-toolkit-kl-divergence-736c134baa3d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpart-2-a-new-tool-to-your-toolkit-kl-divergence-736c134baa3d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpart-2-a-new-tool-to-your-toolkit-kl-divergence-736c134baa3d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpart-2-a-new-tool-to-your-toolkit-kl-divergence-736c134baa3d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----736c134baa3d--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----736c134baa3d--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@mungoliabhishek81?source=post_page-----736c134baa3d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@mungoliabhishek81?source=post_page-----736c134baa3d--------------------------------", "anchor_text": "Abhishek Mungoli"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5f15354d23de&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpart-2-a-new-tool-to-your-toolkit-kl-divergence-736c134baa3d&user=Abhishek+Mungoli&userId=5f15354d23de&source=post_page-5f15354d23de----736c134baa3d---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F736c134baa3d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpart-2-a-new-tool-to-your-toolkit-kl-divergence-736c134baa3d&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F736c134baa3d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpart-2-a-new-tool-to-your-toolkit-kl-divergence-736c134baa3d&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/part-i-a-new-tool-to-your-toolkit-kl-divergence-5b887b5b420e", "anchor_text": "previous post"}, {"url": "https://en.wikipedia.org/wiki/Entropy", "anchor_text": "Entropy"}, {"url": "https://en.wikipedia.org/wiki/Cross_entropy", "anchor_text": "Cross-Entropy"}, {"url": "https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence", "anchor_text": "KL-Divergence"}, {"url": "https://towardsdatascience.com/part-i-a-new-tool-to-your-toolkit-kl-divergence-5b887b5b420e", "anchor_text": "go through"}, {"url": "https://en.wikipedia.org/wiki/Probability_density_function", "anchor_text": "Probability Density Function"}, {"url": "http://www.aishack.in/tutorials/expectation-maximization-gaussian-mixture-model-mixtures/", "anchor_text": "about it here"}, {"url": "https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm", "anchor_text": "Expectation-Maximisation"}, {"url": "https://en.wikipedia.org/wiki/Bayes%27_theorem", "anchor_text": "Bayes theorem"}, {"url": "https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence", "anchor_text": "KL divergence"}, {"url": "https://www.youtube.com/channel/UCg0PxC9ThQrbD9nM_FU1vWA", "anchor_text": "Abhishek MungoliHi Guys, Welcome to the channel. The channel aims to cover various topics from Machine Learning, Data Science\u2026www.youtube.com"}, {"url": "https://medium.com/@mungoliabhishek81", "anchor_text": "Medium"}, {"url": "https://github.com/samread81/KL-Divergence", "anchor_text": "here"}, {"url": "https://en.wikipedia.org/wiki/Probability_density_function", "anchor_text": "https://en.wikipedia.org/wiki/Probability_density_function"}, {"url": "https://en.wikipedia.org/wiki/Entropy", "anchor_text": "https://en.wikipedia.org/wiki/Entropy"}, {"url": "https://en.wikipedia.org/wiki/Cross_entropy", "anchor_text": "https://en.wikipedia.org/wiki/Cross_entropy"}, {"url": "https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence", "anchor_text": "https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence"}, {"url": "https://scikit-learn.org/stable/modules/mixture.html", "anchor_text": "https://scikit-learn.org/stable/modules/mixture.html"}, {"url": "https://towardsdatascience.com/part-i-a-new-tool-to-your-toolkit-kl-divergence-5b887b5b420e", "anchor_text": "https://towardsdatascience.com/part-i-a-new-tool-to-your-toolkit-kl-divergence-5b887b5b420e"}, {"url": "https://towardsdatascience.com/demystifying-entropy-f2c3221e2550", "anchor_text": "https://towardsdatascience.com/demystifying-entropy-f2c3221e2550"}, {"url": "https://towardsdatascience.com/demystifying-cross-entropy-e80e3ad54a8", "anchor_text": "https://towardsdatascience.com/demystifying-cross-entropy-e80e3ad54a8"}, {"url": "http://www.aishack.in/tutorials/expectation-maximization-gaussian-mixture-model-mixtures/", "anchor_text": "http://www.aishack.in/tutorials/expectation-maximization-gaussian-mixture-model-mixtures/"}, {"url": "https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm", "anchor_text": "https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm"}, {"url": "https://brilliant.org/wiki/gaussian-mixture-model/", "anchor_text": "https://brilliant.org/wiki/gaussian-mixture-model/"}, {"url": "https://en.wikipedia.org/wiki/Bayes%27_theorem", "anchor_text": "https://en.wikipedia.org/wiki/Bayes%27_theorem"}, {"url": "https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence", "anchor_text": "https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence"}, {"url": "https://medium.com/tag/data-science?source=post_page-----736c134baa3d---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----736c134baa3d---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/algorithms?source=post_page-----736c134baa3d---------------algorithms-----------------", "anchor_text": "Algorithms"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----736c134baa3d---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/tag/statistics?source=post_page-----736c134baa3d---------------statistics-----------------", "anchor_text": "Statistics"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F736c134baa3d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpart-2-a-new-tool-to-your-toolkit-kl-divergence-736c134baa3d&user=Abhishek+Mungoli&userId=5f15354d23de&source=-----736c134baa3d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F736c134baa3d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpart-2-a-new-tool-to-your-toolkit-kl-divergence-736c134baa3d&user=Abhishek+Mungoli&userId=5f15354d23de&source=-----736c134baa3d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F736c134baa3d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpart-2-a-new-tool-to-your-toolkit-kl-divergence-736c134baa3d&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----736c134baa3d--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F736c134baa3d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpart-2-a-new-tool-to-your-toolkit-kl-divergence-736c134baa3d&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----736c134baa3d---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----736c134baa3d--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----736c134baa3d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----736c134baa3d--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----736c134baa3d--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----736c134baa3d--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----736c134baa3d--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----736c134baa3d--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----736c134baa3d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@mungoliabhishek81?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@mungoliabhishek81?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Abhishek Mungoli"}, {"url": "https://medium.com/@mungoliabhishek81/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "611 Followers"}, {"url": "https://www.youtube.com/channel/UCg0PxC9ThQrbD9nM_FU1vWA", "anchor_text": "https://www.youtube.com/channel/UCg0PxC9ThQrbD9nM_FU1vWA"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5f15354d23de&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpart-2-a-new-tool-to-your-toolkit-kl-divergence-736c134baa3d&user=Abhishek+Mungoli&userId=5f15354d23de&source=post_page-5f15354d23de--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F8d41b74042bf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpart-2-a-new-tool-to-your-toolkit-kl-divergence-736c134baa3d&newsletterV3=5f15354d23de&newsletterV3Id=8d41b74042bf&user=Abhishek+Mungoli&userId=5f15354d23de&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}