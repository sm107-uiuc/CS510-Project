{"url": "https://towardsdatascience.com/cross-validation-a-beginners-guide-5b8ca04962cd", "time": 1682996375.515022, "path": "towardsdatascience.com/cross-validation-a-beginners-guide-5b8ca04962cd/", "webpage": {"metadata": {"title": "Cross Validation: A Beginner\u2019s Guide | by Caleb Neale | Towards Data Science", "h1": "Cross Validation: A Beginner\u2019s Guide", "description": "In beginning your journey into the world of machine learning and data science, there is often a temptation to jump into algorithms and model creation, without gaining an understanding of how to test\u2026"}, "outgoing_paragraph_urls": [{"url": "https://machinelearningmastery.com/data-leakage-machine-learning/", "anchor_text": "preventing data leakage", "paragraph_index": 4}, {"url": "https://towardsdatascience.com/introduction-to-linear-regression-in-python-c12a072bedf0", "anchor_text": "a linear regression", "paragraph_index": 5}, {"url": "https://scikit-learn.org/stable/", "anchor_text": "scikit-learn", "paragraph_index": 5}, {"url": "https://towardsdatascience.com/train-validation-and-test-sets-72cb40cba9e7", "anchor_text": "on a multitude of factors about your data", "paragraph_index": 8}, {"url": "https://docs.aws.amazon.com/machine-learning/latest/dg/model-fit-underfitting-vs-overfitting.html", "anchor_text": "here", "paragraph_index": 17}, {"url": "http://www.luigifreda.com/2017/03/22/bias-variance-tradeoff/", "anchor_text": "here", "paragraph_index": 18}, {"url": "http://scott.fortmann-roe.com/docs/BiasVariance.html", "anchor_text": "here", "paragraph_index": 24}, {"url": "http://www.ebc.cat/2017/01/31/cross-validation-strategies/#k-fold", "anchor_text": "here", "paragraph_index": 29}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_predict.html", "anchor_text": "cross_val_predict", "paragraph_index": 33}, {"url": "https://www.kaggle.com/uciml/forest-cover-type-dataset", "anchor_text": "a very large dataset with approximately 580,000 rows", "paragraph_index": 50}, {"url": "http://www.linkedin.com/in/calebneale", "anchor_text": "www.linkedin.com/in/calebneale", "paragraph_index": 53}], "all_paragraphs": ["By: Caleb Neale, Demetri Workman, Abhinay Dommalapati", "In beginning your journey into the world of machine learning and data science, there is often a temptation to jump into algorithms and model creation, without gaining an understanding of how to test the effectiveness of a generated model on real world data. Cross validation is a form of model validation which attempts to improve on the basic methods of hold-out validation by leveraging subsets of our data and an understanding of the bias/variance trade-off in order to gain a better understanding of how our models will actually perform when applied outside of the data it was trained on. Don\u2019t worry, it\u2019ll all be explained!", "This article seeks to be a beginning to execution guide for three methods of model validation (hold out, k-fold, and LOOCV) and the concepts behind them, with links and references to guide you to further reading. We make use of scikit learn, pandas, numpy and other python libraries in the given examples.", "What will be addressed in this article:", "Model validation is the process by which we ensure that our models can perform acceptable in \u201cthe real world.\u201d In more technical terms, model validation allows you to predict how your model will perform on datasets not used in the training (model validation is a big part of why preventing data leakage is so important). Model validation is important because we don\u2019t actually care how well the model predicts data we trained it on. We already know the target values for the data we used to train a model, and as such it is much more important to consider how robust and capable a model is when tasked to model new datasets of the same distribution and characteristics, but with different individual values from our training set. The first form of model validation introduced is usually what is known as holdout validation, often considered to be the simplest form of cross validation and thus the easiest to implement. Let\u2019s work through an example below.", "For this example, we\u2019ll use a linear regression on the scikit-learn database of California housing data.", "Once the data is stored into a variable we can more easily work with, we\u2019ll convert in into a pandas dataframe so we can more easily view and work with the data.", "Now that we\u2019ve seen the data we\u2019re working with, we can begin the process of generating a model and cross validation. In holdout validation, we split the data into a training and testing set. The training set will be what the model is created on and the testing data will be used to validate the generated model. Though there are (fairly easy) ways to do this using pandas methods, we can make use of scikit-learns \u201ctrain_test_split\u201d method to accomplish this.", "As you can see, we use the \u201ctrain_test_split\u201d with three parameters: the input (X) data, the target (y) data, and the percentage of data we\u2019d like to remove and put into the test dataset, in this case 25% (common split is usually 70\u201330, depending on a multitude of factors about your data). We then assign the split X and y data to a set of new variables to work with later.", "Your output should appear at this point as:", "Now that we\u2019ve created our test/train split we can create a model and generate some predictions based on the train data. Though there are other methods of creating a model which show more of the nitty gritty, we\u2019ll use scikit learn to make our lives a little easier. I\u2019ve included a few lines to time the runtime of the function, which we will use for later comparison.", "Let\u2019s pause here for a moment and look at what we\u2019ve done. Everything up to this point is just setup in creating a linear model and using it to make predictions on a dataset. This is how far you get without model validation. In other words, we have yet to look at how the model performs on its predictions of the test data when compared to the actual target values in the test data. The test/train split we did earlier was necessary to divide the data such that we can now test the model on data that was not used in training (see: data leakage). Now that we have a model, and have created some predictions, let\u2019s go though with our holdout validation.", "We\u2019ll start by graphing our given target data vs our predicted target data to give us a visualization of how our model performs.", "In a perfect model (overfit maybe), all our data points would be on that red line, but as our data points approximate that trend, we can see the model is roughly appropriate for the test data.", "Now, lets get a score for the model to evaluate it against later methods.", "That\u2019s model validation! We created a model using training data, used it to predict outcomes on a split segment of test data then used a scoring method to determine a measure of effectiveness (negative mean squared error) of the model on the testing data. This gives us an approximation of how well the model will perform on other similar datasets.", "Now, a few things to consider. We validated our model once. What if the split we made just happened to be very conducive to this model? What if the split we made introduced a large skew into the date? Didn\u2019t we significantly reduce the size of our training dataset by splitting it like that? These are a few questions we\u2019ll consider as we move into cross validation, but first a few background concepts.", "To understand bias and variance, let\u2019s first address over and under fit models. On overfit model is generated when the model is so tightly fit to the training data that it may account for random noise or unwanted trends which will not be present or useful in predicting targets for subsequent datasets. Underfit occurs when the model is not complex enough to account for general trends in the data which would be useful in predicting targets in subsequent datasets, such as using a linear fit on a polynomial trend(An awesome visualization and further explanation of this concept from AWS can be found here).", "When creating a model, we account for a few types of error: validation error, testing error, error due to bias, and error due to variance in a relationship known as the bias variance trade-off (another great visual here).", "As mentioned earlier, we want to know how the model will perform \u201cin the real world.\u201d Part of that is validation error, which is comprised of error due to bias and error due to variance (training error does not provide information on how the model will perform on future datasets, and can be set aside for now).", "Minimizing model validation error requires finding the point of model complexity where the combination of bias and variance error is minimized, as shown in the linked visual. As model complexity increases, error due to bias decreases, while error due to variance increases, creating the bias-variance trade-off, which we will seek to address later with various methods of cross validation.", "Now let\u2019s define bias and variance:", "Bias is the error resulting from the difference between the expected value(s) of a model and the actual (or \u201ccorrect\u201d) value(s) for which we want to predict over multiple iterations. In the scientific concepts of accuracy and precision, bias is very similar to accuracy.", "Variance is defined as the error resulting from the variability between different data predictions in a model. In variance, the correct value(s) don\u2019t matter as much as the range of differences in value between the predictions. Variance also comes into play more when we run multiple model creation trials.", "More complete definitions and visuals here.", "In machine learning, bias and variance are often discussed together as a \u201cbias-variance tradeoff,\u201d saying that minimizing one error effectively makes the one more likely to be present when creating and assessing a model. Ideally, we would seek a model whose tradeoff results in both low bias and low variance, and we would look to achieve this by using cross validation. Depending on characteristics of the dataset, one method of cross validation is likely to be more ideal to achieving the bias-variance tradeoff when creating and assessing a model.", "What if the split we made just happened to be very conducive to this model? What if the split we made introduced a large skew into the date? Didn\u2019t we significantly reduce the size of our training dataset by splitting it like that?", "Cross validation is a method of model validation which splits the data in creative ways in order to obtain the better estimates of \u201creal world\u201d model performance, and minimize validation error.", "Remember those questions we asked about hold out validation? Cross validation is our answer.", "K-fold validation is a popular method of cross validation which shuffles the data and splits it into k number of folds (groups). In general K-fold validation is performed by taking one group as the test data set, and the other k-1 groups as the training data, fitting and evaluating a model, and recording the chosen score. This process is then repeated with each fold (group) as the test data and all the scores averaged to obtain a more comprehensive model validation score. (More reading and a helpful visualization here).", "When choosing a value for k each fold(group) should be large enough to be representative of the model (commonly k=10 or k=5) and small enough to be computed in a reasonable amount of time. Depending on the dataset size, different k values can sometimes be experimented with. As a general rule, as k increases, bias decreases and variance increases.", "Lets work though an example with our dataset from earlier.", "We\u2019ll make use of a linear model again, but this time do model validation with scikit learn\u2019s cross_val_predict method which will do most of the heavy lifting in generating K-Fold predictions. In this case, I chose to set k=10.", "\u2018cross_val_predict\u2019 takes the model used on the data, the input and target data, as well as a \u2018cv\u2019 argument \u2014 which is essentially our k value \u2014 and returns the predicted values for each input. Now we can plot the predictions as we did with the hold out method.", "Now let\u2019s get the scores of the 10 generated models and plot them into a visualization.", "You\u2019ll notice that the score is a little farther from zero than the holdout method (not good). We\u2019ll discuss that later.", "Leave One Out Cross Validation (LOOCV) can be considered a type of K-Fold validation where k=n given n is the number of rows in the dataset. Other than that the methods are quire similar. You will notice, however, that running the following code will take much longer than previous methods. We\u2019ll dig into that later.", "Let\u2019s work an example with the same dataset, following the same process and modifying k:", "Now lets compare the run times and scores of our three methods:", "Let\u2019s dig into these results a little, as well as some of the points raised earlier.", "As we noticed in the results of our comparison, we can see that the LOOCV method takes way longer to complete than our other two. This is because that method creates and evaluates a model for each row in the dataset, in this case over 20,000. Even though our MSE is a little lower, this may not be worth it given the additional computational requirements. Here are some heuristics which can help in choosing a method.", "The hold out method can be effective and computationally inexpensive on very large datasets, or on limited computational resources. It is also often easier to implement and understand for beginners. However, it is very rarely good to apply to small datasets as it can significantly reduce the training data available and hurt model performance.", "K-Fold can be very effective on medium sized datasets, though by adjusting the K value can significantly alter the results of the validation. Let\u2019s add to our rule from earlier; as k increases, bias decreases, and variance and computational requirements increase. K-Fold cross validation is likely the most common of the three methods due to the versatility of adjusting K-values.", "LOOCV is most useful in small datasets as it allows for the smallest amount of data to be removed from the training data in each iteration. However, in large datasets the process of generating a model for each row in the dataset can be incredibly computationally expensive and thus prohibitive for larger datasets.", "In holdout validation, we are doing nothing more than performing a simple train/test split in which we fit our model to our training data and apply it to our testing data to generate predicted values. We \u201chold out\u201d the testing data to be strictly used for prediction purposes only. Holdout validation is NOT a cross validation technique. But we must discuss the standard method of model evaluation so that we can compare its attributes with the actual cross validation techniques.", "When it comes to code, holdout validation is easy to use. The implementation is simple and doesn\u2019t require large dedications to computational power and time complexity. Moreover, we can interpret and understand the results of holdout validation better as they don\u2019t require us to figure out how the iterations are performing in the grand scheme of things.", "However, holdout validation does not preserve the statistical integrity of the dataset in many cases. For instance, a holdout validation that splits the data into training and testing segments causes bias by not incorporating the testing data into the model. The testing data could contain some important observations. This would result in a detriment to the accuracy of the model. Furthermore, this will cause an underfitting and overfitting of the data in addition to an introduction of validation and/or training error.K-fold", "In K-fold cross validation, we answer many of the problems inherent in holdout validation such as underfitting/overfitting and validation and training error. This is done by using all of the observations in our validation set at some iteration. We compute an average accuracy score of all the accuracy scores that are calculated in each k iteration. By doing so, we minimize bias and variation that may be present in our initial model evaluation technique, holdout validation. However, in terms of computational power, k-fold cross validation is very costly. The computer has to perform several iterations to generate a proper accuracy score. The accuracy score of the model will in theory increase with each added k iteration. This will decrease bias while increasing variation. We will see an example of this later in this article when we attempt to apply k-fold validation to a very large dataset that contains about 580,000 observations.", "LOOCV is very similar to K-fold, with a special case in which k is equal to the length (or number of samples/rows) of the whole dataset. Thus the training set will be of length k-1, and the testing set will be a single sample of the data. LOOCV is particularly useful in the case that our data set is not large enough to sensibly do Kfold. LOOCV is also less computationally expensive in general, although it is usually due to the inherently smaller datasets that tend utilize it.", "However, LOOCV tends to yield high variance due to the fact that the method would pick up on all of the possible noise and outlier values in the data through the individual testing values. LOOCV would be very computationally expensive for very large data sets; in this case, it would be better to use regular k-fold.", "Cross validation becomes a computationally expensive and taxing method of model evaluation when dealing with large datasets. Generating prediction values ends up taking a very long time because the validation method have to run k times in K-Fold strategy, iterating through the entire dataset. Thus cross validation becomes a very costly model evaluation strategy in terms of time complexity. We will examine this phenomenon by performing a normal holdout validation and a K-Fold cross validation on a very large dataset with approximately 580,000 rows. See if you can figure it out, why it works the way it does (and the new data visualizations), and comment any questions. Good luck!", "Model Validation: Any process by which a generated model is verified against additional data not used in the process of generating the model. E.g. cross validation, K-Fold validation, hold out validation, etc.Cross Validation: A type of model validation where multiple subsets of a given dataset are created and verified against each-other, usually in an iterative approach requiring the generation of a number of separate models equivalent to the number of groups generated.K-Fold Cross Validation: A type of cross validation where a given dataset is split into k number of groups and k number of models are generated. One of the groups is chosen as the test data, and the other k-1 groups are used as training data, and model generated and scored. This process is repeated k times such that each k-fold (group) serves as the testing group once.LOOCV: A type of cross validation similar to K-Fold validation, where k is equal to the length of the dataset which the validation is being performed on. Bias: The error resulting from the difference between the expected value(s) of a model and the actual (or \u201ccorrect\u201d) value(s) for which we want to predict over multiple iterations. In the scientific concepts of accuracy and precision, bias is very similar to accuracy.Variance: The error resulting from the variability between different data predictions in a model. In variance, the correct value(s) don\u2019t matter as much as the range of differences in value between the predictions. Variance also comes into play more when we run multiple model creation trials.Under fit: Occurs when the model is so tightly fit to the training data that it may account for random noise or unwanted trends which will not be present or useful in predicting targets for subsequent datasets.Over fit: Occurs when the model is not complex enough to account for general trends in the data which would be useful in predicting targets in subsequent datasets, such as using a linear fit on a polynomial trend.Bias-Variance Trade-off: The idea that as error due to bias decreases error due to variance increases, creating a trade-off which should be minimized in model validation, and other circumstances.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Systems Engineering Student at the University of Virginia | Data Scientist in Training | www.linkedin.com/in/calebneale"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F5b8ca04962cd&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcross-validation-a-beginners-guide-5b8ca04962cd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcross-validation-a-beginners-guide-5b8ca04962cd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcross-validation-a-beginners-guide-5b8ca04962cd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcross-validation-a-beginners-guide-5b8ca04962cd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----5b8ca04962cd--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5b8ca04962cd--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@can4ku?source=post_page-----5b8ca04962cd--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@can4ku?source=post_page-----5b8ca04962cd--------------------------------", "anchor_text": "Caleb Neale"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd0393ab47c83&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcross-validation-a-beginners-guide-5b8ca04962cd&user=Caleb+Neale&userId=d0393ab47c83&source=post_page-d0393ab47c83----5b8ca04962cd---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5b8ca04962cd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcross-validation-a-beginners-guide-5b8ca04962cd&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5b8ca04962cd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcross-validation-a-beginners-guide-5b8ca04962cd&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://machinelearningmastery.com/data-leakage-machine-learning/", "anchor_text": "preventing data leakage"}, {"url": "https://towardsdatascience.com/introduction-to-linear-regression-in-python-c12a072bedf0", "anchor_text": "a linear regression"}, {"url": "https://scikit-learn.org/stable/", "anchor_text": "scikit-learn"}, {"url": "https://towardsdatascience.com/train-validation-and-test-sets-72cb40cba9e7", "anchor_text": "on a multitude of factors about your data"}, {"url": "https://docs.aws.amazon.com/machine-learning/latest/dg/model-fit-underfitting-vs-overfitting.html", "anchor_text": "here"}, {"url": "https://docs.aws.amazon.com/machine-learning/latest/dg/model-fit-underfitting-vs-overfitting.html", "anchor_text": "https://docs.aws.amazon.com/machine-learning/latest/dg/model-fit-underfitting-vs-overfitting.html"}, {"url": "http://www.luigifreda.com/2017/03/22/bias-variance-tradeoff/", "anchor_text": "here"}, {"url": "http://www.luigifreda.com/2017/03/22/bias-variance-tradeoff/", "anchor_text": "http://www.luigifreda.com/2017/03/22/bias-variance-tradeoff/"}, {"url": "http://scott.fortmann-roe.com/docs/BiasVariance.html", "anchor_text": "here"}, {"url": "http://scott.fortmann-roe.com/docs/BiasVariance.html", "anchor_text": "http://scott.fortmann-roe.com/docs/BiasVariance.html"}, {"url": "http://www.ebc.cat/2017/01/31/cross-validation-strategies/#k-fold", "anchor_text": "here"}, {"url": "http://www.ebc.cat/2017/01/31/cross-validation-strategies/#k-fold", "anchor_text": "http://www.ebc.cat/2017/01/31/cross-validation-strategies/#k-fold"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_predict.html", "anchor_text": "cross_val_predict"}, {"url": "https://www.kaggle.com/uciml/forest-cover-type-dataset", "anchor_text": "a very large dataset with approximately 580,000 rows"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----5b8ca04962cd---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/model-validation?source=post_page-----5b8ca04962cd---------------model_validation-----------------", "anchor_text": "Model Validation"}, {"url": "https://medium.com/tag/crossvalidation?source=post_page-----5b8ca04962cd---------------crossvalidation-----------------", "anchor_text": "Crossvalidation"}, {"url": "https://medium.com/tag/python?source=post_page-----5b8ca04962cd---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/data-science?source=post_page-----5b8ca04962cd---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5b8ca04962cd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcross-validation-a-beginners-guide-5b8ca04962cd&user=Caleb+Neale&userId=d0393ab47c83&source=-----5b8ca04962cd---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5b8ca04962cd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcross-validation-a-beginners-guide-5b8ca04962cd&user=Caleb+Neale&userId=d0393ab47c83&source=-----5b8ca04962cd---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5b8ca04962cd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcross-validation-a-beginners-guide-5b8ca04962cd&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5b8ca04962cd--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F5b8ca04962cd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcross-validation-a-beginners-guide-5b8ca04962cd&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----5b8ca04962cd---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----5b8ca04962cd--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----5b8ca04962cd--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----5b8ca04962cd--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----5b8ca04962cd--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----5b8ca04962cd--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----5b8ca04962cd--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----5b8ca04962cd--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----5b8ca04962cd--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@can4ku?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@can4ku?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Caleb Neale"}, {"url": "https://medium.com/@can4ku/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "100 Followers"}, {"url": "http://www.linkedin.com/in/calebneale", "anchor_text": "www.linkedin.com/in/calebneale"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd0393ab47c83&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcross-validation-a-beginners-guide-5b8ca04962cd&user=Caleb+Neale&userId=d0393ab47c83&source=post_page-d0393ab47c83--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fcc91d845816e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcross-validation-a-beginners-guide-5b8ca04962cd&newsletterV3=d0393ab47c83&newsletterV3Id=cc91d845816e&user=Caleb+Neale&userId=d0393ab47c83&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}