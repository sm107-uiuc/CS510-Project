{"url": "https://towardsdatascience.com/dqn-part-1-vanilla-deep-q-networks-6eb4a00febfb", "time": 1682997192.5835872, "path": "towardsdatascience.com/dqn-part-1-vanilla-deep-q-networks-6eb4a00febfb/", "webpage": {"metadata": {"title": "Vanilla Deep Q Networks. Deep Q Learning Explained | by Chris Yoon | Towards Data Science", "h1": "Vanilla Deep Q Networks", "description": "We will briefly go through general policy iteration and temporal difference methods. We will then understand Q learning as a general policy iteration. Finally, we will understand and implement DQN\u2026"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/abs/1312.5602", "anchor_text": "Playing Atari with Deep Reinforcement Learning (Mnih et al. 2013)", "paragraph_index": 1}, {"url": "https://towardsdatascience.com/tagged/Dqn-family", "anchor_text": "My series", "paragraph_index": 25}, {"url": "https://towardsdatascience.com/double-deep-q-networks-905dd8325412", "anchor_text": "my next post", "paragraph_index": 25}], "all_paragraphs": ["This post will be structured as followed:", "We will briefly go through general policy iteration and temporal difference methods. We will then understand Q learning as a general policy iteration. Finally, we will understand and implement DQN presented in Deepmind\u2019s paper \u201cPlaying Atari with Deep Reinforcement Learning (Mnih et al. 2013).", "We call general policy iteration the alternation between policy evaluation and policy iteration. We start with some arbitrarily initialized policy, evaluate the policy (denoted as E), derive a new policy from the evaluation (denoted as I), and repeat this process until we reach an optimal policy. Through this iterative process, we obtain monotonically increasing (improving) sequences of {V_\u03c0} and {\u03c0}.", "How can we guarantee that? We will take a look at the Policy Improvement Theorem:", "The theorem states that the value of following policy \u03c0\u2019 yields greater value than that of following policy \u03c0. That is, policy \u03c0\u2019 is better than \u03c0. We will now prove the policy improvement theorem.", "Therefore, through the iterative interplay of policy evaluation and iteration, we can eventually reach our optimal policy and value function.", "Temporal difference methods are a combination of Monte Carlo methods and Dynamic Programming methods. Recall each method:", "In TD methods, we combine the sampling of Monte Carlo with the bootstrapping of Dynamic Programming. We sample our expected values as shown in (1), and use current estimates of our next state values to update the value of our original state as shown in (3). The TD target \\delta can come in various forms, but below is the most fundamental form:", "In Q learning, we directly approximate our optimal action-value function. In a GPI sense, we derive our policy from our Q function and carry out policy evaluation via TD methods to obtain our next Q function.", "Now let our Q function be parameterized by some \u03b8\u2014 which is, in our case, neural networks. Following the formulation of GPI, we want to minimize the difference between our current Q and our target Q. To do that, we want to take the mean squared error between the two:", "and then perform gradient descent to minimize the error between the two.", "Deep Q learning, as published in (Mnih et al, 2013), leverages advances in deep learning to learn policies from high dimensional sensory input. Specifically, it learns with raw pixels from Atari 2600 games using convolutional networks, instead of low-dimensional feature vectors. The figure below illustrates the architecture of DQN:", "To make computations more viable, the raw 4 frames of gameplay image \u2014 RGB pixels \u2014 are scaled down to four frames of (84 x 84) images, resulting in an (84 x 84 x 4) tensor. We then feed that to a convolutional neural network which outputs a vector containing the Q value of every action. From there, we use an exploration scheme (typically epsilon-greedy) and probabilistically choose between the action with the highest Q value and a random action.", "On a higher level, Deep Q learning works as such:", "We will take a closer look at steps (2) and (3), which will let us move straight into implementation.", "Why should we random sample experiences, instead of just using past sequential experiences? Sequential experiences are highly correlated (temporally) with each other. In statistical learning and optimization tasks, we want our data to be independently distributed. That is, we don\u2019t want the data we are feeding to be correlated with each other in any way. Random sampling of experiences breaks this temporal correlation of behavior and distributes/averages it over many of its previous states. By doing so, we avoid significant oscillations or divergence in our model \u2014 problems that can arise from correlated data.", "For updating the Q network, we want to minimize the mean squared error between our target Q value (according to the Bellman equation) and our current Q output:", "Optimally, we want the error to decrease, meaning that our current policy\u2019s outputs are becoming more similar to the true Q values. Therefore, with the loss function defined as above, we perform a gradient step on the loss function according to:", "We will start with constructing our deep Q network equipped with a convolutional neural network:", "Where in the forward function, we input the pixel image, and pass through our model to output a vector of Q values corresponding to each action.", "We will then construct our replay buffer, where we can store experiences \u2014 (state, action, reward, next state, bool(is_done)) transitions \u2014 and sample random experiences for learning:", "Next, we write a function that computes the value loss at each gradient step. This looks like:", "Finally, we will put them all together in our DQN agent:", "This concludes the implementation of vanilla DQN. You can find the full run-able implementation on my GitHub repository:", "In the later parts of this series, we will explore numerous variants of DQN that improves the original in numerous areas.", "My series will start with vanilla deep Q-learning (this post) and lead up to Deepmind\u2019s Rainbow DQN, the current state-of-the-art. Check my next post on reducing overestimation bias with double Q-learning!", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F6eb4a00febfb&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdqn-part-1-vanilla-deep-q-networks-6eb4a00febfb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdqn-part-1-vanilla-deep-q-networks-6eb4a00febfb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdqn-part-1-vanilla-deep-q-networks-6eb4a00febfb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdqn-part-1-vanilla-deep-q-networks-6eb4a00febfb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----6eb4a00febfb--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----6eb4a00febfb--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@thechrisyoon?source=post_page-----6eb4a00febfb--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@thechrisyoon?source=post_page-----6eb4a00febfb--------------------------------", "anchor_text": "Chris Yoon"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb24112d01863&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdqn-part-1-vanilla-deep-q-networks-6eb4a00febfb&user=Chris+Yoon&userId=b24112d01863&source=post_page-b24112d01863----6eb4a00febfb---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6eb4a00febfb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdqn-part-1-vanilla-deep-q-networks-6eb4a00febfb&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6eb4a00febfb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdqn-part-1-vanilla-deep-q-networks-6eb4a00febfb&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/tagged/Dqn-family", "anchor_text": "DQN Family"}, {"url": "https://arxiv.org/abs/1312.5602", "anchor_text": "Playing Atari with Deep Reinforcement Learning (Mnih et al. 2013)"}, {"url": "https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/image_folder_7/DQNBreakoutBlocks.png", "anchor_text": "https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/image_folder_7/DQNBreakoutBlocks.png"}, {"url": "https://github.com/cyoon1729/deep-Q-networks", "anchor_text": "cyoon1729/deep-Q-networksModular Implementations of algorithms from the Q-learning family (PyTorch). Implementations inlcude: DQN, DDQN, Dueling\u2026github.com"}, {"url": "https://arxiv.org/abs/1312.5602", "anchor_text": "Playing Atari with Deep Reinforcement Learning (Mnih et al. 2013)"}, {"url": "https://towardsdatascience.com/tagged/Dqn-family", "anchor_text": "My series"}, {"url": "https://towardsdatascience.com/double-deep-q-networks-905dd8325412", "anchor_text": "my next post"}, {"url": "https://towardsdatascience.com/double-deep-q-networks-905dd8325412", "anchor_text": "Double Deep Q Networks"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----6eb4a00febfb---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----6eb4a00febfb---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/deep-q-learning?source=post_page-----6eb4a00febfb---------------deep_q_learning-----------------", "anchor_text": "Deep Q Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----6eb4a00febfb---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/dqn-family?source=post_page-----6eb4a00febfb---------------dqn_family-----------------", "anchor_text": "Dqn Family"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6eb4a00febfb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdqn-part-1-vanilla-deep-q-networks-6eb4a00febfb&user=Chris+Yoon&userId=b24112d01863&source=-----6eb4a00febfb---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6eb4a00febfb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdqn-part-1-vanilla-deep-q-networks-6eb4a00febfb&user=Chris+Yoon&userId=b24112d01863&source=-----6eb4a00febfb---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6eb4a00febfb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdqn-part-1-vanilla-deep-q-networks-6eb4a00febfb&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----6eb4a00febfb--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F6eb4a00febfb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdqn-part-1-vanilla-deep-q-networks-6eb4a00febfb&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----6eb4a00febfb---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----6eb4a00febfb--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----6eb4a00febfb--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----6eb4a00febfb--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----6eb4a00febfb--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----6eb4a00febfb--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----6eb4a00febfb--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----6eb4a00febfb--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----6eb4a00febfb--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@thechrisyoon?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@thechrisyoon?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Chris Yoon"}, {"url": "https://medium.com/@thechrisyoon/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "631 Followers"}, {"url": "https://www.linkedin.com/in/chris-yoon-75847418b/", "anchor_text": "https://www.linkedin.com/in/chris-yoon-75847418b/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb24112d01863&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdqn-part-1-vanilla-deep-q-networks-6eb4a00febfb&user=Chris+Yoon&userId=b24112d01863&source=post_page-b24112d01863--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F6d3234499fec&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdqn-part-1-vanilla-deep-q-networks-6eb4a00febfb&newsletterV3=b24112d01863&newsletterV3Id=6d3234499fec&user=Chris+Yoon&userId=b24112d01863&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}