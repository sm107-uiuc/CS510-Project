{"url": "https://towardsdatascience.com/gans-from-scratch-8f5da17b3fb4", "time": 1683002582.12881, "path": "towardsdatascience.com/gans-from-scratch-8f5da17b3fb4/", "webpage": {"metadata": {"title": "GANs from scratch.. Generative adversarial networks(GANs)\u2026 | by Sivasurya Santhanam | Towards Data Science", "h1": "GANs from scratch.", "description": "Generative adversarial networks(GANs) took AI by storm last year with those impressive human-like faces. They were really cool weren\u2019t they? They are basically generated from nothing. Nothing\u2026"}, "outgoing_paragraph_urls": [{"url": "https://github.com/chmodsss", "anchor_text": "Github", "paragraph_index": 27}, {"url": "https://twitter.com/chmodsss", "anchor_text": "Twitter", "paragraph_index": 27}, {"url": "https://www.linkedin.com/in/sivasuryas", "anchor_text": "Linkedin", "paragraph_index": 27}, {"url": "https://medium.com/ai-society/gans-from-scratch-1-a-deep-introduction-with-code-in-pytorch-and-tensorflow-cb03cdcdba0f", "anchor_text": "article", "paragraph_index": 28}, {"url": "https://medium.com/u/2ae3e490bdf8?source=post_page-----8f5da17b3fb4--------------------------------", "anchor_text": "Diego Gomez Mosquera", "paragraph_index": 28}], "all_paragraphs": ["Generative adversarial networks(GANs) took AI by storm last year with those impressive human-like faces. They were really cool weren\u2019t they? They are basically generated from nothing.", "Nothing??. *Coughs* Show-off *Coughs*. You people used data to train the model. We know how these \u201cmachine learning\u201d things work. It's just an Input-Output function approximation. There is nothing impressive about it. It\u2019s just another algorithm.", "Well, not really. GANs belong to implicit learning methods. In explicit learning models, the model learns its weights directly from the data. whereas in implicit learning, the model learns without the data directly passing through the network.", "Ah!, so it is reinforcement learning?", "There are some similarities between RL and GANs, as they both use Actor-Critic approach. But the nature of learning is different in GAN.", "All right! I give up. So, explain GAN like I\u2019m 5.", "Okay. So in the city of Barcelona, a new police got appointed to verify the authenticity of driver licenses. His work is to classify the legit ones and fake ones. Since he is new, he gets feedback from his colleague whether he classifies it correctly or not. There is also a new Forger in town and he aims at producing fake driver licenses. So, the Forger prints whatever he thinks is a license and submits it to the police. The police then accepts/rejects it. whenever the police rejects it, the Forger learns from the mistakes and tries to develop a foolproof license, and when it is accepted he produces more like it. But when the police accepts a fake license, he gets corrected by the colleague. Both the Police and the Forger becomes better in what they do by learning from each other.", "Duh! Seems like the colleague should do the verification", "I thought you were 5. Okay, so here is a better version. In GANs, there are two network Discriminators (the police) and the Generator (the forger). The generator creates fake data(images) and the discriminator classifies the images. Based on the results from the discriminator, the generator starts to learn to create better and better images. Thus, both the Discriminator and the Generator compete to perform better. They learn from each other and get better and better with each run. The interesting point to note is that both the networks are in the process of learning. So, even the discriminator cannot classify it correctly all the time. Let\u2019s say the discriminator classified a fake image as real, the generator learns from this result that the generated image was a good one. This will be fixed when the discriminator learns by getting feedback from the training data (the colleague). It may sound that they both will surely converge. But Convergence and stability on GANs is a separate topic of its own.", "Well, I must say GANs now sound pretty interesting. So, basically the generator learns the underlying distribution of the data without seeing them. But it is learning from the discriminator network, which is also learning simultaneously along with the generator. That\u2019s impressive. Now I wanna implement GANs and create these new AI people.", "Uh. Not so fast. For this result you need better knowledge about CNNs, lots of hyper parameter tuning, 8 Tesla GPUs and a week of training time. How about a vanilla GAN on a basic dataset that trains for 20 minutes on your laptop. Sounds good? Let\u2019s do it then.", "I\u2019ve used PyTorch for implementing them. Yea, I could already hear \u201cboo\u201d from Keras/Tensorflow people. Keep calm and take the opportunity to get comfortable with it, just as I did.", "Heard of MNIST dataset?. Yea, we will be using that. Images are gray-scaled with a shape of 28\u00d728 pixels. Let\u2019s load the dataset from torch.datasetsmodule. The images are flattened and the values are normalized to 0\u20131 using multiple_transforms. The DataLoaderfunction helps to slice the training data in batches.", "We will be using a fully connected network with the following configuration. Don\u2019t get perplexed with the layers and neurons. You can remove/add layers as you wish. This configuration has some best practices, which I will also explain later. The input to Generator is noise of any arbitrary value. Here, I have chosen 128 neurons. The output of Generator has to match the shape of the training data value, which is 784. The Discriminator gets the input of 784 neurons and outputs a single value, whether it is real(1) or fake(0).", "Translating the whole Architecture into network is fairly straightforward. Implement the nn.Modulesclass and define the forwardfunction. The best thing about pytorch is the functionality of autograd. What it means is, we don\u2019t have to do the math behind the backprop. The gradients for all the neurons are automatically calculated.", "Cool. Once both the Classes are defined, we could just instantiate them. And don\u2019t you forget to define the cost function and the optimization algorithm to be used.", "Why this BCELoss and Adam optimization?", "Vanilla GAN uses Minimax algorithm. Minimax loss is computed by the log loss of both generator and discriminator predicted probabilities. BCELoss is Binary cross entropy loss, which does the log loss of probabilities. One can use two different loss functions for training Generator and Discriminator, but here we use a single loss function for both. And for weight updates we use Adam optimizer, because everybody uses it. haha, jk. You could try with other optimizers like SGD, Adagrad.", "So, The model design is complete. Yay. Now, let\u2019s train the model.", "Whatever the confusion that may occur, it occurs here. Let\u2019s go step by step. So we have to train both the discriminator as well as the generator simultaneously. That fundamentally means the following steps.", "Woah Woah. Slow down. I have lots of\u00a0questions!", "Why do you index only the data, pos_samples[0]. What happened to the labels of the training data?- Nicely spotted. In Vanilla GAN, we don\u2019t care about the labels. We basically give all the training data to train the networks irrespective of the class it is from. So, the Generator network has to fit the weights to reproduce all the variations for different noise inputs. That being said, there are several variations in GANs, which takes into consideration of labels like the Auxiliary GANs.", "What is this zero_grad() for the optimizer?- For each epoch, we want the gradients to be zero so that the gradients computed during each backpropagation may occur\u00a0without\u00a0residual\u00a0gradients\u00a0in\u00a0the\u00a0neurons. Without zero_grad(), the gradients will be accumulated for each epoch, which is useful in networks like RNNs.", "How can you just add both errors and execute the backward()?- It\u2019s so pythonic, isn\u2019t it? It\u2019s the autograd module in pytorch. It takes care of back propagating both the errors.", "All right, so now how do you know when the network is trained?. Which cost function should I have to observe?- As previously stated, convergence is an interesting problem in GAN. Strictly speaking, when both the Discriminator and the Generator reach the Nash equilibrium, the GAN is said to be trained. Since GAN is a minimax problem when one network maximizes its cost function the other one tries to minimize it. And we are training both to improve. Nash equilibrium states that the agent doesn\u2019t change its course of action irrespective of other agent\u2019s decision. During training, one of the network trains from the other one right, but when it reaches a point where the Discriminator or Generator doesn\u2019t get better irrespective of the other network\u2019s decision, it has reached the Nash equilibrium. In practical terms, given an equal set of real and fake images, the Discriminator will detect every real and fake images as real, thus the prediction accuracy will be 50%.", "There are few best practices\u00a0suggested\u00a0for\u00a0a\u00a0better\u00a0model\u00a0and faster\u00a0convergence. I deliberately saved this at the end as some of this discussion might change the code and will lead to confusion if explained in the main content.", "Thank you for reading the post. If you have spotted any errors or having doubts, please let me know in the comments.", "Feel free to contact me via Github, Twitter and Linkedin. Cheers!.", "Many thanks to the article by Diego Gomez Mosquera.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F8f5da17b3fb4&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgans-from-scratch-8f5da17b3fb4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgans-from-scratch-8f5da17b3fb4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgans-from-scratch-8f5da17b3fb4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgans-from-scratch-8f5da17b3fb4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----8f5da17b3fb4--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8f5da17b3fb4--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@chmodsss?source=post_page-----8f5da17b3fb4--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@chmodsss?source=post_page-----8f5da17b3fb4--------------------------------", "anchor_text": "Sivasurya Santhanam"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F583e91571492&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgans-from-scratch-8f5da17b3fb4&user=Sivasurya+Santhanam&userId=583e91571492&source=post_page-583e91571492----8f5da17b3fb4---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8f5da17b3fb4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgans-from-scratch-8f5da17b3fb4&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8f5da17b3fb4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgans-from-scratch-8f5da17b3fb4&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://arxiv.org/abs/1812.04948", "anchor_text": "https://arxiv.org/abs/1812.04948"}, {"url": "https://github.com/chmodsss", "anchor_text": "Github"}, {"url": "https://twitter.com/chmodsss", "anchor_text": "Twitter"}, {"url": "https://www.linkedin.com/in/sivasuryas", "anchor_text": "Linkedin"}, {"url": "https://medium.com/ai-society/gans-from-scratch-1-a-deep-introduction-with-code-in-pytorch-and-tensorflow-cb03cdcdba0f", "anchor_text": "article"}, {"url": "https://medium.com/u/2ae3e490bdf8?source=post_page-----8f5da17b3fb4--------------------------------", "anchor_text": "Diego Gomez Mosquera"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----8f5da17b3fb4---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/gans?source=post_page-----8f5da17b3fb4---------------gans-----------------", "anchor_text": "Gans"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----8f5da17b3fb4---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/pytorch?source=post_page-----8f5da17b3fb4---------------pytorch-----------------", "anchor_text": "Pytorch"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8f5da17b3fb4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgans-from-scratch-8f5da17b3fb4&user=Sivasurya+Santhanam&userId=583e91571492&source=-----8f5da17b3fb4---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8f5da17b3fb4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgans-from-scratch-8f5da17b3fb4&user=Sivasurya+Santhanam&userId=583e91571492&source=-----8f5da17b3fb4---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8f5da17b3fb4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgans-from-scratch-8f5da17b3fb4&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8f5da17b3fb4--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F8f5da17b3fb4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgans-from-scratch-8f5da17b3fb4&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----8f5da17b3fb4---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----8f5da17b3fb4--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----8f5da17b3fb4--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----8f5da17b3fb4--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----8f5da17b3fb4--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----8f5da17b3fb4--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----8f5da17b3fb4--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----8f5da17b3fb4--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----8f5da17b3fb4--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@chmodsss?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@chmodsss?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Sivasurya Santhanam"}, {"url": "https://medium.com/@chmodsss/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "125 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F583e91571492&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgans-from-scratch-8f5da17b3fb4&user=Sivasurya+Santhanam&userId=583e91571492&source=post_page-583e91571492--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fc1d34e2aecb5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgans-from-scratch-8f5da17b3fb4&newsletterV3=583e91571492&newsletterV3Id=c1d34e2aecb5&user=Sivasurya+Santhanam&userId=583e91571492&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}