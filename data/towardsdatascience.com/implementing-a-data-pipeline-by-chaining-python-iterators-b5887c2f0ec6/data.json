{"url": "https://towardsdatascience.com/implementing-a-data-pipeline-by-chaining-python-iterators-b5887c2f0ec6", "time": 1683004830.849876, "path": "towardsdatascience.com/implementing-a-data-pipeline-by-chaining-python-iterators-b5887c2f0ec6/", "webpage": {"metadata": {"title": "Implementing a data pipeline by chaining Python iterators | by Armando Rivero | Towards Data Science", "h1": "Implementing a data pipeline by chaining Python iterators", "description": "In this article I\u2019ll talk about how to process a collection of items in Python through several steps with relative efficiency and flexibility while keeping your code clean. We\u2019ll do it by creating\u2026"}, "outgoing_paragraph_urls": [{"url": "https://joblib.readthedocs.io/en/latest/", "anchor_text": "joblib", "paragraph_index": 24}], "all_paragraphs": ["In this article I\u2019ll talk about how to process a collection of items in Python through several steps with relative efficiency and flexibility while keeping your code clean. We\u2019ll do it by creating iterators that feed other iterators and so on, hence creating an iterator chain.", "If you\u2019ve read the Gang of Four book or similar material you\u2019ve probably found the Iterator design pattern. An iterator is just an object with methods that allow the client code using it to traverse a collection of items. The beauty of iterators is that they encapsulate (hide) the nature of the collection. The collection could be the set of rows on a database table, or nodes on a elaborate graph model, or simply elements within an array in memory, but the users of the iterator don\u2019t know about it. They only need to know they can get one element after another until the collection is exhausted.", "Python has great support for iterators, and to understand how it works, let\u2019s talk about a few concepts.", "First, an iterator in Python is any object with a __next__ method that returns the next element of the collection until the collection is over, and, after that, will raise a StopIteration exception every time is called. Let\u2019s do a very simple iterator for pedagogical purposes", "However, if you try to use this iterator on a for loop, you\u2019ll get a \u201cTypeError: \u2018MyIterator\u2019 object is not iterable\u201d. That\u2019s because for loops require iterables, which are objects with an __iter__ method that returns an iterator. Most collections (lists, dictionaries, sets, etc.) are iterables. They aren\u2019t iterators (actually, calling next() on them will throw an error) but they can create an iterator so client code can traverse them. In our iterator, just adding an __iter__ method that returns self makes it an iterable.", "But that\u2019s not all with iterators. Another awesome Python syntax is the generator. A generator is an iterator that can be created like a function, simply by replacing return with yield. Let\u2019s see how our iterator example would translate as a generator", "Generators are also iterables that you can use in for loops straight ahead. Something important to keep in mind is that any function (or method) with a single yield anywhere won\u2019t do anything when called. I mean, not a single line of the code in the function will be executed. Instead, the function will create and return the generator object. The code will only be executed once you start iterating over the generator. When you call __next__ on the generator, the code will execute until it reaches the first yield. Then it will return whatever is on the yield and will stop. The state is saved, so the __next__ time :) the execution will resume right after that yield and will stop upon reaching the following yield statement. Unlike return, you can write a yield below another yield and both will be reached. Finally, when the generator reaches the end of the code it will raise the StopIteration exception. If the end is reached by a return statement, the returned value will be included on the exception and WON\u2019T be yielded.", "Suppose we have a collection of items that need to undergo some processing. Now if the processing is quite simple you can do it all on a single method, using some helper methods and so on. However doing everything on a single place it\u2019s a very bad software design approach, it\u2019s how you get functions with hundreds of lines of code that make you wanna cry when you have to read them :(", "A way of avoiding this pain is to make the items go through a simple pipeline. In a simple pipeline each method receives an item, makes some changes on it and returns the transformed item. Each method can stay short and simple. Also, they share the same interface (item in, item out) and don\u2019t need to know where in the pipeline they are, or what happened to the items before they reached the method. This allows you to change the order of the transformations, or even the client can hook new methods on the pipeline (a tribute to the Open-Closed Principle).", "A limit of the simple pipeline we just presented is that the transformations act on an item-by-item basis. However, many operations are more efficient by acting on batches. For example, imagine a remote API call that gives you information you need to process your item. If the API provides a bulk endpoint to send a collection of items and receive a collection of results, that will be a lot faster than calling the API item by item. Just think how long it takes to send the HTTP request and wait for the response before you can proceed with the next item. In our simple pipeline we can\u2019t process batches because the method receives a single item and has to return a transformed item right away.", "A way of solving this could be to make each transformation to receive a batch and return a batch. The problem would be that the most convenient batch size might differ from method to method. Method A might work better with 500 items batches but method B can only swallow 100. Also, by forcing batches upon everybody, simple methods (like a regular expression substitution) will get more complex by having to unpack the input batch, do their thing, and then assemble an output batch.", "And that\u2019s exactly why an iterator chain is so convenient in this case. In an iterator chain each transformation receives an iterable of items and returns an iterable of transformed items. We\u2019ll see how this patterns allows to implement batching while keeping the code clean.", "Let\u2019s start with a real life, yet simple example. Reading from a remote DB server is an expensive operation if you do it item by item. It\u2019s way faster to fetch by batches. Before you read the example you might wanna know I\u2019m using the psycopg library cursors to access a PostgreSQL DB. Also, I assume there\u2019s a format_item function that will build the item object from the table data.", "If you haven\u2019t seen it before, the \u201cyield from iterable\u201d statement is a short form of \u201cfor element in iterable: yield element\u201d. This example function is\u2026 you guess it, a generator function. It returns a generator of items. The clients of this generator will get the items one by one and won\u2019t need to know anything about the DB, the batch size or whatever details. If you wanna change the batch size, nothing happens (aside of performance issues) if you wanna change the DB software, nobody outside needs to know, if you even want to read the items from a file, it\u2019s all ok. That\u2019s the beauty of iterators.", "You might have noticed this method it\u2019s not what I described for an iterator chain link: it doesn\u2019t receive an iterable, although it returns one. This is because it\u2019s the first link of the chain, the original source of the data. The last link of the chain is also special, it receives an iterable but doesn\u2019t return one. So let\u2019s show a normal, intermediate link of the chain that calls a REST API. I assume you have a method called get_api_results that fetches a batch of results from the API and returns the batch of transformed items.", "This method receives an items generator and returns another items generator. I bet you can see how the batching works after reading the code, but no gossip client will have to know.", "As we said, the final method of the chain is also special. This will just consume the generator by iterating over it until exhaustion, and will write results somewhere (another PostgreSQL DB for instance). Batching on this method will work just the same.", "At the same time, with this approach the methods that don\u2019t need batches can remain simple:", "We didn\u2019t mention it before, but the iterator pipeline can be really efficient on memory usage. To understand this, you can depict the pipeline in your head as a cascading array of water containers (like a fountain). There\u2019s a big tank above (the items in the DB) and a big tank below (the transformed items in another DB) and the water flows from container to container. The size of each container is the batch the generator keeps inside. The memory the pipeline uses is approximately the sum of the water in all the containers. Usually, this will be a lot less than the big tank above, but when the tank is really huge it can mean a world of difference from loading all the items and processing them in memory.", "A gotcha that hit me, and you should pay attention to, is that if you exhaust the iterator in an intermediate transformation, for instance if you write something like list(items), then all the items will be loaded into memory at that spot. It could be confusing to see that a lot of items are going through the previous transformations, but aren\u2019t moving beyond that point, until all items are loaded, only then they\u2019ll be able to keep moving down the stream. You\u2019d be creating an infinite size container in the middle of your fountain, one that sucks and sucks until there\u2019s nothing else to suck and only then opens up to let the water keep flowing.", "Obviously there\u2019s a lot of room for improvement on our iterators pipeline. The pipeline can grow in complexity while keeping the functionality split into simple nice generators. You can get fancy on how to chain the generators, maybe even swapping them depending on the processed results. However, to conclude I will only mention other performance tricks you can add. Please, just remember unneeded optimization is the root of pretty much evil in programming. Optimize only when performance becomes an issue, and always make sure the optimizations make a real, sizable impact on the performance, otherwise the aren\u2019t worth it.", "If you make changes on your pipeline very often, trying this and that to get the final results you want, some heavy operations might really slow you down. Maybe you introduced a new string transformation that only affects 1% of your items but your are processing all of them on every run. A simple hack is to make the first generator selective, so it generates only the items that might need re-processing. I mean, if you know how to select those items.", "Let\u2019s say you have the same problem (the changes on the pipeline only affect a few items) but you don\u2019t know how to select those items. Then you can use a cache. In the cache you\u2019ll keep the results of heavy operations that you don\u2019t want to run for nothing.", "First, you need a way of check the equality of your items. In general it could be something like a hash of all the item\u2019s content. But you might have a particular, better solution for your case. You can take the slower methods on your pipeline and store the hash of the input item together with the data needed to build the output. When you read from the DB you can include the cache into the item, and when the item reaches the slow method, the method can check if the item\u2019s hash is the same as the cached hash. If it is, you can use the cached results, instead of performing the slow operation.", "Some Python libraries provide ways of parallelizing iterators. To be honest, I haven\u2019t tried that yet seriously, but just experimented a bit with a library called joblib that seemed to have the simplest interface. A code like this should spread the execution over 6 processors. Try and see if it works for you.", "A downside of this particular method is that loads all the items in memory, so if you\u2019re processing from a big pool of data you might need to batch the whole pipeline itself.", "You just learned the basics of implementing a data pipeline in Python using iterators, congrats!!! No less, you managed to cope with my poor writing style, praise on you.", "After this you might wanna learn more about iterators in Python but above anything, I invite you to play with this marvelous language feature and make some mistakes on your own.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "\u201cLearning is the new knowing\u201d Physicist by training, in love with programming."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fb5887c2f0ec6&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-a-data-pipeline-by-chaining-python-iterators-b5887c2f0ec6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-a-data-pipeline-by-chaining-python-iterators-b5887c2f0ec6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-a-data-pipeline-by-chaining-python-iterators-b5887c2f0ec6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-a-data-pipeline-by-chaining-python-iterators-b5887c2f0ec6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----b5887c2f0ec6--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b5887c2f0ec6--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://armando-rivero.medium.com/?source=post_page-----b5887c2f0ec6--------------------------------", "anchor_text": ""}, {"url": "https://armando-rivero.medium.com/?source=post_page-----b5887c2f0ec6--------------------------------", "anchor_text": "Armando Rivero"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb3fb636c333&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-a-data-pipeline-by-chaining-python-iterators-b5887c2f0ec6&user=Armando+Rivero&userId=b3fb636c333&source=post_page-b3fb636c333----b5887c2f0ec6---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb5887c2f0ec6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-a-data-pipeline-by-chaining-python-iterators-b5887c2f0ec6&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb5887c2f0ec6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-a-data-pipeline-by-chaining-python-iterators-b5887c2f0ec6&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/photos/xNdPWGJ6UCQ", "anchor_text": "https://unsplash.com/photos/xNdPWGJ6UCQ"}, {"url": "https://joblib.readthedocs.io/en/latest/", "anchor_text": "joblib"}, {"url": "https://medium.com/tag/python?source=post_page-----b5887c2f0ec6---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/data-processing?source=post_page-----b5887c2f0ec6---------------data_processing-----------------", "anchor_text": "Data Processing"}, {"url": "https://medium.com/tag/iterators?source=post_page-----b5887c2f0ec6---------------iterators-----------------", "anchor_text": "Iterators"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb5887c2f0ec6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-a-data-pipeline-by-chaining-python-iterators-b5887c2f0ec6&user=Armando+Rivero&userId=b3fb636c333&source=-----b5887c2f0ec6---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb5887c2f0ec6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-a-data-pipeline-by-chaining-python-iterators-b5887c2f0ec6&user=Armando+Rivero&userId=b3fb636c333&source=-----b5887c2f0ec6---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb5887c2f0ec6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-a-data-pipeline-by-chaining-python-iterators-b5887c2f0ec6&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b5887c2f0ec6--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fb5887c2f0ec6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-a-data-pipeline-by-chaining-python-iterators-b5887c2f0ec6&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----b5887c2f0ec6---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----b5887c2f0ec6--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----b5887c2f0ec6--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----b5887c2f0ec6--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----b5887c2f0ec6--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----b5887c2f0ec6--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----b5887c2f0ec6--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----b5887c2f0ec6--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----b5887c2f0ec6--------------------------------", "anchor_text": ""}, {"url": "https://armando-rivero.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://armando-rivero.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Armando Rivero"}, {"url": "https://armando-rivero.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "38 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb3fb636c333&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-a-data-pipeline-by-chaining-python-iterators-b5887c2f0ec6&user=Armando+Rivero&userId=b3fb636c333&source=post_page-b3fb636c333--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F22b233be652a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-a-data-pipeline-by-chaining-python-iterators-b5887c2f0ec6&newsletterV3=b3fb636c333&newsletterV3Id=22b233be652a&user=Armando+Rivero&userId=b3fb636c333&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}