{"url": "https://towardsdatascience.com/mathematical-intuition-behind-gradient-descent-f1b959a59e6d", "time": 1682996123.402663, "path": "towardsdatascience.com/mathematical-intuition-behind-gradient-descent-f1b959a59e6d/", "webpage": {"metadata": {"title": "Mathematical Intuition behind Gradient Descent | by Shahbaz Khan | Towards Data Science", "h1": "Mathematical Intuition behind Gradient Descent", "description": "If you\u2019ve ever seen or heard of the term \u2018Gradient Descent\u2019 in your life, you must have most certainly come across the following equation: In the above equation L is a loss function (or a cost\u2026"}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/Taylor_series", "anchor_text": "Taylor\u2019s series", "paragraph_index": 15}, {"url": "https://medium.com/p/f1b959a59e6d#b45d", "anchor_text": "general learning algorithm", "paragraph_index": 24}, {"url": "https://medium.com/p/f1b959a59e6d#d66d", "anchor_text": "what we set out to derive", "paragraph_index": 25}], "all_paragraphs": ["Gradient descent is an iterative optimization algorithm for finding the minimum of a function, most commonly used in Machine Learning and Deep Learning.", "If you\u2019ve ever seen or heard of the term \u2018Gradient Descent\u2019 in your life, you must have most certainly come across the following equation:", "In the above equation L is a loss function (or a cost function) and \u03b8 is any parameter on which the cost function depends. These are weights(W) and biases(b) in case of Neural Networks (or Deep Learning).", "The goal is to find the global minima of the loss function.These parameters are updated during each iteration of training algorithm until we reach a minimum value of the loss function.", "In the context of Deep Learning (or Neural Networks), we write the above equations in terms of weights and biases, as:", "This is a fundamental step in Gradient Descent optimization algorithm, that is executed during each iteration of training.Let\u2019s derive this equation mathematically (don\u2019t panic\u00a0! all you need is high school basic calculus to do this). After doing this, whenever you encounter the parameter update step you would know of its origins, and feel more empowered\u00a0!", "Let\u2019s first quickly review the general Learning Algorithm:", "Here \u2018till satisfied\u2019 is a subjective condition and could be one of the many stopping criteria such as reaching threshold loss value or iterating some fixed number of times, etc.", "Note that the update step involves adding adding some change \u0394w, \u0394b to w,b. We will soon find out that those are indeed negative partial derivatives of loss w.r.t w,b respectively i.e - \ud835\udeffL/\ud835\udeffw and - \ud835\udeffL/\ud835\udeffb where L = f(w,b).", "Note that we have not introduced learning rate \u03b1 yet. Let\u2019s first understand the need of learning rates.", "We know that \u03b8 is a vector, and so is \u0394\u03b8.Let us consider the sum of these two vectors:", "It is evident that the resultant sum of the two vectors is quite large compared to individual vectors. This is because we are taking a large step \u0394\u03b8. We need to take small steps in that direction so that vector sum is small. This is also important because if we make such large updates (\u0394\u03b8) to parameters \u03b8, we might miss out on global minimum of loss function L. Hence we introduce learning rate that limits the size of update we make to parameters \u03b8.", "Note how, with the help of learning rate \u03b1 < 1, we limit the amount of update we make to \u03b8.", "Let us now find out the correct value of \u0394\u03b8 that will reduce the value of loss.", "Before we move ahead, let me introduce the (in)famous Taylor Series, which we will use to find \u0394w, \u0394b and hence \u0394\u03b8", "Taylor\u2019s series is used to find the value of a function at a distance \u0394x from a point x, given the derivatives of the function at that point.", "Let\u2019s find the value of \u0394w using Taylor\u2019s series.In this case, function f will be loss function L, and we will expand the series for L(w + \u03b1*\u0394w).We have to find a value of \u0394w such that L(w + \u03b1*\u0394w) < L(w).", "At this step, we can infer that we\u2019d need second term onwards to be negative for the new loss to be smaller than the old loss.", "But Loss L(\u03b8) is a multivariate function. It is a function of not just weight w, but also bias b. We have represented them as a vector \u03b8 = [w, b].So we need to write down the vector form of Taylor series to find \u0394\u03b8.", "Here \u2207 L(\u03b8) represents first-order gradient of loss w.r.t \u03b8.Gradient is nothing but a vector of partial derivatives of the function w.r.t each of its parameters.Similarly \u2207\u00b2 will be a vector of second-order partial derivatives and so on.", "In practice learning rate \u03b1 is very small (0.001, 0.00001, etc.), so \u03b1\u00b2, \u03b1\u00b3, \u2026 will be very small and their contribution to the loss L will be negligible.So they can be ignored from the equation.The final equation will then become", "Since we want the updated loss L(\u03b8 + \u03b1\u0394\u03b8) to be less than the previous loss L(\u03b8), and since loss is a positive quantity, the second term in above equation has to be negative.So we need such value of \u0394\u03b8 so that the dot product in second term becomes negative, i.e we need", "Now we want the dot product to be as negative as possible (so that loss can be as low as possible)But as we can see from the above inequality, the most negative value it can acquire is -k.", "Since the two vectors are in opposite direction, from vector properties we know", "Now if we substitute these values in the parameter update step in general learning algorithm, it becomes", "Now that equation, my friend, is exactly similar to what we set out to derive.Each time you update your parameters (w and b) using this rule, the loss on your training set will decrease, until it cannot reduce further i.e when slopes(or partial derivatives) become 0.", "We have derived the update rule for a single weight and bias.In reality a deep neural network has a lot of weights and biases, which are represented as matrices (or tensors), and so our update rule should also be modified to update all weights and biases of the network simultaneously.", "Note that most deep learning texts use the notation \u0394 instead of \u2207 to represent gradient in equations.", "That is all for this one. I hope after reading this, you have developed a more intuitive understanding of gradient descent algorithm.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Ff1b959a59e6d&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmathematical-intuition-behind-gradient-descent-f1b959a59e6d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmathematical-intuition-behind-gradient-descent-f1b959a59e6d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmathematical-intuition-behind-gradient-descent-f1b959a59e6d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmathematical-intuition-behind-gradient-descent-f1b959a59e6d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----f1b959a59e6d--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f1b959a59e6d--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@shahbazkhan185?source=post_page-----f1b959a59e6d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@shahbazkhan185?source=post_page-----f1b959a59e6d--------------------------------", "anchor_text": "Shahbaz Khan"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe24ea3f6b2fc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmathematical-intuition-behind-gradient-descent-f1b959a59e6d&user=Shahbaz+Khan&userId=e24ea3f6b2fc&source=post_page-e24ea3f6b2fc----f1b959a59e6d---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff1b959a59e6d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmathematical-intuition-behind-gradient-descent-f1b959a59e6d&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff1b959a59e6d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmathematical-intuition-behind-gradient-descent-f1b959a59e6d&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://en.wikipedia.org/wiki/Taylor_series", "anchor_text": "Taylor\u2019s series"}, {"url": "https://medium.com/p/f1b959a59e6d#b45d", "anchor_text": "general learning algorithm"}, {"url": "https://medium.com/p/f1b959a59e6d#d66d", "anchor_text": "what we set out to derive"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----f1b959a59e6d---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----f1b959a59e6d---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/mathematics?source=post_page-----f1b959a59e6d---------------mathematics-----------------", "anchor_text": "Mathematics"}, {"url": "https://medium.com/tag/data-science?source=post_page-----f1b959a59e6d---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----f1b959a59e6d---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff1b959a59e6d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmathematical-intuition-behind-gradient-descent-f1b959a59e6d&user=Shahbaz+Khan&userId=e24ea3f6b2fc&source=-----f1b959a59e6d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff1b959a59e6d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmathematical-intuition-behind-gradient-descent-f1b959a59e6d&user=Shahbaz+Khan&userId=e24ea3f6b2fc&source=-----f1b959a59e6d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff1b959a59e6d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmathematical-intuition-behind-gradient-descent-f1b959a59e6d&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f1b959a59e6d--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Ff1b959a59e6d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmathematical-intuition-behind-gradient-descent-f1b959a59e6d&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----f1b959a59e6d---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----f1b959a59e6d--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----f1b959a59e6d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----f1b959a59e6d--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----f1b959a59e6d--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----f1b959a59e6d--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----f1b959a59e6d--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----f1b959a59e6d--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----f1b959a59e6d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@shahbazkhan185?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@shahbazkhan185?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Shahbaz Khan"}, {"url": "https://medium.com/@shahbazkhan185/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "18 Followers"}, {"url": "http://difference-engine.ai", "anchor_text": "difference-engine.ai"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe24ea3f6b2fc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmathematical-intuition-behind-gradient-descent-f1b959a59e6d&user=Shahbaz+Khan&userId=e24ea3f6b2fc&source=post_page-e24ea3f6b2fc--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Fe24ea3f6b2fc%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmathematical-intuition-behind-gradient-descent-f1b959a59e6d&user=Shahbaz+Khan&userId=e24ea3f6b2fc&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}