{"url": "https://towardsdatascience.com/urban-sound-classification-with-librosa-nuanced-cross-validation-5b5eb3d9ee30", "time": 1683012056.144004, "path": "towardsdatascience.com/urban-sound-classification-with-librosa-nuanced-cross-validation-5b5eb3d9ee30/", "webpage": {"metadata": {"title": "Urban Sound Classification with Librosa \u2014 tricky cross-validation | by Marc Kelechava | Towards Data Science", "h1": "Urban Sound Classification with Librosa \u2014 tricky cross-validation", "description": "Learn how to implement a research paper on a multi-class audio classification problem. Uses the librosa library and custom cross-validation splitting with sci-kit learn Leave One Group Out."}, "outgoing_paragraph_urls": [{"url": "https://urbansounddataset.weebly.com/taxonomy.html", "anchor_text": "urban sound taxonomy", "paragraph_index": 1}, {"url": "https://urbansounddataset.weebly.com/download-urbansound8k.html", "anchor_text": "https://urbansounddataset.weebly.com/download-urbansound8k.html", "paragraph_index": 5}, {"url": "http://www.justinsalamon.com/uploads/4/3/9/4/4394963/salamon_urbansound_acmmm14.pdf", "anchor_text": "paper", "paragraph_index": 20}, {"url": "https://towardsdatascience.com/audio-genre-classification-with-python-oop-66119e10cd05", "anchor_text": "I recently wrote another blog post", "paragraph_index": 22}, {"url": "https://towardsdatascience.com/audio-genre-classification-with-python-oop-66119e10cd05", "anchor_text": "blog post", "paragraph_index": 24}, {"url": "https://urbansounddataset.weebly.com/urbansound8k.html#10foldCV", "anchor_text": "10-fold cross validation using the provided folds", "paragraph_index": 44}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LeaveOneGroupOut.html", "anchor_text": "the sklearn docs", "paragraph_index": 49}, {"url": "http://www.justinsalamon.com/uploads/4/3/9/4/4394963/salamon_urbansound_acmmm14.pdf", "anchor_text": "A Dataset and Taxonomy for Urban Sound Research", "paragraph_index": 65}, {"url": "http://dl.acm.org/citation.cfm?id=2655045", "anchor_text": "ACM", "paragraph_index": 65}, {"url": "http://www.justinsalamon.com/uploads/4/3/9/4/4394963/salamon_urbansound_acmmm14.pdf", "anchor_text": "PDF", "paragraph_index": 65}, {"url": "http://www.justinsalamon.com/uploads/4/3/9/4/4394963/salamon_urbansound_acmmm14.bib", "anchor_text": "BibTeX", "paragraph_index": 65}], "all_paragraphs": ["The goal of this post is two-fold:", "\u201cThis dataset contains 8732 labeled sound excerpts (<=4s) of urban sounds from 10 classes: air_conditioner, car_horn, children_playing, dog_bark, drilling, engine_idling, gun_shot, jackhammer, siren, and street_music. The classes are drawn from the urban sound taxonomy.\u201d", "I\u2019ll extract features from these sound excerpts and fit a classifier to predict one of the 10 classes. Let\u2019s get started!", "I\u2019ve created a repo that allows you to re-create my example in full:", "The script runner handles loading the source audio from disk, parsing the metadata about the source audio, and passing this information to the feature extractor and the model.", "You can download the data, which extracts to 7.09GB, using this form from the research paper authors: https://urbansounddataset.weebly.com/download-urbansound8k.html", "Obviously you can fork the code and re-map it to whatever directory structure you want, but if you want to follow mine:", "Within the code, I use some methods to automatically write the extracted feature vectors for each audio file into ~/projects/urban_sound_classification/data", "I do this because the feature extraction takes a long time and you won\u2019t want to do it twice. There\u2019s also code that checks to see if these feature vectors exist.", "tl;dr \u2014 if you follow my directory structure, you can simply run the main.py script and everything should work!", "Note that the source data is split up into 10 sub-folders, labeled \u2018Fold1\u2019, \u2018Fold2\u2019, etc.", "We have 8,732 four-second audio clips of various city sounds. These clips were manually created by the research paper authors, where they labeled them into groups such as \u2018car horn\u2019, \u2018jackhammer\u2019, \u2018children playing\u2019, and so on. In addition to the 10 folds, there are 10 classes.", "The fold numbers do not have anything to do with the class labels; rather, the folds refer to the uncut audio file(s) that these 4-second training examples were spliced from.", "What we don\u2019t want is for the model to be able to learn how to classify things based on aspects of the particular underlying recording.", "We want a generalizable classifier that will work with a wide array of recording types, but that still classifies the sounds correctly.", "That\u2019s why the authors have pre-built folds for us, and offered the following guidance, which is worth quoting:", "Don\u2019t reshuffle the data! Use the predefined 10 folds and perform 10-fold (not 5-fold) cross validation\u2026", "\u2026If you reshuffle the data (e.g. combine the data from all folds and generate a random train/test split) you will be incorrectly placing related samples in both the train and test sets, leading to inflated scores that don\u2019t represent your model\u2019s performance on unseen data. Put simply, your results will be wrong.", "Note that the research paper does not have any code examples. What I want to do is first see if I can re-create (more or less) the results from the paper with my own implementation.", "Then if that looks in line, I\u2019ll work on some model improvements to see if I can beat it.", "Here\u2019s a snapshot of their model accuracy across folds from the paper [their image, not mine]:", "Thus we\u2019d like to get up to the high 60%/low 70% accuracy across the folds as shown in 3a.", "Librosa is an excellent and easy to use Python library that implements music information retrieval techniques. I recently wrote another blog post on a model using the librosa library here. The goal of that exercise was to train an audio genre classifier on labeled audio files (label=music genre) from my personal library. Then I use that trained model to predict the genre for other untagged files in my music library.", "I will use some of the music information retrieval techniques I learned from that exercise and apply them to audio feature extraction for the city sound classification problem. In particular I\u2019ll use:", "[My other blog post expands on some of this section in a bit more detail if any of this is of particular interest]", "Note that it is technically possible to convert a raw audio source to a numerical vector and train that directly. However, a (downsampled) 7-minute audio file will yield a time series vector nearly ~9,000,000 floating point numbers in length!", "Even for our 4-second clips, the raw time series representation is a vector ~7000-dim. Given we only have 8,732 training examples, this is likely too high-dim to be workable.", "The various music informational retrieval techniques reduce the dimensionality of the raw audio vector representation and make this more tractable for modeling.", "The techniques that we\u2019ll be using to extract features seek to capture different qualities about the audio over time. For instance, the MFCCs describe the spectral envelope [amplitude spectrum] of a sound. Using librosa we get this information over time \u2014 i.e., we get a matrix!", "The MFCC matrix for a particular audio file will have coefficients on the y-axis and time on the x-axis. Thus we want to summarize these coefficients over time (across the x-axis, or axis=1 in numpy land). Say we take an average over time \u2014 then we get the average value for each MFCC coefficient across time, i.e., a feature vector of numbers for that particular audio file!", "What we can do is repeat this process for different music informational retrieval techniques, or different summary statistics. For instance, the spectral contrast technique will also yield a matrix of different spectral characteristics for different frequency ranges over time. Again we can repeat the aggregation process over time and pack it into our feature vector.", "The paper authors call out MFCC explicitly. They mention pulling the first 25 MFCC coefficients and", "\u201cThe per-frame values for each coefficient are summarized across time using the following summary statistics: minimum, maximum, median, mean, variance, skewness, kurtosis and the mean and variance of the first and second derivatives, resulting in a feature vector of dimension 225 per slice.\u201d", "Thus in their case they kept aggregating the 25 MFCCs over different summary statistics and packed them into a feature vector.", "I\u2019m going to implement something slightly different here, since it worked quite well for me in the genre classifier problem mentioned previously.", "My output (for each audio clip) will only be 82-dimensional as opposed to the 225-dim of the paper, so modeling should be quite a bit faster.", "[Note that I\u2019ll be posting code snippets both within the blog post and with GitHub Gist links. Sometime Medium does not render Github Gists correctly, which is why I\u2019m doing this. Also all the in-document code is copy and pasteable to an ipython terminal, but GitHub gists are not].", "Referring to my script runner here:", "I parse through the metadata (given with the dataset) and grab the filename, fold, and class label for each audio file. Then this gets sent to an audio feature extractor class.", "The AudioFeature class wraps around librosa, and extracts the features you feed in as strings as shown above. It also then saves the AudioFeature object to disk for every audio clip. The process takes a while, so I save the class label and fold number in the AudioFeature object along with the feature vector. This way you can come back and play around with the model later on the extracted features.", "This class implements what I described earlier \u2014 which is aggregating the various music information retrieval techniques over time, and then packing everything into a single feature vector for each audio clip.", "Since we put all the AudioFeature objects in a list above, we can do some quick comprehensions to get what we need for modeling:", "The Model class will implement the cross-validation loop as described by the authors (keeping the relevant pitfalls in mind!).", "As a reminder, here\u2019s a second warning from the authors:", "\u201cDon\u2019t evaluate just on one split! Use 10-fold (not 5-fold) cross validation and average the scoresWe have seen reports that only provide results for a single train/test split, e.g. train on folds 1\u20139, test on fold 10 and report a single accuracy score. We strongly advise against this. Instead, perform 10-fold cross validation using the provided folds and report the average score.", "Not all the splits are as \u201ceasy\u201d. That is, models tend to obtain much higher scores when trained on folds 1\u20139 and tested on fold 10, compared to (e.g.) training on folds 2\u201310 and testing on fold 1. For this reason, it is important to evaluate your model on each of the 10 splits and report the average accuracy.", "Again, your results will NOT be comparable to previous results in the literature.\u201d", "On their latter point \u2014(this is from the paper) it\u2019s worth noting that different recordings/folds have different distributions of when the snippets appear in either the foreground or the background \u2014this is why some folds are easy and some are hard.", "Initially, I coded the split process described above by hand using numpy with respect to the given folds. While it wasn\u2019t too bad, I realized that scikit-learn provides a perfect solution in the form of LeaveOneGroupOut KFold splitting.", "To prove to myself it is what we want, I ran a slightly altered version of the test code for the splitter from the sklearn docs:", "When I feed the group membership list for each training example to the splitter, it correctly ensures that the same group examples never appear in both train and test.", "Thanks to sklearn this ends up being pretty easy to implement!", "Here I add in some scaling, but in essence the splitter will give us the desired CV. After each iteration of the splitter I train the fold on 9 folds and predict on the holdout fold. This happens 10 times, and then we can average over the returned list of 10 scores on the holdout folds.", "69.5% is about right in line with what the authors have in their paper for the top models! Thus I\u2019m feeling good that this was implemented as they envisioned. Also note they also show that fold10 was the easiest to score on (and we have that too), so we\u2019re in line there as well.", "Here\u2019s where things get a little tricky.", "If we could train/test/split arbitrarily, we could do something like:", "Because we have the validation set in part 5, we can repeat steps 3 and 4 a bunch of times on different model families or different parameter search ranges.", "Then when we are done we\u2019d take our final model and see if it generalizes using the holdout test set, which we hadn\u2019t touched to that point.", "But how is this going to work within our Fold based LeaveOneGroupOut approach? Imagine we tried to setup a GridSearchCV as follows:", "But now when GridSearchCV runs the inner split, we\u2019ll run into the same problem that we had solved by using LeaveOneGroupOut!", "That is, imagine the first run of this loop where the test set is fold 1 and the train set is on folds 2\u201310. If we then pass the train set (of folds 2\u201310) into the inner GridSearchCV loop, we\u2019ll end up with inner KFold cases where the same fold is used in the inner GridSearchCV train and the inner GridSearchCV test.", "Thus it\u2019s going to end up (very likely) overfitting the choice of best params within the inner GridSearchCV loop.", "And hence, I\u2019m not going to run a hyperparameter search within the LeaveOneGroupOut loop.", "I\u2019m pretty pleased this correctly implemented the research paper \u2014 at least in terms of very closely matching their results.", "Thanks for reading this far! I intend to do a 2nd part of this post addressing the Next Steps soon. Some other work that might be of interest can be found here:", "J. Salamon, C. Jacoby and J. P. Bello, \u201cA Dataset and Taxonomy for Urban Sound Research\u201d, 22nd ACM International Conference on Multimedia, Orlando USA, Nov. 2014.[ACM][PDF][BibTeX]", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F5b5eb3d9ee30&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Furban-sound-classification-with-librosa-nuanced-cross-validation-5b5eb3d9ee30&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Furban-sound-classification-with-librosa-nuanced-cross-validation-5b5eb3d9ee30&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Furban-sound-classification-with-librosa-nuanced-cross-validation-5b5eb3d9ee30&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Furban-sound-classification-with-librosa-nuanced-cross-validation-5b5eb3d9ee30&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----5b5eb3d9ee30--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5b5eb3d9ee30--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@marckelechava?source=post_page-----5b5eb3d9ee30--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@marckelechava?source=post_page-----5b5eb3d9ee30--------------------------------", "anchor_text": "Marc Kelechava"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa0cc3baa6435&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Furban-sound-classification-with-librosa-nuanced-cross-validation-5b5eb3d9ee30&user=Marc+Kelechava&userId=a0cc3baa6435&source=post_page-a0cc3baa6435----5b5eb3d9ee30---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5b5eb3d9ee30&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Furban-sound-classification-with-librosa-nuanced-cross-validation-5b5eb3d9ee30&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5b5eb3d9ee30&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Furban-sound-classification-with-librosa-nuanced-cross-validation-5b5eb3d9ee30&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "http://www.justinsalamon.com/uploads/4/3/9/4/4394963/salamon_urbansound_acmmm14.pdf", "anchor_text": "http://www.justinsalamon.com/uploads/4/3/9/4/4394963/salamon_urbansound_acmmm14.pdf"}, {"url": "https://urbansounddataset.weebly.com/urbansound8k.html", "anchor_text": "https://urbansounddataset.weebly.com/urbansound8k.html"}, {"url": "https://urbansounddataset.weebly.com/taxonomy.html", "anchor_text": "urban sound taxonomy"}, {"url": "https://github.com/marcmuon/urban_sound_classification/blob/master/main.py", "anchor_text": "https://github.com/marcmuon/urban_sound_classification/blob/master/main.py"}, {"url": "https://github.com/marcmuon/urban_sound_classification/blob/master/audio.py", "anchor_text": "https://github.com/marcmuon/urban_sound_classification/blob/master/audio.py"}, {"url": "https://github.com/marcmuon/urban_sound_classification/blob/master/model.py", "anchor_text": "https://github.com/marcmuon/urban_sound_classification/blob/master/model.py"}, {"url": "https://urbansounddataset.weebly.com/download-urbansound8k.html", "anchor_text": "https://urbansounddataset.weebly.com/download-urbansound8k.html"}, {"url": "http://www.justinsalamon.com/uploads/4/3/9/4/4394963/salamon_urbansound_acmmm14.pdf", "anchor_text": "paper"}, {"url": "https://towardsdatascience.com/audio-genre-classification-with-python-oop-66119e10cd05", "anchor_text": "I recently wrote another blog post"}, {"url": "https://librosa.org/doc/latest/generated/librosa.feature.mfcc.html#librosa.feature.mfcc", "anchor_text": "Mel-Frequency Cepstral Coefficients (MFCC)"}, {"url": "https://librosa.org/doc/latest/generated/librosa.feature.spectral_contrast.html", "anchor_text": "Spectral Contrast"}, {"url": "https://librosa.org/doc/latest/generated/librosa.feature.chroma_stft.html", "anchor_text": "Chromagram"}, {"url": "https://towardsdatascience.com/audio-genre-classification-with-python-oop-66119e10cd05", "anchor_text": "blog post"}, {"url": "https://github.com/marcmuon/urban_sound_classification/blob/master/main.py", "anchor_text": "marcmuon/urban_sound_classificationCode for a series of blog posts on models using the Urban Sound 8K dataset - marcmuon/urban_sound_classificationgithub.com"}, {"url": "https://urbansounddataset.weebly.com/urbansound8k.html#10foldCV", "anchor_text": "10-fold cross validation using the provided folds"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LeaveOneGroupOut.html", "anchor_text": "the sklearn docs"}, {"url": "https://github.com/marcmuon/audio_genre_classification/blob/master/model.py#L84-L128", "anchor_text": "https://github.com/marcmuon/audio_genre_classification/blob/master/model.py#L84-L128"}, {"url": "https://github.com/marcmuon", "anchor_text": "https://github.com/marcmuon"}, {"url": "https://medium.com/@marckelechava", "anchor_text": "https://medium.com/@marckelechava"}, {"url": "http://www.justinsalamon.com/uploads/4/3/9/4/4394963/salamon_urbansound_acmmm14.pdf", "anchor_text": "A Dataset and Taxonomy for Urban Sound Research"}, {"url": "http://dl.acm.org/citation.cfm?id=2655045", "anchor_text": "ACM"}, {"url": "http://www.justinsalamon.com/uploads/4/3/9/4/4394963/salamon_urbansound_acmmm14.pdf", "anchor_text": "PDF"}, {"url": "http://www.justinsalamon.com/uploads/4/3/9/4/4394963/salamon_urbansound_acmmm14.bib", "anchor_text": "BibTeX"}, {"url": "https://medium.com/tag/data-science?source=post_page-----5b5eb3d9ee30---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----5b5eb3d9ee30---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/python?source=post_page-----5b5eb3d9ee30---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/statistics?source=post_page-----5b5eb3d9ee30---------------statistics-----------------", "anchor_text": "Statistics"}, {"url": "https://medium.com/tag/editors-pick?source=post_page-----5b5eb3d9ee30---------------editors_pick-----------------", "anchor_text": "Editors Pick"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5b5eb3d9ee30&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Furban-sound-classification-with-librosa-nuanced-cross-validation-5b5eb3d9ee30&user=Marc+Kelechava&userId=a0cc3baa6435&source=-----5b5eb3d9ee30---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5b5eb3d9ee30&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Furban-sound-classification-with-librosa-nuanced-cross-validation-5b5eb3d9ee30&user=Marc+Kelechava&userId=a0cc3baa6435&source=-----5b5eb3d9ee30---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5b5eb3d9ee30&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Furban-sound-classification-with-librosa-nuanced-cross-validation-5b5eb3d9ee30&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5b5eb3d9ee30--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F5b5eb3d9ee30&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Furban-sound-classification-with-librosa-nuanced-cross-validation-5b5eb3d9ee30&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----5b5eb3d9ee30---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----5b5eb3d9ee30--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----5b5eb3d9ee30--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----5b5eb3d9ee30--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----5b5eb3d9ee30--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----5b5eb3d9ee30--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----5b5eb3d9ee30--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----5b5eb3d9ee30--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----5b5eb3d9ee30--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@marckelechava?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@marckelechava?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Marc Kelechava"}, {"url": "https://medium.com/@marckelechava/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "432 Followers"}, {"url": "https://www.linkedin.com/in/marckelechava/", "anchor_text": "https://www.linkedin.com/in/marckelechava/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa0cc3baa6435&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Furban-sound-classification-with-librosa-nuanced-cross-validation-5b5eb3d9ee30&user=Marc+Kelechava&userId=a0cc3baa6435&source=post_page-a0cc3baa6435--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F6e9afdb68ae&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Furban-sound-classification-with-librosa-nuanced-cross-validation-5b5eb3d9ee30&newsletterV3=a0cc3baa6435&newsletterV3Id=6e9afdb68ae&user=Marc+Kelechava&userId=a0cc3baa6435&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}