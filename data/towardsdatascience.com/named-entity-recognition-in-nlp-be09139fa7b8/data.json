{"url": "https://towardsdatascience.com/named-entity-recognition-in-nlp-be09139fa7b8", "time": 1683010595.241696, "path": "towardsdatascience.com/named-entity-recognition-in-nlp-be09139fa7b8/", "webpage": {"metadata": {"title": "Named Entity Recognition in NLP. Real-world use cases, models, methods\u2026 | by Arun Jagota | Towards Data Science", "h1": "Named Entity Recognition in NLP", "description": "Detect and extract specific entities in text. People, places, things, medical terms, sports names, dates, ... Covers a variety of use cases. Covers methods: dictionary-based, pattern-based, probabilistic, ML."}, "outgoing_paragraph_urls": [], "all_paragraphs": ["In natural language processing, named entity recognition (NER) is the problem of recognizing and extracting specific types of entities in text. Such as people or place names. In fact, any concrete \u201cthing\u201d that has a name. At any level of specificity. Job titles, public school names, sport names, music album names, musician names, music genres, \u2026 You get the idea.", "As we will see below, NER is both an interesting problem in NLP and also has many applications. First, let\u2019s see a concrete example.", "Let\u2019s muse on how we might use the extracted entities in the above example (and imagined extensions). This will reveal some ideas that will play out in various use cases of NER. We\u2019ll then discuss the use cases \u2014 \u201con the shoulders\u201d of these ideas.", "To discover the text\u2019s subject: This example is about the sport of tennis and the two players. It seems less about the country (UK), or the month (July), or the year (2019). From an entire article, i.e. a lot more than two sentences, we might be able to sharpen (or refute) this assessment.", "For example, if Novak Djokovic was frequently mentioned in the article, it might be about him. On the other hand, if tennis was mentioned frequently whereas each player only a few times, the article may be more about tennis. Even more so if there were other person names also mentioned in the article, each only a few times.", "If 2019 was mentioned repeatedly in the article, perhaps the article is about the tournament that year.", "To discover relationships among entities: Say we had a large corpus. Say the person name Roger Federer occurs frequently in it within the proximity of the sport tennis. It would be reasonable to infer that the two are related. In fact, this inference is just a specific instance of the type:", "In our case, the qualification is that we are limiting ourselves to entities, not to arbitrary words or phrases in the text. Plus we know the entity type of each.", "What strength should we attach to an inferred association? We won\u2019t go into details here. Google association rules, confidence, and lift if you are curious.", "So, from a large corpus and powered by a high-quality NER approach, we can build what is called an entity graph. The graph\u2019s nodes are entities, with attached attributes such as entity types. The graph is directed. Each arc on the graph has an attribute that captures the strength of the directed association. Arcs may have additional meta-data.", "Next, let\u2019s discuss specific use cases.", "An important one is in web search. By understanding the entities in a query, search engines deliver better results. Google uses this approach.", "Let\u2019s discuss this in more detail. Traditionally, search engines use an approach called keyword search. The document is indexed by the words that appear in it. The document is scored for relevance to a query by computing which query words appear in the document and weighing each hit suitably. A hit, i.e. a query word that appears in the document, is weighed by a factor called TF-IDF. This is based on the frequency of the word in the document adjusted for its rarity in the corpus. So if the query contains an uncommon word and this word appears very frequently in the document, the document\u2019s relevance to the query is high. Makes sense right?", "Keywords search is definitely very useful. However, it can be improved further by augmenting it with entity recognition. This uses, in addition, knowledge about the main entities found in the document together with judgments on how significant of a role they play in that document.", "Beyond improving relevance, this approach has another major benefit. If the query contains recognized entities, the search engine can in addition return key facts about them. The reader might have seen this at Google. Enter Barack Obama as the search term. The search engine understands this entity. It uses this understanding to return other key facts about Obama. Such as those gleaned from his Wikipedia page. And pictures of him.", "Search engines backed by an entity graph (in addition to entity recognition) can improve search relevance even further. They can serve up better suggestions, e.g of the type one sees as \u201cPeople also search for\u201d.", "Another important one is in cataloging or indexing text documents such as web pages or online articles. This involves extracting the salient entities in a document and indexing the document using these. Online magazines and news publishers do this a lot. And of course search engines.", "One way to think of the relevance value of this is the following. A document is more likely to be about frequently-mentioned entities in a document than about frequently mentioned arbitrary words. That is, an entity check acts as a quality filter.", "A third use case is in extracting product names or other salient entities from online reviews in a retail setting. These reviews may be at eCommerce sites or at social media ones. Clearly it is useful to know which products and features are being discussed in a particular review. As an example, consider the review", "XYZ smartphone and camera are the entities we\u2019d want to extract. We\u2019d also want to extract the sentiment words like and sucks and associate each with the correct entity. Sentiment analysis is beyond the scope of the present post. Other than to say that sentiment could also be modeled as entities. For example, we might have two entity types positive-sentiment and negative-sentiment with like an instance of the former sucks an instance of the latter. Sentiment analysis has been studied extensively. More advanced approaches exist.", "Reviews may then be automatically cataloged under the products or other salient entities they apply to. The resulting groupings can also reveal new insights. Such as which products or features people are complaining (or praising) the most.", "A fourth use case is in detecting and extracting entities from emails and automatically triggering certain actions. Many readers might have seen this happening on their own smart devices. As an example, say you get an email from an airline confirming your purchase of a flight ticket. State-of-the-art email engines can automatically extract the relevant information (airline, departure date and time, \u2026) and create an entry in your digital calendar from it. The entities here are airline name, date, and time, with date and time tagged as departure.", "Building a structured database from a corpus", "A fifth use case is in building a structured database by recognizing entities of the desired type from a corpus. For example, we might build a database of medical disease names by scraping the web and recognizing entities of this type. (As an aside, a pure dictionary-based recognition approach wouldn\u2019t work because the result we seek is that dictionary.)", "There is a market for specialized lists of this sort. There are niche vendors who\u2019ve been making money for a while selling such lists.", "A sixth use case is essentially building an entity graph from a corpus of documents such as web pages or those internal to a company. It may also be seen as an enhanced version of the fifth use case in which relationships among entities are also inferred.", "The nodes are the recognized entities. The arcs come from pairs of entities being in sufficiently close proximity frequently enough. For finer points on arcs, such as weights, see the previous discussion.", "Beyond the uses of an entity graph mentioned earlier, there is a market for deep and rich domain-specific ontologies to be offered as data products, for example, to improve browsing and search at niche e-commerce sites. Some combination of automated entity recognition and arcs inference combined with human curation has a lot of potential. There are niche companies making money in this arena.", "Next, we discuss various approaches to recognizing and extracting entities, from the simplest to the more advanced. We also discuss their benefits and limitations. We also describe how and why to combine some of the simpler approaches that are complementary, to build more advanced ones.", "This is one of the simplest. We have a dictionary of values for every entity type to be recognized. To recognize and extract the entities we simply scan the text and find hits in the various dictionaries. A hit also reveals the entity type as we know the dictionary that was hit.", "Say we have the following dictionaries:", "Consider the text we saw earlier.", "Ah, surprise \u2014 the extraction is grossly incomplete! In fact, to a lesser extent, this happens even in real-world NER. We are getting a foretaste of reality.", "Well, let\u2019s dig deeper \u2026 First of all, the entity recognition is only as good as the dictionaries. Novak was missing from our person\u2019s first name dictionary, so it was missed. The lack of completeness of dictionaries and the challenge of maintaining them is definitely a major issue in this approach. Second, we chose to have separate dictionaries for a person\u2019s first and last name rather than a single dictionary for a person\u2019s full name. Since person first and last names are somewhat independent, the latter type of dictionary would have been huge. Even more challenging to put together and maintain. Given this choice, we would need some mechanism for combining a person\u2019s first and last names into a full name. In fact, it would likely be pattern-based. That is, we are already getting a glimpse of the utility of combining dictionary-based and pattern-based approaches.", "Another significant issue with dictionary-based approaches, albeit one that has not surfaced in our example, is that they don\u2019t accommodate ambiguity well. The same value being in multiple dictionaries. Consider Carter. This is typically a person\u2019s last name but on occasion can also be a person\u2019s first name.", "We could, of course, place it in both dictionaries. We wouldn\u2019t be capturing the probabilities that it is more likely to be the last name than a first name though. And without capturing such probabilities, the more the number of ambiguous values there are, the more \u201cpolluted\u201d the dictionaries will get. We won\u2019t be able to distinguish obscure memberships from significant ones.", "Say we wish to extract US phone numbers in some text. Such as (123) 456\u20137890. Using a dictionary-based approach doesn\u2019t make sense. That would require storing all possible US phone numbers in the dictionary. Including all formatting variations such as 123 456 7890.", "Such entities are better recognized via pattern matching. Typically, this is done using regular expressions.", "The point of this example is to reveal that the patterns to be recognized do start getting intricate. Now add to this list international phone numbers from across the globe. Now, this gets very intricate. In software engineering parlance, pattern matching in intricate scenarios has three issues involving the \u2018software\u2019, here regular expressions.", "Despite 1\u20133 above, pattern-matching is the approach to use when, well, the entity is best described by structural patterns. Examples of other such entities are email addresses, URLs, and US zip codes.", "Previously we mentioned that dictionary-based approaches don\u2019t satisfactorily accommodate ambiguity. Our example was Carter. It can be a person\u2019s first name or a person\u2019s last name. It should, therefore, go into both dictionaries. Doing so however implicitly assumes that Carter is equally likely to be a person\u2019s first name or a person\u2019s last name. This unwarranted inference can adversely affect the quality of the entity recognition results.", "Fortunately, the dictionary-based approach is easy to enhance to accommodate ambiguity. To every value in a dictionary, we attach an attribute, its frequency. If a value\u2019s frequency is unavailable, we default it to 1. The frequency is a positive number though can be a fraction.", "Here is an example. Say, we have 10 occurrences of Carter of which 2 are known to be first names and 8 as last names. (These counts are fictitious.) We could place Carter in both dictionaries and assign the frequency 0.2 in the person\u2019s first name dictionary and 0.8 in the person\u2019s last name dictionary. (Or we could just use the actual counts, 2 and 8, as the frequencies.) Think of this as Carter\u2019s default frequency of 1 proportionally distributed into the two entity types.", "Okay, so we have frequency attached to every value in a dictionary. How do we actually use it to predict a value\u2019s entity? We pick the dictionary which has the highest probability for that value. We describe this formally below.", "Consider value v. A simple empirical estimate of P(D|v) is the frequency of v in D divided by the sum of the frequencies of v over all the dictionaries.", "Let\u2019s see a numeric example. Below are three dictionaries.", "What should we predict as a\u2019s entity type? Of the 11 occurrences of a, 10 are in D1, 1 in D2, and none in D3. So D1 should have the highest probability, D2 the next highest, and D3 the lowest. So we\u2019d predict D1 to be a\u2019s entity with confidence 10/11. (Note that we are using the terms entity type and dictionary interchangeably.)", "Sometimes we have some additional prior knowledge about the dictionaries not captured in the frequencies. Or the summed frequency of some particular value v in our frequencies is so small that we are unwilling to trust our empirical probability estimate.", "Consider this example. Say we have two dictionaries: first name and last name. A reasonable prior on these is (1/2, 1/2). That is, both are equally likely to be the entity type of a particular value before we have seen it. Consider the made-up value Xincal. Say it occurs only once, in the dictionary first name. The empirical probability of first name given Xincal is 1. Should we really be that certain about Xincal\u2019s entity given that we have only seen Xincal once? And given that the first name and the last name entity types are equally abundant.", "Bayes rule lets us combine evidence from the data with prior beliefs to mitigate this issue. That is, our probability of a dictionary for a given value depends not just on our data but also on our prior beliefs.", "In more detail, we estimate the posterior using Bayes rule", "Here P(v|D) is the likelihood of value v for dictionary D. Empirically, it is simply the frequency of v in D divided by the sum of the frequencies of all values in D.", "Next, let\u2019s note that P(v) = sum_D\u2019 P(v|D\u2019)P(D\u2019). So we can compute P(v) on-the-fly during the process of inferring v\u2019s likely dictionary.", "Back To Empirical Posterior, with Pseudo-counts added", "After explicitly invoking Bayes rule it turns out we don\u2019t actually have to use it. There is a simple, intuitive approach that effectively produces the same behavior.", "Consider our (first name, last name) example. Say we have a prior belief that both entity types are equally likely. We can manifest this belief by adding, for each value v, a pseudo-count of n to each of the two dictionaries. Think of n as the initial frequency of a value in a dictionary. The actual observed frequency of that value is then added to n.", "Once we have both pseudo-counts and counts in the dictionaries we compute the empirical probability of a dictionary given a value exactly as before.", "What is n? It influences the strength of our prior beliefs. The larger n is the stronger our prior belief is. That is, the more evidence it takes for us to revise that belief. That said, even n = 1 is typically an improvement over n = 0 so often it's not worth fretting over the choice.", "Pseudo-counts are backed up by theory. In particular, they behave as if modeling a so-called Dirichlet prior. We won\u2019t say more on this topic in this post. We refer you to Google.", "Example: Consider our first name and last name example. Consider the value Xincal. Say we choose n = 1. The pseudo-frequency of Xincal in first name and last name would be 1 each. Now add the actual frequency of Xincal, which is 1 in first name and 0 in last name. So the combined frequencies are", "So the posterior of first name given Xinal is 2/3. The entity type first name still wins albeit with lower confidence than before in view of the prior belief.", "Our choices of the pseudo-counts in this example may be interpreted as capturing the following prior belief. If a value occurs only once in our data set, let\u2019s not predict the dictionary it occurs in with probability 1.", "By composite entities we mean entities that are themselves composed of other entities. Here are two types of examples:", "In each case, the vertical bar separates entity values.", "There is an important special case of composite entities that we call multi-token entities. We will cover this case separately, following the present section\u2019s (long) discussion on composite entities. We have structured the content this way because going deep into composite entities will benefit us in the subsequent discussion on multi-token entities. The reader curious about multi-token entities should scroll down to that section once they have had \u201cenough\u201d of the present one.", "When working with composite entities, it helps to distinguish between two computational problems.", "Parsing aka Decomposition: Break a composite entity down into its component entities. Here is an example.", "Recognition: Here the composite entity is embedded in some unstructured text. The task is to recognize the boundaries of the entity as well as its type. And maybe also parse, i.e. decompose, it into its constituents. Here is an example.", "We\u2019d want our recognition to produce the following results:", "One reason for distinguishing between parsing and recognition is that various use cases map to one or the other. For example, we may have a database of full person names and seek to parse each person\u2019s name into its parts (first name, last name, etc). This does not involve any recognition. Just parsing.", "Another reason is the \u201cdivide-and-conquer\u201d. Say our task is to recognize composite entities in unstructured text and parse them. A two-stage approach \u2014 first recognize the entity boundaries and their types, then parse for the recognized entity \u2014 makes sense.", "We are not implying that a two-stage approach will be more accurate than an integrated approach which simultaneously does recognition and parsing, only that it is worth considering when you quickly want to get a reasonably effective solution and you have easier means to solve the two problems separately.", "In the rest of this post, we will focus on parsing composite entities.", "First, we\u2019d like to observe that parsing composite entities may be framed as a sequence labeling problem. The composite entity is suitably tokenized, to yield a sequence of tokens. The label associated with a token is its constituent entity. Below is an example of the composite entity person\u2019s name.", "We suppose we have available a training set in the form of (token sequence, label sequence) pairs. This is similar to training sets in supervised learning in fixed-dimensional spaces except that here our inputs and labels are (aligned) sequences.", "Why model parsing of composite entities this way? The main reason is that there is information buried in the sequence of labels that can be fruitfully exploited.", "Say we have a rich data set of (token sequence, label sequence) pairs available for learning to parse person names. There are useful signals buried in just the label sequences. Salutation typically comes before first_name, not after. First_name typically comes before last_name, not after.", "Okay, so we have a training set of (token sequence, label sequence) pairs. Our task is to learn a machine learning model from this data set so that when a token sequence is an input, likely one that was never seen during training, the model outputs the best label sequence for it. Again, this is similar to supervised learning in vector spaces except that our inputs and outputs are sequences.", "This is the most widely known method for modeling sequence labeling problems. It fits our needs quite well. Having said that, it does make a strong assumption, the Markovian one. At times this can be overly limiting. The reader interested in more advanced methods or having use cases in which there are longer-range interactions (i.e. composite entities with long token sequences in which tokens far apart influence each other) should also read about conditional random fields.", "Okay, back to HMMs. We\u2019ll illustrate setting this up to parse person name entities. Structurally, an HMM is a directed acyclic graph with nodes (also called states) and arcs (also called transitions). Actually there\u2019s more. Nodes may emit observables from some alphabet.", "An HMM has parameters attached to the various nodes and the arcs. These parameters form suitable probability distributions. In more detail, emissions from a state are governed by a probability distribution over the alphabet of observables. This distribution is state-specific. Transitions from a state are governed by a probability distribution over states. These distributions are specific to the starting state. In short, from a state you have to go somewhere. The probability distribution over the transitions from the state captures our preferences of where (which state) we should go next.", "This is all a bit abstract. Let\u2019s make it concrete in our setting. This exercise will also reveal the \u2018H\u2019 in HMM, which stands for Hidden.", "We will depict an HMM for parsing a somewhat simplified version of a person\u2019s full name. Specifically, we will only parse names that are a match to the following regular expression format.", "Let\u2019s read out what this regular expression says. A full name optionally starts with a salutation, followed by a first name, then optionally a middle name, and finally the last name. All these entities must be single-word entities.", "First question, why not just use this regular expression to parse? That is, why not apply a patterns-based approach to this parsing problem. You could if your aim was to quickly develop a light-weight approach.", "The HMM-based approach is more powerful in the following ways.", "Partial parses: Consider the input Smith. The HMM-based approach would likely parse it as last_name = Smith. This is because it models entity probabilities as well. (Details below.) The patterns-based approach is incapable of doing this. It has no way of knowing that Smith is more likely to be the last name than the first name.", "Resolve ambiguities better: Consider the input Dr. Smith. The HMM-based approach would likely parse this as salutation = Dr. and last_name = Smith. The patterns-based approach would likely parse this as first_name = Dr. and last_name = Smith. Once again, since it doesn\u2019t model entity probabilities it has no way of knowing that Dr. is much more likely to be a salutation than a first name.", "Attach a parse confidence score: Consider two inputs John Smith and Smith John. As would the patterns-based approach, the HMM-based one may very well parse these as {first_name=John, last_name = Smith} and {first_name = Smith, last_name = John} respectively. The HMM-based one would likely attach higher confidence to the first parse than the second one, as it knows that John is much more likely to be the first name than the last name, and Smith much more likely to be the last name than a first name.", "Such confidence scores can be used to find low-confidence parses (and maybe treat them differently). They can also surface data issues. As in this example, in which the first and last names seem to have been reversed.", "Person Names Parsing HMM: The Structure", "Okay, let\u2019s come back to the regular expression that defines what the HMM will match.", "From this, we will determine the states of the HMM to be salutation_word, first_name_word, middle_name_word, and last_name_word. To these, we will add two more states: begin and end.", "We will use the regular expression to determine the arcs as well. Below are the arcs", "Computer scientists, think of the structure of the HMM as defining a finite-state automaton.", "Before moving off this topic, we\u2019d like to note that it is possible to infer the structure of the HMM (nodes and transitions) directly from the training set. This can be more flexible. As the training set evolves, so can the structure.", "Our main reason for fixing the structure in advance based on the regular expression is pedagogical. It allows us to separate the discussion on the HMM\u2019s structure from a description of its learning (of the probability parameters on its emissions and transitions).", "We\u2019d also like to note that even when a training set can in principle determine the structure, it is helpful to consider initializing the structure, before seeing the training set, with prior domain knowledge if available. This lets one use domain knowledge (for Bayesians, prior beliefs) into the modeling. This can compensate for weaknesses in the training set.", "Next, we will illustrate training the parameters of the HMM from a training set.", "As we mentioned earlier, the training set is a collection of (token sequence, label sequence) pairs. From such a training set, the emission and the transition probabilities are learned in the HMM. The training of these two types of probabilities decomposes, as we will see soon. We might call such a training modular. Following the training illustration, we will touch on some benefits of modular training.", "Let\u2019s see an example. It has three training instances. Notice that we have added the begin and end states to the flanks. These will play a role in the training, as we will see soon.", "The begin and end states are called silent because they don\u2019t emit any tokens.", "For training these, we only need to look at the state sequences. It is as if our training set was", "The probability of the transition from the state a to state b, i.e. on the arc a \u2192 b, is simply the fraction of occurrences of state a (excluding any as the last state in a state sequence) in which it was succeeded by state b.", "Let\u2019s see some of the learned transition probabilities.", "One of three occurrences of the state begin is followed by the state salutation. Two of three occurrences of the state first_name are followed by the state middle_name, one by the state last_name.", "For training these, we throw away the sequence information and just track (state, token) pairs every time token token is emitted from state state. It is as if the training set were", "The probability of emitting token t from state s is just the fraction of occurrences of state s in which token t was emitted.", "The number of different values a state can plausibly emit is quite large. For example, state first_name can emit millions of different values. Each needs to be explicitly modeled, so as to distinguish first names, however rare, from gibberish strings such as xyz.", "It is unrealistic to expect that the training set of (token sequence, label sequence) pairs will be rich enough to cover all plausible first names from it. Fortunately, it doesn\u2019t have to be. The emissions can be trained independently of the transitions. So long as we have rich dictionaries of values for each entity (state), preferably augmented with frequency information, we can learn the emission probabilities from these.", "Also note that, by contrast, this explosion does not apply to learn transition probabilities. A small number of curated state sequences can adequately capture the sequence structure of most full names. \u201cSmall\u201d may not be as small as one might think if one wishes to be able to parse person names in all kinds of locales. Spanish and Arabic for example. Still, manual curation of a training set even in this situation is feasible.", "Okay, now that we have the trained HMM, or at least we can imagine it, let\u2019s walk through an example of inference. The specific aim of inference is to input a sequence of tokens and output the best parse. In order for this process to be well-defined, we need (i) a score function that evaluates the quality of a specific parse of the token sequence and (ii) a search procedure that finds a highest-scoring parse. The reason we need a search procedure is that the approach of exhaustive search \u2014 score all possible parses and pick one that scores highest \u2014 can be too slow, even when there are only a few tokens.", "The gold standard search procedure is a form of dynamic programming called the Viterbi algorithm. The score function it implicitly maximizes is the joint probability of the (token sequence, label sequence) pair. The token sequence is fixed; the label sequence is variable, one per parse. The Viterbi algorithm operates as if it was doing an exhaustive search over all possible label sequences for the given token sequence, only it is much faster.", "We will not describe the Viterbi algorithm formally here. It is too complex for this already long post. However, we will illustrate some aspects of its working on an example. Admittedly in a sketchy way. Our hope is that those reading about it for the first time will get curious enough to learn about it more from elsewhere (see a reference below), and those having already read about it will find some insight to take away from our illustration to add to their understanding.", "The input we will illustrate it on is Dr. John Smith. The first key idea we\u2019d like to depict is that the Viterbi algorithm operates by maintaining a matrix whose rows index the states of the HMM and whose columns index the tokens in the input. In our example, this matrix has the structure", "What goes into each cell of the matrix and how does it get there? That\u2019s much too complicated to fully specify and explain in the space we have allocated for it. So instead we have put values or formulae in some of the cells, which we will explain below.", "Let\u2019s start in the first column. The values there are easy to interpret. To parse an input, the HMM must start from the state begin. This is represented in the first column as \u201cthe probability of being in state begin is 1, being in in any other state is 0\u201d.", "Next, let\u2019s look at the cell [salutation, Dr.]. The value in there is the probability of parsing up to the first token and ending up in state salutation. (In our case, \u201cending up in state salutation\u201d would mean that Dr. was emitted from salutation.)", "This probability is the probability of starting from the state begin, transiting to the state salutation, and emitting Dr. from salutation. This is what we see in that cell.", "Now let\u2019s look at what goes into the cell [first_name, John]. We have placed a question-mark there because the expression is much too complicated to put in there in a way that captures all the important things that are happening under the hood. So we\u2019ll just talk through them here.", "First, let\u2019s remind ourselves of what we want to go into this cell. It is the joint probability of the first two tokens and the best parse for these tokens. Say what?", "Let\u2019s ease into it. First, from now on, let\u2019s abbreviate references to the aforementioned joint probability to \u201cprobability of the best parse of these tokens\u201d. This loses some precision but is much more readable. From inspection, what do you think is the best parse? [begin,salutation, first_name] right? The probability of this parse is what would go into this cell.", "Next comes a key observation. We won\u2019t prove it. It is essential to the understanding of how this algorithm works. The probability of this parse is the probability of [begin,salutation] being the parse for [Dr.] multiplied by the probability of extending this parse to cover John being emitted from first_name. We have already computed the portion in bold and deposited its result into the cell [salutation, Dr.] of the matrix. So we just need to multiply this cell\u2019s value with the probability of the aforementioned extension, which is the probability of transiting from state salutation to state first_name multiplied by the probability of emitting John from first_name. This reuse of probabilities computed earlier is the key to the speedup the algorithm achieves over exhaustive search.", "One issue remains. In the previous two paragraphs, to ease into the explanation, we assumed we knew that the best parse of [Dr., John] was [begin, salutation, first_name]. The algorithm of course doesn\u2019t. How do we cope with this?", "First, let\u2019s be clear on exactly what we don\u2019t know. We do know John must be emitted from first_name because we are trying to fill out that cell\u2019s value. What we don\u2019t know is the best parse of [Dr.] and which state it ends up in. Let\u2019s call this unknown \u2018Dr.\u2019s state\u2019. For each possible value of Dr.\u2019s state, we know the probability of the best parse of [Dr.] of the form [begin, Dr.\u2019s state]. To each of these, we multiply out the probability of extending the parse to become [begin, Dr.\u2019s state, first_name]. The only variable here is Dr.\u2019s state so since we want the probability of the best parse we just take the maximum of the probabilities over all values of Dr.\u2019s state. Note that in these calculations we are reusing the probabilities in bold, as they have already been computed and stored in the matrix before then.", "We are not done yet. After we have filled in the matrix all we know is the probability of the best parse of the full input. This sits in the bottom-right cell of the matrix. We don\u2019t know the actual parse.", "To recover an actual (highest-scoring) parse, first, during the \u201cbottom-up\u201d phase in which the matrix of scores is built, we capture some additional meta-data. Once the matrix has been built, we use this meta-data to find an actual parse.", "Let\u2019s see the matrix of scores, with some cells augmented with the aforementioned meta-data. Let\u2019s depict this in its own matrix, which we will call the back-pointers matrix. This matrix has the same structure as the matrix of scores. The only difference is that its cells don\u2019t store scores, rather indices of other cells. Hence the name \u201cback-pointers\u201d.", "Below is the back-pointers matrix with some values filled in. Let\u2019s explain one. Cell [2,2] contains the value [1,1]. This captures the information that the best parse of [Dr., John] in which state first_name is constrained to emit John emits Dr. from state salutation. This information emerges during the process of computing the score in the corresponding cell in the scores matrix. Specifically during the process of computing the maximum over the various terms.", "Next, let\u2019s walk through how this matrix is used to find an actual best parse. We start at the bottom-right cell, i.e. at cell [5,3]. From this cell, we follow a path back to the cell [0,0]. In our case, the path is", "This path contains within it the information to recover a (best) parse in reverse. This is depicted below.", "In each back-pointer step, we uncover the state of one token in the reversed token sequence. In the step [5,3] \u2192 [4,3] we recover [,end]. The token is missing as end is a silent state. In the step [4,3] \u2192 [2,2] we are moving from state 4 (last_name) to another state (state 2) and simultaneously from token 3 (Smith) to token 2. These moves yield the pair (Smith, last_name). And so it goes.", "The final step is to reverse the parse, which is easy to do with the help of a stack.", "Multi-token entities are a special case of composite entities in which the value of an entity spans multiple tokens but the entity type itself is not decomposed into finer entity types. (Or we don\u2019t care to.)", "Here are two examples of such entities: national park names and medical disease names. Let\u2019s see some instances of each:", "In multi-token entities, there is no parsing problem, only one of recognition. That is, the aim is to recognize entities and their boundaries in flowing text. As in", "This is one of the simplest. However, it won\u2019t work when the dictionary is incomplete, as would be the case when we are trying to use entity recognition to build a rich dictionary in the first place. (See the use case Building a structured database from a corpus described earlier in this post.)", "This is an attractive approach to this problem. A training instance would be a short phrase labeled as positive (instance of the entity) or negative (not an instance of the entity). We\u2019d also choose appropriate features to extract from the input phrase. (More on this later.)", "An important consideration is how to choose the negative instances for the training set. The universe of negative instances is huge \u2014 all phrases up to a certain length (what should this be?). Randomly sampling from the universe may give us weak negatives. Consequently, the learned classifier may be weaker than we think it is, relative to our assessment on a train-test split of the labeled data set. Point being that the negative instances are not real \u2014 we constructed them via a certain process \u2014 and that process may not be representative of the actual negatives we get in the field, which would be sufficiently short phrases in the text that are not instances of this entity.", "By contrast, adding human-curated negatives that superficially look like positives will likely yield a more accurate classifier. Here is an example of one such negative for the entity national park name: Castle Rock State Park. It is a park but not a national one.", "The discussion of the previous paragraphs also implies that no matter how carefully we pick the negatives, the ultimate evaluation of the classifier\u2019s accuracy should be done on its entity recognition accuracy on actual text representative of how it will be used in practice.", "Once again, this is because we are picking the negatives from a huge space, and we cannot rule out selection bias. A second important reason is that the class proportions in the field, specifically the fraction of phrases input to the classifier during operation that is negative, will heavily depend on the text to which it is applied. Consider disease names. The fraction of phrases in financial documents that are disease names will likely be far lower than in documents at WebMD. Consequently, the false positive rate on the two might be quite different.", "Consider recognizing national park names, i.e. distinguishing them from other phrases. The examples we have seen suggest a for-purpose heuristic variant of bag-of-words as the features. Specifically, first, we tokenize the input into words, then combine the last two words into a bigram, together with capturing that they are the last two words. This is easiest illustrated with examples.", "Why did we combine the last two words into a bigram? Because leaving them separate risks a higher false-positive rate. For example, Yellowstone national bank (a made-up example).", "It's worth noting that this scheme offers some flexibility in that should we decide at a later stage to also deem national monuments as national parks, we need just add some to our positives. An example is Dinosaur National Monument. The features we have chosen will pick these up.", "These features leveraged the structure of national park names. How well they work on some other multi-token problem depends on whether or not its structure matches that of national park names. Consider disease names. Sure, the last word is often a predictor of positive versus negative. (See disease names examples we previously listed.) That said, combining the last two words into a bigram may not generalize well enough. This is because in disease names only the last word may be a cue word. (E.g. disease.) Using the national park name features will not detect diseases that end in word disease unless the training set has some positives that end in word disease. word may be a very specific word. This is why we say these features don\u2019t generalize well to disease names.", "Now that we have our labeled training set and features defined, we may in principle use any binary classifier algorithm for this problem. Two that come to mind our logistic regression and naive Bayes. We won\u2019t delve into either.", "A different way to solve the problem is to formulate it as a sequence labeling problem, albeit with a twist. This merits consideration as multi-token entity recognition definitely has a sequential structure. That said, it does need some thought on what the primitive entities would be. Plus, how we will turn an HMM into a binary classifier.", "Consider national park names. Take Yellowstone National Park. Say we model its corresponding state sequence as name_word geo_level_word park_type_word. That is, as a (token sequence, label sequence) instance this would be represented as", "Now consider a negative: Castle Rock State Park. Its labeled instance may look like this.", "Let\u2019s look at one more negative: Mocca Ice Cream. Its labeled instance may look like this.", "Our HMM will have the states we see in these three label sequences. The idea here is the following. On any particular input, we will force a path that ends in the state positive or negative. If both states are reachable from begin on some input, then the hope is that our HMM structure and training will favor the path that leads to the class (positive or negative) of the input.", "What is offered in the previous section are the key ingredients of such a solution. In practical settings, this approach may need further refinement.", "For example in the states (for example further refinements of the state word, perhaps part-of-speech: proper-noun, noun, verb, adjective, article, \u2026", "Or in relaxing the first-order Markovian assumption. Most generally, as the conditional random field does.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "PhD, Computer Science, neural nets. 14+ years in industry: data science algos developer. 24+ patents issued. 50 academic pubs. Blogs on ML/data science topics."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fbe09139fa7b8&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnamed-entity-recognition-in-nlp-be09139fa7b8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnamed-entity-recognition-in-nlp-be09139fa7b8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnamed-entity-recognition-in-nlp-be09139fa7b8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnamed-entity-recognition-in-nlp-be09139fa7b8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----be09139fa7b8--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----be09139fa7b8--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://jagota-arun.medium.com/?source=post_page-----be09139fa7b8--------------------------------", "anchor_text": ""}, {"url": "https://jagota-arun.medium.com/?source=post_page-----be09139fa7b8--------------------------------", "anchor_text": "Arun Jagota"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fef9ed921edad&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnamed-entity-recognition-in-nlp-be09139fa7b8&user=Arun+Jagota&userId=ef9ed921edad&source=post_page-ef9ed921edad----be09139fa7b8---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbe09139fa7b8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnamed-entity-recognition-in-nlp-be09139fa7b8&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbe09139fa7b8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnamed-entity-recognition-in-nlp-be09139fa7b8&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@mariannelong?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Marianne Long"}, {"url": "https://unsplash.com/s/photos/text-message?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://medium.com/data-science-in-your-pocket/named-entity-recognition-ner-using-conditional-random-fields-in-nlp-3660df22e95c", "anchor_text": "Named Entity Recognition(NER) using Conditional Random Fields in NLPIts time to jump on Information Extraction in NLP after a thorough discussion on algorithms in NLP for pos tagging\u2026medium.com"}, {"url": "https://medium.com/tag/hidden-markov-models?source=post_page-----be09139fa7b8---------------hidden_markov_models-----------------", "anchor_text": "Hidden Markov Models"}, {"url": "https://medium.com/tag/pattern-matching?source=post_page-----be09139fa7b8---------------pattern_matching-----------------", "anchor_text": "Pattern Matching"}, {"url": "https://medium.com/tag/dictionary?source=post_page-----be09139fa7b8---------------dictionary-----------------", "anchor_text": "Dictionary"}, {"url": "https://medium.com/tag/sequence-labeling?source=post_page-----be09139fa7b8---------------sequence_labeling-----------------", "anchor_text": "Sequence Labeling"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbe09139fa7b8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnamed-entity-recognition-in-nlp-be09139fa7b8&user=Arun+Jagota&userId=ef9ed921edad&source=-----be09139fa7b8---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbe09139fa7b8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnamed-entity-recognition-in-nlp-be09139fa7b8&user=Arun+Jagota&userId=ef9ed921edad&source=-----be09139fa7b8---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbe09139fa7b8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnamed-entity-recognition-in-nlp-be09139fa7b8&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----be09139fa7b8--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fbe09139fa7b8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnamed-entity-recognition-in-nlp-be09139fa7b8&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----be09139fa7b8---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----be09139fa7b8--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----be09139fa7b8--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----be09139fa7b8--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----be09139fa7b8--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----be09139fa7b8--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----be09139fa7b8--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----be09139fa7b8--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----be09139fa7b8--------------------------------", "anchor_text": ""}, {"url": "https://jagota-arun.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://jagota-arun.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Arun Jagota"}, {"url": "https://jagota-arun.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "685 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fef9ed921edad&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnamed-entity-recognition-in-nlp-be09139fa7b8&user=Arun+Jagota&userId=ef9ed921edad&source=post_page-ef9ed921edad--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F1638f1de39a6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnamed-entity-recognition-in-nlp-be09139fa7b8&newsletterV3=ef9ed921edad&newsletterV3Id=1638f1de39a6&user=Arun+Jagota&userId=ef9ed921edad&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}