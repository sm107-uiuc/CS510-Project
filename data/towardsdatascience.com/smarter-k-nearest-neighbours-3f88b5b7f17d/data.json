{"url": "https://towardsdatascience.com/smarter-k-nearest-neighbours-3f88b5b7f17d", "time": 1682997554.913878, "path": "towardsdatascience.com/smarter-k-nearest-neighbours-3f88b5b7f17d/", "webpage": {"metadata": {"title": "Smarter K-Nearest Neighbours. Should we vary \u2018K\u2019? | by A S | Towards Data Science", "h1": "Smarter K-Nearest Neighbours", "description": "The K-nearest neighbours (KNN) algorithm is one of the most simple and intuitive machine learning algorithms to understand. If you are trying to classify something into two categories: \u2018Yes\u2019 or \u2018No\u2019\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/background-d5f101e00afc?source=your_stories_page---------------------------", "anchor_text": "dealing with highly correlated columns", "paragraph_index": 1}, {"url": "https://towardsdatascience.com/soft-voting-bayesian-style-5306a48d26d1?source=your_stories_page---------------------------", "anchor_text": "Bayesian soft voting", "paragraph_index": 1}, {"url": "https://en.wikipedia.org/wiki/Sequential_analysis", "anchor_text": "sequential sampling", "paragraph_index": 13}, {"url": "https://en.wikipedia.org/wiki/Additive_smoothing", "anchor_text": "additive smoothing", "paragraph_index": 14}, {"url": "https://towardsdatascience.com/expertise-and-inference-570bd535bce3?source=your_stories_page---------------------------", "anchor_text": "here", "paragraph_index": 14}], "all_paragraphs": ["The K-nearest neighbours (KNN) algorithm is one of the most simple and intuitive machine learning algorithms to understand. If you are trying to classify something into two categories: \u2018Yes\u2019 or \u2018No\u2019, for example, you take the \u2018k\u2019 most similar data points, and see if they are \u2018Yes\u2019 or \u2018No\u2019. If k=5, you look at the five most similar data points, and if, for example, you see 4 \u2018Yes\u2019 and 1 \u2018No\u2019, you would guess that the point is a \u2018Yes\u2019.", "In the title I discuss making KNN smarter. What does \u201csmarter\u201d mean? In this context- I mean when the algorithm analyses the data in a particular case that a human can see is absurd. When we find those cases, we can change the behaviour of the algorithm to account for them, and thus get better results. It is best to think about each individual point, and come up with some general rules for when the algorithm might be wrong, and how to reclassify them. My previous posts about dealing with highly correlated columns, and Bayesian soft voting were in this vein.", "The KNN algorithm needs a way to define \u2018distance\u2019 from one data point to another, as it defines neighbours as those points with the smallest distance. Euclidean distance (straight line distance) is typically used. However, some features are more important than others. For example, if you were predicting who survived on the Titanic, variability in age is more important than variability in the last 4 digits of their social security number.", "To make the KNN algorithm account for this, you would typically boost the features that are more important, by multiplying them by a number>1, so that they contribute more to the \u2018distance\u2019. This is known as \u2018Feature Boosting\u2019, and should be done after scaling your data, and typically after running another Machine Learning model, to figure out which features to boost and by how much. However, I will not discuss this further as it is not the intended purpose of this article.", "You can change your voting mechanism from 1 vote per point, to giving more votes to nearer points. This should in theory improve the accuracy of your model as nearby points are likely more predictive than further away points. If you were trying to design the best possible KNN you should likely use some sort of soft voting as well, however, in practice it is best to be careful as it is very easy to over-correct for the effect of proximity.", "We typically try to optimise for the best value of \u2018k\u2019. Whether it be k=5, k=11, k=25, etc. However, does this really make sense? Do we want the same value of \u2018k\u2019 to predict every point in the data set? If we were to vary the value of \u2018k\u2019, how do we do so without biasing ourselves by looking at the test data? Here I will bring up two cases where having a fixed K makes our algorithm dumb. For simplicity we will assume a binary classification where 50% of our data is in each class.", "In the above diagram, let\u2019s say we want to classify the yellow point, and decide whether it is more likely to be red or blue. If we have k=7 and hard voting, we have 4 red and 3 blue, so the algorithm would guess \u2018red\u2019. However, it seems more sensible to choose k=4 here, as there are 4 points very close to our target, and choosing this would make us guess \u2018blue\u2019, which seems more sensible. This seems intuitive just by looking at the above photograph, we don\u2019t need to look at our test data to figure this out.", "It is possible to fix this problem with adequate soft voting if you had set k=7, however, there could be another problem.", "There are 8 blue points, and 4 slightly closer red points. If you had set k 1\u20137, this would have classified the yellow point as red. However, since the red and blue points are almost the same distance away, it does not make sense to give the reds so much extra influence over the vote. I would intuitively guess blue.", "The optimal solution would be to either 1) Run some algorithm to look at a point and decide a good value of \u2018k\u2019, i.e. cut it off when increasing k by 1 really increases the distance, or 2) Set a really big value of \u2018k\u2019, but have your distance metrics be good enough to exhibit this behaviour by itself (e.g. negative exponential weights).", "For the above example, suppose you had set k=5. There are 3 blue points and 2 red points closest to the point we are trying to classify.", "Let\u2019s say you were testing a hypothesis about whether more NYU students preferred Taylor or Kanye. Furthermore, let us assume you have no way of knowing who is an NYU student and who isn\u2019t. How would you being to answer this? One approach would be to start from the centre of NYU, and ask random people who their preferred artist is. People closer to the centre of NYU are more likely to be NYU students, so you trust their data the most. However, if the results are statistically inconclusive, you will have to go further out and ask more people what their preference is, even at the risk of interviewing more and more non-NYU students.", "This is analogous to the KNN problem above. The points closest to the yellow point are most similar to the yellow point, however, if it is statistically inconclusive whether red or blue is preferred, we need to go further away and check some more. If you saw 3 students in favour of Taylor and 2 in favour of Kanye, would you conclude that Taylor is more popular, or would you get more data?", "This idea is similar to sequential sampling, created by the great economist Milton Friedman.", "If you are working on a big data set, it is possible to wait until you achieve a high degree of statistical significance, however, on a smaller dataset, I would suggest a different approach. I would use an additive smoothing approach, to create a Bayesian estimate of the probability, similar to what I discussed here. I would then use a probability threshold of say 70% to make predictions.", "As the size of your sample becomes big, it\u2019s better to switch to statistical significance tests, and if your sample becomes too big- just pick the majority.", "I think the suggested above methods are a more sensible way to approach the nearest neighbours problem, although they are more tedious to implement. However, as with many things in programming, once you implement it once, it is easy to implement it again, and you will continue to reap the benefits.", "As the amount of data you use increases, which model you use does not particularly matter. However, smarter models would allow you to make stronger conclusions based on small amounts of data, which is a real problem in data science. Inference on small data sets is one of the areas where expert humans are far superior to current machines, and perhaps picking off low hanging fruit like this will help to bridge that gap.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Maths graduate, former algorithmic trader, aspiring data scientist, played online poker."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F3f88b5b7f17d&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsmarter-k-nearest-neighbours-3f88b5b7f17d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsmarter-k-nearest-neighbours-3f88b5b7f17d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsmarter-k-nearest-neighbours-3f88b5b7f17d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsmarter-k-nearest-neighbours-3f88b5b7f17d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----3f88b5b7f17d--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3f88b5b7f17d--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@asquant?source=post_page-----3f88b5b7f17d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@asquant?source=post_page-----3f88b5b7f17d--------------------------------", "anchor_text": "A S"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F77c9d1ca99db&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsmarter-k-nearest-neighbours-3f88b5b7f17d&user=A+S&userId=77c9d1ca99db&source=post_page-77c9d1ca99db----3f88b5b7f17d---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3f88b5b7f17d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsmarter-k-nearest-neighbours-3f88b5b7f17d&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3f88b5b7f17d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsmarter-k-nearest-neighbours-3f88b5b7f17d&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/background-d5f101e00afc?source=your_stories_page---------------------------", "anchor_text": "dealing with highly correlated columns"}, {"url": "https://towardsdatascience.com/soft-voting-bayesian-style-5306a48d26d1?source=your_stories_page---------------------------", "anchor_text": "Bayesian soft voting"}, {"url": "https://en.wikipedia.org/wiki/Sequential_analysis", "anchor_text": "sequential sampling"}, {"url": "https://en.wikipedia.org/wiki/Additive_smoothing", "anchor_text": "additive smoothing"}, {"url": "https://towardsdatascience.com/expertise-and-inference-570bd535bce3?source=your_stories_page---------------------------", "anchor_text": "here"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----3f88b5b7f17d---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----3f88b5b7f17d---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/statistics?source=post_page-----3f88b5b7f17d---------------statistics-----------------", "anchor_text": "Statistics"}, {"url": "https://medium.com/tag/k-nearest-neighbours?source=post_page-----3f88b5b7f17d---------------k_nearest_neighbours-----------------", "anchor_text": "K Nearest Neighbours"}, {"url": "https://medium.com/tag/mathematics?source=post_page-----3f88b5b7f17d---------------mathematics-----------------", "anchor_text": "Mathematics"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3f88b5b7f17d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsmarter-k-nearest-neighbours-3f88b5b7f17d&user=A+S&userId=77c9d1ca99db&source=-----3f88b5b7f17d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3f88b5b7f17d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsmarter-k-nearest-neighbours-3f88b5b7f17d&user=A+S&userId=77c9d1ca99db&source=-----3f88b5b7f17d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3f88b5b7f17d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsmarter-k-nearest-neighbours-3f88b5b7f17d&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3f88b5b7f17d--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F3f88b5b7f17d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsmarter-k-nearest-neighbours-3f88b5b7f17d&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----3f88b5b7f17d---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----3f88b5b7f17d--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----3f88b5b7f17d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----3f88b5b7f17d--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----3f88b5b7f17d--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----3f88b5b7f17d--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----3f88b5b7f17d--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----3f88b5b7f17d--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----3f88b5b7f17d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@asquant?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@asquant?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "A S"}, {"url": "https://medium.com/@asquant/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "112 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F77c9d1ca99db&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsmarter-k-nearest-neighbours-3f88b5b7f17d&user=A+S&userId=77c9d1ca99db&source=post_page-77c9d1ca99db--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F85d08d9c8f11&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsmarter-k-nearest-neighbours-3f88b5b7f17d&newsletterV3=77c9d1ca99db&newsletterV3Id=85d08d9c8f11&user=A+S&userId=77c9d1ca99db&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}