{"url": "https://towardsdatascience.com/gradient-descent-one-step-at-a-time-3c39a3642333", "time": 1683016043.9761899, "path": "towardsdatascience.com/gradient-descent-one-step-at-a-time-3c39a3642333/", "webpage": {"metadata": {"title": "Gradient Descent \u2014 One Step at a Time | by Asad Mumtaz | Towards Data Science", "h1": "Gradient Descent \u2014 One Step at a Time", "description": "An intuitive, beginner\u2019s guide to the underlying mechanics of gradient descent that is used to minimize various loss functions in machine learning"}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/Least_squares", "anchor_text": "Least Squares", "paragraph_index": 2}, {"url": "https://en.wikipedia.org/wiki/Residual_sum_of_squares", "anchor_text": "Residual Sum of Squares", "paragraph_index": 9}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html", "anchor_text": "SGDRegressor", "paragraph_index": 47}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html", "anchor_text": "SGDClassifier", "paragraph_index": 47}, {"url": "https://www.finlyticshub.com/", "anchor_text": "me", "paragraph_index": 49}, {"url": "http://www.finltyicshub.com", "anchor_text": "www.finltyicshub.com", "paragraph_index": 51}], "all_paragraphs": ["Gradient Descent (GD) is an optimization algorithm used to minimize any given cost function iteratively. But what is a cost function? It is a function that measures the accuracy of a trained machine learning model in making predictions. Common examples include Mean Squared Error (MSE) and Cross-Entropy (or Log-Loss).", "GD is heavily utilized in deep machine learning models as an alternative to more traditional methodologies to minimize cost functions. It deals more efficiently with enormous volumes of data that are common in deep learning.", "We will go through the GD in some detail to model an overly simplified linear regression problem for an intuitive and deep understanding of its mechanics. I will assume that you have some knowledge of Least Squares, Calculus, and Linear Regression while working through the problem.", "Here is our toy data comprising of 3 data points:", "We aim to find the line of best fit using GD that can then be used to predict Y values based on some new X values (aka Linear Regression). Recall that a straight line can be described entirely by just two parameters: intercept and slope. Hence, if we can find the ideal intercept and slope parameters through an optimization algorithm, we will get our line of best fit.", "Our line of best fit can be represented as follows:", "To keep it simple initially, we will start with finding the optimal value of only \u03b8\u2080, while assuming \u03b8\u2081 to be equal to 0.64 (determined through Least Squares). Then, once we understand how GD works, we will use it to solve for both the optimal intercept and slope parameters together. So our revised equation for the line becomes:", "We start with picking a random value for the intercept \u03b8\u2080, called random initialization. The random initialization is our initial guess and can literally be any real number used by GD to improve upon.", "Let\u2019s start with 0 for \u03b8\u2080 (not coincidentally, also my favorite number!), that gives us the following line:", "Next, we will evaluate how well this line fits our data with the Residual Sum of Squares (RSS) loss function:", "Using Eq. 2 and with \u03b8\u2080 = 0, we get the following predictions of y, Residuals, Squared Residuals, and Residual Sum of Squares:", "Repeating the above calculation for RSS with increasing values for the intercept \u03b8\u2080, we get the following scatter plot of \u03b8\u2080 against RSS:", "It is quite evident from the plot that an intercept value of 0.95 resulted in the least RSS. But how do we know that it is, in fact, 0.95, and not some other value between 0.75 and 1.15? We can plug in a bunch of different intercept values and find the optimal value through trial and error. But why do so when we have GD at our disposal?", "When GD calculates RSS at the point of random initialization, it takes a relatively bigger step to determine the next intercept to be evaluated in case of a high RSS value. As RSS approaches zero in the subsequent iterations, the change in intercept (called step sizes) gets smaller. In other words, GD identifies the optimal parameter value by taking big steps when it is far away from the minimum RSS and baby steps when it is closer to it.", "We can get an equation for the cost function that plots all possible intercepts against RSS by using RSS and our Eq. 2 as follows (with our three original data points):", "The function\u2019s plot will look something like this, which is basically the same scatter plot as above with a polynomial trendline added to it:", "We can take the derivative of this function with respect to \u03b8\u2080 and determine the slope at any value for the intercept \u03b8\u2080:", "Now that we have the derivative, GD will use it to find where the RSS is at its lowest value. Like before, suppose that we initialize GD with an initial value of \u03b8\u2080 = 0.", "Plugging \u03b8\u2080 = 0 in the derivative above gives us -5.6, which is the gradient or slope of the RSS cost function at \u03b8\u2080 = 0. Note that the optimal value of \u03b8\u2080 is obtained at the point on the curve where its gradient is 0, i.e., at the curve\u2019s bottom. Recall also that the GD takes a relatively big step when the curve\u2019s slope is far from 0 and baby steps when it is close to 0. So the size of the step should be related to the curve\u2019s slope since it tells GD whether it should take a baby step or a big step. However, the step should not be too big so as to jump over the minimum point on the curve and go over to the other side.", "The step size is regulated by a parameter called the learning rate. The step size then determines the new intercept to be used by the GD to calculate RSS:", "Continuing with our example, when \u03b8\u2080 = 0 and with a learning rate of 0.1, the step size is -0.56:", "The cost function\u2019s plot with the two intercepts evaluated so far looks something like the below:", "The above plot clearly shows that with a relatively big first step, we are already pretty close to the minimum value of RSS! Going back to our line of best-fit, we can see that the new intercept of 0.56 has substantially shrunk the residuals:", "Now let\u2019s take another step closer to the optimal value for the intercept. We go back to the derivative in Eq. 3 and plug in the new intercept value of 0.56, which gives us the slope of the curve to be -2.2 at this intercept value:", "Our two plots look like this below:", "Overall, the RSS is getting smaller. Also, notice that in the RSS Cost Function Plot, the first step was relatively large compared to the second step.", "Let\u2019s also do the third step:", "Our two plots look like this below:", "Continuing for three more steps, the GD\u2019s estimate for the intercept comes out to be 0.95, which is precisely the same as that calculated by the Least Squares method (you can check the Least Squares\u2019 calculation yourself!).", "But how does GD know when to stop doing additional calculations? In other words, how does it know that it has arrived at an optimal estimate? It does so when the step size becomes very close to 0, which will occur when the gradient is very close to 0. Practitioners usually control this through the Minimum Step Size parameter, which is generally set at 0.001 or smaller.", "That said, GD also includes a limit on the number of steps it will take before it gives up, which is generally parameterized at 1,000 steps or more. So, even if the step size is large, it will stop if the GD has performed the maximum number of steps.", "Now that we have understood GD\u2019s basics, let\u2019s work out both the optimal intercept and the slope together.", "This means that we will not be using the slope of 0.64, as in Eq. 2. Instead, we will have two independent parameters, \u03b8\u2080 and \u03b8\u2081, that we need to optimize together. This calls for multivariable calculus and taking partial derivatives with respect to both the intercept \u03b8\u2080 and the slope \u03b8\u2081. The Cost Function looks like this now:", "The partial derivative with respect to the intercept \u03b8\u2080 looks like the following:", "The partial derivative with respect to the intercept \u03b8\u2081 looks like the following:", "We will use these two partial derivatives to find the lowest point in the RSS cost function (Eq. 4). Just like before, we will start by selecting random numbers for the intercept \u03b8\u2080 and the slope \u03b8\u2081. Let\u2019s start with \u03b8\u2080 = 0 and \u03b8\u2081 = 1. Thus, GD will commence from this line:", "Now let\u2019s plug in \u03b8\u2080 = 0 and \u03b8\u2081 = 1 in our partial derivative equations:", "Calculating the step sizes with a learning rate of 0.01 together with the new values for the intercept and the slope:", "Our updated line of best fit looks something like this:", "The difference does not appear to be material. However, after some more iterations (until either the step size is smaller than the threshold or the maximum number of iterations have been reached) of calculating the step sizes and updated parameters, we get the following line of best fit:", "The optimum parameters found by the GD will be the same or very close to those find by the Least Squares method, i.e. \u03b8\u2080 = 0.95 and \u03b8\u2081 = 0.64.", "We just completed the GD calculation for two parameters. If our data had more features (as is usually the case with practical datasets), we simply need to take more partial derivatives, and everything else will remain the same.", "A critical point to note here is that all the parameters should be updated simultaneously at each step (and not one-by-one).", "In this example, we optimized the RSS cost function. However, there are several other cost functions that work with different data types, distributions, and prediction problems. Regardless of which cost function is used, GD works the same way.", "What we have performed in this problem is called Batch Gradient Descent, whereby all the available data points (training data) are used when calculating the slope of the cost function (Step 3 above). This can cause a problem and inefficiencies when we have a large number of data points.", "This inefficiency is handled very well by a different beast, called Stochastic Gradient Descent. Stochastic Gradient Descent uses a randomly selected observation at every step rather than the entire dataset. This reduces the time spent calculating the slope of the cost function (step 3). The overall concept and methodology remain similar to Batch Gradient Descent.", "You would have noticed that we used two different learning rates: a higher learning rate when finding only the optimal intercept and a lower learning rate when finding both the optimal intercept and slope together.", "GD is highly sensitive to the learning rate. In practice, a reasonable learning rate can be automatically determined by starting large and getting smaller with each step \u2014 through a Learning Schedule. The learning rate and the learning schedule are available as parameters in most machine learning algorithms. For example, in scikit-learn\u2019s SGDRegressor and SGDClassifier classes, the learning rate, and the learning schedule are controlled by the eta0 and the learning_rate parameters, respectively.", "There you have it, a very simple explanation to how GD works in the background. Let us now summarize the steps taken by the GD while optimizing the parameters given to it:", "Feel free to reach out to me if you would like to discuss anything related to data analytics, machine learning, financial or credit analysis.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "A finance professional by education with a keen interest in data analytics and machine learning. www.finltyicshub.com"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F3c39a3642333&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-one-step-at-a-time-3c39a3642333&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-one-step-at-a-time-3c39a3642333&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-one-step-at-a-time-3c39a3642333&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-one-step-at-a-time-3c39a3642333&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----3c39a3642333--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3c39a3642333--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@asad_mumtaz?source=post_page-----3c39a3642333--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@asad_mumtaz?source=post_page-----3c39a3642333--------------------------------", "anchor_text": "Asad Mumtaz"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1f9c6741ffa5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-one-step-at-a-time-3c39a3642333&user=Asad+Mumtaz&userId=1f9c6741ffa5&source=post_page-1f9c6741ffa5----3c39a3642333---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3c39a3642333&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-one-step-at-a-time-3c39a3642333&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3c39a3642333&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-one-step-at-a-time-3c39a3642333&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/tagged/getting-started", "anchor_text": "Getting Started"}, {"url": "https://unsplash.com/@tricell1991?utm_source=medium&utm_medium=referral", "anchor_text": "Waranont (Joe)"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://en.wikipedia.org/wiki/Least_squares", "anchor_text": "Least Squares"}, {"url": "https://en.wikipedia.org/wiki/Residual_sum_of_squares", "anchor_text": "Residual Sum of Squares"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html", "anchor_text": "SGDRegressor"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html", "anchor_text": "SGDClassifier"}, {"url": "https://www.finlyticshub.com/", "anchor_text": "me"}, {"url": "https://medium.com/tag/data-science?source=post_page-----3c39a3642333---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----3c39a3642333---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/programming?source=post_page-----3c39a3642333---------------programming-----------------", "anchor_text": "Programming"}, {"url": "https://medium.com/tag/python?source=post_page-----3c39a3642333---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/getting-started?source=post_page-----3c39a3642333---------------getting_started-----------------", "anchor_text": "Getting Started"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3c39a3642333&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-one-step-at-a-time-3c39a3642333&user=Asad+Mumtaz&userId=1f9c6741ffa5&source=-----3c39a3642333---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3c39a3642333&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-one-step-at-a-time-3c39a3642333&user=Asad+Mumtaz&userId=1f9c6741ffa5&source=-----3c39a3642333---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3c39a3642333&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-one-step-at-a-time-3c39a3642333&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3c39a3642333--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F3c39a3642333&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-one-step-at-a-time-3c39a3642333&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----3c39a3642333---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----3c39a3642333--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----3c39a3642333--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----3c39a3642333--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----3c39a3642333--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----3c39a3642333--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----3c39a3642333--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----3c39a3642333--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----3c39a3642333--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@asad_mumtaz?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@asad_mumtaz?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Asad Mumtaz"}, {"url": "https://medium.com/@asad_mumtaz/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "457 Followers"}, {"url": "http://www.finltyicshub.com", "anchor_text": "www.finltyicshub.com"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1f9c6741ffa5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-one-step-at-a-time-3c39a3642333&user=Asad+Mumtaz&userId=1f9c6741ffa5&source=post_page-1f9c6741ffa5--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F83025c62db23&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-one-step-at-a-time-3c39a3642333&newsletterV3=1f9c6741ffa5&newsletterV3Id=83025c62db23&user=Asad+Mumtaz&userId=1f9c6741ffa5&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}