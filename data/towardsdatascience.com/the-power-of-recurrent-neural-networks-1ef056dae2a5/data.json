{"url": "https://towardsdatascience.com/the-power-of-recurrent-neural-networks-1ef056dae2a5", "time": 1683002124.263461, "path": "towardsdatascience.com/the-power-of-recurrent-neural-networks-1ef056dae2a5/", "webpage": {"metadata": {"title": "The Power of Recurrent Neural Networks | by Manuel Brenner | Towards Data Science", "h1": "The Power of Recurrent Neural Networks", "description": "Neural networks have come to dominate modern AI Research in recent years, and with good cause: they have provided powerful tools in extracting complex patterns from data, be it when classifying or\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/how-to-make-computers-dream-3b4b10e4463a", "anchor_text": "generating images", "paragraph_index": 1}, {"url": "https://medium.com/dair-ai/deep-learning-for-nlp-an-overview-of-recent-trends-d0d8f40a776d", "anchor_text": "processing natural language", "paragraph_index": 1}, {"url": "https://towardsdatascience.com/ants-and-the-problems-with-neural-networks-778caa73f77b?source=friends_link&sk=57658f48821ba0cf7732bcf579364996", "anchor_text": "me in this article", "paragraph_index": 4}, {"url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.441.7873&rep=rep1&type=pdf", "anchor_text": "Cybenko for the sigmoid activation function", "paragraph_index": 10}, {"url": "https://link.springer.com/article/10.1007/s11766-015-3000-9", "anchor_text": "hyperbolic tangent", "paragraph_index": 10}, {"url": "https://en.wikipedia.org/wiki/Perceptrons_(book)", "anchor_text": "on the perceptron in 1969,", "paragraph_index": 11}, {"url": "https://neptune.ai/blog/recurrent-neural-network-guide", "anchor_text": "read here", "paragraph_index": 14}, {"url": "https://www.sciencedirect.com/science/article/pii/S089360800580125X", "anchor_text": "It was shown in 1993 by Funakashi and Nakamura", "paragraph_index": 15}, {"url": "https://link.springer.com/chapter/10.1007/11840817_66", "anchor_text": "extends to dynamical systems", "paragraph_index": 15}, {"url": "https://www.sciencedirect.com/science/article/pii/S089360800580125X", "anchor_text": "considerable general applications", "paragraph_index": 22}, {"url": "https://en.wikipedia.org/wiki/Hodgkin%E2%80%93Huxley_model", "anchor_text": "Hodgin-Huxley model", "paragraph_index": 30}, {"url": "https://arxiv.org/pdf/1809.06303.pdf", "anchor_text": "read here for a great overview", "paragraph_index": 32}, {"url": "https://www.pnas.org/content/79/8/2554", "anchor_text": "Hopfield network", "paragraph_index": 32}, {"url": "https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1007263", "anchor_text": "We can then try to learn the dynamical system underlying the data with", "paragraph_index": 33}, {"url": "https://towardsdatascience.com/how-to-make-computers-dream-3b4b10e4463a", "anchor_text": "I explain what generative models are and why they are so cool in this article", "paragraph_index": 34}, {"url": "https://towardsdatascience.com/explainability-and-the-art-of-confabulation-d4fb176de982", "anchor_text": "the problem of explainability", "paragraph_index": 35}], "all_paragraphs": ["With four parameters I can fit an elephant, and with five I can make him wiggle his trunk.John von Neumann", "Neural networks have come to dominate modern AI Research in recent years, and with good cause: they have provided powerful tools in extracting complex patterns from data, be it when classifying or generating images, aiding in medical diagnosis or processing natural language.", "As a short reminder: neural networks are formed by a set of directed units called neurons. These neurons can connect to and pass input between each other, depending on the strength of their respective connections. The sum of the inputs to a given unit is run through an activation function to give its output.", "Biological intuitions can come in handy here (although they have their limits): you can think of a neural network as a set of neurons connected via synapses, which either fire or don\u2019t, depending on how much input they receive. The strength of their respective connections determines how much the input from one neuron excites or inhibits the activity of the neuron it is connected, and the activation function determines how and when it fires given the inputs.", "While neural networks have shown to be really good at doing a bunch of things, many people (among them me in this article) argue that pure neural networks are, at least to some extent, overrated when it comes to their power to explain our brain and provide the architectural basis for more sophisticated, human-like AI. There are a lot of things they can\u2019t do and won\u2019t be doing any time soon.", "But at the same time, neural networks are powerful approximators. Looking into more detail at what neural nets can do in a more formal sense leads us to some fascinating results at the overlap between mathematics and theoretical computer science, with some neuroscientific implications thrown into the mix as well.", "Moving from the two-body problem to the three-body problem, physics students in their second semester usually find out to their horror that almost no real-world problem can be solved analytically.", "But luckily there are ways around having to find perfect solutions to difficult problems: approximations with and decompositions into smaller, simpler bits and functions are among the neatest tricks in the mathematician\u2019s and physicist\u2019s cookbooks. The trigonometric functions and the exponential function can be decomposed into polynomials, functions can be Taylor expanded around a point, probability distributions are approximated by Gaussian, numerical methods like the Newton-Raphson method allows us to iteratively find the roots of a function, etc.", "As von Neumann notes in his cheeky comment, given enough parameters, and parameters that are sufficiently expressive, you can fit pretty much anything to anything.", "And so likewise it can be shown that all functions on a continuous subset of real space can be approximated by combining a set of simple functions. This simple combination of functions needed to approximate any other function can be expressed by a single-layer neural network equipped with the right activation functions.", "This theorem known under the name of The Universal Approximation Theorem was shown to hold by Cybenko for the sigmoid activation function, but can also be extended to others, such as the hyperbolic tangent.", "Nevertheless, Marvin Minsky and Seymour Papert caused quite the stir when they published their book on the perceptron in 1969, in which they showed that there are a lot of intuitively very simple functions that neural networks can\u2019t approximate, e.g. parity (which relates to the problem that they don\u2019t exist on connected subsets of real space), but as it turns out using deep neural networks with multiple layers does the trick for approximating these kinds of functions as well.", "So we can express, under some mild constraints, almost any function we want (given a sufficiently large neural network with enough layers and enough parameters to tune) and use this function to map an input to an output, which is essentially what a function does.", "This is already pretty neat. We can fit the elephant. But can we make him wiggle his trunk?", "Recurrent neural networks (RNNs) (read here for an overview) are a subtype of artificial neural networks in which a temporal sequence is included, allowing the network to exhibit dynamical behavior (how a system behaves as time is passing).", "It was shown in 1993 by Funakashi and Nakamura that the universal approximation theorem extends to dynamical systems as well.", "To understand how it can, we need to understand that a dynamical system is characterized by its flow field plus some initial conditions. This flow field determines the derivative of its state variables, and therefore, its behavior in time. This flow field can be written as a function (where the apostrophe after x denotes a derivative\u2026sorry for that but Medium is not really optimized for math equations):", "We can usually discretize these dynamical systems in time, in the sense that we define x at a certain time t+1 through a function on its value at time t and reexpress the flow field through a new function G(x(t)) that takes care of this mapping:", "The mathematical details of how we move from F(x) to G(x) can be a bit complicated, but the bottom line is that the state of the system at time t+1 depends on the state at time t, sent through this function G. The variable x(t) can be thought of as the value of a unit within the output layer of the recurrent neural network at time t, which you can read out just as in every feedforward neural network.", "The dynamics of the system in time then become clear by iteratively applying this mapping again and again:", "If you know the initial state of the system at time t=0, you will know its state at every point in the future t=T, and you will, therefore, have completely characterized the system.", "Behold, the elephant is wiggling his trunk.", "As Funahashi and Nakamura state, dynamical time-variant systems have \u201cconsiderable general applications\u201d in all of the sciences. Life is not static, and almost all things worth studying move around and wiggle constantly. The Dow Jones is an observable of a dynamical system. So are tectonic shifts potentially leading to earthquakes.", "The examples are infinite, but let\u2019s say we have found a dynamical system in the wild that interests us.", "Say we only have some observations x_i at different times t_i, but we are still lacking an understanding of the underlying dynamics of the system, encoded by the function F(x), or respectively G(x).", "Notice that this is the state of affairs for pretty much every scientific experiment ever.", "Now we can ask the following question from a purely data scientific point of view: how do we learn to explain the data x_i by fitting a model to it without having to analytically figure out what is going on?", "The answer follows from everything we discussed so far: we do this by figuring out how the recurrent connections of our network approximate the function G(x) equivalent to the flow field of the dynamical system.", "This allows us to encode its entire dynamics within the parameters of the network, which can be learned by the usual machine learning optimization algorithms like gradient descent from the data.", "One of the most well-known and most puzzled-over dynamical systems is the brain. The interactions of neurons form a dynamical system where the intuitive connection to recurrent neural networks seems pretty clear.", "But then there are many dynamical subsystems in the brain. Every single neuron, for instance, is in itself is again a complex dynamical system. The Hodgin-Huxley model describes how the firing of a neuron depends on the changing synaptic conductances caused by time dependence of ion flows within the synapse.", "But as we can learn any dynamical system with an RNN, this means that we can also use an RNN to learn to model the dynamics single neuron.", "Research indicates that many mental illnesses are connected to changes in network dynamics (read here for a great overview), and many useful computational abilities of the brain, such as associative memory, might emerge from dynamical system properties like attractor states (the Hopfield network is an early implementation of this).", "Assume, for example, we have brain data, be it FMRI, EEG or single neuron spike trains from healthy patients vs. schizophrenic patients. We can then try to learn the dynamical system underlying the data with a recurrent neural network with machine learning algorithms like the variational autoencoder.", "Once we have learned the best model from the data and encoded it in the RNN, we obtain a generative model of the data (I explain what generative models are and why they are so cool in this article). We can then analyze this model in order to improve our understanding of what might cause the changes in network dynamics associated with pathologies like schizophrenia.", "Depending on how sophisticated the model is, we can also try to make it biologically interpretable (the problem of explainability is still present in this context, as in many other machine learning applications) in order to extract biologically causal mechanism from the learned model and simulate the effect of novel medical interventions.", "Nevertheless, the brain is very messy and complicated, and these approaches are yet tricky to carry out in practice.", "But on the other hand, the complexity of the brain indicates that it will be hard to come by analytical descriptions of it, so we need approaches ground in statistics and data science to learn good approximate models.", "The approximating power of recurrent neural networks can provide an invaluable tool to do just that.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F1ef056dae2a5&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-power-of-recurrent-neural-networks-1ef056dae2a5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-power-of-recurrent-neural-networks-1ef056dae2a5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-power-of-recurrent-neural-networks-1ef056dae2a5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-power-of-recurrent-neural-networks-1ef056dae2a5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----1ef056dae2a5--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1ef056dae2a5--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://manuel-brenner.medium.com/?source=post_page-----1ef056dae2a5--------------------------------", "anchor_text": ""}, {"url": "https://manuel-brenner.medium.com/?source=post_page-----1ef056dae2a5--------------------------------", "anchor_text": "Manuel Brenner"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1fde95441432&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-power-of-recurrent-neural-networks-1ef056dae2a5&user=Manuel+Brenner&userId=1fde95441432&source=post_page-1fde95441432----1ef056dae2a5---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1ef056dae2a5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-power-of-recurrent-neural-networks-1ef056dae2a5&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1ef056dae2a5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-power-of-recurrent-neural-networks-1ef056dae2a5&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/how-to-make-computers-dream-3b4b10e4463a", "anchor_text": "generating images"}, {"url": "https://medium.com/dair-ai/deep-learning-for-nlp-an-overview-of-recent-trends-d0d8f40a776d", "anchor_text": "processing natural language"}, {"url": "https://towardsdatascience.com/ants-and-the-problems-with-neural-networks-778caa73f77b?source=friends_link&sk=57658f48821ba0cf7732bcf579364996", "anchor_text": "me in this article"}, {"url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.441.7873&rep=rep1&type=pdf", "anchor_text": "Cybenko for the sigmoid activation function"}, {"url": "https://link.springer.com/article/10.1007/s11766-015-3000-9", "anchor_text": "hyperbolic tangent"}, {"url": "https://en.wikipedia.org/wiki/Perceptrons_(book)", "anchor_text": "on the perceptron in 1969,"}, {"url": "https://neptune.ai/blog/recurrent-neural-network-guide", "anchor_text": "read here"}, {"url": "https://www.sciencedirect.com/science/article/pii/S089360800580125X", "anchor_text": "It was shown in 1993 by Funakashi and Nakamura"}, {"url": "https://link.springer.com/chapter/10.1007/11840817_66", "anchor_text": "extends to dynamical systems"}, {"url": "https://unsplash.com/@superdyz123?utm_source=medium&utm_medium=referral", "anchor_text": "\u9ad8\u5174\u7684 \u8001\u7236\u4eb2"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://www.sciencedirect.com/science/article/pii/S089360800580125X", "anchor_text": "considerable general applications"}, {"url": "https://unsplash.com/@chrisliverani?utm_source=medium&utm_medium=referral", "anchor_text": "Chris Liverani"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://en.wikipedia.org/wiki/Hodgkin%E2%80%93Huxley_model", "anchor_text": "Hodgin-Huxley model"}, {"url": "https://arxiv.org/pdf/1809.06303.pdf", "anchor_text": "read here for a great overview"}, {"url": "https://www.pnas.org/content/79/8/2554", "anchor_text": "Hopfield network"}, {"url": "https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1007263", "anchor_text": "We can then try to learn the dynamical system underlying the data with"}, {"url": "https://towardsdatascience.com/how-to-make-computers-dream-3b4b10e4463a", "anchor_text": "I explain what generative models are and why they are so cool in this article"}, {"url": "https://towardsdatascience.com/explainability-and-the-art-of-confabulation-d4fb176de982", "anchor_text": "the problem of explainability"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----1ef056dae2a5---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----1ef056dae2a5---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/data-science?source=post_page-----1ef056dae2a5---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----1ef056dae2a5---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/science?source=post_page-----1ef056dae2a5---------------science-----------------", "anchor_text": "Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1ef056dae2a5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-power-of-recurrent-neural-networks-1ef056dae2a5&user=Manuel+Brenner&userId=1fde95441432&source=-----1ef056dae2a5---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1ef056dae2a5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-power-of-recurrent-neural-networks-1ef056dae2a5&user=Manuel+Brenner&userId=1fde95441432&source=-----1ef056dae2a5---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1ef056dae2a5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-power-of-recurrent-neural-networks-1ef056dae2a5&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1ef056dae2a5--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F1ef056dae2a5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-power-of-recurrent-neural-networks-1ef056dae2a5&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----1ef056dae2a5---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----1ef056dae2a5--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----1ef056dae2a5--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----1ef056dae2a5--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----1ef056dae2a5--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----1ef056dae2a5--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----1ef056dae2a5--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----1ef056dae2a5--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----1ef056dae2a5--------------------------------", "anchor_text": ""}, {"url": "https://manuel-brenner.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://manuel-brenner.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Manuel Brenner"}, {"url": "https://manuel-brenner.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "3.8K Followers"}, {"url": "https://anchor.fm/acit-science", "anchor_text": "https://anchor.fm/acit-science"}, {"url": "https://www.linkedin.com/in/manuel-brenner-772261191", "anchor_text": "https://www.linkedin.com/in/manuel-brenner-772261191"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1fde95441432&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-power-of-recurrent-neural-networks-1ef056dae2a5&user=Manuel+Brenner&userId=1fde95441432&source=post_page-1fde95441432--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fd29a36fb919d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-power-of-recurrent-neural-networks-1ef056dae2a5&newsletterV3=1fde95441432&newsletterV3Id=d29a36fb919d&user=Manuel+Brenner&userId=1fde95441432&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}