{"url": "https://towardsdatascience.com/feature-selection-in-machine-learning-d5af31f7276", "time": 1683005410.394252, "path": "towardsdatascience.com/feature-selection-in-machine-learning-d5af31f7276/", "webpage": {"metadata": {"title": "Feature selection in machine learning | by Tatiana Gabruseva | Towards Data Science", "h1": "Feature selection in machine learning", "description": "The gradient boosted decision trees, such as XGBoost and LightGBM [1\u20132], became a popular choice for classification and regression tasks for tabular data and time series. Usually, at first, the\u2026"}, "outgoing_paragraph_urls": [], "all_paragraphs": ["The gradient boosted decision trees, such as XGBoost and LightGBM [1\u20132], became a popular choice for classification and regression tasks for tabular data and time series. Usually, at first, the features representing the data are extracted and then they are used as the input for the trees.", "The feature is an individual measurable property or characteristic of a phenomenon being observed [3] \u2014 an attribute in your data set. Features may include various statistics (mean, std, median, percentiles, min, max, etc), trends (rise and decays), peak analysis (periods, average peaks width, peaks number, frequencies), autocorrelations and cross-correlations and many others. There are several open-source libraries that help to extract all basic features, like NumPy, SciPy, sklearn, tsfresh [4\u20137], and others. Often, it\u2019s also useful to design custom-made features for the task, based on the domain knowledge and physical understanding of the problem.", "Once the features are extracted from the data they are used as in input for the gradient boosted decision trees (GBDT) [8]. However, the GBDT are prone to overfitting, and for the relatively small data sets, it\u2019s important to reduce the number of features, leaving only those that help the classifier.", "An important part of the pipeline with decision trees is the features selection process. The features selection helps to reduce overfitting, remove redundant features, and avoid confusing the classifier. Here, I describe several popular approaches used to select the most relevant features for the task.", "One of the possibilities to remove extra features is an automatic tool for recursive feature elimination from sklearn library [9]. Recursive Feature Elimination with Cross-Validation [10] is used more often than the option without cross-validation.", "The goal of this tool is to select features by recursively considering smaller and smaller sets of features.", "This tool provides the first approximation of the useful feature set. However, the automated feature elimination is not always optimal, and often it requires further fine-tuning. After the initial set of features is selected with the recursive elimination described above, I use permutation importance for selecting features.", "This method is known as \u201cpermutation importance\u201d or \u201cMean Decrease Accuracy\u201d and is described in Breiman [11]. The permutation importance is computed as a decrease in the score when feature values are permuted (become noise).", "One of the ways to measure feature importance is to remove it entirely, train the classifier without that feature and see how doing so affects the score. However, this approach requires re-training of the classifier for each feature which is computationally expensive. The way around it is removing the feature under question from the validation set only, and computing the score for the validation set without that feature. As the trained classifier still expects to have this feature available, instead of removing the feature it can be replaced with random noise from the same distribution, as initial feature values. The easiest way to get such distribution is simply shuffling (or permuting) original feature values. And this is exactly how the permutation importance is implemented. The feature is still there for the classifier, but it does not contain any useful information.", "The permutation importance can be computed using the eli5 package [12]. It provides a ranking of the features, and then I remove ones with negative or small importance. The eli5 package provides a way to compute feature importances for any black-box estimator by measuring how the score decreases when a feature is not available.", "The method is most suitable for computing feature importance when a number of features are not huge; it can be resource-intensive otherwise. Therefore, I use it as the second step after the initial automated features elimination described above.", "I remove features with low or negative permutation importance while checking improvement for the model performance.", "Some features can have high permutation importance but define the very similar aspects of the data. In this case, finding a correlation between features can help to identify redundant features.", "Correlation is a statistical technique that can show whether and how strongly pairs of variables are related.", "When the correlation is equal to 1 the features are perfectly correlatedWhen the correlation is zero, the features are totally independent", "Features with high correlation are more linearly dependent and hence have almost the same effect on the dependent variable. Hence, when two features are highly correlated, we can drop one of them.", "The correlation matrix can be calculated simply as corr = X.corr(), where vector X contained all columns with considered features.", "The correlation matrix between all features has the 1 on the diagonal elements, as the feature is 100% correlated with itself. The off-diagonal elements with high correlation values indicate redundant features.", "Removing redundant features (those, that are highly correlated) manually one-by-one while checking the metric on validation helps to reduce the features set and improve the performance of the GBDT. To do so, simply compare the correlation between different features (off-diagonal elements) and try to remove one of two features that have a correlation higher than some threshold (e.g. 0.9)", "Another way to reduce the number of features is by using principal component analysis (PCA) [13]. This technique allows a reduction in the dimensionality of the features space while finding the most prominent components from the combination of features.", "The main idea of PCA is to reduce the dimensionality of a data set consisting of many variables correlated with each other, either heavily or lightly, while retaining the variation present in the dataset, up to the maximum extent.", "It can be easily implemented using the sklearn library [14]. PCA is sensitive to scaling and the features need to be normalised before applying this algorithm. An example:", "There are many courses and blog-posts, where you can read in details about this technique, for example, in [15].", "There are, of course, other methods for features selection, such as using autoencoders, P-value, LightGBM importance, and others. Here I described the subset of my personal choice, that I developed during competitive machine learning on Kaggle.", "I perform steps 1\u20132\u20133 one by one for the features selection. Here is the example of applying feature selection techniques at Kaggle competition PLAsTiCC Astronomical Classification [16]. At first, the features were selected using automatic Recursive feature elimination with cross-validation [10], giving 167 features. Then, the remaining features were ranked using the permutation importance algorithm implemented in eli5 and top features were chosen. As we can see, 50 features were not sufficient and 100 features performed better for this data set. Finally, the redundant features were removed from the top 100 using correlation, leaving us with selected 82 features [17].", "Such a multi-step approach improves the model performance compared to the automatic feature selection, but not dramatically. Defining and designing the most relevant features is still the priority to get the best model with GBDT classifiers/ regressors.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data scientist, artist, CV engineer, physicist"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fd5af31f7276&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffeature-selection-in-machine-learning-d5af31f7276&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffeature-selection-in-machine-learning-d5af31f7276&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffeature-selection-in-machine-learning-d5af31f7276&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffeature-selection-in-machine-learning-d5af31f7276&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----d5af31f7276--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d5af31f7276--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@tatihabru?source=post_page-----d5af31f7276--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@tatihabru?source=post_page-----d5af31f7276--------------------------------", "anchor_text": "Tatiana Gabruseva"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F450d72191b4a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffeature-selection-in-machine-learning-d5af31f7276&user=Tatiana+Gabruseva&userId=450d72191b4a&source=post_page-450d72191b4a----d5af31f7276---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd5af31f7276&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffeature-selection-in-machine-learning-d5af31f7276&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd5af31f7276&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffeature-selection-in-machine-learning-d5af31f7276&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@brune", "anchor_text": "Kai Brune"}, {"url": "https://unsplash.com/photos/Z-gTyL0dOH4", "anchor_text": "Upslash"}, {"url": "https://medium.com/@mohtedibf", "anchor_text": "Mohtadi Ben Fraj"}, {"url": "https://medium.com/all-things-ai/in-depth-parameter-tuning-for-gradient-boosting-3363992e9bae", "anchor_text": "medium"}, {"url": "https://pixabay.com/users/qimono-1962238/", "anchor_text": "qimono"}, {"url": "https://pixabay.com/illustrations/cubes-choice-one-yellow-light-2492010/", "anchor_text": "pixabay"}, {"url": "https://pixabay.com/illustrations/calculator-hand-robot-count-695084/", "anchor_text": "pixabay"}, {"url": "https://knowyourmeme.com/memes/spider-man-pointing-at-spider-man", "anchor_text": "knowyourmeme.com"}, {"url": "https://scikit-learn.org/stable/auto_examples/decomposition/plot_kernel_pca.html#sphx-glr-auto-examples-decomposition-plot-kernel-pca-py", "anchor_text": "scikit-learn.org"}, {"url": "https://lightgbm.readthedocs.io/en/latest/", "anchor_text": "https://lightgbm.readthedocs.io/en/latest/"}, {"url": "https://xgboost.readthedocs.io/en/latest/tutorials/index.html", "anchor_text": "https://xgboost.readthedocs.io/en/latest/tutorials/index.html"}, {"url": "https://en.wikipedia.org/wiki/International_Standard_Book_Number", "anchor_text": "ISBN"}, {"url": "https://en.wikipedia.org/wiki/Special:BookSources/0-387-31073-8", "anchor_text": "0\u2013387\u201331073\u20138"}, {"url": "https://numpy.org/", "anchor_text": "https://numpy.org/"}, {"url": "https://www.scipy.org/", "anchor_text": "https://www.scipy.org/"}, {"url": "https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction", "anchor_text": "https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction"}, {"url": "https://tsfresh.readthedocs.io/en/latest/text/introduction.html", "anchor_text": "https://tsfresh.readthedocs.io/en/latest/text/introduction.html"}, {"url": "https://en.wikipedia.org/wiki/Gradient_boosting", "anchor_text": "https://en.wikipedia.org/wiki/Gradient_boosting"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html", "anchor_text": "https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html", "anchor_text": "https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html"}, {"url": "https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf", "anchor_text": "https://www.stat.berkeley.edu/%7Ebreiman/randomforest2001.pdf"}, {"url": "https://eli5.readthedocs.io/en/latest/overview.html", "anchor_text": "https://eli5.readthedocs.io/en/latest/overview.html"}, {"url": "https://en.wikipedia.org/wiki/Principal_component_analysis", "anchor_text": "https://en.wikipedia.org/wiki/Principal_component_analysis"}, {"url": "https://scikit-learn.org/stable/modules/classes.html#module-sklearn.decomposition", "anchor_text": "sklearn.decomposition"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html", "anchor_text": "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html"}, {"url": "https://medium.com/apprentice-journal/pca-application-in-machine-learning-4827c07a61db", "anchor_text": "https://medium.com/apprentice-journal/pca-application-in-machine-learning-4827c07a61db"}, {"url": "https://www.kaggle.com/c/PLAsTiCC-2018", "anchor_text": "https://www.kaggle.com/c/PLAsTiCC-2018"}, {"url": "https://arxiv.org/pdf/1909.05032.pdf", "anchor_text": "https://arxiv.org/pdf/1909.05032.pdf"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----d5af31f7276---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/feature-selection?source=post_page-----d5af31f7276---------------feature_selection-----------------", "anchor_text": "Feature Selection"}, {"url": "https://medium.com/tag/permutation-importance?source=post_page-----d5af31f7276---------------permutation_importance-----------------", "anchor_text": "Permutation Importance"}, {"url": "https://medium.com/tag/eli5?source=post_page-----d5af31f7276---------------eli5-----------------", "anchor_text": "Eli5"}, {"url": "https://medium.com/tag/decision-tree?source=post_page-----d5af31f7276---------------decision_tree-----------------", "anchor_text": "Decision Tree"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd5af31f7276&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffeature-selection-in-machine-learning-d5af31f7276&user=Tatiana+Gabruseva&userId=450d72191b4a&source=-----d5af31f7276---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd5af31f7276&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffeature-selection-in-machine-learning-d5af31f7276&user=Tatiana+Gabruseva&userId=450d72191b4a&source=-----d5af31f7276---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd5af31f7276&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffeature-selection-in-machine-learning-d5af31f7276&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d5af31f7276--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fd5af31f7276&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffeature-selection-in-machine-learning-d5af31f7276&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----d5af31f7276---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----d5af31f7276--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----d5af31f7276--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----d5af31f7276--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----d5af31f7276--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----d5af31f7276--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----d5af31f7276--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----d5af31f7276--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----d5af31f7276--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@tatihabru?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@tatihabru?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Tatiana Gabruseva"}, {"url": "https://medium.com/@tatihabru/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "66 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F450d72191b4a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffeature-selection-in-machine-learning-d5af31f7276&user=Tatiana+Gabruseva&userId=450d72191b4a&source=post_page-450d72191b4a--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F450d72191b4a%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffeature-selection-in-machine-learning-d5af31f7276&user=Tatiana+Gabruseva&userId=450d72191b4a&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}