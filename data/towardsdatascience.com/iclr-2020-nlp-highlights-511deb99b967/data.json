{"url": "https://towardsdatascience.com/iclr-2020-nlp-highlights-511deb99b967", "time": 1683007858.658258, "path": "towardsdatascience.com/iclr-2020-nlp-highlights-511deb99b967/", "webpage": {"metadata": {"title": "ICLR 2020: NLP Highlights. A Summary of ICLR 2020 with focus on\u2026 | by Hieu Nguyen | Towards Data Science", "h1": "ICLR 2020: NLP Highlights", "description": "A Summary of ICLR 2020 with a focus on Natural Language Processing (NLP), especially on Transformers and BERT variants. Computer Vision, Quantum-inspired algorithms, General Deep Learning methods, etc. will be mentioned as well."}, "outgoing_paragraph_urls": [{"url": "https://iclr.cc/virtual_2020/index.html", "anchor_text": "All materials can be accessed on ICLR Virtual Page", "paragraph_index": 6}, {"url": "https://paperswithcode.com/conference/iclr-2020-1", "anchor_text": "Papers with code for ICLR 2020", "paragraph_index": 7}, {"url": "https://venturebeat.com/2020/05/02/yann-lecun-and-yoshua-bengio-self-supervised-learning-is-the-key-to-human-level-intelligence/", "anchor_text": "VentureBeat", "paragraph_index": 14}, {"url": "https://openreview.net/forum?id=HJeT3yrtDr", "anchor_text": "Cross-Lingual Ability of Multilingual BERT: An Empirical Study", "paragraph_index": 15}, {"url": "https://openreview.net/forum?id=r1xMH1BtvB", "anchor_text": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators", "paragraph_index": 17}, {"url": "https://openreview.net/forum?id=HJlnC1rKPB", "anchor_text": "On the Relationship between Self-Attention and Convolutional Layers", "paragraph_index": 19}, {"url": "https://openreview.net/forum?id=BJgQ4lSFPH", "anchor_text": "StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding", "paragraph_index": 21}, {"url": "https://openreview.net/forum?id=ByxRM0Ntvr", "anchor_text": "Are Transformers universal approximators of sequence-to-sequence functions?", "paragraph_index": 23}, {"url": "https://www.linkedin.com/in/hieu-chi-nguyen/", "anchor_text": "LinkedIn", "paragraph_index": 25}], "all_paragraphs": ["This post is some of my highlights for ICLR 2020. Because my current research is Natural Language Processing (NLP), this post will focus on this area. However, Computer Vision, Quantum-inspired algorithms, General Deep Learning methods, etc. will be mentioned as well.", "This is also my first post on Medium, so I would love to connect and hear feedback from the community as well!", "\u201cThe International Conference on Learning Representations (ICLR) is the premier gathering of professionals dedicated to the advancement of the branch of artificial intelligence called representation learning, but generally referred to as deep learning.\u201d", "ICLR is one of the top-tier international AI conferences, along with NIPS and ICML. One of the differences is that ICLR\u2019s main focus is on Deep Learning.", "Participants can come from many backgrounds, \u201cfrom academic and industrial researchers, to entrepreneurs and engineers, to graduate students and postdocs.\u201d", "The review process is double-blind and open-review, meaning that the authors and reviewers do not know each other during the review process, and all reviews and authors\u2019 responses can be publicly viewed.", "All materials can be accessed on ICLR Virtual Page", "Papers with code for ICLR 2020", "My main interest is in Natural Language Processing (NLP), so I will focus my recap on this area.", "There are many other topics of NLP that I do not cover here, so please come to the website to read and discover those topics.", "There are several topics I found are emerging in the conferences:", "This year is the first time ICLR is held virtually, and from my perspective the organizers have done an excellent job: Accepted papers\u2019 search engine, papers\u2019 visualization with the measure of similarity, Chat forums for authors and participants, Zoom rooms, etc.", "I had a chance to ask many of the authors directly, as well as participating in public forums on NLP and Huggingface (a popular NLP framework) researchers. I also had a chance to participate with companies\u2019 sponsor chats.", "The last talk from Yann LeCun and Yoshua Bengio about the directions of deep learning was very insightful as well, such as how to apply self-supervised learning successes in NLP to Computer Vision, how they think quantum algorithms, neuroscience ideas will also be important. Their stories on how they get to where they are now are inspiring as well.", "You can read one of the summaries on VentureBeat.", "Cross-Lingual Ability of Multilingual BERT: An Empirical Study", "One of the popular hypotheses of multilingual models is that they learn well because of lexical overlaps between languages. In this paper, the authors find out that \u201clexical overlap between languages plays a negligible role in the cross-lingual success, while the depth of the network is an integral part of it.\u201d (Karthikeyan et al., 2020)", "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators", "This paper opens the door for entities with low-computing resources to pre-train their own language models. In particular, the authors \u201ctrain a model on one GPU for 4 days that outperforms GPT (trained using 30x more compute) on the GLUE natural language understanding benchmark.\u201d (Clark et al., 2020)", "On the Relationship between Self-Attention and Convolutional Layers", "After the success of replacing RNN with attention-based models with NLP tasks, researchers are now exploring if they can do the same with convolutional layers and in Computer Vision tasks. In this paper, the authors \u201cprove that a multi-head self-attention layer with sufficient number of heads is at least as expressive as any convolutional layer. Our numerical experiments then show that self-attention layers attend to pixel-grid patterns similarly to CNN layers, corroborating our analysis.\u201d (Cordornnier et al., 2019)", "StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding", "In this paper, the authors add two additional pre-training tasks which \u201cleverage language structures at the word and sentence levels\u201d and achieve SOTA results in GLUE benchmark. (Wang et al., 2019)", "Are Transformers universal approximators of sequence-to-sequence functions?", "This paper is more on the theory side. In this paper, the authors \u201cshow that Transformer models can universally approximate arbitrary continuous sequence-to-sequence functions on a compact domain.\u201d They also prove that \u201cfixed width self-attention layers can compute contextual mappings of the input sequences, playing a key role in the universal approximation property of Transformers.\u201d (Yun et al., 2019)", "I am an AI Engineer and Data Engineer, focusing on researching state-of-the-art AI solutions and building machine learning systems. You can reach me on LinkedIn.", "Cordonnier, Jean-Baptiste, Andreas Loukas, and Martin Jaggi. \u201cOn the Relationship between Self-Attention and Convolutional Layers.\u201d arXiv preprint arXiv:1911.03584 (2019).", "Lee, Cheolhyoung, Kyunghyun Cho, and Wanmo Kang. \u201cMixout: Effective Regularization to Finetune Large-scale Pretrained Language Models.\u201d arXiv preprint arXiv:1909.11299 (2019).", "You, Yang, et al. \u201cLarge batch optimization for deep learning: Training bert in 76 minutes.\u201d International Conference on Learning Representations. 2019.", "Lee, Cheolhyoung, Kyunghyun Cho, and Wanmo Kang. \u201cMixout: Effective Regularization to Finetune Large-scale Pretrained Language Models.\u201d arXiv preprint arXiv:1909.11299 (2019).", "Tu, Zhouzhou, Fengxiang He, Dacheng Tao. \u201cUnderstanding Generalization in Recurrent Neural Networks.\u201d International Conference on Learning Representations. 2019", "Karthikeyan, K., et al. \u201cCross-lingual ability of multilingual BERT: An empirical study.\u201d International Conference on Learning Representations. 2020.", "Kerenidis, Iordanis, Jonas Landman, and Anupam Prakash. \u201cQuantum Algorithms for Deep Convolutional Neural Networks.\u201d arXiv preprint arXiv:1911.01117 (2019).", "Chen, Yu, Lingfei Wu, and Mohammed J. Zaki. \u201cReinforcement learning based graph-to-sequence model for natural question generation.\u201d arXiv preprint arXiv:1908.04942 (2019).", "Zhang, Matthew Shunshi, and Bradly Stadie. \u201cOne-Shot Pruning of Recurrent Neural Networks by Jacobian Spectrum Evaluation.\u201d arXiv preprint arXiv:1912.00120 (2019).", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F511deb99b967&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ficlr-2020-nlp-highlights-511deb99b967&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ficlr-2020-nlp-highlights-511deb99b967&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ficlr-2020-nlp-highlights-511deb99b967&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ficlr-2020-nlp-highlights-511deb99b967&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----511deb99b967--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----511deb99b967--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@nguyenhc95?source=post_page-----511deb99b967--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@nguyenhc95?source=post_page-----511deb99b967--------------------------------", "anchor_text": "Hieu Nguyen"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F46906d99ca89&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ficlr-2020-nlp-highlights-511deb99b967&user=Hieu+Nguyen&userId=46906d99ca89&source=post_page-46906d99ca89----511deb99b967---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F511deb99b967&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ficlr-2020-nlp-highlights-511deb99b967&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F511deb99b967&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ficlr-2020-nlp-highlights-511deb99b967&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://iclr.cc/", "anchor_text": "https://iclr.cc/"}, {"url": "https://iclr.cc/", "anchor_text": "https://iclr.cc/"}, {"url": "https://iclr.cc/virtual_2020/index.html", "anchor_text": "All materials can be accessed on ICLR Virtual Page"}, {"url": "https://paperswithcode.com/conference/iclr-2020-1", "anchor_text": "Papers with code for ICLR 2020"}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "Vaswani et al., 2017"}, {"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "Devlin et al., 2018"}, {"url": "https://arxiv.org/abs/2002.06622", "anchor_text": "Shi et al., 2020"}, {"url": "https://arxiv.org/abs/1908.04211", "anchor_text": "Brunner et al., 2020"}, {"url": "https://arxiv.org/abs/1912.10077", "anchor_text": "Yun et al., 2019"}, {"url": "https://arxiv.org/abs/1911.03584", "anchor_text": "Cordonnier et al., 2019"}, {"url": "https://arxiv.org/abs/1908.04577", "anchor_text": "Wang et al. 2019"}, {"url": "https://arxiv.org/abs/1909.11299", "anchor_text": "Lee et al., 2019"}, {"url": "https://arxiv.org/abs/2004.11886", "anchor_text": "Wu et al., 2020"}, {"url": "https://arxiv.org/abs/1909.11942", "anchor_text": "Lan et al., 2019"}, {"url": "https://arxiv.org/abs/2001.04451", "anchor_text": "Kitaev et al., 2020"}, {"url": "https://arxiv.org/abs/2003.10555", "anchor_text": "Clark et al., 2020"}, {"url": "https://arxiv.org/abs/1911.05507", "anchor_text": "Rae et al., 2019"}, {"url": "https://arxiv.org/abs/1909.11556", "anchor_text": "Fan et al., 2019"}, {"url": "https://arxiv.org/abs/1904.00962", "anchor_text": "You et al., 2019"}, {"url": "https://arxiv.org/abs/1909.01792", "anchor_text": "Melis et al., 2019"}, {"url": "https://arxiv.org/abs/1905.13715", "anchor_text": "Orhan et al., 2019"}, {"url": "https://openreview.net/forum?id=rkgg6xBYDH", "anchor_text": "Tu et al., 2019"}, {"url": "https://arxiv.org/abs/1912.07840", "anchor_text": "Karthikeyan et al., 2019"}, {"url": "https://openreview.net/forum?id=HyeYTgrFPB", "anchor_text": "Berend 2020"}, {"url": "https://arxiv.org/abs/2002.03518", "anchor_text": "Cao et al., 2020"}, {"url": "https://arxiv.org/abs/1910.04708", "anchor_text": "Wang et al., 2019"}, {"url": "https://arxiv.org/abs/1911.04975", "anchor_text": "Panahi et al., 2019"}, {"url": "https://arxiv.org/abs/1911.01117", "anchor_text": "Kerenidis et al., 2019"}, {"url": "https://arxiv.org/abs/1908.08530", "anchor_text": "Su et al., 2019"}, {"url": "https://arxiv.org/abs/1906.02768", "anchor_text": "Yu et al., 2019"}, {"url": "https://arxiv.org/abs/1909.00668", "anchor_text": "Clift et al., 2019"}, {"url": "https://arxiv.org/abs/1912.00120", "anchor_text": "Zhang et al., 2019"}, {"url": "https://venturebeat.com/2020/05/02/yann-lecun-and-yoshua-bengio-self-supervised-learning-is-the-key-to-human-level-intelligence/", "anchor_text": "VentureBeat"}, {"url": "https://openreview.net/forum?id=HJeT3yrtDr", "anchor_text": "Cross-Lingual Ability of Multilingual BERT: An Empirical Study"}, {"url": "https://openreview.net/forum?id=r1xMH1BtvB", "anchor_text": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators"}, {"url": "https://openreview.net/forum?id=r1xMH1BtvB", "anchor_text": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators"}, {"url": "https://github.com/google-research/electra", "anchor_text": "Github"}, {"url": "https://huggingface.co/transformers/model_doc/electra.html", "anchor_text": "Huggingface"}, {"url": "https://github.com/nguyenvulebinh/vietnamese-electra", "anchor_text": "Github"}, {"url": "https://openreview.net/forum?id=HJlnC1rKPB", "anchor_text": "On the Relationship between Self-Attention and Convolutional Layers"}, {"url": "https://openreview.net/forum?id=HJlnC1rKPB", "anchor_text": "On the Relationship between Self-Attention and Convolutional Layers"}, {"url": "https://github.com/epfml/attention-cnn", "anchor_text": "Github"}, {"url": "https://openreview.net/forum?id=BJgQ4lSFPH", "anchor_text": "StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding"}, {"url": "https://openreview.net/forum?id=BJgQ4lSFPH", "anchor_text": "StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding"}, {"url": "https://openreview.net/forum?id=ByxRM0Ntvr", "anchor_text": "Are Transformers universal approximators of sequence-to-sequence functions?"}, {"url": "https://www.linkedin.com/in/hieu-chi-nguyen/", "anchor_text": "LinkedIn"}, {"url": "https://medium.com/tag/ai?source=post_page-----511deb99b967---------------ai-----------------", "anchor_text": "AI"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----511deb99b967---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----511deb99b967---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/nlp?source=post_page-----511deb99b967---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----511deb99b967---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F511deb99b967&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ficlr-2020-nlp-highlights-511deb99b967&user=Hieu+Nguyen&userId=46906d99ca89&source=-----511deb99b967---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F511deb99b967&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ficlr-2020-nlp-highlights-511deb99b967&user=Hieu+Nguyen&userId=46906d99ca89&source=-----511deb99b967---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F511deb99b967&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ficlr-2020-nlp-highlights-511deb99b967&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----511deb99b967--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F511deb99b967&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ficlr-2020-nlp-highlights-511deb99b967&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----511deb99b967---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----511deb99b967--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----511deb99b967--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----511deb99b967--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----511deb99b967--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----511deb99b967--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----511deb99b967--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----511deb99b967--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----511deb99b967--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@nguyenhc95?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@nguyenhc95?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Hieu Nguyen"}, {"url": "https://medium.com/@nguyenhc95/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "84 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F46906d99ca89&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ficlr-2020-nlp-highlights-511deb99b967&user=Hieu+Nguyen&userId=46906d99ca89&source=post_page-46906d99ca89--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F31c4ed923dc7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ficlr-2020-nlp-highlights-511deb99b967&newsletterV3=46906d99ca89&newsletterV3Id=31c4ed923dc7&user=Hieu+Nguyen&userId=46906d99ca89&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}