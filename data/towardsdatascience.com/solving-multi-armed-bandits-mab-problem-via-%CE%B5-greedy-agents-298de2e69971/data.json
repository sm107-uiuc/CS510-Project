{"url": "https://towardsdatascience.com/solving-multi-armed-bandits-mab-problem-via-%CE%B5-greedy-agents-298de2e69971", "time": 1683007239.339905, "path": "towardsdatascience.com/solving-multi-armed-bandits-mab-problem-via-%CE%B5-greedy-agents-298de2e69971/", "webpage": {"metadata": {"title": "Solving Multi-Armed Bandits (MAB) problem via \u03b5-greedy agents | by Ashik Poovanna | Towards Data Science", "h1": "Solving Multi-Armed Bandits (MAB) problem via \u03b5-greedy agents", "description": "A Slot machine that you see in a typical casino is another name for the 1-armed bandit. The name 1-armed comes from the lever/arm that one needed to pull to stop the spinning reels (Nowadays buttons\u2026"}, "outgoing_paragraph_urls": [{"url": "https://mitpress.mit.edu/books/reinforcement-learning-second-edition", "anchor_text": "https://mitpress.mit.edu/books/reinforcement-learning-second-edition", "paragraph_index": 19}, {"url": "http://absurd-musings.com", "anchor_text": "absurd-musings.com", "paragraph_index": 21}], "all_paragraphs": ["(This is my implementation & revision of concepts from Chapter-2 of [1])", "Before we talk about MABs, we need to understand what a 1-armed bandit is.", "A Slot machine that you see in a typical casino is another name for the 1-armed bandit. The name 1-armed comes from the lever/arm that one needed to pull to stop the spinning reels (Nowadays buttons to do the job) and bandits cause they loot you, duh!", "These Slot machines have an RNG (Random Number Generator) chip implanted in the machine that generates a random combination of symbols. So a gambler invests some amount in the machine and plays the game. Some rare combinations are rewarded while most are not. All the payouts are done while maintaining an average payback percentage in the long run.", "Now MABs (or k-armed bandits) can be thought of as multiple slot machines with different payback percentages or as a single slot machine with k-arms and each arm having a different payback percentage. The aim of a player/agent is to come up with a policy (In this case it is non-associative) for choosing an action (here the action is to decide which arm to pull) at each time step (t) such that the total reward at the end of some fixed time period (T) is maximized.", "The MAB problem is a subclass of a more general class of Stochastic Scheduling problems [5] that deals with optimal resource allocations. Also its a single state MDP (Markov Decision process) problem.", "Real-World Applications of MABs can be read here [6].", "We are going to make some assumptions to simplify the experiment -", "To simulate the workings of the MAB problem, [1] suggests sampling the true average action-value for all arms(actions) from a Standard Normal Distribution N(0,1). So the arm having a higher average action-value leads to better payouts in the long run. These values are hidden from the agents/players. To solve the problem we have to figure this out by playing the bandits multiple times. What is available to the agents are the immediate rewards and these are sampled from Normal distributions N(q*(a),1) for the respective arms/actions.", "What makes this problem non-trivial is that at any point in time we don\u2019t know what the true average action-values (q*(a)) for all actions are. If we knew that we could just choose the action (a) having the max q*(a). So we need to come up with some ways to estimate q*(a) at each time step.", "One way to solve the problem is via a Pure Greedy Action Selection Method. In this method, the agent always exploits the current knowledge (Q_t(a)s) to maximize the immediate reward (as shown in the below equation).", "In Action-value estimation methods, the aim is to estimate the true action-values for each action (i.e Q_t(a)) and using this to decide which action to take in the given time step using equation (1).", "Now according to [1] for the Stationary k-armed bandit problems, action-value estimation can be done using the two methods described below-", "2. Incremental Update method (IUM): It is a Mathematical Optimization of the previous SAM. Notice how the equation below is memory efficient since you don\u2019t need to store history of past rewards and there is no need for average re-calculations at each time step. (I\u2019ve used this in my code)", "The methods presented above creates a dilemma. If you solely chose the action based on the action-value estimates you will end up with what we call a pure greedy agent (The pure greedy agent always exploits the current knowledge to maximize its immediate reward). But now the question arises what if there was another untried arm that could have led to better total reward in the end but ended up not being selected (or explored) at the start due to lower initial action-values. This is where \u03b5-greedy agents come in. They try to handle this dilemma.", "As described in the figure above the idea behind a simple \u03b5-greedy bandit algorithm is to get the agent to explore other actions randomly with a very small probability (\u03b5) while at other times you go with selecting the action greedily. It can be asymptotically shown that the estimated action values converge to true action-values, but the practical effectiveness of the algorithm itself is unknown.", "Another alternative to the dilemma of explorations v/s exploitation is to use the Upper Confidence Bound (UCB) Action-Selection Method. In this approach during exploration instead of randomly selecting from available action space, UCB takes into account the uncertainties (with (c>0) determining the confidence levels of the estimate that decide the degree of exploration) in those estimates. But this method is not practically found to scale well beyond the MAB problem.", "The plot above compares the performance of different agents on the same 10-armed bandit problem. The statistics are obtained by averaging over 1000 such runs.Some other inferences that hold for asymptotic/more episodes, different experimental settings & might not be visible here:", "Thank you for reading the article. I hope it was useful. Any corrections or advice is welcome. I hope to improve my understandings of RL from the grass-root levels. I plan to cover Non-Stationary MAB and Gradient Bandit Algorithm in the next article.", "[1] https://mitpress.mit.edu/books/reinforcement-learning-second-edition : All the algorithms and equations & derivations except the code are from this book.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Software Engineer @ Amazon | absurd-musings.com"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F298de2e69971&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsolving-multi-armed-bandits-mab-problem-via-%CE%B5-greedy-agents-298de2e69971&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsolving-multi-armed-bandits-mab-problem-via-%CE%B5-greedy-agents-298de2e69971&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsolving-multi-armed-bandits-mab-problem-via-%CE%B5-greedy-agents-298de2e69971&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsolving-multi-armed-bandits-mab-problem-via-%CE%B5-greedy-agents-298de2e69971&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----298de2e69971--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----298de2e69971--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@NoblesseCoder?source=post_page-----298de2e69971--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@NoblesseCoder?source=post_page-----298de2e69971--------------------------------", "anchor_text": "Ashik Poovanna"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3fdaf344b12f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsolving-multi-armed-bandits-mab-problem-via-%CE%B5-greedy-agents-298de2e69971&user=Ashik+Poovanna&userId=3fdaf344b12f&source=post_page-3fdaf344b12f----298de2e69971---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F298de2e69971&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsolving-multi-armed-bandits-mab-problem-via-%CE%B5-greedy-agents-298de2e69971&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F298de2e69971&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsolving-multi-armed-bandits-mab-problem-via-%CE%B5-greedy-agents-298de2e69971&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://github.com/NoblesseCoder/gym-karmedbandits", "anchor_text": "k-armed bandits environment"}, {"url": "https://github.com/NoblesseCoder/reinforcei", "anchor_text": "reinforcei"}, {"url": "https://mitpress.mit.edu/books/reinforcement-learning-second-edition", "anchor_text": "https://mitpress.mit.edu/books/reinforcement-learning-second-edition"}, {"url": "https://youtu.be/7Wkubf1PrWg", "anchor_text": "https://youtu.be/7Wkubf1PrWg"}, {"url": "https://youtu.be/BFlRH99TQOw", "anchor_text": "https://youtu.be/BFlRH99TQOw"}, {"url": "https://youtu.be/YBQhCP1IdWo", "anchor_text": "https://youtu.be/4wzg-8QKC5s"}, {"url": "https://en.wikipedia.org/wiki/Stochastic_scheduling", "anchor_text": "https://en.wikipedia.org/wiki/Stochastic_scheduling"}, {"url": "https://www.researchgate.net/publication/332604222_A_Survey_on_Practical_Applications_of_Multi-Armed_and_Contextual_Bandits", "anchor_text": "https://www.researchgate.net/publication/332604222_A_Survey_on_Practical_Applications_of_Multi-Armed_and_Contextual_Bandits"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----298de2e69971---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----298de2e69971---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----298de2e69971---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/python?source=post_page-----298de2e69971---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----298de2e69971---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F298de2e69971&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsolving-multi-armed-bandits-mab-problem-via-%25CE%25B5-greedy-agents-298de2e69971&user=Ashik+Poovanna&userId=3fdaf344b12f&source=-----298de2e69971---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F298de2e69971&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsolving-multi-armed-bandits-mab-problem-via-%25CE%25B5-greedy-agents-298de2e69971&user=Ashik+Poovanna&userId=3fdaf344b12f&source=-----298de2e69971---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F298de2e69971&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsolving-multi-armed-bandits-mab-problem-via-%CE%B5-greedy-agents-298de2e69971&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----298de2e69971--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F298de2e69971&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsolving-multi-armed-bandits-mab-problem-via-%CE%B5-greedy-agents-298de2e69971&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----298de2e69971---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----298de2e69971--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----298de2e69971--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----298de2e69971--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----298de2e69971--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----298de2e69971--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----298de2e69971--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----298de2e69971--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----298de2e69971--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@NoblesseCoder?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@NoblesseCoder?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ashik Poovanna"}, {"url": "https://medium.com/@NoblesseCoder/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "104 Followers"}, {"url": "http://absurd-musings.com", "anchor_text": "absurd-musings.com"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3fdaf344b12f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsolving-multi-armed-bandits-mab-problem-via-%CE%B5-greedy-agents-298de2e69971&user=Ashik+Poovanna&userId=3fdaf344b12f&source=post_page-3fdaf344b12f--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F21faf9612b63&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsolving-multi-armed-bandits-mab-problem-via-%CE%B5-greedy-agents-298de2e69971&newsletterV3=3fdaf344b12f&newsletterV3Id=21faf9612b63&user=Ashik+Poovanna&userId=3fdaf344b12f&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}