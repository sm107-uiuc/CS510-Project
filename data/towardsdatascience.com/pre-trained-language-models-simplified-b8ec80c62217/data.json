{"url": "https://towardsdatascience.com/pre-trained-language-models-simplified-b8ec80c62217", "time": 1683002146.028509, "path": "towardsdatascience.com/pre-trained-language-models-simplified-b8ec80c62217/", "webpage": {"metadata": {"title": "Pre-trained Language Models: Simplified | by Prakhar Ganesh | Towards Data Science", "h1": "Pre-trained Language Models: Simplified", "description": "The intuition behind pre-trained language models is to create a black box which understands the language and can then be asked to do any specific task in that language. The idea is to create the\u2026"}, "outgoing_paragraph_urls": [], "all_paragraphs": ["The intuition behind pre-trained language models is to create a black box which understands the language and can then be asked to do any specific task in that language. The idea is to create the machine equivalent of a \u2018well-read\u2019 human being.", "The language model is first fed a large amount of unannotated data (for example, the complete Wikipedia dump). This lets the model learn the usage of various words and how the language is written in general. The model is now transferred to an NLP task where it is fed another smaller task-specific dataset, which is used to fine tune and create the final model capable of performing the aforementioned task.", "In one sentence : they are better read!! A model which trains only on the task-specific dataset needs to both understand the language and the task using a comparatively smaller dataset. The language model on the other hand already understands the language since it has \u2018read\u2019 large language dumps during pre-training. Thus the language model can directly fine tune itself to match the required task and performs better than the existing SOTA.", "Every word in NLP needs to be represented mathematically in order to let machines do further processing. You can gain more intuitions on this through my earlier blog on Distributed Vector representation\u2026", "Many different algorithms have been proposed to create these embeddings, by pre-training the models on a separate larger dataset to capture the essence of the language. For example, Word2Vec embeddings gained incredible popularity and these embeddings were directly used for a number of tasks across NLP.", "However, these word representations are learned in a generalized context and do not represent task-specific information. This is where the fine-tuning part of a language model comes into account. Directly using the pre-trained embeddings can decrease the overall model size but forces us to only utilize generalized word representations. Language model fine-tuning on the other hand allows the user to fine-tune these word embeddings/representations by training on the task-specific dataset.", "For example, the representation of the word \u2018current\u2019 might have good relation with both \u2018news\u2019 and \u2018electricity\u2019 in a generalised context. However, for a specific task which talks about electric circuits, allowing the model to fine-tune the word representations such that \u2018current\u2019 and \u2018electricity\u2019 match better can help improve the model performance.", "BERT (Bi-directional Encoder Representations from Transformers) was proposed by Google last year and was able to single-handedly achieve SOTA performances on 11 separate NLP tasks!! It has since been the source of multiple language models spawning from it.", "The model owes it\u2019s success mainly to the training methods proposed in the paper. The two training protocols, namely \u2018Masked LM\u2019 and \u2018NSP : Next Sentence Prediction\u2019 (which was later improved to \u2018SOP : Sentence Order Prediction\u2019), helps BERT learn from the huge language corpus available to it.", "The \u2018Masked LM\u2019 task is implemented by masking 15% of the word randomly in every sentence and training the model to predict them. The \u2018SOP\u2019 task is a classification task with two sentences input and the model is expected to recognize the original order between these 2 sentences, which increases it\u2019s document level understanding. The impact these training tasks create and the internal working of BERT requires a more detailed analysis which I will not go into right now.", "While different variations of language models are starting to dominate across various NLP tasks, everybody is starting to realize two important facts. One, although the original BERT model had the right idea, it was poorly trained and thus has incredible untapped potential. And two, the monstrous size of these pre-trained language models like BERT are a big hurdle towards future research and deployability of these models. It seems like these two major questions need to be answered before this field can move forward.", "This blog is a part of an effort to create simplified introductions to the field of Machine Learning. Follow the complete series here", "Or simply read the next blog in the series", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Computer Vision and Deep Learning enthusiast"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fb8ec80c62217&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpre-trained-language-models-simplified-b8ec80c62217&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpre-trained-language-models-simplified-b8ec80c62217&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpre-trained-language-models-simplified-b8ec80c62217&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpre-trained-language-models-simplified-b8ec80c62217&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----b8ec80c62217--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b8ec80c62217--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@prakhargannu?source=post_page-----b8ec80c62217--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@prakhargannu?source=post_page-----b8ec80c62217--------------------------------", "anchor_text": "Prakhar Ganesh"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F318f2765b461&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpre-trained-language-models-simplified-b8ec80c62217&user=Prakhar+Ganesh&userId=318f2765b461&source=post_page-318f2765b461----b8ec80c62217---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb8ec80c62217&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpre-trained-language-models-simplified-b8ec80c62217&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb8ec80c62217&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpre-trained-language-models-simplified-b8ec80c62217&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/distributed-vector-representation-simplified-55bd2965333e", "anchor_text": "Distributed Vector Representation : SimplifiedArguably the most essential feature representation method in Machine Learningtowardsdatascience.com"}, {"url": "https://towardsdatascience.com/machine-learning-simplified-1fe22fec0fac", "anchor_text": "Machine Learning : SimplifiedKnow it before you dive intowardsdatascience.com"}, {"url": "https://towardsdatascience.com/types-of-convolution-kernels-simplified-f040cb307c37", "anchor_text": "Types of Convolution Kernels : SimplifiedAn intuitive introduction to different variations of the glamorous CNN layertowardsdatascience.com"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----b8ec80c62217---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/naturallanguageprocessing?source=post_page-----b8ec80c62217---------------naturallanguageprocessing-----------------", "anchor_text": "Naturallanguageprocessing"}, {"url": "https://medium.com/tag/language-model?source=post_page-----b8ec80c62217---------------language_model-----------------", "anchor_text": "Language Model"}, {"url": "https://medium.com/tag/surveys?source=post_page-----b8ec80c62217---------------surveys-----------------", "anchor_text": "Surveys"}, {"url": "https://medium.com/tag/data-science?source=post_page-----b8ec80c62217---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb8ec80c62217&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpre-trained-language-models-simplified-b8ec80c62217&user=Prakhar+Ganesh&userId=318f2765b461&source=-----b8ec80c62217---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb8ec80c62217&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpre-trained-language-models-simplified-b8ec80c62217&user=Prakhar+Ganesh&userId=318f2765b461&source=-----b8ec80c62217---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb8ec80c62217&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpre-trained-language-models-simplified-b8ec80c62217&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b8ec80c62217--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fb8ec80c62217&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpre-trained-language-models-simplified-b8ec80c62217&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----b8ec80c62217---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----b8ec80c62217--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----b8ec80c62217--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----b8ec80c62217--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----b8ec80c62217--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----b8ec80c62217--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----b8ec80c62217--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----b8ec80c62217--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----b8ec80c62217--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@prakhargannu?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@prakhargannu?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Prakhar Ganesh"}, {"url": "https://medium.com/@prakhargannu/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "649 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F318f2765b461&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpre-trained-language-models-simplified-b8ec80c62217&user=Prakhar+Ganesh&userId=318f2765b461&source=post_page-318f2765b461--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F6fd6e4d21dc5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpre-trained-language-models-simplified-b8ec80c62217&newsletterV3=318f2765b461&newsletterV3Id=6fd6e4d21dc5&user=Prakhar+Ganesh&userId=318f2765b461&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}