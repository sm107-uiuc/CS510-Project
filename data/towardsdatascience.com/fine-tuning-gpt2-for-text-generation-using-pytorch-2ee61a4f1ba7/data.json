{"url": "https://towardsdatascience.com/fine-tuning-gpt2-for-text-generation-using-pytorch-2ee61a4f1ba7", "time": 1683010031.844615, "path": "towardsdatascience.com/fine-tuning-gpt2-for-text-generation-using-pytorch-2ee61a4f1ba7/", "webpage": {"metadata": {"title": "Fine-tune GPT2 for Text Generation Using Pytorch | Towards Data Science", "h1": "Fine-tuning GPT2 for Text Generation Using Pytorch", "description": "Fine-tune GPT2 for text generation using Pytorch and Huggingface. We train on the CMU Book Summary Dataset to generate creative book summaries."}, "outgoing_paragraph_urls": [{"url": "https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf", "anchor_text": "Transformers", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/bert-text-classification-using-pytorch-723dfb8b6b5b", "anchor_text": "BERT for text classification", "paragraph_index": 0}, {"url": "https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf", "anchor_text": "GPT2", "paragraph_index": 0}, {"url": "http://www.cs.cmu.edu/~dbamman/booksummaries.html", "anchor_text": "CMU Books Summary Dataset", "paragraph_index": 1}, {"url": "https://huggingface.co/", "anchor_text": "Huggingface", "paragraph_index": 1}, {"url": "https://github.com/itsuncheng/fine-tuning-GPT2", "anchor_text": "here", "paragraph_index": 2}, {"url": "http://www.cs.cmu.edu/~dbamman/booksummaries.html", "anchor_text": "here", "paragraph_index": 4}, {"url": "https://github.com/itsuncheng/fine-tuning-GPT2/blob/master/preprocessing.ipynb", "anchor_text": "here", "paragraph_index": 6}, {"url": "https://huggingface.co/transformers/main_classes/model.html?highlight=generate#transformers.TFPreTrainedModel.generate", "anchor_text": "docstring", "paragraph_index": 19}, {"url": "https://github.com/itsuncheng/fine-tuning-GPT2/blob/master/generated_summaries.txt", "anchor_text": "here", "paragraph_index": 24}, {"url": "https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf", "anchor_text": "official paper", "paragraph_index": 25}, {"url": "https://openai.com/blog/better-language-models/", "anchor_text": "blog", "paragraph_index": 25}, {"url": "https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf", "anchor_text": "Attention Is All You Need", "paragraph_index": 27}, {"url": "https://www.linkedin.com/in/itsuncheng/", "anchor_text": "https://www.linkedin.com/in/itsuncheng/", "paragraph_index": 29}], "all_paragraphs": ["The past few years have been especially booming in the world of NLP. This is mainly due to one of the most important breakthroughs of NLP in the modern decade \u2014 Transformers. If you haven\u2019t read my previous article on BERT for text classification, go ahead and take a look! Another popular transformer that we will talk about today is GPT2. Developed by OpenAI, GPT2 is a large-scale transformer-based language model that is pre-trained on a large corpus of text: 8 million high-quality webpages. It results in competitive performance on multiple language tasks using only the pre-trained knowledge without explicitly training on them. GPT2 is really useful for language generation tasks as it is an autoregressive language model.", "Here in today\u2019s article, we will dive deeply into how to implement another popular transformer, GPT2, to write interesting and creative stories! Specifically, we will test the ability of GPT2 to write creative book summaries using the CMU Books Summary Dataset. We will be using the Huggingface repository for building our model and generating the texts.", "The entire codebase for this article can be viewed here.", "Before building the model, we need to download and preprocess the dataset first.", "We are using The CMU Books Summary Dataset, which contains 16,559 books extracted from Wikipedia along with the metadata including title, author, publication date, genres, and plot summary. Download the dataset here. Here is what the dataset looks like:", "For data preprocessing, we first split the entire dataset into the train, validation, and test datasets with the train-valid-test ratio: 70\u201320\u201310. We add a bos token <BOS> to the start of each summary and eos token <EOS> to the end of each summary for later training purposes. We finally save the summaries into .txt files, getting train.txt, valid.txt, test.txt.", "You can get the preprocessing notebook here.", "To build and train GPT2, we need to install the Huggingface library, as well as its repository.", "If you want to see visualizations of your model and hyperparameters during training, you can also choose to install tensorboard or wandb:", "Before training, we should set the bos token and eos token as defined earlier in our datasets.", "We should also set the pad token because we will be using LineByLineDataset, which will essentially treat each line in the dataset as distinct examples. In transformers/example/language-modeling/run-language-modelling.py, we should append the following code for the model before training:", "After running this code, the special tokens will be added to the tokenizer and the model will resize its embedding to fit with the modified tokenizer.", "For training, we define some parameters first and then run the language modeling script:", "We set per_device_train_batch_size=2 and per_device_eval_batch_size=2 because of the GPU constraints. Feel free to use a batch size that fits your GPU. We use line_by_line, which tells our model to treat each line in our dataset as an individual example, as explained earlier. Evaluate_during_training runs evaluation on the evaluation dataset after each logging_steps, which is defaulted to 500.", "In case you want to continue training from the last checkpoint, you can run:", "This step is optional depending on whether you want to evaluate the performance of your trained GPT2. You can do this by evaluating perplexity on the test dataset.", "Here, in my case, we attained a loss of 2.46 and a perplexity of 11.70 after training for 5 epochs:", "Before generating texts using our trained model, we first enable special tokens in our prompt by setting add_special_tokens=True in the transformers/examples/text-generation/run_generation.py:", "Then, we are ready to generate some text! Start generating by:", "We feed in the prompt \u201c<BOS>\u201d as the input, which represents the beginning of each example and stops the model from generating once the \u201c<EOS>\u201d token is generated. This way, our GPT2 will learn to generate a full example of the summary from the beginning to the end, leveraging what it learned of the bos token and eos token during training. In addition, we are using the top-k sampling decoder which has been proven to be very effective in generating irrepetitive and better texts. k=50 is a good value to start off with. Huggingface also supports other decoding methods, including greedy search, beam search, and top-p sampling decoder. For more information, look into the docstring of model.generate.", "Here are a few examples of the generated texts with k=50.", "The protagonist is an Englishman, William Lark, who has been sent on an adventure with the British Government on a mission to the Arctic. The novel tells the story of how his friends and family are being sold into slavery in the small Norwegian town of Shok\u2026", "A new world is awakening, and the humans of the planet Vorta must work together to save it from destruction. The New Earth is now populated by three species. The first are the humans who are a bit older, the second are the Vorta, and the third are the humans with dark blue eyes\u2026", "The novel begins in the year 2143, when a group of \u201cdungeons\u201d, or witches, decide to break the spell that prevents the power of the dead by consuming the souls of those who died to them. They use the bodies to help the dying, as well as to raise the dead themselves\u2026", "You can see more generated examples here.", "In this article, we showed how to implement one of the most popular transformer models, GPT2, to create interesting texts. GPT2\u2019s large-scale pre-trained dataset and architecture allows it to produce coherent and fluent pieces of writing. Although GPT2\u2019s texts are still distinguishable from those written by humans, this is proof that creativity by machines is only going upwards from now. For more info, you can take a look at the official paper or OpenAI\u2019s blog on GPT2.", "This article only showed how to generate text that is determined by AI. If you are wondering whether it\u2019s possible to control the text being generated (and it\u2019s possible!), take a read at the following article I wrote \ud83d\ude0a.", "[1] A. Vaswani, N. Shazeer, N. Parmar, etc., Attention Is All You Need (2017), 31st Conference on Neural Information Processing Systems", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Master\u2019s Student at Carnegie Mellon, Top Writer in AI, Top 1000 Writer, Blogging on ML | Data Science | NLP. Linkedin: https://www.linkedin.com/in/itsuncheng/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F2ee61a4f1ba7&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tuning-gpt2-for-text-generation-using-pytorch-2ee61a4f1ba7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tuning-gpt2-for-text-generation-using-pytorch-2ee61a4f1ba7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tuning-gpt2-for-text-generation-using-pytorch-2ee61a4f1ba7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tuning-gpt2-for-text-generation-using-pytorch-2ee61a4f1ba7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----2ee61a4f1ba7--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2ee61a4f1ba7--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://itsuncheng.medium.com/?source=post_page-----2ee61a4f1ba7--------------------------------", "anchor_text": ""}, {"url": "https://itsuncheng.medium.com/?source=post_page-----2ee61a4f1ba7--------------------------------", "anchor_text": "Raymond Cheng"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4c697cd55840&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tuning-gpt2-for-text-generation-using-pytorch-2ee61a4f1ba7&user=Raymond+Cheng&userId=4c697cd55840&source=post_page-4c697cd55840----2ee61a4f1ba7---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2ee61a4f1ba7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tuning-gpt2-for-text-generation-using-pytorch-2ee61a4f1ba7&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2ee61a4f1ba7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tuning-gpt2-for-text-generation-using-pytorch-2ee61a4f1ba7&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@agkdesign?utm_source=medium&utm_medium=referral", "anchor_text": "Alex Knight"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf", "anchor_text": "Transformers"}, {"url": "https://towardsdatascience.com/bert-text-classification-using-pytorch-723dfb8b6b5b", "anchor_text": "BERT for text classification"}, {"url": "https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf", "anchor_text": "GPT2"}, {"url": "http://www.cs.cmu.edu/~dbamman/booksummaries.html", "anchor_text": "CMU Books Summary Dataset"}, {"url": "https://huggingface.co/", "anchor_text": "Huggingface"}, {"url": "https://github.com/itsuncheng/fine-tuning-GPT2", "anchor_text": "here"}, {"url": "http://www.cs.cmu.edu/~dbamman/booksummaries.html", "anchor_text": "here"}, {"url": "https://github.com/itsuncheng/fine-tuning-GPT2/blob/master/preprocessing.ipynb", "anchor_text": "here"}, {"url": "https://huggingface.co/transformers/main_classes/model.html?highlight=generate#transformers.TFPreTrainedModel.generate", "anchor_text": "docstring"}, {"url": "https://github.com/itsuncheng/fine-tuning-GPT2/blob/master/generated_summaries.txt", "anchor_text": "here"}, {"url": "https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf", "anchor_text": "official paper"}, {"url": "https://openai.com/blog/better-language-models/", "anchor_text": "blog"}, {"url": "https://towardsdatascience.com/controlling-text-generation-from-language-models-6334935e80cf", "anchor_text": "Controlling Text Generation for Language ModelsHands-on approach to control style and content of machine-generated texttowardsdatascience.com"}, {"url": "https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf", "anchor_text": "Attention Is All You Need"}, {"url": "https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf", "anchor_text": "Language Models are Unsupervised Multitask Learners"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----2ee61a4f1ba7---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----2ee61a4f1ba7---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----2ee61a4f1ba7---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----2ee61a4f1ba7---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/technology?source=post_page-----2ee61a4f1ba7---------------technology-----------------", "anchor_text": "Technology"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2ee61a4f1ba7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tuning-gpt2-for-text-generation-using-pytorch-2ee61a4f1ba7&user=Raymond+Cheng&userId=4c697cd55840&source=-----2ee61a4f1ba7---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2ee61a4f1ba7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tuning-gpt2-for-text-generation-using-pytorch-2ee61a4f1ba7&user=Raymond+Cheng&userId=4c697cd55840&source=-----2ee61a4f1ba7---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2ee61a4f1ba7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tuning-gpt2-for-text-generation-using-pytorch-2ee61a4f1ba7&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2ee61a4f1ba7--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F2ee61a4f1ba7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tuning-gpt2-for-text-generation-using-pytorch-2ee61a4f1ba7&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----2ee61a4f1ba7---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----2ee61a4f1ba7--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----2ee61a4f1ba7--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----2ee61a4f1ba7--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----2ee61a4f1ba7--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----2ee61a4f1ba7--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----2ee61a4f1ba7--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----2ee61a4f1ba7--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----2ee61a4f1ba7--------------------------------", "anchor_text": ""}, {"url": "https://itsuncheng.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://itsuncheng.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Raymond Cheng"}, {"url": "https://itsuncheng.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "731 Followers"}, {"url": "https://www.linkedin.com/in/itsuncheng/", "anchor_text": "https://www.linkedin.com/in/itsuncheng/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4c697cd55840&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tuning-gpt2-for-text-generation-using-pytorch-2ee61a4f1ba7&user=Raymond+Cheng&userId=4c697cd55840&source=post_page-4c697cd55840--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fa637a6d3749b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tuning-gpt2-for-text-generation-using-pytorch-2ee61a4f1ba7&newsletterV3=4c697cd55840&newsletterV3Id=a637a6d3749b&user=Raymond+Cheng&userId=4c697cd55840&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}