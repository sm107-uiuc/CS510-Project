{"url": "https://towardsdatascience.com/reinforcement-learning-concept-on-cart-pole-with-dqn-799105ca670", "time": 1683000751.607195, "path": "towardsdatascience.com/reinforcement-learning-concept-on-cart-pole-with-dqn-799105ca670/", "webpage": {"metadata": {"title": "Reinforcement Learning Concept on Cart-Pole with DQN | by Vitou Phy | Towards Data Science", "h1": "Reinforcement Learning Concept on Cart-Pole with DQN", "description": "A simple introduction on Reinforcement Learning concept with Deep Q-Network (DQN) on Cart-Pole environment."}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/Inverted_pendulum", "anchor_text": "inverted pendulum", "paragraph_index": 0}], "all_paragraphs": ["CartPole, also known as inverted pendulum, is a game in which you try to balance the pole as long as possible. It is assumed that at the tip of the pole, there is an object which makes it unstable and very likely to fall over. The goal of this task is to move the cart left and right so that the pole can stand (within a certain angle) as long as possible.", "In this post, we will look at reinforcement learning, a field in artificial intelligence where the AI explores the environment all by itself by playing the game many many times until it learns the right way to play the game.", "As you can see here, at the beginning of the training, the agent has no idea of where to move a cart. After a while, the agent moves toward a direction but, of course, it is impossible to bring the pole to the other side with such speed. For the last one, the agent knows the correct way to balance the pole which is the move left and right repeatedly.", "Let\u2019s try to understand this game a little bit more. Again, the objective is to stay alive as long as possible. The longer you keep the pole standing, the more score you will get. This score, also called reward, is what we give to the agent to know if its action is good or not. Based on that, the agent will try to optimize and pick the right action. Note that, the game is over when the pole exceeds 12-degree angle or the cart is going out of the screen.", "But how does the agent know the current status of the pole? What should the data look like?", "The current condition of the cart-pole (of whether it tips over to the left or right) is known as state. A state can be the current frame (in pixels) or it can be some other information that can represent the cart-pole, for instance, the velocity and position of the cart, the angle of the pole and pole velocity at the tip. For this post, let\u2019s assume the state of the cart is the 4 properties mentioned above.", "Depending on the action we take, it can lead to different other states. Suppose the pole is starting straight, if we go left, the pole is mostly to go right, which is a new state. Therefore, during each time-step, any action we make will always lead to a different state.", "From the state diagram above, we can see that if we make the right decision, the pole will not fall over and we will get a reward. In other words, for those state and action pairs that lead to further and further states is also expected to get a large reward. So, let\u2019s call this expected reward for each state-action pair as Q-value denoted as Q(s,a).", "During a state (s) and the agent takes an action (a), they will immediately get the reward for making that action (1 if the game is still on and 0 otherwise). However, earlier we mentioned that we want to consider the potential future reward of that state-action pair. Let\u2019s consider this case formally with the equation below.", "Intuitively, we want to know Q(s,a) which is the expected reward we can get if we are in the state (s) and making (a) action. After getting a reward (r) by making an action (a), we will reach another state (s\u2019). Then, we just look up in our Q table and find the best action to take at state (s\u2019). So, the idea here is we don\u2019t need to consider the entire future actions, but only the one at the next time-step. The gamma symbol (\u03b3) indicates how much we should focus on the future reward of state s\u2019.", "At the beginning of the training, we don\u2019t know the Q-value for any state-action pair. Based on the equation above, we can make some changes to Q-value little by little toward a direction of optimal value.", "The equation might look a bit terrifying. Don\u2019t worry. We\u2019ll gonna break it down to see what\u2019s going on here with the figure below.", "Here, the Q(s,a) will be updated based on the difference between what we think we know about Q(s,a) and what the current episode tell us about Q(s,a) (what it should be). If our Q(s,a) is overestimated, the value in the red dashed box will be negative and we will lower Q(s,a) value by a little. Otherwise, we will increase Q(s,a) value. The amount we make changes to Q(s,a) depends on the difference (red box) and the learning rate (\u03b1).", "There is one problem with current Q-Learning. That is the state space is huge. Each small change to the angle of the pole or the velocity of the cart represents a new state. We would need to have a very big memory to store all possible states. For this cart-pole, it may be possible to handle that many states but for more complex games like StarCraft 2 or Dota 2, Q-Learning by itself is not enough to get the job done.", "To cope with this problem, we need something to approximate a function that takes in a state-action pair (s,a) and returns the expected reward for that pair. That is when deep learning comes in. It is renowned for approximating a function just from the training data. Keep in mind that we will not go through the detail of the neural network in this post. It will be just a brief on how we can incorporate deep learning with reinforcement learning.", "Suppose we want to know Q(s, a=right), we feed in the state (s) of the environment into the model. Let the neural network do the calculation, and it will return 2 values. One is the expected reward when making the left move and another is for making the right move. Since we are interested in a=right, we would just get the value from the lower node of that output. Now we have Q(s,a) by using the network.", "To fully train the network, loss function is essential. Intuitively, before the network output an approximated value Q(s,a), we already have an idea of what the value should be (based on equation 2). Hence, we can punish the network for the mistake it makes, and let it learn that mistake through back-propagation.", "Now we have talked about the state, reward, action, Q-learning and DQN specifically on this cart-pole environment. Let\u2019s look at the result of DQN on the cart-pole environment.", "So, this is it. We just went through the concept of DQN on a cart-pole game. DQN was successfully applied to many more games especially Atari games. It\u2019s pretty generic and robust. So, next time you have an environment in mind, you can just plug-in DQN and let it learn to play the game by itself. Keep in mind that recently, there are many other deep-reinforcement learning techniques that can learn more complex game like Go or Dota 2 better and faster, but that is the topic for another time.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F799105ca670&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-concept-on-cart-pole-with-dqn-799105ca670&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-concept-on-cart-pole-with-dqn-799105ca670&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-concept-on-cart-pole-with-dqn-799105ca670&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-concept-on-cart-pole-with-dqn-799105ca670&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----799105ca670--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----799105ca670--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@phyvitou?source=post_page-----799105ca670--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@phyvitou?source=post_page-----799105ca670--------------------------------", "anchor_text": "Vitou Phy"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F81157f79faae&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-concept-on-cart-pole-with-dqn-799105ca670&user=Vitou+Phy&userId=81157f79faae&source=post_page-81157f79faae----799105ca670---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F799105ca670&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-concept-on-cart-pole-with-dqn-799105ca670&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F799105ca670&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-concept-on-cart-pole-with-dqn-799105ca670&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://en.wikipedia.org/wiki/Inverted_pendulum", "anchor_text": "inverted pendulum"}, {"url": "https://arxiv.org/pdf/1312.5602.pdf", "anchor_text": "https://arxiv.org/pdf/1312.5602.pdf"}, {"url": "https://www.analyticsvidhya.com/blog/2019/04/introduction-deep-q-learning-python/", "anchor_text": "https://www.analyticsvidhya.com/blog/2019/04/introduction-deep-q-learning-python/"}, {"url": "https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf", "anchor_text": "https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf"}, {"url": "https://gym.openai.com/envs/CartPole-v1/", "anchor_text": "https://gym.openai.com/envs/CartPole-v1/"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----799105ca670---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----799105ca670---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----799105ca670---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----799105ca670---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/dqn?source=post_page-----799105ca670---------------dqn-----------------", "anchor_text": "Dqn"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F799105ca670&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-concept-on-cart-pole-with-dqn-799105ca670&user=Vitou+Phy&userId=81157f79faae&source=-----799105ca670---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F799105ca670&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-concept-on-cart-pole-with-dqn-799105ca670&user=Vitou+Phy&userId=81157f79faae&source=-----799105ca670---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F799105ca670&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-concept-on-cart-pole-with-dqn-799105ca670&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----799105ca670--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F799105ca670&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-concept-on-cart-pole-with-dqn-799105ca670&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----799105ca670---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----799105ca670--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----799105ca670--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----799105ca670--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----799105ca670--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----799105ca670--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----799105ca670--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----799105ca670--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----799105ca670--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@phyvitou?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@phyvitou?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Vitou Phy"}, {"url": "https://medium.com/@phyvitou/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "82 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F81157f79faae&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-concept-on-cart-pole-with-dqn-799105ca670&user=Vitou+Phy&userId=81157f79faae&source=post_page-81157f79faae--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F7c7adf70813c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-concept-on-cart-pole-with-dqn-799105ca670&newsletterV3=81157f79faae&newsletterV3Id=7c7adf70813c&user=Vitou+Phy&userId=81157f79faae&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}