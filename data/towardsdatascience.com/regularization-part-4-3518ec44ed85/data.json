{"url": "https://towardsdatascience.com/regularization-part-4-3518ec44ed85", "time": 1683010454.900171, "path": "towardsdatascience.com/regularization-part-4-3518ec44ed85/", "webpage": {"metadata": {"title": "Initialization and Transfer for Deep Learning | Towards Data Science", "h1": "Regularization \u2014 Part 4", "description": "In this blog post, we discuss ideas for initialisation of weights for fully connected layers. Also, we look into the topic of transfer learning."}, "outgoing_paragraph_urls": [{"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/regularization-part-4-2ee8e7aa60ec", "anchor_text": "Previous Lecture", "paragraph_index": 1}, {"url": "https://youtu.be/sXCpj6sfxBc", "anchor_text": "Watch this Video", "paragraph_index": 1}, {"url": "https://towardsdatascience.com/all-you-want-to-know-about-deep-learning-8d68dcffc258", "anchor_text": "Top Level", "paragraph_index": 1}, {"url": "https://towardsdatascience.com/regularization-part-5-b4019720b020", "anchor_text": "Next Lecture", "paragraph_index": 1}, {"url": "https://youtu.be/o9qg7BmNVSQ", "anchor_text": "our videos on visualization", "paragraph_index": 10}, {"url": "https://youtu.be/fXO1fOXnOTI", "anchor_text": "in a different video", "paragraph_index": 12}, {"url": "https://medium.com/@akmaier", "anchor_text": "more essays here", "paragraph_index": 14}, {"url": "https://lme.tf.fau.de/teaching/free-deep-learning-resources/", "anchor_text": "here", "paragraph_index": 14}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj", "anchor_text": "Deep", "paragraph_index": 14}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Learning", "paragraph_index": 14}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj", "anchor_text": "Lecture", "paragraph_index": 14}, {"url": "https://www.youtube.com/c/AndreasMaierTV", "anchor_text": "YouTube", "paragraph_index": 14}, {"url": "https://twitter.com/maier_ak", "anchor_text": "Twitter", "paragraph_index": 14}, {"url": "https://www.facebook.com/andreas.maier.31337", "anchor_text": "Facebook", "paragraph_index": 14}, {"url": "https://www.linkedin.com/in/andreas-maier-a6870b1a6/", "anchor_text": "LinkedIn", "paragraph_index": 14}, {"url": "https://creativecommons.org/licenses/by/4.0/deed.de", "anchor_text": "Creative Commons 4.0 Attribution License", "paragraph_index": 14}, {"url": "https://www.springer.com/us/book/9780387310732", "anchor_text": "Link", "paragraph_index": 15}, {"url": "https://arxiv.org/abs/1206.5533", "anchor_text": "Link", "paragraph_index": 15}, {"url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.207.2059&rep=rep1&type=pdf", "anchor_text": "Link", "paragraph_index": 15}], "all_paragraphs": ["These are the lecture notes for FAU\u2019s YouTube Lecture \u201cDeep Learning\u201d. This is a full transcript of the lecture video & matching slides. We hope, you enjoy this as much as the videos. Of course, this transcript was created with deep learning techniques largely automatically and only minor manual modifications were performed. If you spot mistakes, please let us know!", "Previous Lecture / Watch this Video / Top Level / Next Lecture", "Welcome back to deep learning! So today, we want to look at a couple of initialization techniques that will come in really handy throughout your work with deep learning networks. So, you may wonder why does initialization matter if you have a convex function, actually, it doesn\u2019t matter at all because you follow the negative gradient direction and you will always find the global minimum. So, no problem for convex optimization.", "However, many of the problems that we are dealing with are non-convex. A non-convex function may have different local minima. If I start at this point you can see that I achieve one local minimum by the optimization. But if I were to start at this point, you can see that I would end up with a very different local minimum. So for non-convex problems, initialization is actually a big deal. Neural networks with non-linearities are in general non-convex.", "So, what can be done? Well, of course, you have to work with some initialization. For the biases, you can start quite easily and initialize them to 0. This is very typical. Keep in mind that if you\u2019re working with a ReLU, you may want to start with a small positive constant, This is better because of the dying ReLU issue. For the weights, you need to be random to break the symmetry. We already had this problem. In dropout, we saw that we need additional regularization in order to break the symmetry. Also, it would be especially bad to initialize them with zeros because then the gradient is zero. So, this is something that you don\u2019t want to do. Similar to the learning rate, their variance influences the stability of the learning process. Small uniform gaussian values work.", "Now, you may wonder how can we calibrate those variances. Let\u2019s suppose we have a single linear neuron with weights W and input X. Remember that the capital letters here mark them as random variables. Then, you can see that the output is W times X. So, this is this linear combination of the respective inputs plus some bias. Now, we are interested in the variance of Y hat. If we assume that W and X are independent, then the variance of every product can be actually computed as the expected value of X to the power of 2 times the variance of W plus the expected value of W to the power of 2 times the variance of X and then you add the variances of the two random variables. Now if we require W and X to have 0 mean, then this would simplify the whole issue. The means would be 0, so the expected values cancel out and our variance would simply be the multiplication of the two variances. Now, we assume that X subscript n and W subscript n are independent and identically distributed. In this special case, we can then see that essentially the N here scaled our variances. So, it\u2019s actually dependent on the number of inputs that you have towards your layer. This is a scaling of the variance with your W subscript n. So, you see that the weights are very important. Effectively, the more weights you have, the more it scales the variance.", "As a result, we then can work with Xavier initialization. So, we calibrate the variances for the forward pass. We initialize with a zero-mean Gaussian and we simply set the standard deviation to one over fan_in, where fan_in is the input dimension of the weights. So, we simply scale the variance to be 1 over the number of input dimensions. In the backward pass, however, we would need the same effect backward. So, we would have to scale the standard deviation with 1 over fan_out, where fan_out is the output dimension of the weights. So, you just average those two and compute a new standard deviation. This initialization is called after the first author of [21]. Well, what else can be done?", "There\u2019s He initialization which then also considers that the assumption of linear neurons is a problem. So in [12], they showed that for ReLUs, it\u2019s better to actually use the square root of 2 over fan_in as standard deviation. So this is a very typical choice for initializing the weights randomly.", "Then, other conventional initial choices are that you do L2 regularization, you use dropout with a probability of 0.5 for fully connected layers, and you use them selectively in convolutional neural networks. You do mean subtraction, batch normalization, and He initialization. So this is the very typical setup.", "So which other tricks do we have left? One important technique is transfer learning. Now, transfer learning is typically used in all situations where you have little data. One example is medical data. There, you typically have very little data. So, the idea is then to reuse models, e.g., trained on ImageNet. You can even reuse things that have been trained on a different task for the same data. You can also use different data for the same task or you can even do different data on a different task.", "So now, the question is what should we transfer? Well, the convolutional layers extract features and the expectation now is that less task-specific features are in earlier layers. We have seen that in a couple of papers. We can also see that in our videos on visualization. So typically, those have more basic information and are likely to contain information that is worth transferring. We cut the network at some depth in the feature extraction part. For those extracted parts, we can fix the learning rate to zero. So, if we set it to zero, they won\u2019t change. You can start fine-tuning then.", "One example here is skin cancer classification. They use a deep convolutional neural network based on Inception V3. They have a state-of-the-art architecture that was pre-trained on ImageNet. Then, they fine-tune it on skin cancer data. So, they essentially take the network and what you have to replace is essentially the right-hand part. The training of the classes is something that you won\u2019t find on ImageNet. So, there you have to replace the entire network because you want to predict very different classes. Then, you can use a couple of fully connected layers in order to map your learned feature representations to a different space. Then, you can do a classification from there.", "There\u2019s also transfer between modalities. This was also found to be beneficial. Now, you can transfer from color to x-ray and here it\u2019s actually sufficient to simply copy the input three times. Then, you don\u2019t need that much fine-tuning. So, this works pretty well. One alternative is that you use feature representations of other networks as a loss function. This then leads to perceptual loss. We will talk about perceptual loss in a different video. In any case transfer learning is typically a very good idea. There are many many applications and many papers where they have been using transfer learning. A long time ago, they didn\u2019t say transfer learning but they said adaptation. In particular, in speech processing, they have speaker and noise adaptation and so on. But nowadays you say transfer learning. It\u2019s essentially the same concept.", "Next time in deep learning, we will talk about the remaining tricks of the trade and regularization. I\u2019m looking forward to seeing you in the next lecture. So thank you very much for listening and goodbye!", "If you liked this post, you can find more essays here, more educational material on Machine Learning here, or have a look at our Deep LearningLecture. I would also appreciate a follow on YouTube, Twitter, Facebook, or LinkedIn in case you want to be informed about more essays, videos, and research in the future. This article is released under the Creative Commons 4.0 Attribution License and can be reprinted and modified if referenced.", "Link \u2014 for details on Maximum A Posteriori estimation and the bias-variance decompositionLink \u2014 for a comprehensive text about practical recommendations for regularizationLink \u2014 the paper about calibrating the variances", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "I do research in Machine Learning. My positions include being Prof @FAU_Germany, President @DataDonors, and Board Member for Science & Technology @TimeMachineEU"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F3518ec44ed85&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-part-4-3518ec44ed85&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-part-4-3518ec44ed85&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-part-4-3518ec44ed85&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-part-4-3518ec44ed85&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----3518ec44ed85--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3518ec44ed85--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://akmaier.medium.com/?source=post_page-----3518ec44ed85--------------------------------", "anchor_text": ""}, {"url": "https://akmaier.medium.com/?source=post_page-----3518ec44ed85--------------------------------", "anchor_text": "Andreas Maier"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb1444918afee&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-part-4-3518ec44ed85&user=Andreas+Maier&userId=b1444918afee&source=post_page-b1444918afee----3518ec44ed85---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3518ec44ed85&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-part-4-3518ec44ed85&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3518ec44ed85&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-part-4-3518ec44ed85&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/tagged/fau-lecture-notes", "anchor_text": "FAU LECTURE NOTES"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning"}, {"url": "https://towardsdatascience.com/regularization-part-4-2ee8e7aa60ec", "anchor_text": "Previous Lecture"}, {"url": "https://youtu.be/sXCpj6sfxBc", "anchor_text": "Watch this Video"}, {"url": "https://towardsdatascience.com/all-you-want-to-know-about-deep-learning-8d68dcffc258", "anchor_text": "Top Level"}, {"url": "https://towardsdatascience.com/regularization-part-5-b4019720b020", "anchor_text": "Next Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://youtu.be/o9qg7BmNVSQ", "anchor_text": "our videos on visualization"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://youtu.be/fXO1fOXnOTI", "anchor_text": "in a different video"}, {"url": "https://creativecommons.org/licenses/by/4.0/", "anchor_text": "CC BY 4.0"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Deep Learning Lecture"}, {"url": "https://medium.com/@akmaier", "anchor_text": "more essays here"}, {"url": "https://lme.tf.fau.de/teaching/free-deep-learning-resources/", "anchor_text": "here"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj", "anchor_text": "Deep"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj&index=1", "anchor_text": "Learning"}, {"url": "https://www.youtube.com/watch?v=p-_Stl0t3kU&list=PLpOGQvPCDQzvgpD3S0vTy7bJe2pf_yJFj", "anchor_text": "Lecture"}, {"url": "https://www.youtube.com/c/AndreasMaierTV", "anchor_text": "YouTube"}, {"url": "https://twitter.com/maier_ak", "anchor_text": "Twitter"}, {"url": "https://www.facebook.com/andreas.maier.31337", "anchor_text": "Facebook"}, {"url": "https://www.linkedin.com/in/andreas-maier-a6870b1a6/", "anchor_text": "LinkedIn"}, {"url": "https://creativecommons.org/licenses/by/4.0/deed.de", "anchor_text": "Creative Commons 4.0 Attribution License"}, {"url": "https://www.springer.com/us/book/9780387310732", "anchor_text": "Link"}, {"url": "https://arxiv.org/abs/1206.5533", "anchor_text": "Link"}, {"url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.207.2059&rep=rep1&type=pdf", "anchor_text": "Link"}, {"url": "http://www.deeplearningbook.org.", "anchor_text": "http://www.deeplearningbook.org."}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----3518ec44ed85---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----3518ec44ed85---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/fau-lecture-notes?source=post_page-----3518ec44ed85---------------fau_lecture_notes-----------------", "anchor_text": "Fau Lecture Notes"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----3518ec44ed85---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----3518ec44ed85---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "http://creativecommons.org/licenses/by/4.0/", "anchor_text": "Some rights reserved"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3518ec44ed85&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-part-4-3518ec44ed85&user=Andreas+Maier&userId=b1444918afee&source=-----3518ec44ed85---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3518ec44ed85&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-part-4-3518ec44ed85&user=Andreas+Maier&userId=b1444918afee&source=-----3518ec44ed85---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3518ec44ed85&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-part-4-3518ec44ed85&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3518ec44ed85--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F3518ec44ed85&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-part-4-3518ec44ed85&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----3518ec44ed85---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----3518ec44ed85--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----3518ec44ed85--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----3518ec44ed85--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----3518ec44ed85--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----3518ec44ed85--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----3518ec44ed85--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----3518ec44ed85--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----3518ec44ed85--------------------------------", "anchor_text": ""}, {"url": "https://akmaier.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://akmaier.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Andreas Maier"}, {"url": "https://akmaier.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "2.2K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb1444918afee&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-part-4-3518ec44ed85&user=Andreas+Maier&userId=b1444918afee&source=post_page-b1444918afee--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fa5f0dee142a2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-part-4-3518ec44ed85&newsletterV3=b1444918afee&newsletterV3Id=a5f0dee142a2&user=Andreas+Maier&userId=b1444918afee&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}