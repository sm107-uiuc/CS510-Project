{"url": "https://towardsdatascience.com/gradient-descent-3a7db7520711", "time": 1682994434.763371, "path": "towardsdatascience.com/gradient-descent-3a7db7520711/", "webpage": {"metadata": {"title": "Gradient Descent. It is a slippery slope, but promise it\u2026 | by Hamza Mahmood | Towards Data Science", "h1": "Gradient Descent", "description": "In our previous post, we talked about activation functions (link here) and where it is used in machine learning models. However, we also heavily used the term \u2018Gradient Descent\u2019 which is a key\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/activation-functions-in-neural-networks-83ff7f46a6bd", "anchor_text": "here", "paragraph_index": 1}, {"url": "http://ruder.io/optimizing-gradient-descent/", "anchor_text": "article", "paragraph_index": 9}, {"url": "https://medium.com/@hamzamahmood", "anchor_text": "here", "paragraph_index": 16}, {"url": "https://www.linkedin.com/in/hmahmood93/", "anchor_text": "Linkedin", "paragraph_index": 16}, {"url": "https://twitter.com/mahmooyo", "anchor_text": "tweet", "paragraph_index": 16}], "all_paragraphs": ["tl;dr Gradient Descent is an optimization technique that is used to improve deep learning and neural network-based models by minimizing the cost function.", "In our previous post, we talked about activation functions (link here) and where it is used in machine learning models. However, we also heavily used the term \u2018Gradient Descent\u2019 which is a key element in deep learning models, which are going to talk about in this post.", "Gradient Descent is a process that occurs in the backpropagation phase where the goal is to continuously resample the gradient of the model\u2019s parameter in the opposite direction based on the weight w, updating consistently until we reach the global minimum of function J(w).", "To put it simply, we use gradient descent to minimize the cost function, J(w).", "An analogy could be drawn in the form of a steep mountain whose base touches the sea. We assume a person\u2019s goal is to reach down to sea level. Ideally, the person would have to take one step at a time to reach the goal. Each step has a gradient in the negative direction (Note: the value can be of different magnitude). The person continues hiking down till he reaches the bottom or to a threshold point, where there is no room to go further down.", "Let\u2019s formalize the analogy into an algorithmic form. We compute the activation for the incoming parameters, we carry out feedforward by taking the weighted sum of the activation and its bias. We extract the error term of the output sample by subtracting it with the actual \u2018target\u2019 value.", "The gradient descent process is exhibited in the form of the backpropagation step where we compute the error vectors \u03b4 backward, starting from the final layer. Depending upon the activation function, we identify how much change is required by much change is required by taking the partial derivative of the function with respect to w. The change value gets multiplied by the learning rate. As part of the output, we subtract this value from the previous output to get the updated value. We continue this till we reach convergence.", "In the code below, I wanted to highlight how one can write a simple code to get a visualization of how gradient descent works. Running this piece of code; using the Tanh activation function, we will observe the current value of 10 go down to a value of 8.407e-06 on the 10000th iteration, which is our global minima.", "There are a number of gradient descent algorithms out there. I\u2019ll mention a few below:", "If you want to go into the technicalities of the more recent ones, I highly recommend going through Sebastian Ruder\u2019s article on the topic.", "In deep networks or recurrent neural networks, there are two known issues explained in a paper by Pascanu et al (1994) \u2014 of exploding and vanishing gradients. This happens when are doing back propagation as we iterate through the code, there is a chance that the normal of the weight matrices going beyond 1. If this happens, the gradient explodes but if the normal is below 1, the gradient vanishes.", "If we want to visualize exploding gradients, you will encounter at least one of the problems:", "The solution to the exploding and vanishing gradient problem, we introduce gradient clipping, where we \u2018clip\u2019 the gradients if it goes over a certain threshold represented by a maximum absolute value. Hence, we keep the neural network stable as the weight values never reach the point that it returns a \u2018Nan\u2019. In a coded implementation removing the clipped gradients leads to \u2018Nan\u2019 values or infinite in the losses and fails to run further.", "The code below showcases how to perform gradient clipping. Given that we have a vector of losses and a learning rate, we are able to compute a vector of gradients which are next clipped based on a maximum L2-norm value, which in this case I have written as 5.", "So at the end of the day, when a question is posed to data scientist of what optimizer to use in order to minimize loss, there are a couple of factors to consider:", "In this write-up, we covered a number of things: We covered what is Gradient Descent is and how it works in a neural network. We went through the mathematics involved and implemented a coded version of it. Lastly, we covered the issues involving gradient descent in the form of the vanishing and exploding gradient problem and discussed the solution for it using gradient clipping. In the next lecture, we will explore what activation functions are and how they are crucial in deep learning models, so stay tuned for that!", "Spread and share knowledge. If this article piqued your interest, share it among your friends or professional circles. For more data science and technology related posts follow me here.I am also available on Linkedin and occasionally tweet stuff as well. :)", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Business Solutions @Voiant. Solving problems for humanity through design. I sometimes write for Startup Grind and UX Collective. Twitter: @mahmooyo"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F3a7db7520711&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-3a7db7520711&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-3a7db7520711&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-3a7db7520711&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-3a7db7520711&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----3a7db7520711--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3a7db7520711--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://hamzamahmood.medium.com/?source=post_page-----3a7db7520711--------------------------------", "anchor_text": ""}, {"url": "https://hamzamahmood.medium.com/?source=post_page-----3a7db7520711--------------------------------", "anchor_text": "Hamza Mahmood"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9a046831034&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-3a7db7520711&user=Hamza+Mahmood&userId=9a046831034&source=post_page-9a046831034----3a7db7520711---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3a7db7520711&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-3a7db7520711&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3a7db7520711&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-3a7db7520711&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/activation-functions-in-neural-networks-83ff7f46a6bd", "anchor_text": "here"}, {"url": "http://ruder.io/optimizing-gradient-descent/", "anchor_text": "article"}, {"url": "http://proceedings.mlr.press/v28/pascanu13.pdf", "anchor_text": "http://proceedings.mlr.press/v28/pascanu13.pdf"}, {"url": "https://medium.com/@hamzamahmood", "anchor_text": "here"}, {"url": "https://www.linkedin.com/in/hmahmood93/", "anchor_text": "Linkedin"}, {"url": "https://twitter.com/mahmooyo", "anchor_text": "tweet"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----3a7db7520711---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----3a7db7520711---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----3a7db7520711---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----3a7db7520711---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/technology?source=post_page-----3a7db7520711---------------technology-----------------", "anchor_text": "Technology"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3a7db7520711&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-3a7db7520711&user=Hamza+Mahmood&userId=9a046831034&source=-----3a7db7520711---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3a7db7520711&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-3a7db7520711&user=Hamza+Mahmood&userId=9a046831034&source=-----3a7db7520711---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3a7db7520711&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-3a7db7520711&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3a7db7520711--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F3a7db7520711&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-3a7db7520711&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----3a7db7520711---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----3a7db7520711--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----3a7db7520711--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----3a7db7520711--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----3a7db7520711--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----3a7db7520711--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----3a7db7520711--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----3a7db7520711--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----3a7db7520711--------------------------------", "anchor_text": ""}, {"url": "https://hamzamahmood.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://hamzamahmood.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Hamza Mahmood"}, {"url": "https://hamzamahmood.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "555 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9a046831034&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-3a7db7520711&user=Hamza+Mahmood&userId=9a046831034&source=post_page-9a046831034--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fa1ab461a97bc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-descent-3a7db7520711&newsletterV3=9a046831034&newsletterV3Id=a1ab461a97bc&user=Hamza+Mahmood&userId=9a046831034&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}