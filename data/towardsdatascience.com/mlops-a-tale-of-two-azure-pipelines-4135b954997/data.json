{"url": "https://towardsdatascience.com/mlops-a-tale-of-two-azure-pipelines-4135b954997", "time": 1683012729.8044941, "path": "towardsdatascience.com/mlops-a-tale-of-two-azure-pipelines-4135b954997/", "webpage": {"metadata": {"title": "MLOps: a tale of two Azure pipelines | by Luuk van der Velden | Towards Data Science", "h1": "MLOps: a tale of two Azure pipelines", "description": "MLOps seeks to deliver fresh and reliable AI products through continuous integration, continuous training and continuous delivery of machine learning systems. When new data becomes available, we\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/u/8d3b69256f17?source=post_page-----4135b954997--------------------------------", "anchor_text": "Rik Jongerius", "paragraph_index": 0}, {"url": "https://docs.microsoft.com/en-us/azure/devops/pipelines/get-started/what-is-azure-pipelines?view=azure-devops", "anchor_text": "Azure DevOps pipelines", "paragraph_index": 1}, {"url": "https://azure.microsoft.com/en-us/services/machine-learning/", "anchor_text": "AzureML", "paragraph_index": 1}, {"url": "https://www.terraform.io/", "anchor_text": "Terraform", "paragraph_index": 2}, {"url": "https://docs.microsoft.com/en-us/azure/devops/pipelines/yaml-schema", "anchor_text": "YAML", "paragraph_index": 4}, {"url": "https://github.com/microsoft/azure-pipelines-extensions/tree/master/Extensions/Terraform/Src", "anchor_text": "Terraform task", "paragraph_index": 4}, {"url": "https://docs.microsoft.com/en-us/azure/devops/pipelines/tasks/deploy/azure-key-vault?view=azure-devops", "anchor_text": "Azure KeyVault task", "paragraph_index": 4}, {"url": "https://www.terraform.io/docs/providers/azurerm/r/function_app.html", "anchor_text": "documented Linux example", "paragraph_index": 6}, {"url": "https://github.com/microsoft/azure-pipelines-extensions/tree/master/Extensions/Terraform/Src", "anchor_text": "documentation", "paragraph_index": 6}, {"url": "https://towardsdatascience.com/vscode-devcontainers-and-the-python-azureml-sdk-323dec18a675)", "anchor_text": "here", "paragraph_index": 7}, {"url": "https://towardsdatascience.com/unifying-remote-and-local-azureml-environments-bcea1292e37f", "anchor_text": "part 2", "paragraph_index": 7}, {"url": "https://docs.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.pipelinedata?view=azure-ml-py", "anchor_text": "PipelineData", "paragraph_index": 7}, {"url": "https://docs.microsoft.com/en-us/azure/devops/pipelines/process/scheduled-triggers?view=azure-devops&tabs=yaml", "anchor_text": "cron trigger", "paragraph_index": 11}, {"url": "https://docs.microsoft.com/en-us/azure/azure-functions/functions-run-local", "anchor_text": "azure-functions-core-tools", "paragraph_index": 18}, {"url": "https://github.com/Microsoft/azure-pipelines-tasks/blob/master/Tasks/AzureFunctionAppV1/README.md", "anchor_text": "task", "paragraph_index": 18}, {"url": "https://docs.microsoft.com/en-us/azure/data-factory/transform-data-machine-learning-service", "anchor_text": "withAzure Datafactory", "paragraph_index": 20}, {"url": "https://docs.microsoft.com/en-us/rest/api/azure/devops/pipelines/runs/run%20pipeline?view=azure-devops-rest-6.0", "anchor_text": "http endpoint", "paragraph_index": 20}], "all_paragraphs": ["Luuk van der Velden & Rik Jongerius", "MLOps seeks to deliver fresh and reliable AI products through continuous integration, continuous training and continuous delivery of machine learning systems. When new data becomes available, we update the AI model and deploy it (if improved) with DevOps practices. Azure DevOps pipelines support such practices and is our platform of choice. AI or Machine Learning is however focused around AzureML, which has its own pipeline and artifact system. Our goal is to combine DevOps with AzureML pipelines in an end-to-end solution. We want to continuously train models and conditionally deploy them on our infrastructure and applications. More specifically, our goal is to continuously update a PyTorch model running within an Azure function.", "The following diagram shows our target solution. Three triggers are shown at the top. Changes to the infrastructure-as-code triggers the Terraform infrastructure pipeline. Changes to the Python code of the function triggers the Azure function deploy pipeline. Finally, new data on a schedule triggers the model training pipeline, which does a conditional deploy of the function with the new model if it performs better.", "Within a DevOps pipeline we can organize almost any aspect of the Azure cloud. They allow repeatable and reproducible infrastructure and application deployment. Some of the key features are:", "DevOps pipelines are written in YAML and have several possible levels of organization: stages, jobs, steps. Stages consist of multiple jobs and jobs consist of multiple steps. Here we focus on jobs and steps. Each job can include a condition for execution and each of its steps contains a task specific to a certain framework or service. For instance, a Terraform task to deploy infrastructure or an Azure KeyVault task to manage secrets. Most tasks need a service connection linked to our Azure subscription to be allowed to access and alter our resources. In our case, we appropriate the authentication done by the Azure CLI task to run Python scripts with the right credentials to interact with our AzureML workspace.", "There are good arguments to use tools such as Terraform and Azure Resource Manager to manage our infrastructure, which we will try not to repeat here. Important for us, these tools can be launched repeatedly from our DevOps pipeline and always lead to the same resulting infrastructure (idempotence). So, we can launch the infrastructure pipeline often not only when there are changes to the infrastructure-as-code. We use Terraform to manage our infrastructure, which requires the appropriate service connection.", "The following Terraform definition (Code 1) will create a function app service with its storage account and Standard app service plan. We have included it as the documented Linux example did not work for us. For full serverless benefits we were able to deploy on a consumption plan (elastic), but the azurerm provider for Terraform seems to have an interfering bug that prevented us from including it here. For brevity, we did not include the DevOps pipelines steps for applying Terraform and refer to the relevant documentation.", "AzureML is one of the ways to do data science on Azure, besides Databricks and the legacy HDInsight cluster. We use the Python SDK for AzureML to create and run our pipelines. Setting up an AzureML development environment and running of training code on AMLCompute targets I explain here. In part 2 of that blog I describe the AzureML Environment and Estimator, which we use in the following sections. The AzureML pipeline combines preprocessing with estimators and allows data transfer with PipelineData objects.", "Our Estimator wraps a PyTorch training script and passes command line arguments to it. We add an Estimator to the pipeline by wrapping it with the EstimatorStep class (Code 2).", "To create an AzureML pipeline we need to pass in the Experiment context and a list of steps to run in sequence (Code 3). The goal of our current Estimator is to register an updated model with the AzureML workspace.", "PyTorch (and other) models can be serialized and registered with the AzureML workspace with the Model class. Registering a model uploads it to centralized blob storage and allows it to be published wrapped in a Docker container to Azure Docker instances and Azure Kubernetes Service. We wanted to keep it simple and treat the AzureML model registration as an artifact storage. Our estimator step loads an existing PyTorch model and trains it on the newly available data. This updated model is registered under the same name every time the pipeline runs (code 4). The model version is auto incremented. When we retrieve our model without specifying a version it will grab the latest version. With each model iteration we decide whether we want to deploy the latest version.", "In our approach to MLOps / Continuous AI the DevOps pipeline is leading. It has better secrets management and broader capabilities than the AzureML pipeline. When new data is available the DevOps pipeline starts the AzureML pipeline and waits for it to finish with a conditional decision whether to deploy the model. This decision is based on the performance of the model compared to the previous best model. We schedule the model DevOps pipeline at regular intervals when new data is expected using the cron trigger.", "An Azure CLI task authenticates the task with our service connection, which has access to our AzureML Workspace. This access is used by the Python script to create the Workspace and Experiment context to allow us to run the Pipeline using the AzureML SDK. We wait for the AzureML pipeline to complete, with a configurable timeout. The overall DevOps timeout is 2 hours. The implications of this are discussed at the end of the blog. A basic Python script is shown (Code 5) that starts the AzureML pipeline from Code 3.", "This script is launched from an AzureCLI task (Code 6) for the required authentication. Note: It is not ideal that we need an account with rights on the Azure subscription level to interact with AzureML even for the most basic operations, such as downloading a model.", "An updated model trained with the latest data will not perform better by definition. We want to decide whether to deploy the latest model based on its performance. Thus, we want to communicate our intent to deploy the model from AzureML to the DevOps pipeline. To output a variable to the DevOps context we need to write a specific string to the stdout of our Python script.", "In our implementation each step in the AzureML pipeline can trigger a deployment by creating the following local empty file \u201coutputs/trigger\u201d. The \u201coutputs\u201d directory is special and Azure uploads its content automatically to the central blob storage accessible through the PipelineRun object and ML studio. After the AzureML pipeline is completed we inspect the steps in the PipelineRun for the existence of the trigger file (Code 7). Based on this an output variable is written to the DevOps context as a Task output variable (Code 7).", "We have trained a new model and want to deploy it. We need a DevOps job to take care of the deployment, which runs conditionally on the output of our AzureML training pipeline. We can access the output variable described above and perform an equality check within the jobs\u2019 condition clause. Code 8 below shows how we access the task output variable from the previous train job in the condition of the deploy job.", "To retrieve the latest model from the AzureML Workspace we use an Azure CLI task to handle the required authentication. Within it we run a Python script, which attaches to our AzureML workspace and downloads the latest model within the directory that holds our function (Code 9). When we deploy our function this script is called to package our model with our Python code and requirements (Code 10, task 3). Each model release thus implies a function deploy.", "The azure-functions-core-tools support local development and deployment to a Function App. For our deployment, the function build agent is used to install Python requirements and copy the package to the function App. There is a dedicated DevOps task for function deployments, which you can explore. For the moment we had a better experience installing the azure-functions-core-tools on the DevOps build agent (Ubuntu) and publishing our function with it (Code 10, step 5).", "In this blog we present a pipeline architecture that supports Continuous AI on Azure with a minimal amount of moving parts. Other solutions we encountered add Kubernetes or Docker Instances for publishing the AzureML models for consumption by frontend facing functions. This is an option, but it might increase the engineering load on your team. We do think that adding Databricks could enrich our workflow with collaborative notebooks and more interactive model training, especially in the exploration phase of the project. The AzureML-MLFlow API allows you to register model from Databricks notebooks and hook into our workflow at that point.", "Our focus is on model training for incremental updates with training times measured in hours or less. When we consider full model training measured in days, the pipeline architecture can be expanded to support non-blocking processing. Databricks could be a platform full model training on GPUs as described above. Another option is to run the AzureML pipeline withAzure Datafactory, which is suitable for long running orchestration of data intensive jobs. If the trained model is deemed viable a follow-up DevOps pipeline can be triggered to deploy it. A low-tech trigger option (with limited authentication options) is the http endpoint associated with each DevOps pipeline.", "AI is not the only use case for our approach, but it is a significant one. Related use cases are interactive reporting applications running on streamlit, which can contain representations of knowledge that have to be updated. Machine Learning models, interactive reports and facts from the datalake work in tandem to inform management or customer and lead to action. Thank you for reading.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F4135b954997&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmlops-a-tale-of-two-azure-pipelines-4135b954997&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmlops-a-tale-of-two-azure-pipelines-4135b954997&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmlops-a-tale-of-two-azure-pipelines-4135b954997&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmlops-a-tale-of-two-azure-pipelines-4135b954997&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----4135b954997--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4135b954997--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://luukvandervelden.medium.com/?source=post_page-----4135b954997--------------------------------", "anchor_text": ""}, {"url": "https://luukvandervelden.medium.com/?source=post_page-----4135b954997--------------------------------", "anchor_text": "Luuk van der Velden"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff6c293b24331&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmlops-a-tale-of-two-azure-pipelines-4135b954997&user=Luuk+van+der+Velden&userId=f6c293b24331&source=post_page-f6c293b24331----4135b954997---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4135b954997&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmlops-a-tale-of-two-azure-pipelines-4135b954997&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4135b954997&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmlops-a-tale-of-two-azure-pipelines-4135b954997&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/u/8d3b69256f17?source=post_page-----4135b954997--------------------------------", "anchor_text": "Rik Jongerius"}, {"url": "https://docs.microsoft.com/en-us/azure/devops/pipelines/get-started/what-is-azure-pipelines?view=azure-devops", "anchor_text": "Azure DevOps pipelines"}, {"url": "https://azure.microsoft.com/en-us/services/machine-learning/", "anchor_text": "AzureML"}, {"url": "https://www.pexels.com/photo/silver-pipes-2310904/?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels", "anchor_text": "Pexels"}, {"url": "https://www.terraform.io/", "anchor_text": "Terraform"}, {"url": "https://docs.microsoft.com/en-us/azure/devops/pipelines/yaml-schema", "anchor_text": "YAML"}, {"url": "https://github.com/microsoft/azure-pipelines-extensions/tree/master/Extensions/Terraform/Src", "anchor_text": "Terraform task"}, {"url": "https://docs.microsoft.com/en-us/azure/devops/pipelines/tasks/deploy/azure-key-vault?view=azure-devops", "anchor_text": "Azure KeyVault task"}, {"url": "https://www.terraform.io/docs/providers/azurerm/r/function_app.html", "anchor_text": "documented Linux example"}, {"url": "https://github.com/microsoft/azure-pipelines-extensions/tree/master/Extensions/Terraform/Src", "anchor_text": "documentation"}, {"url": "https://towardsdatascience.com/vscode-devcontainers-and-the-python-azureml-sdk-323dec18a675)", "anchor_text": "here"}, {"url": "https://towardsdatascience.com/unifying-remote-and-local-azureml-environments-bcea1292e37f", "anchor_text": "part 2"}, {"url": "https://docs.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.pipelinedata?view=azure-ml-py", "anchor_text": "PipelineData"}, {"url": "https://docs.microsoft.com/en-us/azure/devops/pipelines/process/scheduled-triggers?view=azure-devops&tabs=yaml", "anchor_text": "cron trigger"}, {"url": "https://docs.microsoft.com/en-us/azure/azure-functions/functions-run-local", "anchor_text": "azure-functions-core-tools"}, {"url": "https://github.com/Microsoft/azure-pipelines-tasks/blob/master/Tasks/AzureFunctionAppV1/README.md", "anchor_text": "task"}, {"url": "https://docs.microsoft.com/en-us/azure/data-factory/transform-data-machine-learning-service", "anchor_text": "withAzure Datafactory"}, {"url": "https://docs.microsoft.com/en-us/rest/api/azure/devops/pipelines/runs/run%20pipeline?view=azure-devops-rest-6.0", "anchor_text": "http endpoint"}, {"url": "https://codebeez.nl/blogs/mlops-tale-two-azure-pipelines/", "anchor_text": "https://codebeez.nl"}, {"url": "https://medium.com/tag/mlops?source=post_page-----4135b954997---------------mlops-----------------", "anchor_text": "Mlops"}, {"url": "https://medium.com/tag/devops?source=post_page-----4135b954997---------------devops-----------------", "anchor_text": "DevOps"}, {"url": "https://medium.com/tag/terraform?source=post_page-----4135b954997---------------terraform-----------------", "anchor_text": "Terraform"}, {"url": "https://medium.com/tag/azure?source=post_page-----4135b954997---------------azure-----------------", "anchor_text": "Azure"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----4135b954997---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4135b954997&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmlops-a-tale-of-two-azure-pipelines-4135b954997&user=Luuk+van+der+Velden&userId=f6c293b24331&source=-----4135b954997---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4135b954997&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmlops-a-tale-of-two-azure-pipelines-4135b954997&user=Luuk+van+der+Velden&userId=f6c293b24331&source=-----4135b954997---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4135b954997&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmlops-a-tale-of-two-azure-pipelines-4135b954997&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4135b954997--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F4135b954997&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmlops-a-tale-of-two-azure-pipelines-4135b954997&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----4135b954997---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----4135b954997--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----4135b954997--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----4135b954997--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----4135b954997--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----4135b954997--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----4135b954997--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----4135b954997--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----4135b954997--------------------------------", "anchor_text": ""}, {"url": "https://luukvandervelden.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://luukvandervelden.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Luuk van der Velden"}, {"url": "https://luukvandervelden.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "35 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff6c293b24331&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmlops-a-tale-of-two-azure-pipelines-4135b954997&user=Luuk+van+der+Velden&userId=f6c293b24331&source=post_page-f6c293b24331--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F7ad4b2d3ea43&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmlops-a-tale-of-two-azure-pipelines-4135b954997&newsletterV3=f6c293b24331&newsletterV3Id=7ad4b2d3ea43&user=Luuk+van+der+Velden&userId=f6c293b24331&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}