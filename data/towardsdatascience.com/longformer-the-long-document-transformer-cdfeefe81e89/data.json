{"url": "https://towardsdatascience.com/longformer-the-long-document-transformer-cdfeefe81e89", "time": 1683017154.992022, "path": "towardsdatascience.com/longformer-the-long-document-transformer-cdfeefe81e89/", "webpage": {"metadata": {"title": "Longformer: The Long-Document Transformer | by Rohan Jagtap | Towards Data Science", "h1": "Longformer: The Long-Document Transformer", "description": "Transformer-based language models have been leading the NLP benchmarks lately. Models like BERT, RoBERTa have been state-of-the-art for a while. However, one major drawback of these models is that\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/ai-in-plain-english/transformer-xl-going-beyond-fixed-length-contexts-e160b62d6f4d", "anchor_text": "Transformer-XL in this", "paragraph_index": 1}, {"url": "https://towardsdatascience.com/reformer-the-efficient-transformer-dd9830164703", "anchor_text": "Reformer in this", "paragraph_index": 1}, {"url": "https://arxiv.org/abs/2004.05150", "anchor_text": "\u201cLongformer: The Long-Document Transformer.\u201d", "paragraph_index": 2}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "Vaswani et. al.", "paragraph_index": 18}, {"url": "https://tvm.apache.org/", "anchor_text": "TVM (Tensor Virtual Machine)", "paragraph_index": 21}], "all_paragraphs": ["Transformer-based language models have been leading the NLP benchmarks lately. Models like BERT, RoBERTa have been state-of-the-art for a while. However, one major drawback of these models is that they cannot \u201cattend\u201d to longer sequences. For example, BERT is limited to a max of 512 tokens at a time.", "To overcome these long sequence issues, several approaches burgeoned. Models like Transformer-XL and Reformer propose decent ways to reduce the model parameters, and hence, the complexity. I have already covered Transformer-XL in this and the Reformer in this article, respectively. Consider giving them a read if you\u2019re interested.", "In this article, we\u2019ll be discussing the Longformer model proposed by Allen AI in the paper, \u201cLongformer: The Long-Document Transformer.\u201d It is a transformer-based architecture that reformulates the self-attention computation to reduce the model complexity.", "The paper initially addresses the issues with existing long document transformers. Models like Transformer-XL partitions the input and apply full self-attention locally as well as in a cross-partition setting (to an extent). However, this doesn\u2019t promise a long-term cross-partition context, which is crucial in some NLP tasks.", "For example, for QA tasks, the stereotype for input representations is to separate the question and the document with a special token. The full attention setting will find the mapping between the question and the answer from the document using attention scores. But, the input partition setting wouldn\u2019t guarantee that the questions would attend to the answers and vice versa.", "In the table above, you can see that Transformer-XL has an ltr (left-to-right) attention matrix (i.e., causal or autoregressive language modeling). Moreover, most of the other models cannot be used for NLP tasks other than language modeling (See the table).", "So, the Longformer essentially combines several attention patterns to overcome the above drawbacks:", "we will cover these in-depth in the coming sections:", "The name speaks for itself. In this approach, we take an arbitrary window size w, and each token in the sequence will only attend to some w tokens (mostly w/2 to the left and w/2 to the right).", "To understand the working of this attention pattern, let\u2019s consider the example of convolutions. Say we take a kernel of size w and slide it through all the tokens in the sequence. After this operation, we\u2019ll have the hidden state representations of all the tokens in the sequence when attended with w adjacent tokens. Now, if we do the same thing for l layers, each token in our sequence would\u2019ve attended (l x w) adjacent tokens, so more or less, the entire input sequence. The authors call this space a receptive field (the reach of attention for a given token). The sliding window attention has a receptive field of (l x w).", "Depending on the application, it might be helpful to use different values of w for each layer to balance between efficiency and model representation capacity.", "Longformer uses dilated sliding window attention to have a much larger receptive field without increasing the computation.", "The idea comes from dilated CNNs. Hence, we will once again take the example of CNNs. Normally, we slide the kernel over the sequence to obtain a feature map that encodes the features of a given token with its adjacent tokens. Now consider having holes in the kernel, i.e., having 0\u2019s between the alternate kernel cells.", "So how does this help the receptive field without harming the computation anyway? Since the kernel has gaps in between, it spans wider. Hence, a single kernel is capable of capturing distant features from the sequence in a single slide. This clearly increases the size of the receptive field. Plus, due to larger kernels, the number of feature maps decreases, which cancels out the increase in computational complexity due to larger kernels. With the same logic, we can improve the performance of the plain sliding window attention by adding dilation.", "We take a dilation size of d for the dilated attention, where d is the number of gaps between each token in the window. The paper suggests using different values of d for different heads in the multi-headed self-attention for better attention scores. The dilated sliding window attention has a receptive field of (l x d x w). Even very small values of d can lead to receptive fields of thousands of tokens.", "In case of Autoregressive Language Modeling, the lower layers do not have dilation so as to capture the immediate local context. But for higher layers, a small amount of dilation is added only on 2 heads.", "Although the longformer has proposed a very efficient attention computation pattern, it still doesn\u2019t resolve the issue we saw earlier with Transformer-XL, i.e., we still can\u2019t guarantee support for long term dependencies for specific downstream NLP tasks. For this, the model uses plain-old global self-attention.", "But here\u2019s the catch. It doesn\u2019t do it for all the tokens like in the traditional transformer architecture. Let\u2019s consider the same example of QA tasks. In case of longformer, we can have all the question tokens to have a global attention pattern, i.e., to have them attend to all the other tokens in the sequence. Moreover, the rest of the tokens, too, attend to all the tokens in the question along with the tokens in their own window. This is shown in the figure above.", "In the standard implementation of Transformer (Vaswani et. al.), we use linear layers to obtain 3 separate representations of the hidden vectors as query (Q), key (K), and value (V) for attention computation. However, in the longformer, we use 2 separate sets of these vectors: Q_s, K_s, V_s, and Q_g, K_g, V_g for sliding window and global attention, respectively. This provides flexibility to model the different types of attention patterns. Q_g, K_g, V_g are initialized with the values of Q_s, K_s, V_s, respectively.", "The main idea of the Transformer was parallelization of the attention computation over the sequential nature of the RNNs. The sliding window attention takes away the whole point of using Transformer-based architectures.", "To add parallelization, one can implement a huge matrix with all zeroes except for the diagonals (as shown in the attention figures), indicating windowed attention. But that takes away the whole point of the sliding window pattern as anyway we have to maintain the matrix for the whole sequence length.", "To answer this, the authors have implemented a custom CUDA kernel for the proposed attention pattern. The kernel is built using TVM (Tensor Virtual Machine).", "a deep learning compiler stack that compiles high-level description of a function into optimized device-specific code", "The authors have written a form of banded matrix multiplication in python for which the TVM generates the corresponding CUDA code and compiles it for the specific GPU.", "The parallelization is achieved by leveraging GPU threads, and only the non-zero values of the huge matrix (i.e., just the sliding window values) are stored. So, the model is time as well as space-efficient. This is shown in the figure above.", "In this article, we saw another model that uses a modified form of attention to optimize the performance of the traditional Transformer architecture. The Longformer provides computational as well as memory efficiency. Moreover, it also provides support for multiple NLP downstream tasks, unlike other long document Transformer architectures. One drawback of this model is that the implementation operations are not directly available in the standard Deep Learning libraries. They have to be customized instead, which opens up room for research.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Immensely interested in AI Research | I read papers and post my notes on Medium"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fcdfeefe81e89&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flongformer-the-long-document-transformer-cdfeefe81e89&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flongformer-the-long-document-transformer-cdfeefe81e89&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flongformer-the-long-document-transformer-cdfeefe81e89&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flongformer-the-long-document-transformer-cdfeefe81e89&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----cdfeefe81e89--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----cdfeefe81e89--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://rojagtap.medium.com/?source=post_page-----cdfeefe81e89--------------------------------", "anchor_text": ""}, {"url": "https://rojagtap.medium.com/?source=post_page-----cdfeefe81e89--------------------------------", "anchor_text": "Rohan Jagtap"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F39646f947a4c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flongformer-the-long-document-transformer-cdfeefe81e89&user=Rohan+Jagtap&userId=39646f947a4c&source=post_page-39646f947a4c----cdfeefe81e89---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcdfeefe81e89&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flongformer-the-long-document-transformer-cdfeefe81e89&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcdfeefe81e89&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flongformer-the-long-document-transformer-cdfeefe81e89&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@josephgardnerphotography?utm_source=medium&utm_medium=referral", "anchor_text": "Joe Gardner"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://medium.com/ai-in-plain-english/transformer-xl-going-beyond-fixed-length-contexts-e160b62d6f4d", "anchor_text": "Transformer-XL in this"}, {"url": "https://towardsdatascience.com/reformer-the-efficient-transformer-dd9830164703", "anchor_text": "Reformer in this"}, {"url": "https://arxiv.org/abs/2004.05150", "anchor_text": "\u201cLongformer: The Long-Document Transformer.\u201d"}, {"url": "https://arxiv.org/abs/2004.05150", "anchor_text": "Longformer Paper"}, {"url": "https://arxiv.org/abs/2004.05150", "anchor_text": "Longformer Paper"}, {"url": "https://arxiv.org/abs/2004.05150", "anchor_text": "Longformer Paper"}, {"url": "https://arxiv.org/abs/2004.05150", "anchor_text": "Longformer Paper"}, {"url": "https://arxiv.org/abs/2004.05150", "anchor_text": "Longformer Paper"}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "Transformer Paper"}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "Vaswani et. al."}, {"url": "https://arxiv.org/abs/2004.05150", "anchor_text": "Longformer Paper"}, {"url": "https://tvm.apache.org/", "anchor_text": "TVM (Tensor Virtual Machine)"}, {"url": "https://arxiv.org/abs/2004.05150", "anchor_text": "Longformer Paper"}, {"url": "https://arxiv.org/abs/2004.05150", "anchor_text": "Longformer: The Long-Document TransformerTransformer-based models are unable to process long sequences due to their self-attention operation, which scales\u2026arxiv.org"}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "Attention Is All You NeedThe dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an\u2026arxiv.org"}, {"url": "https://towardsdatascience.com/transformers-explained-65454c0f3fa7", "anchor_text": "Transformers ExplainedAn exhaustive explanation of Google\u2019s Transformer model; from theory to implementationtowardsdatascience.com"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----cdfeefe81e89---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----cdfeefe81e89---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----cdfeefe81e89---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----cdfeefe81e89---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----cdfeefe81e89---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcdfeefe81e89&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flongformer-the-long-document-transformer-cdfeefe81e89&user=Rohan+Jagtap&userId=39646f947a4c&source=-----cdfeefe81e89---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcdfeefe81e89&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flongformer-the-long-document-transformer-cdfeefe81e89&user=Rohan+Jagtap&userId=39646f947a4c&source=-----cdfeefe81e89---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcdfeefe81e89&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flongformer-the-long-document-transformer-cdfeefe81e89&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----cdfeefe81e89--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fcdfeefe81e89&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flongformer-the-long-document-transformer-cdfeefe81e89&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----cdfeefe81e89---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----cdfeefe81e89--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----cdfeefe81e89--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----cdfeefe81e89--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----cdfeefe81e89--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----cdfeefe81e89--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----cdfeefe81e89--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----cdfeefe81e89--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----cdfeefe81e89--------------------------------", "anchor_text": ""}, {"url": "https://rojagtap.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://rojagtap.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Rohan Jagtap"}, {"url": "https://rojagtap.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "465 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F39646f947a4c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flongformer-the-long-document-transformer-cdfeefe81e89&user=Rohan+Jagtap&userId=39646f947a4c&source=post_page-39646f947a4c--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fe51e2b6202c5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flongformer-the-long-document-transformer-cdfeefe81e89&newsletterV3=39646f947a4c&newsletterV3Id=e51e2b6202c5&user=Rohan+Jagtap&userId=39646f947a4c&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}