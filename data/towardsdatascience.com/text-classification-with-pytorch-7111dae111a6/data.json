{"url": "https://towardsdatascience.com/text-classification-with-pytorch-7111dae111a6", "time": 1683010416.778933, "path": "towardsdatascience.com/text-classification-with-pytorch-7111dae111a6/", "webpage": {"metadata": {"title": "Text Classification with LSTMs in PyTorch | by Fernando L\u00f3pez | Towards Data Science", "h1": "Text Classification with LSTMs in PyTorch", "description": "The aim of this blog is to explain how to build a text classifier based on LSTMs as well as how it is built by using the PyTorch framework. I would like to start with the following question: how to\u2026"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/pdf/2004.03705.pdf", "anchor_text": "Deep Learning Based Text Classification: A Comprehensive Review", "paragraph_index": 4}, {"url": "https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html", "anchor_text": "Introduction to Information Retrieval", "paragraph_index": 5}, {"url": "https://colah.github.io/posts/2015-08-Understanding-LSTMs/", "anchor_text": "Understanding LSTMs Networks.", "paragraph_index": 6}, {"url": "https://www.kaggle.com/c/nlp-getting-started", "anchor_text": "aggle contest", "paragraph_index": 8}, {"url": "https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/", "anchor_text": "BERT", "paragraph_index": 26}, {"url": "https://radimrehurek.com/gensim/models/word2vec.html", "anchor_text": "word2vec-gensim", "paragraph_index": 26}], "all_paragraphs": ["The question remains open: how to learn semantics? what is semantics? would DL-based models be capable to learn semantics?", "The aim of this blog is to explain how to build a text classifier based on LSTMs as well as how it is built by using the PyTorch framework.", "I would like to start with the following question: how to classify a text? Several approaches have been proposed from different viewpoints under different premises, but what is the most suitable one?. It\u2019s interesting to pause for a moment and question ourselves: how we as humans can classify a text?, what do our brains take into account to be able to classify a text?. Such questions are complex to be answered.", "Currently, we have access to a set of different text types such as emails, movie reviews, social media, books, etc. In this sense, the text classification problem would be determined by what\u2019s intended to be classified (e.g. is it intended to classify the polarity of given text? Is it intended to classify a set of movie reviews by category? Is it intended to classify a set of texts by topic? ). In this regard, the problem of text classification is categorized most of the time under the following tasks:", "In order to go deeper into this hot topic, I really recommend to take a look at this paper: Deep Learning Based Text Classification: A Comprehensive Review.", "The two keys in this model are: tokenization and recurrent neural nets. Tokenization refers to the process of splitting a text into a set of sentences or words (i.e. tokens). In this regard, tokenization techniques can be applied at sequence-level or word-level. In order to understand the bases of tokenization you can take a look at: Introduction to Information Retrieval.", "In the other hand, RNNs (Recurrent Neural Networks) are a kind of neural network which are well-known to work well on sequential data, such as the case of text data. In this case, it\u2019s been implemented a special kind of RNN which is LSTMs (Long-Short Term Memory). LSTMs are one of the improved versions of RNNs, essentially LSTMs have shown a better performance working with longer sentences. In order to go deeper about what RNNs and LSTMs are, you can take a look at: Understanding LSTMs Networks.", "Since the idea of this blog is to present a baseline model for text classification, the text preprocessing phase is based on the tokenization technique, meaning that each text sentence will be tokenized, then each token will be transformed into its index-based representation. Then, each token sentence based indexes will be passed sequentially through an embedding layer, this embedding layer will output an embedded representation of each token whose are passed through a two-stacked LSTM neural net, then the last LSTM\u2019s hidden state will be passed through a two-linear layer neural net which outputs a single value filtered by a sigmoid activation function. The following image describes the model architecture:", "The dataset used in this project was taken from a kaggle contest which aimed to predict which tweets are about real disasters and which ones are not. Essentially, the dataset is about a set of tweets in raw format labeled with 1s and 0s (1 means real disaster and 0 means not real disaster). Taking a look a the head of the dataset, it looks like:", "As we can see, there are some columns that must be removed because are meaningless, so after removing the unnecessary columns the resultant dataset will look like:", "At this moment, we can already apply the tokenization technique as well as transforming each token into its index-based representation; this process is explained in the following code snippet:", "There are some fixed hyperparameters that it\u2019s worth to mention. For example, max_len = 10 refers to the maximum length for each sequence and max_words = 100 refers to the top 100 frequent words to be considered given the entire corpus. The function prepare_tokens() transforms the entire corpus into a set of sequences of tokens. The function sequence_to_token() transform each token into its index representation.", "As input layer it is implemented an embedding layer. This embedding layer takes each token and transforms it into an embedded representation. Such an embedded representations is then passed through a two stacked LSTM layer. Finally, the last hidden state of the LSTM is passed through a two-linear layer neural net. The following code snippet shows the mentioned model architecture coded in PyTorch.", "So, let\u2019s analyze some important parts of the showed model architecture. In line 16 the embedding layer is initialized, it receives as parameters: input_size which refers to the size of the vocabulary, hidden_dim which refers to the dimension of the output vector and padding_idx which completes sequences that do not meet the required sequence length with zeros.", "In line 17 the LSTM layer is initialized, it receives as parameters: input_size which refers to the dimension of the embedded token, hidden_size which refers to the dimension of the hidden and cell states, num_layers which refers to the number of stacked LSTM layers and batch_first which refers to the first dimension of the input vector, in this case, it refers to the batch size.", "In lines 18 and 19, the linear layers are initialized, each layer receives as parameters: in_features and out_features which refers to the input and output dimension respectively.", "In order to get ready the training phase, first, we need to prepare the way how the sequences will be fed to the model. For this purpose, PyTorch provides two very useful classes: Dataset and DataLoader. The aim of Dataset class is to provide an easy way to iterate over a dataset by batches. The aim of DataLoader is to create an iterable object of the Dataset class. The following code snippet shows a minimalistic implementation of both classes.", "Now, it\u2019s time to iterate over the training set. First, let\u2019s take a look at how the training phase looks like:", "In line 2 the optimizer is defined. From line 4 the loop over the epochs is realized. It is important to mention that in PyTorch we need to \u201cturn the training mode on\u201d as you can see in line 9, it is necessary to do this especially when we have to change from \u201ctraining mode\u201d to \u201cevaluation mode\u201d (we will see it later). Essentially, the training mode allows updates to gradients and evaluation mode cancels updates to gradients.", "It\u2019s important to highlight that, in line 11 we are using the object created by DatasetLoader to iterate on. In PyTorch is relatively easy to calculate the loss function, calculate the gradients, update the parameters by implementing some optimizer method and take the gradients to zero. As we can see, in line 20 the loss is calculated by implementing binary_cross_entropy as loss function, in line 24 the error is propagated backward (i.e. the gradients are calculated), in line 30 each parameter is updated by implementing RMSprop as the optimizer, then the gradients got free in order to start a new epoch.", "The evaluation part is pretty similar as we did in the training phase, the main difference is about changing from training mode to evaluation mode.", "As we can see, in line 6 the model is changed to evaluation mode, as well as skipping gradients update in line 9. Then, the test set is iterated through the DatasetLoader object (line 12), likewise, the predicted values are saved in the predictions list in line 21.", "Finally, we just need to calculate the accuracy. In order to keep in mind how accuracy is calculated, let\u2019s take a look at the formula:", "In this regard, the accuracy is calculated by:", "In this blog, it\u2019s been explained the importance of text classification as well as the different approaches that can be taken in order to address the problem of text classification under different viewpoints.", "It\u2019s been implemented a baseline model for text classification by using LSTMs neural nets as the core of the model, likewise, the model has been coded by taking the advantages of PyTorch as framework for deep learning models. The dataset used in this model was taken from a Kaggle competition. This dataset is made up of tweets. In the preprocessing step was showed a special technique to work with text data which is Tokenization.", "As it was mentioned, the aim of this blog is to provide a baseline model for the text classification task. It\u2019s important to mention that, the problem of text classifications goes beyond than a two-stacked LSTM architecture where texts are preprocessed under tokens-based methodology. Recent works have shown impressive results by implementing transformers based architectures (e.g. BERT). Nevertheless, by following this thread, this proposed model can be improved by removing the tokens-based methodology and implementing a word embeddings based model instead (e.g. word2vec-gensim). Likewise, bi-directional LSTMs can be applied in order to catch more context (in a forward and backward way).", "Feel free to clone or fork :)", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Machine Learning Engineer | Data Scientist | Software Engineer"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F7111dae111a6&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-classification-with-pytorch-7111dae111a6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-classification-with-pytorch-7111dae111a6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-classification-with-pytorch-7111dae111a6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-classification-with-pytorch-7111dae111a6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----7111dae111a6--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----7111dae111a6--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://ferneutron.medium.com/?source=post_page-----7111dae111a6--------------------------------", "anchor_text": ""}, {"url": "https://ferneutron.medium.com/?source=post_page-----7111dae111a6--------------------------------", "anchor_text": "Fernando L\u00f3pez"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd606f5d846f2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-classification-with-pytorch-7111dae111a6&user=Fernando+L%C3%B3pez&userId=d606f5d846f2&source=post_page-d606f5d846f2----7111dae111a6---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7111dae111a6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-classification-with-pytorch-7111dae111a6&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7111dae111a6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-classification-with-pytorch-7111dae111a6&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://arxiv.org/pdf/2004.03705.pdf", "anchor_text": "Deep Learning Based Text Classification: A Comprehensive Review"}, {"url": "https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html", "anchor_text": "Introduction to Information Retrieval"}, {"url": "https://colah.github.io/posts/2015-08-Understanding-LSTMs/", "anchor_text": "Understanding LSTMs Networks."}, {"url": "https://www.kaggle.com/c/nlp-getting-started", "anchor_text": "aggle contest"}, {"url": "https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/", "anchor_text": "BERT"}, {"url": "https://radimrehurek.com/gensim/models/word2vec.html", "anchor_text": "word2vec-gensim"}, {"url": "https://github.com/FernandoLpz/Text-Classification-LSTMs-PyTorch", "anchor_text": "https://github.com/FernandoLpz/Text-Classification-LSTMs-PyTorch"}, {"url": "https://medium.com/tag/pytorch?source=post_page-----7111dae111a6---------------pytorch-----------------", "anchor_text": "Pytorch"}, {"url": "https://medium.com/tag/python?source=post_page-----7111dae111a6---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/text-mining?source=post_page-----7111dae111a6---------------text_mining-----------------", "anchor_text": "Text Mining"}, {"url": "https://medium.com/tag/text-classification?source=post_page-----7111dae111a6---------------text_classification-----------------", "anchor_text": "Text Classification"}, {"url": "https://medium.com/tag/lstm?source=post_page-----7111dae111a6---------------lstm-----------------", "anchor_text": "Lstm"}, {"url": "https://creativecommons.org/licenses/by-nc/4.0/", "anchor_text": "Some rights reserved"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7111dae111a6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-classification-with-pytorch-7111dae111a6&user=Fernando+L%C3%B3pez&userId=d606f5d846f2&source=-----7111dae111a6---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7111dae111a6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-classification-with-pytorch-7111dae111a6&user=Fernando+L%C3%B3pez&userId=d606f5d846f2&source=-----7111dae111a6---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7111dae111a6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-classification-with-pytorch-7111dae111a6&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----7111dae111a6--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F7111dae111a6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-classification-with-pytorch-7111dae111a6&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----7111dae111a6---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----7111dae111a6--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----7111dae111a6--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----7111dae111a6--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----7111dae111a6--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----7111dae111a6--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----7111dae111a6--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----7111dae111a6--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----7111dae111a6--------------------------------", "anchor_text": ""}, {"url": "https://ferneutron.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://ferneutron.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Fernando L\u00f3pez"}, {"url": "https://ferneutron.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "537 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd606f5d846f2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-classification-with-pytorch-7111dae111a6&user=Fernando+L%C3%B3pez&userId=d606f5d846f2&source=post_page-d606f5d846f2--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F14c367392dfa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-classification-with-pytorch-7111dae111a6&newsletterV3=d606f5d846f2&newsletterV3Id=14c367392dfa&user=Fernando+L%C3%B3pez&userId=d606f5d846f2&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}