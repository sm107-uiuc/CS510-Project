{"url": "https://towardsdatascience.com/mitigating-algorithmic-bias-in-predictive-justice-ux-design-principles-for-ai-fairness-machine-learning-d2227ce28099", "time": 1682993988.5072482, "path": "towardsdatascience.com/mitigating-algorithmic-bias-in-predictive-justice-ux-design-principles-for-ai-fairness-machine-learning-d2227ce28099/", "webpage": {"metadata": {"title": "Mitigating algorithmic bias in predictive justice: 4 design principles for AI fairness | by Slava Polonski, PhD | Towards Data Science", "h1": "Mitigating algorithmic bias in predictive justice: 4 design principles for AI fairness", "description": "When Netflix gets a movie recommendation wrong, you\u2019d probably think that it\u2019s not a big deal. Likewise, when your favourite sneakers don\u2019t make it into Amazon\u2019s list of recommended products, it\u2019s\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.thrillist.com/entertainment/nation/how-new-netflix-recommendation-algorithm-works", "anchor_text": "movie recommendation", "paragraph_index": 0}, {"url": "https://www.amazon.com/gp/help/customer/display.html?nodeId=13316081", "anchor_text": "recommended products", "paragraph_index": 0}, {"url": "https://www.reuters.com/article/us-chicago-police-technology/as-shootings-soar-chicago-police-use-technology-to-predict-crime-idUSKBN1AL08P", "anchor_text": "threat score", "paragraph_index": 0}, {"url": "https://www.wired.com/2014/11/algorithms-great-can-also-ruin-lives/", "anchor_text": "cost of error", "paragraph_index": 1}, {"url": "https://www.wired.com/2014/11/algorithms-great-can-also-ruin-lives/", "anchor_text": "high-stakes settings", "paragraph_index": 1}, {"url": "https://medium.com/@drpolonski/ai-trust-and-ai-fears-a-media-debate-that-could-divide-society-52e16a74c979", "anchor_text": "recommend new cancer treatments", "paragraph_index": 1}, {"url": "http://parisinnovationreview.com/articles-en/predictive-justice-when-algorithms-pervade-the-law", "anchor_text": "help judges assess a suspect\u2019s likelihood of reoffending", "paragraph_index": 1}, {"url": "https://gizmodo.com/study-finds-predictive-policing-no-more-racist-than-reg-1823733844", "anchor_text": "predictive policing", "paragraph_index": 1}, {"url": "http://content.time.com/time/magazine/article/0,9171,2099708-13,00.html", "anchor_text": "predictive policing", "paragraph_index": 2}, {"url": "https://www.imdb.com/title/tt0181689/", "anchor_text": "Minority Report", "paragraph_index": 2}, {"url": "https://en.wikipedia.org/wiki/The_Minority_Report", "anchor_text": "Philip K. Dick", "paragraph_index": 2}, {"url": "http://time.com/4966125/police-departments-algorithms-chicago/", "anchor_text": "already exists today", "paragraph_index": 2}, {"url": "https://www.washingtonpost.com/news/monkey-cage/wp/2016/10/17/can-an-algorithm-be-racist-our-analysis-is-more-cautious-than-propublicas/?noredirect=on&utm_term=.68bfbc303ed5", "anchor_text": "risk-assessment algorithms", "paragraph_index": 2}, {"url": "https://www.nytimes.com/2017/05/01/us/politics/sent-to-prison-by-a-software-programs-secret-algorithms.html?_r=0", "anchor_text": "opaque algorithmic prediction", "paragraph_index": 3}, {"url": "http://www.equivant.com/", "anchor_text": "company", "paragraph_index": 3}, {"url": "http://www.northpointeinc.com/files/downloads/Northpointe_Suite.pdf", "anchor_text": "proprietary software", "paragraph_index": 3}, {"url": "http://www.scotusblog.com/wp-content/uploads/2017/02/16-6387-op-bel-wis.pdf", "anchor_text": "Eric Loomis\u2019 case", "paragraph_index": 3}, {"url": "https://www.theatlantic.com/technology/archive/2018/01/equivant-compas-algorithm/550646/", "anchor_text": "one of many", "paragraph_index": 4}, {"url": "https://epic.org/algorithmic-transparency/crim-justice/", "anchor_text": "Electronic Privacy Information Center", "paragraph_index": 4}, {"url": "https://phys.org/news/2018-01-ai-court-algorithms.html", "anchor_text": "extremely beneficial", "paragraph_index": 4}, {"url": "https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing", "anchor_text": "ProPublica", "paragraph_index": 5}, {"url": "https://www.documentcloud.org/documents/2840784-Practitioner-s-Guide-to-COMPAS-Core.html#document/p30/a296482", "anchor_text": "future criminals", "paragraph_index": 5}, {"url": "https://cloud.google.com/blog/products/ai-machine-learning/steering-the-right-course-for-ai", "anchor_text": "as reliable as the data they\u2019re trained on", "paragraph_index": 6}, {"url": "https://gizmodo.com/black-data-is-the-reason-why-smart-policing-is-still-1819288923", "anchor_text": "amplified", "paragraph_index": 6}, {"url": "https://cloud.google.com/blog/products/ai-machine-learning/steering-the-right-course-for-ai", "anchor_text": "small decisions", "paragraph_index": 6}, {"url": "https://www.politifact.com/punditfact/statements/2016/jul/13/van-jones/van-jones-claim-drug-use-imprisonment-rates-blacks/", "anchor_text": "evidence", "paragraph_index": 8}, {"url": "https://researchportal.bath.ac.uk/en/publications/semantics-derived-automatically-from-language-corpora-necessarily", "anchor_text": "embedded bias", "paragraph_index": 8}, {"url": "https://seejane.org/wp-content/uploads/gender-bias-without-borders-executive-summary.pdf", "anchor_text": "vastly underrepresented", "paragraph_index": 9}, {"url": "https://seejane.org/research-informs-empowers/data/", "anchor_text": "quantify these inequalities", "paragraph_index": 9}, {"url": "https://ai.googleblog.com/2017/08/exploring-and-visualizing-open-global.html?m=1", "anchor_text": "\u201cQuick, Draw!\u201d", "paragraph_index": 10}, {"url": "https://www.youtube.com/watch?v=X8v1GWzZYJ4", "anchor_text": "2 billion diverse drawings", "paragraph_index": 10}, {"url": "https://www.youtube.com/watch?v=59bMh59JQDo", "anchor_text": "shoes", "paragraph_index": 11}, {"url": "https://www.converse.com/us/en_us/c/converse/chuck-taylor", "anchor_text": "Converse sneakers", "paragraph_index": 11}, {"url": "https://www.youtube.com/watch?v=NF98WCdvR6U", "anchor_text": "shoes that did not look like sneakers", "paragraph_index": 11}, {"url": "https://papers.nips.cc/paper/6228-man-is-to-computer-programmer-as-woman-is-to-homemaker-debiasing-word-embeddings.pdf", "anchor_text": "studies", "paragraph_index": 12}, {"url": "https://medium.com/@drpolonski/algorithmic-determinism-and-the-limits-of-artificial-intelligence-d32397b8f618", "anchor_text": "exclusively male candidates as the likely winner of the presidential race", "paragraph_index": 12}, {"url": "https://techcrunch.com/2016/11/06/would-you-let-an-algorithm-choose-the-next-u-s-president/", "anchor_text": "TechCrunch", "paragraph_index": 13}, {"url": "http://www.ewdn.com/2016/08/08/neural-networks-see-donald-trump-as-us-president/", "anchor_text": "neural networks", "paragraph_index": 13}, {"url": "https://www.weforum.org/agenda/2016/08/the-biggest-threat-to-democracy-your-social-media-feed", "anchor_text": "echo chambers in our social media feeds", "paragraph_index": 14}, {"url": "http://www.parliament.uk/about/living-heritage/building/palace/architecture/palacestructure/churchill/", "anchor_text": "Winston Churchill", "paragraph_index": 15}, {"url": "https://www.huntonprivacyblog.com/wp-content/uploads/sites/28/2018/10/ICDPPC-40th_AI-Declaration_ADOPTED.pdf", "anchor_text": "Declaration on Ethics and Protection in Artificial Intelligence", "paragraph_index": 16}, {"url": "https://developers.google.com/machine-learning/crash-course/classification/accuracy", "anchor_text": "classification accuracy", "paragraph_index": 17}, {"url": "https://medium.com/hugo-ferreiras-blog/confusion-matrix-and-other-metrics-in-machine-learning-894688cb1c0a", "anchor_text": "confusion matrices", "paragraph_index": 17}, {"url": "https://quickdraw.withgoogle.com/", "anchor_text": "Quick, draw!", "paragraph_index": 18}, {"url": "https://ai.googleblog.com/2017/08/exploring-and-visualizing-open-global.html?m=1", "anchor_text": "additional training examples of other shoe types", "paragraph_index": 18}, {"url": "https://arxiv.org/abs/1801.07593", "anchor_text": "algorithmic techniques", "paragraph_index": 18}, {"url": "https://github.com/conversationai/unintended-ml-bias-analysis/blob/master/presentations/measuring-mitigating-unintended-bias-paper.pdf", "anchor_text": "measure misrepresentation", "paragraph_index": 18}, {"url": "https://papers.nips.cc/paper/6316-satisfying-real-world-goals-with-dataset-constraints", "anchor_text": "mitigate unwanted bias", "paragraph_index": 18}, {"url": "http://nautil.us/issue/55/trust/are-algorithms-building-the-new-infrastructure-of-racism", "anchor_text": "confounding factors", "paragraph_index": 20}, {"url": "https://arxiv.org/abs/1703.04957", "anchor_text": "research", "paragraph_index": 20}, {"url": "https://arxiv.org/pdf/1703.04957v1.pdf", "anchor_text": "supplementary machine learning algorithm", "paragraph_index": 20}, {"url": "https://rework.withgoogle.com/guides/unbiasing-raise-awareness/steps/watch-unconscious-bias-at-work/", "anchor_text": "unconscious bias busting exercises", "paragraph_index": 22}, {"url": "https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing", "anchor_text": "ProPublica", "paragraph_index": 22}, {"url": "https://edps.europa.eu/sites/edp/files/publication/16-06-17_presentation_implementation_of_gdpr_en.pdf", "anchor_text": "Privacy International", "paragraph_index": 22}, {"url": "https://www.youtube.com/watch?v=gLoI9hAX9dw", "anchor_text": "deep-fakes", "paragraph_index": 23}, {"url": "https://www.vox.com/2018/4/18/17252410/jordan-peele-obama-deepfake-buzzfeed", "anchor_text": "fake videos", "paragraph_index": 23}, {"url": "https://blogs.nvidia.com/blog/2017/12/03/nvidia-research-nips/", "anchor_text": "fake images of situations", "paragraph_index": 23}, {"url": "https://www.youtube.com/watch?v=gLoI9hAX9dw", "anchor_text": "no longer determine", "paragraph_index": 23}, {"url": "https://www.vox.com/2018/4/18/17252410/jordan-peele-obama-deepfake-buzzfeed", "anchor_text": "truth", "paragraph_index": 23}, {"url": "https://twitter.com/olifranklin/status/937660128974852096?lang=en", "anchor_text": "conclude", "paragraph_index": 23}, {"url": "https://www.ischool.berkeley.edu/projects/2018/unmasking-deepfakes", "anchor_text": "effective and scalable counter-measures", "paragraph_index": 23}, {"url": "https://design.google/library/fair-not-default/", "anchor_text": "proactive steps to change this default", "paragraph_index": 24}, {"url": "https://ai.google/education/responsible-ai-practices?category=fairness", "anchor_text": "measure and mitigate unfair bias", "paragraph_index": 25}, {"url": "https://www.vyacheslavpolonski.com", "anchor_text": "Dr Vyacheslav Polonski", "paragraph_index": 27}, {"url": "http://www.ox.ac.uk", "anchor_text": "University of Oxford", "paragraph_index": 27}, {"url": "http://www.weforum.org/", "anchor_text": "World Economic Forum", "paragraph_index": 27}, {"url": "https://www.weforum.org/communities/expert-network", "anchor_text": "Expert Network", "paragraph_index": 27}, {"url": "http://www.weforum.org/community/global-shapers", "anchor_text": "WEF Global Shapers", "paragraph_index": 27}, {"url": "https://www.forbes.com/30-under-30-europe/2018/#7ab751f47eaa", "anchor_text": "Forbes 30 Under 30", "paragraph_index": 27}, {"url": "https://www.weforum.org/agenda/2018/11/algorithms-court-criminals-jail-time-fair/", "anchor_text": "World Economic Forum Agenda", "paragraph_index": 28}, {"url": "https://www.weforum.org/events/annual-meeting-of-the-global-future-councils", "anchor_text": "WEF Annual Meeting of the Global Future Councils 2018", "paragraph_index": 28}], "all_paragraphs": ["When Netflix gets a movie recommendation wrong, you\u2019d probably think that it\u2019s not a big deal. Likewise, when your favourite sneakers don\u2019t make it into Amazon\u2019s list of recommended products, it\u2019s probably not the end of the world. But when an algorithm assigns you a threat score from 1 to 500 that is used to rule on jail time, you might have some concerns about this use of predictive analytics.", "Artificial intelligence (AI) has now permeated almost every aspect of our lives. Naturally, machine predictions cannot always be 100% accurate. But the cost of error dramatically increases when AI is implemented in high-stakes settings. This could include medicine to recommend new cancer treatments, or criminal justice to help judges assess a suspect\u2019s likelihood of reoffending. In fact, one of the most controversial uses of AI in recent years has been predictive policing.", "To the general audience, predictive policing methods are probably best known from the 2002 science fiction movie Minority Report starring Tom Cruise. Based on a short story by Philip K. Dick, the movie presents a vision of the future in which crimes can be predicted and prevented. This may sound like a far-fetched utopian scenario. However, predictive justice already exists today. Built on advanced machine learning systems, there is a wave of new companies that provide predictive services to courts; for example, in the form of risk-assessment algorithms that estimate the likelihood of recidivism for criminals to help judges in their decision-making.", "Prison terms in the US are increasingly based on biased AI predictions. For example, after his arrest in 2013, Eric Loomis was sentenced to six years in prison based in part on an opaque algorithmic prediction that he would commit more crimes. Equivant (formerly Northpointe), the company behind the proprietary softwareused in Eric Loomis\u2019 case, claims to have provided a 360-degree view of the defendant in order to provide detailed algorithmic assistance in judicial decision-making.", "This company is one of many players in the predictive justice field in the US. A recent report by the Electronic Privacy Information Center finds that algorithms are increasingly used in court to \u201cset bail, determine sentences, and even contribute to determinations about guilt or innocence\u201d. This shift towards more machine intelligence in courts, allowing AI to augment human judgement, could be extremely beneficial for the judicial system as a whole.", "However, an investigative report by ProPublica found that these algorithms tend to reinforce racial bias in law enforcement data. Algorithmic assessments tend to falsely flag black defendants as future criminals at almost twice the rate as white defendants. What is more, the judges who relied on these risk-assessments typically did not understand how the scores were computed.", "This is problematic, because machine learning models are only as reliable as the data they\u2019re trained on. If the underlying data is biased in any form, there is a risk that structural inequalities and unfair biases are not just replicated, but also amplified. In this regard, AI engineers must be especially wary of their blind spots and implicit assumptions; it is not just the choice of machine learning techniques that matters, but also all the small decisions about finding, organising and labelling training data for AI models.", "Even small irregularities and biases can produce a measurable difference in the final risk-assessment. The critical issue is that problems like racial bias and structural discrimination are baked into the world around us.", "For instance, there is evidence that, despite similar rates of drug use, black Americans are arrested at four times the rate of white Americans on drug-related charges. Even if engineers were to faithfully collect this data and train a machine learning model with it, the AI would still pick up the embedded bias as part of the model.", "Systematic patterns of inequality are everywhere. If you look at the top grossing movies of 2014/2015 you can see that female characters are vastly underrepresented both in terms of screen time and speaking time. New machine learning models can quantify these inequalities, but there are a lot of open questions about how engineers can proactively mitigate them.", "Google\u2019s recent \u201cQuick, Draw!\u201d experiment vividly demonstrates why addressing bias matters. The experiment invited internet users worldwide to participate in a fun game of drawing. In every round of the game, users were challenged to draw an object in under 20 seconds. The AI system would then try to guess what their drawing depicts. More than 20 million people from 100 nations participated in the game, resulting in over 2 billion diverse drawings of all sorts of objects, including cats, chairs, postcards, butterflies, skylines, etc.", "But when the researchers examined the drawings of shoes in the data-set, they realised that they were dealing with strong cultural bias. A large number of early users drew shoes that looked like Converse sneakers. This led the model to pick up the typical visual attributes of sneakers as the prototypical example of what a \u201cshoe\u201d should look like. Consequently, shoes that did not look like sneakers, such as high heels, ballerinas or clogs, were not recognized as shoes.", "Furthermore, recent studies show that, if left unchecked, machine learning models will learn outdated gender stereotypes, such as \u201cdoctors\u201d being male and \u201creceptionists\u201d being female. In a similar fashion, AI models trained on images of past US presidents have been shown to predict exclusively male candidates as the likely winner of the presidential race.", "As early as 2016, I wrote a piece for TechCrunch, where I explained that machine learning systems can only use historical data to make predictions about the future. This is why neural networks trained on images of past U.S. presidents predicted Donald Trump would win the upcoming U.S. election, after being trained with images of past (male) presidents. Because there were no female presidents in the data set, the AI was unable to deduce that gender was not a relevant characteristic for the model. In practice, if this particular AI were to elect the next president, it would vote for Trump.", "These inferences result in increasingly deterministic recommendation systems, which tend to reinforce existing beliefs and practices similar to the echo chambers in our social media feeds. The more we rely on personalized algorithms in everyday life, the more they will shape what we see, what we read, who we talk to and how we live. By relentlessly focusing on the status quo, new recommendations on books to read, movies to watch and people to meet will give us more of the same things that have previously delighted us.", "When your past unequivocally dictates your future, personal development through spontaneity, open-mindedness and experimentation becomes more difficult. In this way, the notion of algorithmic determinism echoes what Winston Churchill once said about buildings: We shape our algorithms; thereafter, they shape us.", "In October 2018, the International Conference of Data Protection and Privacy Commissioners released the Declaration on Ethics and Protection in Artificial Intelligence, one of the first steps towards a set of international governance principles on AI. The declaration states that \u201cunlawful biases or discriminations that may result from the use of data in artificial intelligence should be reduced and mitigated\u201d.", "Inherent to this notion is the assertion that AI needs to be evaluated on a broader set of ethical and legal criteria; not just based on classification accuracy and confusion matrices. Expanding on this argument, I propose the following principles of AI fairness for the purposes of predictive justice:", "In order to guard against unfair bias, all subjects should have an equal chance of being represented in the data. Sometimes this means that underrepresented populations need to be thoughtfully added to any training datasets. Sometimes this also means that a biased machine learning model needs to be substantially retrained on diverse data sources. In the case of Google\u2019s Quick, draw!experiment, the engineering team had to intentionally seek out additional training examples of other shoe types, like high heels and crocs, to compensate for gaps in representation. What is more, recent research offers new algorithmic techniques to measure misrepresentation and help mitigate unwanted bias in machine learning.", "Machine learning systems need to avoid unjust effects on individuals, especially impacts related to social and physical vulnerabilities, and other sensitive attributes. These could include race, ethnicity, gender, nationality, sexual orientation, religion and political beliefs. The overall fairness of an algorithm must be judged by how it impacts the most vulnerable people affected by it.", "However, simply omitting sensitive variables from machine learning models would not solve the problem due to the variety of confounding factors that may be correlated with them. With regard to criminal justice, research shows that omitting race from a dataset of criminal histories still results in racially disparate predictions. Instead, there is early evidence that racial disparities and other pieces of sensitive information can be removed from data-sets using a supplementary machine learning algorithm. The hope is that, in the future, this approach could help engineers build a \u201crace-neutral\u201d AI system for recidivism prediction.", "Algorithmic fairness means much more than the absence of injustice; it represents the active responsibility to continuously strive for fairness in the design of machine learning systems. In this regard, the spirit of stewardship can only be borne by a diverse team that challenges each other\u2019s implicit assumptions.", "In regular unconscious bias busting exercises, for example, teams can develop an appreciation for the diversity of perspectives. Several NGOs, including ProPublica and Privacy International, have also begun advocating for diversity stewardship in companies that build large-scale AI models. Thus, only by creating a culture of inclusiveness can companies create the right conditions for teams to address unfair bias in machine learning.", "The final principle refers not just to the authenticity of training data, but also the authenticity of AI predictions as they are used to inform human decision-making. For instance, despite continued efforts to limit potentially harmful or abusive applications, machine learning has been regretfully used in the past to distort reality through deep-fakes. In this context, the pervasive misuse of AI could help malicious actors to generate fake videos of people saying things they never said before, or fake images of situations that never happened in real life. Taken to the extreme, this could lead to a world when judges can no longer determine whether any depicted media or evidence corresponds to the truth. Hence, this has led some media pundits to conclude that the \u201cbiggest casualty to AI won\u2019t be jobs, but the final and complete eradication of trust in anything you see or hear.\u201d Fortunately, AI researchers are already working on effective and scalable counter-measures to detect various forms of manipulated media.", "These four principles can help to start a conversation about AI fairness, especially when used for predictive justice. Fairness is never the default in machine learning. As such, engineers need to take proactive steps to change this default. If we do not actively design for AI fairness, we risk perpetuating harmful biases and stereotypes.", "One of the most impressive things about AI, however, is that algorithms can also be effectively used to measure and mitigate unfair bias. Going forward, there\u2019s hope that machine learning engineers will further develop these techniques; extending them to meaningfully assist human decision-makers with predictions that will be free from prejudice.", "The views expressed in this article are my own and are based on my previous research at Oxford University. They do not represent the views of Google in any way.", "About the author: Dr Vyacheslav Polonski is a UX researcher Google. Previously, he was a researcher at the University of Oxford, studying complex social networks and collective behaviour. He holds a PhD in computational social science and has previously studied at Harvard, Oxford and LSE. He is actively involved in the World Economic Forum Expert Network and the WEF Global Shapers community. In 2018, Forbes Magazine featured his work and research on the Forbes 30 Under 30 list for Europe. He writes about the intersection of sociology, network science and technology.", "An earlier version of this article appeared in the World Economic Forum Agenda as part of the WEF Annual Meeting of the Global Future Councils 2018.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "UX Research Lead @ Google Flights | 20% People+AI Guidebook | Forbes 30 Under 30 | PhD | Global Shaper & Expert @WEF | Prevsly @UniofOxford @Harvard"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fd2227ce28099&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmitigating-algorithmic-bias-in-predictive-justice-ux-design-principles-for-ai-fairness-machine-learning-d2227ce28099&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmitigating-algorithmic-bias-in-predictive-justice-ux-design-principles-for-ai-fairness-machine-learning-d2227ce28099&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmitigating-algorithmic-bias-in-predictive-justice-ux-design-principles-for-ai-fairness-machine-learning-d2227ce28099&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmitigating-algorithmic-bias-in-predictive-justice-ux-design-principles-for-ai-fairness-machine-learning-d2227ce28099&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----d2227ce28099--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d2227ce28099--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@slavaxyz?source=post_page-----d2227ce28099--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@slavaxyz?source=post_page-----d2227ce28099--------------------------------", "anchor_text": "Slava Polonski, PhD"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fba50eee34de9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmitigating-algorithmic-bias-in-predictive-justice-ux-design-principles-for-ai-fairness-machine-learning-d2227ce28099&user=Slava+Polonski%2C+PhD&userId=ba50eee34de9&source=post_page-ba50eee34de9----d2227ce28099---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd2227ce28099&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmitigating-algorithmic-bias-in-predictive-justice-ux-design-principles-for-ai-fairness-machine-learning-d2227ce28099&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd2227ce28099&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmitigating-algorithmic-bias-in-predictive-justice-ux-design-principles-for-ai-fairness-machine-learning-d2227ce28099&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.thrillist.com/entertainment/nation/how-new-netflix-recommendation-algorithm-works", "anchor_text": "movie recommendation"}, {"url": "https://www.amazon.com/gp/help/customer/display.html?nodeId=13316081", "anchor_text": "recommended products"}, {"url": "https://www.reuters.com/article/us-chicago-police-technology/as-shootings-soar-chicago-police-use-technology-to-predict-crime-idUSKBN1AL08P", "anchor_text": "threat score"}, {"url": "https://www.wired.com/2014/11/algorithms-great-can-also-ruin-lives/", "anchor_text": "cost of error"}, {"url": "https://www.wired.com/2014/11/algorithms-great-can-also-ruin-lives/", "anchor_text": "high-stakes settings"}, {"url": "https://medium.com/@drpolonski/ai-trust-and-ai-fears-a-media-debate-that-could-divide-society-52e16a74c979", "anchor_text": "recommend new cancer treatments"}, {"url": "http://parisinnovationreview.com/articles-en/predictive-justice-when-algorithms-pervade-the-law", "anchor_text": "help judges assess a suspect\u2019s likelihood of reoffending"}, {"url": "https://gizmodo.com/study-finds-predictive-policing-no-more-racist-than-reg-1823733844", "anchor_text": "predictive policing"}, {"url": "http://content.time.com/time/magazine/article/0,9171,2099708-13,00.html", "anchor_text": "predictive policing"}, {"url": "https://www.imdb.com/title/tt0181689/", "anchor_text": "Minority Report"}, {"url": "https://en.wikipedia.org/wiki/The_Minority_Report", "anchor_text": "Philip K. Dick"}, {"url": "http://time.com/4966125/police-departments-algorithms-chicago/", "anchor_text": "already exists today"}, {"url": "https://www.washingtonpost.com/news/monkey-cage/wp/2016/10/17/can-an-algorithm-be-racist-our-analysis-is-more-cautious-than-propublicas/?noredirect=on&utm_term=.68bfbc303ed5", "anchor_text": "risk-assessment algorithms"}, {"url": "https://www.nytimes.com/2017/05/01/us/politics/sent-to-prison-by-a-software-programs-secret-algorithms.html?_r=0", "anchor_text": "opaque algorithmic prediction"}, {"url": "http://www.equivant.com/", "anchor_text": "company"}, {"url": "http://www.northpointeinc.com/files/downloads/Northpointe_Suite.pdf", "anchor_text": "proprietary software"}, {"url": "http://www.scotusblog.com/wp-content/uploads/2017/02/16-6387-op-bel-wis.pdf", "anchor_text": "Eric Loomis\u2019 case"}, {"url": "https://www.theatlantic.com/technology/archive/2018/01/equivant-compas-algorithm/550646/", "anchor_text": "one of many"}, {"url": "https://epic.org/algorithmic-transparency/crim-justice/", "anchor_text": "Electronic Privacy Information Center"}, {"url": "https://phys.org/news/2018-01-ai-court-algorithms.html", "anchor_text": "extremely beneficial"}, {"url": "https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing", "anchor_text": "ProPublica"}, {"url": "https://www.documentcloud.org/documents/2840784-Practitioner-s-Guide-to-COMPAS-Core.html#document/p30/a296482", "anchor_text": "future criminals"}, {"url": "https://cloud.google.com/blog/products/ai-machine-learning/steering-the-right-course-for-ai", "anchor_text": "as reliable as the data they\u2019re trained on"}, {"url": "https://gizmodo.com/black-data-is-the-reason-why-smart-policing-is-still-1819288923", "anchor_text": "amplified"}, {"url": "https://cloud.google.com/blog/products/ai-machine-learning/steering-the-right-course-for-ai", "anchor_text": "small decisions"}, {"url": "https://www.politifact.com/punditfact/statements/2016/jul/13/van-jones/van-jones-claim-drug-use-imprisonment-rates-blacks/", "anchor_text": "evidence"}, {"url": "https://researchportal.bath.ac.uk/en/publications/semantics-derived-automatically-from-language-corpora-necessarily", "anchor_text": "embedded bias"}, {"url": "https://seejane.org/wp-content/uploads/gender-bias-without-borders-executive-summary.pdf", "anchor_text": "vastly underrepresented"}, {"url": "https://seejane.org/research-informs-empowers/data/", "anchor_text": "quantify these inequalities"}, {"url": "https://ai.googleblog.com/2017/08/exploring-and-visualizing-open-global.html?m=1", "anchor_text": "\u201cQuick, Draw!\u201d"}, {"url": "https://www.youtube.com/watch?v=X8v1GWzZYJ4", "anchor_text": "2 billion diverse drawings"}, {"url": "https://www.youtube.com/watch?v=59bMh59JQDo", "anchor_text": "shoes"}, {"url": "https://www.converse.com/us/en_us/c/converse/chuck-taylor", "anchor_text": "Converse sneakers"}, {"url": "https://www.youtube.com/watch?v=NF98WCdvR6U", "anchor_text": "shoes that did not look like sneakers"}, {"url": "https://papers.nips.cc/paper/6228-man-is-to-computer-programmer-as-woman-is-to-homemaker-debiasing-word-embeddings.pdf", "anchor_text": "studies"}, {"url": "https://medium.com/@drpolonski/algorithmic-determinism-and-the-limits-of-artificial-intelligence-d32397b8f618", "anchor_text": "exclusively male candidates as the likely winner of the presidential race"}, {"url": "https://techcrunch.com/2016/11/06/would-you-let-an-algorithm-choose-the-next-u-s-president/", "anchor_text": "TechCrunch"}, {"url": "http://www.ewdn.com/2016/08/08/neural-networks-see-donald-trump-as-us-president/", "anchor_text": "neural networks"}, {"url": "https://www.weforum.org/agenda/2016/08/the-biggest-threat-to-democracy-your-social-media-feed", "anchor_text": "echo chambers in our social media feeds"}, {"url": "http://www.parliament.uk/about/living-heritage/building/palace/architecture/palacestructure/churchill/", "anchor_text": "Winston Churchill"}, {"url": "https://www.huntonprivacyblog.com/wp-content/uploads/sites/28/2018/10/ICDPPC-40th_AI-Declaration_ADOPTED.pdf", "anchor_text": "Declaration on Ethics and Protection in Artificial Intelligence"}, {"url": "https://developers.google.com/machine-learning/crash-course/classification/accuracy", "anchor_text": "classification accuracy"}, {"url": "https://medium.com/hugo-ferreiras-blog/confusion-matrix-and-other-metrics-in-machine-learning-894688cb1c0a", "anchor_text": "confusion matrices"}, {"url": "https://quickdraw.withgoogle.com/", "anchor_text": "Quick, draw!"}, {"url": "https://ai.googleblog.com/2017/08/exploring-and-visualizing-open-global.html?m=1", "anchor_text": "additional training examples of other shoe types"}, {"url": "https://arxiv.org/abs/1801.07593", "anchor_text": "algorithmic techniques"}, {"url": "https://github.com/conversationai/unintended-ml-bias-analysis/blob/master/presentations/measuring-mitigating-unintended-bias-paper.pdf", "anchor_text": "measure misrepresentation"}, {"url": "https://papers.nips.cc/paper/6316-satisfying-real-world-goals-with-dataset-constraints", "anchor_text": "mitigate unwanted bias"}, {"url": "http://nautil.us/issue/55/trust/are-algorithms-building-the-new-infrastructure-of-racism", "anchor_text": "confounding factors"}, {"url": "https://arxiv.org/abs/1703.04957", "anchor_text": "research"}, {"url": "https://arxiv.org/pdf/1703.04957v1.pdf", "anchor_text": "supplementary machine learning algorithm"}, {"url": "https://rework.withgoogle.com/guides/unbiasing-raise-awareness/steps/watch-unconscious-bias-at-work/", "anchor_text": "unconscious bias busting exercises"}, {"url": "https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing", "anchor_text": "ProPublica"}, {"url": "https://edps.europa.eu/sites/edp/files/publication/16-06-17_presentation_implementation_of_gdpr_en.pdf", "anchor_text": "Privacy International"}, {"url": "https://www.youtube.com/watch?v=gLoI9hAX9dw", "anchor_text": "deep-fakes"}, {"url": "https://www.vox.com/2018/4/18/17252410/jordan-peele-obama-deepfake-buzzfeed", "anchor_text": "fake videos"}, {"url": "https://blogs.nvidia.com/blog/2017/12/03/nvidia-research-nips/", "anchor_text": "fake images of situations"}, {"url": "https://www.youtube.com/watch?v=gLoI9hAX9dw", "anchor_text": "no longer determine"}, {"url": "https://www.vox.com/2018/4/18/17252410/jordan-peele-obama-deepfake-buzzfeed", "anchor_text": "truth"}, {"url": "https://twitter.com/olifranklin/status/937660128974852096?lang=en", "anchor_text": "conclude"}, {"url": "https://www.ischool.berkeley.edu/projects/2018/unmasking-deepfakes", "anchor_text": "effective and scalable counter-measures"}, {"url": "https://design.google/library/fair-not-default/", "anchor_text": "proactive steps to change this default"}, {"url": "https://ai.google/education/responsible-ai-practices?category=fairness", "anchor_text": "measure and mitigate unfair bias"}, {"url": "https://www.vyacheslavpolonski.com", "anchor_text": "Dr Vyacheslav Polonski"}, {"url": "http://www.ox.ac.uk", "anchor_text": "University of Oxford"}, {"url": "http://www.weforum.org/", "anchor_text": "World Economic Forum"}, {"url": "https://www.weforum.org/communities/expert-network", "anchor_text": "Expert Network"}, {"url": "http://www.weforum.org/community/global-shapers", "anchor_text": "WEF Global Shapers"}, {"url": "https://www.forbes.com/30-under-30-europe/2018/#7ab751f47eaa", "anchor_text": "Forbes 30 Under 30"}, {"url": "https://www.weforum.org/agenda/2018/11/algorithms-court-criminals-jail-time-fair/", "anchor_text": "World Economic Forum Agenda"}, {"url": "https://www.weforum.org/events/annual-meeting-of-the-global-future-councils", "anchor_text": "WEF Annual Meeting of the Global Future Councils 2018"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----d2227ce28099---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----d2227ce28099---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/algorithmic-bias?source=post_page-----d2227ce28099---------------algorithmic_bias-----------------", "anchor_text": "Algorithmic Bias"}, {"url": "https://medium.com/tag/predictive-justice?source=post_page-----d2227ce28099---------------predictive_justice-----------------", "anchor_text": "Predictive Justice"}, {"url": "https://medium.com/tag/ux-research?source=post_page-----d2227ce28099---------------ux_research-----------------", "anchor_text": "UX Research"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd2227ce28099&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmitigating-algorithmic-bias-in-predictive-justice-ux-design-principles-for-ai-fairness-machine-learning-d2227ce28099&user=Slava+Polonski%2C+PhD&userId=ba50eee34de9&source=-----d2227ce28099---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd2227ce28099&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmitigating-algorithmic-bias-in-predictive-justice-ux-design-principles-for-ai-fairness-machine-learning-d2227ce28099&user=Slava+Polonski%2C+PhD&userId=ba50eee34de9&source=-----d2227ce28099---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd2227ce28099&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmitigating-algorithmic-bias-in-predictive-justice-ux-design-principles-for-ai-fairness-machine-learning-d2227ce28099&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d2227ce28099--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fd2227ce28099&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmitigating-algorithmic-bias-in-predictive-justice-ux-design-principles-for-ai-fairness-machine-learning-d2227ce28099&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----d2227ce28099---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----d2227ce28099--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----d2227ce28099--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----d2227ce28099--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----d2227ce28099--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----d2227ce28099--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----d2227ce28099--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----d2227ce28099--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----d2227ce28099--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@slavaxyz?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@slavaxyz?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Slava Polonski, PhD"}, {"url": "https://medium.com/@slavaxyz/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "2.2K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fba50eee34de9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmitigating-algorithmic-bias-in-predictive-justice-ux-design-principles-for-ai-fairness-machine-learning-d2227ce28099&user=Slava+Polonski%2C+PhD&userId=ba50eee34de9&source=post_page-ba50eee34de9--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Faec5396a3770&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmitigating-algorithmic-bias-in-predictive-justice-ux-design-principles-for-ai-fairness-machine-learning-d2227ce28099&newsletterV3=ba50eee34de9&newsletterV3Id=aec5396a3770&user=Slava+Polonski%2C+PhD&userId=ba50eee34de9&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}