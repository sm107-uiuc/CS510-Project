{"url": "https://towardsdatascience.com/hsic-bottleneck-an-alternative-to-back-propagation-36e951d4582c", "time": 1683007212.7351658, "path": "towardsdatascience.com/hsic-bottleneck-an-alternative-to-back-propagation-36e951d4582c/", "webpage": {"metadata": {"title": "HSIC bottleneck: An alternative to Back-Propagation | by David St\u00e9phane | Towards Data Science", "h1": "HSIC bottleneck: An alternative to Back-Propagation", "description": "Deep learning models learn using gradient descent based methods. This is achieved thanks to a particular algorithm , back-propagation, that allows us to retro-propagate the training error through the\u2026"}, "outgoing_paragraph_urls": [{"url": "http://neuralnetworksanddeeplearning.com/chap2.html", "anchor_text": "2", "paragraph_index": 4}, {"url": "http://michaelnielsen.org/", "anchor_text": "Michael Nielsen", "paragraph_index": 4}, {"url": "https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained", "anchor_text": "Kullback Leibler divergence", "paragraph_index": 12}, {"url": "https://en.wikipedia.org/wiki/Naftali_Tishby", "anchor_text": "Naftali Tishby", "paragraph_index": 22}, {"url": "https://en.wikipedia.org/wiki/William_Bialek", "anchor_text": "William Bialek", "paragraph_index": 22}, {"url": "http://neuralnetworksanddeeplearning.com/chap2.html", "anchor_text": "How the backpropagation algorithm works", "paragraph_index": 53}, {"url": "https://papers.nips.cc/paper/3201-a-kernel-statistical-test-of-independence.pdf", "anchor_text": "A Kernel Statistical Test of Independence", "paragraph_index": 54}, {"url": "https://arxiv.org/abs/1612.00410", "anchor_text": "Deep variational information bottleneck", "paragraph_index": 55}], "all_paragraphs": ["Deep learning models learn using gradient descent based methods. This is achieved thanks to a particular algorithm , back-propagation, that allows us to retro-propagate the training error through the entire model parameters, by computing the partial derivatives, for the parameters update.This algorithm is very costly since it requires a backward operation through the entire model to compute the partial derivatives, after a forward pass.", "But what if we could avoid that? What if there was no need for us to compute a global loss at the very end of the model and then retro-propagate this error through the entire model? What if we could estimate directly during the forward pass the \u201ctraining objective\u201d of each hidden layer of our models and then compute directly the gradients of the parameters involved with no need of back-propagation?", "Well, I came across a very interesting paper [1] that investigates this: The HSIC Bottleneck: Deep Learning without Back-Propagation by Wan-Duo Kurt Ma, J.P. Lewis, W. Bastiaan Kleijn. They use Information Theory in order to achieve such a task.", "Back-propagation is an algorithm based on chain rule, that enables the computation of the partial derivatives of a loss function with respect to all the parameters in a feed-forward neural network. In this algorithm, the partial derivatives are computed in a backward pass from the last layer to the first one, thus the name back-propagation, with no possibility of parallelization.", "I will not be extending any further on back-propagation. Interested in an in-depth analysis of this algorithm, have a look at this amazing blog [2] by Michael Nielsen.", "Before attacking the core of this article, let us first introduce some quick notions in Information Theory.", "Information theory gives us tools to measure some quantities of information one can hold about the distribution of a given random variable. For example with Information Theory, we can compute the entropy or the uncertainty of a random variable X.", "Let us consider a set of events A, B, C, D, E, F and a probability distribution over these events, regarding their occurrence likelihood. Let A be a very likely event to occur (with a probability near 1), while B, C, D, E, F are very unlikely.", "It clearly appears that the entropy of our variable here (which can take values A, B, C, D, E or F) is near 0, regarding the entropy formula.", "If we consider a second scenario with the same variable and the same set of events, but this time a uniform probability distribution over our set of events, we end up with an entropy of 0.77.", "Is this surprising? Not really. In the first scenario, we are very certain about which event is likely to occur resulting in a low measure of uncertainty (entropy). On the other hand, the uniform distribution over our set of events in the second scenario increases the uncertainty about the information we hold, we are not sure anymore about which event will occur.", "Another interesting tool from Information Theory is the mutual information (MI). MI measures the dependence between two random variables.", "As we can see, MI measures the Kullback Leibler divergence [2] between the joint probability of two random variables and the product of the probability of both variables, as if they were independent. With two independent variables, we have p(X,Y) = p(X).p(Y), resulting in a null KL divergence and then a null MI. The opposite is observed when the two variables are strongly dependent : exploding KL divergence .", "Well, that was interesting! But what does it have to do with our topic today? Keep holding on, we are coming to it ;) .", "Let\u2019s take a moment first to think about what our deep learning models actually do. Roughly speaking, when building and training a neural network for a given task like classification, we are just learning a model that can take a complicated input , transform it successively into something much simpler (the hidden layers or internal representations), still highly \u201ccorrelated\u201d with a target variable. The complicated input is whether a text, an image, or a sound signal etc \u2014 yet readable by humans, but complicated for our computers.", "Those internal representations capture only a part of the signal of the input X. An intuitive way to understand this is to think of convolution neural nets. After a convolutional and a max pooling layers, we get a smaller figure which could not be readable, but has kept the tiny sufficient signal it needed from the source image to still be \u201cassociated\u201d with the target corresponding value.", "With that in mind, we know that our neural nets learn to produce hidden representations (throughout all the layers) that are less \u201ccorrelated\u201d with the input by keeping only the relevant information, and as much correlated with the target output as possible. We want to train our neural nets to learn doing this without back propagation, therefore estimate an objective function to optimize for each hidden layer.", "I am absolutly sure that, at this point, you have a little hint :).I can almost hear you saying :", "\u201cSt\u00e9phane, we just need to quantify and minimize the dependence between hidden representations produced by the neural network and the corresponding input but also quantify and maximize the dependence between the same hidden representations and the output variable. And we can quantify those informations with the Mutual Information measure.\u201d", "You are absolutely right, regarding what the authors of the paper proposed anyway : ). Bravo!", "This method is not new at all. In fact it comes from the Information Theory : the Information Bottleneck principle.", "The authors found the objective function they needed for each hidden layer of the neural network.", "IB is a method introduced by Naftali Tishby, Fernando C. Pereira, and William Bialek.", "Optimizing IB is equivalent to minimizing the mutual information (dependence) between X and the new variable T while maximizing the MI between T and the target variable Y.", "In other words, IB allows us to learn to produce a new variable T which is the least dependent on the source input X (compression) and dependent as much as possible on the target Y.", "Here T can be any of the hidden representations of X, in the neural network.", "The authors introduced an objective function that can be used for each hidden layer. But there is still a remaining problem.", "Let us suppose that you use continuous differentiable activation functions like the sigmoid or the leaky-relu. Also, let\u2019s assume that your input X is a continuous variable (it is generally the case- eg. : your 32x32 image at the entry of your model takes continuous values lying in [0,1]\u00b3\u00b2*\u00b3\u00b2).", "The first assumption leaves us with a deterministic neural network. Consequently, with the second assumption, the hidden representations are just continuous variables absolutely dependent on X. More precisely, T\u1d62=f\u1d62(X), where f\u1d62 is a deterministic function and T\u1d62, the i-th hidden representation of X. We then en up with MI(X, T\u1d62) = \u221e almost always, no matter the initial weights of the neural net.", "How can we optimize a function that is infinite almost always? The problem is ill-posed, MI is not a convenient measure for the actual purpose.", "The authors proposed then to replace MI with another dependence measure tool: HSIC for Hilbert Schmidt Independence Criterion.", "HSIC is just the norm of the cross-covariance matrix between two sets of variables.", "In our case, let\u2019s consider a batch of input data X = (X\u2081, X\u2082, \u2026, X\u2098). Every X\u1d62 is a random variable since it could be any input sample from the dataset. Also, let\u2019s consider the k\u1d57\u02b0 hidden layer obtained from X with our neural net, Z\u1d4f = (Z\u1d4f\u2081, Z\u1d4f\u2082, \u2026, Z\u1d4fm). The cross covariance will just measure the covariance between each variable from X and each variable from Z\u1d4f.", "HSIC can be re-written as the trace of a matrix.", "Let\u2019s break this formula down. K_x or K_z are similarity matrices where each row is the distance vector from an individual to the other individuals of the same variable. Those distances can be computed using a Gaussian kernel parameterized by a length scale \u03c3.When computing K_x.H or K_z.H, we just center the distance vectors of each individual in each variable by subtracting the mean distance of each individual to the others. For example, K_X\u2081 (distance vector from individual 1 to all the others) is substracted from its mean.We finally get a sort of encoding vector representing each element in the two variables. This vector encodes the relation of each individual to the others in the same variable. Computing the above trace is equivalent to summing the inner product between the encodings of the individuals from X and the corresponding individuals from Y.", "This inner product will be large when the relation between each point i of X and all other points of X is similar to the relation between the corresponding point i of Y and all other points of Y , summed over all i, and where similarity is measured through the Gaussian kernel.", "With HSIC we can empirically estimate some dependency between two variables. Despite MI, HSIC won\u2019t explode or be likely to exhibit infinite values. Thus, it can be used as a replacement in our objective function.", "In this section, I am going to introduce the overall method, and how it all comes together.", "The initial goal is to find an alternative to back-propagation. For that, the authors proposed a robust objective method for each hidden layer of the neural network : HSIC Bottleneck.", "The hidden representations Z are parameterized by our neural net weights. Then, when the authors say that we are seeking for the hidden representations (Z) that are less dependent on X and as much dependent on Y as possible, they meant looking for the parameters weights that produce Z given the input X. And since we have an objective function for each hidden layer, we can compute directly the gradients of the weights involved in each layer, with no need of back-propagation.", "We iterate over the training dataset by sampling m input examples along with their targets. Be aware, T\u1d62-1 represents here the weights at layer i-1 of our model; in the previous paragraphs we referred to T as a hidden variable, the output of a hidden layer; it is not the same here. Also, (X\u2c7c, Y\u2c7c) in the algorithm above refers to the j-th batch data.", "The following lines describe the forward pass in our model:", "As we can see, with just a forward pass we have been able to compute an objective function to minimize and directly update the weights with no need of a backward pass.", "In the case of a classification, the last hidden representation dimension will equal the number of classes. The maximum value at a given position will serve to predict the corresponding class. This is the unformatted model.In the formatted model, the authors appended a fully connected and a softmax activation output at the end of the neural network and train this single layer with SGD and no back-propagation. They use a cross-entropy loss to train this layer.", "As we\u2019ve seen before HSIC bottleneck will depend on a hyper-parameter \u03c3. The authors found that the results are really impacted by the choice of this hyper-parameter. To overcome this, concurrent neural nets are learned using HSIC bottleneck with different length scales \u03c3. The final hidden representations of each network are then aggregated (averaging or summation). They further train a single fully connected layer with a softmax output using SGD and a cross entropy loss.", "That\u2019s it! Here is your deep learning framework with no backpropagation for training.", "The authors run a lot of experiments to support the framework they introduced.", "First, they empirically prove that neural networks actually follow the Information Bottleneck principle: neural networks learn to produce hidden representations that are less dependent on the input X by keeping the relevant information and very dependent on the target Y. For that matter, they compute the HSIC between the hidden layers learned and X, and the HSIC between the hidden layers and Y, for a numerous number of neural networks.Here is an example for fully connected neural networks, trained on MNIST dataset.", "As we can see, the further we train our neural network, the more it tends to follow the Information Bottleneck Principle.", "They also experimented unformatted fully connected models, where the last layers are trained with HSIC bottleneck. They experimented such models on CIFAR dataset.", "They finally tested the formatted models with an additional fully connected layer trained using SGD and a cross entropy loss. They found out that those models were converging faster than models trained using back-propagation.", "Is there any deep learning model that is trained nowadays without back-propagation? If it exists, it must be rare. Back-propagation is practically an unavoidable algorithm to compute partial derivatives in a deep neural network. However, this algorithm is costly, often suffers from gradient vanishing, offers no possibility of parallelization. It is in this context that the authors of the paper we studied, propose HSIC-Bottleneck, an alternative to back-propagation. This algorithm relies on the Information Bottleneck principle from the Information Theory. Using this method, the authors were able to propose a robust objective function for each hidden layer of our deep network, consequently the ability to compute the gradients with respect to all the parameters during the forward pass, with no need of propagating the error in a backward pass, with chain rule.", "I found the solution absolutely elegant, and can\u2019t wait for the subsequent works in this direction.", "[2] Michael Nielsen, How the backpropagation algorithm works (Dec 2019)", "[3] Arthur Gretton , Kenji Fukumizu, Choon Hui Teo , Le Song, Bernhard Scholkopf, Alexander J. Smola, A Kernel Statistical Test of Independence 2008, NeurIPS 2008", "[4] Alexander A. Alemi, Ian Fischer, Joshua V. Dillon, Kevin Murphy Deep variational information bottleneck 2017, ICLR 2017", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Engineering Degree in computer science, Leonard de Vinci ; MSc in Applied Mathematics , Ecole Polytechnique Palaiseau; NLP, Computer Vision"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F36e951d4582c&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhsic-bottleneck-an-alternative-to-back-propagation-36e951d4582c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhsic-bottleneck-an-alternative-to-back-propagation-36e951d4582c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhsic-bottleneck-an-alternative-to-back-propagation-36e951d4582c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhsic-bottleneck-an-alternative-to-back-propagation-36e951d4582c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----36e951d4582c--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----36e951d4582c--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@steph1793?source=post_page-----36e951d4582c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@steph1793?source=post_page-----36e951d4582c--------------------------------", "anchor_text": "David St\u00e9phane"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F840493a14b7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhsic-bottleneck-an-alternative-to-back-propagation-36e951d4582c&user=David+St%C3%A9phane&userId=840493a14b7&source=post_page-840493a14b7----36e951d4582c---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F36e951d4582c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhsic-bottleneck-an-alternative-to-back-propagation-36e951d4582c&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F36e951d4582c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhsic-bottleneck-an-alternative-to-back-propagation-36e951d4582c&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@herfrenchness?utm_source=medium&utm_medium=referral", "anchor_text": "Clarisse Croset"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "http://neuralnetworksanddeeplearning.com/chap2.html", "anchor_text": "2"}, {"url": "http://michaelnielsen.org/", "anchor_text": "Michael Nielsen"}, {"url": "https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained", "anchor_text": "Kullback Leibler divergence"}, {"url": "https://www.mdpi.com/2624-6511/2/2/9", "anchor_text": "Source"}, {"url": "https://en.wikipedia.org/wiki/Naftali_Tishby", "anchor_text": "Naftali Tishby"}, {"url": "https://en.wikipedia.org/wiki/William_Bialek", "anchor_text": "William Bialek"}, {"url": "https://www.researchgate.net/publication/265497360_Computational_Intelligence_Techniques_for_Missing_Data_Imputation", "anchor_text": "Source"}, {"url": "https://arxiv.org/pdf/1908.01580.pdf", "anchor_text": "Source"}, {"url": "https://arxiv.org/pdf/1908.01580.pdf", "anchor_text": "Source"}, {"url": "https://arxiv.org/pdf/1908.01580.pdf", "anchor_text": "Source"}, {"url": "https://arxiv.org/pdf/1908.01580.pdf", "anchor_text": "Source"}, {"url": "https://arxiv.org/pdf/1908.01580.pdf", "anchor_text": "Source"}, {"url": "https://arxiv.org/abs/1908.01580v3", "anchor_text": "The HSIC Bottleneck: Deep Learning without Back-Propagation"}, {"url": "http://neuralnetworksanddeeplearning.com/chap2.html", "anchor_text": "How the backpropagation algorithm works"}, {"url": "https://papers.nips.cc/paper/3201-a-kernel-statistical-test-of-independence.pdf", "anchor_text": "A Kernel Statistical Test of Independence"}, {"url": "https://arxiv.org/abs/1612.00410", "anchor_text": "Deep variational information bottleneck"}, {"url": "https://medium.com/tag/backpropagation?source=post_page-----36e951d4582c---------------backpropagation-----------------", "anchor_text": "Backpropagation"}, {"url": "https://medium.com/tag/optimization?source=post_page-----36e951d4582c---------------optimization-----------------", "anchor_text": "Optimization"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----36e951d4582c---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/information-theory?source=post_page-----36e951d4582c---------------information_theory-----------------", "anchor_text": "Information Theory"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F36e951d4582c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhsic-bottleneck-an-alternative-to-back-propagation-36e951d4582c&user=David+St%C3%A9phane&userId=840493a14b7&source=-----36e951d4582c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F36e951d4582c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhsic-bottleneck-an-alternative-to-back-propagation-36e951d4582c&user=David+St%C3%A9phane&userId=840493a14b7&source=-----36e951d4582c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F36e951d4582c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhsic-bottleneck-an-alternative-to-back-propagation-36e951d4582c&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----36e951d4582c--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F36e951d4582c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhsic-bottleneck-an-alternative-to-back-propagation-36e951d4582c&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----36e951d4582c---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----36e951d4582c--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----36e951d4582c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----36e951d4582c--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----36e951d4582c--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----36e951d4582c--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----36e951d4582c--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----36e951d4582c--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----36e951d4582c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@steph1793?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@steph1793?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "David St\u00e9phane"}, {"url": "https://medium.com/@steph1793/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "19 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F840493a14b7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhsic-bottleneck-an-alternative-to-back-propagation-36e951d4582c&user=David+St%C3%A9phane&userId=840493a14b7&source=post_page-840493a14b7--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F6ec07dbd8c5a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhsic-bottleneck-an-alternative-to-back-propagation-36e951d4582c&newsletterV3=840493a14b7&newsletterV3Id=6ec07dbd8c5a&user=David+St%C3%A9phane&userId=840493a14b7&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}