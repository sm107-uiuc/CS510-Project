{"url": "https://towardsdatascience.com/9-promising-applications-of-reinforcement-learning-in-2021-c56f16bc8b79", "time": 1683016821.5500102, "path": "towardsdatascience.com/9-promising-applications-of-reinforcement-learning-in-2021-c56f16bc8b79/", "webpage": {"metadata": {"title": "9 Promising Applications of Reinforcement Learning in 2021 | by mugoh mwaura | Towards Data Science", "h1": "9 Promising Applications of Reinforcement Learning in 2021", "description": "Reinforcement Learning(RL) provides solutions to a sequential decision making problem or a problem that can be re-structured as sequential in nature. Such puzzles do not depend on a single decision\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/@yuxili/rl-applications-73ef685c07eb", "anchor_text": "application success", "paragraph_index": 1}, {"url": "https://sites.google.com/view/RL4RealLife", "anchor_text": "RL for real-life workshop", "paragraph_index": 2}, {"url": "https://en.wikipedia.org/wiki/Collaborative_filtering", "anchor_text": "filtering", "paragraph_index": 4}, {"url": "https://humboldt-wi.github.io/blog/research/applied_predictive_modeling_19/causalrecommendersystem/", "anchor_text": "causality", "paragraph_index": 4}, {"url": "https://towardsdatascience.com/bandits-for-recommender-system-optimization-1d702662346e", "anchor_text": "bandits", "paragraph_index": 4}, {"url": "https://recsys.acm.org/recsys20/", "anchor_text": "RecSys 2020 conference", "paragraph_index": 6}, {"url": "https://www.slideshare.net/justinbasilico/recent-trends-in-personalization-at-netflix", "anchor_text": "presented", "paragraph_index": 6}, {"url": "https://github.com/guyulongcs/Deep-Reinforcement-Learning-for-Recommender-Systems", "anchor_text": "a lot", "paragraph_index": 7}, {"url": "https://towardsdatascience.com/reinforcement-learning-ddpg-and-td3-for-news-recommendation-d3cddec26011", "anchor_text": "news-feed recommendations", "paragraph_index": 7}, {"url": "https://towardsdatascience.com/recommendation-system-with-reinforcement-learning-3362cb4422c8", "anchor_text": "music personalization", "paragraph_index": 7}, {"url": "https://par.nsf.gov/servlets/purl/10111008", "anchor_text": "used", "paragraph_index": 9}, {"url": "https://news.mit.edu/2019/robots-object-manipulation-particle-simulator-0417", "anchor_text": "object manipulation", "paragraph_index": 11}, {"url": "https://openai.com/blog/solving-rubiks-cube/", "anchor_text": "solving Rubiks\u2019 cube", "paragraph_index": 11}, {"url": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.15.5078&rep=rep1&type=pdf", "anchor_text": "Early efforts", "paragraph_index": 12}, {"url": "https://arxiv.org/pdf/2005.13857.pdf", "anchor_text": "parallel training environments", "paragraph_index": 14}, {"url": "https://thegradient.pub/the-promise-of-hierarchical-reinforcement-learning/", "anchor_text": "hierarchical RL", "paragraph_index": 14}, {"url": "https://www.frontiersin.org/articles/10.3389/fnbot.2020.00063/full", "anchor_text": "improved path planning", "paragraph_index": 14}, {"url": "https://arxiv.org/pdf/1703.00420.pdf", "anchor_text": "mapless robot navigation", "paragraph_index": 14}, {"url": "https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwiOrvbdmP_sAhX5RBUIHZMkAawQFjAJegQIAxAC&url=https%3A%2F%2Fwww.mdpi.com%2F2076-3417%2F10%2F18%2F6567%2Fpdf&usg=AOvVaw1Mufvl1J2Bnvw_WTfdAomX", "anchor_text": "advancing missile guidance", "paragraph_index": 16}, {"url": "https://futureoflife.org/open-letter-autonomous-weapons/", "anchor_text": "concerns this might have", "paragraph_index": 16}, {"url": "https://arxiv.org/pdf/1801.05086.pdf", "anchor_text": "navigate without prior knowledge of the environment", "paragraph_index": 19}, {"url": "https://link.springer.com/article/10.1007/s00521-020-05097-x", "anchor_text": "this", "paragraph_index": 19}, {"url": "https://arxiv.org/pdf/1902.09458.pdf", "anchor_text": "long-horizon", "paragraph_index": 19}, {"url": "https://arxiv.org/abs/1910.11432", "anchor_text": "interactive", "paragraph_index": 19}, {"url": "https://en.wikipedia.org/wiki/Computer_cluster#Task_scheduling", "anchor_text": "cluster scheduling", "paragraph_index": 21}, {"url": "https://www.cl.cam.ac.uk/~ey204/pubs/posters/EuroSys_2018_TensorForce.pdf", "anchor_text": "suggests", "paragraph_index": 22}, {"url": "https://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-946.pdf", "anchor_text": "need to decouple the design of RL", "paragraph_index": 23}, {"url": "https://rise.cs.berkeley.edu/blog/sql-query-optimization-meets-deep-reinforcement-learning/", "anchor_text": "drastic speedups for join queries", "paragraph_index": 26}, {"url": "https://arxiv.org/pdf/2007.14244.pdf", "anchor_text": "optimizing database indexing", "paragraph_index": 27}, {"url": "https://dl.acm.org/doi/abs/10.1145/1128022.1128042", "anchor_text": "phase ordering", "paragraph_index": 30}, {"url": "https://arxiv.org/pdf/2008.08951.pdf", "anchor_text": "framable as an RL problem", "paragraph_index": 31}, {"url": "https://cs.lmu.edu/~ray/notes/ir/", "anchor_text": "intermediate representation", "paragraph_index": 31}, {"url": "https://twitter.com/neuralnets4life", "anchor_text": "Jason Gauci", "paragraph_index": 35}, {"url": "https://arxiv.org/abs/1902.06228.pdf", "anchor_text": "RL to minimize passenger waiting time", "paragraph_index": 36}, {"url": "https://www.coursera.org/lecture/advanced-algorithms-and-complexity/bipartite-matching-g81sM", "anchor_text": "bipartite matching", "paragraph_index": 36}, {"url": "https://paperswithcode.com/method/pointer-net", "anchor_text": "Pointer models", "paragraph_index": 38}, {"url": "https://cse.msu.edu/~mayao4/tutorials/aaai2020/", "anchor_text": "graphical neural", "paragraph_index": 38}, {"url": "https://towardsdatascience.com/a-gentle-introduction-to-graph-neural-network-basics-deepwalk-and-graphsage-db5d540d50b3", "anchor_text": "networks", "paragraph_index": 38}, {"url": "https://ai.googleblog.com/2020/04/chip-design-with-deep-reinforcement.html", "anchor_text": "designing computer chips with deep RL", "paragraph_index": 40}, {"url": "https://deepmind.com/research/publications/An-empirical-investigation-of-the-challenges-of-real-world-reinforcement-learning", "anchor_text": "challenges that address it", "paragraph_index": 42}, {"url": "https://medium.com/@sergey.levine/decisions-from-data-how-offline-reinforcement-learning-will-change-how-we-use-ml-24d98cb069b0", "anchor_text": "offline RL", "paragraph_index": 42}], "all_paragraphs": ["Reinforcement Learning(RL) provides solutions to a sequential decision making problem or a problem that can be re-structured as sequential in nature. Such puzzles do not depend on a single decision made at a certain point in time but on an entire sequence of trailing choices \u2014 an example of this is treatment procedures in healthcare.", "It is desirable to run RL systems in the real world and have real benefits. This is something that has seen growing application success, but with its own set of challenges.", "This post explores exciting applications of RL in the real world that promise to give beneficial use cases, as discussed in this year\u2019s RL for real-life workshop. Most uses might receive more attention, not just in the coming days, but onwards. This list might be subjective to the research areas of the participants in the discussion. But most of the applications, as of now, are not trendy. For each application, I will identify which new solution approach RL offers, an example of how it\u2019s being done, and where necessary, what is unexplored.", "New and current focus areas of Reinforcement Learning in the real world.", "Recommendations help personalize a user\u2019s preferences. In music and video platforms, for example, recommendations help members find entertainment to engage with and enjoy while maximizing satisfaction and user retention. Recommender systems (RecSys) have, for a long time, been used to predict the next item of interest to a user. RecSys approaches like filtering, causality or bandits may have the ability to remember historical experiences from user interactions and be able to play them back when needed. But they are restricted in the sense that they cannot plan.", "\u201cA good recommender system would be able to curve a true learning curriculum for [anyone trying to learn a skill] over time\u201d Ed Chi, Google Brain", "This need for the ability to plan is a motivation for using RL in recommender systems. The key benefit of RL is that it personalises for the user to maximize long term satisfaction. In the recent RecSys 2020 conference, Netflix presented RL as a recent trend in their personalization, where they use it to optimize varied user interactions. These optimizations happen in a user session and for user interactions across multiple device sessions.", "There is a lot of recent research involving RL in recommender systems, and this seems to be an area that started receiving focus in 2018. It now extends to include news-feed recommendations and music personalization.", "While a recommender system shows the user engaging content upon a form of a user request, a notification system invites the user to engage with such content every time it\u2019s newly available or is updated.", "However, push notifications can be irritating to users when sent at inappropriate times. So can promotional emails. A desirable notification system should determine which users get notified and when to increase positive user-engagement. RL has been used to determine the right time to send a notification to a given user. When successful, that minimizes ignoring of user-notifications and creates positive user satisfaction.", "We can also reframe this as using RL to predict a user\u2019s behaviour towards a notification, the intent being to minimize the negative impact on users such as a decrease in current task performance.", "Much focus on reinforcement learning in robots has been in object manipulation, which has given rise to robots with some impressive capabilities such as solving Rubiks\u2019 cube. It helps robots learn to handle and move objects around effectively.", "However, deep RL hasn\u2019t had as much application in continuous control of real mobile robots. Early efforts used Q-learning, which could only handle discrete state and action spaces. In contrast, robots have continuous spaces \u2014 as an example, rotating an arm will involve many degrees of freedom.", "With more interest arising in using RL for mobile robot systems, we will have more robots with the ability to move and interact with the world, collecting diverse experiences and generalizing better to different observations or datasets.", "The progress in RL for real-world mobile robots has recently had inputs such as the use of parallel training environments, combining hierarchical RL with environment feature extractors for improved path planning, and mapless robot navigation.", "Another promising area is the use of RL in real-world mobile robot swarms.", "I was pretty surprised to see RL is openly being studied in advancing missile guidance, given the concerns this might have, as expressed through The Future of Life Institute.", "That said, there have been applications of RL in un-manned aerial vehicles (UAV) and flight control. But flight systems such as drones used for data collection, surveillance and monitoring, have mostly been task-specific.", "RL helps flight agents find efficient routes which are needed from the constraints laid on task-time and battery life, and also perform long-range navigation planning.", "Success in UAVs and flight control will have impactful benefits in search and rescue operations, identification of forest fires, gas leaks, or even in food and parcel deliveries. So far, quadrotors can navigate without prior knowledge of the environment model, and this recent work shows that deep RL can be coupled with memory and incremental curriculum learning for quick navigation in safety-critical situations. Hierarchical RL has proven useful in long-horizon navigation and interactive navigation i.e., navigation that involves object manipulation at some point, and is likely to be used more in improving navigation control techniques.", "RL approaches have inherent ability to control dynamic behaviour using raw system performance. However, practical applications in computer systems have been elusive due to large data requirements. It\u2019s an area which offers the potential possibility of collecting a lot of data and would benefit from the adoption of standard tools for model evaluation.", "Systems have a wide variety of parameter configurations needed to customize aspects that influence computer performance. These include cluster scheduling for jobs with different dependencies, adaptive video streaming which determines the optimal bitrate for network bandwidth and cache cleaning.", "These parameters get determined through manual implementations, community recommendations, and of late, autotuning techniques like Bayesian optimization. The issue with these procedures is they produce static parameter configurations offline. In contrast, RL suggests matching different configurations to distinct tasks online, based on continuous monitoring and adaptive control. Remember such an agent needs to make decisions indefinitely with no terminal state.", "Schaarschmidt identifies a need to decouple the design of RL from the execution process to make the execution more flexible in different distribution systems.", "Imagine computer systems that can continuously self-configure \u2014 it would mean computers that work better for us.", "Database queries and configurations are representable as Markov Decision Processes (MDPs) that optimize database access. For example, in the ordering of join queries, we can frame the MDP as:", "Deep RL has approaches to tailor the query plan to a specific dataset, the query workload and observed access cost. The cost might be expressed as the total runtime or the number of queries required to meet an objective. Deep Q-learning has achieved drastic speedups for join queries involving large amounts of data compared to dynamic programming access methods.", "In database optimization, RL has plenty of room for evaluation in big data technologies like Apache Spark, optimizing database indexing, including predicting queries to index ahead of time. To save on resources, it\u2019s also desirable to have agents that can efficiently scale to large database systems when trained in less resource-consuming data environments.", "Why would anyone want to use Machine Learning in compilers? While translating code to binary, correctness is critical.", "Still, there\u2019s the aspect of optimization. Besides translating code correctly, compilers need to find the most efficient translation. The vast majority of engineering focus has been on this second objective of performance. Computer architecture has been evolving at a fast pace, each generation having its compilation quirks. And instead of reliance on heuristics, Machine Learning is suited to make code optimization decisions where the performance impact is reliant on the underlying platform.", "Compilers these days let you specify an optimization level on a build. What this does is trigger a sequence of passes on the code. But these passes are initialized with some pre-fixed parameters and run in a pre-defined order. The order of applying such compiler optimizations to code, known as phase ordering, is a long-standing problem in compiler research, for each of those optimization phases interacts with the intermediate code in complex ways.", "It\u2019s framable as an RL problem aiming to optimize for the compiler optimization strategy tailored to a specific intermediate representation(IR). The states get represented by the current code IR and history of optimization phases. The reward depends on the runtime of the new IR compared to the previous state IR.", "\u201ca reduction in runtime produces a positive reward for that phase while an increase produces a negative one\u201d", "From exploration, the agent learns to assign the correct value to the optimization strategies.", "The future of RL in compiler optimization will also involve agents that can meta-learn or do self-supervised learning in new target systems. That would save on computational resources as the current design requires programs to be compiled, and benchmarked for performance on unseen systems.", "If we are at a conference with lots of people, and all search \u201crestaurant\u201d in our favourite review app, we will get the restaurant with the best reviews, which works well for us \u2014 the hungry, but not so much for the restaurant. (This is an example given by Jason Gauci). What will have happened is the app has optimized for just one side of the market \u2014 the customer, and not for the restaurant.", "This form of decision making involving two parties, who want a solution optimizing for both of their needs (almost) equally, is a factor in many market places. Take, for example, planning to hitch a bodaboda ride to avoid traffic and get somewhere quickly, but the rider also wants to save on fuel and economize. Such an optimization framework in ride-sourcing services can use RL to minimize passenger waiting time while promoting driver utility. Such is a bipartite matching problem, on which graph matching is the prevalent approach.", "Application of RL for market optimization problems is scalable to services like accommodation sharing between tourists and locals, where we desire to optimize for the preferences of both the host and the tourist. Any market setting involving this bipartite balance between two parties stands to benefit from the ability of RL to plan and make decisions. And many markets in the hospitality and service industries are of this nature.", "There is potential for RL to drive further scientific discoveries. Quite interesting. For example, vaccine discovery and optimal sequencing in biology. RL applies to sequence decoding. Protein sequence design problems comprise a sequence encoding bar and a decoding bar. By use of an RL policy, it\u2019s possible to control the decoding. Pointer models and graphical neural networks are some of the approaches that get prevalent use in sequence design problems.", "Gene sequencing needs a good understanding of chemical building blocks, which might not easily be represented by a neural network.", "Most scientific problems that involve planning and optimizing of production procedures can benefit from reinforcement learning. For example, designing computer chips with deep RL has been used to optimize placements for accelerator chips in TPUs at Google.", "With these kinds of problems, success in one gives insight that might prove valuable in progressing the other. A naive example is in the mentioned chip design RL solution, where the goal is to place blocks on the computer chip optimally. Though with added difficulty, the approach used can have some transferrable aspects to the protein sequencing problem, which also involves optimizing for the placement of (building) blocks.", "The contribution towards making reinforcement learning beneficial to more real-world systems has seen growth. If we are to see more RL agents deployed in our everyday systems, it\u2019s critical to find viable ways around the challenges that address it. Exploring emergent, potentially fruitful for real-world use, but unsolved subfields, like offline RL, will also help make significant steps in the widespread use of RL for decision making and planning. We see more in the next decade.", "Fascinated by bread | Learning to smile | RL & Decision control"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fc56f16bc8b79&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F9-promising-applications-of-reinforcement-learning-in-2021-c56f16bc8b79&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2F9-promising-applications-of-reinforcement-learning-in-2021-c56f16bc8b79&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F9-promising-applications-of-reinforcement-learning-in-2021-c56f16bc8b79&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2F9-promising-applications-of-reinforcement-learning-in-2021-c56f16bc8b79&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/@mugoh?source=post_page-----c56f16bc8b79--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c56f16bc8b79--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@mugoh?source=post_page-----c56f16bc8b79--------------------------------", "anchor_text": "mugoh mwaura"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1bcb39f5cedf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F9-promising-applications-of-reinforcement-learning-in-2021-c56f16bc8b79&user=mugoh+mwaura&userId=1bcb39f5cedf&source=post_page-1bcb39f5cedf----c56f16bc8b79---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----c56f16bc8b79--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc56f16bc8b79&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F9-promising-applications-of-reinforcement-learning-in-2021-c56f16bc8b79&user=mugoh+mwaura&userId=1bcb39f5cedf&source=-----c56f16bc8b79---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc56f16bc8b79&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F9-promising-applications-of-reinforcement-learning-in-2021-c56f16bc8b79&source=-----c56f16bc8b79---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://unsplash.com/@hudsoncrafted", "anchor_text": "Becky Hudson"}, {"url": "https://unsplash.com/photos/qlUfwjkA5pg", "anchor_text": "Unsplash"}, {"url": "https://medium.com/@yuxili/rl-applications-73ef685c07eb", "anchor_text": "application success"}, {"url": "https://sites.google.com/view/RL4RealLife", "anchor_text": "RL for real-life workshop"}, {"url": "https://en.wikipedia.org/wiki/Collaborative_filtering", "anchor_text": "filtering"}, {"url": "https://humboldt-wi.github.io/blog/research/applied_predictive_modeling_19/causalrecommendersystem/", "anchor_text": "causality"}, {"url": "https://towardsdatascience.com/bandits-for-recommender-system-optimization-1d702662346e", "anchor_text": "bandits"}, {"url": "https://recsys.acm.org/recsys20/", "anchor_text": "RecSys 2020 conference"}, {"url": "https://www.slideshare.net/justinbasilico/recent-trends-in-personalization-at-netflix", "anchor_text": "presented"}, {"url": "https://github.com/guyulongcs/Deep-Reinforcement-Learning-for-Recommender-Systems", "anchor_text": "a lot"}, {"url": "https://towardsdatascience.com/reinforcement-learning-ddpg-and-td3-for-news-recommendation-d3cddec26011", "anchor_text": "news-feed recommendations"}, {"url": "https://towardsdatascience.com/recommendation-system-with-reinforcement-learning-3362cb4422c8", "anchor_text": "music personalization"}, {"url": "https://par.nsf.gov/servlets/purl/10111008", "anchor_text": "used"}, {"url": "https://news.mit.edu/2019/robots-object-manipulation-particle-simulator-0417", "anchor_text": "object manipulation"}, {"url": "https://openai.com/blog/solving-rubiks-cube/", "anchor_text": "solving Rubiks\u2019 cube"}, {"url": "https://unsplash.com/@jamesponddotco", "anchor_text": "James Pond"}, {"url": "https://unsplash.com/photos/1qkyck-UL3g", "anchor_text": "Unsplash"}, {"url": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.15.5078&rep=rep1&type=pdf", "anchor_text": "Early efforts"}, {"url": "https://arxiv.org/pdf/2005.13857.pdf", "anchor_text": "parallel training environments"}, {"url": "https://thegradient.pub/the-promise-of-hierarchical-reinforcement-learning/", "anchor_text": "hierarchical RL"}, {"url": "https://www.frontiersin.org/articles/10.3389/fnbot.2020.00063/full", "anchor_text": "improved path planning"}, {"url": "https://arxiv.org/pdf/1703.00420.pdf", "anchor_text": "mapless robot navigation"}, {"url": "https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwiOrvbdmP_sAhX5RBUIHZMkAawQFjAJegQIAxAC&url=https%3A%2F%2Fwww.mdpi.com%2F2076-3417%2F10%2F18%2F6567%2Fpdf&usg=AOvVaw1Mufvl1J2Bnvw_WTfdAomX", "anchor_text": "advancing missile guidance"}, {"url": "https://futureoflife.org/open-letter-autonomous-weapons/", "anchor_text": "concerns this might have"}, {"url": "https://arxiv.org/pdf/1801.05086.pdf", "anchor_text": "navigate without prior knowledge of the environment"}, {"url": "https://link.springer.com/article/10.1007/s00521-020-05097-x", "anchor_text": "this"}, {"url": "https://arxiv.org/pdf/1902.09458.pdf", "anchor_text": "long-horizon"}, {"url": "https://arxiv.org/abs/1910.11432", "anchor_text": "interactive"}, {"url": "https://en.wikipedia.org/wiki/Computer_cluster#Task_scheduling", "anchor_text": "cluster scheduling"}, {"url": "https://slideslive.com/38922484/reinforcement-learning-for-continual-control-in-computer-systems", "anchor_text": "source"}, {"url": "https://www.cl.cam.ac.uk/~ey204/pubs/posters/EuroSys_2018_TensorForce.pdf", "anchor_text": "suggests"}, {"url": "https://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-946.pdf", "anchor_text": "need to decouple the design of RL"}, {"url": "https://rise.cs.berkeley.edu/blog/sql-query-optimization-meets-deep-reinforcement-learning/", "anchor_text": "drastic speedups for join queries"}, {"url": "https://arxiv.org/pdf/2007.14244.pdf", "anchor_text": "optimizing database indexing"}, {"url": "https://unsplash.com/@_louisreed", "anchor_text": "Louis Reed"}, {"url": "https://unsplash.com/photos/wSTCaQpiLtc", "anchor_text": "Unsplash"}, {"url": "https://dl.acm.org/doi/abs/10.1145/1128022.1128042", "anchor_text": "phase ordering"}, {"url": "https://arxiv.org/pdf/2008.08951.pdf", "anchor_text": "framable as an RL problem"}, {"url": "https://cs.lmu.edu/~ray/notes/ir/", "anchor_text": "intermediate representation"}, {"url": "https://twitter.com/neuralnets4life", "anchor_text": "Jason Gauci"}, {"url": "https://arxiv.org/abs/1902.06228.pdf", "anchor_text": "RL to minimize passenger waiting time"}, {"url": "https://www.coursera.org/lecture/advanced-algorithms-and-complexity/bipartite-matching-g81sM", "anchor_text": "bipartite matching"}, {"url": "https://unsplash.com/@hugoramos", "anchor_text": "Hugo Ramos"}, {"url": "https://unsplash.com/photos/lo0wIu1hPWc", "anchor_text": "Unsplash"}, {"url": "https://paperswithcode.com/method/pointer-net", "anchor_text": "Pointer models"}, {"url": "https://cse.msu.edu/~mayao4/tutorials/aaai2020/", "anchor_text": "graphical neural"}, {"url": "https://towardsdatascience.com/a-gentle-introduction-to-graph-neural-network-basics-deepwalk-and-graphsage-db5d540d50b3", "anchor_text": "networks"}, {"url": "https://ai.googleblog.com/2020/04/chip-design-with-deep-reinforcement.html", "anchor_text": "designing computer chips with deep RL"}, {"url": "https://deepmind.com/research/publications/An-empirical-investigation-of-the-challenges-of-real-world-reinforcement-learning", "anchor_text": "challenges that address it"}, {"url": "https://medium.com/@sergey.levine/decisions-from-data-how-offline-reinforcement-learning-will-change-how-we-use-ml-24d98cb069b0", "anchor_text": "offline RL"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----c56f16bc8b79---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----c56f16bc8b79---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----c56f16bc8b79---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/ai?source=post_page-----c56f16bc8b79---------------ai-----------------", "anchor_text": "AI"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc56f16bc8b79&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F9-promising-applications-of-reinforcement-learning-in-2021-c56f16bc8b79&user=mugoh+mwaura&userId=1bcb39f5cedf&source=-----c56f16bc8b79---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc56f16bc8b79&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F9-promising-applications-of-reinforcement-learning-in-2021-c56f16bc8b79&user=mugoh+mwaura&userId=1bcb39f5cedf&source=-----c56f16bc8b79---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc56f16bc8b79&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F9-promising-applications-of-reinforcement-learning-in-2021-c56f16bc8b79&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/@mugoh?source=post_page-----c56f16bc8b79--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c56f16bc8b79--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1bcb39f5cedf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F9-promising-applications-of-reinforcement-learning-in-2021-c56f16bc8b79&user=mugoh+mwaura&userId=1bcb39f5cedf&source=post_page-1bcb39f5cedf----c56f16bc8b79---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F1bcb39f5cedf%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F9-promising-applications-of-reinforcement-learning-in-2021-c56f16bc8b79&user=mugoh+mwaura&userId=1bcb39f5cedf&source=-----c56f16bc8b79---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/@mugoh?source=post_page-----c56f16bc8b79--------------------------------", "anchor_text": "Written by mugoh mwaura"}, {"url": "https://medium.com/@mugoh/followers?source=post_page-----c56f16bc8b79--------------------------------", "anchor_text": "43 Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----c56f16bc8b79--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1bcb39f5cedf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F9-promising-applications-of-reinforcement-learning-in-2021-c56f16bc8b79&user=mugoh+mwaura&userId=1bcb39f5cedf&source=post_page-1bcb39f5cedf----c56f16bc8b79---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F1bcb39f5cedf%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F9-promising-applications-of-reinforcement-learning-in-2021-c56f16bc8b79&user=mugoh+mwaura&userId=1bcb39f5cedf&source=-----c56f16bc8b79---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/so-model-predictive-control-is-this-enticing-for-model-based-rl-e69bb5255ce9?source=author_recirc-----c56f16bc8b79----0---------------------59e9887b_2003_4846_bead_8c4b5a310c2d-------", "anchor_text": ""}, {"url": "https://medium.com/@mugoh?source=author_recirc-----c56f16bc8b79----0---------------------59e9887b_2003_4846_bead_8c4b5a310c2d-------", "anchor_text": ""}, {"url": "https://medium.com/@mugoh?source=author_recirc-----c56f16bc8b79----0---------------------59e9887b_2003_4846_bead_8c4b5a310c2d-------", "anchor_text": "mugoh mwaura"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----c56f16bc8b79----0---------------------59e9887b_2003_4846_bead_8c4b5a310c2d-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/so-model-predictive-control-is-this-enticing-for-model-based-rl-e69bb5255ce9?source=author_recirc-----c56f16bc8b79----0---------------------59e9887b_2003_4846_bead_8c4b5a310c2d-------", "anchor_text": "So Model Predictive Control is this Enticing for Model-Based RL?A colored view of how Model Predictive Control Completes any Model-Based Reinforcement Learning Picture"}, {"url": "https://towardsdatascience.com/so-model-predictive-control-is-this-enticing-for-model-based-rl-e69bb5255ce9?source=author_recirc-----c56f16bc8b79----0---------------------59e9887b_2003_4846_bead_8c4b5a310c2d-------", "anchor_text": "\u00b79 min read\u00b7Jun 14, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe69bb5255ce9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fso-model-predictive-control-is-this-enticing-for-model-based-rl-e69bb5255ce9&user=mugoh+mwaura&userId=1bcb39f5cedf&source=-----e69bb5255ce9----0-----------------clap_footer----59e9887b_2003_4846_bead_8c4b5a310c2d-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/so-model-predictive-control-is-this-enticing-for-model-based-rl-e69bb5255ce9?source=author_recirc-----c56f16bc8b79----0---------------------59e9887b_2003_4846_bead_8c4b5a310c2d-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe69bb5255ce9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fso-model-predictive-control-is-this-enticing-for-model-based-rl-e69bb5255ce9&source=-----c56f16bc8b79----0-----------------bookmark_preview----59e9887b_2003_4846_bead_8c4b5a310c2d-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----c56f16bc8b79----1---------------------59e9887b_2003_4846_bead_8c4b5a310c2d-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----c56f16bc8b79----1---------------------59e9887b_2003_4846_bead_8c4b5a310c2d-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----c56f16bc8b79----1---------------------59e9887b_2003_4846_bead_8c4b5a310c2d-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----c56f16bc8b79----1---------------------59e9887b_2003_4846_bead_8c4b5a310c2d-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----c56f16bc8b79----1---------------------59e9887b_2003_4846_bead_8c4b5a310c2d-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----c56f16bc8b79----1---------------------59e9887b_2003_4846_bead_8c4b5a310c2d-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----59e9887b_2003_4846_bead_8c4b5a310c2d-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----c56f16bc8b79----1---------------------59e9887b_2003_4846_bead_8c4b5a310c2d-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----c56f16bc8b79----1-----------------bookmark_preview----59e9887b_2003_4846_bead_8c4b5a310c2d-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----c56f16bc8b79----2---------------------59e9887b_2003_4846_bead_8c4b5a310c2d-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----c56f16bc8b79----2---------------------59e9887b_2003_4846_bead_8c4b5a310c2d-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----c56f16bc8b79----2---------------------59e9887b_2003_4846_bead_8c4b5a310c2d-------", "anchor_text": "Jacob Marks, Ph.D."}, {"url": "https://towardsdatascience.com/?source=author_recirc-----c56f16bc8b79----2---------------------59e9887b_2003_4846_bead_8c4b5a310c2d-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----c56f16bc8b79----2---------------------59e9887b_2003_4846_bead_8c4b5a310c2d-------", "anchor_text": "How I Turned My Company\u2019s Docs into a Searchable Database with OpenAIAnd how you can do the same with your docs"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----c56f16bc8b79----2---------------------59e9887b_2003_4846_bead_8c4b5a310c2d-------", "anchor_text": "15 min read\u00b7Apr 25"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&user=Jacob+Marks%2C+Ph.D.&userId=f7dc0c0eae92&source=-----4f2d34bd8736----2-----------------clap_footer----59e9887b_2003_4846_bead_8c4b5a310c2d-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----c56f16bc8b79----2---------------------59e9887b_2003_4846_bead_8c4b5a310c2d-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "20"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&source=-----c56f16bc8b79----2-----------------bookmark_preview----59e9887b_2003_4846_bead_8c4b5a310c2d-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/uncertainty-aware-reinforcement-learning-c95c25c220d3?source=author_recirc-----c56f16bc8b79----3---------------------59e9887b_2003_4846_bead_8c4b5a310c2d-------", "anchor_text": ""}, {"url": "https://medium.com/@mugoh?source=author_recirc-----c56f16bc8b79----3---------------------59e9887b_2003_4846_bead_8c4b5a310c2d-------", "anchor_text": ""}, {"url": "https://medium.com/@mugoh?source=author_recirc-----c56f16bc8b79----3---------------------59e9887b_2003_4846_bead_8c4b5a310c2d-------", "anchor_text": "mugoh mwaura"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----c56f16bc8b79----3---------------------59e9887b_2003_4846_bead_8c4b5a310c2d-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/uncertainty-aware-reinforcement-learning-c95c25c220d3?source=author_recirc-----c56f16bc8b79----3---------------------59e9887b_2003_4846_bead_8c4b5a310c2d-------", "anchor_text": "Uncertainty Aware Reinforcement LearningA peek into building agents that know what they are doing"}, {"url": "https://towardsdatascience.com/uncertainty-aware-reinforcement-learning-c95c25c220d3?source=author_recirc-----c56f16bc8b79----3---------------------59e9887b_2003_4846_bead_8c4b5a310c2d-------", "anchor_text": "\u00b714 min read\u00b7Jul 5, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc95c25c220d3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funcertainty-aware-reinforcement-learning-c95c25c220d3&user=mugoh+mwaura&userId=1bcb39f5cedf&source=-----c95c25c220d3----3-----------------clap_footer----59e9887b_2003_4846_bead_8c4b5a310c2d-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/uncertainty-aware-reinforcement-learning-c95c25c220d3?source=author_recirc-----c56f16bc8b79----3---------------------59e9887b_2003_4846_bead_8c4b5a310c2d-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc95c25c220d3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funcertainty-aware-reinforcement-learning-c95c25c220d3&source=-----c56f16bc8b79----3-----------------bookmark_preview----59e9887b_2003_4846_bead_8c4b5a310c2d-------", "anchor_text": ""}, {"url": "https://medium.com/@mugoh?source=post_page-----c56f16bc8b79--------------------------------", "anchor_text": "See all from mugoh mwaura"}, {"url": "https://towardsdatascience.com/?source=post_page-----c56f16bc8b79--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----c56f16bc8b79----0---------------------ae4a274b_46ff_43a1_9a31_179ea07cd93b-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData?source=read_next_recirc-----c56f16bc8b79----0---------------------ae4a274b_46ff_43a1_9a31_179ea07cd93b-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData?source=read_next_recirc-----c56f16bc8b79----0---------------------ae4a274b_46ff_43a1_9a31_179ea07cd93b-------", "anchor_text": "Andrew Austin"}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----c56f16bc8b79----0---------------------ae4a274b_46ff_43a1_9a31_179ea07cd93b-------", "anchor_text": "AI Anyone Can Understand Part 1: Reinforcement LearningReinforcement learning is a way for machines to learn by trying different things and seeing what works best. For example, a robot could\u2026"}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----c56f16bc8b79----0---------------------ae4a274b_46ff_43a1_9a31_179ea07cd93b-------", "anchor_text": "\u00b74 min read\u00b7Dec 11, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F6c3b3d623a2d&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40MoneyAndData%2Fai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d&user=Andrew+Austin&userId=42d388912d13&source=-----6c3b3d623a2d----0-----------------clap_footer----ae4a274b_46ff_43a1_9a31_179ea07cd93b-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----c56f16bc8b79----0---------------------ae4a274b_46ff_43a1_9a31_179ea07cd93b-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6c3b3d623a2d&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40MoneyAndData%2Fai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d&source=-----c56f16bc8b79----0-----------------bookmark_preview----ae4a274b_46ff_43a1_9a31_179ea07cd93b-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----c56f16bc8b79----1---------------------ae4a274b_46ff_43a1_9a31_179ea07cd93b-------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=read_next_recirc-----c56f16bc8b79----1---------------------ae4a274b_46ff_43a1_9a31_179ea07cd93b-------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=read_next_recirc-----c56f16bc8b79----1---------------------ae4a274b_46ff_43a1_9a31_179ea07cd93b-------", "anchor_text": "Wouter van Heeswijk, PhD"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----c56f16bc8b79----1---------------------ae4a274b_46ff_43a1_9a31_179ea07cd93b-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----c56f16bc8b79----1---------------------ae4a274b_46ff_43a1_9a31_179ea07cd93b-------", "anchor_text": "Proximal Policy Optimization (PPO) ExplainedThe journey from REINFORCE to the go-to algorithm in continuous control"}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----c56f16bc8b79----1---------------------ae4a274b_46ff_43a1_9a31_179ea07cd93b-------", "anchor_text": "\u00b713 min read\u00b7Nov 29, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fabed1952457b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fproximal-policy-optimization-ppo-explained-abed1952457b&user=Wouter+van+Heeswijk%2C+PhD&userId=33f45c9ab481&source=-----abed1952457b----1-----------------clap_footer----ae4a274b_46ff_43a1_9a31_179ea07cd93b-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----c56f16bc8b79----1---------------------ae4a274b_46ff_43a1_9a31_179ea07cd93b-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fabed1952457b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fproximal-policy-optimization-ppo-explained-abed1952457b&source=-----c56f16bc8b79----1-----------------bookmark_preview----ae4a274b_46ff_43a1_9a31_179ea07cd93b-------", "anchor_text": ""}, {"url": "https://medium.datadriveninvestor.com/finrl-meta-market-environments-and-benchmarks-for-data-driven-financial-reinforcement-learning-7af8e747c4bd?source=read_next_recirc-----c56f16bc8b79----0---------------------ae4a274b_46ff_43a1_9a31_179ea07cd93b-------", "anchor_text": ""}, {"url": "https://byfintech.medium.com/?source=read_next_recirc-----c56f16bc8b79----0---------------------ae4a274b_46ff_43a1_9a31_179ea07cd93b-------", "anchor_text": ""}, {"url": "https://byfintech.medium.com/?source=read_next_recirc-----c56f16bc8b79----0---------------------ae4a274b_46ff_43a1_9a31_179ea07cd93b-------", "anchor_text": "Bruce Yang ByFinTech"}, {"url": "https://medium.datadriveninvestor.com/?source=read_next_recirc-----c56f16bc8b79----0---------------------ae4a274b_46ff_43a1_9a31_179ea07cd93b-------", "anchor_text": "DataDrivenInvestor"}, {"url": "https://medium.datadriveninvestor.com/finrl-meta-market-environments-and-benchmarks-for-data-driven-financial-reinforcement-learning-7af8e747c4bd?source=read_next_recirc-----c56f16bc8b79----0---------------------ae4a274b_46ff_43a1_9a31_179ea07cd93b-------", "anchor_text": "FinRL-Meta: Market Environments and Benchmarks for Data-Driven Financial Reinforcement LearningNeurIPS 2022 Datasets and Benchmarks."}, {"url": "https://medium.datadriveninvestor.com/finrl-meta-market-environments-and-benchmarks-for-data-driven-financial-reinforcement-learning-7af8e747c4bd?source=read_next_recirc-----c56f16bc8b79----0---------------------ae4a274b_46ff_43a1_9a31_179ea07cd93b-------", "anchor_text": "\u00b79 min read\u00b7Nov 13, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fdatadriveninvestor%2F7af8e747c4bd&operation=register&redirect=https%3A%2F%2Fmedium.datadriveninvestor.com%2Ffinrl-meta-market-environments-and-benchmarks-for-data-driven-financial-reinforcement-learning-7af8e747c4bd&user=Bruce+Yang+ByFinTech&userId=a878fc45fb3f&source=-----7af8e747c4bd----0-----------------clap_footer----ae4a274b_46ff_43a1_9a31_179ea07cd93b-------", "anchor_text": ""}, {"url": "https://medium.datadriveninvestor.com/finrl-meta-market-environments-and-benchmarks-for-data-driven-financial-reinforcement-learning-7af8e747c4bd?source=read_next_recirc-----c56f16bc8b79----0---------------------ae4a274b_46ff_43a1_9a31_179ea07cd93b-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7af8e747c4bd&operation=register&redirect=https%3A%2F%2Fmedium.datadriveninvestor.com%2Ffinrl-meta-market-environments-and-benchmarks-for-data-driven-financial-reinforcement-learning-7af8e747c4bd&source=-----c56f16bc8b79----0-----------------bookmark_preview----ae4a274b_46ff_43a1_9a31_179ea07cd93b-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----c56f16bc8b79----1---------------------ae4a274b_46ff_43a1_9a31_179ea07cd93b-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----c56f16bc8b79----1---------------------ae4a274b_46ff_43a1_9a31_179ea07cd93b-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----c56f16bc8b79----1---------------------ae4a274b_46ff_43a1_9a31_179ea07cd93b-------", "anchor_text": "The PyCoach"}, {"url": "https://artificialcorner.com/?source=read_next_recirc-----c56f16bc8b79----1---------------------ae4a274b_46ff_43a1_9a31_179ea07cd93b-------", "anchor_text": "Artificial Corner"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----c56f16bc8b79----1---------------------ae4a274b_46ff_43a1_9a31_179ea07cd93b-------", "anchor_text": "You\u2019re Using ChatGPT Wrong! Here\u2019s How to Be Ahead of 99% of ChatGPT UsersMaster ChatGPT by learning prompt engineering."}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----c56f16bc8b79----1---------------------ae4a274b_46ff_43a1_9a31_179ea07cd93b-------", "anchor_text": "\u00b77 min read\u00b7Mar 17"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fartificial-corner%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&user=The+PyCoach&userId=fb44e21903f3&source=-----886a50dabc54----1-----------------clap_footer----ae4a274b_46ff_43a1_9a31_179ea07cd93b-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----c56f16bc8b79----1---------------------ae4a274b_46ff_43a1_9a31_179ea07cd93b-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "276"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&source=-----c56f16bc8b79----1-----------------bookmark_preview----ae4a274b_46ff_43a1_9a31_179ea07cd93b-------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----c56f16bc8b79----2---------------------ae4a274b_46ff_43a1_9a31_179ea07cd93b-------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk?source=read_next_recirc-----c56f16bc8b79----2---------------------ae4a274b_46ff_43a1_9a31_179ea07cd93b-------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk?source=read_next_recirc-----c56f16bc8b79----2---------------------ae4a274b_46ff_43a1_9a31_179ea07cd93b-------", "anchor_text": "Steve Roberts"}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----c56f16bc8b79----2---------------------ae4a274b_46ff_43a1_9a31_179ea07cd93b-------", "anchor_text": "State Values and Policy Evaluation in 5 minutesAn Introduction to Reinforcement Learning"}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----c56f16bc8b79----2---------------------ae4a274b_46ff_43a1_9a31_179ea07cd93b-------", "anchor_text": "\u00b75 min read\u00b7Jan 11"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2Ff3e00f3c1a50&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40tinkertytonk%2Fstate-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50&user=Steve+Roberts&userId=6b6735266652&source=-----f3e00f3c1a50----2-----------------clap_footer----ae4a274b_46ff_43a1_9a31_179ea07cd93b-------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----c56f16bc8b79----2---------------------ae4a274b_46ff_43a1_9a31_179ea07cd93b-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff3e00f3c1a50&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40tinkertytonk%2Fstate-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50&source=-----c56f16bc8b79----2-----------------bookmark_preview----ae4a274b_46ff_43a1_9a31_179ea07cd93b-------", "anchor_text": ""}, {"url": "https://medium.com/@anandmishra.kanha/deep-reinforcement-learning-current-state-of-art-383190b14464?source=read_next_recirc-----c56f16bc8b79----3---------------------ae4a274b_46ff_43a1_9a31_179ea07cd93b-------", "anchor_text": ""}, {"url": "https://medium.com/@anandmishra.kanha?source=read_next_recirc-----c56f16bc8b79----3---------------------ae4a274b_46ff_43a1_9a31_179ea07cd93b-------", "anchor_text": ""}, {"url": "https://medium.com/@anandmishra.kanha?source=read_next_recirc-----c56f16bc8b79----3---------------------ae4a274b_46ff_43a1_9a31_179ea07cd93b-------", "anchor_text": "Anand Mishra"}, {"url": "https://medium.com/@anandmishra.kanha/deep-reinforcement-learning-current-state-of-art-383190b14464?source=read_next_recirc-----c56f16bc8b79----3---------------------ae4a274b_46ff_43a1_9a31_179ea07cd93b-------", "anchor_text": "Deep reinforcement learning \u2014 current state of artCurrent"}, {"url": "https://medium.com/@anandmishra.kanha/deep-reinforcement-learning-current-state-of-art-383190b14464?source=read_next_recirc-----c56f16bc8b79----3---------------------ae4a274b_46ff_43a1_9a31_179ea07cd93b-------", "anchor_text": "5 min read\u00b7Dec 15, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F383190b14464&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40anandmishra.kanha%2Fdeep-reinforcement-learning-current-state-of-art-383190b14464&user=Anand+Mishra&userId=86f86a9a5573&source=-----383190b14464----3-----------------clap_footer----ae4a274b_46ff_43a1_9a31_179ea07cd93b-------", "anchor_text": ""}, {"url": "https://medium.com/@anandmishra.kanha/deep-reinforcement-learning-current-state-of-art-383190b14464?source=read_next_recirc-----c56f16bc8b79----3---------------------ae4a274b_46ff_43a1_9a31_179ea07cd93b-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F383190b14464&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40anandmishra.kanha%2Fdeep-reinforcement-learning-current-state-of-art-383190b14464&source=-----c56f16bc8b79----3-----------------bookmark_preview----ae4a274b_46ff_43a1_9a31_179ea07cd93b-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----c56f16bc8b79--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----c56f16bc8b79--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----c56f16bc8b79--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----c56f16bc8b79--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----c56f16bc8b79--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----c56f16bc8b79--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----c56f16bc8b79--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----c56f16bc8b79--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----c56f16bc8b79--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----c56f16bc8b79--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}