{"url": "https://towardsdatascience.com/intuitively-how-can-we-understand-different-classification-algorithms-principles-d45cf8ef54e3", "time": 1683006318.9684522, "path": "towardsdatascience.com/intuitively-how-can-we-understand-different-classification-algorithms-principles-d45cf8ef54e3/", "webpage": {"metadata": {"title": "Intuitively, How Can We Understand Different Classification Algorithms Principles | by Angela Shi | Towards Data Science", "h1": "Intuitively, How Can We Understand Different Classification Algorithms Principles", "description": "In the graph below, given the blue dots and the red dots, we can see that there is a pattern. As humans, we can use our \u201cintuition\u201d to separate them and to predict the color of a new dot. For\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/intuitively-how-can-we-build-non-linear-classifiers-10c381ed633e", "anchor_text": "Intuitively, How Can We build Non-Linear Classifiers", "paragraph_index": 31}, {"url": "https://nlp.stanford.edu/IR-book/html/htmledition/support-vector-machines-the-linearly-separable-case-1.html", "anchor_text": "Support vector machines: The linearly separable case", "paragraph_index": 37}, {"url": "https://towardsdatascience.com/intuitively-how-can-we-better-understand-logistic-regression-97af9e77e136", "anchor_text": "Intuitively, How Can We (Better) Understand Logistic Regression", "paragraph_index": 58}, {"url": "https://towardsdatascience.com/intuitively-how-can-we-build-non-linear-classifiers-10c381ed633e", "anchor_text": "Intuitively, How Can We build Non-Linear Classifiers", "paragraph_index": 75}], "all_paragraphs": ["In the graph below, given the blue dots and the red dots, we can see that there is a pattern. As humans, we can use our \u201cintuition\u201d to separate them and to predict the color of a new dot.", "For example, most of us would probably say that the black dot in the graph belongs to the blue dots category. But how can you mathematically express this \u201cintuition\u201d? And you will see that different intuitions lead to all the algorithms we know\u2026", "If blue dots and red dots may be abstract for you, let\u2019s see some real-world examples.", "You may already know some classification algorithms that would work: logistic regression, kNN, LDA (linear discriminant analysis), SVM, decision tree, etc.", "But is it intuitive to understand them?", "First, to simplify the problem, we are going to consider the 1D situation in this article (we\u2019ll consider the 2D situation in another article)", "As we can see in the graph below, we have only one predictor, which is X and the target variable is Y with two classes, red dots and blue dots.", "Note that mathematically, \u201cred\u201d or \u201cblue\u201d doesn\u2019t mean anything, so we\u2019ll transform this variable into a binary variable: 1 for \u201cblue\u201d and 0 for \u201cred\u201d. This corresponds to the question: is the dot blue ? 1 for True, and 0 for False. Of course, this is just a convention.", "In real-world example, \u201cis the dot blue?\u201d can be: Is the tumor malign or not? Is the email spam or not?", "For a given dot, the idea is to look at the neighbors of the dot.", "What are the neighbors? the closest ones?", "What does \u201cclose\u201d mean? the shortest distance?", "What is a distance? Here, it just means the difference between two values of x (x is a real number).", "Now, given a new dot, we can just calculate the distance between this one and all the others. And we can select the closest ones.", "But how many? That is the big question with this approach: how many points do we choose?", "To begin with, let\u2019s call this number k, and say k=5.", "Now we can examine the class of the neighbors. If you have a majority of one class, then you can predict with the majority class. In our case, if the majority class of the neighbors is 1, then the new point has a strong probability to belong to class 1.", "Note the \u201cmajority\u201d principle. So if we chose k=4, then it will be difficult to decide. So if k is odd, it helps to make a definitive decision.", "Now it is possible to calculate the probability for a new point and we have the graph below :", "This principle is called, guess what, kNN for K-Nearest-Neighbours.", "Now, what is special about this algorithm is that since you don\u2019t know which points to keep, you have to keep them all for each prediction. That\u2019s why we say that this algorithm is not model-based, but instance-based.", "We just said above that it is inconvenient to choose the number k and we didn\u2019t model anything about our observations.", "So now what do we do? First, let\u2019s consider all the population. If you do so, then your prediction will be the same for all the new dots: the majority class and the probability will be the proportion of the majority class.", "That may be a little simplistic. To do better, we can consider a normal distribution of the dots. Why the normal distribution? Well, it is simple, and simplify all the calculations. Is this really a good reason? Well, that is what we call \u201cmodeling\u201d.", "All models are wrong, but some are useful. \u2014 George Box, famous statistician", "Now instead of counting neighbors, we can ask a better question: how far am I from the blue dots, or the red dots. In other terms: given a new dot, what is the probability that this dot is blue or red?", "In order to know to which color the new dot is closer, we just have to compare the two probability densities. In the graph below, the black curve represents the ratio: PDF_b/(PDF_b+PDF_r)", "And now, what is quite amazing in terms of modeling, is that: with kNN, you had to keep all the dots to make a decision, now you only have to use several parameters, like the means and the standard deviation in order to define the normal distribution.", "In the previous dataset, we had the same number of blue dots and red dots. If the number is different, we can weight the two densities by their proportions.", "Now this principle is used by algorithms like linear discriminant analysis, quadratic discriminant analysis, naive Bayes classifier.", "What are the differences between them?", "Remember that we had to calculate normal densities? And to get a normal distribution, we had to calculate the mean and the standard deviation. For the means, it\u2019s easy. But for the standard deviation, we have two choices. We can calculate the standard deviation for each class, or, to simplify, we can consider that the standard deviation is the same for the two classes. How so? We can use the weighted value of the two standard deviations. Now why \u201clinear\u201d vs \u201cquadratic\u201d? For this, I wrote another article: Intuitively, How Can We build Non-Linear Classifiers.", "From the previous principle, we can see that after modeling the entire dataset, we come up with a decision boundary.", "Now, we can wonder: if the goal is to find a frontier, why don\u2019t we directly analyze the frontier areas?", "The \u201cfrontier area\u201d can be defined with M (for margin), which is the distance between two points that we choose to define the \u201cfrontier area\u201d. We can note as A the red dots\u2019 candidate point and B the blue dots\u2019 candidate point. In mathematical terms, we have:", "First let\u2019s consider that the two classes are linearly separable. Then we can calculate the maximum of the red dots (we get A), and the minimum of the blue dots (we have our B). (Note that the minimum and the maximum are easy to define in the 1D case. But it can be more complicated when the dimensionality is higher.)", "Now very intuitively (see the quotation below), we can choose the mean of the two values as the decision boundary.", "Maximizing the margin seems good because points near the decision [interval] represent very uncertain classification decisions: there is almost a 50% chance of the classifier deciding either way. A classifier with a large margin makes no low certainty classification decisions. This gives you a classification safety margin \u2014 Support vector machines: The linearly separable case", "But what if there is one value of class 0 that is very close to class 1? Then we will have the frontier as drawn in the graph below:", "Now we have this intuition that the decision boundary defined in the second situation is not that optimal: the frontier area is small. Now, what if we keep the previous M, and let the anomaly point, as shown below:", "And we can add a penalty term for the anomaly point, which can be the distance between the anomaly point and A.", "And even better, we can weight the penalty, with a coefficient, and let\u2019s call it C. So the final decision criterion is:", "In general, if there are several anomaly points, we can do the sum corresponding to all of them:", "By doing so, we also solve the problem for the situation when the dots are not linearly separable as showed in the following graph (remember, we said that we can first consider that the dots are linearly separable so that we get our intuitive \u201cfrontier area\u201d.)", "Now, this principle is used for SVM (Support Vector Machine), in order to get the final version of SVM, we have to do some small adjustments (we will discuss them in another article). And why is it called \u201csupport\u201d? Well, because you use different points (called \u201csupport vectors\u201d) in order to maximize the margin (or more precisely the penalized margin).", "For this principle, let\u2019s consider a straight line to model y as a function of x.", "To simplify the resolution of this problem, we can consider that the straight line will pass through the means of each class, as shown in the following graph.", "And now, you can consider the value of x when y=0.5, and this can be your decision boundary to decide if y belongs to class 0 or to class 1.", "But with this model, for big values of x, you will get y > 1 and for small values of x, y<0. So what can we do? Let\u2019s smooth it. Like this?", "Come on, we can do better, like this?", "To do the smoothing in the graph above, we can use this function for example", "To understand why we use this function, you can note that we find the initial straight line : y(x)=a \u00d7 x + b in this function, and we can define: sigma(y)= 1 / (1+exp( -y )))", "To visualize the smoothing effect, we can see the graph of sigma below", "And now the task is to find the parameters: a and b. To achieve this goal, we can consider the probability of each point to be correctly classified.", "The criterion is the maximization of the overall probability: we multiply all the probabilities (for class 0 and class 1). And we try to maximize the result.", "The mathematical trick is then to take the log and to take the derivatives, etc. But it turns out that there is not an easy way (a closed formula) to find the parameters, and we have to solve it numerically.", "Below we can see the situation for different values of a and b. The vertical segments represent the probabilities for each point. The product of all these probabilities should be maximized in order to optimize a and b.", "And this is the principle used by logistic regression because of the name of the sigma function that we saw before, which is called the logistic function.", "(If you find this explanation not intuitive enough, you can read this article: Intuitively, How Can We (Better) Understand Logistic Regression )", "The final principle is about mistakes you can make when choosing the decision boundary.", "When a decision boundary is found, the dots are separated into two zones. The idea is to describe the \u201chomogeneity\u201d of the zones: the fewer mistakes there are, the better it is.", "Now let\u2019s find a function to describe the \u201chomogeneity\u201d of the zones. Consider p, as the proportion of class 1 for example. And we get (1-p) for the proportion of class 0.", "Now with p and (1-p), what can we do? Let\u2019s begin with very simple operations.", "So the function is symmetrical, which is a required characteristic because this function should work for either of the classes. And the output of this function can indicate the \u201chomogeneity\u201d of the zone: the lower, the better", "In order to find the decision boundary, we have to test different values of x. For each value of x, two zones are created: left-hand side zone and right-hand side zone. For each side, we can calculate the indicator, then we weight them with the proportion of points in each zone in order to get the overall indicator.", "Now we can test all the dots as a decision boundary, and see how the indicator varies.", "Now we can take the lowest level of the indicator to find the optimal value of x. Note that since we have a step function, we can calculate the mean of the two values of x that define the lowest level of the indicator.", "Now, what is special about this principle is that, you can continue to find other decision boundaries in each zones. We will see in another article dedicated to this principle that the advantage of this is that you can easily handle the non-linear situation.", "So step by step, we can find the optimal frontiers one by one.", "Here we have the decision tree about different decision boundaries for each step.", "Now the question is: when do we stop? There can be different rules\u2026", "This principle is also considered as \u201cdivide to conquer\u201d. It allows us to grow decision trees. These trees are also the basis of more complicated algorithms such as Random Forest, or Gradient Boosting Machines.", "Note that we didn\u2019t finish all the reasoning for the five principles:", "For machine learning practitioners, you can note that I voluntary didn\u2019t use technical terms, maybe you can comment and put the technical terms in the right context: hard margin, soft margin, gradient descent, convexity, hyperplane, prior probability, posterior probability, loss function, crossed entropy, maximum likelihood estimation, overfitting \u2026", "When we talk about machine learning and artificial intelligence, we always talk about neural networks, but why didn\u2019t we mention them here? Well actually, we did see an example of a simple neural network which is logistic regression, and the article about it is coming soon.", "The toy data used was quite simple: the two classes are linearly separable. How do these algorithms deal with data that are non-linearly separable? You can read this article: Intuitively, How Can We build Non-Linear Classifiers.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "a data science and machine learning enthusiast, dedicated to simplifying complex concepts in a clear way."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fd45cf8ef54e3&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuitively-how-can-we-understand-different-classification-algorithms-principles-d45cf8ef54e3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuitively-how-can-we-understand-different-classification-algorithms-principles-d45cf8ef54e3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuitively-how-can-we-understand-different-classification-algorithms-principles-d45cf8ef54e3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuitively-how-can-we-understand-different-classification-algorithms-principles-d45cf8ef54e3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----d45cf8ef54e3--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d45cf8ef54e3--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@angela.shi?source=post_page-----d45cf8ef54e3--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@angela.shi?source=post_page-----d45cf8ef54e3--------------------------------", "anchor_text": "Angela Shi"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2bf03e38122e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuitively-how-can-we-understand-different-classification-algorithms-principles-d45cf8ef54e3&user=Angela+Shi&userId=2bf03e38122e&source=post_page-2bf03e38122e----d45cf8ef54e3---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd45cf8ef54e3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuitively-how-can-we-understand-different-classification-algorithms-principles-d45cf8ef54e3&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd45cf8ef54e3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuitively-how-can-we-understand-different-classification-algorithms-principles-d45cf8ef54e3&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/intuitively-how-can-we-build-non-linear-classifiers-10c381ed633e", "anchor_text": "Intuitively, How Can We build Non-Linear Classifiers"}, {"url": "https://nlp.stanford.edu/IR-book/html/htmledition/support-vector-machines-the-linearly-separable-case-1.html", "anchor_text": "Support vector machines: The linearly separable case"}, {"url": "https://towardsdatascience.com/intuitively-how-can-we-better-understand-logistic-regression-97af9e77e136", "anchor_text": "Intuitively, How Can We (Better) Understand Logistic Regression"}, {"url": "https://towardsdatascience.com/intuitively-how-can-we-build-non-linear-classifiers-10c381ed633e", "anchor_text": "Intuitively, How Can We build Non-Linear Classifiers"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----d45cf8ef54e3---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/classification?source=post_page-----d45cf8ef54e3---------------classification-----------------", "anchor_text": "Classification"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----d45cf8ef54e3---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/data-science?source=post_page-----d45cf8ef54e3---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----d45cf8ef54e3---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd45cf8ef54e3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuitively-how-can-we-understand-different-classification-algorithms-principles-d45cf8ef54e3&user=Angela+Shi&userId=2bf03e38122e&source=-----d45cf8ef54e3---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd45cf8ef54e3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuitively-how-can-we-understand-different-classification-algorithms-principles-d45cf8ef54e3&user=Angela+Shi&userId=2bf03e38122e&source=-----d45cf8ef54e3---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd45cf8ef54e3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuitively-how-can-we-understand-different-classification-algorithms-principles-d45cf8ef54e3&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d45cf8ef54e3--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fd45cf8ef54e3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuitively-how-can-we-understand-different-classification-algorithms-principles-d45cf8ef54e3&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----d45cf8ef54e3---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----d45cf8ef54e3--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----d45cf8ef54e3--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----d45cf8ef54e3--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----d45cf8ef54e3--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----d45cf8ef54e3--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----d45cf8ef54e3--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----d45cf8ef54e3--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----d45cf8ef54e3--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@angela.shi?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@angela.shi?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Angela Shi"}, {"url": "https://medium.com/@angela.shi/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.6K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2bf03e38122e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuitively-how-can-we-understand-different-classification-algorithms-principles-d45cf8ef54e3&user=Angela+Shi&userId=2bf03e38122e&source=post_page-2bf03e38122e--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F2aafe7bac5e3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintuitively-how-can-we-understand-different-classification-algorithms-principles-d45cf8ef54e3&newsletterV3=2bf03e38122e&newsletterV3Id=2aafe7bac5e3&user=Angela+Shi&userId=2bf03e38122e&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}