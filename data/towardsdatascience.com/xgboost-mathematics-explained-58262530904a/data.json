{"url": "https://towardsdatascience.com/xgboost-mathematics-explained-58262530904a", "time": 1682993931.008575, "path": "towardsdatascience.com/xgboost-mathematics-explained-58262530904a/", "webpage": {"metadata": {"title": "XGBoost Mathematics Explained. A walk-through of the Gradient Boosted\u2026 | by Dimitris Leventis | Medium", "h1": "XGBoost Mathematics Explained", "description": "XGBoost (https://github.com/dmlc/xgboost) is one of the most popular and efficient implementations of the Gradient Boosted Trees algorithm, a supervised learning method that is based on function\u2026"}, "outgoing_paragraph_urls": [{"url": "https://github.com/dmlc/xgboost", "anchor_text": "https://github.com/dmlc/xgboost", "paragraph_index": 0}, {"url": "https://en.wikipedia.org/wiki/Sigmoid_function", "anchor_text": "sigmoid function", "paragraph_index": 24}, {"url": "https://stats.stackexchange.com/questions/231220/how-to-compute-the-gradient-and-hessian-of-logarithmic-loss-question-is-based", "anchor_text": "stats.stackexchange", "paragraph_index": 25}, {"url": "https://towardsdatascience.com/back-propagation-explained-9720c2d4a566", "anchor_text": "here", "paragraph_index": 28}], "all_paragraphs": ["XGBoost (https://github.com/dmlc/xgboost) is one of the most popular and efficient implementations of the Gradient Boosted Trees algorithm, a supervised learning method that is based on function approximation by optimizing specific loss functions as well as applying several regularization techniques.", "The purpose of this post is to explain the mathematics of some critical parts of the paper as well as to give some insights.", "The objective function (loss function and regularization) at iteration t that we need to minimize is the following:", "It is easy to see that the XGBoost objective is a function of functions (i.e. l is a function of CART learners, a sum of the current and previous additive trees), and as the authors refer in the paper [2] \u201ccannot be optimized using traditional optimization methods in Euclidean space\u201d.", "From the reference [1] we can see as an example that the best linear approximation for a function f(x) at point a is:", "Why we need to use the Taylor approximation?Because we need to transform the original objective function to a function in the Euclidean domain, in order to be able to use traditional optimization techniques.", "Explanation of the above answer:Let\u2019s take the simplest linear approximation of a function f(x) from [1]:", "The \u201ctrick\u201d here is that we can transform a function f(x) to a simplest function of \u0394x around a specific point a by using Taylor\u2019s theorem.", "To understand better, remember that before the Taylor approximation the x in objective function f(x) was the sum of t CART trees and after this it becomes a function of the current tree (step t) only.", "Note that the objective function must be differentiable.", "In our case f(x) is the loss function l, while a is the previous step (t-1) predicted value and \u0394x is the new learner we need to add in step t.", "Using the above in each iteration t we can write the objective (loss) function as a simple function of the new added learner and thus to apply Euclidean space optimization techniques.", "As we already said, a is the prediction at step (t-1) while (x-a) is the new learner that we need to add in step (t) in order to greedily minimize the objective.", "So, if we decide to take the second-order Taylor approximation, we have:", "Finally, if we remove the constant parts, we have the following simplified objective to minimize at step t:", "The above is a sum of simple quadratic functions of one variable and can be minimized by using known techniques, so our next goal is to find a learner that minimizes the loss function at iteration t.", "Being at iteration t we need to build a learner that achieves the maximum possible reduction of loss, the following questions arrive:", "The good news is that there is a way to \u201cmeasure the quality of a tree structure q\u201d as the authors refer, and the scoring function is the following (see the correspondence to the \u201csimple quadratic function solution\u201d above):", "While the bad news is that it is impossible in terms of required calculations to \u201cenumerate all the possible tree structures q\u201d and thus find the one with maximum loss reduction.", "Note that the \u201cquality scoring function\u201d above returns the minimum loss value for a given tree structure, meaning that the original loss function is evaluated by using the optimal weight values. So, for any given tree structure we have a way to calculate the optimal weights in leaves.", "In practice what we do in order to build the learner is to:", "The above algorithm is called the \u201cExact Greedy Algorithm\u201d and its complexity is O(n*m) where n is the number of training samples and m is the features dimension.", "Let\u2019s take the case of binary classification and log loss objective function:", ",where y is the real label in {0,1} and p is the probability score.", "Note that p (score or pseudo-probability) is calculated after applying the famous sigmoid function into the output of the GBT model x.The output x of the model is the sum across the the CART tree learners.", "So, in order to minimize the log loss objective function we need to find its 1st and 2nd derivatives (gradient and hessian) with respect to x.In this stats.stackexchange post you can find that gradient = (p-y) and hessian = p*(1-p).", "Summarizing the GBT model which \u2014 remember \u2014 is a sum of CART (tree) learners will try to minimize the log loss objective and the scores at leaves which are actually the weights have a meaning as a sum across all the trees of the model and are always adjusted in order to minimize the loss. This is why we need to apply the sigmoid function in the output of GBT models in case of binary classification probability scoring.", "Below you can find an awesome pure Python implementation of the Gradient Boosted Trees algorithm that is using the \u201cExact Greedy Algorithm\u201d:", "If you are interested in Neural Networks as well please check my latest post about back-propagation algorithm here.", "Senior Applied Scientist at Microsoft | Kaggle Master", "Senior Applied Scientist at Microsoft | Kaggle Master"], "all_outgoing_urls": [{"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----58262530904a--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----58262530904a--------------------------------", "anchor_text": "Dimitris Leventis"}, {"url": "https://github.com/dmlc/xgboost", "anchor_text": "https://github.com/dmlc/xgboost"}, {"url": "https://arxiv.org/pdf/1603.02754.pdf", "anchor_text": "https://arxiv.org/pdf/1603.02754.pdf"}, {"url": "https://en.wikipedia.org/wiki/Sigmoid_function", "anchor_text": "sigmoid function"}, {"url": "https://stats.stackexchange.com/questions/231220/how-to-compute-the-gradient-and-hessian-of-logarithmic-loss-question-is-based", "anchor_text": "stats.stackexchange"}, {"url": "https://github.com/lancifollia/tinygbt", "anchor_text": "https://github.com/lancifollia/tinygbt"}, {"url": "https://github.com/dimleve/tinygbt", "anchor_text": "https://github.com/dimleve/tinygbt"}, {"url": "https://towardsdatascience.com/back-propagation-explained-9720c2d4a566", "anchor_text": "here"}, {"url": "https://mathinsight.org/taylors_theorem_multivariable_introduction", "anchor_text": "https://mathinsight.org/taylors_theorem_multivariable_introduction"}, {"url": "https://arxiv.org/abs/1603.02754", "anchor_text": "https://arxiv.org/abs/1603.02754"}, {"url": "https://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf", "anchor_text": "https://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf"}, {"url": "https://stats.stackexchange.com/questions/231220/how-to-compute-the-gradient-and-hessian-of-logarithmic-loss-question-is-based", "anchor_text": "https://stats.stackexchange.com/questions/231220/how-to-compute-the-gradient-and-hessian-of-logarithmic-loss-question-is-based"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----58262530904a---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/xgboost?source=post_page-----58262530904a---------------xgboost-----------------", "anchor_text": "Xgboost"}, {"url": "https://medium.com/tag/mathematics?source=post_page-----58262530904a---------------mathematics-----------------", "anchor_text": "Mathematics"}, {"url": "https://medium.com/tag/kaggle?source=post_page-----58262530904a---------------kaggle-----------------", "anchor_text": "Kaggle"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----58262530904a---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://towardsdatascience.com/?source=post_page-----58262530904a--------------------------------", "anchor_text": "More from Dimitris Leventis"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F9ae444f524f8&operation=register&redirect=https%3A%2F%2Fdimleve.medium.com%2Fxgboost-mathematics-explained-58262530904a&newsletterV3=90ecc357b1cf&newsletterV3Id=9ae444f524f8&user=Dimitris+Leventis&userId=90ecc357b1cf&source=-----58262530904a---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----58262530904a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----58262530904a--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----58262530904a--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----58262530904a--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----58262530904a--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----58262530904a--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----58262530904a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/plans?source=upgrade_membership---two_column_layout_sidebar----------------------------------", "anchor_text": "Get unlimited access"}, {"url": "https://towardsdatascience.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Dimitris Leventis"}, {"url": "https://towardsdatascience.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "109 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F9ae444f524f8&operation=register&redirect=https%3A%2F%2Fdimleve.medium.com%2Fxgboost-mathematics-explained-58262530904a&newsletterV3=90ecc357b1cf&newsletterV3Id=9ae444f524f8&user=Dimitris+Leventis&userId=90ecc357b1cf&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}