{"url": "https://towardsdatascience.com/latest-winning-techniques-for-kaggle-image-classification-with-limited-data-5259e7736327", "time": 1683000997.561615, "path": "towardsdatascience.com/latest-winning-techniques-for-kaggle-image-classification-with-limited-data-5259e7736327/", "webpage": {"metadata": {"title": "Latest Winning Techniques for Kaggle Image Classification with Limited Data | by Kayo Yin | Towards Data Science", "h1": "Latest Winning Techniques for Kaggle Image Classification with Limited Data", "description": "Tutorial on how to prevent your model from overfitting on a small dataset but still make accurate classifications"}, "outgoing_paragraph_urls": [{"url": "https://www.kaggle.com/c/cs-ioc5008-hw1/overview", "anchor_text": "in-class Kaggle challenge", "paragraph_index": 0}, {"url": "https://medium.com/neuralspace/kaggle-1-winning-approach-for-image-classification-challenge-9c1188157a86", "anchor_text": "this article", "paragraph_index": 0}, {"url": "https://arxiv.org/abs/1904.11486", "anchor_text": "recent April 2019 paper", "paragraph_index": 19}, {"url": "https://github.com/kayoyin/GreyClassifier", "anchor_text": "GitHub", "paragraph_index": 35}, {"url": "http://kayoyin.github.io/blog", "anchor_text": "kayoyin.github.io/blog", "paragraph_index": 37}], "all_paragraphs": ["In this article, I will go through the approach I used for an in-class Kaggle challenge. I spent about two weeks on the challenge, with a final submission score of 0.97115 which places me second on the final leaderboard. I started off by borrowing ideas from this article, which I recommend as well.", "The proposed challenge is a natural images classification task with 13 classes. The first difficulty in this challenge is the scarcity of available data: only 3 859 images for training. The rules of the challenge was not to use external data during training as well. With little data, the model will be more prone to overfitting without learning to generalize.", "Moreover, because these images are in grayscale, they contain less information than color images such as the ImageNet dataset, so a pre-trained model on color images cannot be applied directly to this task. Upon further inspection of the dataset, many classes contain images that are visually very similar or containing the same elements. The model will lose accuracy when confusing such classes.", "First, the images in our dataset do not all have the same dimensions, so we resize all images before inputting them to the model. Over half of the training images have dimension 256 x 256, so we resize or crop the other images to this dimension.", "We will also apply normalization. Originally, the images are represented as tensors with pixel values ranging from 0 to 255. We simply divide each value by 255 to rescale and obtain values between 0 and 1 that are preferred by neural networks. Additionally, we apply contrast stretching to all images for image enhancement. This will help the model \u2018see\u2019 the details in images more clearly.", "The classes are also unbalanced, meaning we have an unequal amount of data between each class. This would make the model more or less biased towards certain classes. To address this issue, we artificially add more images such that each class has as many images as the largest class. To resample from small classes, we randomly crop an area in the image to create a new sample. This is based on the assumption that the cropped image will contain the same elements that are characteristic of the class.", "Finally, as deep networks perform and generalize better with a large amount of training data, we perform data augmentation. Our aim is to artificially create new images that contain the features characteristic of its class. To do so, the techniques I used can be summarized below:", "Before starting training, we split the dataset into a training set (80%) and a validation set (20%). We apply all of the processing techniques discussed above on both sets, except image augmentation that is only used on the training set.", "Because our dataset contains images similar to those in ImageNet, we will start off with a CNN model that has been pre-trained on ImageNet. The idea is to freeze lower layers of the pre-trained model that can capture generic features while fine-tuning the higher layers to our specific domain. We also redefine the last layer to output 13 values, one for each class.", "PyTorch provides several pre-trained models with different architectures. Among them, ResNet18 is the architecture I adopted as it gave the best validation accuracy upon training on our data, after running various architectures for 5 epochs. After experimenting with different numbers of frozen layers, 7 was found to be the best one. I also used the SGD optimizer with weight decay to discourage overfitting.", "To further improve results and make the model converge to the global minimum, we want to adjust the learning rate. Instead of determining the optimal learning rate experimentally, I chose to use cyclic learning rate scheduling. This method makes the learning rate vary cyclically, which allows the model to converge to and escape several local minima. It also eliminates the need to find the best learning rate \u201cby hand\u201d.", "Ensemble methods are very powerful in improving the model\u2019s overall performance. However, it can also be computationally expensive to separately train several different models for ensemble learning. This is why I chose to use snapshot ensembling with cyclic LR scheduling.", "Snapshot ensembling saves the model\u2019s parameters periodically during training. The idea is that during cyclic LR scheduling, the model converges to different local minima. Therefore, by saving the model parameters at different local minima, we obtain a set of models that can give different insights for our prediction. This allows us to gather an ensemble of models in a single training cycle.", "For each image, we concatenate the class probability predictions of each of the \u201csnapshot\u201d models to form a new data point. This new data is then inputted into an XGBoost model to give a prediction based on the snapshot models.", "Upon inspection of the confusion matrix on the validation set for a single model, we discover that it often confuses one class for the same other one. In fact, we find three subclasses that are often confused together:", "Also, the model is already very good at differentiating these subclasses (and finding suburbs). All that remains to get a great performance is for the model to accurately identify classifications within the subclasses.", "To do so, we train three new separate models on each subclass, using the same approach as before. Some classes have very few training data, so we increase the amount of data augmentation. We also find new parameters adjusted to each subclass.", "During prediction, we first use the model trained on the entire dataset. Then, for each prediction obtained, if the class probability is lower than a certain threshold, we take the class predicted by the relevant subclass model instead.", "Most modern convolutional networks, such as ResNet18, are not shift-invariant. The network outputs can change drastically with small shifts or translations to the input. This is because the striding operation in the convolutional network ignores the Nyquist sampling theorem and aliases, which breaks shift equivariance.", "I decided to apply an anti-aliasing method proposed in the recent April 2019 paper. This is done by simply adding a \u201cBlurPool\u201d layer, that is a blurring filter and a subsampling layer, after the convolution layers of the network. This method has been shown to improve both classification consistency between different shifts of the image, and greater classification accuracy due to better generalization.", "I used the pre-trained anti-aliased ResNet18 model to fine-tune on the challenge\u2019s dataset. With anti-aliasing, I hope to overcome overfitting from scarcity of data by having the model generalize to image translation and shifting.", "If you want to know more about this anti-aliasing method, I explain the \u201cMaking Convolutional Networks Shift-Invariant Again\u201d paper in more detail here:", "The methodology used can be summarized as below:", "Fine-tuning a ResNet18 model for 5 epochs on data without any processing except resizing already gives a testing accuracy of 0.91442. This reveals the remarkable efficiency of transfer learning \u2014 with little data and computations, the model can already show good performance on relevant tasks.", "Adding data augmentation and training longer for 10 epochs, we obtain a testing accuracy of 0.93076. This confirms the importance of having a large training dataset and the scalability of augmentation techniques.", "Adding class balancing and learning rate scheduling, the testing accuracy goes up to 0.94230. Moreover, the confusion matrices show that after balancing, the model predicts underrepresented classes with higher accuracy. This also shows that the learning rate is an important parameter in the convergence of the model.", "Then, with snapshot ensembling on the model trained on all of the data, the testing accuracy improves to 0.95000. This illustrates how cyclic LR scheduling allows us to obtain through a single training cycle models with different behaviour, and an XGBoost meta-learner can extract useful information from their predictions.", "By contrast stretching all images and also training models on specific subclasses and combining their predictions, the testing accuracy rises to 0.95865. The confusion matrix shows an improvement in accurately classifying within the subclasses, especially for the \u201curban\u201d subclass. Developing models that are \u201cexperts\u201d on certain classes and using them with a model good at differentiating the subclasses proves to be very efficient.", "Finally, after anti-aliasing the ResNet18 network and combining the training and validation sets to use all annotated data available for training, the testing accuracy rises to 0.97115. Anti-aliasing is a powerful method to improve generalization, which is crucial when the image data is limited.", "Here are a few other ideas I had to tackle this challenge, that either did not work well or I did not have the means to try.", "The images are in greyscale so though they are encoded into three channels when loading them, they can be represented as single-channel matrices. My idea was that this dimension reduction can speed up training while preserving all necessary information, but through experiments, this method showed to lose accuracy without speeding up training significantly.", "I also tried ensembling on models retrieved with other ways, such as models trained on images after different processing methods (with/without class balancing, different image enhancement techniques, different data augmentation methods) but these approaches are more computationally expensive and do not give significantly better accuracy.", "Data augmentation and class balancing, as seen previously, plays a key role in the model performance. Besides classic image processing, generative models can be used solely for synthesizing annotated data. For example, DAGAN models can be used for data augmentation while BAGAN can be used for balancing.", "The images in the provided dataset have similar contents as the natural images composing the ImageNet dataset, the difference being that our images are black and white. Therefore, a model pre-trained on greyscale images would be even more relevant for this task.", "If I cannot obtain a pre-trained model for greyscale images, my next idea was to colorize the images artificially, to hopefully add additional information. Pre-trained models for artificial image colorization do exist and are publically available, let me know if you give this method a go!", "Thanks for reading, I hope you enjoyed this article! You can find the full code of my approach on GitHub.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "PhD student at UC Berkeley researching AI. Now writing at kayoyin.github.io/blog"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F5259e7736327&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flatest-winning-techniques-for-kaggle-image-classification-with-limited-data-5259e7736327&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flatest-winning-techniques-for-kaggle-image-classification-with-limited-data-5259e7736327&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flatest-winning-techniques-for-kaggle-image-classification-with-limited-data-5259e7736327&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flatest-winning-techniques-for-kaggle-image-classification-with-limited-data-5259e7736327&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----5259e7736327--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5259e7736327--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@kayo.yin?source=post_page-----5259e7736327--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@kayo.yin?source=post_page-----5259e7736327--------------------------------", "anchor_text": "Kayo Yin"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fee0d7547aae1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flatest-winning-techniques-for-kaggle-image-classification-with-limited-data-5259e7736327&user=Kayo+Yin&userId=ee0d7547aae1&source=post_page-ee0d7547aae1----5259e7736327---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5259e7736327&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flatest-winning-techniques-for-kaggle-image-classification-with-limited-data-5259e7736327&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5259e7736327&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flatest-winning-techniques-for-kaggle-image-classification-with-limited-data-5259e7736327&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "http://www.dakanarts.com/13-black-white/", "anchor_text": "http://www.dakanarts.com/13-black-white/"}, {"url": "https://www.kaggle.com/c/cs-ioc5008-hw1/overview", "anchor_text": "in-class Kaggle challenge"}, {"url": "https://medium.com/neuralspace/kaggle-1-winning-approach-for-image-classification-challenge-9c1188157a86", "anchor_text": "this article"}, {"url": "https://www.researchgate.net/figure/Examples-in-the-ImageNet-dataset_fig7_314646236", "anchor_text": "ImageNet dataset samples"}, {"url": "https://arxiv.org/abs/1704.00109", "anchor_text": "https://arxiv.org/abs/1704.00109"}, {"url": "https://arxiv.org/abs/1904.11486", "anchor_text": "recent April 2019 paper"}, {"url": "https://arxiv.org/abs/1904.11486", "anchor_text": "https://arxiv.org/abs/1904.11486"}, {"url": "https://towardsdatascience.com/https-towardsdatascience-com-making-convolutional-networks-shift-invariant-again-f16acca06df2", "anchor_text": "Making Convolutional Networks Shift-Invariant AgainWhat\u2019s wrong with modern convolutional networks and how can we fix them? If you use CNNs, you may want to read this.towardsdatascience.com"}, {"url": "https://github.com/kayoyin/GreyClassifier", "anchor_text": "GitHub"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----5259e7736327---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/computer-vision?source=post_page-----5259e7736327---------------computer_vision-----------------", "anchor_text": "Computer Vision"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----5259e7736327---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----5259e7736327---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----5259e7736327---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5259e7736327&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flatest-winning-techniques-for-kaggle-image-classification-with-limited-data-5259e7736327&user=Kayo+Yin&userId=ee0d7547aae1&source=-----5259e7736327---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5259e7736327&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flatest-winning-techniques-for-kaggle-image-classification-with-limited-data-5259e7736327&user=Kayo+Yin&userId=ee0d7547aae1&source=-----5259e7736327---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5259e7736327&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flatest-winning-techniques-for-kaggle-image-classification-with-limited-data-5259e7736327&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5259e7736327--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F5259e7736327&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flatest-winning-techniques-for-kaggle-image-classification-with-limited-data-5259e7736327&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----5259e7736327---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----5259e7736327--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----5259e7736327--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----5259e7736327--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----5259e7736327--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----5259e7736327--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----5259e7736327--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----5259e7736327--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----5259e7736327--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@kayo.yin?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@kayo.yin?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Kayo Yin"}, {"url": "https://medium.com/@kayo.yin/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "538 Followers"}, {"url": "http://kayoyin.github.io/blog", "anchor_text": "kayoyin.github.io/blog"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fee0d7547aae1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flatest-winning-techniques-for-kaggle-image-classification-with-limited-data-5259e7736327&user=Kayo+Yin&userId=ee0d7547aae1&source=post_page-ee0d7547aae1--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F97e53eb796c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flatest-winning-techniques-for-kaggle-image-classification-with-limited-data-5259e7736327&newsletterV3=ee0d7547aae1&newsletterV3Id=97e53eb796c&user=Kayo+Yin&userId=ee0d7547aae1&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}