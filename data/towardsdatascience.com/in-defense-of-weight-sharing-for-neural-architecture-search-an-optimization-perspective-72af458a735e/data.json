{"url": "https://towardsdatascience.com/in-defense-of-weight-sharing-for-neural-architecture-search-an-optimization-perspective-72af458a735e", "time": 1683011101.34608, "path": "towardsdatascience.com/in-defense-of-weight-sharing-for-neural-architecture-search-an-optimization-perspective-72af458a735e/", "webpage": {"metadata": {"title": "In defense of weight-sharing for neural architecture search: an optimization perspective | by Liam Li | Towards Data Science", "h1": "In defense of weight-sharing for neural architecture search: an optimization perspective", "description": "This collaborative work between CMU and Determined AI is jointly authored by Misha Khodak and Liam Li. Neural architecture search (NAS) \u2014 selecting which neural model to use for your learning problem\u2026"}, "outgoing_paragraph_urls": [{"url": "http://www.cs.cmu.edu/~mkhodak/", "anchor_text": "Misha Khodak", "paragraph_index": 0}, {"url": "http://proceedings.mlr.press/v80/pham18a.html", "anchor_text": "weight-sharing method", "paragraph_index": 1}, {"url": "http://proceedings.mlr.press/v80/pham18a.html", "anchor_text": "paper", "paragraph_index": 3}, {"url": "https://arxiv.org/abs/1707.07012", "anchor_text": "2,000\u201320,000", "paragraph_index": 3}, {"url": "https://openreview.net/forum?id=S1eYHoC5FX", "anchor_text": "DARTS", "paragraph_index": 4}, {"url": "http://openaccess.thecvf.com/content_CVPR_2019/papers/Dong_Searching_for_a_Robust_Neural_Architecture_in_Four_GPU_Hours_CVPR_2019_paper", "anchor_text": "GDAS", "paragraph_index": 4}, {"url": "https://arxiv.org/abs/1902.07638", "anchor_text": "Random Search with Weight-Sharing", "paragraph_index": 8}, {"url": "https://openreview.net/forum?id=SJx9ngStPH", "anchor_text": "several", "paragraph_index": 9}, {"url": "https://arxiv.org/abs/2001.01431", "anchor_text": "papers", "paragraph_index": 9}, {"url": "https://arxiv.org/abs/2002.04289", "anchor_text": "observed", "paragraph_index": 9}, {"url": "https://openreview.net/forum?id=H1loF2NFwr", "anchor_text": "rank disorder", "paragraph_index": 9}, {"url": "https://openreview.net/forum?id=H1gDNyrKDS", "anchor_text": "robustness evaluation", "paragraph_index": 10}, {"url": "https://openreview.net/forum?id=HJxyZkBKDr", "anchor_text": "NAS-Bench-201", "paragraph_index": 10}, {"url": "https://openreview.net/forum?id=rylqooRqK7", "anchor_text": "not", "paragraph_index": 14}, {"url": "https://arxiv.org/abs/1909.11926", "anchor_text": "all", "paragraph_index": 14}, {"url": "https://www.automl.org/wp-content/uploads/2020/07/AutoML_2020_paper_46.pdf", "anchor_text": "bilevel optimization can be viewed as a (possibly critical) method of regularizing by splitting the data", "paragraph_index": 14}, {"url": "https://papers.nips.cc/paper/563-a-simple-weight-decay-can-improve-generalization", "anchor_text": "regularization", "paragraph_index": 15}, {"url": "http://jmlr.org/papers/v15/srivastava14a.html", "anchor_text": "schemes", "paragraph_index": 15}, {"url": "http://proceedings.mlr.press/v28/sutskever13.html", "anchor_text": "optimization", "paragraph_index": 15}, {"url": "https://arxiv.org/abs/1412.6980", "anchor_text": "algorithms", "paragraph_index": 15}, {"url": "https://openreview.net/forum?id=H1gDNyrKDS", "anchor_text": "partial channel connections", "paragraph_index": 17}, {"url": "https://openreview.net/forum?id=H1gDNyrKDS", "anchor_text": "penalizing the Hessian of the validation loss", "paragraph_index": 17}, {"url": "https://pdf.sciencedirectassets.com/272575/1-s2.0-S0890540100X00648/1-s2.0-S0890540196926127/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEKD%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIQD7%2BtHFNUL7RXYPJ0EDCm3b1MAsXlCnZB3Af0knFN7XfwIgX%2B71daM8A8Xs74oA1F9BN7L2MdX30ynAuwa2vi9JubYqtAMISRADGgwwNTkwMDM1NDY4NjUiDJrntPy%2Btpy9pllYliqRA%2FnQ1fCJZf1myhJd2TQzWswcCWAv%2FQntgZdm2QqNtxnhSv0nAByRpO3xi22KKJwg%2BCLPnYrZUqxa4P4SHmqvulAI%2Fxf3l6uZ7wspesGp%2FMDBv9V3kCK6dcK3w4mpNks6S8yWqzpaS0BxPXLKxA9KarZR2uoY%2B4ocQLNOe6XRWgI847lEb9P%2BTQMc01RszzqexHl9B2YiCkV9Tc7BYSWgn0F4k0gifjkESbKB1rQ%2BISmegkffzK1X%2FUgoO75yALnzAMu9qudwQzsfS%2Ftp%2BHwZxnvPnTh3JSguAzD4mA9S7UYSocZIAYVyajcCM6jZAqfMd9H4xCsvtvqnj6BvMft%2B1MzYOrSVNSJofR01U9oyCAapyLVg1QWyoMVjUk9hN0XpZjldXoX1MFSCS%2BBn6ZPJwXSpl6w4PSDG7u%2BBk6bMiZzsVIggEPd5rCELVeQFdxJrkqUAdoiK8H0zvHjtlFonXoVkr4%2Fe4h7%2BCLBMwdFAHFUvhbqEGQfJsS%2BNjyBzvKUsEmbISKWPjhzStsTMGtGztpvPMJKyp%2FgFOusBAe3cKtpSi6K%2Bl%2BD2rY16%2Fs80QxiEAFjOW4KHhWd3JFv8oUcum7KgOqmDtF3nfxBVRdkfQKZyg1fEIPMExldSSzxoJbseZBG8IvbZ7e41a0lORahGPSgHD69Anphbyb3XzmEX%2B7KdCGYbMDRWm6mO5%2FH8u1P9IplyiHZI4JdyK5Ofim9%2BYsdAmaF0RIxMncLkhpZ1MION6mtkn4yuzqpwW%2FCXui2EMWJY5hWUJj5sN%2B0QSkHPT%2BcLJ%2FalcKHEXFBbuS0X3XoVOopsvTZZUD5uNFQU7qOzgKOkL711LbBma0Lu%2Fcpx8fLHvRHhfQ%3D%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20200711T162208Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTYWZDVZUM7%2F20200711%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=40b87202e72cb5f0d6ce7e70cd60faf656a7510b700cbd87a7ca95230ea5eb20&hash=60545d1176fe9465628c11f98e59d807c0541a838531a443e9b9311d1fe87e06&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0890540196926127&tid=spdf-a878840f-3818-4937-b5e7-c08bf521cf0a&sid=749b22f7830723476d9a089-8d75fac41b81gxrqa&type=client", "anchor_text": "exponentiated gradient descent", "paragraph_index": 19}, {"url": "https://web.iem.technion.ac.il/images/user-files/becka/papers/3.pdf", "anchor_text": "iteration complexity", "paragraph_index": 21}, {"url": "https://www.youtube.com/watch?v=uylS0FiuCK4", "anchor_text": "video link", "paragraph_index": 21}, {"url": "https://arxiv.org/abs/1806.04781", "anchor_text": "Zhang & He", "paragraph_index": 21}, {"url": "https://openreview.net/forum?id=HJxyZkBKDr", "anchor_text": "NAS-Bench-201", "paragraph_index": 23}], "all_paragraphs": ["This collaborative work between CMU and Determined AI is jointly authored by Misha Khodak and Liam Li.", "Neural architecture search (NAS) \u2014 selecting which neural model to use for your learning problem \u2014 is a promising but computationally expensive direction for automating and democratizing machine learning. The weight-sharing method, whose initial success at dramatically accelerating NAS surprised many in the field, has come under scrutiny due to its poor performance as a surrogate for full model-training (a miscorrelation problem known as rank disorder) and inconsistent results on recent benchmarks. In this post, we give a quick overview of weight-sharing and argue in favor of its continued use for NAS. To do so, we will consider a simple optimization formulation that reveals the following key takeaways:", "NAS is usually formulated as a bilevel optimization problem in which we are searching over some architecture domain A for the architecture a \u2208 A that achieves the lowest validation loss \u2113\u2c7d(w\u2090, a) after training weights w\u2090 that minimize the training loss \u2113\u1d40(\u22c5, a):", "First-generation NAS methods were astronomically expensive due to the combinatorially large search space, requiring the training of thousands of neural networks to completion. Then, in their 2018 ENAS (for Efficient NAS) paper, Pham et al. introduced the idea of weight-sharing, in which only one shared set of model parameters w \u2208 \u211d\u1d48 is trained for all architectures. The validation losses \u2113\u2c7d(w, a) of different architectures a \u2208 A computed using these shared weights are then used as estimates of their validation losses \u2113\u2c7d(w\u2090, a) using standalone weights w\u2090 (i.e. weights trained individually for each architecture by solving the inner optimization problem above). Because only one set of parameters has to be trained, weight-sharing led to a massive speedup over earlier methods, reducing search time on CIFAR-10 from 2,000\u201320,000 GPU-hours to just 16. Of great surprise to many was that the validation accuracies \u2113\u2c7d(w, a) computed using shared weights w were sufficiently good surrogates for \u2113\u2c7d(w\u2090, a), meaning ENAS was able to find good models cheaply. We will see that this correlation is actually a sufficient but not necessary condition for weight-sharing to do well and that weight-sharing\u2019s overall success is less surprising than initially thought.", "Following the ENAS breakthrough, several simpler methods such as DARTS and GDAS were proposed in which the categorical architecture decisions (e.g. for each network edge e \u2208 E, which of some fixed set of operations O to use at e are relaxed into continuous parameters \u03b8 \u2208 \u0398 (e.g. so \u0398 is a product of |E| simplices of size |O|). As animated in Figure 1, these architecture parameters govern the architecture used for updating the shared weights using the gradient of the training loss; for example, in DARTS, \u03b8\u2091\u2092 determines the weighting in the weighted sum of operations output by edge e in the network. Together, this parameterization leads to a continuous relaxation of the earlier bilevel problem:", "Since \u0398 is a constrained subset of \u211d\u1d31\u1d3c, DARTS and GDAS avoid simplex projections by instead re-parameterizing the \u201clogit\u201d parameters \u03b1 \u2208 \u211d\u1d31\u1d3c, with \u03b8 = Softmax(\u03b1) defined as", "The relaxed optimization problem can then be approximated via the following alternating gradient update scheme (here \u03b7 > 0 is a stepsize):", "Note that at the end of search, we need to recover a discrete architecture a \u2208 A from the architecture weights \u03b8; in DARTS this is done in a pruning step that simply chooses the highest-weighted operations. This post-hoc pruning is a source of error that our method, GAEA, ameliorates, as we discuss later.", "A further simplification, and perhaps the most striking example of using shared weights as surrogates for standalone weights, is the Random Search with Weight-Sharing (RS-WS) method, in which the shared parameters are optimized by taking gradient steps using architectures sampled uniformly at random from the search space. Despite not even updating architecture parameters, RS-WS achieved competitive and, in some cases, state-of-the-art results.", "More recently, weight-sharing has come under increased scrutiny: does sharing weights between models really accelerate NAS? Can it preclude the recovery of optimal architectures? In particular, several papers have observed the issue of rank disorder, which is when the shared-weight performance of architectures does not correlate well with their stand-alone performance; this issue is illustrated in the figure below. Rank disorder is a problem for methods such as RS-WS that rely on the shared-weights performance to rank architectures for evaluation, as it will cause them to ignore networks that achieve high accuracy when their parameters are trained without sharing.", "This skepticism has been reinforced by recent cases where well-known weight-sharing methods have performed poorly; in particular, DARTS was found to overfit to the upper-level validation set in a recent robustness evaluation, and both GDAS and DARTS were outperformed by standard hyperparameter optimization methods on NAS-Bench-201 given a similar time budget. Here DARTS had especially poor performance, converging to architectures consisting of only identity maps (skip-connections).", "Given the questions raised about weight-sharing and recent poor results, is it time to rethink its use? In the next section, we answer in the negative by showing that (a) correlation of the performance of the weight-sharing \u201csupernet\u201d with that of fully trained models is a sufficient but not necessary condition for weight-sharing to work, i.e. we need not be afraid of rank disorder, and (b) obtaining high-quality architectural solutions using weight-sharing mostly comes down to regularization and optimization, two well-understood aspects of machine learning. In the last section, we quickly highlight some recent work on regularization in NAS before presenting our own work looking at the optimization side.", "Weight-sharing is made possible by the fact that architecture parameters, unlike hyperparameters such as regularization and stepsize, directly affect the loss function, in that changing from one architecture a \u2208 A to a different architecture a\u2019 \u2208 A causes a change in the loss from \u2113(w, a) to \u2113(w,a\u2019), as in the latter case a different function is being used for prediction. On the other hand, changing the stepsize setting would not change the loss unless the weights were also changed by training w using the new stepsize; this would mean the weights were no longer shared by different hyperparameter settings.", "In fact, we can use the fact that architecture parameters can be subsumed as parameters of a supernet to pose NAS with weight-sharing as empirical risk minimization (ERM) over an extended class of functions encoded by both weights w \u2208 \u211d\u1d48 and architecture parameters \u03b8 \u2208 \u0398:", "Most (but not all) empirical work on NAS uses the bilevel formulation described earlier rather than solving this single-level ERM problem. However, we believe ERM should be the baseline object of study in NAS because it is better understood statistically and algorithmically; the more common bilevel optimization can be viewed as a (possibly critical) method of regularizing by splitting the data.", "The single-level formulation makes clear that, rather than rank disorder, NAS can fail for either of the usual reasons ERM fails: unsuccessful optimization or poor generalization. For example, these respective failures can largely explain the issues faced by DARTS on NAS-Bench-201 and NAS-Bench-1Shot1 that were discussed above. Of course, it is not surprising that supernets might face optimization and generalization issues, since they are non-convex and over-parameterized models; however, NAS practitioners are usually very comfortable training regular deep nets, which are also non-convex and have almost as many parameters. One major difference is that, in the latter case, we have had many years of intensive effort designing regularization schemes and optimization algorithms; if we were to dedicate a similar amount of NAS research to these two issues, then perhaps we would be no more afraid of weight-sharing than we are of training standard deep nets.", "One caveat to this discussion is that NAS has the additional challenge of recovering discrete architectures from continuously-relaxed architecture weights. The optimization scheme we propose next ameliorates this issue by promoting sparse architectural parameters in the first place.", "Our discussion above reduces the problem of designing NAS methods to that of designing good regularization and optimization schemes for the supernet. There has been a good amount of recent work on better regularization schemes for the supernet, including partial channel connections, penalizing the Hessian of the validation loss, and the bilevel formulation itself; we thus focus instead on improving the optimization scheme, which we do with our geometry-aware exponentiated algorithm (GAEA).", "As usual, we want an optimization method that can converge to a better solution faster. In the case of weight-sharing NAS, a high-quality solution will not only have good generalization but also induce sparse architecture weights \u03b8 \u2208 \u0398, which recall is related to the optimized parameters \u03b1 \u2208 \u211d\u1d31\u1d3c via a softmax. We expect sparse architecture parameters to be better because, as discussed earlier, the architecture parameters are rounded in a post-processing step to derive the final discrete architecture.", "In order to achieve this, we use exponentiated gradient descent to directly optimize over the elements \u03b8 \u2208 \u0398 instead of over unconstrained values \u03b1 \u2208 \u211d\u1d31\u1d3c. The update scheme replaces subtracting the gradient w.r.t. \u03b1 (line 4 in the pseudo-code) with element-wise multiplication by the negative exponentiated gradient w.r.t. \u03b8 (4.a), followed by projections to the simplices comprising \u03b8 (4.b):", "Note that each iteration is roughly as expensive as in SGD.", "For convex problems, exponentiated gradient is known to be well-suited for the simplex geometry, with iteration complexity depending only logarithmically on the size k of the simplex, rather than the O(k) dependence of gradient descent. Under the mirror descent view of this result for linear prediction ( video link), the improvement stems from the implicit regularizer encouraging larger updates when far away from a sparse target solution. For our non-convex problem, we obtain a similar guarantee by extending recent mirror descent results of Zhang & He to show that alternating the exponentiated update to the architecture parameters with SGD updates to the shared-weights yields an \u03b5-stationary-point in O(log k/\u03b5\u00b2) iterations. We also show experimentally in Figure 4 that this approach encourages sparser solutions than DARTS and PC-DARTS.", "Our GAEA approach, which is applicable to any method using the softmax formulation described earlier (this includes DARTS, GDAS, PC-DARTS, and others), can be summarized in two simple steps:", "So does the sparsity and faster convergence rates of GAEA result in better performance empirically? To test this, we simply apply the two steps above to modify existing state-of-the-art NAS methods. First, we evaluate GAEA applied to DARTS on the NAS-Bench-201 search space by Dong et al. Of the methods evaluated by Dong et al., non-weight-sharing methods outperformed weight-sharing methods on all three datasets. However, GAEA DARTS applied to the single-level ERM objective achieves the best accuracy across all three datasets, reaching near oracle-optimal performance on two of them. Notably, it fixes the catastrophically poor performance of DARTS on this space and is the only weight-sharing method that beats standard hyperparameter optimization.", "We also evaluated GAEA applied to PC-DARTS on the original DARTS CNN search space. With improved regularization for the weight-sharing optimization problem, PC-DARTS was able to recently match the performance of computationally expensive non-weight-sharing methods on CIFAR-10 and ImageNet. We are able to further boost the performance of PC-DARTS with GAEA and achieve state-of-the-art performance on this search space, demonstrating the importance of efficiently optimizing in the right geometry.", "To learn more about our results, weight-sharing, and NAS you can", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F72af458a735e&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fin-defense-of-weight-sharing-for-neural-architecture-search-an-optimization-perspective-72af458a735e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fin-defense-of-weight-sharing-for-neural-architecture-search-an-optimization-perspective-72af458a735e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fin-defense-of-weight-sharing-for-neural-architecture-search-an-optimization-perspective-72af458a735e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fin-defense-of-weight-sharing-for-neural-architecture-search-an-optimization-perspective-72af458a735e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----72af458a735e--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----72af458a735e--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@liamcli?source=post_page-----72af458a735e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@liamcli?source=post_page-----72af458a735e--------------------------------", "anchor_text": "Liam Li"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fec8ce5487f11&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fin-defense-of-weight-sharing-for-neural-architecture-search-an-optimization-perspective-72af458a735e&user=Liam+Li&userId=ec8ce5487f11&source=post_page-ec8ce5487f11----72af458a735e---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F72af458a735e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fin-defense-of-weight-sharing-for-neural-architecture-search-an-optimization-perspective-72af458a735e&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F72af458a735e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fin-defense-of-weight-sharing-for-neural-architecture-search-an-optimization-perspective-72af458a735e&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "http://www.cs.cmu.edu/~mkhodak/", "anchor_text": "Misha Khodak"}, {"url": "http://proceedings.mlr.press/v80/pham18a.html", "anchor_text": "weight-sharing method"}, {"url": "http://proceedings.mlr.press/v80/pham18a.html", "anchor_text": "paper"}, {"url": "https://arxiv.org/abs/1707.07012", "anchor_text": "2,000\u201320,000"}, {"url": "https://openreview.net/forum?id=S1eYHoC5FX", "anchor_text": "DARTS"}, {"url": "http://openaccess.thecvf.com/content_CVPR_2019/papers/Dong_Searching_for_a_Robust_Neural_Architecture_in_Four_GPU_Hours_CVPR_2019_paper", "anchor_text": "GDAS"}, {"url": "https://arxiv.org/abs/1902.07638", "anchor_text": "Random Search with Weight-Sharing"}, {"url": "https://openreview.net/forum?id=SJx9ngStPH", "anchor_text": "several"}, {"url": "https://arxiv.org/abs/2001.01431", "anchor_text": "papers"}, {"url": "https://arxiv.org/abs/2002.04289", "anchor_text": "observed"}, {"url": "https://openreview.net/forum?id=H1loF2NFwr", "anchor_text": "rank disorder"}, {"url": "https://openreview.net/forum?id=H1loF2NFwr", "anchor_text": "Yu et al., 2020"}, {"url": "https://openreview.net/forum?id=H1gDNyrKDS", "anchor_text": "robustness evaluation"}, {"url": "https://openreview.net/forum?id=HJxyZkBKDr", "anchor_text": "NAS-Bench-201"}, {"url": "https://openreview.net/forum?id=rylqooRqK7", "anchor_text": "not"}, {"url": "https://arxiv.org/abs/1909.11926", "anchor_text": "all"}, {"url": "https://www.automl.org/wp-content/uploads/2020/07/AutoML_2020_paper_46.pdf", "anchor_text": "bilevel optimization can be viewed as a (possibly critical) method of regularizing by splitting the data"}, {"url": "https://papers.nips.cc/paper/563-a-simple-weight-decay-can-improve-generalization", "anchor_text": "regularization"}, {"url": "http://jmlr.org/papers/v15/srivastava14a.html", "anchor_text": "schemes"}, {"url": "http://proceedings.mlr.press/v28/sutskever13.html", "anchor_text": "optimization"}, {"url": "https://arxiv.org/abs/1412.6980", "anchor_text": "algorithms"}, {"url": "https://openreview.net/forum?id=H1gDNyrKDS", "anchor_text": "partial channel connections"}, {"url": "https://openreview.net/forum?id=H1gDNyrKDS", "anchor_text": "penalizing the Hessian of the validation loss"}, {"url": "https://pdf.sciencedirectassets.com/272575/1-s2.0-S0890540100X00648/1-s2.0-S0890540196926127/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEKD%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIQD7%2BtHFNUL7RXYPJ0EDCm3b1MAsXlCnZB3Af0knFN7XfwIgX%2B71daM8A8Xs74oA1F9BN7L2MdX30ynAuwa2vi9JubYqtAMISRADGgwwNTkwMDM1NDY4NjUiDJrntPy%2Btpy9pllYliqRA%2FnQ1fCJZf1myhJd2TQzWswcCWAv%2FQntgZdm2QqNtxnhSv0nAByRpO3xi22KKJwg%2BCLPnYrZUqxa4P4SHmqvulAI%2Fxf3l6uZ7wspesGp%2FMDBv9V3kCK6dcK3w4mpNks6S8yWqzpaS0BxPXLKxA9KarZR2uoY%2B4ocQLNOe6XRWgI847lEb9P%2BTQMc01RszzqexHl9B2YiCkV9Tc7BYSWgn0F4k0gifjkESbKB1rQ%2BISmegkffzK1X%2FUgoO75yALnzAMu9qudwQzsfS%2Ftp%2BHwZxnvPnTh3JSguAzD4mA9S7UYSocZIAYVyajcCM6jZAqfMd9H4xCsvtvqnj6BvMft%2B1MzYOrSVNSJofR01U9oyCAapyLVg1QWyoMVjUk9hN0XpZjldXoX1MFSCS%2BBn6ZPJwXSpl6w4PSDG7u%2BBk6bMiZzsVIggEPd5rCELVeQFdxJrkqUAdoiK8H0zvHjtlFonXoVkr4%2Fe4h7%2BCLBMwdFAHFUvhbqEGQfJsS%2BNjyBzvKUsEmbISKWPjhzStsTMGtGztpvPMJKyp%2FgFOusBAe3cKtpSi6K%2Bl%2BD2rY16%2Fs80QxiEAFjOW4KHhWd3JFv8oUcum7KgOqmDtF3nfxBVRdkfQKZyg1fEIPMExldSSzxoJbseZBG8IvbZ7e41a0lORahGPSgHD69Anphbyb3XzmEX%2B7KdCGYbMDRWm6mO5%2FH8u1P9IplyiHZI4JdyK5Ofim9%2BYsdAmaF0RIxMncLkhpZ1MION6mtkn4yuzqpwW%2FCXui2EMWJY5hWUJj5sN%2B0QSkHPT%2BcLJ%2FalcKHEXFBbuS0X3XoVOopsvTZZUD5uNFQU7qOzgKOkL711LbBma0Lu%2Fcpx8fLHvRHhfQ%3D%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20200711T162208Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTYWZDVZUM7%2F20200711%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=40b87202e72cb5f0d6ce7e70cd60faf656a7510b700cbd87a7ca95230ea5eb20&hash=60545d1176fe9465628c11f98e59d807c0541a838531a443e9b9311d1fe87e06&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0890540196926127&tid=spdf-a878840f-3818-4937-b5e7-c08bf521cf0a&sid=749b22f7830723476d9a089-8d75fac41b81gxrqa&type=client", "anchor_text": "exponentiated gradient descent"}, {"url": "https://web.iem.technion.ac.il/images/user-files/becka/papers/3.pdf", "anchor_text": "iteration complexity"}, {"url": "https://www.youtube.com/watch?v=uylS0FiuCK4", "anchor_text": "video link"}, {"url": "https://arxiv.org/abs/1806.04781", "anchor_text": "Zhang & He"}, {"url": "https://openreview.net/forum?id=HJxyZkBKDr", "anchor_text": "NAS-Bench-201"}, {"url": "https://arxiv.org/abs/2004.07802", "anchor_text": "paper"}, {"url": "https://github.com/liamcli/gaea_release", "anchor_text": "Download our code"}, {"url": "http://proceedings.mlr.press/v80/pham18a.html", "anchor_text": "ENAS"}, {"url": "https://arxiv.org/abs/1902.07638", "anchor_text": "RS-WS"}, {"url": "http://jmlr.org/papers/v20/18-598.html", "anchor_text": "survey of the field"}, {"url": "https://determined.ai/blog/ws-optimization-for-nas/", "anchor_text": "https://determined.ai"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----72af458a735e---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----72af458a735e---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----72af458a735e---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/editors-pick?source=post_page-----72af458a735e---------------editors_pick-----------------", "anchor_text": "Editors Pick"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----72af458a735e---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F72af458a735e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fin-defense-of-weight-sharing-for-neural-architecture-search-an-optimization-perspective-72af458a735e&user=Liam+Li&userId=ec8ce5487f11&source=-----72af458a735e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F72af458a735e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fin-defense-of-weight-sharing-for-neural-architecture-search-an-optimization-perspective-72af458a735e&user=Liam+Li&userId=ec8ce5487f11&source=-----72af458a735e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F72af458a735e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fin-defense-of-weight-sharing-for-neural-architecture-search-an-optimization-perspective-72af458a735e&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----72af458a735e--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F72af458a735e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fin-defense-of-weight-sharing-for-neural-architecture-search-an-optimization-perspective-72af458a735e&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----72af458a735e---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----72af458a735e--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----72af458a735e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----72af458a735e--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----72af458a735e--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----72af458a735e--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----72af458a735e--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----72af458a735e--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----72af458a735e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@liamcli?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@liamcli?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Liam Li"}, {"url": "https://medium.com/@liamcli/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "12 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fec8ce5487f11&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fin-defense-of-weight-sharing-for-neural-architecture-search-an-optimization-perspective-72af458a735e&user=Liam+Li&userId=ec8ce5487f11&source=post_page-ec8ce5487f11--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Fec8ce5487f11%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fin-defense-of-weight-sharing-for-neural-architecture-search-an-optimization-perspective-72af458a735e&user=Liam+Li&userId=ec8ce5487f11&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}