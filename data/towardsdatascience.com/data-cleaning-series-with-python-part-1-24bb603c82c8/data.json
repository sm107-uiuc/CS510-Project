{"url": "https://towardsdatascience.com/data-cleaning-series-with-python-part-1-24bb603c82c8", "time": 1683004964.7561312, "path": "towardsdatascience.com/data-cleaning-series-with-python-part-1-24bb603c82c8/", "webpage": {"metadata": {"title": "Data cleaning series with Python: Part 1 | by Sayar Banerjee | Towards Data Science", "h1": "Data cleaning series with Python: Part 1", "description": "A few months ago, I was talking to a friend of mine who had recently applied for a Senior Data Science position in an IT company in Bangalore. As part of the interview process, he was presented with\u2026"}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/Gradient_boosting", "anchor_text": "certain algorithms", "paragraph_index": 4}, {"url": "https://en.wikipedia.org/wiki/Missing_data", "anchor_text": "here", "paragraph_index": 8}, {"url": "https://www.kaggle.com/uciml/pima-indians-diabetes-database", "anchor_text": "here", "paragraph_index": 11}, {"url": "https://en.wikipedia.org/wiki/Listwise_deletion", "anchor_text": "listwise deletion", "paragraph_index": 14}, {"url": "https://en.wikipedia.org/wiki/Power_(statistics)", "anchor_text": "power of statistical tests", "paragraph_index": 15}, {"url": "https://en.wikipedia.org/wiki/Errors_and_residuals", "anchor_text": "error", "paragraph_index": 26}, {"url": "https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm", "anchor_text": "K-Nearest Neighbors algorithm", "paragraph_index": 27}, {"url": "https://en.wikipedia.org/wiki/Metric_(mathematics)", "anchor_text": "metrics", "paragraph_index": 29}, {"url": "https://en.wikipedia.org/wiki/Taxicab_geometry", "anchor_text": "L1 norm", "paragraph_index": 29}, {"url": "https://en.wikipedia.org/wiki/Euclidean_distance", "anchor_text": "L2 norm", "paragraph_index": 29}, {"url": "https://en.wikipedia.org/wiki/Cosine_similarity", "anchor_text": "Cosine Similarity", "paragraph_index": 29}, {"url": "https://en.wikipedia.org/wiki/Jaccard_index", "anchor_text": "Jaccard Distance", "paragraph_index": 29}, {"url": "https://en.wikipedia.org/wiki/Hamming_distance", "anchor_text": "Hamming Distance", "paragraph_index": 29}, {"url": "https://en.wikipedia.org/wiki/Curse_of_dimensionality", "anchor_text": "curse of dimensionality", "paragraph_index": 30}, {"url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3074241/", "anchor_text": "MICE imputation", "paragraph_index": 31}, {"url": "https://www.linkedin.com/in/sayarbanerjee/", "anchor_text": "LinkedIn", "paragraph_index": 32}], "all_paragraphs": ["Garbage in, garbage out \u2014 Wilf Hey", "A few months ago, I was talking to a friend of mine who had recently applied for a Senior Data Science position in an IT company in Bangalore. As part of the interview process, he was presented with a dataset in order to solve a business problem. He began explaining his approach and was soon cut short by the interviewer. When I asked him the reason he said that, \u201cAll they cared about was my choice of algorithm to be used on the data.\u201d", "The field of Data Science has been around for quite some time now. In spite of that, there are still a lot of folks who understate the value of clean data. A machine learning algorithm is only as good as the data that is fed into it. Hence, it is essential to cleanse and wrangle your data before feeding it to your algorithms.", "This series will help instill best practices regarding data preprocessing towards newcomers in the field as well as provide a comprehensive refresher to the more established members of the community.", "Researchers often take very careful measures when performing studies. The primary outcome of a lot of such studies is to gather data. Unfortunately, a lot of research studies end up having missing information. It is imperative to deal with missing values as they can adversely affect the predictive capabilities of our machine learning algorithms. There are, however, certain algorithms which work their way around missing values.", "There are reasons galore for missing values in data. Participants of a survey dataset may choose not to share information, mistakes could be made by data entry personnel, automated machines gathering data could be faulty etc.", "We must accept human error as inevitable \u2014 and design around that fact \u2014 Donald Berwick", "Wikipedia defines three kinds of missing data:", "I will not go into explicit details about the intricacies but you can read about it here.", "Often, we find multiple representations of missing values in data. Sometimes they are represented by a single special character such as .(full stop), * (asterisk). Other times they can be represented by N/A , NaN or -999.", "Let\u2019s look at one such dataset.", "The dataset we will be using is a diabetes dataset from the UCI machine learning repository. It can be downloaded from here. Let us quickly have a look at it.", "The data has 768 observations/rows and 8 variables/columns. There also seems to be no missing values in it. Let us take care of that by randomly inserting missing values in each column. We will introduce missing values in every column by a factor of 0.1 or 10%.", "Notice how certain observations in the data are called NaN. This is the default notation for missing values in Python. Next, let's discuss a few techniques to handle such values.", "Also called listwise deletion, this technique simply involves removing the entire observation if it had one or more missing values. This method can be an option if you have a small number of missing values in a very large dataset.", "We observe that more than half of our dataset has been wiped off after listwise deletion. It looks like we have lost a lot of valuable information for our prospective models to learn. Additionally, it also affects power of statistical tests as they require a large sample size. Clearly, this method doesn\u2019t seem favorable in this case.", "In this method, all the missing values (represented by NaN) in the data are replaced with the number 0.", "The pandas library in Python has a method calledpandas.DataFrame.fillna that helps us accomplish this.", "At first, this method may seem like an attractive option as we are able to retain all of our observations. Moreover, it is a swift method for imputation. However, on closer inspection of the individual columns we can see that adding 0 to a bunch of rows will induce a lot of bias into the dataset. In reality, those missing entries may have values far from the number 0.", "Central value imputation is a method whereby missing values are replaced by their respective measures of central tendencies, a.k.a, Mean, Median, Mode. For numerical variables using either the Mean or Median is preferable whereas for categorical variables, Mode is used. The reason behind this is that for categorical variables, mean and median have no meaning because categorical variables have qualitative properties rather than quantitative ones. Hence, to account for central tendency, we use Mode as it is the most frequently occurring value.", "Python\u2019s scikit-learn library has a module called SimpleImputer which executes Central Value Imputation. The strategy parameter is used to set the type of imputation we require.", "This form of imputation has it\u2019s merits, especially with low-variance data. It may not be a good idea to impute this way if we have very volatile features. Nevertheless, it often serves as a fast and generally better method of imputation as compared to the previous techniques.", "So far, we have only looked at simple methods to deal with missing values. When our data gets complicated, we need better ways of imputation. In this section, we will look at a couple of our beloved learning algorithms and how they can help us impute missing values.", "Regression imputation is quite self-explanatory. This form of imputation uses the principles of multiple linear regression in order to impute values. If you need a refresher on linear regression then check out my series on linear regression.", "To summarize linear regression, we pick a dependent variable or output y and use a number of independent variables X to fit a function capable of predicting y. The variables in X are used as inputs or regressors in order to predict y.", "In Regression imputation, a column with missing values is picked as our y and the rest of the columns are used as input variables X to fit to y. This function is used to predict the missing values in column y.scikit-learn has an IterativeImputer module that performs a variation of regression imputation.", "While regression imputation seems viable, it also has limitations. Since we are using the fitted values as outputs for imputation, our imputed values will be extremely precise. This is because unlike normal linear regression models, the regression imputation is missing an error term. The uncertainty aspect of these imputed values is completely removed.", "Also called Knn imputation, this form of imputation uses the K-Nearest Neighbors algorithm in order to impute values.", "This algorithm is easy to explain. It looks at k number of entries and compares the \u201cdistance\u201d between them and the entry where there are missing values. The value for k here is defined by the user. A rule of thumb for a suitable value for k is taking the square root of the number of observations (N).", "The k-NN algorithm uses distance metrics to choose the nearest neighbors to our row/s of interest.The most common distance metrics used for numerical attributes are usually either Manhattan distance (L1 norm) or Euclidean distance (L2 norm). Among categorical variables, the most popular distance metrics are Cosine Similarity, Jaccard Distance and Hamming Distance. After computing distances, the missing value is determined by either majority voting (categorical) or mean/weighted mean (numeric) of the \u201cneighboring\u201d rows.", "k-NN imputation is a very effective method of imputation. It usually deals very good results for small, medium sized datasets. In large datasets, the run-time of the algorithm increases a lot. Moreover, with datasets that especially have a lot of features (columns) the curse of dimensionality problem kicks in.", "In this blog post,we take a look at a bunch of imputation methods in order to handle missing data. As practitioners in the Data Science field, we must be comfortable understanding the benefits and limitations of each of these methods. This will help us make informed decisions when building our data preprocessing pipelines. Note that the above list of imputation techniques isn\u2019t an exhaustive list. There are a number of other, advanced methods of treating missing values which are beyond the scope of this blog post. One such type of imputation is MICE imputation.", "If you enjoyed the post, please follow me on Medium and send me an invite on LinkedIn. Check out my other posts on Towards Data Science. Until next time. \u270b", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Graduate Student in Analytics at UC Davis | Data Scientist | Amateur crypto investor | UIUC Alumnus"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F24bb603c82c8&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-cleaning-series-with-python-part-1-24bb603c82c8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-cleaning-series-with-python-part-1-24bb603c82c8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-cleaning-series-with-python-part-1-24bb603c82c8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-cleaning-series-with-python-part-1-24bb603c82c8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----24bb603c82c8--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----24bb603c82c8--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://sayarbanerjee.medium.com/?source=post_page-----24bb603c82c8--------------------------------", "anchor_text": ""}, {"url": "https://sayarbanerjee.medium.com/?source=post_page-----24bb603c82c8--------------------------------", "anchor_text": "Sayar Banerjee"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9c7b7e60ff&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-cleaning-series-with-python-part-1-24bb603c82c8&user=Sayar+Banerjee&userId=9c7b7e60ff&source=post_page-9c7b7e60ff----24bb603c82c8---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F24bb603c82c8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-cleaning-series-with-python-part-1-24bb603c82c8&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F24bb603c82c8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-cleaning-series-with-python-part-1-24bb603c82c8&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@markusspiske?utm_source=medium&utm_medium=referral", "anchor_text": "Markus Spiske"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://en.wikipedia.org/wiki/Gradient_boosting", "anchor_text": "certain algorithms"}, {"url": "https://en.wikipedia.org/wiki/Missing_data", "anchor_text": "here"}, {"url": "https://www.kaggle.com/uciml/pima-indians-diabetes-database", "anchor_text": "here"}, {"url": "https://en.wikipedia.org/wiki/Listwise_deletion", "anchor_text": "listwise deletion"}, {"url": "https://en.wikipedia.org/wiki/Power_(statistics)", "anchor_text": "power of statistical tests"}, {"url": "https://unsplash.com/@emilymorter?utm_source=medium&utm_medium=referral", "anchor_text": "Emily Morter"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://towardsdatascience.com/linear-regression-moneyball-part-1-b93b3b9f5b53", "anchor_text": "Linear Regression: Moneyball \u2014 Part 1A statistical case study of the popular sports storytowardsdatascience.com"}, {"url": "https://en.wikipedia.org/wiki/Errors_and_residuals", "anchor_text": "error"}, {"url": "https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm", "anchor_text": "K-Nearest Neighbors algorithm"}, {"url": "https://unsplash.com/@jontyson?utm_source=medium&utm_medium=referral", "anchor_text": "Jon Tyson"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://en.wikipedia.org/wiki/Metric_(mathematics)", "anchor_text": "metrics"}, {"url": "https://en.wikipedia.org/wiki/Taxicab_geometry", "anchor_text": "L1 norm"}, {"url": "https://en.wikipedia.org/wiki/Euclidean_distance", "anchor_text": "L2 norm"}, {"url": "https://en.wikipedia.org/wiki/Cosine_similarity", "anchor_text": "Cosine Similarity"}, {"url": "https://en.wikipedia.org/wiki/Jaccard_index", "anchor_text": "Jaccard Distance"}, {"url": "https://en.wikipedia.org/wiki/Hamming_distance", "anchor_text": "Hamming Distance"}, {"url": "https://en.wikipedia.org/wiki/Curse_of_dimensionality", "anchor_text": "curse of dimensionality"}, {"url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3074241/", "anchor_text": "MICE imputation"}, {"url": "https://www.linkedin.com/in/sayarbanerjee/", "anchor_text": "LinkedIn"}, {"url": "https://scikit-learn.org/stable/modules/classes.html#module-sklearn.impute", "anchor_text": "https://scikit-learn.org/stable/modules/classes.html#module-sklearn.impute"}, {"url": "https://en.wikipedia.org/wiki/Imputation_(statistics)", "anchor_text": "https://en.wikipedia.org/wiki/Imputation_(statistics)"}, {"url": "https://medium.com/tag/data-preparation?source=post_page-----24bb603c82c8---------------data_preparation-----------------", "anchor_text": "Data Preparation"}, {"url": "https://medium.com/tag/data-preprocessing?source=post_page-----24bb603c82c8---------------data_preprocessing-----------------", "anchor_text": "Data Preprocessing"}, {"url": "https://medium.com/tag/data-science?source=post_page-----24bb603c82c8---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----24bb603c82c8---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/analysis?source=post_page-----24bb603c82c8---------------analysis-----------------", "anchor_text": "Analysis"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F24bb603c82c8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-cleaning-series-with-python-part-1-24bb603c82c8&user=Sayar+Banerjee&userId=9c7b7e60ff&source=-----24bb603c82c8---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F24bb603c82c8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-cleaning-series-with-python-part-1-24bb603c82c8&user=Sayar+Banerjee&userId=9c7b7e60ff&source=-----24bb603c82c8---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F24bb603c82c8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-cleaning-series-with-python-part-1-24bb603c82c8&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----24bb603c82c8--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F24bb603c82c8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-cleaning-series-with-python-part-1-24bb603c82c8&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----24bb603c82c8---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----24bb603c82c8--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----24bb603c82c8--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----24bb603c82c8--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----24bb603c82c8--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----24bb603c82c8--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----24bb603c82c8--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----24bb603c82c8--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----24bb603c82c8--------------------------------", "anchor_text": ""}, {"url": "https://sayarbanerjee.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://sayarbanerjee.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Sayar Banerjee"}, {"url": "https://sayarbanerjee.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "569 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9c7b7e60ff&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-cleaning-series-with-python-part-1-24bb603c82c8&user=Sayar+Banerjee&userId=9c7b7e60ff&source=post_page-9c7b7e60ff--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F6347ee85ea2d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-cleaning-series-with-python-part-1-24bb603c82c8&newsletterV3=9c7b7e60ff&newsletterV3Id=6347ee85ea2d&user=Sayar+Banerjee&userId=9c7b7e60ff&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}