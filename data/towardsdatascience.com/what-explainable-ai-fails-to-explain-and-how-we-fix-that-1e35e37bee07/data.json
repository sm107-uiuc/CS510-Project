{"url": "https://towardsdatascience.com/what-explainable-ai-fails-to-explain-and-how-we-fix-that-1e35e37bee07", "time": 1683005971.4699059, "path": "towardsdatascience.com/what-explainable-ai-fails-to-explain-and-how-we-fix-that-1e35e37bee07/", "webpage": {"metadata": {"title": "What Explainable AI fails to explain (and how we fix that) | by Alvin Wan | Towards Data Science", "h1": "What Explainable AI fails to explain (and how we fix that)", "description": "Don\u2019t take it from me. Take it from IEEE Fellow Cuntai Guan, who recognizes \u201cmany machine decisions are still poorly understood\u201d. Most papers even suggest a rigid dichotomy between accuracy and\u2026"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/pdf/1907.07374.pdf", "anchor_text": "many machine decisions are still poorly understood", "paragraph_index": 0}, {"url": "https://www.researchgate.net/publication/325398586_Explainable_Artificial_Intelligence_A_Survey", "anchor_text": "suggest a rigid dichotomy between accuracy and interpretability", "paragraph_index": 0}, {"url": "https://medium.com/datadriveninvestor/visualizing-neural-networks-using-saliency-maps-in-pytorch-289d8e244ab4", "anchor_text": "these", "paragraph_index": 4}, {"url": "https://medium.com/@thelastalias/saliency-maps-for-deep-learning-part-1-vanilla-gradient-1d0665de3284", "anchor_text": "saliency", "paragraph_index": 4}, {"url": "https://towardsdatascience.com/saliency-based-image-segmentation-473b4cb31774", "anchor_text": "tutorials", "paragraph_index": 4}, {"url": "https://github.com/utkuozbulak/pytorch-cnn-visualizations", "anchor_text": "Github", "paragraph_index": 4}, {"url": "https://github.com/PAIR-code/saliency", "anchor_text": "repositories", "paragraph_index": 4}, {"url": "https://arxiv.org/abs/2004.00221", "anchor_text": "paper", "paragraph_index": 8}, {"url": "http://nbdt.alvinwan.com", "anchor_text": "Neural-Backed Decision Trees", "paragraph_index": 9}, {"url": "https://arxiv.org/abs/2004.00221", "anchor_text": "paper", "paragraph_index": 17}, {"url": "https://arxiv.org/pdf/2004.00221.pdf", "anchor_text": "paper", "paragraph_index": 18}, {"url": "http://nbdt.alvinwan.com", "anchor_text": "view more example outputs online", "paragraph_index": 20}, {"url": "http://nbdt.alvinwan.com/demo/", "anchor_text": "try out our web demo", "paragraph_index": 20}, {"url": "https://images.pexels.com/photos/126407/pexels-photo-126407.jpeg?auto=compress&cs=tinysrgb&dpr=2&w=200", "anchor_text": "picture of a cat", "paragraph_index": 20}, {"url": "https://github.com/alvinwan/neural-backed-decision-trees/blob/master/nbdt/bin/nbdt", "anchor_text": "script for the command-line tool", "paragraph_index": 23}, {"url": "https://github.com/alvinwan/neural-backed-decision-trees", "anchor_text": "Github repository", "paragraph_index": 23}, {"url": "https://arxiv.org/abs/2004.00221", "anchor_text": "paper", "paragraph_index": 25}, {"url": "http://nbdt.alvinwan.com", "anchor_text": "project page", "paragraph_index": 27}, {"url": "http://alvinwan.com/", "anchor_text": "Alvin Wan", "paragraph_index": 28}, {"url": "https://github.com/lisadunlap", "anchor_text": "Lisa Dunlap", "paragraph_index": 28}, {"url": "https://github.com/daniel-ho", "anchor_text": "Daniel Ho", "paragraph_index": 28}, {"url": "https://www.linkedin.com/in/jihanyin/", "anchor_text": "Jihan Yin", "paragraph_index": 28}, {"url": "https://www.linkedin.com/in/scottjlee98/", "anchor_text": "Scott Lee", "paragraph_index": 28}, {"url": "https://www.linkedin.com/in/henryjin99/", "anchor_text": "Henry Jin", "paragraph_index": 28}, {"url": "https://spetryk.github.io/", "anchor_text": "Suzanne Petryk", "paragraph_index": 28}, {"url": "https://cs-people.bu.edu/sbargal/", "anchor_text": "Sarah Adel Bargal", "paragraph_index": 28}, {"url": "https://people.eecs.berkeley.edu/~jegonzal/", "anchor_text": "Joseph E. Gonzalez", "paragraph_index": 28}, {"url": "http://openaccess.thecvf.com/content_ICCV_2017/papers/Selvaraju_Grad-CAM_Visual_Explanations_ICCV_2017_paper.pdf", "anchor_text": "http://openaccess.thecvf.com/content_ICCV_2017/papers/Selvaraju_Grad-CAM_Visual_Explanations_ICCV_2017_paper.pdf", "paragraph_index": 30}, {"url": "http://bmvc2018.org/contents/papers/1064.pdf", "anchor_text": "http://bmvc2018.org/contents/papers/1064.pdf", "paragraph_index": 30}, {"url": "https://paperswithcode.com/sota/image-classification-on-imagenet", "anchor_text": "took 3 years", "paragraph_index": 32}, {"url": "https://wordnet.princeton.edu/", "anchor_text": "official website", "paragraph_index": 33}, {"url": "https://aaalv.in", "anchor_text": "https://aaalv.in", "paragraph_index": 37}], "all_paragraphs": ["Don\u2019t take it from me. Take it from IEEE Fellow Cuntai Guan, who recognizes \u201cmany machine decisions are still poorly understood\u201d. Most papers even suggest a rigid dichotomy between accuracy and interpretability.", "Explainable AI (XAI) attempts to bridge this divide, but as we explain below, XAI justifies decisions without interpreting the model directly. This means practitioners in applications such as finance and medicine are forced into a dilemma: pick an un-interpretable, accurate model or an inaccurate, interpretable model.", "Defining explainability or interpretability for computer vision is challenging: What does it even mean to explain a classification for high-dimensional inputs like images? As we discuss below, two popular definitions involve saliency maps and decision trees, but both approaches have their weaknesses.", "Many XAI methods produce heatmaps known as saliency maps, which highlight important input pixels that influence the prediction. However, saliency maps focus on the input and neglect to explain how the model makes decisions.", "For more on saliency maps, see these saliency tutorials and Github repositories.", "To illustrate why saliency maps do not fully explain how the model predicts, here is an example: Below, the saliency maps are identical, but the predictions differ. Why? Even though both saliency maps highlight the correct object, one prediction is incorrect. How? Answering this could help us improve the model, but as shown below, saliency maps fail to explain the model\u2019s decision process.", "Another approach is to replace neural networks with interpretable models. Before deep learning, decision trees were the gold standard for accuracy and interpretability. Below, we illustrate the interpretability of decision trees, which works by breaking up each prediction into a sequence of decisions.", "For accuracy, however, decision trees lag behind neural networks by up to 40% accuracy on image classification datasets\u00b2. Neural-network-and-decision-tree hybrids also underperform, failing to match neural networks on even the dataset CIFAR10, which features tiny 32x32 images like the one below.", "As we show in our paper (Sec 5.2), this accuracy gap damages interpretability: high-accuracy, interpretable models are needed to explain high-accuracy neural networks.", "We challenge this false dichotomy by building models that are both interpretable and accurate. Our key insight is to combine neural networks with decision trees, preserving high-level interpretability while using neural networks for low-level decisions, as shown below. We call these models Neural-Backed Decision Trees (NBDTs) and show they can match neural network accuracy while preserving the interpretability of a decision tree.", "NBDTs are as interpretable as decision trees. Unlike neural networks today, NBDTs can output intermediate decisions for a prediction. For example, given an image, a neural network may output Dog. However, an NBDT can output both Dog and Animal, Chordate, Carnivore (below).", "NBDTs achieve neural network accuracy. Unlike any other decision-tree-based method, NBDTs match neural network accuracy (< 1% difference) on 3 image classification datasets\u00b3. NBDTs also achieve accuracy within 2% of neural networks on ImageNet, one of the largest image classification datasets with 1.2 million 224x224 images.", "Furthermore, NBDTs set new state-of-the-art accuracies for interpretable models. The NBDT\u2019s ImageNet accuracy of 75.30% outperforms the best competing decision-tree-based method by a whole ~14%. To contextualize this accuracy gain: A similar gain of ~14% for non-interpretable neural networks took 3 years of research\u2074.", "The most insightful justifications are for objects the model has never seen before. For example, consider an NBDT (below), and run inference on a Zebra. Although this model has never seen Zebra, the intermediate decisions shown below are correct \u2014 Zebras are both Animals and Ungulates (hoofed animal). The ability to see justification for individual predictions is quintessential for unseen objects.", "Furthermore, we find that with NBDTs, interpretability improves with accuracy. This is contrary to the dichotomy in the introduction: NBDTs not only have both accuracy and interpretability; they also make both accuracy and interpretability the same objective.", "For example, the lower-accuracy ResNet\u2076 hierarchy (left) makes less sense, grouping Frog, Cat, and Airplane together. This is \u201cless sensible,\u201d as it is difficult to find an obvious visual feature shared by all three classes. By contrast, the higher-accuracy WideResNet hierarchy (right) makes more sense, cleanly separating Animal from Vehicle \u2014 thus, the higher accuracy, the more interpretable the NBDT.", "With low-dimensional tabular data, decision rules in a decision tree are simple to interpret e.g., if the dish contains a bun, then pick the right child, as shown below. However, decision rules are not as straightforward for inputs like high-dimensional images.", "As we qualitatively find in the paper (Sec 5.3), the model\u2019s decision rules are based not only on object type but also on context, shape, and color.", "To interpret decision rules quantitatively, we leverage an existing hierarchy of nouns called WordNet\u2077; with this hierarchy, we can find the most specific shared meaning between classes. For example, given the classes Cat and Dog, WordNet would provide Mammal. In our paper (Sec 5.2) and pictured below, we quantitatively verify these WordNet hypotheses.", "Note that in small datasets with 10 classes i.e., CIFAR10, we can find WordNet hypotheses for all nodes. However, in large datasets with 1000 classes i.e., ImageNet, we can only find WordNet hypotheses for a subset of nodes.", "Interested in trying out an NBDT, now? Without installing anything, you can view more example outputs online and even try out our web demo. Alternatively, use our command-line utility to run inference (Install with pip install nbdt). Below, we run inference on a picture of a cat.", "This outputs both the class prediction and all the intermediate decisions.", "You can load a pretrained NBDT in just a few lines of Python as well. Use the following to get started. We support several neural networks and datasets.", "For reference, see the script for the command-line tool we ran above; only ~20 lines are directly involved in transforming the input and running inference. For more instructions on getting started and examples, see our Github repository.", "The training and inference process for a Neural-Backed Decision Tree can be broken down into four steps.", "For more detail, see our paper (Sec 3).", "Explainable AI does not fully explain how the neural network reaches a prediction: Existing methods explain the image\u2019s impact on model predictions but do not explain the decision process. Decision trees address this, but unfortunately, images\u2077 are kryptonite for decision tree accuracy.", "We thus combine neural networks and decision trees. Unlike predecessors that arrived at the same hybrid design, our neural-backed decision trees (NBDTs) simultaneously address the failures (1) of neural networks to provide justification and (2) of decision trees to attain high accuracy. This primes a new category of accurate, interpretable NBDTs for applications like medicine and finance. To get started, see the project page.", "By Alvin Wan, *Lisa Dunlap, *Daniel Ho, Jihan Yin, Scott Lee, Henry Jin, Suzanne Petryk, Sarah Adel Bargal, Joseph E. Gonzalez", "[0] Designed by author Alvin Wan. Footnote exists to clarify we have rights to use this graphic.", "[1] There are two types of saliency maps: one is white-box, where the method has access to the model and its parameters. One popular white-box method is Grad-CAM, which uses both gradients and class activation maps to visualize attention. You can learn more from the paper, \u201cGrad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization\u201d http://openaccess.thecvf.com/content_ICCV_2017/papers/Selvaraju_Grad-CAM_Visual_Explanations_ICCV_2017_paper.pdf. The other type of saliency map is black-box, where the model does not have access to the model parameters. RISE is one such saliency method. RISE masks random portions of the input image and passes this image through the model \u2014 the mask that damages accuracy the most is the most \u201cimportant\u201d portion. You can learn more from the paper \u201cRISE: Randomized Input Sampling for Explanation of Black-box Models\u201d, http://bmvc2018.org/contents/papers/1064.pdf.", "[2] This 40% gap between decision tree and neural network accuracy shows up on TinyImageNet200.", "[4] This ImageNet accuracy gain is significant: for non-interpretable neural networks, a similar 14% gain on ImageNet took 3 years of research. To make this comparison, we examine a similar accuracy gain which took 3 years, from AlexNet in 2013 (63.3%) to Inception V3 (78.8%). The NBDT improves on previously state-of-the-art results by ~14% at around the same range, from NofE (61.29%) to our NBDTs (75.30%). There are other factors at play, however: One obvious one is that compute and deep learning libraries were not as readily available in 2013. A fairer comparison may to be use the latest the latest 14%-gain on ImageNet. The latest 14% gain took 5 years, starting from VGG-19 in 2015 (74.5%) and leading up to FixEfficientNet-L2 in 2020 (88.5%). However, this technically isn\u2019t comparable either since large gains are harder at higher accuracies. Despite this lack of perfectly comparable benchmark progress, we just took the minimum of the two ranges in time, to try and illustrate how large of a gap 14% is.", "[7] WordNet is a lexical hierarchy of various words. A large majority of words are nouns, but other parts of speech are included as well. For more information, see the official website.", "[8] To understand the basic idea for a Tree Supervision Loss: Horse is just one class. However, it is also an Ungulate and an Animal. (See the figure in \u201cJustifications for Individual Predictions\u201d.) At the root node, the Horse sample thus needs to be passed to the child node Animal. Furthermore, the node Animal needs to pass the sample to Ungulate. Finally, the node Ungulate must pass the sample to Horse. Train each node to predict the correct child node. We call the loss that enforces this the Tree Supervision Loss.", "[9] In general, decision trees perform best with low-dimensional data. Images are the antithesis of this best-case scenario, being extremely high-dimensional.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "PhD in AI at UC Berkeley, focusing on small neural networks in perception for autonomous vehicles. Big fan of cheesecake, corgis, Disneyland. https://aaalv.in"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F1e35e37bee07&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-explainable-ai-fails-to-explain-and-how-we-fix-that-1e35e37bee07&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-explainable-ai-fails-to-explain-and-how-we-fix-that-1e35e37bee07&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-explainable-ai-fails-to-explain-and-how-we-fix-that-1e35e37bee07&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-explainable-ai-fails-to-explain-and-how-we-fix-that-1e35e37bee07&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----1e35e37bee07--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1e35e37bee07--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@lvinwan?source=post_page-----1e35e37bee07--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@lvinwan?source=post_page-----1e35e37bee07--------------------------------", "anchor_text": "Alvin Wan"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8317d215973d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-explainable-ai-fails-to-explain-and-how-we-fix-that-1e35e37bee07&user=Alvin+Wan&userId=8317d215973d&source=post_page-8317d215973d----1e35e37bee07---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1e35e37bee07&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-explainable-ai-fails-to-explain-and-how-we-fix-that-1e35e37bee07&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1e35e37bee07&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-explainable-ai-fails-to-explain-and-how-we-fix-that-1e35e37bee07&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://arxiv.org/pdf/1907.07374.pdf", "anchor_text": "many machine decisions are still poorly understood"}, {"url": "https://www.researchgate.net/publication/325398586_Explainable_Artificial_Intelligence_A_Survey", "anchor_text": "suggest a rigid dichotomy between accuracy and interpretability"}, {"url": "https://medium.com/datadriveninvestor/visualizing-neural-networks-using-saliency-maps-in-pytorch-289d8e244ab4", "anchor_text": "these"}, {"url": "https://medium.com/@thelastalias/saliency-maps-for-deep-learning-part-1-vanilla-gradient-1d0665de3284", "anchor_text": "saliency"}, {"url": "https://towardsdatascience.com/saliency-based-image-segmentation-473b4cb31774", "anchor_text": "tutorials"}, {"url": "https://github.com/utkuozbulak/pytorch-cnn-visualizations", "anchor_text": "Github"}, {"url": "https://github.com/PAIR-code/saliency", "anchor_text": "repositories"}, {"url": "https://github.com/kazuto1011/grad-cam-pytorch", "anchor_text": "https://github.com/kazuto1011/grad-cam-pytorch"}, {"url": "https://arxiv.org/abs/2004.00221", "anchor_text": "paper"}, {"url": "http://nbdt.alvinwan.com", "anchor_text": "Neural-Backed Decision Trees"}, {"url": "https://arxiv.org/abs/2004.00221", "anchor_text": "paper"}, {"url": "https://arxiv.org/pdf/2004.00221.pdf", "anchor_text": "paper"}, {"url": "http://nbdt.alvinwan.com", "anchor_text": "view more example outputs online"}, {"url": "http://nbdt.alvinwan.com/demo/", "anchor_text": "try out our web demo"}, {"url": "https://images.pexels.com/photos/126407/pexels-photo-126407.jpeg?auto=compress&cs=tinysrgb&dpr=2&w=200", "anchor_text": "picture of a cat"}, {"url": "https://github.com/alvinwan/neural-backed-decision-trees/blob/master/nbdt/bin/nbdt", "anchor_text": "script for the command-line tool"}, {"url": "https://github.com/alvinwan/neural-backed-decision-trees", "anchor_text": "Github repository"}, {"url": "https://arxiv.org/abs/2004.00221", "anchor_text": "paper"}, {"url": "http://nbdt.alvinwan.com", "anchor_text": "project page"}, {"url": "http://alvinwan.com/", "anchor_text": "Alvin Wan"}, {"url": "https://github.com/lisadunlap", "anchor_text": "Lisa Dunlap"}, {"url": "https://github.com/daniel-ho", "anchor_text": "Daniel Ho"}, {"url": "https://www.linkedin.com/in/jihanyin/", "anchor_text": "Jihan Yin"}, {"url": "https://www.linkedin.com/in/scottjlee98/", "anchor_text": "Scott Lee"}, {"url": "https://www.linkedin.com/in/henryjin99/", "anchor_text": "Henry Jin"}, {"url": "https://spetryk.github.io/", "anchor_text": "Suzanne Petryk"}, {"url": "https://cs-people.bu.edu/sbargal/", "anchor_text": "Sarah Adel Bargal"}, {"url": "https://people.eecs.berkeley.edu/~jegonzal/", "anchor_text": "Joseph E. Gonzalez"}, {"url": "http://openaccess.thecvf.com/content_ICCV_2017/papers/Selvaraju_Grad-CAM_Visual_Explanations_ICCV_2017_paper.pdf", "anchor_text": "http://openaccess.thecvf.com/content_ICCV_2017/papers/Selvaraju_Grad-CAM_Visual_Explanations_ICCV_2017_paper.pdf"}, {"url": "http://bmvc2018.org/contents/papers/1064.pdf", "anchor_text": "http://bmvc2018.org/contents/papers/1064.pdf"}, {"url": "https://paperswithcode.com/sota/image-classification-on-imagenet", "anchor_text": "took 3 years"}, {"url": "https://wordnet.princeton.edu/", "anchor_text": "official website"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----1e35e37bee07---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/explainability?source=post_page-----1e35e37bee07---------------explainability-----------------", "anchor_text": "Explainability"}, {"url": "https://medium.com/tag/decision-tree?source=post_page-----1e35e37bee07---------------decision_tree-----------------", "anchor_text": "Decision Tree"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----1e35e37bee07---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/interpretability?source=post_page-----1e35e37bee07---------------interpretability-----------------", "anchor_text": "Interpretability"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1e35e37bee07&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-explainable-ai-fails-to-explain-and-how-we-fix-that-1e35e37bee07&user=Alvin+Wan&userId=8317d215973d&source=-----1e35e37bee07---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1e35e37bee07&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-explainable-ai-fails-to-explain-and-how-we-fix-that-1e35e37bee07&user=Alvin+Wan&userId=8317d215973d&source=-----1e35e37bee07---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1e35e37bee07&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-explainable-ai-fails-to-explain-and-how-we-fix-that-1e35e37bee07&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1e35e37bee07--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F1e35e37bee07&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-explainable-ai-fails-to-explain-and-how-we-fix-that-1e35e37bee07&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----1e35e37bee07---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----1e35e37bee07--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----1e35e37bee07--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----1e35e37bee07--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----1e35e37bee07--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----1e35e37bee07--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----1e35e37bee07--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----1e35e37bee07--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----1e35e37bee07--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@lvinwan?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@lvinwan?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Alvin Wan"}, {"url": "https://medium.com/@lvinwan/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "65 Followers"}, {"url": "https://aaalv.in", "anchor_text": "https://aaalv.in"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8317d215973d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-explainable-ai-fails-to-explain-and-how-we-fix-that-1e35e37bee07&user=Alvin+Wan&userId=8317d215973d&source=post_page-8317d215973d--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F479abd2cefed&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-explainable-ai-fails-to-explain-and-how-we-fix-that-1e35e37bee07&newsletterV3=8317d215973d&newsletterV3Id=479abd2cefed&user=Alvin+Wan&userId=8317d215973d&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}