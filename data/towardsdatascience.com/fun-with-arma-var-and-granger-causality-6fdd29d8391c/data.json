{"url": "https://towardsdatascience.com/fun-with-arma-var-and-granger-causality-6fdd29d8391c", "time": 1683014901.113037, "path": "towardsdatascience.com/fun-with-arma-var-and-granger-causality-6fdd29d8391c/", "webpage": {"metadata": {"title": "Fun with ARMA, VAR, and Granger Causality | by ARIMITRA MAITI | Towards Data Science", "h1": "Fun with ARMA, VAR, and Granger Causality", "description": "Wendy presumably suggested that the lucidity of sight during an eclipse in dimness is generously more clear than an ordinary night. Or maybe a normal night isn\u2019t anything when contrasted with the\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.youtube.com/watch?v=EY5OQ2iVA50", "anchor_text": "link", "paragraph_index": 2}, {"url": "https://towardsdatascience.com/stationarity-in-time-series-analysis-90c94f27322", "anchor_text": "here", "paragraph_index": 6}, {"url": "https://github.com/statsmodels/statsmodels/issues/6225", "anchor_text": "here", "paragraph_index": 21}, {"url": "https://sites.google.com/site/homepageasc/home", "anchor_text": "Chakrabarti", "paragraph_index": 48}], "all_paragraphs": ["Comparing what you see during an eclipse to the darkness at night is like comparing an ocean to a teardrop. ~Wendy Mass", "Wendy presumably suggested that the lucidity of sight during an eclipse in dimness is generously more clear than an ordinary night. Or maybe a normal night isn\u2019t anything when contrasted with the obscurity during a shroud. Similarly, I feel Time-series ideas stay in obscurity until Maths make it more understood. Even though the recurrence of using legitimate Maths in routine is practically similar to an obscuration, yet the intensity of clarifying Time-series through mathematical proofs isn\u2019t anything when contrasted with utilizing a measurable statistical tool to fight and envision some information. However, I wish to engage my readers with the \u201cteardrop\u201d today before they take a plunge into the ocean. The explanation being each night has the Moon, and strolling in the evening glow is precisely sentimental until science hands over some light to see through. The motivation behind this article is to see a tad bit of Time-series precisely through various tools (simply like each night) expecting the obscuration of numerical clarifications is uncommon and distant yet unquestionably important to take a gander at the sky in another manner.", "In literary terms, white noise is the sound produced by all frequencies of sound together at once. It can be heard and we must have heard one way or the other. Here is a link if we haven't. In our case white noise is a time series that satisfies three conditions; a series with zero mean, constant variance, and no correlation between two consecutive elements of the series. A white noise series is not predictable and hence an indication that it does not require any analysis further. To test the third condition we generally use an ACF (autocorrelation function) plot, which tests the correlation between two consecutive observations. If the correlation is non zero then the bars at that lag would cross the boundary lines on each side.", "The first series has an approximate zero mean and 1 variance, whereas the second series has an approximate mean of 0.5 and 0.7 variance. The ACF plot has no bands crossing the threshold boundary except 0th lag (which is the correlation of the observation with itself). The ACF plot of the second series has outright lagged correlations. Visually too, we can verify the variations change in the second series.", "Which one looks simpler? Which one better resembles a full-moon?", "All alkalis are bases but not all bases are not alkalis. ~School Chemistry", "Stationary time series follows almost similar conditions to the White Noise series. A stationary series must satisfy three conditions; a series with constant mean, constant variance, and no seasonality patterns. Basically, a series whose statistical properties such as mean, variance, autocorrelation are all constant over time. Hence all white noise series are essentially Stationary, but all Stationary series are not always White Noise, because the mean can be constant at greater than zero. A critical step for any time series modeling is to convert the original series into a stationary series before trying to fit the model. Without awaiting the eclipse it is fair to say that it is similar to something where Gauss Markov's assumptions are necessary before fitting an OLS model. Shay Palachy has made an excellent job here to explain the reason for stationarity and indeed the ocean is infinite. The bottom line is that estimating a model with non-stationary data may generate incorrect estimates of the model. We would next look at some examples which are not stationary series.", "Before we test the stationarity condition, the best thing is to be aware of the popular methods that can convert an original time series into a stationary series. The most traditional and convenient methods are termed as \u201cdifferencing\u201d and \u201cgrowth rate\u201d. There are also more customized methods of \u201cremoving trend component\u201d, \u201clevel shift detection\u201d etc which are by default incorporated in high-end time-series specific machine learning packages nowadays.", "Differencing is a very common practice to remove a random trend or seasonality from the series. The first order differencing implies simply taking the difference of current and previous observations at 1 lag, whereas second-order differencing does the same thing at lag 2. The growth rate method (sometimes referred to as One-period Simple return) is primarily used in financial or macroeconomic data (e.g. GDP per capita, Consumption Expenses). It takes the logarithm of current observation concerning the previous time period (e.g. log[xt\u00f7xt-1]).", "Augmented Dickey-Fuller (ADF) is the most commonly practiced stationarity test that follows its own Augmented Dickey-Fuller distribution instead of regular t-distribution or z-distribution. The Null hypothesis is that the series is Not Stationary whereas the Alternative hypothesis is that the series is Stationary.", "Did we notice that using both tools we got the same result, however still there are some differences in the p-values obtained from ADF tests in R & Python? Well, a probable answer may be the default lags used by both the software are different.", "ADF test at lags=0 becomes the Dickey-Fuller test. The result may or may not be similar all the time, however, in this particular example we observe similarity (i.e. raw series is not stationary whereas the transformed series is). While using Stata the Null hypothesis is the same, that the series is Not Stationary whereas the Alternative hypothesis is that the series is Stationary. The Decision Rule is if the Absolute value of the test statistic is less than 5% critical (from ADF/DF distribution) then we fail to reject Null and conclude that the series is not stationary. Hence the two charts on the left in the above figure have an absolute value less than the test statistic, due to which we Fail to Reject the Null hypothesis.", "Autoregressive (AR) models imply the current value of a series depends on the past values of the series. For a simple AR(1) model the current value of the series is assumed to depend on its first lag. The parameter coefficient value, corresponding standard errors, and its p-value imply the strength of the dependence. The error term is assumed to normally distributed with zero mean and constant variance of sigma squared. Whereas Moving Average (MA) models imply the current value of a series depends on past errors from the series. An AR model can be represented as an infinite MA model and vice versa. ARMA is a mixed model where current value accounts for both AR and MA effects from the series.", "George Box and Gwilym Jenkins provided a workflow to forecast time series data. Although the system has various other representations and customizations yet the basic overview would remain almost similar. This workflow can be extended to each level of detail in an analytics workbench depending on the nature of the problem and sophistication of the forecasting algorithm. The above flowchart is my representation only to remember the major steps and I urge my readers to make one of their own while working with more and more time-series problems.", "Before I take my readers further, I would like to mention that the Akaike Information Criterion (AIC), Bayesian Information Criterion (BIC) or Hannan-Quinn Information Criterion (HQIC) do the same job in helping to select the best model. We always strive for a less parsimonious model (have fewer parameters) but at the same choose a relatively lesser value of the above measures. The lesser the IC is, the better it is to minimize residuals. Although AIC is most common, yet BIC or HQIC is a little advanced and stronger for large data in terms of penalizing more parameters.", "AIC is minimum at ARMA (4,2), BIC is minimum at ARMA (2,0). BIC suggests a less parsimonious model with 3 parameters whereas the AIC model has 8 parameters. The sigma coefficient for ARMA(2,0) is 0.0052 which is more than ARMA(4,2) at 0.0049. The Maximum Log-likelihood for ARMA(2,0) is 461.51 which is lower than ARMA(4,2) at 466.40. However, at the cost of five extra parameters, we may select ARMA(2,0). Sigma is the constant standard deviation which the model assumes, for the innovations or random shocks (uncorrelated zero-mean random variables) that the model uses behind the scenes. In Stata it is sigma, but in R the same thing is shown as sigma squared.", "Although we aim to achieve a model that has a minimum random shock variance, yet choosing the red line would imply a relatively large number of parameters. The cost of this is higher than the blue line which has fewer parameters to maintain.", "Ease of Access: This is very clear because Stata is an authorized apparatus where both R and Python are open source. Henceforth getting to any open-source instruments is simpler than sitting tight for authorized programming.", "User-friendliness: Stata gives a graphical UI (GUI) which is the reason I some way or another vibe it is all the more inviting to an apprentice or a transitional. It is like SPSS, SAS, or Eviews. It is similar to SPSS, SAS, or Eviews. I somehow get to know my gaps in learning, the topics I need to learn, and the catalog of available techniques. On the contrary, both R and Python are scripting tools, they don't interact unless I take the initiative. Any device with a GUI at its front makes it less lumbering in starting days. The structure becomes more refined unlike stumbling upon a lot in a scripting tool. The focus shifts away from interpreting the output to actually getting the output in the first place. I guess this is a primary reason why any such tool comes at a high cost.", "Ease of Learning: The advantage of user-friendliness above makes it easy and fun to learn through Stata undoubtedly. However, in this case, the order of R & Python may not be hard-wired, because it depends on the order of one\u2019s exposure to the tool also. I got exposed to R first and Python later, however, things can be opposite to the majority too.", "Visualization: The comfort of python visualization facilities and its glamour are not debatable, considering it as an open-source freeware. On the contrary, R is obviously the successor. However, I somehow feel from this as well as a few other exercises that although ggplot comes to rescue many times compared to default plot methods, yet it is nicer to exploit python in the first go. It would be nicer if in python there is something similar to R time series plot functionality that can adjust x-axis labels automatically though. They did decommission sns.ts plots and introduced sns.lineplot() but it does not match every time with R.", "ARMA Estimation: Stata estimated better in this specific case. The warning message, \u201cpossible convergence problem: optimal gave code = 1\u201d is quite common in ARIMA/ARMA estimation through R, but in most cases, it does generate most of the outputs apart from p-values and standard errors. Added to this, heteroskedasticity corrected standard errors are somewhat difficult to achieve unless manually coded. Unlike R, heteroskedasticity corrected standard errors can be accessed easily in python, however, ARIMA function from statsmodels.tsa.arima_model surprisingly stops working most often. It forces us to use SARIMAX instead. That's why we got an ARMA(1,1) which was quite close to an ARMA(2,0) in Stata or R. Plain vanilla ARMA/ARIMA does not allow to override enforcing stationarity or invertibility due to lag polynomial inversion. Ideally, an ARMA process can be transformed into lag polynomial terms also and it can't be put optional in this function, unlike SARIMAX. A detailed discussion can be found here.", "Scalability: In most cases, python is preferred to any other tool. In this specific case, I somehow found the process of iterating across the orders to estimate model coefficients, and plotting the residuals on a 5 by 5 matrix is relatively easier and lighter than R. I would be dishonest to say that working in R for this example was also fun and I have no intention to demote this almost equally powerful tool.", "At first glance, VAR(s) appear to be straightforward multivariate generalizations of univariate autoregressive models. At second sight, they turn out to be one of the key empirical tools in modern macroeconomics. ~Del Negro and Schorfheide", "It is shown by researchers that an ARMA process of order (p,q) can be transformed into an AR process of order(p). Hence any AR(p) process of two or more series can be converted into a VAR(p) process. In our above example, we have seen two different series, GDP per capita and Consumption expenses for 120-time points. These two individual series would have its own ARMA(p,q) process. Based on Box-Jenkins we derived that once we achieve stationarity in each series we can iterate the series across different values of AR(p) and MA(q) and choose an order of ARMA(p,q) where we have a minimum AIC, BIC, or HQIC. In this case, the stationary series of GDP per capita is the GDP growth rate which follows an ARMA(2,0) or ARMA(1,1) depending on the tool we used. Also, the stationary series of Consumption expenses can be its growth rate of which follows an ARMA(1,0) process.", "The basic framework of a vector autoregression is required when we have multiple linear time series to deal with. If we consider two series X and Y, then VAR runs two regression models. The first system of equations regress X on the past lagged values of X as well as Y followed by some error terms of X. Similarly in the second system of equations, we regress Y on the past lagged values of Y as well as X followed by some error terms of Y. The order of lags can range from order 0 to order p just like an AR(p) model or in other words an ARMA(p,q) model. If we have 3 multiple linear time series to deal with then we would have 3 system of regression equations, if we have 4 multiple linear time series to deal with then we would have 4 system of regression equations, and so if have k multiple linear time series to deal with then we would have k system of regression equations.", "In 1956 Norbert Wiener introduced a concept where while measuring two linear time series, if one can predict the first series by using the past information from both the series better than just plain vanilla prediction of the first series using its past information alone without the second series, then one may infer that the second series causes the first one. The first practical work was done by Clive Granger after which the method is named Granger causality. Further advancements were also done by economist Gweke in 1982 and known as Gweke-Granger causality. Therefore this concept extends the use cases of VAR models further where one can statistically test if one time series is the cause of the other. If we have evidence that the first series granger causes the second one, we may infer that the first series is the cause and the second series is the effect. Being said that, the effect must depend on past values of the cause. Also, the past values of the cause must assist in identifying the current value of the effect in the presence of the past values of the effect.", "The above test is similar to ACF correlogram diagrams we saw post finding ARMA models. The objective is almost similar to check if errors coming out of the model have any correlation between them. In other words, we do not want any correction between an error of term of any given time point with its lagged error values. The Null hypothesis for this test is that Errors have no serial correlation whereas the Alternate hypothesis is Errors have a serial correlation. The decision rule is if p>0.05, then we fail to reject the null hypothesis and conclude that errors have indeed no serial correlation. The code for the test is given below.", "Once the VAR model is identified and estimated, we may have to test the causality hypothesis for VAR(1) model. The Null Hypothesis is there is no short-run causality from the Independent variable to the dependent variable. The Alternate Hypothesis is there is short-run causality from the Independent variable to the dependent variable. The decision rule is if p<0.05 then reject the Null and conclude there is causality, else conclude there is no short-run causality.", "It is advised to check whether the VAR model chosen is stable before the causality test. If the model is not stable then it may imply that either the series entered in the model are not stationary or there is a cointegration test required for long-run causality which essentially suggests building a Vector Error Correction Model. However, the VAR(1) chosen in this case is stable.", "In VAR(1) we observe there is conclusive evidence that overall there is some causality running from GDP growth rate towards the growth rate of Consumption expenses. Interestingly the vice versa (from Consumption growth to GDP growth) is not true.", "Therefore we have evidence that the growth rate in GDP Granger causes the growth rate in Consumption expenses. Being said that, the effect (growth rate in Consumption expenses) must depend on past values of the cause (growth rate in GDP). Also, the past values of the cause (growth rate in GDP) must assist in identifying the current value of the effect in the presence of the past values of the effect (growth rate in Consumption expenses).", "In Stata, we iterated VAR models for p ranging from 1 to 5 and collected the values of information criterion measures one by one. However, in R, the package \u201cvars\u201d allow us to mention the number of lags we want to use and it would provide us the best order depending on the information criterion. We have seen in the ARMA case that every tool would not evaluate the same values of information criterion and hence there may be some difference in the order suggested for some measures. In this case, we see somehow AIC suggests a VAR(3) model, unlike a VAR(1) suggested by Stata. However, the BIC (also known as Schwarz IC) and HQIC suggest a VAR(1) model in R. Therefore we would now fit a VAR(1) model to our combines series and look at its output.", "Once we have estimated the model in R and looked at the output briefly, its time to test if errors coming out of the model have a serial autocorrelation. The Null hypothesis for this test is that Errors have no serial correlation whereas the Alternate hypothesis is Errors have a serial correlation. The decision rule is if p>0.05, then we fail to reject the null hypothesis and conclude that errors have indeed no serial correlation.", "We are now aware that the errors from the VAR model have no serial correlation. Hence we do not need to revisit the model order of VAR, which implies we can test for causality.", "In VAR(1) we observe there is conclusive evidence that overall there is some causality running from GDP growth rate towards the growth rate of Consumption expenses. Interestingly the vice versa (from Consumption growth to GDP growth) is not true.", "Note: Instant causality is to test whether the future values of the cause (growth rate in GDP) must assist in identifying the current value of the effect in the presence of the past values of the effect (growth rate in Consumption expenses)", "We get almost the same suggestions as from R. We would now fit a VAR(1) model to our combines series and look at its output.", "Once we have estimated the model in Python and looked at the output briefly, its time to test if errors coming out of the model have a serial autocorrelation. The Null hypothesis for this test is that Errors have no serial correlation whereas the Alternate hypothesis is Errors have a serial correlation. The decision rule is if p>0.05, then we fail to reject the null hypothesis and conclude that errors have indeed no serial correlation. In the case of Python, we do get a p-value of 0.15 and hence we conclude Errors have no serial correlation.", "We are now aware that the errors from the VAR model have no serial correlation. Hence we do not need to revisit the model order of VAR, which implies we can test for causality.", "In VAR(1) we observe there is conclusive evidence that overall there is some causality running from GDP growth rate towards the growth rate of Consumption expenses. Interestingly the vice versa (from Consumption growth to GDP growth) is not true.", "We can execute the Instant Causality test in Python as well using the same function but with a different argument.", "VAR Estimation: All three tools responded similarly while estimating VAR(1) models. I did not find one overcoming the other by a big margin. In almost all tools the basic structure to select the lag order of VAR, estimating the suggested lag order through a VAR model and thereby testing autocorrelation, causality, and stability seemed lightweight.", "The output of the VAR Model: Post model estimation the output generated from Stata or the summary results from R is relatively more detailed than Python. For example, Python VAR output does not provide the model equations of the respective system equations. To get the results from residual autocorrelation results, stability, or causality, I found Python asks to enter attribute values individually which is a little cumbersome compared to Stata or R.", "The intent of this article is not to convey negligence on mathematical proofs and focus more on hands-on practice to master ARMA or VAR techniques. It is critical to explore a formal textbook with the assistance of experienced personnel and then venture into the practical exercises. However, the intent is to help my readers during their planning phase where we keep searching for a starting point and then get lost. The fun part of this article was to try out sample data through different tools and notice the gaps or sameness. The fun part was to focus on the notion of keeping more than one tool handy in our toolkit before a problem like this is identified and we decide to model the problem.", "Analysis of Financial Time Series by Ruey Tsay", "Time Series Analysis and its Applications by Shumway & Stoffer", "Web and Open Video platform sharing knowledge on Time Series Analysis", "Professor Anindya S. Chakrabarti, Economics Area, IIM-Ahmedabad", "If you can look into the seeds of time, and say which grain will grow and which will not, speak then unto me. ~ William Shakespeare", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Passionate Analytics Professional. Resolute in keeping the learning mindset alive forever. Diligent in shaping my perspective."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F6fdd29d8391c&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffun-with-arma-var-and-granger-causality-6fdd29d8391c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffun-with-arma-var-and-granger-causality-6fdd29d8391c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffun-with-arma-var-and-granger-causality-6fdd29d8391c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffun-with-arma-var-and-granger-causality-6fdd29d8391c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----6fdd29d8391c--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----6fdd29d8391c--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@arimitra.maiti?source=post_page-----6fdd29d8391c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@arimitra.maiti?source=post_page-----6fdd29d8391c--------------------------------", "anchor_text": "ARIMITRA MAITI"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb927ec346e3e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffun-with-arma-var-and-granger-causality-6fdd29d8391c&user=ARIMITRA+MAITI&userId=b927ec346e3e&source=post_page-b927ec346e3e----6fdd29d8391c---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6fdd29d8391c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffun-with-arma-var-and-granger-causality-6fdd29d8391c&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6fdd29d8391c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffun-with-arma-var-and-granger-causality-6fdd29d8391c&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@jakehills?utm_source=medium&utm_medium=referral", "anchor_text": "Jake Hills"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://www.youtube.com/watch?v=EY5OQ2iVA50", "anchor_text": "link"}, {"url": "https://gist.github.com/arimitramaiti/218783f86d984fd3081600d9331b7989", "anchor_text": "Code"}, {"url": "https://gist.github.com/arimitramaiti/87c24c4842386561341b92cbebef5ee4", "anchor_text": "Code"}, {"url": "https://towardsdatascience.com/stationarity-in-time-series-analysis-90c94f27322", "anchor_text": "here"}, {"url": "https://gist.github.com/arimitramaiti/bb8358faa99e863af5dda894ae142bfc", "anchor_text": "Link"}, {"url": "https://raw.githubusercontent.com/arimitramaiti/datasets/master/articles/a5_data.csv", "anchor_text": "Link"}, {"url": "https://gist.github.com/arimitramaiti/294378141e885447f420a1d8197154fd", "anchor_text": "Code"}, {"url": "https://raw.githubusercontent.com/arimitramaiti/datasets/master/articles/a5_data.csv", "anchor_text": "Link"}, {"url": "https://gist.github.com/arimitramaiti/87e9644e5e554e7853d22f8764652000", "anchor_text": "Code"}, {"url": "https://raw.githubusercontent.com/arimitramaiti/datasets/master/articles/a5_data.csv", "anchor_text": "Link"}, {"url": "https://gist.github.com/arimitramaiti/0c32eb4fd1d6614ed8c500a303d4f991", "anchor_text": "Code"}, {"url": "https://raw.githubusercontent.com/arimitramaiti/datasets/master/articles/a5_data.csv", "anchor_text": "Link"}, {"url": "https://raw.githubusercontent.com/arimitramaiti/datasets/master/articles/a5_data.csv", "anchor_text": "Link"}, {"url": "https://raw.githubusercontent.com/arimitramaiti/datasets/master/articles/a5_data.csv", "anchor_text": "Link"}, {"url": "https://raw.githubusercontent.com/arimitramaiti/datasets/master/articles/a5_data.csv", "anchor_text": "Link"}, {"url": "https://raw.githubusercontent.com/arimitramaiti/datasets/master/articles/a5_data.csv", "anchor_text": "Link"}, {"url": "https://raw.githubusercontent.com/arimitramaiti/datasets/master/articles/a5_data.csv", "anchor_text": "Link"}, {"url": "https://raw.githubusercontent.com/arimitramaiti/datasets/master/articles/a5_data.csv", "anchor_text": "Link"}, {"url": "https://raw.githubusercontent.com/arimitramaiti/datasets/master/articles/a5_data.csv", "anchor_text": "Link"}, {"url": "https://raw.githubusercontent.com/arimitramaiti/datasets/master/articles/a5_data.csv", "anchor_text": "Link"}, {"url": "https://www.google.com/search?q=stata+icon&safe=active&sxsrf=ALeKk009zuLZ9NYZfTQyI4sYOMciZ1OLYA:1602173259501&tbm=isch&source=iu&ictx=1&fir=e7JObCDad1xjWM%252CILUZVjjbJS8lFM%252C_&vet=1&usg=AI4_-kTA0h70sVhNVapDx1c8RWaun3iX8w&sa=X&ved=2ahUKEwi56IDAsKXsAhWazzgGHVH5DqQQ9QF6BAgNEFE&biw=1366&bih=625#imgrc=e7JObCDad1xjWM", "anchor_text": "Stata"}, {"url": "https://www.google.com/search?q=rstudio+icon&tbm=isch&ved=2ahUKEwiyufbCsKXsAhVQWisKHVNRAd8Q2-cCegQIABAA&oq=rstudio+icon&gs_lcp=CgNpbWcQAzICCAAyAggAMgYIABAHEB4yBggAEAcQHjIGCAAQBxAeMgYIABAFEB4yBggAEAUQHjIECAAQHjIECAAQGDIECAAQGDoECAAQQzoICAAQBxAFEB46BggAEAgQHlD_xQJYpdUCYMraAmgAcAB4AIABvwGIAb8GkgEDMC43mAEAoAEBqgELZ3dzLXdpei1pbWfAAQE&sclient=img&ei=UTl_X_LMJdC0rQHTooX4DQ&bih=625&biw=1366&safe=active#imgrc=eXBjG6YYM22OHM", "anchor_text": "R"}, {"url": "https://www.google.com/search?q=python+logo&tbm=isch&ved=2ahUKEwib1571sKXsAhUTRysKHUOmCwIQ2-cCegQIABAA&oq=python+logo&gs_lcp=CgNpbWcQAzIFCAAQsQMyBQgAELEDMgIIADICCAAyAggAMgIIADICCAAyAggAMgIIADICCAA6BAgjECc6BAgAEENQmYQDWK6QA2DLkgNoAHAAeACAAYMBiAGGB5IBAzAuOJgBAKABAaoBC2d3cy13aXotaW1nwAEB&sclient=img&ei=uzl_X9uNCJOOrQHDzK4Q&bih=625&biw=1366&safe=active#imgrc=TwZF3yMqP3cXhM", "anchor_text": "Python"}, {"url": "https://github.com/statsmodels/statsmodels/issues/6225", "anchor_text": "here"}, {"url": "https://raw.githubusercontent.com/arimitramaiti/datasets/master/articles/a5_data.csv", "anchor_text": "Link"}, {"url": "https://www.econometrics-with-r.org/16-1-vector-autoregressions.html", "anchor_text": "Link"}, {"url": "https://raw.githubusercontent.com/arimitramaiti/datasets/master/articles/a5_data.csv", "anchor_text": "Link"}, {"url": "https://raw.githubusercontent.com/arimitramaiti/datasets/master/articles/a5_data.csv", "anchor_text": "Link"}, {"url": "https://raw.githubusercontent.com/arimitramaiti/datasets/master/articles/a5_data.csv", "anchor_text": "Link"}, {"url": "https://raw.githubusercontent.com/arimitramaiti/datasets/master/articles/a5_data.csv", "anchor_text": "Link"}, {"url": "https://raw.githubusercontent.com/arimitramaiti/datasets/master/articles/a5_data.csv", "anchor_text": "Link"}, {"url": "https://raw.githubusercontent.com/arimitramaiti/datasets/master/articles/a5_data.csv", "anchor_text": "Link"}, {"url": "https://raw.githubusercontent.com/arimitramaiti/datasets/master/articles/a5_data.csv", "anchor_text": "Link"}, {"url": "https://raw.githubusercontent.com/arimitramaiti/datasets/master/articles/a5_data.csv", "anchor_text": "Link"}, {"url": "https://raw.githubusercontent.com/arimitramaiti/datasets/master/articles/a5_data.csv", "anchor_text": "Link"}, {"url": "https://raw.githubusercontent.com/arimitramaiti/datasets/master/articles/a5_data.csv", "anchor_text": "Link"}, {"url": "https://raw.githubusercontent.com/arimitramaiti/datasets/master/articles/a5_data.csv", "anchor_text": "Link"}, {"url": "https://sites.google.com/site/homepageasc/home", "anchor_text": "Chakrabarti"}, {"url": "https://medium.com/tag/data-science?source=post_page-----6fdd29d8391c---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/timeseries?source=post_page-----6fdd29d8391c---------------timeseries-----------------", "anchor_text": "Timeseries"}, {"url": "https://medium.com/tag/analytics?source=post_page-----6fdd29d8391c---------------analytics-----------------", "anchor_text": "Analytics"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6fdd29d8391c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffun-with-arma-var-and-granger-causality-6fdd29d8391c&user=ARIMITRA+MAITI&userId=b927ec346e3e&source=-----6fdd29d8391c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6fdd29d8391c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffun-with-arma-var-and-granger-causality-6fdd29d8391c&user=ARIMITRA+MAITI&userId=b927ec346e3e&source=-----6fdd29d8391c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6fdd29d8391c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffun-with-arma-var-and-granger-causality-6fdd29d8391c&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----6fdd29d8391c--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F6fdd29d8391c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffun-with-arma-var-and-granger-causality-6fdd29d8391c&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----6fdd29d8391c---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----6fdd29d8391c--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----6fdd29d8391c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----6fdd29d8391c--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----6fdd29d8391c--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----6fdd29d8391c--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----6fdd29d8391c--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----6fdd29d8391c--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----6fdd29d8391c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@arimitra.maiti?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@arimitra.maiti?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "ARIMITRA MAITI"}, {"url": "https://medium.com/@arimitra.maiti/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "53 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb927ec346e3e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffun-with-arma-var-and-granger-causality-6fdd29d8391c&user=ARIMITRA+MAITI&userId=b927ec346e3e&source=post_page-b927ec346e3e--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F1ed9c94685a4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffun-with-arma-var-and-granger-causality-6fdd29d8391c&newsletterV3=b927ec346e3e&newsletterV3Id=1ed9c94685a4&user=ARIMITRA+MAITI&userId=b927ec346e3e&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}