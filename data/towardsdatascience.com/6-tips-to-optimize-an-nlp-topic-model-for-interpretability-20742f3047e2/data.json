{"url": "https://towardsdatascience.com/6-tips-to-optimize-an-nlp-topic-model-for-interpretability-20742f3047e2", "time": 1683015851.911339, "path": "towardsdatascience.com/6-tips-to-optimize-an-nlp-topic-model-for-interpretability-20742f3047e2/", "webpage": {"metadata": {"title": "6 Tips for Interpretable Topic Models | by Nicha Ruchirawat | Towards Data Science", "h1": "6 Tips for Interpretable Topic Models", "description": "With so much text outputted on digital platforms, the ability to automatically understand key topic trends can reveal tremendous insight. For example, businesses can benefit from understanding\u2026"}, "outgoing_paragraph_urls": [{"url": "http://jmcauley.ucsd.edu/data/amazon/", "anchor_text": "Amazon Office Product reviews", "paragraph_index": 4}, {"url": "https://nicharuc.github.io/topic_modeling/", "anchor_text": "here", "paragraph_index": 5}, {"url": "https://nicharuc.github.io/topic_modeling/", "anchor_text": "topic_model.ipynb", "paragraph_index": 5}, {"url": "http://bit.ly/2HGWhl8", "anchor_text": "here", "paragraph_index": 7}, {"url": "https://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf", "anchor_text": "Cv", "paragraph_index": 13}, {"url": "https://nlp.stanford.edu/events/illvi2014/papers/sievert-illvi2014.pdf", "anchor_text": "pyLDAvis", "paragraph_index": 22}, {"url": "https://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf)", "anchor_text": "Exploring the Space of Topic Coherence Measures", "paragraph_index": 29}, {"url": "https://nlp.stanford.edu/events/illvi2014/papers/sievert-illvi2014.pdf", "anchor_text": "A method for visualizing and interpreting topics", "paragraph_index": 30}], "all_paragraphs": ["With so much text outputted on digital platforms, the ability to automatically understand key topic trends can reveal tremendous insight. For example, businesses can benefit from understanding customer conversation trends around their brand and products. A common method to pick up key topics is Latent Dirichlet Allocation (LDA). However, outputs are often difficult to interpret for useful insights. We will explore techniques to enhance interpretability.", "Latent Dirichlet Allocation (LDA) is a generative statistical model that helps pick up similarities across a collection of different data parts. In topic modeling, each data part is a word document (e.g. a single review on a product page) and the collection of documents is a corpus (e.g. all users\u2019 reviews for a product page). Similar sets of words occurring repeatedly may likely indicate topics.", "LDA assumes that each document is represented by a distribution of a fixed number of topics, and each topic is a distribution of words.", "Algorithm\u2019s high level key steps to approximate these distributions:", "We will explore techniques to optimize interpretability using LDA on Amazon Office Product reviews. To prepare the reviews data, we clean the reviews text with typical text cleaning steps:", "All code in the tutorial can be found here, where the functions for cleaning are located in clean_text.py. The main notebook for the whole process is topic_model.ipynb.", "Tip #1: Identify phrases through n-grams and filter noun-type structures", "We want to identify phrases so the topic model can recognize them. Bigrams are phrases containing 2 words e.g. \u2018social media\u2019. Likewise, trigrams are phrases containing 3 words e.g. \u2018Proctor and Gamble\u2019. There are many ways to detect n-grams, explained here. In this example, we will use Pointwise Mutual Information (PMI) score. This measures how much more likely the words co-occur than if they were independent. The metric is sensitive to rare combination of words, so it is used with an occurrence frequency filter to ensure phrase relevance. Bigram example below (trigram code included in Jupyter Notebook):", "Additionally, we filter bigrams or trigrams with noun structures. This helps the LDA model better cluster topics, as nouns are better indicators of a topic being talked about. We use NLTK package to tag part of speech and filter these structures.", "Lastly, we concatenate these phrases together into one word.", "Tip #2: Filter remaining words for nouns", "In the sentence, \u2018The store is nice\u2019, we know the sentence is talking about \u2018store\u2019. The other words in the sentence provide more context and explanation about the topic (\u2018store\u2019) itself. Therefore, filtering for noun extracts words that are more interpretable for the topic model. An alternative is also to filter for both nouns and verbs.", "Tip #3: Optimize choice for number of topics through coherence measure", "LDA requires specifying the number of topics. We can tune this through optimization of measures such as predictive likelihood, perplexity, and coherence. Much literature has indicated that maximizing a coherence measure, named Cv [1], leads to better human interpretability. We can test out a number of topics and asses the Cv measure:", "The improvement stops significantly improving after 15 topics. It is not always best where the highest Cv is, so we can try multiple to find the best result. We tried 15 and 23 here, and 23 yielded clearer results. Adding topics can help reveal further sub topics. Nonetheless, if the same words start to appear across multiple topics, the number of topics is too high.", "If your topics still do not make sense, try increasing passes and iterations, while increasing chunksize to the extent your memory can handle.", "chunksize is the number of documents to be loaded into memory each time for training.passes is the number of training iterations through the entire corpus. iterations is the maximum iterations over each document to reach convergence \u2014 limiting this means that some documents may not converge in time. If the training corpus has 200 documents, chunksize is 100, passes is 2, and iterations is 10, algorithm goes through these rounds:", "Each round will iterate each document\u2019s probability distribution assignments for a maximum of 10 times, moving to the next document before 10 times if it already reached convergence. This is basically algorithm\u2019s key steps 2\u20134 explained earlier, repeated for the number of passes, while step 3 is repeated for 10 iterationsor less.", "The topic distributions for entire corpus is updated after each chunksize, and after each passes. Increasing chunksize to the extent your memory can handle will increase speed as topic distribution update is expensive. However, increasing chunksize requires increasing number of passes to ensure sufficient corpus topic distribution updates, especially in small corpuses. iterations also needs to be high enough to ensure a good amount of documents reach convergence before moving on. We can try increasing these parameters when topics still don\u2019t make sense, but logging can also help debug:", "Look for a lines that look like this in the log, which will repeat for the number of passes that you set:", "By the end of the passes, most of the documents should have converged. If not, increase passes and iterations.", "Tip #5: Use pyLDAvis to visualize topic relationships", "The pyLDAvis [2] package in Python gives two important pieces of information. The circles represent each topic. The distance between the circles visualizes topic relatedness. These are mapped through dimensionality reduction (PCA/t-sne) on distances between each topic\u2019s probability distributions into 2D space. This shows whether our model developed distinct topics. We want to tune model parameters and number of topics to minimize circle overlap.", "Topic distance also shows how related topics are. Topics 1,2,13 clustered together talk about electronics (printers, scanners, phone/fax). Topics in quadrant 3 such as 6,14,19 are about office stationary (packaging materials, post-its, file organizer). Additionally, circle size represents topic prevalence. For example, topic 1 makes up the biggest portion of topics being talked about amongst documents, constituting 17.1% of the tokens.", "Tip #6: Tune relevancy score to prioritize terms more exclusive to a topic", "Words representing a given topic may be ranked high because they are globally frequent across a corpus. Relevancy score helps prioritize terms that belong more exclusively to a given topic, making the topic more obvious. The relevance of term w to topic k is defined as:", "where \u03d5_kw is the probability of word w in topic k and \u03d5_kw/p_kw is the lift in term\u2019s probability within a topic to its marginal probability across the entire corpus (this helps discards globally frequent terms). A lower \u03bb gives more importance to the second term (\u03d5_kw/p_kw), which gives more importance to topic exclusivity. We can again use pyLDAvis for this. For instance, when lowering \u03bb to 0.6, we can see that topic 13 ranked terms that are even more relevant to the topic of phones.", "Dial the lambda around to get the result that makes the most sense and apply the optimal lambda value to obtain the output:", "From here, we can further analyze sentiment around these topics keywords (e.g. search for adjectives or reviews star ratings associated). In business applications, this provides insight into which topics customers deem important, as well as how they feel about it. This enables targeted product development and customer experience improvements. This example contains a variety of products, but a separate topic model into each product may reveal aspects that customers care about. For example, this analysis already started to reveal important aspects of calculators (topic 21) such as display, easy to press buttons, battery, weight. Sellers then need to make sure to highlight these features in their product descriptions or improve upon these aspects for competitiveness.", "[1] Michael R\u00f6der, Andreas Both, Alexander Hinneburg, Exploring the Space of Topic Coherence Measures", "[2] Carson Sievert, Kenneth E. Shirley, LDAvis: A method for visualizing and interpreting topics", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F20742f3047e2&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F6-tips-to-optimize-an-nlp-topic-model-for-interpretability-20742f3047e2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2F6-tips-to-optimize-an-nlp-topic-model-for-interpretability-20742f3047e2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F6-tips-to-optimize-an-nlp-topic-model-for-interpretability-20742f3047e2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2F6-tips-to-optimize-an-nlp-topic-model-for-interpretability-20742f3047e2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----20742f3047e2--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----20742f3047e2--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@nicharuch?source=post_page-----20742f3047e2--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@nicharuch?source=post_page-----20742f3047e2--------------------------------", "anchor_text": "Nicha Ruchirawat"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff8ecd6cdacf9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F6-tips-to-optimize-an-nlp-topic-model-for-interpretability-20742f3047e2&user=Nicha+Ruchirawat&userId=f8ecd6cdacf9&source=post_page-f8ecd6cdacf9----20742f3047e2---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F20742f3047e2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F6-tips-to-optimize-an-nlp-topic-model-for-interpretability-20742f3047e2&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F20742f3047e2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F6-tips-to-optimize-an-nlp-topic-model-for-interpretability-20742f3047e2&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "http://jmcauley.ucsd.edu/data/amazon/", "anchor_text": "Amazon Office Product reviews"}, {"url": "https://nicharuc.github.io/topic_modeling/", "anchor_text": "here"}, {"url": "https://nicharuc.github.io/topic_modeling/", "anchor_text": "topic_model.ipynb"}, {"url": "http://bit.ly/2HGWhl8", "anchor_text": "here"}, {"url": "https://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf", "anchor_text": "Cv"}, {"url": "https://nlp.stanford.edu/events/illvi2014/papers/sievert-illvi2014.pdf", "anchor_text": "pyLDAvis"}, {"url": "https://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf)", "anchor_text": "Exploring the Space of Topic Coherence Measures"}, {"url": "https://nlp.stanford.edu/events/illvi2014/papers/sievert-illvi2014.pdf", "anchor_text": "A method for visualizing and interpreting topics"}, {"url": "https://medium.com/tag/data-science?source=post_page-----20742f3047e2---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/naturallanguageprocessing?source=post_page-----20742f3047e2---------------naturallanguageprocessing-----------------", "anchor_text": "Naturallanguageprocessing"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----20742f3047e2---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/nlp?source=post_page-----20742f3047e2---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/topic-modeling?source=post_page-----20742f3047e2---------------topic_modeling-----------------", "anchor_text": "Topic Modeling"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F20742f3047e2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F6-tips-to-optimize-an-nlp-topic-model-for-interpretability-20742f3047e2&user=Nicha+Ruchirawat&userId=f8ecd6cdacf9&source=-----20742f3047e2---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F20742f3047e2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F6-tips-to-optimize-an-nlp-topic-model-for-interpretability-20742f3047e2&user=Nicha+Ruchirawat&userId=f8ecd6cdacf9&source=-----20742f3047e2---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F20742f3047e2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F6-tips-to-optimize-an-nlp-topic-model-for-interpretability-20742f3047e2&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----20742f3047e2--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F20742f3047e2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F6-tips-to-optimize-an-nlp-topic-model-for-interpretability-20742f3047e2&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----20742f3047e2---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----20742f3047e2--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----20742f3047e2--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----20742f3047e2--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----20742f3047e2--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----20742f3047e2--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----20742f3047e2--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----20742f3047e2--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----20742f3047e2--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@nicharuch?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@nicharuch?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Nicha Ruchirawat"}, {"url": "https://medium.com/@nicharuch/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "199 Followers"}, {"url": "https://github.com/nicharuc/", "anchor_text": "https://github.com/nicharuc/"}, {"url": "https://www.linkedin.com/in/nicharuchirawat/", "anchor_text": "https://www.linkedin.com/in/nicharuchirawat/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff8ecd6cdacf9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F6-tips-to-optimize-an-nlp-topic-model-for-interpretability-20742f3047e2&user=Nicha+Ruchirawat&userId=f8ecd6cdacf9&source=post_page-f8ecd6cdacf9--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F93c3fdcb8688&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F6-tips-to-optimize-an-nlp-topic-model-for-interpretability-20742f3047e2&newsletterV3=f8ecd6cdacf9&newsletterV3Id=93c3fdcb8688&user=Nicha+Ruchirawat&userId=f8ecd6cdacf9&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}