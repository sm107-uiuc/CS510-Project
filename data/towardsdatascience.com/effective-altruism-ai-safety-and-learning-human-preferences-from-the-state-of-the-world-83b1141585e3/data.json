{"url": "https://towardsdatascience.com/effective-altruism-ai-safety-and-learning-human-preferences-from-the-state-of-the-world-83b1141585e3", "time": 1683015829.0258539, "path": "towardsdatascience.com/effective-altruism-ai-safety-and-learning-human-preferences-from-the-state-of-the-world-83b1141585e3/", "webpage": {"metadata": {"title": "Effective altruism, AI safety, and learning human preferences from the world\u2019s state | by Jeremie Harris | Towards Data Science", "h1": "Effective altruism, AI safety, and learning human preferences from the world\u2019s state", "description": "Editor\u2019s note: This episode is part of our podcast series on emerging problems in data science and machine learning, hosted by Jeremie Harris. Apart from hosting the podcast, Jeremie helps run a data\u2026"}, "outgoing_paragraph_urls": [{"url": "http://sharpestminds.com", "anchor_text": "SharpestMinds", "paragraph_index": 0}, {"url": "https://twitter.com/rohinmshah", "anchor_text": "follow Rohin on Twitter here", "paragraph_index": 7}, {"url": "https://rohinshah.com/alignment-newsletter/", "anchor_text": "AI alignment newsletter here", "paragraph_index": 7}, {"url": "https://twitter.com/jeremiecharris", "anchor_text": "follow me on Twitter here", "paragraph_index": 7}, {"url": "http://shorturl.at/jtMN0", "anchor_text": "shorturl.at/jtMN0", "paragraph_index": 118}], "all_paragraphs": ["Editor\u2019s note: This episode is part of our podcast series on emerging problems in data science and machine learning, hosted by Jeremie Harris. Apart from hosting the podcast, Jeremie helps run a data science mentorship startup called SharpestMinds. You can listen to the podcast below:", "If you walked into a room filled with objects that were scattered around somewhat randomly, how important or expensive would you assume those objects were?", "What if you walked into the same room, and instead found those objects carefully arranged in a very specific configuration that was unlikely to happen by chance?", "These two scenarios hint at something important: human beings have shaped our environments in ways that reflect what we value. You might just learn more about what I value by taking a 10 minute stroll through my apartment than by spending 30 minutes talking to me as I try to put my life philosophy into words.", "And that\u2019s a pretty important idea, because as it turns out, one of the most important challenges in advanced AI today is finding ways to communicate our values to machines. If our environments implicitly encode part of our value system, then we might be able to teach machines to observe it, and learn about our preferences without our having to express them explicitly.", "The idea of leveraging deriving human values from the state of an human-inhabited environment was first developed in a paper co-authored by Berkeley PhD and incoming DeepMind researcher Rohin Shah. Rohin has spent the last several years working on AI safety, and publishes the widely read AI alignment newsletter \u2014 and he was kind enough to join us for this episode of the Towards Data Science podcast, where we discussed his approach to AI safety, and his thoughts on risk mitigation strategies for advanced AI systems.", "Here were some of my favourite take-homes from our conversation:", "You can follow Rohin on Twitter here, sign up for his AI alignment newsletter here, or follow me on Twitter here.", "Jeremie (00:00):Hey, everyone. Welcome to another episode of the Towards Data Science podcast. My name is Jeremie and, apart from hosting the podcast, I\u2019m also on the team at the SharpestMinds data science mentorship program. I\u2019m really excited about today\u2019s episode because I\u2019ve been thinking about getting today\u2019s guest on the podcast for a very long time. I\u2019m so glad we finally made it happen.", "Jeremie (00:16):He\u2019s actually in between a transition right now from Berkley where he\u2019s wrapping up his PhD. He\u2019s working at the Center for Human Compatible AI, and he\u2019s transitioning over into DeepMind where he\u2019ll be doing some alignment work. His name is Rohin Shah. And, apart from being a very prolific researcher in AI and AI alignment in particular, he is also the publisher of the AI alignment newsletter which is a really, really great resource if you\u2019re looking to get oriented in the space to learn about some of the open problems and open questions in AI alignment. I really recommend checking it out.", "Jeremie (00:47):We\u2019re going to talk about a whole bunch of different things, including the philosophy of AI, the philosophy of machine learning and AI alignment, ways in which it can be accomplished, some of the challenges that exist, and we\u2019re going to explore one of the most interesting proposals, I think, that Rohin\u2019s come up with, which is an idea about extracting human preferences from the state of the environment. So, basically, the idea here is that humans, through their activity, have encoded their preferences implicitly in their environments, we do a whole bunch of different things, different actions, that reveal our preferences. And it would be great if we could have AIs look at the world and figure out what our preferences are implicitly based on the state of that world. And that might be a great way to bootstrap AI alignment efforts.", "Jeremie (01:30):We\u2019ll be talking about that proposal in depth, along with a whole bunch of other things. I\u2019m really looking forward to the episode, so I hope you enjoy. Without further ado, here we go.", "Jeremie (01:39):Hello, and thank you so much for joining us for the podcast.", "Jeremie (01:44):Well, I\u2019m very excited to have you. There\u2019s so many interesting things that you\u2019ve been working on in the alignment space in general. But, before we tackle some of the more technical questions, there\u2019s an observation that I think anybody who\u2019s spent any time working on alignment or talking to alignment researchers is going to end up making at some point, which is that the vast majority of people in the space seem to come from the effective altruism community. And I\u2019d love to get your take on, number one, what the effective altruism community is, what effective altruism is, and number two, why you think there\u2019s this deep connection between EA, effective altruism, and AI alignment and AI safety research.", "Rohin (02:20):Yeah, sure. The overarching idea effective altruism, the very easy to defend one that doesn\u2019t get into specifics, is with whatever money, time, resources, whatever you\u2019re willing to spend altruistically, you should try to do the most good you can with it, rather than\u2026 And you should think about that. It\u2019s pretty hard to argue against this. I don\u2019t think I\u2019ve really seen people disagree with that part.", "Rohin (02:56):Now, in practice, effective altruism the movement has whole bunch of additional premises that are meant to be in support of the skill, but are more controversial. I think the really fundamental big idea of effective altruism is that of cause prioritization. Many people will say, \u201cOkay, I want to have, say, clean water in Africa. I will work towards that.\u201d And they\u2019ll think about different ways in which you can get clean water in Africa, maybe you could try sanitizing the water that people already get, or you could try building some new treatment plants in order to provide fresh, flowing water that\u2019s drinkable to everyone. And they\u2019ll think about how best can they achieve their goal of delivering clean water.", "Rohin (03:47):It\u2019s much, much, much less common for people to think, \u201cOkay, well, should I work on getting clean water to people in Africa, or combating racism in the US? Which of these should I put my effort into? Or my money into?\u201d The main premise of effective altruism is you can, in fact, do this. There are actually significant differences between causes and, by thinking about it, have much more impact by selecting the right cause to work on in the first place.", "Rohin (04:20):It\u2019s very focused on this take weird\u2026 Take ideas seriously, actually evaluate them, figure out whether or not they are true and not whether or not they sound crazy. Whether or not they sound crazy does have some relation to whether or not they are true, but they are not necessarily the same. I think that ties into why it\u2019s also such a hotbed for AI safety research. The EA case for AI safety, for work on AI safety, is that AI has a good chance of being extremely influential in the next century, let\u2019s say.", "Rohin (05:00):There is some argument that is debatable, but it doesn\u2019t seem like you can rule it out. It seems like at least moderately likely that if we don\u2019t take care of how we do this, the AI system might \u201ctake over\u201d in the sense of all of the important decisions about the world are made by the AI system and not by humans. And one possible consequence of this is humans go extinct. I\u2019ll go into this argument later, I\u2019m sure, but-", "Rohin (05:37):[crosstalk 00:05:37] believe this argument somewhat, and so then it becomes extremely important and impactful to work on. It sounds crazy, but one of the EA\u2019s, to me, strengths is that it separates what sounds crazy from what is true.", "Jeremie (05:52):It seems like, really, the focus is there\u2019s this extra missing step that a lot of people don\u2019t apply to their thinking when deciding what causes to contribute to, what to work on, what to spend their lives on, and that is the step of going, \u201cWhat areas are going to give outsized returns on my time?\u201d", "Jeremie (06:12):I can really think back to most of the conversations I ever had with people about causes, about charity, and it\u2019s usually focused on stuff like what are the administrative fees associated with this charity? Oh, I want to donate to a place where all my dollars go to the cause, rather than asking the more fundamental question, is this cause actually going to give the best ROI from the standpoint of benefiting everyone or benefiting humanity? It\u2019s interesting that that kind of thinking, a more first principles approach, leads a lot of people to the area of AI alignment and AI safety. As you said, it makes sense, you\u2019ve got this super high risk high reward profile.", "Jeremie (06:50):What was it that drew you, for example, to AI alignment, AI safety work in particular, rather than any of the other, I could imagine, bio terrorism, I could imagine all kinds of horrible things that could happen to us, but why AI alignment in particular?", "Rohin (07:05):Yeah, so, my story is kind of weird. It may be a classic AI story in that convinced by weird very weird arguments. I got into effective altruism in 2014. I heard the arguments for AI risk within a year of that, probably. I was deeply unconvinced by them. I just did not buy them.", "Rohin (07:37):And, so, until 2017, I basically didn\u2019t engage very much with AI safety. I was also unconvinced of, basically, there\u2019s this field of ethics called population ethics which tries to deal with the question of how do you compare how good different worlds are when they have different populations of people in them? We don\u2019t need to go into the details, but it\u2019s a very confusing area. Lots of impossibility results that say you might want these six very intuitive properties, but, no, you can\u2019t actually have all of them at the same time, stuff like this. So you\u2019re [crosstalk 00:08:21]-", "Jeremie (08:20):Would the idea here be like is a world with 100 decently happy people better than a world with 1,000 decent minus epsilon happy people? Is that the kind of calculation?", "Rohin (08:31):Yes. That\u2019s an example of the question that I would deal with, yeah.", "Rohin (08:35):So, anyway, I was thinking about this question a lot in the summer of 2017. And, eventually, I was like, \u201cOkay, I think I should actually put a fair amount of weight,\u201d not certainty, certainly, but a fair amount of weight on the view that more happy people is, in fact, just means a better world, even if they\u2019re in the future. Once you put a decent probability on that, it starts looking overwhelmingly important to ensure that the future continues to exist and have happy people because it\u2019s just so big relevant to the present.", "Rohin (09:21):And so, then, I wanted to do something that was more future oriented and I had a ton of skills in computer science and math and, basically, everything you would want to work in AI alignment. I still was not very convinced of AI risk but I was like, \u201cOkay, a bunch of smart people have thought about this, maybe I should work on it for a while, see whether or not it makes sense.\u201d That\u2019s what caused me to actually switch, and a year later I actually started believing the arguments.", "Jeremie (10:00):You were led by\u2026 Is it the quality of the people who were drawn to the problem more so than necessarily the initial arguments themselves? Do you remember an ah-ha moment as you were working on this stuff where you\u2019re like, \u201cWell, wait a minute. This is actually for real.\u201d I can now see why Nick Bostrom, and maybe Eliezer Yudkowsky, and whoever else is talking about it back then was on to something?", "Rohin (10:21):I never really had an ah-ha moment. I remember, at one point, I was like, \u201cI guess I now believe these arguments,\u201d but it wasn\u2019t like I\u2026 I guess I now believe that AI risk is substantial and real. I can\u2019t point to a specific point in time where yes, now I believe it. I just, one day, was reflecting on it and noticed, \u201cOh, yeah. I used to not believe this. And now I do.\u201d", "Jeremie (10:52):That\u2019s so interesting. It seems like there\u2019s a bifurcation between people who they read Superintelligence, or they read less wrong, and they get really excited about the problem and really scared of it right off the bat because, for whatever reason, they\u2019re wired in such ways to have that happen. And then people, yeah, who are like you. It\u2019s like a slow burn and you ease into it. I guess this is part of the problem, almost, of articulating the problem if it takes that long to get people to think of this as a really important thing.", "Jeremie (11:18):Do you have a strategy that you use when you try to explain to people why is AI risk so serious? Why is the probability nontrivial that you think might\u2019ve worked on you back then to accelerate the process?", "Rohin (11:32):Yeah, I should note that I still\u2026 I\u2019m not super happy with the arguments in like Superintelligence, for example. I would say that it\u2019s slightly different arguments that are motivating it for me with still a fair amount of emphasis on things that were in Superintelligence.", "Jeremie (11:50):I think a lot of people won\u2019t have heard of Superintelligence, by the way.", "Jeremie (11:55):If you want to address any of the arguments that you raise, please feel free to give that background, too.", "Rohin (12:00):Yeah, maybe I\u2019ll just talk about the arguments I personally like since I can explain them better. But, just for context, Superintelligence is a book by a professor at Oxford named Nick Bostrom. It was published in 2014 and it was the first [inaudible 00:12:19] treatment of why AI risk is a thing that might occur and why we should think it might be plausible, what solutions might seem like they should be investigated and stuff like that.", "Rohin (12:33):And then, for me, personally, the argument I would give\u2026 So, A, we\u2019re going to [inaudible 00:12:45] as a premise that we build AI systems that are intelligent, like as intelligent as a human, let\u2019s say. We can talk about that later, but that\u2019s a whole other discussion. I\u2019ll just say that I think it is not\u2026 I think it is [inaudible 00:13:03] reasonably likely to happen in the next century. But, for now, take it as an assumption.", "Rohin (13:10):One thing about intelligence is it means that you can adapt to new situations, you\u2019re presented with a new situation, you learn about it and you do something and that something is coherent. It makes sense. One example I give of this, where we see this even with current neural nets, is a specific example from GPT-3. I believe viewers will be familiar with GPT-3\u2026 Listeners, not viewers. But if not, GPT-3 is a [inaudible 00:13:47] language generation [inaudible 00:13:48] that OpenAI developed and released recently.", "Rohin (13:51):I think I like one particular example which ones from the the post giving GPT-3 a Turing test. Where the context to GPT-3 was a bunch of questions and answers. And GPT-3 would pose a question, how may bonks are in a quoit. These are nonsense words, you did not mishear me. GPT-3, in some sense, this is outside of its training distribution. It has never seen this sentence in its training corpus, presumably. It may not have even seen the words bonk and quoit ever. It\u2019s actual distribution shift and you\u2019re relying on some sort of generalization out of distribution.", "Rohin (14:43):Nonetheless, I think we can all predict that GPT-3 is not going to output some random string of characters. It\u2019s going to probably say something sensible. In fact, the thing that it says is that, \u201cYou know, there are three bonks in a quoit.\u201d Why three? I have no idea. But, you know, it\u2019s sensible in some sense. It produced an answer that sounds like English.", "Jeremie (15:10):And we\u2019ve all been there to some degree, if we write exams or whatever, we\u2019re asked how many bonks are in a quoit, we haven\u2019t done our studying, and, hey, there are three bonks in a quoit. There we go.", "Rohin (15:18):Exactly, right? In some sense, GPT-3 did generalize, it generalized the way a student taking a test would. In the original post, this was taken as evidence of GPT-3 not actually being reasonable because it doesn\u2019t know how to say, \u201cThis question is nonsense.\u201d", "Rohin (15:39):But then a followup post was like, \u201cActually, you totally can get GPT-3 to do that!\u201d If you tell GPT-3 that\u2026 If in the context, you say whenever it sees a nonsense question the AI responds, \u201cYo, be real.\u201d Then, when it\u2019s asked how many bonks are in a quoit? It says, \u201cYo, be real.\u201d So you know it\u2019s got the ability to tell that this is nonsense, it just turned out that it generalized in a way where it was more like a test taker and less like somebody in conversation. Did we know that ahead of time? No, we did not. We had to actually run GPT-3 in order to figure this out.", "Rohin (16:23):I think AI risk is basically like this, but supercharged where your AI system, if it is human level intelligent, it\u2019s definitely going to be deployed in new areas, in new situations, that we haven\u2019t seen before. We just don\u2019t really have a compelling reason to believe that it will continue to do the thing that we were training it to do as opposed to something else. In GPT-3, what were we training it to do? Well, on the training data set, at least, we were training it to do whatever a human would write in that context.", "Rohin (17:05):When you see there are three bonks in a\u2026 Sorry, how many bonks are in a quoit? What would a human do in that circumstance? I don\u2019t know. It\u2019s not really well defined, and GPT-3 did something sensible. I don\u2019t think you could reasonably say it wasn\u2019t doing what we trained it to do, it just did something that was coherent. And, similarly, if you\u2019ve got AI systems that are human level intelligent or more, taking super impactful actions upon the world and they are put in these new situations where there\u2019s not really a fact of the matter about how they will generalize then they might take actions that have a high impact on the world that aren\u2019t what we want.", "Rohin (17:49):And then, as maybe intuition [inaudible 00:17:52] for why this could be really, really bad, like human extinction level bad. One particular distribution shift is you go from the training setting where humans have more power than the AI and can turn off the AI to the setting where the AI is sufficiently intelligent and sufficiently widely deployed, but no human can\u2026 Or humanity as a whole cannot turn it off. In that situation, that\u2019s a new situation. AI has never been in a situation where it had this sort of power before. Will it use it in some way that\u2026 Will it generalize in some way that was different than what we expected during training? We don\u2019t really have a reason to say no, it won\u2019t do that.", "Jeremie (18:33):Is there an analogy you think here with child rearing? I\u2019m just thinking of here intergenerational human propagation where our ancestors in the 1600s, at least in the west, I\u2019m sure would be absolutely disgusted by our vile ways today the way that we deal with sex, the way that we communicate to our elders, the way that we manage our institutions and so on, all our hierarchies are just completely different. And, in many ways, we\u2019re [inaudible 00:19:00] to the moral frameworks that were applied in the middle ages or the early renaissance.", "Jeremie (19:07):I guess there is a difference here in the sense that at least we are still running on the same fundamental hardware, or something very similar. Maybe that ensures a minimum level of alignment, but does this analogy break apart in some way?", "Rohin (19:18):I think that\u2019s a pretty good intuition. There are some ways that the analogy breaks, like for example, well\u2026 The analogy doesn\u2019t break so much as I would say put a little bit less weight on it for these reasons. One is, in child rearing, you have some influence over children, but you don\u2019t get to do a full training process where you give them gradients for every single time step of action that they ever do. You might hope that, given that you can have way, way, way more selection pressure on AI systems, you would be able to avoid this problem.", "Rohin (20:00):But, yes, I think that that is the same fundamental dynamic that I\u2019m pointing at. You have some amount of influence over these agents, but those agents that encounter new situations and they do something in those situations and you didn\u2019t think about those situations ahead of time and you didn\u2019t train them to do the right thing.", "Jeremie (20:23):I definitely buy the idea here that this AI risk, this is really significant risk. The stakes are very high. When it comes to the solutions or the strategies that you think are most promising, you yourself are specialized, obviously, in one category, everyone has to be, in one subspace within the alignment problem domain. What is it that the area that you\u2019ve decided to focus on and why do you think that is most deserving of the attention at this point?", "Rohin (20:50):The story that I\u2019ve told you so far is one of generalization. The main issue is we don\u2019t know how to generalize and, plausibly, you could get AI systems that are single mindedly pursuing power and that\u2019s similar to the Superintelligence story and those could cause human extinction. The fundamental mechanism is bad generalization, or generalization that\u2019s like your capabilities generalize. You do something coherent and high impact, but the thing you\u2019re trying to do doesn\u2019t generalize, relative to what humans wanted.", "Rohin (21:31):A lot of the things I\u2019m most excited about are somehow generalization related. One thing that I\u2019m interested in is can we get a better understanding of empirically how do neural nets tend to generalize? Can we say anything about this? There\u2019s a lot of theory that tries to explain why neural nets have as good generalization power as they do. It can\u2019t be explained by statistical learning theory because neural nets can memorize random noise, but nonetheless seem to generalize reasonably well on when the labels are not random noise.", "Jeremie (22:15):And do you mind explaining statistical learning theory as a reference? I\u2019m actually not so sure that I can to the connection.", "Rohin (22:23):Statistical learning theory is like a branch of machine learning theory that tries to do several things. But, among other things, try to prove that if we train a machine learning model on such and such training data with such and such training properties, then we know that it will generalize in such and such way and it proves theorems about this.", "Rohin (22:50):Importantly, most approaches right now focus on making assumptions about your model, your hypothesis class. These assumptions usually preclude the ability to overfit to an arbitrary sized data set because if you could, then you can\u2019t really say anything about generalization. But the fact of the matter is neural nets really can actually overfit to any data set. They can memorize labels that are literal random noise. And, so, these assumptions just don\u2019t apply to neural nets.", "Rohin (23:28):The thing I\u2019m excited about is can we talk about assumptions on the data set, rather than just the model? And using, if we think about assumptions on the data set and assumptions on the model, then can we say something about how neural nets tend to generalize? This is like a super vague not fleshed out hope that I have not really started working on, nor to my knowledge has anyone else.", "Rohin (23:55):There\u2019s just so many empirical things about neural nets that are so deeply confusing to me, like deep double descent. I don\u2019t get it. It\u2019s an empirical phenomenon. If you don\u2019t know, you can look it up. It\u2019s probably not that worth me going into, just so confusing. I don\u2019t know why it happens. It makes no sense to me. And I will want to know why, and I think that if we understood things like this, we might be able to start making statements about how neural nets tend to generalize that maybe that translates into things we can say about safety.", "Jeremie (24:27):That\u2019s interesting, because the generalization story seems to be one ingredient of the problem, of course, and then the other ingredient, which, I mean, there\u2019s some overlap, but it does seem like they have distinct components. Is this challenge of telling machines what human preferences even are, our ability to tell each other what we want out of life is already so limited, and it\u2019s something that, I mean, at least I personally find somewhat jarring as a prospect, having to actually not only express our preference, but quantify them and etch them into some kind of loss function that we then feed to a model. You\u2019ve done a lot of interesting work on this.", "Jeremie (25:09):And, actually, there\u2019s one of your papers that I wanted to talk about. We discussed this before we started recording and I was so glad to hear that it was also the one that you thought was the most interesting. We have compatible views, at least on that. It was this idea of\u2026 Well, the paper\u2019s title is Preferences Implicit in the State of the World. I guess, first, I wanted to ask a question to set the scene a little bit. What is preference learning? What is that concept?", "Rohin (25:34):This is actually the next thing I was going to say I was excited by.", "Rohin (25:39):Which is I\u2019ve talked about generalization, but before you get to generalization, you want to train on the right thing in the first place. That seems like a good starting point for AI system. If you don\u2019t have that, you\u2019re probably toast. Lots of ink has been spilled on how it\u2019s actually very typical to specify what you want by writing a program or an equation that captures it in a number which, as you know, how deep reinforcement learning, or any deep learning system, works. But it\u2019s most commonly associating with deep reinforcement learning.", "Rohin (26:21):The idea of preferenced learning is rather than having to specify what you want by writing down an equation, you specify it by some easier method. For example, you could look at two trajectories in a reinforcement setting, you can look at two behaviors that the agent took, and you can say, \u201cAh, yes, the left one. That one was better.\u201d That\u2019s giving the agent some feedback about what it should be doing. It\u2019s not trying to write down an equation that captures the ideal behavior in every possible scenario. It\u2019s just saying, out of these two, which one is better? You would imagine that that\u2019s easier for humans to do and more likely to be correct.", "Rohin (27:08):This preferenced learning field, I think of it as the field of how do we design mechanisms for humans to give feedback to an AI system, such that we can actually give feedback that incentivizes the behavior we actually want and we don\u2019t make as many mistakes in specification as we do with reward functions.", "Jeremie (27:34):So, what I find really exciting about that aspect, too, is there\u2019s this well known difference in humans between expressed desires and revealed desires, or expressed intent and revealed intent. I\u2019ll say I want to work out for three hours today, I want to do a bunch of coding, and I want to have a bunch of vegan meals for the next month. And then if you check in on me next month, I will have not have done all those things, I wouldn\u2019t have done nearly all those things. And the question is well, which me is me? Am I the aspirational self that said hey, I would love to be that person. Or am I the jackass who actually sat on his couch and watched Netflix the entire time?", "Jeremie (28:16):This seems to really scratch that itch in the sense that if probes are revealed preferences, for better or for worse, I guess that could also be a failure mode. Is that something that you see as valuable in this approach?", "Rohin (28:29):Yeah, I think you want to use both sources of information and not do either one. Actually, let me take a step back and distinguish between two different things you could be trying to do. There\u2019s one thing where you\u2019re trying to learn what humans value, which is the sort of thing that you\u2019re talking about, and there\u2019s another framing where you\u2019re just like, \u201cI want my AI system to do such and such task and I want to train it to do that, but I can\u2019t write down a reward function for that task.\u201d", "Rohin (29:01):I\u2019m actually more interested in the latter, honestly, but the former is also something I\u2019ve spent a lot of time on and I\u2019m excited by. Right now, we\u2019re talking about the former.", "Jeremie (29:13):Can I ask a na\u00efve question? I think I understand what the difference is, but I just want to put it to you to tackle it explicitly. What is the difference between those two things?", "Rohin (29:24):One thing is maybe I want my AI system to vacuum my floors, or something. The task of vacuuming my floors is not well specified just by that sentence. Anyone who has a Roomba will tell you stories of the Roomba being super dumb. Some of those are just the Roomba being not intelligent enough, but some are also the task is not super well specified.", "Rohin (29:57):Should the agent vacuum underneath a Christmas tree where there\u2019s a bunch of needles that might ruin their vacuum? Who knows. If there\u2019s some random shiny button on the floor, should it be vacuumed up or left alone? Because maybe that button\u2019s important. What sorts of things, should the cat ever be vacuumed? The cat has a lot of hair that gets everywhere. If you vacuum the cat, that seems like it would make your house cleaner.", "Rohin (30:31):There\u2019s lots of ambiguity here. I wouldn\u2019t really say that these are human values, like teaching your Roomba how to vacuum does not seem to be the same thing as teaching the Roomba about human values. For one thing, you can\u2019t really talk that much about revealed preferences here because I don\u2019t vacuum my house very often. If an AI system were going to queue vacuum, I might have it vacuum more often.", "Jeremie (31:06):Would you say this is a narrow application of human preferences? It almost seems like the distinction between narrow AI and AGI somehow maps onto this.", "Rohin (31:16):Yeah, and I think I agree with that. I would say, but in this sense, everything is narrow AI. You just get narrow AI that becomes more and more general, and at some point we decide to stop calling it narrow AI and start to call it AGI because of how broad it has become.", "Rohin (31:34):I like the idea of you start with something that can be applied to systems today and you just scale it up. It becomes more and more capable, more and more general, but it\u2019s always the same technique. Eventually, the systems that we create with it, we would label them as AGI or human level intelligence or super intelligent. It\u2019s the same technique, it\u2019s the same general principle. That\u2019s why I\u2019m more excited by this framing of the problem, rather than the human value spamming.", "Rohin (32:07):As you get to more general systems, it merges in with the human values. Once you get AI systems that are designing government policies or something, whatever feedback you\u2019re giving them, it better teach them about human values.", "Jeremie (32:23):Yeah, and hopefully, I guess, we start to do that at higher and higher levels of abstraction as you say as we climb that ladder. We fill in the convolutional filters in a sense as we go up.", "Rohin (32:34):Yes, exactly. You had asked a question about revealed preferences versus spoken preferences, or express preferences. I think, yes, this is an important distinction. I definitely want any method that we propose to not be dependent on one or the other but to instead be using both, and there will be conflicts, I\u2019m mostly hoping that we can just have AI systems that set aside the conflicts and do things that are robustly good according to either set. Probably, you\u2019ll have to have some amount of conflict resolution mechanism, but humans already have to do this in some sense. It seems plausible that we could do it.", "Rohin (33:28):I think it is a good, very nice aspect of this is that you don\u2019t have to commit yourself to finding the behavior up front in every possible situation. We just don\u2019t know this. Our values are not well defined enough, honestly, for that to be right. Our values are constantly in the process of being updated as we encounter new situations. Right now, we talk about democracy, one vote per person. If, someday, in the transhumanist future, if someday it becomes possible to just make copies of people, I think we would pretty quickly no longer want to have one vote per person. Because otherwise you can just pay to have anyone elected if you are sufficiently rich.", "Jeremie (34:19):Yeah. Or, I guess, just in the limited better information about brain states, we could say well, sure, this policy makes the majority of people happier, but the people it makes more unhappy, I mean, look at that horrible dopamine cycle. Those people are really taking a big hit and you wake those responses.", "Rohin (34:37):Yep, yeah, you could definitely optimize better for social welfare potentially, and maybe then you don\u2019t want to just have one vote per person.", "Jeremie (34:45):Right, now, I guess this brings us back to preferences implicit in the state of the world, there are things, presumably, about the structure of the world that reveal our, I guess this is revealed preferences, mostly, right?", "Rohin (34:58):Yep, this is definitely a revealed preferences method. I think an important aspect of this is people will\u2026 I think one of the reasons I\u2019m especially excited about this, which I want to say as a prelude, is that it\u2019s not trying to do the hard things. When people think about value learning, they think about should a self driving car if it has a choice between running into two passengers, or killing the driver, what should it do? Those are hard, ethical questions. I\u2019m less interested in them. I want to start with can we get an AI system that reliably knows that it should not kill humans. If there are two options where, yeah\u2026", "Rohin (35:50):Anyway, the basic stuff that we all agree on or nearly all agree on, and so I think looking at the state of the world is a good way to see this and the basic intuition here is that we\u2019ve been acting in the world for so long. We have preferences, we\u2019ve been rearranging the world to fit the way that we want the world to be. As a result, you can invert that process to figure out what things we probably want.", "Rohin (36:26):There\u2019s this nice toy example that illustrates this. Suppose there\u2019s a room, and in the middle of the room there is this breakable vase. And vases, once they\u2019re broken, they can never be repaired. We assume that the AI knows this. We\u2019re going to assume that the AI knows all empirical facts. It knows how the world works, it knows what actions the human can take, it knows what actions the human can take, it knows what actions it itself can take, it knows what states of the world are possible, but it doesn\u2019t know anything about the [inaudible 00:36:58] function which is the equivalent of human values.", "Rohin (37:02):It knows empirical facts. It knows that this vase, once broken, cannot be fixed. We\u2019re going to leave aside glue and things like that. It then looks at the fact that it sees, it is deployed in this room, and it sees that its human, who I\u2019ll call Alice, is in the room, the vase is unbroken. Now you can pose hypothetical questions like all right, well, what would I have expected to see if Alice wanted to break the vase? Well, I would have seen a broken vase. What would I have expected if Alice didn\u2019t care about the vase? Well, probably, at some point, the most efficient way would have been to just walk through the room while knocking over the vase. So, probably in that case, also I would have seen the broken vase.", "Rohin (37:58):What would I expect to see if Alice did not want the vase to be, or actively wanted the vase to not be broken? In that case, I actually see an unbroken vase, probably. Since I actually see an unbroken vase, that tells me that of those three situations, only the last one seems consistent with my observations. So, probably, Alice did not want to break the vase. You can infer this fact about that Alice doesn\u2019t want to break the vase just by looking at the state of the world and seeing that the vase is not broken.", "Jeremie (38:33):It seems like there\u2019s a very deep connection here to the second law of thermodynamics and the universe has there\u2019s so many more ways to end up in a situation where you have a broken vase, but the fact that there isn\u2019t a broken vase is a huge piece of information.", "Jeremie (38:59):Well, it just strikes me, the physicist instinct in me, but to the extent that the world looks any different from what we would expect with pure thermodynamic randomness, the assumption here is those differences come from human preferences. Would that be a fair way to characterize the\u2026", "Jeremie (39:19):And does that imply certain failure modes then? Because I guess we encode information in our environment, I guess this is [inaudible 00:39:25] revealed preferences thing, but implicitly, I\u2019ve hard coded my brain state into my apartment, every arrangement of things, any misogyny, any racism, any foot fetishes, the whole laundry list of weird quirks that may or may not be part of my personality are implicitly encoded in the room. Is this part of the risk of applying a technique like this?", "Rohin (39:54):Yeah, so, in theory, if you [inaudible 00:39:59] this method, it would\u2026 Is this going to get everything that is a revealed preference? And there are, well, I don\u2019t know that it gets everything. But to a first approximation, it gets your revealed preferences. I\u2019m sure there is some that it does not get. Sometimes, you just don\u2019t like your revealed preferences and you think they should be different.", "Rohin (40:27):You have a revealed preference, many people have a revealed preference, to procrastinate that they probably do not in fact endorse and they wouldn\u2019t want their AI system giving them more and more addictive material so that they can procrastinate better which is plausibly something that could happen. I would have to think significantly harder about how exactly concretely that could happen, but I could believe that that would be an effect.", "Rohin (41:03):Similarly, the technique as I\u2019ve explained it so far it seems that there is only one human in the world and things get a lot more complicated if there are multiple humans and I have just ignored that case so far.", "Jeremie (41:20):It\u2019s what you need to get the thing off the ground, right?", "Jeremie (41:26):In this context, I imagine at least, there\u2019s another risk mode which is if, by pure chance, let\u2019s say, in the example with the vase, let\u2019s say that the human actually doesn\u2019t care about the vase but just happens, in her demonstration, to have avoided the vase. Is there the risk that, I guess this is always a risk in machine learning, it sounds like just a case of added distribution sampling, like you would learn-", "Rohin (41:58):Yeah, that\u2019s right. If the vase is kept in an inconspicuous out of the way location where it\u2019s not actually that likely that Alice would have broken the vase on the course of moving around the room, we actually have this on the paper, we show that in that environment you actually don\u2019t learn anything significant about the vase. You\u2019re just like, \u201cEh, she probably didn\u2019t want it broken.\u201d You infer that she did not deeply desire for the vase to be broken, but you don\u2019t infer anything stronger than that. You\u2019re uncertain between whether it\u2019s bad to break vases rather versus yeah, it doesn\u2019t really matter.", "Rohin (42:45):It\u2019s more like if there were efficiency gains to be had by breaking the vase, then you infer and you observe that actually the vase wasn\u2019t broken, then you infer that it\u2019s bad to break vases. It\u2019s still possible that humans, we\u2019re not perfect optimal people, we might not pick up an efficiency gain and so we might go around a vase even though it would be faster to go to the vase even if we didn\u2019t care about the vase. And, yeah, this method would make an incorrect inference. In general, in preference learning, there\u2019s a big tension between you\u2019re assuming that humans do the things that the humans do to reflect what they want. And not always true.", "Jeremie (43:37):Right, and sometimes just for reasons of pure stupidity as well, I guess. We may want a thing, just not know how to make it happen.", "Rohin (43:45):Yup, exactly. That is a big challenge in preference learning and people have tried to tackle it, including me, actually. But I wouldn\u2019t say that there has been super substantial progress on separating stupidity from the things you actually wanted.", "Jeremie (44:10):I think we\u2019d end up solving a lot of other problems instead [crosstalk 00:44:12] if we do that. Actually, there\u2019s one more aspect I wanted to ask you about, with respect to the paper. The rule of time horizons, or the rule that time horizons play in the paper is, I think, just really interesting because there\u2019s certain assumptions that the robot makes or the AI makes about what is the time horizon that the human has in mind for this action that if there\u2019s assumptions about that time horizon shift, you start to see different behavior. I\u2019d love to hear you expand on that and describe that setting a little bit.", "Rohin (44:43):I think the main takeaway from that I think I would have about time horizons is if you assume a short time horizon, then cases where the state hasn\u2019t been fully optimized are much more excusable because the human just hasn\u2019t had enough time to fully optimize the state towards what would be optimal for them and so you can-", "Jeremie (45:11):And so maybe I should fill in that gap, I realize it was a little ambiguous, but by time horizon, I guess we\u2019re talking about the amount of time the human would have to, say, go from one point in the room to a desired end point, right? [crosstalk 00:45:24]", "Rohin (45:23):Yeah, it\u2019s like the amount of time that the robot assumes the human has been acting in the environment before the robot showed up. In the room case, it\u2019s robot is deployed and sees an intact vase and it\u2019s like, \u201cAh, yes, the human has been walking around this room for an hour,\u201d or something like that.", "Jeremie (45:40):Right, if you\u2019ve been walking around the room for a full hour and the vase is still there, you can then assume that the vase is probably pretty important.", "Rohin (45:48):Yeah, something along those lines, exactly. The actual setting in the paper is slightly different, but that\u2019s the right intuition. Yeah, and so this isn\u2019t illustrated best with the vase example, but imagine you\u2019re trying to build a house of cards. This is another example where the state of the world is really very informative about your preferences. House of cards are super, super not entropic. You can infer a lot.", "Jeremie (46:21):Yeah, the more specific the arrangement, I guess the more\u2026 Which is interesting because that\u2019s exactly what\u2019s so challenging about preserving humanity in general, there\u2019s almost a philosophically conservative streak to this approach in the sense that we\u2019re assuming that we\u2019ve gotten to somewhere that\u2019s worth preserving because we\u2019ve encoded so much of ourselves, so much of what\u2019s good about us already in the environment and it almost seems like what I love about this time horizon stuff is the political philosophy behind it, it almost gives you a dial that you can tune from the progressive to the conservative end of the spectrum just by assuming different time horizons. If you assume that we just got here and it\u2019s sort of blank slate, then, hey, we can try anything. We\u2019re not really certain about what humans want in this environment, conversely\u2026", "Rohin (47:13):That\u2019s true, I\u2019ve never actually thought about it that way, but you\u2019re right. That is basically what it is. Another way of thinking about it is the way I actually got to this point, to the point of writing this paper, was asking myself why is it that we privilege the do nothing action? We\u2019ll say that the safe action is to do nothing. Why? It\u2019s just an action. This is an answer, we\u2019ve already optimized the environment, random actions are probably, so\u2026 The current state is high on our preference ranking. Random actions take us out of that state into some random other state, so probably an expectation to go lower in our ranking whereas the do nothing action preserves it and so it\u2019s good. The longer the time horizon, the stronger you want to do nothing as a default.", "Jeremie (48:07):Yeah, yeah, I remember reading that in the paper, actually, as almost a derivation of that intuition which is it\u2019s so beautiful when you can see it laid out like that.", "Jeremie (48:18):Yeah, yeah. In a way, it makes me think of so many arguments among and between people of different political stripes could be so much easier if we applied toy models like this where you can say, well, hey, there is value to the conservative. There is value to the progressive. We end up in a dystopia either way, and here\u2019s the parameter we can tune to see how dystopic things get one way or the other, depending on how much we value things.", "Jeremie (48:43):Yeah, anyway I love the work and I thought it was\u2026 Anyway, again, one of these ah-ha moments, for anybody who\u2019s interested in the intersection between philosophy, moral philosophy, and then an AI, anyway, it\u2019s just such a cool piece of work.", "Rohin (48:58):Yeah, thanks. I like it for basically the same reasons.", "Jeremie (49:03):Sweet. Well, I\u2019m glad we have compatible [inaudible 00:49:06], then. Awesome. Well, I think we\u2019ve covered a lot of the bases here, but was there anything else you wanted to talk about? One thing I do want to make sure we get to is a reference to the alignment newsletter that you put out. I think everybody should check that out, especially if you\u2019re looking to get into the space. Rohin puts out this amazing newsletter and anyway, we\u2019ll link to it on the blog post that will come with the podcast.", "Jeremie (49:30):Did you have any social media links or things like that that you want to share?", "Rohin (49:35):I think the alignment newsletter is the best way to get my current thinking on things. If you\u2019re new to the space, I\u2019d probably recommend other things. Specific ranking of mine that I like as more introductory\u2026 It\u2019s not exactly introductory, but more timeless material, on the alignment forum there is a sequence of blog posts called the value learning sequence that I wrote. I like that as a good introduction, there are two other recommended sequences on that forum that I also recommend, that I also think are pretty great.", "Rohin (50:21):In terms of social media, I have a Twitter. It\u2019s @RohinMShah, but mostly it just sends out the alignment newsletter links. People can also feel free to e-mail me. My e-mail\u2019s on my website, can\u2019t guarantee that I will send you a response because I do actually get a lot of e-mail but I think I have a fairly high response rate.", "Jeremie (50:50):Yeah, well, I can vouch for that at my end. Thanks for making the time, really appreciate it, and I\u2019m really looking forward actually to putting this out and also good luck with DeepMind because you\u2019re heading over there in a couple of days, really, right?", "Jeremie (51:09):All right, yeah, enjoy the long weekend such as it is.", "Co-founder of Gladstone AI \ud83e\udd16 an AI safety company. Author of Quantum Mechanics Made Me Do It (preorder: shorturl.at/jtMN0)."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F83b1141585e3&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Feffective-altruism-ai-safety-and-learning-human-preferences-from-the-state-of-the-world-83b1141585e3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Feffective-altruism-ai-safety-and-learning-human-preferences-from-the-state-of-the-world-83b1141585e3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Feffective-altruism-ai-safety-and-learning-human-preferences-from-the-state-of-the-world-83b1141585e3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Feffective-altruism-ai-safety-and-learning-human-preferences-from-the-state-of-the-world-83b1141585e3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://podcasts.apple.com/ca/podcast/towards-data-science/id1470952338?mt=2", "anchor_text": "APPLE"}, {"url": "https://www.google.com/podcasts?feed=aHR0cHM6Ly9hbmNob3IuZm0vcy8zNmI0ODQ0L3BvZGNhc3QvcnNz", "anchor_text": "GOOGLE"}, {"url": "https://open.spotify.com/show/63diy2DtpHzQfeNVxAPZgU", "anchor_text": "SPOTIFY"}, {"url": "https://anchor.fm/towardsdatascience", "anchor_text": "OTHERS"}, {"url": "https://towardsdatascience.com/tagged/tds-podcast", "anchor_text": "TDS podcast"}, {"url": "https://medium.com/@JeremieHarris?source=post_page-----83b1141585e3--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----83b1141585e3--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@JeremieHarris?source=post_page-----83b1141585e3--------------------------------", "anchor_text": "Jeremie Harris"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F59564831d1eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Feffective-altruism-ai-safety-and-learning-human-preferences-from-the-state-of-the-world-83b1141585e3&user=Jeremie+Harris&userId=59564831d1eb&source=post_page-59564831d1eb----83b1141585e3---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----83b1141585e3--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F83b1141585e3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Feffective-altruism-ai-safety-and-learning-human-preferences-from-the-state-of-the-world-83b1141585e3&user=Jeremie+Harris&userId=59564831d1eb&source=-----83b1141585e3---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F83b1141585e3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Feffective-altruism-ai-safety-and-learning-human-preferences-from-the-state-of-the-world-83b1141585e3&source=-----83b1141585e3---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://www.youtube.com/watch?v=uHiL6GNXHvw&feature=emb_title", "anchor_text": "here"}, {"url": "http://sharpestminds.com", "anchor_text": "SharpestMinds"}, {"url": "https://podcasts.apple.com/ca/podcast/towards-data-science/id1470952338?mt=2", "anchor_text": "Apple"}, {"url": "https://www.google.com/podcasts?feed=aHR0cHM6Ly9hbmNob3IuZm0vcy8zNmI0ODQ0L3BvZGNhc3QvcnNz", "anchor_text": "Google"}, {"url": "https://open.spotify.com/show/63diy2DtpHzQfeNVxAPZgU", "anchor_text": "Spotify"}, {"url": "https://twitter.com/rohinmshah", "anchor_text": "follow Rohin on Twitter here"}, {"url": "https://rohinshah.com/alignment-newsletter/", "anchor_text": "AI alignment newsletter here"}, {"url": "https://twitter.com/jeremiecharris", "anchor_text": "follow me on Twitter here"}, {"url": "https://medium.com/tag/ai-alignment?source=post_page-----83b1141585e3---------------ai_alignment-----------------", "anchor_text": "Ai Alignment"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----83b1141585e3---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/deepmind?source=post_page-----83b1141585e3---------------deepmind-----------------", "anchor_text": "Deepmind"}, {"url": "https://medium.com/tag/tds-podcast?source=post_page-----83b1141585e3---------------tds_podcast-----------------", "anchor_text": "Tds Podcast"}, {"url": "https://medium.com/tag/ai-alignment-and-safety?source=post_page-----83b1141585e3---------------ai_alignment_and_safety-----------------", "anchor_text": "Ai Alignment And Safety"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F83b1141585e3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Feffective-altruism-ai-safety-and-learning-human-preferences-from-the-state-of-the-world-83b1141585e3&user=Jeremie+Harris&userId=59564831d1eb&source=-----83b1141585e3---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F83b1141585e3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Feffective-altruism-ai-safety-and-learning-human-preferences-from-the-state-of-the-world-83b1141585e3&user=Jeremie+Harris&userId=59564831d1eb&source=-----83b1141585e3---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F83b1141585e3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Feffective-altruism-ai-safety-and-learning-human-preferences-from-the-state-of-the-world-83b1141585e3&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/@JeremieHarris?source=post_page-----83b1141585e3--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----83b1141585e3--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F59564831d1eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Feffective-altruism-ai-safety-and-learning-human-preferences-from-the-state-of-the-world-83b1141585e3&user=Jeremie+Harris&userId=59564831d1eb&source=post_page-59564831d1eb----83b1141585e3---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F15c61aaa3274&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Feffective-altruism-ai-safety-and-learning-human-preferences-from-the-state-of-the-world-83b1141585e3&newsletterV3=59564831d1eb&newsletterV3Id=15c61aaa3274&user=Jeremie+Harris&userId=59564831d1eb&source=-----83b1141585e3---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/@JeremieHarris?source=post_page-----83b1141585e3--------------------------------", "anchor_text": "Written by Jeremie Harris"}, {"url": "https://medium.com/@JeremieHarris/followers?source=post_page-----83b1141585e3--------------------------------", "anchor_text": "122K Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----83b1141585e3--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "http://shorturl.at/jtMN0", "anchor_text": "shorturl.at/jtMN0"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F59564831d1eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Feffective-altruism-ai-safety-and-learning-human-preferences-from-the-state-of-the-world-83b1141585e3&user=Jeremie+Harris&userId=59564831d1eb&source=post_page-59564831d1eb----83b1141585e3---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F15c61aaa3274&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Feffective-altruism-ai-safety-and-learning-human-preferences-from-the-state-of-the-world-83b1141585e3&newsletterV3=59564831d1eb&newsletterV3Id=15c61aaa3274&user=Jeremie+Harris&userId=59564831d1eb&source=-----83b1141585e3---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-4-fastest-ways-not-to-get-hired-as-a-data-scientist-565b42bd011e?source=author_recirc-----83b1141585e3----0---------------------686a6eb8_f059_4b17_83fd_bd2227ba5f31-------", "anchor_text": ""}, {"url": "https://medium.com/@JeremieHarris?source=author_recirc-----83b1141585e3----0---------------------686a6eb8_f059_4b17_83fd_bd2227ba5f31-------", "anchor_text": ""}, {"url": "https://medium.com/@JeremieHarris?source=author_recirc-----83b1141585e3----0---------------------686a6eb8_f059_4b17_83fd_bd2227ba5f31-------", "anchor_text": "Jeremie Harris"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----83b1141585e3----0---------------------686a6eb8_f059_4b17_83fd_bd2227ba5f31-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-4-fastest-ways-not-to-get-hired-as-a-data-scientist-565b42bd011e?source=author_recirc-----83b1141585e3----0---------------------686a6eb8_f059_4b17_83fd_bd2227ba5f31-------", "anchor_text": "The 4 fastest ways not to get hired as a data scientistAvoiding these common mistakes won\u2019t get you hired. But not avoiding them guarantees your application a one-way ticket to the \u201cno\u201d pile."}, {"url": "https://towardsdatascience.com/the-4-fastest-ways-not-to-get-hired-as-a-data-scientist-565b42bd011e?source=author_recirc-----83b1141585e3----0---------------------686a6eb8_f059_4b17_83fd_bd2227ba5f31-------", "anchor_text": "6 min read\u00b7Jun 12, 2018"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F565b42bd011e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-4-fastest-ways-not-to-get-hired-as-a-data-scientist-565b42bd011e&user=Jeremie+Harris&userId=59564831d1eb&source=-----565b42bd011e----0-----------------clap_footer----686a6eb8_f059_4b17_83fd_bd2227ba5f31-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-4-fastest-ways-not-to-get-hired-as-a-data-scientist-565b42bd011e?source=author_recirc-----83b1141585e3----0---------------------686a6eb8_f059_4b17_83fd_bd2227ba5f31-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "29"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F565b42bd011e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-4-fastest-ways-not-to-get-hired-as-a-data-scientist-565b42bd011e&source=-----83b1141585e3----0-----------------bookmark_preview----686a6eb8_f059_4b17_83fd_bd2227ba5f31-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----83b1141585e3----1---------------------686a6eb8_f059_4b17_83fd_bd2227ba5f31-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----83b1141585e3----1---------------------686a6eb8_f059_4b17_83fd_bd2227ba5f31-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----83b1141585e3----1---------------------686a6eb8_f059_4b17_83fd_bd2227ba5f31-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----83b1141585e3----1---------------------686a6eb8_f059_4b17_83fd_bd2227ba5f31-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----83b1141585e3----1---------------------686a6eb8_f059_4b17_83fd_bd2227ba5f31-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----83b1141585e3----1---------------------686a6eb8_f059_4b17_83fd_bd2227ba5f31-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----686a6eb8_f059_4b17_83fd_bd2227ba5f31-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----83b1141585e3----1---------------------686a6eb8_f059_4b17_83fd_bd2227ba5f31-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----83b1141585e3----1-----------------bookmark_preview----686a6eb8_f059_4b17_83fd_bd2227ba5f31-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----83b1141585e3----2---------------------686a6eb8_f059_4b17_83fd_bd2227ba5f31-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----83b1141585e3----2---------------------686a6eb8_f059_4b17_83fd_bd2227ba5f31-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----83b1141585e3----2---------------------686a6eb8_f059_4b17_83fd_bd2227ba5f31-------", "anchor_text": "Jacob Marks, Ph.D."}, {"url": "https://towardsdatascience.com/?source=author_recirc-----83b1141585e3----2---------------------686a6eb8_f059_4b17_83fd_bd2227ba5f31-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----83b1141585e3----2---------------------686a6eb8_f059_4b17_83fd_bd2227ba5f31-------", "anchor_text": "How I Turned My Company\u2019s Docs into a Searchable Database with OpenAIAnd how you can do the same with your docs"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----83b1141585e3----2---------------------686a6eb8_f059_4b17_83fd_bd2227ba5f31-------", "anchor_text": "15 min read\u00b7Apr 25"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&user=Jacob+Marks%2C+Ph.D.&userId=f7dc0c0eae92&source=-----4f2d34bd8736----2-----------------clap_footer----686a6eb8_f059_4b17_83fd_bd2227ba5f31-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----83b1141585e3----2---------------------686a6eb8_f059_4b17_83fd_bd2227ba5f31-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "20"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&source=-----83b1141585e3----2-----------------bookmark_preview----686a6eb8_f059_4b17_83fd_bd2227ba5f31-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/why-youre-not-a-job-ready-data-scientist-yet-1a0d73f15012?source=author_recirc-----83b1141585e3----3---------------------686a6eb8_f059_4b17_83fd_bd2227ba5f31-------", "anchor_text": ""}, {"url": "https://medium.com/@JeremieHarris?source=author_recirc-----83b1141585e3----3---------------------686a6eb8_f059_4b17_83fd_bd2227ba5f31-------", "anchor_text": ""}, {"url": "https://medium.com/@JeremieHarris?source=author_recirc-----83b1141585e3----3---------------------686a6eb8_f059_4b17_83fd_bd2227ba5f31-------", "anchor_text": "Jeremie Harris"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----83b1141585e3----3---------------------686a6eb8_f059_4b17_83fd_bd2227ba5f31-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/why-youre-not-a-job-ready-data-scientist-yet-1a0d73f15012?source=author_recirc-----83b1141585e3----3---------------------686a6eb8_f059_4b17_83fd_bd2227ba5f31-------", "anchor_text": "Why you\u2019re not a job-ready data scientist (yet)You\u2019re getting rejected for a reason, but it\u2019s almost always something you can fix."}, {"url": "https://towardsdatascience.com/why-youre-not-a-job-ready-data-scientist-yet-1a0d73f15012?source=author_recirc-----83b1141585e3----3---------------------686a6eb8_f059_4b17_83fd_bd2227ba5f31-------", "anchor_text": "6 min read\u00b7Jun 16, 2019"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1a0d73f15012&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-youre-not-a-job-ready-data-scientist-yet-1a0d73f15012&user=Jeremie+Harris&userId=59564831d1eb&source=-----1a0d73f15012----3-----------------clap_footer----686a6eb8_f059_4b17_83fd_bd2227ba5f31-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/why-youre-not-a-job-ready-data-scientist-yet-1a0d73f15012?source=author_recirc-----83b1141585e3----3---------------------686a6eb8_f059_4b17_83fd_bd2227ba5f31-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "38"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1a0d73f15012&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-youre-not-a-job-ready-data-scientist-yet-1a0d73f15012&source=-----83b1141585e3----3-----------------bookmark_preview----686a6eb8_f059_4b17_83fd_bd2227ba5f31-------", "anchor_text": ""}, {"url": "https://medium.com/@JeremieHarris?source=post_page-----83b1141585e3--------------------------------", "anchor_text": "See all from Jeremie Harris"}, {"url": "https://towardsdatascience.com/?source=post_page-----83b1141585e3--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----83b1141585e3----0---------------------c44d63c5_a7d3_42f4_bdfa_d66e761e2f1c-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----83b1141585e3----0---------------------c44d63c5_a7d3_42f4_bdfa_d66e761e2f1c-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----83b1141585e3----0---------------------c44d63c5_a7d3_42f4_bdfa_d66e761e2f1c-------", "anchor_text": "The PyCoach"}, {"url": "https://artificialcorner.com/?source=read_next_recirc-----83b1141585e3----0---------------------c44d63c5_a7d3_42f4_bdfa_d66e761e2f1c-------", "anchor_text": "Artificial Corner"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----83b1141585e3----0---------------------c44d63c5_a7d3_42f4_bdfa_d66e761e2f1c-------", "anchor_text": "You\u2019re Using ChatGPT Wrong! Here\u2019s How to Be Ahead of 99% of ChatGPT UsersMaster ChatGPT by learning prompt engineering."}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----83b1141585e3----0---------------------c44d63c5_a7d3_42f4_bdfa_d66e761e2f1c-------", "anchor_text": "\u00b77 min read\u00b7Mar 17"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fartificial-corner%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&user=The+PyCoach&userId=fb44e21903f3&source=-----886a50dabc54----0-----------------clap_footer----c44d63c5_a7d3_42f4_bdfa_d66e761e2f1c-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----83b1141585e3----0---------------------c44d63c5_a7d3_42f4_bdfa_d66e761e2f1c-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "276"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&source=-----83b1141585e3----0-----------------bookmark_preview----c44d63c5_a7d3_42f4_bdfa_d66e761e2f1c-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----83b1141585e3----1---------------------c44d63c5_a7d3_42f4_bdfa_d66e761e2f1c-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----83b1141585e3----1---------------------c44d63c5_a7d3_42f4_bdfa_d66e761e2f1c-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----83b1141585e3----1---------------------c44d63c5_a7d3_42f4_bdfa_d66e761e2f1c-------", "anchor_text": "Alexander Nguyen"}, {"url": "https://levelup.gitconnected.com/?source=read_next_recirc-----83b1141585e3----1---------------------c44d63c5_a7d3_42f4_bdfa_d66e761e2f1c-------", "anchor_text": "Level Up Coding"}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----83b1141585e3----1---------------------c44d63c5_a7d3_42f4_bdfa_d66e761e2f1c-------", "anchor_text": "Why I Keep Failing Candidates During Google Interviews\u2026They don\u2019t meet the bar."}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----83b1141585e3----1---------------------c44d63c5_a7d3_42f4_bdfa_d66e761e2f1c-------", "anchor_text": "\u00b74 min read\u00b7Apr 13"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fgitconnected%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&user=Alexander+Nguyen&userId=a148fd75c2e9&source=-----dc8f865b2c19----1-----------------clap_footer----c44d63c5_a7d3_42f4_bdfa_d66e761e2f1c-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----83b1141585e3----1---------------------c44d63c5_a7d3_42f4_bdfa_d66e761e2f1c-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "91"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&source=-----83b1141585e3----1-----------------bookmark_preview----c44d63c5_a7d3_42f4_bdfa_d66e761e2f1c-------", "anchor_text": ""}, {"url": "https://medium.com/better-advice/10-things-to-do-in-the-evening-instead-of-watching-netflix-4e270e9dd6b9?source=read_next_recirc-----83b1141585e3----0---------------------c44d63c5_a7d3_42f4_bdfa_d66e761e2f1c-------", "anchor_text": ""}, {"url": "https://aleid-tw.medium.com/?source=read_next_recirc-----83b1141585e3----0---------------------c44d63c5_a7d3_42f4_bdfa_d66e761e2f1c-------", "anchor_text": ""}, {"url": "https://aleid-tw.medium.com/?source=read_next_recirc-----83b1141585e3----0---------------------c44d63c5_a7d3_42f4_bdfa_d66e761e2f1c-------", "anchor_text": "Aleid ter Weel"}, {"url": "https://medium.com/better-advice?source=read_next_recirc-----83b1141585e3----0---------------------c44d63c5_a7d3_42f4_bdfa_d66e761e2f1c-------", "anchor_text": "Better Advice"}, {"url": "https://medium.com/better-advice/10-things-to-do-in-the-evening-instead-of-watching-netflix-4e270e9dd6b9?source=read_next_recirc-----83b1141585e3----0---------------------c44d63c5_a7d3_42f4_bdfa_d66e761e2f1c-------", "anchor_text": "10 Things To Do In The Evening Instead Of Watching NetflixDevice-free habits to increase your productivity and happiness."}, {"url": "https://medium.com/better-advice/10-things-to-do-in-the-evening-instead-of-watching-netflix-4e270e9dd6b9?source=read_next_recirc-----83b1141585e3----0---------------------c44d63c5_a7d3_42f4_bdfa_d66e761e2f1c-------", "anchor_text": "\u00b75 min read\u00b7Feb 15, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fbetter-advice%2F4e270e9dd6b9&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fbetter-advice%2F10-things-to-do-in-the-evening-instead-of-watching-netflix-4e270e9dd6b9&user=Aleid+ter+Weel&userId=6ffe087f07e5&source=-----4e270e9dd6b9----0-----------------clap_footer----c44d63c5_a7d3_42f4_bdfa_d66e761e2f1c-------", "anchor_text": ""}, {"url": "https://medium.com/better-advice/10-things-to-do-in-the-evening-instead-of-watching-netflix-4e270e9dd6b9?source=read_next_recirc-----83b1141585e3----0---------------------c44d63c5_a7d3_42f4_bdfa_d66e761e2f1c-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "204"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4e270e9dd6b9&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fbetter-advice%2F10-things-to-do-in-the-evening-instead-of-watching-netflix-4e270e9dd6b9&source=-----83b1141585e3----0-----------------bookmark_preview----c44d63c5_a7d3_42f4_bdfa_d66e761e2f1c-------", "anchor_text": ""}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----83b1141585e3----1---------------------c44d63c5_a7d3_42f4_bdfa_d66e761e2f1c-------", "anchor_text": ""}, {"url": "https://medium.com/@timothymugayi?source=read_next_recirc-----83b1141585e3----1---------------------c44d63c5_a7d3_42f4_bdfa_d66e761e2f1c-------", "anchor_text": ""}, {"url": "https://medium.com/@timothymugayi?source=read_next_recirc-----83b1141585e3----1---------------------c44d63c5_a7d3_42f4_bdfa_d66e761e2f1c-------", "anchor_text": "Timothy Mugayi"}, {"url": "https://betterprogramming.pub/?source=read_next_recirc-----83b1141585e3----1---------------------c44d63c5_a7d3_42f4_bdfa_d66e761e2f1c-------", "anchor_text": "Better Programming"}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----83b1141585e3----1---------------------c44d63c5_a7d3_42f4_bdfa_d66e761e2f1c-------", "anchor_text": "How To Build Your Own Custom ChatGPT With Custom Knowledge BaseFeed your ChatGPT bot with custom data sources"}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----83b1141585e3----1---------------------c44d63c5_a7d3_42f4_bdfa_d66e761e2f1c-------", "anchor_text": "\u00b711 min read\u00b7Apr 7"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fbetter-programming%2F4e61ad82427e&operation=register&redirect=https%3A%2F%2Fbetterprogramming.pub%2Fhow-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e&user=Timothy+Mugayi&userId=34774d6cac27&source=-----4e61ad82427e----1-----------------clap_footer----c44d63c5_a7d3_42f4_bdfa_d66e761e2f1c-------", "anchor_text": ""}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----83b1141585e3----1---------------------c44d63c5_a7d3_42f4_bdfa_d66e761e2f1c-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "83"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4e61ad82427e&operation=register&redirect=https%3A%2F%2Fbetterprogramming.pub%2Fhow-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e&source=-----83b1141585e3----1-----------------bookmark_preview----c44d63c5_a7d3_42f4_bdfa_d66e761e2f1c-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----83b1141585e3----2---------------------c44d63c5_a7d3_42f4_bdfa_d66e761e2f1c-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----83b1141585e3----2---------------------c44d63c5_a7d3_42f4_bdfa_d66e761e2f1c-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----83b1141585e3----2---------------------c44d63c5_a7d3_42f4_bdfa_d66e761e2f1c-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----83b1141585e3----2---------------------c44d63c5_a7d3_42f4_bdfa_d66e761e2f1c-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----83b1141585e3----2---------------------c44d63c5_a7d3_42f4_bdfa_d66e761e2f1c-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----83b1141585e3----2---------------------c44d63c5_a7d3_42f4_bdfa_d66e761e2f1c-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----2-----------------clap_footer----c44d63c5_a7d3_42f4_bdfa_d66e761e2f1c-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----83b1141585e3----2---------------------c44d63c5_a7d3_42f4_bdfa_d66e761e2f1c-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----83b1141585e3----2-----------------bookmark_preview----c44d63c5_a7d3_42f4_bdfa_d66e761e2f1c-------", "anchor_text": ""}, {"url": "https://medium.com/geekculture/stop-doing-this-on-chatgpt-get-ahead-99-users-ai-artificial-intelligence-productivity-prompt-engineering-4-f3441bf7a25a?source=read_next_recirc-----83b1141585e3----3---------------------c44d63c5_a7d3_42f4_bdfa_d66e761e2f1c-------", "anchor_text": ""}, {"url": "https://medium.com/@rfeers?source=read_next_recirc-----83b1141585e3----3---------------------c44d63c5_a7d3_42f4_bdfa_d66e761e2f1c-------", "anchor_text": ""}, {"url": "https://medium.com/@rfeers?source=read_next_recirc-----83b1141585e3----3---------------------c44d63c5_a7d3_42f4_bdfa_d66e761e2f1c-------", "anchor_text": "Josep Ferrer"}, {"url": "https://medium.com/geekculture?source=read_next_recirc-----83b1141585e3----3---------------------c44d63c5_a7d3_42f4_bdfa_d66e761e2f1c-------", "anchor_text": "Geek Culture"}, {"url": "https://medium.com/geekculture/stop-doing-this-on-chatgpt-get-ahead-99-users-ai-artificial-intelligence-productivity-prompt-engineering-4-f3441bf7a25a?source=read_next_recirc-----83b1141585e3----3---------------------c44d63c5_a7d3_42f4_bdfa_d66e761e2f1c-------", "anchor_text": "Stop doing this on ChatGPT and get ahead of the 99% of its usersUnleash the Power of AI Writing with Effective Prompts"}, {"url": "https://medium.com/geekculture/stop-doing-this-on-chatgpt-get-ahead-99-users-ai-artificial-intelligence-productivity-prompt-engineering-4-f3441bf7a25a?source=read_next_recirc-----83b1141585e3----3---------------------c44d63c5_a7d3_42f4_bdfa_d66e761e2f1c-------", "anchor_text": "\u00b78 min read\u00b7Mar 31"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fgeekculture%2Ff3441bf7a25a&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fgeekculture%2Fstop-doing-this-on-chatgpt-get-ahead-99-users-ai-artificial-intelligence-productivity-prompt-engineering-4-f3441bf7a25a&user=Josep+Ferrer&userId=8213af8f3ccf&source=-----f3441bf7a25a----3-----------------clap_footer----c44d63c5_a7d3_42f4_bdfa_d66e761e2f1c-------", "anchor_text": ""}, {"url": "https://medium.com/geekculture/stop-doing-this-on-chatgpt-get-ahead-99-users-ai-artificial-intelligence-productivity-prompt-engineering-4-f3441bf7a25a?source=read_next_recirc-----83b1141585e3----3---------------------c44d63c5_a7d3_42f4_bdfa_d66e761e2f1c-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "71"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff3441bf7a25a&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fgeekculture%2Fstop-doing-this-on-chatgpt-get-ahead-99-users-ai-artificial-intelligence-productivity-prompt-engineering-4-f3441bf7a25a&source=-----83b1141585e3----3-----------------bookmark_preview----c44d63c5_a7d3_42f4_bdfa_d66e761e2f1c-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----83b1141585e3--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----83b1141585e3--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----83b1141585e3--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----83b1141585e3--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----83b1141585e3--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----83b1141585e3--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----83b1141585e3--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----83b1141585e3--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----83b1141585e3--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----83b1141585e3--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}