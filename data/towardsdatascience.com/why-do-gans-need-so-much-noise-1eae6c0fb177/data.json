{"url": "https://towardsdatascience.com/why-do-gans-need-so-much-noise-1eae6c0fb177", "time": 1683004247.3604732, "path": "towardsdatascience.com/why-do-gans-need-so-much-noise-1eae6c0fb177/", "webpage": {"metadata": {"title": "Why Do GANs Need So Much Noise?. Visualizing how GANs learn in\u2026 | by Conor Lazarou | Towards Data Science", "h1": "Why Do GANs Need So Much Noise?", "description": "Generative Adversarial Networks (GANs) are a tool for generating new, \u201cfake\u201d samples given a set of old, \u201creal\u201d samples. These samples can be practically anything: hand-drawn digits, photographs of\u2026"}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/Quantile_function", "anchor_text": "quantile function", "paragraph_index": 2}, {"url": "https://towardsdatascience.com/training-a-gan-to-sample-from-the-normal-distribution-4095a11e78de", "anchor_text": "previous article", "paragraph_index": 5}, {"url": "https://en.wikipedia.org/wiki/Space-filling_curve", "anchor_text": "Peano curves", "paragraph_index": 7}, {"url": "https://towardsdatascience.com/gans-and-inefficient-mappings-f185fdd62a78", "anchor_text": "another article", "paragraph_index": 22}, {"url": "http://flatland.ai", "anchor_text": "flatland.ai", "paragraph_index": 27}], "all_paragraphs": ["Generative Adversarial Networks (GANs) are a tool for generating new, \u201cfake\u201d samples given a set of old, \u201creal\u201d samples. These samples can be practically anything: hand-drawn digits, photographs of faces, expressionist paintings, you name it. To do this, GANs learn the underlying distribution behind the original dataset. Throughout training, the generator approximates this distribution while the discriminator tells it what it got wrong, and the two alternatingly improve through an arms race. In order to draw random samples from the distribution, the generator is given random noise as input. But, have you ever wondered why GANs need random input? The common answer is \u201cso they don\u2019t generate the same thing every time\u201d, and that\u2019s true, but the answer is a bit more nuanced than that.", "Before we continue with GANs, let\u2019s take a detour and consider sampling from the normal distribution. Suppose you want to do this in Python, but you never read the numpy docs and don\u2019t know that np.random.normal() exists. Instead, all you\u2019ve got to work with is random.random(), which produces values uniformly in the interval (0, 1).", "In short, we want to transform the blue distribution into the orange distribution in figure 1. Fortunately, there is a function to do this: the inverse cumulative distribution function, also called the quantile function. The (non-inverted) cumulative distribution function, or CDF, illustrated in figure 2, describes the probability that any random value drawn from the distribution in question will be equal to or less than x, for some specified x.", "For instance, at the point x=0 in figure 2, y=0.5; this means that 50% of the distribution lies below zero. A handy quality of the CDF is that the output ranges from 0 to 1, which is exactly the input we have available to us from the random.random() function! If we invert the CDF (flip it on its side), we get the quantile function:", "This function gives us the exact relationship between the quantile (our x, ranging from 0 to 1) and the corresponding value in the normal distribution, allowing us to sample directly from the normal distribution. That is, f(random.random()) ~ N(0, 1), where each point in the input space corresponds to a unique point in the output space.", "In the above scenario, we had the quantile function at our disposal, but what if we didn\u2019t, and had to learn a mapping from the input space to the output space? That is exactly the problem that GANs aim to solve. In a previous article, I illustrated how GANs can be used to sample from the normal distribution if you\u2019re in a data emergency and don\u2019t have the quantile function available to you. In this light, I find it much more helpful to think of GANs not as tools for random sampling, but as functions that map some k-dimensional latent (input) space to some p-dimensional sample (output) space, which can then be used to transform samples from the latent space to samples from the sample space. In this view, much like the quantile function, there\u2019s no randomness involved.", "With maps on the mind, let\u2019s consider how we might draw random samples from a 2D normal distribution with only 1D random samples between 0 and 1 as input.", "How would we map the 100k samples in that blue line to the 100k samples in the orange blob? There\u2019s no good way to do it. Sure, we could use Peano curves, but then we lose the useful property of having points close together in the input space result in points close together in the output space, and vice-versa. It\u2019s for this reason that the dimensionality of the latent space of a GAN must equal or exceed the dimensionality of its sample space. That way, the function has enough degrees of freedom to map the input to the output.", "But just for fun, let\u2019s visualize what happens when a GAN with only one-dimensional input is tasked with learning multi-dimensional distributions. The results hopefully won\u2019t surprise you, but they are fun to watch.", "Let\u2019s start out with the issue illustrated in figure 5: mapping the 1D range between 0 and 1 to the 2D normal (or \u201cGaussian\u201d) distribution. We will be using a typical vanilla GAN architecture (code available at the end of the article).", "As you can see, the poor thing is at a loss for what to do. Having only one degree of freedom, it is hardly able to explore the sample space. What\u2019s worse, because the generated samples are so densely-packed in that 1D manifold (there are as many grey dots in this gif as red dots!), the discriminator is able to slack off, never having to try hard to discern the real points from the fakes, and as such the generator doesn\u2019t get very useful information (and certainly not enough to learn a space-filling curve, even if it had the capacity!).", "Figure 6 shows the first 600 training steps. After 30k, this was the result:", "It\u2019s a cute little squiggle, but hardly a Gaussian distribution. The GAN completely failed to learn the mapping after 30k steps. For context, let\u2019s consider how a GAN with the same architecture and training routine fares when given 2D, 3D, 10D, and 100D latent spaces to map to the above distribution:", "The 2D latent space GAN is much better than the 1D GAN above, but is still nowhere near the target distribution and had several obvious kinks in it. The 3D and 10D latent spaces produced GANs with visually convincing results, and the 100D GAN produced what appears to be a Gaussian distribution with the right variance but wrong mean. But, we should keep in mind that the high-dimensional GANs are cheating in this particular problem, since the mean of many uniform distributions is approximately normally-distributed.", "The eight Guassians distribution (figure 9) is exactly as it sounds: a mixture of eight 2D Gaussians arranged in a circle about the origin, each with small enough variance that they hardly overlap, and with zero covariance. Although the sample space is 2D, a reasonable encoding of this distribution has three dimensions: the first dimension being discrete and describing the mode (numbered one through eight), and the other two describing the x and y displacement from that mode, respectively.", "I trained a GAN with latent_dim=1 on the eight Gaussians distribution 600 steps, and these were the results:", "As expected, the GAN struggles to learn an effective mapping. After 30k steps, this is the learned distribution:", "The GAN is clearly struggling to map the 1D latent space to this 3D distribution: The right-most mode is ignored, a considerable number of samples are being generated between modes, and samples aren\u2019t normally-distributed. For comparison, let\u2019s consider four more GANs after 30k steps, with latent dimensions of 2, 3, 10, and 100:", "It\u2019s hard to tell which is best without actually measuring the KL divergence between the true distribution and the learned distribution (coming soon\u2122\ufe0f in a follow-up article!), but the low-dimensional GANs seem to produce fewer samples in the negative space between modes. Even more interesting, the 2D GAN does not show mode collapse, the 3D and 10D GANs show only slight mode collapse, and the 100D GAN failed to generate samples in two of the modes.", "The spiral distribution, illustrated in figure 13, is in some ways simpler than the eight Gaussians distribution. Having only one mode (albeit elongated and twisty), the GAN isn\u2019t forced to discretize its continuous input. It can be described efficiently with two dimensions: one describing position along the spiral, the other describing position laterally within the spiral.", "I trained a GAN with latent_dim=1 for 600 steps, and these were the results:", "Again, the GAN struggles to learn an effective mapping. After 30k steps, this is the learned distribution:", "Similar to the case of the eight Gaussians distribution, the GAN does a poor job of mapping the spiral distribution. Two regions of the spiral are omitted and many samples are generated in the negative space. I address this inefficient mapping problem in detail in another article, so I won\u2019t belabour the point here; instead, let\u2019s consider four more GANs tasked with learning this distribution after 30k steps, again with latent dimensions of 2, 3, 10, and 100:", "Again, it\u2019s hard to tell which is best without actually measuring the KL divergence, but the differences in coverage, uniformity, and amount of sampling in negative space are interesting to consider.", "It\u2019s easy to get caught up in the GAN fervor and treat them like magic machines that use random numbers as fuel to pop out new samples. Understanding the fundamentals of how a tool works is essential to using it effectively and troubleshooting it when it breaks. With GANs, that means understanding that the generator is learning a mapping from some latent space to some sample space, and understanding how that learning unfolds. The extreme case of mapping a 1D distribution to a higher-dimensional distribution clearly illustrates how complicated this task is.", "All code used in this project is available in the following GitHub repo:", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data science and ML consultant, generative artist, writer. flatland.ai"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F1eae6c0fb177&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-do-gans-need-so-much-noise-1eae6c0fb177&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-do-gans-need-so-much-noise-1eae6c0fb177&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-do-gans-need-so-much-noise-1eae6c0fb177&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-do-gans-need-so-much-noise-1eae6c0fb177&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----1eae6c0fb177--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1eae6c0fb177--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://conor-lazarou.medium.com/?source=post_page-----1eae6c0fb177--------------------------------", "anchor_text": ""}, {"url": "https://conor-lazarou.medium.com/?source=post_page-----1eae6c0fb177--------------------------------", "anchor_text": "Conor Lazarou"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdeb461dc9d26&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-do-gans-need-so-much-noise-1eae6c0fb177&user=Conor+Lazarou&userId=deb461dc9d26&source=post_page-deb461dc9d26----1eae6c0fb177---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1eae6c0fb177&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-do-gans-need-so-much-noise-1eae6c0fb177&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1eae6c0fb177&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-do-gans-need-so-much-noise-1eae6c0fb177&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.pexels.com/photo/acrylic-art-chaos-chaotic-989580/", "anchor_text": "Pexels"}, {"url": "https://en.wikipedia.org/wiki/Quantile_function", "anchor_text": "quantile function"}, {"url": "https://towardsdatascience.com/training-a-gan-to-sample-from-the-normal-distribution-4095a11e78de", "anchor_text": "previous article"}, {"url": "https://en.wikipedia.org/wiki/Space-filling_curve", "anchor_text": "Peano curves"}, {"url": "https://towardsdatascience.com/gans-and-inefficient-mappings-f185fdd62a78", "anchor_text": "another article"}, {"url": "https://github.com/ConorLazarou/medium/tree/master/12020/visualizing_gan_dimensions", "anchor_text": "ConorLazarou/mediumGANs and Low Dimensional Latent Spaces: Visualizing how GANs learn when starved for random noise.github.com"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----1eae6c0fb177---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----1eae6c0fb177---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/gans?source=post_page-----1eae6c0fb177---------------gans-----------------", "anchor_text": "Gans"}, {"url": "https://medium.com/tag/data-science?source=post_page-----1eae6c0fb177---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/visualization?source=post_page-----1eae6c0fb177---------------visualization-----------------", "anchor_text": "Visualization"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1eae6c0fb177&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-do-gans-need-so-much-noise-1eae6c0fb177&user=Conor+Lazarou&userId=deb461dc9d26&source=-----1eae6c0fb177---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1eae6c0fb177&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-do-gans-need-so-much-noise-1eae6c0fb177&user=Conor+Lazarou&userId=deb461dc9d26&source=-----1eae6c0fb177---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1eae6c0fb177&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-do-gans-need-so-much-noise-1eae6c0fb177&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1eae6c0fb177--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F1eae6c0fb177&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-do-gans-need-so-much-noise-1eae6c0fb177&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----1eae6c0fb177---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----1eae6c0fb177--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----1eae6c0fb177--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----1eae6c0fb177--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----1eae6c0fb177--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----1eae6c0fb177--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----1eae6c0fb177--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----1eae6c0fb177--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----1eae6c0fb177--------------------------------", "anchor_text": ""}, {"url": "https://conor-lazarou.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://conor-lazarou.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Conor Lazarou"}, {"url": "https://conor-lazarou.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.1K Followers"}, {"url": "http://flatland.ai", "anchor_text": "flatland.ai"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdeb461dc9d26&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-do-gans-need-so-much-noise-1eae6c0fb177&user=Conor+Lazarou&userId=deb461dc9d26&source=post_page-deb461dc9d26--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F51220adfabae&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-do-gans-need-so-much-noise-1eae6c0fb177&newsletterV3=deb461dc9d26&newsletterV3Id=51220adfabae&user=Conor+Lazarou&userId=deb461dc9d26&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}