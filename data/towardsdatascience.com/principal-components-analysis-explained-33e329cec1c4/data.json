{"url": "https://towardsdatascience.com/principal-components-analysis-explained-33e329cec1c4", "time": 1683012544.918328, "path": "towardsdatascience.com/principal-components-analysis-explained-33e329cec1c4/", "webpage": {"metadata": {"title": "Principal Component Analysis explained | by Matyas Amrouche | Towards Data Science", "h1": "Principal Component Analysis explained", "description": "Principal Components Analysis (PCA) is one of the most famous algorithms in Machine Learning (ML), it aims to reduce the dimensionality of your data or to perform unsupervised clustering. PCA is\u2026"}, "outgoing_paragraph_urls": [{"url": "http://l", "anchor_text": "PS", "paragraph_index": 20}], "all_paragraphs": ["Principal Components Analysis (PCA) is one of the most famous algorithms in Machine Learning (ML), it aims to reduce the dimensionality of your data or to perform unsupervised clustering. PCA is undoubtedly used worldwide \ud83c\udf0d, in any fields that manipulate data, from finance to biology.", "While there are many great resources that give the recipe to perform PCA or nice spatial interpretation of what it does, there are few that goes under the hood of the mathematical concepts behind it.", "Although it is not necessary to understand the maths to use the PCA out of the box, I strongly believe that a deep understanding of the algorithms makes you a better user, able to understand its performance and drawbacks in any specific situations. Besides, mathematical concepts are interconnected in ML and understanding PCA may help you get on with other ML notion that uses algebra (for the curious, check Figure 3. in the post-scriptum section at the end of the post).", "This post attempts to explain the different steps with the mathematical concepts behind it. I assume the reader is already familiar with algebra fundamentals.", "First things first, let\u2019s recap the PCA recipe for a quick refresh of the different involved steps:", "Given that k<<D, we are free from the curse of dimensionality that was previously hitting us \ud83d\udde1\ufe0f \ud83d\udc09", "So cool! But how is it possible that taking the covariance matrix and somehow mixing it with eigenvectors decomposition captures our dataset in a lower dimension space!?", "To answer this question, it is time to get our hands dirty and see what is going on under the hood of PCA engine! \ud83d\udee0\ufe0f", "PCA goals it to project the original dataset into a lower dimension space that captures the most information of the original dataset. So the first question we should ask ourselves is:", "What captures the most information of a dataset?", "The fundamental relation behind PCA, is that variance captures the most information of the dataset. The more variance (dispersion) the more information. Therefore, PCA aims to find a lower dimension space that keeps a maximum variance.", "So, the first normalization step aims to normalize the range of every dimension of the original dataset, making the variance of every dimension comparable. Nothing too crazy here \ud83d\ude4f\ud83c\udffb", "Great! But now that we understood that maximum variance is the key to preserve the most information of our original dataset in a lower dimension space, one can ask:", "How to find a lower dimension space that keeps maximum variance?", "To achieve this, we can simply start by finding the vector u that captures the most variance when our dataset is projected on it.", "We just showed that the eigenvector with the largest eigenvalue of the covariance matrix \u03a3 points in the maximum variance direction and the variance in this direction is \u03bb.", "We can now project our original dataset X on 1 axis, the vector u, that preserves a maximum of its original variance. But maybe we would like to find one more axis, or k more..?", "Never mind! We simply repeat the same process showed in Figure 1. above k times, with a new constraint added each time: previous largest (regarding their associated eigenvalues) eigenvectors must be orthogonal to the new candidate.", "Here we are, we have proven that the eigenvectors of the covariance matrix are pointing in the maximum variance directions and their associated eigenvalues equal the variance along these directions, just as Figure 2. illustrates it.", "Finally, it is obvious why we selected the k eigenvectors with the highest eigenvalues to retain the maximum variance of the original dataset.", "PS: In neural network optimization it is useful to compute the Hessian matrix of the loss function in order to help the gradient descent steps to adapt its learning rate regarding its direction. In short, the Hessian shows how fast the gradient of the loss function changes in any direction.", "Using the same algebra concepts we saw in PCA, it now makes sense that the eigenvectors of the Hessian matrix are pointing in the directions of the maximum curvature and their associated eigenvalues represent the curvature of the function is in that directions. Hence, during the gradient descent update, a high curvature (high eigenvalue) direction needs a lower learning rate since the gradient changes faster in this direction.And vice versa a low curvature direction a higher learning rate.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "ML & NLP for Search Relevance @ Leboncoin \ud83d\udce6"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F33e329cec1c4&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprincipal-components-analysis-explained-33e329cec1c4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprincipal-components-analysis-explained-33e329cec1c4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprincipal-components-analysis-explained-33e329cec1c4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprincipal-components-analysis-explained-33e329cec1c4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----33e329cec1c4--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----33e329cec1c4--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://matyasamrouche.medium.com/?source=post_page-----33e329cec1c4--------------------------------", "anchor_text": ""}, {"url": "https://matyasamrouche.medium.com/?source=post_page-----33e329cec1c4--------------------------------", "anchor_text": "Matyas Amrouche"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fad07e8173fb2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprincipal-components-analysis-explained-33e329cec1c4&user=Matyas+Amrouche&userId=ad07e8173fb2&source=post_page-ad07e8173fb2----33e329cec1c4---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F33e329cec1c4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprincipal-components-analysis-explained-33e329cec1c4&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F33e329cec1c4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprincipal-components-analysis-explained-33e329cec1c4&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@timmossholder?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Tim Mossholder"}, {"url": "https://unsplash.com/s/photos/car-engine?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/", "anchor_text": "source"}, {"url": "https://math.stackexchange.com/questions/23596/why-is-the-eigenvector-of-a-covariance-matrix-equal-to-a-principal-component", "anchor_text": "StackExchange"}, {"url": "https://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/", "anchor_text": "geometric interpretation of the covariance matrix"}, {"url": "http://www.cs.toronto.edu/~mbrubake/teaching/C11/Handouts/LagrangeMultipliers.pdf", "anchor_text": "course"}, {"url": "http://l", "anchor_text": "PS"}, {"url": "https://mlexplained.com/2018/02/02/an-introduction-to-second-order-optimization-for-deep-learning-practitioners-basic-math-for-deep-learning-part-1/", "anchor_text": "source"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----33e329cec1c4---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/algebra?source=post_page-----33e329cec1c4---------------algebra-----------------", "anchor_text": "Algebra"}, {"url": "https://medium.com/tag/data-science?source=post_page-----33e329cec1c4---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/mathematics?source=post_page-----33e329cec1c4---------------mathematics-----------------", "anchor_text": "Mathematics"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----33e329cec1c4---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F33e329cec1c4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprincipal-components-analysis-explained-33e329cec1c4&user=Matyas+Amrouche&userId=ad07e8173fb2&source=-----33e329cec1c4---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F33e329cec1c4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprincipal-components-analysis-explained-33e329cec1c4&user=Matyas+Amrouche&userId=ad07e8173fb2&source=-----33e329cec1c4---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F33e329cec1c4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprincipal-components-analysis-explained-33e329cec1c4&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----33e329cec1c4--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F33e329cec1c4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprincipal-components-analysis-explained-33e329cec1c4&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----33e329cec1c4---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----33e329cec1c4--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----33e329cec1c4--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----33e329cec1c4--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----33e329cec1c4--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----33e329cec1c4--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----33e329cec1c4--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----33e329cec1c4--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----33e329cec1c4--------------------------------", "anchor_text": ""}, {"url": "https://matyasamrouche.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://matyasamrouche.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Matyas Amrouche"}, {"url": "https://matyasamrouche.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "158 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fad07e8173fb2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprincipal-components-analysis-explained-33e329cec1c4&user=Matyas+Amrouche&userId=ad07e8173fb2&source=post_page-ad07e8173fb2--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F5ead0b158385&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprincipal-components-analysis-explained-33e329cec1c4&newsletterV3=ad07e8173fb2&newsletterV3Id=5ead0b158385&user=Matyas+Amrouche&userId=ad07e8173fb2&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}