{"url": "https://towardsdatascience.com/fine-grained-sentiment-analysis-in-python-part-2-2a92fdc0160d", "time": 1683000232.058641, "path": "towardsdatascience.com/fine-grained-sentiment-analysis-in-python-part-2-2a92fdc0160d/", "webpage": {"metadata": {"title": "Fine-grained Sentiment Analysis in Python (Part 2) | by Prashanth Rao | Towards Data Science", "h1": "Fine-grained Sentiment Analysis in Python (Part 2)", "description": "Generating LIME explanations for various Python NLP sentiment analysis libraries: TextBlob, VADER, Logistic Regression, Support Vector Machines (SVM), FastText and Flair"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/abs/1602.04938", "anchor_text": "Ribeiro et al.", "paragraph_index": 0}, {"url": "https://medium.com/@tech_optimist/fine-grained-sentiment-analysis-in-python-part-1-2697bb111ed4", "anchor_text": "Part 1", "paragraph_index": 1}, {"url": "https://arxiv.org/pdf/1602.04938.pdf", "anchor_text": "original paper", "paragraph_index": 5}, {"url": "https://arxiv.org/pdf/1602.04938.pdf", "anchor_text": "From the paper", "paragraph_index": 5}, {"url": "https://en.wikipedia.org/wiki/Multiclass_classification#One-vs.-rest", "anchor_text": "one-vs-rest method", "paragraph_index": 9}, {"url": "https://github.com/prrao87/fine-grained-sentiment/blob/master/explainer.py", "anchor_text": "project\u2019s GitHub repo [explainer.py", "paragraph_index": 10}, {"url": "https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html", "anchor_text": "normal distribution", "paragraph_index": 14}, {"url": "https://github.com/prrao87/fine-grained-sentiment/blob/master/explainer.py", "anchor_text": "explainer.py", "paragraph_index": 15}, {"url": "https://medium.com/@tech_optimist/fine-grained-sentiment-analysis-in-python-part-1-2697bb111ed4", "anchor_text": "EDA done in the previous post", "paragraph_index": 16}, {"url": "https://ai.google/research/pubs/pub41880", "anchor_text": "1 billion words of news crawl data", "paragraph_index": 40}, {"url": "https://github.com/zalandoresearch/flair/issues/915#issuecomment-513152226", "anchor_text": "ELMo embeddings are looked up during run time", "paragraph_index": 43}, {"url": "https://medium.com/the-andela-way/deploying-a-python-flask-app-to-heroku-41250bda27d0", "anchor_text": "in a similar way to this example", "paragraph_index": 44}, {"url": "https://sst5-explainer.herokuapp.com", "anchor_text": "Try out the dashboard here", "paragraph_index": 45}, {"url": "https://medium.com/@tech_optimist/fine-grained-sentiment-analysis-part-3-fine-tuning-transformers-1ae6574f25a6", "anchor_text": "Part 3", "paragraph_index": 49}], "all_paragraphs": ["\u201cWhy Should I Trust You?\u201d \u2014 Ribeiro et al.", "This is Part 2 of a series on fine-grained sentiment analysis in Python. Part 1 covered how to train and evaluate various fine-grained sentiment classifiers in Python. In this post, we\u2019ll discuss why a classifier made a specific class prediction \u2014 that is, how to explain a sentiment classifier\u2019s results using a popular method called LIME.", "To recap, the following six models were used to make fine-grained sentiment class predictions on the Stanford Sentiment Treebank (SST-5) dataset.", "A linear process was used to analyze and explain the sentiment classification results using each method.", "The below sections explain the final step in the process \u2014 generating explanations for the predictions of each method.", "Or, simply put, LIME, is a technique [original paper] that explains the prediction of a classifier in an interpretable manner. The term \u201cexplain\u201d here means generating textual or visual aids, such as highlighted plots and charts, that provide a qualitative understanding of the relationship between a model\u2019s features and its predictions. From the paper, the below key points make LIME very effective at interpreting a complex model\u2019s classification results:", "It is model-agnostic \u2014 it treats the original model as a blackbox, so it can be applied on pretty much any classifier", "It is locally faithful \u2014 it corresponds to how the model behaves in the vicinity of the instance being predicted. So for a given set of features in a test sample, the explanation is meaningful for the decision space occupied by those features, which may or may not not apply globally.", "To provide an explanation for any classifier, LIME uses a very clever approach \u2014 the only thing it needs from the larger, blackbox classifier is its prediction probabilities for each class (from the model\u2019s output layer after the softmax function is applied). The below steps explain the process in a nutshell.", "For multi-class text classification such as this SST-5 dataset, LIME uses the prediction probabilities to highlight the effect of each feature (i.e. token) on the predicted class using a one-vs-rest method. An example multi-class explanation from LIME is shown below. The model predicts a class of 2 for this sentence. The highlighted colours in the text being explained are randomly generated, with more intense colours signifying a greater feature importance for that token (in the below example, \u201cdullness\u201d) for the predicted class.", "Just as in the previous post, an object-oriented approach is applied to reuse code where possible. All code for the explainer is provided in this project\u2019s GitHub repo [explainer.py]. A Python class is defined which takes in the list of variations generated by LIME (random text samples with tokens blanked out), and outputs a class probability for each sample as a numpy array.", "Once the class probabilities for each variation is returned, this can be fed to the LimeTextExplainer class. Enabling bag-of-words (bow) would mean that LIME doesn\u2019t consider word order when generating variations. However, the FastText and Flair models were trained considering n-grams and contextual ordering respectively, so for a fair comparison between models, the bow flag option is disabled for all explanations on SST-5.", "The exp object returned by the LIME explainer is an internal method that converts the local linear model\u2019s predictions (in numerical form) to a visual form that can be interpreted by humans, which is output as an HTML file.", "Since the rule-based methods (TextBlob and VADER) do not output class probabilities (they output just a single sentiment score), in order to explain their results using LIME, we have to artificially generate class probabilities for these methods. Although this isn\u2019t a formal procedure, a simple workaround to simulate class probabilities using a continuous-valued sentiment score (in the range [-1, +1]) would be to normalize the float score to be within the range [0, 1], and then convert it to a discrete integer class by scaling it 5 times in magnitude. This is done for TextBlob and VADER as follows.", "In addition, a rule-based model outputs one and only one prediction, so to avoid outputting zero probabilities for the other classes, a normal distribution consisting of 5 points with the mean as the predicted integer class, is used to assign small non-zero probabilities to the remaining classes. The below plots show how this is done. This array of simulated probabilities can now be used by LIME for generating explanations on the rule-based model.", "To understand the predictions, the file explainer.py is run for each of the six trained classifiers \u2014 this outputs an HTML file with visual content that helps us interpret the models\u2019 feature understanding.", "From the EDA done in the previous post, we know that classes 1 and 3 are the minority classes in the SST-5 dataset, so two samples are chosen from these two classes.", "It\u2019s not horrible, just horribly mediocre. (True label 1)", "The cast is uniformly excellent \u2026 but the film itself is merely mildly charming. (True label 3)", "Each of these samples contains modifiers, conflicting vocabulary and rapidly varying polarities across the sentence, so they should in principle, help expose what each classifier is focusing on.", "Example 1: \u201cIt\u2019s not horrible, just horribly mediocre.\u201d", "For the sentence \u201cIt\u2019s not horrible, just horribly mediocre\u201d, the use of a negation term before the word \u201chorrible\u201d convinces TextBlob that the item isn\u2019t horrible \u2014 however, the second clause of the sentence \u201cjust horribly mediocre\u201d reaffirms the fact that the item is indeed, very mediocre. The sentence as a whole is still overwhelmingly negative, making the true label 1, but TextBlob focuses heavily on the negation term to push the sentiment rating upward to 3.", "Example 2: \u201cThe cast is uniformly excellent \u2026 but the film itself is merely mildly charming.\u201d", "The above sentence is really challenging, because the first half is overwhelmingly positive, while the second half is quite negative \u2014 making the overall sentence neutral in sentiment. The words \u201cmildly\u201d, \u201ccharming\u201d and \u201cexcellent\u201d contribute heavily to TextBlob incorrectly classifying this sentence as strongly positive (label 5). In general, multiple occurrences of strongly positive words tend to push the rule-based algorithm in TextBlob to classify the sentence as positive overall.", "In the above example, VADER predicts a completely opposite sentiment rating to what was expected. VADER tends to heavily penalize negative polarity words (such as \u201chorrible\u201d), but also heavily reward negation terms that precede negative polarity words (\u201cnot horrible\u201d). The presence of \u201chorribly mediocre\u201d in the latter half of the sentence has no impact whatsoever because the strong positive score in the first half outweighs everything that comes after \u2014 the rule-based approach in VADER proved a little too clever for itself in this example.", "Once again, the rule-based approach in VADER heavily weights positive-sounding words like \u201cexcellent\u201d and \u201ccharming\u201d to incorrectly give this sentence a label 5. The words \u201cmerely\u201d and \u201cmildly\u201d, which should be bringing down the overall sentence score, are ignored by VADER\u2014 this is most likely because these words do not appear in VADER\u2019s sentiment lexicon or rule-based modifiers.", "Logistic regression does seem to learn individual features (tokens) that modify the overall sentiment rating of the sentence \u2014 as can be seen above, the words \u201cjust\u201d and \u201cmediocre\u201d contribute more towards label 2, whereas \u201chorrible\u201d and \u201chorribly\u201d contribute to \u201cnot 2\u201d (presumably label 1). The prediction probabilities for both labels 1 and 2 are nearly equal, so the feature-based approach in this case errs by a very fine margin towards label 2.", "In the second example, it is clear that the words \u201cbut\u201d, \u201cmildly\u201d and \u201cmerely\u201d are correctly identified by the logistic regression model as swinging the overall sentence sentiment towards a neutral state.", "The SVM, unlike the logistic regression, focuses more heavily on the strongly negative words \u201chorrible\u201d and \u201chorribly\u201d to assign this a label of 1. The negation term \u201cnot\u201d and the word \u201cmediocre\u201d are identified correctly as pushing the rating away from 1\u2014 but their effect is low and the overall sentiment label is still 1.", "In the second example, it is clear that the SVM learned the right features that make the overall sentence neutral. The words \u201cmerely\u201d, \u201cmildly\u201d and \u201cbut\u201d decrease the probability that the sentence is assigned labels 2, 4 and 5, making the label 3 the most likely.", "FastText focuses heavily on the strongly negative words in example 1 \u2014 the other class labels have a zero probability. It does get the overall prediction right, but this could just have been because FastText learned that \u201chorrible\u201d is a word that regularly appears in strongly negative sentences.", "In example 2, it is more clear why the FastText model correctly predicts the neutral label. Since the model was trained using word-trigrams and a context window of 5 (see the previous post for training parameters) \u2014 the model looks at sequences of tokens while making a prediction. For example, the words \u201cuniformly excellent\u201d are immediately followed by \u201cbut\u201d, and the words \u201cmerely mildly\u201d immediately precede \u201ccharming\u201d, so looking at the highlighted words in the visualization, the model is aware of the sequences of word co-occurrences (and not individual words) that make this sentence neutral.", "The Flair + ELMo embedding model gets both example predictions wrong by a fine margin. In both the above visualizations, the model seems to give a high weight to the period (.) token at the end of each sentence\u2014 this wasn\u2019t the case with the other classifiers. The probabilities for the predicted label are very close to the correct label in either case, so the model seems to be on the right track in terms of how it learns from the data.", "An important point to note: The Flair + ELMo model was underfitting during the training stage, i.e. the validation loss was still decreasing even after 25 epochs of training (see Part 1 of this series) \u2014 this means that further training could push the classifier towards the correct probability outputs for these and other examples.", "Studying the explained results across the various methods, we can make some observations on the merits and demerits, as well as the effects of key variables on each method\u2019s performance.", "TextBlob and VADER tend to heavily weight words with strong polarity, even when there are other words with a milder polarity (or negation terms) that alter the sentiment of the overall sentence. Hardcoded rules work well in many cases, but natural language from the real world has too much variability for these kinds of rules to work well in practice, at least for fine-grained sentiment analysis.", "Embedding-based methods have the best handle on cases that involve strongly polar words. The FastText model was trained with trigrams, so it learned to pick up sequences of words that preceded or succeeded a strongly polar word \u2014 so it wasn\u2019t as easily fooled, allowing it to make better predictions on the neutral class compared to the rule-based and feature-based methods. The Flair model, because of its contextual embeddings and strong underlying language model, was able to more accurately identify patterns involving sequences of strong polarity words.", "Long samples, especially multi-sentence samples can cause trouble when using rule-based methods, which tend to apply some kind of weighted averaging to capture overall sentence polarity. Hence, the longer the sentence, the greater the chance of diffusing the actual sentiment of individual clauses in the long sample.", "Really short (single or two-word) samples pose a different challenge to the models \u2014 they either contain unseen words, or provide too little context such as punctuation or similar subwords that a model could have used to categorize the word. In general, rule-based models would fare badly in these situations, because the lack of modifiers (\u201cvery\u201d or \u201ctoo\u201d) do not provide enough of a notion of sentiment intensity in our fine-grained classification scenario.", "In samples where there numerous unseen words \u2014 or really short samples, where the chances that a word was unseen during training are quite high \u2014 models that lack a sequential (n-gram) or contextual representation (embeddings) tend to struggle to make reliable fine-grained sentiment predictions. The feature-based models in scikit-learn showcase this issue quite clearly. Since they rely on word co-occurrence counts during training, if an unseen word shows up in the test set, the word is ignored by the feature-based classifier. This misses out on features that might have given the model more context.", "FastText is better equipped to deal with unseen words due to its character n-gram subword representations, but it doesn\u2019t have as deep a pre-trained representation as the Flair model does, to properly deal with unseen words. The Flair + ELMo embedding model uses contextual representations from a pretrained vocabulary (1 billion words of news crawl data). This gives it a significant advantage over the shallower models \u2014 the contextualized embeddings carry useful information that identify words similar in meaning as well as their subword representations.", "All rule-based methods involve zero-training time and are very quick during the prediction stage \u2014 however this comes at the cost of lack of stability in results with unseen, real-world data. The feature-based methods are also very quick to train and make inferences with, due to highly efficient vectorized representations in scikit-learn. However, these models also come with limitations in terms of not capturing relationships between words and poor handling of unseen words.", "FastText is a good compromise between cost of computation (it is blazing fast due to its underlying C++ and Cython bindings) and classification accuracy. For fine-grained sentiment analysis, training the model using trigram representations is a must, in order to capture the finer gradations involved in sequences of words that occur in long sentences.", "Flair is the most expensive option of all, mainly due to the fact that it is a large deep learning model that uses pre-trained representations from a combination of string and word embeddings. Training this model can take of the order of several hours (or days, depending on the size of the dataset). A part of the reason that the Flair + ELMo model is slow during training and inference is the way that ELMo embeddings are looked up during run time \u2014 this can be sped up using mini-batching \u2014 however, the reality is that models like Flair might well prove to be too expensive on real-world datasets for fast and efficient inference.", "To make it easier to see explanations for a range of models on a number of difference examples, a dashboard was created using the Flask micro-framework. The dashboard was deployed using Heroku (in a similar way to this example). To test the dashboard interactively, input a text sample and choose the classifier whose classification prediction we want explained, as shown below.", "Try out the dashboard here and make your own explanations!", "In this post, we discussed how to generate and interpret LIME explanations for fine-grained sentiment using six different classifiers in Python. The differences in the features focused on by each model are clearly apparent, making it a more straightforward task to choose the right classifier for the task at hand. The more complex the underlying representation of the vocabulary (especially contextual embeddings), the more reliable the model\u2019s predictions across the five sentiment classes for the SST-5 dataset.", "The below accuracy/F1-scores on the SST-5 dataset were obtained thus far:", "While the Flair + ELMo embedding-based model does achieve a decent accuracy of 48.9%, this is still very far from the state-of-the-art (64.4%). In addition, the Flair model is very expensive to train and make predictions with, so a more computationally efficient, yet contextually aware model is desirable.", "In Part 3 of this series, we\u2019ll see how to improve on these results further using a transformer model with transfer learning. Thanks for reading!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Engineer by training. Machine Learning practitioner. I like writing about science, technology and computing."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F2a92fdc0160d&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-grained-sentiment-analysis-in-python-part-2-2a92fdc0160d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-grained-sentiment-analysis-in-python-part-2-2a92fdc0160d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-grained-sentiment-analysis-in-python-part-2-2a92fdc0160d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-grained-sentiment-analysis-in-python-part-2-2a92fdc0160d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----2a92fdc0160d--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2a92fdc0160d--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@tech_optimist?source=post_page-----2a92fdc0160d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@tech_optimist?source=post_page-----2a92fdc0160d--------------------------------", "anchor_text": "Prashanth Rao"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fefdb4f4a0d1b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-grained-sentiment-analysis-in-python-part-2-2a92fdc0160d&user=Prashanth+Rao&userId=efdb4f4a0d1b&source=post_page-efdb4f4a0d1b----2a92fdc0160d---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2a92fdc0160d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-grained-sentiment-analysis-in-python-part-2-2a92fdc0160d&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2a92fdc0160d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-grained-sentiment-analysis-in-python-part-2-2a92fdc0160d&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://pixabay.com/illustrations/question-mark-important-sign-1872665/", "anchor_text": "Pixabay"}, {"url": "https://arxiv.org/abs/1602.04938", "anchor_text": "Ribeiro et al."}, {"url": "https://medium.com/@tech_optimist/fine-grained-sentiment-analysis-in-python-part-1-2697bb111ed4", "anchor_text": "Part 1"}, {"url": "https://arxiv.org/pdf/1602.04938.pdf", "anchor_text": "original paper"}, {"url": "https://arxiv.org/pdf/1602.04938.pdf", "anchor_text": "From the paper"}, {"url": "https://en.wikipedia.org/wiki/Multiclass_classification#One-vs.-rest", "anchor_text": "one-vs-rest method"}, {"url": "https://github.com/prrao87/fine-grained-sentiment/blob/master/explainer.py", "anchor_text": "project\u2019s GitHub repo [explainer.py"}, {"url": "https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html", "anchor_text": "normal distribution"}, {"url": "https://github.com/prrao87/fine-grained-sentiment/blob/master/explainer.py", "anchor_text": "explainer.py"}, {"url": "https://medium.com/@tech_optimist/fine-grained-sentiment-analysis-in-python-part-1-2697bb111ed4", "anchor_text": "EDA done in the previous post"}, {"url": "https://ai.google/research/pubs/pub41880", "anchor_text": "1 billion words of news crawl data"}, {"url": "https://github.com/zalandoresearch/flair/issues/915#issuecomment-513152226", "anchor_text": "ELMo embeddings are looked up during run time"}, {"url": "https://medium.com/the-andela-way/deploying-a-python-flask-app-to-heroku-41250bda27d0", "anchor_text": "in a similar way to this example"}, {"url": "https://sst5-explainer.herokuapp.com", "anchor_text": "Try out the dashboard here"}, {"url": "https://sst5-explainer.herokuapp.com", "anchor_text": "https://sst5-explainer.herokuapp.com"}, {"url": "https://medium.com/@tech_optimist/fine-grained-sentiment-analysis-part-3-fine-tuning-transformers-1ae6574f25a6", "anchor_text": "Part 3"}, {"url": "https://medium.com/@ageitgey/natural-language-processing-is-fun-part-3-explaining-model-predictions-486d8616813c", "anchor_text": "this excellent post by Adam Geitgey"}, {"url": "https://blog.miguelgrinberg.com/post/the-flask-mega-tutorial-part-ii-templates", "anchor_text": "this excellent Flask tutorial"}, {"url": "https://github.com/prrao87/fine-grained-sentiment", "anchor_text": "this project\u2019s GitHub repo"}, {"url": "https://github.com/prrao87/fine-grained-sentiment-app", "anchor_text": "this separate repo"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----2a92fdc0160d---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/nlp?source=post_page-----2a92fdc0160d---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----2a92fdc0160d---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----2a92fdc0160d---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/data-science?source=post_page-----2a92fdc0160d---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2a92fdc0160d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-grained-sentiment-analysis-in-python-part-2-2a92fdc0160d&user=Prashanth+Rao&userId=efdb4f4a0d1b&source=-----2a92fdc0160d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2a92fdc0160d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-grained-sentiment-analysis-in-python-part-2-2a92fdc0160d&user=Prashanth+Rao&userId=efdb4f4a0d1b&source=-----2a92fdc0160d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2a92fdc0160d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-grained-sentiment-analysis-in-python-part-2-2a92fdc0160d&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2a92fdc0160d--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F2a92fdc0160d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-grained-sentiment-analysis-in-python-part-2-2a92fdc0160d&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----2a92fdc0160d---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----2a92fdc0160d--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----2a92fdc0160d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----2a92fdc0160d--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----2a92fdc0160d--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----2a92fdc0160d--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----2a92fdc0160d--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----2a92fdc0160d--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----2a92fdc0160d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@tech_optimist?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@tech_optimist?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Prashanth Rao"}, {"url": "https://medium.com/@tech_optimist/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "473 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fefdb4f4a0d1b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-grained-sentiment-analysis-in-python-part-2-2a92fdc0160d&user=Prashanth+Rao&userId=efdb4f4a0d1b&source=post_page-efdb4f4a0d1b--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F5aedbeec5972&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-grained-sentiment-analysis-in-python-part-2-2a92fdc0160d&newsletterV3=efdb4f4a0d1b&newsletterV3Id=5aedbeec5972&user=Prashanth+Rao&userId=efdb4f4a0d1b&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}