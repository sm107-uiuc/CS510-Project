{"url": "https://towardsdatascience.com/high-level-history-of-nlp-models-bc8c8b142ef7", "time": 1683002852.558065, "path": "towardsdatascience.com/high-level-history-of-nlp-models-bc8c8b142ef7/", "webpage": {"metadata": {"title": "High-Level History of NLP Models. How we arrived at our current state of\u2026 | by Mallory Hightower | Towards Data Science", "h1": "High-Level History of NLP Models", "description": "Natural Language Processing (NLP), enabling computers to make sense of the human language, is not a novel concept. However, the last decade has witnessed an unprecedented leap in NLP technology\u2026"}, "outgoing_paragraph_urls": [{"url": "https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf", "anchor_text": "paper by Mikolov", "paragraph_index": 1}, {"url": "http://karpathy.github.io/2015/05/21/rnn-effectiveness/", "anchor_text": "Karpathy\u2019s blog post", "paragraph_index": 2}, {"url": "https://arxiv.org/abs/1810.04805v2", "anchor_text": "introduced in 2018", "paragraph_index": 5}, {"url": "https://arxiv.org/pdf/1906.08237.pdf", "anchor_text": "created XLNet", "paragraph_index": 7}, {"url": "https://pdfs.semanticscholar.org/9025/1aa6225fcd5687542eab5819db18afb6a20f.pdf?_ga=2.197402415.1293825788.1578939927-1092130271.1578939927", "anchor_text": "paper", "paragraph_index": 7}, {"url": "https://arxiv.org/abs/1907.11692", "anchor_text": "RoBERTa", "paragraph_index": 7}, {"url": "https://paperswithcode.com/area/natural-language-processing", "anchor_text": "paperswithcode.com", "paragraph_index": 8}], "all_paragraphs": ["Natural Language Processing (NLP), enabling computers to make sense of the human language, is not a novel concept. However, the last decade has witnessed an unprecedented leap in NLP technology advancement, with much of the progress enabled by deep learning. NLP technology has progressed so rapidly that data scientists must continually learn new machine learning techniques and model architectures. Thankfully, since the development of the current state of the art NLP architecture, attention based models, progress in the NLP field seems to have slowed momentarily. Data scientists finally have a moment to catch up!", "But how did we arrive at our current state in NLP? The first big advancement came in 2013 with the breakthrough research of Word2Vec (detailed in a paper by Mikolov). Mikolov et al realized that as they trained neural networks on an NLP task, the network was forced to learn the similarities between words. These vector representations of words were stored in the embedding layer of the neural network, and their discovery has added an entirely new dimension to NLP tasks. Thanks to Word2Vec, we now have a more efficient way to create word vectors. We no longer have to rely on the traditional sparse representation of words with one hot encoding. Additionally, utilizing word embeddings requires less memory, decreases compute time, and has been shown to drastically improve downstream model performance. Other word representation models, such as GloVe, have followed. No more one hot encoding!", "Thanks to advances in deep learning and increasing computational capabilities, Recurrent Neural Networks (RNN) and Long Short-Term Memory networks (LSTM), a version of the RNN, rose in popularity in 2014 and 2015. Andrej Karpathy\u2019s blog post entitled \u201cThe Unreasonable Effectiveness of Recurrent Neural Networks\u201d is a famous and well-referenced love letter to RNN\u2019s. RNN\u2019s and LSTM\u2019s enabled the processing of textual sequence data. The order of the data matters with sequence data and there was no good way to handle sequence data before RNN\u2019s. LSTM\u2019s improved on RNN\u2019s in that for long sequences, the network remembers the earlier sequence inputs. This was a significant problem for RNN\u2019s, also known as the vanishing gradient problem. LSTM\u2019s remember what information is important in the sequence and prevent the weights of the early inputs from decreasing to zero. There is an additional version of the RNN, called a Gated Recurrent Unit (GRU). It is very similar to the LSTM but differs in its special gates for retaining long sequence information. RNN\u2019s and LSTM\u2019s were the bread and butter of NLP tasks for a few years \u2014 everyone used them. But it wasn\u2019t long before they were replaced by an even better architecture: attention networks!", "Attention based networks became popular around the years 2015\u20132016. Attention networks are a type of neural network that allow for the focus on a specific subset of the data input: you can specify what you want the network to pay attention to. These models have been breaking performance records on many NLP tasks, like Neural Machine Translation, Language Modeling, and Question Answering problems. Attention networks are also more efficient and require less computational resources. This is an important improvement, as it often requires significant computing power in the form of a GPU (which is not always accessible) to train RNN\u2019s.", "A specific type of attention based network introduced in 2017, the Transformer model, has been especially dominant in modern NLP architecture. The Transformer is similar to RNN\u2019s in that it handles sequence data, but the data doesn\u2019t need to be input into the model in any particular order. Because of this, the Transformer model can train faster and with much more data using parallelization. The Transformer model led to our current state in NLP: the era of BERT, ERNIE 2.0 and XLNet.", "Bidirectional Encoder Representations from Transformers (BERT) models were introduced in 2018 by researchers at Google. Versions of BERT are one of the most advanced NLP models available. BERT, a deeply bidirectional unsupervised model, is used for pre-training word representations for later use in an NLP task. Bi-directionality is crucial in neural networks as it allows the information to flow forwards and backwards as the model trains, which results in better model performance.", "While the concept of BERT is similar to Word2Vec and GloVe, the BERT word vectors are context sensitive! With Word2Vec and GloVe, words that have high contextual variety (I am feeling blue, blue is my favorite color) are represented by a single vector. You can guess that this type of representation could result in poor model performance downstream, as word meaning relies heavily on context. With BERT, the two contexts of the word blue would be represented with different vectors.", "BERT was just the tip of the iceberg for attention based architectures. In 2019, researchers at Carnegie Mellon and Google created XLNet. The paper claims that XLNet \u201coutperforms BERT on 20 tasks, often by a large margin.\u201d Unlike other recent advancements in NLP, the architecture is not drastically different. Like BERT, XLNet utilizes an attention based network. In the summer of 2019, a Chinese technology company published a paper on another attention based network, ERNIE 2.0. The paper claims that ERNIE 2.0 outperforms BERT and XLNet on 16 tasks, including Chinese language tasks. Like BERT, ERNIE 2.0 and XLNet are both pre-training models that make use of transformer architectures and attention mechanisms. While the original BERT model is no longer king, versions of BERT such as RoBERTa, remain competitive in the NLP leading technology space.", "In conclusion, there is no single, overall best NLP model at the moment. However, the attention based, transformer networks are the reigning architecture. The top models perform well on different tasks and each have their own unique advantages and drawbacks. With all of these competing models, it can be difficult to figure out which model is the best for your task. One of my new, favorite resources is paperswithcode.com. This website conveniently organizes research papers according to specific machine learning tasks, enabling you to stay up to date on the newest models and architectures.", "So there you have it \u2014 a brief history of the rapid advances in NLP within the last decade. NLP is an ever changing and developing field, certainly not for the data scientist that prefers model stability. But that is part of the fun! We will see how long the era of the attention based networks last.", "Data Scientist @ Walmart Technology, SMU Data Science Graduate"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fbc8c8b142ef7&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhigh-level-history-of-nlp-models-bc8c8b142ef7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhigh-level-history-of-nlp-models-bc8c8b142ef7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhigh-level-history-of-nlp-models-bc8c8b142ef7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhigh-level-history-of-nlp-models-bc8c8b142ef7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/@malhightower?source=post_page-----bc8c8b142ef7--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----bc8c8b142ef7--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@malhightower?source=post_page-----bc8c8b142ef7--------------------------------", "anchor_text": "Mallory Hightower"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F64d00e1d18a9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhigh-level-history-of-nlp-models-bc8c8b142ef7&user=Mallory+Hightower&userId=64d00e1d18a9&source=post_page-64d00e1d18a9----bc8c8b142ef7---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----bc8c8b142ef7--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbc8c8b142ef7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhigh-level-history-of-nlp-models-bc8c8b142ef7&user=Mallory+Hightower&userId=64d00e1d18a9&source=-----bc8c8b142ef7---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbc8c8b142ef7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhigh-level-history-of-nlp-models-bc8c8b142ef7&source=-----bc8c8b142ef7---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://pixabay.com/photos/small-toy-figurine-cartoon-3871893/", "anchor_text": "source"}, {"url": "https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf", "anchor_text": "paper by Mikolov"}, {"url": "http://karpathy.github.io/2015/05/21/rnn-effectiveness/", "anchor_text": "Karpathy\u2019s blog post"}, {"url": "https://pixabay.com/illustrations/banner-header-attention-caution-1165973/", "anchor_text": "source"}, {"url": "https://pixabay.com/photos/universal-studios-singapore-2413365/", "anchor_text": "source"}, {"url": "https://arxiv.org/abs/1810.04805v2", "anchor_text": "introduced in 2018"}, {"url": "https://arxiv.org/pdf/1906.08237.pdf", "anchor_text": "created XLNet"}, {"url": "https://pdfs.semanticscholar.org/9025/1aa6225fcd5687542eab5819db18afb6a20f.pdf?_ga=2.197402415.1293825788.1578939927-1092130271.1578939927", "anchor_text": "paper"}, {"url": "https://arxiv.org/abs/1907.11692", "anchor_text": "RoBERTa"}, {"url": "https://paperswithcode.com/area/natural-language-processing", "anchor_text": "paperswithcode.com"}, {"url": "https://paperswithcode.com/area/natural-language-processing", "anchor_text": "Browse state-of-the-art in MLPapers With Code highlights trending ML research and the code to implement it.paperswithcode.com"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----bc8c8b142ef7---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/nlp?source=post_page-----bc8c8b142ef7---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/bert?source=post_page-----bc8c8b142ef7---------------bert-----------------", "anchor_text": "Bert"}, {"url": "https://medium.com/tag/xlnet?source=post_page-----bc8c8b142ef7---------------xlnet-----------------", "anchor_text": "Xlnet"}, {"url": "https://medium.com/tag/data-science?source=post_page-----bc8c8b142ef7---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbc8c8b142ef7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhigh-level-history-of-nlp-models-bc8c8b142ef7&user=Mallory+Hightower&userId=64d00e1d18a9&source=-----bc8c8b142ef7---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbc8c8b142ef7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhigh-level-history-of-nlp-models-bc8c8b142ef7&user=Mallory+Hightower&userId=64d00e1d18a9&source=-----bc8c8b142ef7---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbc8c8b142ef7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhigh-level-history-of-nlp-models-bc8c8b142ef7&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/@malhightower?source=post_page-----bc8c8b142ef7--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----bc8c8b142ef7--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F64d00e1d18a9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhigh-level-history-of-nlp-models-bc8c8b142ef7&user=Mallory+Hightower&userId=64d00e1d18a9&source=post_page-64d00e1d18a9----bc8c8b142ef7---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F497fc328fcdf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhigh-level-history-of-nlp-models-bc8c8b142ef7&newsletterV3=64d00e1d18a9&newsletterV3Id=497fc328fcdf&user=Mallory+Hightower&userId=64d00e1d18a9&source=-----bc8c8b142ef7---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/@malhightower?source=post_page-----bc8c8b142ef7--------------------------------", "anchor_text": "Written by Mallory Hightower"}, {"url": "https://medium.com/@malhightower/followers?source=post_page-----bc8c8b142ef7--------------------------------", "anchor_text": "106 Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----bc8c8b142ef7--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F64d00e1d18a9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhigh-level-history-of-nlp-models-bc8c8b142ef7&user=Mallory+Hightower&userId=64d00e1d18a9&source=post_page-64d00e1d18a9----bc8c8b142ef7---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F497fc328fcdf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhigh-level-history-of-nlp-models-bc8c8b142ef7&newsletterV3=64d00e1d18a9&newsletterV3Id=497fc328fcdf&user=Mallory+Hightower&userId=64d00e1d18a9&source=-----bc8c8b142ef7---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/when-clustering-doesnt-make-sense-c6ed9a89e9e6?source=author_recirc-----bc8c8b142ef7----0---------------------bb76b79c_3f9b_40eb_86bf_31643f85a53a-------", "anchor_text": ""}, {"url": "https://medium.com/@malhightower?source=author_recirc-----bc8c8b142ef7----0---------------------bb76b79c_3f9b_40eb_86bf_31643f85a53a-------", "anchor_text": ""}, {"url": "https://medium.com/@malhightower?source=author_recirc-----bc8c8b142ef7----0---------------------bb76b79c_3f9b_40eb_86bf_31643f85a53a-------", "anchor_text": "Mallory Hightower"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----bc8c8b142ef7----0---------------------bb76b79c_3f9b_40eb_86bf_31643f85a53a-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/when-clustering-doesnt-make-sense-c6ed9a89e9e6?source=author_recirc-----bc8c8b142ef7----0---------------------bb76b79c_3f9b_40eb_86bf_31643f85a53a-------", "anchor_text": "When Clustering Doesn\u2019t Make SenseA few things to consider before clustering your data"}, {"url": "https://towardsdatascience.com/when-clustering-doesnt-make-sense-c6ed9a89e9e6?source=author_recirc-----bc8c8b142ef7----0---------------------bb76b79c_3f9b_40eb_86bf_31643f85a53a-------", "anchor_text": "\u00b75 min read\u00b7Apr 18, 2019"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc6ed9a89e9e6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhen-clustering-doesnt-make-sense-c6ed9a89e9e6&user=Mallory+Hightower&userId=64d00e1d18a9&source=-----c6ed9a89e9e6----0-----------------clap_footer----bb76b79c_3f9b_40eb_86bf_31643f85a53a-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/when-clustering-doesnt-make-sense-c6ed9a89e9e6?source=author_recirc-----bc8c8b142ef7----0---------------------bb76b79c_3f9b_40eb_86bf_31643f85a53a-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "10"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc6ed9a89e9e6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhen-clustering-doesnt-make-sense-c6ed9a89e9e6&source=-----bc8c8b142ef7----0-----------------bookmark_preview----bb76b79c_3f9b_40eb_86bf_31643f85a53a-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----bc8c8b142ef7----1---------------------bb76b79c_3f9b_40eb_86bf_31643f85a53a-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----bc8c8b142ef7----1---------------------bb76b79c_3f9b_40eb_86bf_31643f85a53a-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----bc8c8b142ef7----1---------------------bb76b79c_3f9b_40eb_86bf_31643f85a53a-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----bc8c8b142ef7----1---------------------bb76b79c_3f9b_40eb_86bf_31643f85a53a-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----bc8c8b142ef7----1---------------------bb76b79c_3f9b_40eb_86bf_31643f85a53a-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----bc8c8b142ef7----1---------------------bb76b79c_3f9b_40eb_86bf_31643f85a53a-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----bb76b79c_3f9b_40eb_86bf_31643f85a53a-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----bc8c8b142ef7----1---------------------bb76b79c_3f9b_40eb_86bf_31643f85a53a-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----bc8c8b142ef7----1-----------------bookmark_preview----bb76b79c_3f9b_40eb_86bf_31643f85a53a-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----bc8c8b142ef7----2---------------------bb76b79c_3f9b_40eb_86bf_31643f85a53a-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----bc8c8b142ef7----2---------------------bb76b79c_3f9b_40eb_86bf_31643f85a53a-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----bc8c8b142ef7----2---------------------bb76b79c_3f9b_40eb_86bf_31643f85a53a-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----bc8c8b142ef7----2---------------------bb76b79c_3f9b_40eb_86bf_31643f85a53a-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----bc8c8b142ef7----2---------------------bb76b79c_3f9b_40eb_86bf_31643f85a53a-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----bc8c8b142ef7----2---------------------bb76b79c_3f9b_40eb_86bf_31643f85a53a-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----2-----------------clap_footer----bb76b79c_3f9b_40eb_86bf_31643f85a53a-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----bc8c8b142ef7----2---------------------bb76b79c_3f9b_40eb_86bf_31643f85a53a-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----bc8c8b142ef7----2-----------------bookmark_preview----bb76b79c_3f9b_40eb_86bf_31643f85a53a-------", "anchor_text": ""}, {"url": "https://medium.com/back-to-the-napkin/intro-to-applied-reinforcement-learning-283052acb414?source=author_recirc-----bc8c8b142ef7----3---------------------bb76b79c_3f9b_40eb_86bf_31643f85a53a-------", "anchor_text": ""}, {"url": "https://medium.com/@malhightower?source=author_recirc-----bc8c8b142ef7----3---------------------bb76b79c_3f9b_40eb_86bf_31643f85a53a-------", "anchor_text": ""}, {"url": "https://medium.com/@malhightower?source=author_recirc-----bc8c8b142ef7----3---------------------bb76b79c_3f9b_40eb_86bf_31643f85a53a-------", "anchor_text": "Mallory Hightower"}, {"url": "https://medium.com/back-to-the-napkin?source=author_recirc-----bc8c8b142ef7----3---------------------bb76b79c_3f9b_40eb_86bf_31643f85a53a-------", "anchor_text": "back to the napkin"}, {"url": "https://medium.com/back-to-the-napkin/intro-to-applied-reinforcement-learning-283052acb414?source=author_recirc-----bc8c8b142ef7----3---------------------bb76b79c_3f9b_40eb_86bf_31643f85a53a-------", "anchor_text": "Intro to Applied Reinforcement LearningWhile reinforcement learning (RL) is a hot topic in the data science community, there is a surprising lack of knowledge on how to run a\u2026"}, {"url": "https://medium.com/back-to-the-napkin/intro-to-applied-reinforcement-learning-283052acb414?source=author_recirc-----bc8c8b142ef7----3---------------------bb76b79c_3f9b_40eb_86bf_31643f85a53a-------", "anchor_text": "\u00b79 min read\u00b7Mar 26, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fback-to-the-napkin%2F283052acb414&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fback-to-the-napkin%2Fintro-to-applied-reinforcement-learning-283052acb414&user=Mallory+Hightower&userId=64d00e1d18a9&source=-----283052acb414----3-----------------clap_footer----bb76b79c_3f9b_40eb_86bf_31643f85a53a-------", "anchor_text": ""}, {"url": "https://medium.com/back-to-the-napkin/intro-to-applied-reinforcement-learning-283052acb414?source=author_recirc-----bc8c8b142ef7----3---------------------bb76b79c_3f9b_40eb_86bf_31643f85a53a-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F283052acb414&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fback-to-the-napkin%2Fintro-to-applied-reinforcement-learning-283052acb414&source=-----bc8c8b142ef7----3-----------------bookmark_preview----bb76b79c_3f9b_40eb_86bf_31643f85a53a-------", "anchor_text": ""}, {"url": "https://medium.com/@malhightower?source=post_page-----bc8c8b142ef7--------------------------------", "anchor_text": "See all from Mallory Hightower"}, {"url": "https://towardsdatascience.com/?source=post_page-----bc8c8b142ef7--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://towardsdatascience.com/how-to-train-a-word2vec-model-from-scratch-with-gensim-c457d587e031?source=read_next_recirc-----bc8c8b142ef7----0---------------------cecfd688_4dbe_4192_bf49_a6b190f043a6-------", "anchor_text": ""}, {"url": "https://medium.com/@theDrewDag?source=read_next_recirc-----bc8c8b142ef7----0---------------------cecfd688_4dbe_4192_bf49_a6b190f043a6-------", "anchor_text": ""}, {"url": "https://medium.com/@theDrewDag?source=read_next_recirc-----bc8c8b142ef7----0---------------------cecfd688_4dbe_4192_bf49_a6b190f043a6-------", "anchor_text": "Andrea D'Agostino"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----bc8c8b142ef7----0---------------------cecfd688_4dbe_4192_bf49_a6b190f043a6-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/how-to-train-a-word2vec-model-from-scratch-with-gensim-c457d587e031?source=read_next_recirc-----bc8c8b142ef7----0---------------------cecfd688_4dbe_4192_bf49_a6b190f043a6-------", "anchor_text": "How to Train a Word2Vec Model from Scratch with GensimIn this article we will explore Gensim, a very popular Python library for training text-based machine learning models, to train a Word2Vec\u2026"}, {"url": "https://towardsdatascience.com/how-to-train-a-word2vec-model-from-scratch-with-gensim-c457d587e031?source=read_next_recirc-----bc8c8b142ef7----0---------------------cecfd688_4dbe_4192_bf49_a6b190f043a6-------", "anchor_text": "\u00b79 min read\u00b7Feb 6"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc457d587e031&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-train-a-word2vec-model-from-scratch-with-gensim-c457d587e031&user=Andrea+D%27Agostino&userId=4e8f67b0b09b&source=-----c457d587e031----0-----------------clap_footer----cecfd688_4dbe_4192_bf49_a6b190f043a6-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-to-train-a-word2vec-model-from-scratch-with-gensim-c457d587e031?source=read_next_recirc-----bc8c8b142ef7----0---------------------cecfd688_4dbe_4192_bf49_a6b190f043a6-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc457d587e031&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-train-a-word2vec-model-from-scratch-with-gensim-c457d587e031&source=-----bc8c8b142ef7----0-----------------bookmark_preview----cecfd688_4dbe_4192_bf49_a6b190f043a6-------", "anchor_text": ""}, {"url": "https://thebabar.medium.com/essential-guide-to-foundation-models-and-large-language-models-27dab58f7404?source=read_next_recirc-----bc8c8b142ef7----1---------------------cecfd688_4dbe_4192_bf49_a6b190f043a6-------", "anchor_text": ""}, {"url": "https://thebabar.medium.com/?source=read_next_recirc-----bc8c8b142ef7----1---------------------cecfd688_4dbe_4192_bf49_a6b190f043a6-------", "anchor_text": ""}, {"url": "https://thebabar.medium.com/?source=read_next_recirc-----bc8c8b142ef7----1---------------------cecfd688_4dbe_4192_bf49_a6b190f043a6-------", "anchor_text": "Babar M Bhatti"}, {"url": "https://thebabar.medium.com/essential-guide-to-foundation-models-and-large-language-models-27dab58f7404?source=read_next_recirc-----bc8c8b142ef7----1---------------------cecfd688_4dbe_4192_bf49_a6b190f043a6-------", "anchor_text": "Essential Guide to Foundation Models and Large Language ModelsThe term Foundation Model (FM) was coined by Stanford researchers to introduce a new category of ML models. They defined FMs as models\u2026"}, {"url": "https://thebabar.medium.com/essential-guide-to-foundation-models-and-large-language-models-27dab58f7404?source=read_next_recirc-----bc8c8b142ef7----1---------------------cecfd688_4dbe_4192_bf49_a6b190f043a6-------", "anchor_text": "\u00b714 min read\u00b7Feb 6"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F27dab58f7404&operation=register&redirect=https%3A%2F%2Fthebabar.medium.com%2Fessential-guide-to-foundation-models-and-large-language-models-27dab58f7404&user=Babar+M+Bhatti&userId=10dee34829b&source=-----27dab58f7404----1-----------------clap_footer----cecfd688_4dbe_4192_bf49_a6b190f043a6-------", "anchor_text": ""}, {"url": "https://thebabar.medium.com/essential-guide-to-foundation-models-and-large-language-models-27dab58f7404?source=read_next_recirc-----bc8c8b142ef7----1---------------------cecfd688_4dbe_4192_bf49_a6b190f043a6-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F27dab58f7404&operation=register&redirect=https%3A%2F%2Fthebabar.medium.com%2Fessential-guide-to-foundation-models-and-large-language-models-27dab58f7404&source=-----bc8c8b142ef7----1-----------------bookmark_preview----cecfd688_4dbe_4192_bf49_a6b190f043a6-------", "anchor_text": ""}, {"url": "https://python.plainenglish.io/topic-modeling-for-beginners-using-bertopic-and-python-aaf1b421afeb?source=read_next_recirc-----bc8c8b142ef7----0---------------------cecfd688_4dbe_4192_bf49_a6b190f043a6-------", "anchor_text": ""}, {"url": "https://erickleppen.medium.com/?source=read_next_recirc-----bc8c8b142ef7----0---------------------cecfd688_4dbe_4192_bf49_a6b190f043a6-------", "anchor_text": ""}, {"url": "https://erickleppen.medium.com/?source=read_next_recirc-----bc8c8b142ef7----0---------------------cecfd688_4dbe_4192_bf49_a6b190f043a6-------", "anchor_text": "Eric Kleppen"}, {"url": "https://python.plainenglish.io/?source=read_next_recirc-----bc8c8b142ef7----0---------------------cecfd688_4dbe_4192_bf49_a6b190f043a6-------", "anchor_text": "Python in Plain English"}, {"url": "https://python.plainenglish.io/topic-modeling-for-beginners-using-bertopic-and-python-aaf1b421afeb?source=read_next_recirc-----bc8c8b142ef7----0---------------------cecfd688_4dbe_4192_bf49_a6b190f043a6-------", "anchor_text": "Topic Modeling For Beginners Using BERTopic and PythonHow to make sense of your text data by reducing it to topics"}, {"url": "https://python.plainenglish.io/topic-modeling-for-beginners-using-bertopic-and-python-aaf1b421afeb?source=read_next_recirc-----bc8c8b142ef7----0---------------------cecfd688_4dbe_4192_bf49_a6b190f043a6-------", "anchor_text": "\u00b711 min read\u00b7Feb 12"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fpython-in-plain-english%2Faaf1b421afeb&operation=register&redirect=https%3A%2F%2Fpython.plainenglish.io%2Ftopic-modeling-for-beginners-using-bertopic-and-python-aaf1b421afeb&user=Eric+Kleppen&userId=1e2ea32699c9&source=-----aaf1b421afeb----0-----------------clap_footer----cecfd688_4dbe_4192_bf49_a6b190f043a6-------", "anchor_text": ""}, {"url": "https://python.plainenglish.io/topic-modeling-for-beginners-using-bertopic-and-python-aaf1b421afeb?source=read_next_recirc-----bc8c8b142ef7----0---------------------cecfd688_4dbe_4192_bf49_a6b190f043a6-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Faaf1b421afeb&operation=register&redirect=https%3A%2F%2Fpython.plainenglish.io%2Ftopic-modeling-for-beginners-using-bertopic-and-python-aaf1b421afeb&source=-----bc8c8b142ef7----0-----------------bookmark_preview----cecfd688_4dbe_4192_bf49_a6b190f043a6-------", "anchor_text": ""}, {"url": "https://medium.com/mlearning-ai/understanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f?source=read_next_recirc-----bc8c8b142ef7----1---------------------cecfd688_4dbe_4192_bf49_a6b190f043a6-------", "anchor_text": ""}, {"url": "https://medium.com/@martin-thissen?source=read_next_recirc-----bc8c8b142ef7----1---------------------cecfd688_4dbe_4192_bf49_a6b190f043a6-------", "anchor_text": ""}, {"url": "https://medium.com/@martin-thissen?source=read_next_recirc-----bc8c8b142ef7----1---------------------cecfd688_4dbe_4192_bf49_a6b190f043a6-------", "anchor_text": "Martin Thissen"}, {"url": "https://medium.com/mlearning-ai?source=read_next_recirc-----bc8c8b142ef7----1---------------------cecfd688_4dbe_4192_bf49_a6b190f043a6-------", "anchor_text": "MLearning.ai"}, {"url": "https://medium.com/mlearning-ai/understanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f?source=read_next_recirc-----bc8c8b142ef7----1---------------------cecfd688_4dbe_4192_bf49_a6b190f043a6-------", "anchor_text": "Understanding and Coding the Attention Mechanism \u2014 The Magic Behind TransformersIn this article, I\u2019ll give you an introduction to the attention mechanism and show you how to code the attention mechanism yourself."}, {"url": "https://medium.com/mlearning-ai/understanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f?source=read_next_recirc-----bc8c8b142ef7----1---------------------cecfd688_4dbe_4192_bf49_a6b190f043a6-------", "anchor_text": "\u00b712 min read\u00b7Dec 6, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fmlearning-ai%2Ffe707a85cc3f&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fmlearning-ai%2Funderstanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f&user=Martin+Thissen&userId=f99c73950195&source=-----fe707a85cc3f----1-----------------clap_footer----cecfd688_4dbe_4192_bf49_a6b190f043a6-------", "anchor_text": ""}, {"url": "https://medium.com/mlearning-ai/understanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f?source=read_next_recirc-----bc8c8b142ef7----1---------------------cecfd688_4dbe_4192_bf49_a6b190f043a6-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffe707a85cc3f&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fmlearning-ai%2Funderstanding-and-coding-the-attention-mechanism-the-magic-behind-transformers-fe707a85cc3f&source=-----bc8c8b142ef7----1-----------------bookmark_preview----cecfd688_4dbe_4192_bf49_a6b190f043a6-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/generating-word-embeddings-from-text-data-using-skip-gram-algorithm-and-deep-learning-in-python-a8873b225ab6?source=read_next_recirc-----bc8c8b142ef7----2---------------------cecfd688_4dbe_4192_bf49_a6b190f043a6-------", "anchor_text": ""}, {"url": "https://angeleastbengal.medium.com/?source=read_next_recirc-----bc8c8b142ef7----2---------------------cecfd688_4dbe_4192_bf49_a6b190f043a6-------", "anchor_text": ""}, {"url": "https://angeleastbengal.medium.com/?source=read_next_recirc-----bc8c8b142ef7----2---------------------cecfd688_4dbe_4192_bf49_a6b190f043a6-------", "anchor_text": "Angel Das"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----bc8c8b142ef7----2---------------------cecfd688_4dbe_4192_bf49_a6b190f043a6-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/generating-word-embeddings-from-text-data-using-skip-gram-algorithm-and-deep-learning-in-python-a8873b225ab6?source=read_next_recirc-----bc8c8b142ef7----2---------------------cecfd688_4dbe_4192_bf49_a6b190f043a6-------", "anchor_text": "Generating Word Embeddings from Text Data using Skip-Gram Algorithm and Deep Learning in PythonIntroduction to embeddings in natural language processing using Artificial Neural Network and Gensim"}, {"url": "https://towardsdatascience.com/generating-word-embeddings-from-text-data-using-skip-gram-algorithm-and-deep-learning-in-python-a8873b225ab6?source=read_next_recirc-----bc8c8b142ef7----2---------------------cecfd688_4dbe_4192_bf49_a6b190f043a6-------", "anchor_text": "\u00b713 min read\u00b7Nov 9, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa8873b225ab6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerating-word-embeddings-from-text-data-using-skip-gram-algorithm-and-deep-learning-in-python-a8873b225ab6&user=Angel+Das&userId=8418ab50405a&source=-----a8873b225ab6----2-----------------clap_footer----cecfd688_4dbe_4192_bf49_a6b190f043a6-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/generating-word-embeddings-from-text-data-using-skip-gram-algorithm-and-deep-learning-in-python-a8873b225ab6?source=read_next_recirc-----bc8c8b142ef7----2---------------------cecfd688_4dbe_4192_bf49_a6b190f043a6-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "4"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa8873b225ab6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerating-word-embeddings-from-text-data-using-skip-gram-algorithm-and-deep-learning-in-python-a8873b225ab6&source=-----bc8c8b142ef7----2-----------------bookmark_preview----cecfd688_4dbe_4192_bf49_a6b190f043a6-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----bc8c8b142ef7----3---------------------cecfd688_4dbe_4192_bf49_a6b190f043a6-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----bc8c8b142ef7----3---------------------cecfd688_4dbe_4192_bf49_a6b190f043a6-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----bc8c8b142ef7----3---------------------cecfd688_4dbe_4192_bf49_a6b190f043a6-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----bc8c8b142ef7----3---------------------cecfd688_4dbe_4192_bf49_a6b190f043a6-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----bc8c8b142ef7----3---------------------cecfd688_4dbe_4192_bf49_a6b190f043a6-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----bc8c8b142ef7----3---------------------cecfd688_4dbe_4192_bf49_a6b190f043a6-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----3-----------------clap_footer----cecfd688_4dbe_4192_bf49_a6b190f043a6-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----bc8c8b142ef7----3---------------------cecfd688_4dbe_4192_bf49_a6b190f043a6-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----bc8c8b142ef7----3-----------------bookmark_preview----cecfd688_4dbe_4192_bf49_a6b190f043a6-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----bc8c8b142ef7--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----bc8c8b142ef7--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----bc8c8b142ef7--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----bc8c8b142ef7--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----bc8c8b142ef7--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----bc8c8b142ef7--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----bc8c8b142ef7--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----bc8c8b142ef7--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----bc8c8b142ef7--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----bc8c8b142ef7--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}