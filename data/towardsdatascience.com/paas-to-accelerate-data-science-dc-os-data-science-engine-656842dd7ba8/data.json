{"url": "https://towardsdatascience.com/paas-to-accelerate-data-science-dc-os-data-science-engine-656842dd7ba8", "time": 1683000767.1135051, "path": "towardsdatascience.com/paas-to-accelerate-data-science-dc-os-data-science-engine-656842dd7ba8/", "webpage": {"metadata": {"title": "Using PaaS to Accelerate Data Science | by Russell Jurney | Towards Data Science", "h1": "Using PaaS to Accelerate Data Science", "description": "In this series of posts I explore different Platforms as a Service for Data Science to find a way to easily run the code of my new book, Weakly Supervised Learning. This first tutorial is about DC/OS."}, "outgoing_paragraph_urls": [{"url": "https://www.amazon.com/Agile-Data-Science-2-0-Applications/dp/1491960116", "anchor_text": "Agile Data Science 2.0", "paragraph_index": 1}, {"url": "https://github.com/rjurney/Agile_Data_Code_2", "anchor_text": "my own platform", "paragraph_index": 1}, {"url": "https://stedolan.github.io/jq/", "anchor_text": "jq", "paragraph_index": 1}, {"url": "https://github.com/rjurney/Agile_Data_Code_2/issues", "anchor_text": "85 issues", "paragraph_index": 1}, {"url": "https://github.com/rjurney/paas_blog", "anchor_text": "github.com/rjurney/paas_blog", "paragraph_index": 2}, {"url": "https://docs.d2iq.com/mesosphere/dcos/1.13/installing/evaluation/aws/", "anchor_text": "Universal Installer", "paragraph_index": 3}, {"url": "https://universe.dcos.io/#/package/data-science-engine/version/latest", "anchor_text": "Data Science Engine package", "paragraph_index": 3}, {"url": "https://docs.d2iq.com/mesosphere/dcos/1.13/installing/evaluation/aws/", "anchor_text": "Universal Installer documentation", "paragraph_index": 4}, {"url": "https://docs.d2iq.com/mesosphere/dcos/services/data-science-engine/1.0.0/quick-start/", "anchor_text": "Data Science Engine documentation", "paragraph_index": 4}, {"url": "https://github.com/yahoo/TensorFlowOnSpark", "anchor_text": "TensorFlowOnSpark", "paragraph_index": 7}, {"url": "https://docs.d2iq.com/mesosphere/dcos/1.13/installing/evaluation/aws/", "anchor_text": "DC/OS Universal Installer", "paragraph_index": 8}, {"url": "https://www.python.org/about/gettingstarted/", "anchor_text": "Python", "paragraph_index": 8}, {"url": "https://pip.pypa.io/en/stable/", "anchor_text": "pip", "paragraph_index": 8}, {"url": "https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-welcome.html", "anchor_text": "AWS Command Line Interface (CLI)", "paragraph_index": 8}, {"url": "https://www.terraform.io/intro/index.html", "anchor_text": "Terraform", "paragraph_index": 8}, {"url": "https://www.anaconda.com/distribution/", "anchor_text": "Anaconda Python", "paragraph_index": 8}, {"url": "https://www.anaconda.com/distribution/#download-section", "anchor_text": "here", "paragraph_index": 11}, {"url": "https://docs.anaconda.com/anaconda/install/linux/", "anchor_text": "Linux", "paragraph_index": 11}, {"url": "https://docs.anaconda.com/anaconda/install/mac-os/", "anchor_text": "Mac OS X", "paragraph_index": 11}, {"url": "https://docs.anaconda.com/anaconda/install/windows/", "anchor_text": "Windows", "paragraph_index": 11}, {"url": "https://www.terraform.io/docs/providers/aws/index.html", "anchor_text": "Terraform AWS provider", "paragraph_index": 12}, {"url": "https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html#cli-quick-configuration", "anchor_text": "AWS CLI", "paragraph_index": 12}, {"url": "https://pypi.org/project/awscli/", "anchor_text": "awscli package", "paragraph_index": 13}, {"url": "https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html", "anchor_text": "setup", "paragraph_index": 14}, {"url": "https://console.aws.amazon.com/iam/home#/security_credentials", "anchor_text": "IAM console", "paragraph_index": 14}, {"url": "https://docs.aws.amazon.com/general/latest/gr/aws_service_limits.html", "anchor_text": "AWS Service Limits", "paragraph_index": 20}, {"url": "https://us-west-2.console.aws.amazon.com/servicequotas/home?region=us-west-2#!/services/ec2/quotas", "anchor_text": "https://us-west-2.console.aws.amazon.com/servicequotas/home?region=us-west-2#!/services/ec2/quotas", "paragraph_index": 21}, {"url": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html#concepts-available-regions", "anchor_text": "AWS region", "paragraph_index": 21}, {"url": "https://console.aws.amazon.com/support/home?region=us-west-2", "anchor_text": "Support Center", "paragraph_index": 23}, {"url": "https://docs.d2iq.com/mesosphere/dcos/1.13/installing/evaluation/aws/", "anchor_text": "documentation", "paragraph_index": 24}, {"url": "http://docs.d2iq.com", "anchor_text": "d2iq.com", "paragraph_index": 24}, {"url": "https://universe.dcos.io/#/package/data-science-engine/version/latest", "anchor_text": "Data Science Engine package", "paragraph_index": 24}, {"url": "https://github.com/rjurney/paas_blog", "anchor_text": "Github project", "paragraph_index": 25}, {"url": "https://github.com/rjurney/paas_blog/tree/master/dcos/terraform", "anchor_text": "paas_blog/dcos/terraform", "paragraph_index": 25}, {"url": "https://docs.python.org/3/library/hashlib.html", "anchor_text": "hashlib", "paragraph_index": 32}, {"url": "https://ipython.org/", "anchor_text": "ipython", "paragraph_index": 37}, {"url": "https://github.com/rjurney/paas_blog", "anchor_text": "GitHub project", "paragraph_index": 41}, {"url": "https://dcos.io/releases/", "anchor_text": "releases page", "paragraph_index": 43}, {"url": "https://github.com/dcos-terraform/terraform-aws-dcos", "anchor_text": "Github repo", "paragraph_index": 43}, {"url": "https://www.terraform.io/docs/commands/init.html", "anchor_text": "initialize", "paragraph_index": 45}, {"url": "https://www.terraform.io/docs/commands/plan.html", "anchor_text": "plan of action", "paragraph_index": 46}, {"url": "https://github.com/rjurney/deep_products/blob/master/code/stackoverflow/get_questions.spark.py", "anchor_text": "previously computed", "paragraph_index": 66}, {"url": "https://github.com/rjurney/paas_blog/blob/master/DCOS_Data_Science_Engine.ipynb", "anchor_text": "github.com/rjurney/paas_blog/DCOS_Data_Science_Engine.ipynb", "paragraph_index": 67}, {"url": "https://keras.io/preprocessing/text/#tokenizer", "anchor_text": "keras.preprocessing.text.Tokenizer.sequences_to_texts", "paragraph_index": 76}, {"url": "https://keras.io/preprocessing/sequence/#pad_sequences", "anchor_text": "keras.preprocessing.sequence.pad_sequences", "paragraph_index": 76}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html", "anchor_text": "sklearn.model_selection.train_test_split", "paragraph_index": 77}, {"url": "https://arxiv.org/abs/1408.5882", "anchor_text": "Kim-CNN", "paragraph_index": 79}, {"url": "https://github.com/dcos/dcos-cli", "anchor_text": "Github page", "paragraph_index": 92}, {"url": "https://docs.d2iq.com/mesosphere/dcos/1.12/cli/install/", "anchor_text": "CLI install documentation", "paragraph_index": 93}, {"url": "https://curl.haxx.se/", "anchor_text": "curl", "paragraph_index": 93}, {"url": "https://stedolan.github.io/jq/", "anchor_text": "jq", "paragraph_index": 100}, {"url": "https://docs.d2iq.com/mesosphere/dcos/1.13/cli/command-reference/dcos-cluster/dcos-cluster-setup/", "anchor_text": "documentation", "paragraph_index": 101}, {"url": "http://datasyndrome.com", "anchor_text": "Data Syndrome", "paragraph_index": 117}, {"url": "https://www.snorkel.org/blog/weak-supervision", "anchor_text": "weakly supervised learning", "paragraph_index": 117}, {"url": "http://blog.datasyndrome.com", "anchor_text": "http://blog.datasyndrome.com", "paragraph_index": 117}], "all_paragraphs": ["As a full-stack machine learning engineer that focuses on delivering new products to market, I\u2019ve often found myself at the intersection of data science, data engineering and dev ops. So it has been with great interest that I\u2019ve followed the rise of data science Platforms as a Service (PaaS). In this series of posts, I\u2019ll be evaluating different Platforms as a Service (PaaS) and their potential to automate data science operations. I\u2019m going to explore their capabilities and then automate the setup and execution of code from my forthcoming book Weakly Supervised Learning (O\u2019Reilly, 2020) to find the best way for the book\u2019s readers to work through the book.", "In my last book, Agile Data Science 2.0 (4.5 stars :D), I built my own platform for readers to run the code using bash scripts, the AWS CLI, jq, Vagrant and EC2. While this made the book much more valuable for beginners who would otherwise have trouble running the code, it has been extremely difficult to maintain and keep running. Older software falls off the internet and the platform rots. There have been 85 issues on the project, and while many of those have been fixed by reader contributions, it has still taken up much of the time I have to devote to open source software. This time is going to be different.", "Note: code for this post is available at github.com/rjurney/paas_blog", "The first PaaS for data science I\u2019m evaluating is the newly launched DC/OS Data Science Engine. In this post I\u2019ll walk you through my initial experiment with DC/OS (caveat: I\u2019ve used it in the past) and its Data Science Engine using the GUI and then we\u2019ll cover how to automate that same process in a few lines of code. It turns out this is actually simpler than creating the equivalent resources using the AWS CLI, which impressed me. We\u2019ll set up our environment and software prerequisites, initialize a DC/OS cluster using Terraform and the Universal Installer, install the Data Science Engine package and then the evaluate the environment by running a model that tags Stack Overflow posts.", "A few caveats: we\u2019ll go through how to boot a DC/OS cluster with JupyterLab installed step-by-step, but if you run into trouble you can always refer to the Universal Installer documentation for DC/OS Terraform problems or the Data Science Engine documentation for problems deploying that service.", "It has become fairly easy to setup a Jupyter Notebook in any given cloud environment like Amazon Web Services (AWS), Google Cloud Platform (GCP) and Azure for an individual data scientist to work. For startups and small data science teams, this is a good solution. Nothing stays up to be maintained and notebooks can be saved in Github for persistence and sharing.", "For large enterprises, things are not so simple. At this scale, temporary environments on transitory assets across multiple clouds can create chaos rather than order, as environments and modeling become irreproducible. Enterprises work across multiple clouds and on premises, have particular access control and authentication requirements, and need to provide access to internal resources for data, source control, streaming and other services.", "For these organizations the DC/OS Data Science Engine offers a unified system offering the Python ML stack, Spark, Tensorflow and other DL frameworks, including TensorFlowOnSpark to enable distributed multi-node, multi-GPU model training. Its a pretty compelling setup that works out of the box and can end much frustration and complexity for larger data science teams and companies.", "The DC/OS Universal Installer can run from Linux, Mac OS X and Windows. For all platforms you will need Python, pip, the AWS Command Line Interface (CLI) and Terraform. You probably have Python installed if you\u2019re testing out the Data Science Engine, but if not we\u2019ll install Anaconda Python.", "You will also need to authorize 5 GPU instances in the region in which you will run DC/OS Data Science Engine (in this post we use `us-west-2`). Let\u2019s take it step by step.", "If you already have Python installed on your machine, there is no need to install Anaconda Python. If you don\u2019t, I recommend Anaconda Python as it is easy to install, sits in your home directory, and has the excellent conda package manager.", "Download Anaconda Python 3.X for your platform here, then follow the instructions for installation for Linux, Mac OS X and Windows.", "There are two ways of setting up AWS access for the Terraform AWS provider: via the AWS CLI or by editing (path from the Github project root) paas_blog/dcos/terraform/terraform.tf.", "First we\u2019ll setup our AWS credentials for Terraform to use. To , use the PyPI awscli package:", "Now setup your AWS credentials, using your Access Key ID and Secret Access Key, which you can find in the IAM console. You can alter the region to your own default. Terraform\u2019s AWS module will use these credentials by default. They\u2019re stored in the ~/.aws/ directory.", "You should see something like this:", "You can explicitly setup AWS authentication by editing (from the project root) paas_blog/dcos/terraform/terraform.tf to include your credentials where the AWS provider is called. Simply add your access key/secret key and default region.", "Terraform enables users to define and provision a datacenter infrastructure using a high-level configuration language known as Hashicorp Configuration Language (HCL).  \u2014 Wikipedia, Terraform (software)", "The DC/OS Universal Installer requiresds Terraform 11.x. Ubuntu users can install Terraform 11.x like so:", "Now we\u2019re good to go! Ignore any message about upgrading. Now that Terraform is installed, we\u2019re ready to configure and launch our DC/OS cluster on AWS.", "AWS Service Limits define how many AWS resources you can use in any given region. We\u2019ll be booting 5 p3.2xlarge GPU instances as private agents for our DC/OS cluster where notebooks will run. The default service quota for the p3.2xlarge instance type is 0. In order to run this tutorial, you will need to request that AWS increase this to 5 or more.", "You can do so by logging into the AWS Web Console and visiting the Service Quotas console here: https://us-west-2.console.aws.amazon.com/servicequotas/home?region=us-west-2#!/services/ec2/quotas (you can substitute your preferred AWS region in the url, just be sure to substitute it in _both_ places it appears in the url). Search for p3.2xlarge in the search box and click the orange Request quota increase button to the right.", "Enter 5 into the Change quota value field. Then click the orange Request button at bottom right.", "Now you will have to wait 12\u201348 hours for the request to be approved. I have a basic AWS account and when I request an increase in the afternoon and it is approved the next morning. If you need to speed things up, you can go to the AWS Support Center and request a call with an agent. They can usually accelerate things quite a bit.", "There is good documentation for the DC/OS Universal Installer on d2iq.com, but I\u2019ll provide code that \u2018just works\u2019 as part of this post. We\u2019ll be using Terraform to configure and launch our cluster, then we\u2019ll install the Data Science Engine package and get down to work with JupyterLab!", "Note: change directory to the Github project subdirectory paas_blog/dcos/terraform for the remainder of the tutorial.", "The first step in configuring out DC/OS cluster is to create a cluster-specific ssh key. We\u2019ll call this key my_key.pub.", "Hit enter twice to create the key without a password.", "We need to change the permissions of my_key to be readable only to our user, 0600, or later ssh will complain.", "Now run ssh-agent if it isn\u2019t running and add the key to the agent.", "Note: without this step, you will get ssh errors when you create the cluster. See section Common Errors below.", "Now verify the key has been added:", "Now we\u2019ll create a superuser password hash file for the cluster using Python\u2019s hashlib module. We\u2019ll call ours dcos_superuser_password_hash.", "I\u2019ve created a command line script that will generate, print and write a password hash to disk called paas_blog/dcos/terraform/generate_password_hash.py.", "You should see output something like this:", "Verify the write to the file dcos_superuser_password_hash was successful:", "Creating a Superuser Password Hash in Python", "To create the hash manually, open a Python shell (consider using ipython):", "Change the password hash\u2019s permissions to be readable only to your user, 0600, or the DC/OS CLI will complain:", "Verify the password has saved successfully:", "You should see something like this:", "As we\u2019ll be using the open version of DC/OS, I created an empty license.txt file via touch license.txt. It needs to exist, but it can be empty. It is already committed to Github, so for the open version of DC/OS, you don\u2019t need to create it once you check out the GitHub project. If you\u2019re using the Enterprise Edition, you\u2019ll need to put your actual license in license.txt.", "I\u2019ve edited the file paas_blog/dcos/terraform/desired_cluster_profile.tfvars to personalize the superuser account name, load the superuser password hash from the password hash file we created above, to specify an empty license string and point at an empty license.txt file. The DC/OS variant is set to open, the DC/OS version to 1.13.3, we\u2019ll use an m5.xlarge for our bootstrap instance type and we\u2019ll use p3.2xlarge instances to run JupyterLab. We set the number of GPU agents to 5, which is enough to run Spark and JupyterLab. Finally, we specify the public key we generated earlier, my_key.pub. Remember to use the public and not private key.", "Note: you can find the latest version of DC/OS on the releases page, and you can find the latest version of the Universal Installer by clicking on tags on its Github repo. If you run into problems, use the highest 0.2.x tag available by editing the version key in terraform.tf.", "Note: If you\u2019re using the enterprise edition of DC/OS, you\u2019ll need to fill in dcos_license_key_contents, which we\u2019ll leave blank for the open variant. You\u2019ll also want to change the configuration such that dcos_variant = \u201cee\u201d.", "First we need to initialize Terraform with all the modules we\u2019ll be using:", "Now we need to use the variables we defined in paas_blog/dcos/terraform/desired_cluster_profile.tfvars to generate a plan of action for Terraform to carry out. We\u2019ll save the plan to paas_blog/dcos/terraform/plan.out.", "You should see a lot of output with no errors, ending with:", "Now we have a plan for Terraform to create our DC/OS cluster in plan.out, which is binary and can\u2019t be inspected very easily.", "Now that we have a plan that includes our custom variables, we don\u2019t need to include them again in the apply command. We can just follow the directions at the end of the output of the *plan* command. Note that we don\u2019t use --var-file with apply, because plan has inserted our variables into the plan.", "This command can take as long as 15 minutes to execute as there is a delay in initializing a sequence of AWS resources and then executing commands to initialize the services on EC2 instances. You should see a lot of output, beginning with:", "If you see any errors, the best thing is to destroy the cluster and try again. Occasionally timing issues in delayed initialization of AWS resources can cause boot problems.", "When you\u2019re done with the cluster, if there is an error, or if you need to recreate it, you can destroy all associated resources with the following command. Note that we do need to use--var-file with destroy.", "This may take a second, as there are many resources to remove. Once the destroy is complete you are free to plan and apply again.", "The final output of the `terraform apply` command gives us the address of the master node to connect to and should look something like this:", "Open the master ip address in a browser and you should see a login window. Note that the only IP authorized to connect to this machine is your source IP.", "Select your authentication method \u2014 I use Google. Once you authenticate, it will return you to the home page.", "On the left hand menu, fourth down from the top and circled in orange in this image, is the *Catalog* item. Click it and the DC/OS Service Menu will come up. When I did so, the *data-science-engine* service was visible in the second row from the top, but if it isn\u2019t use the search box at top left to find it.", "Click on the data-science-engine service and its service page will come up. Click Review & Run to install the service.", "This will bring up a window where you can edit the configuration of the Data Science Engine. You need to name the cluster using letters and dashes, but it doesn\u2019t matter what you name it, except that names are unique. Since we\u2019re using p3.2xlarge instances, configure the service to use 58GB of RAM and 8 CPUs. Check the Nvidia GPU Allocation Configuration Enabled checkbox and type 1 for the number of GPUs.", "Click the purple Review & Run button at top right. This will take you to the final review screen. Click the purple Run Service button at top right.", "Note that you can download the service configuration as JSON to run later with the DC/OS CLI, enabling you to automate the deployment of the service, for example as part of your continuous integration system. To do so click Download Config.", "You should see a popup announcing the successful launch of the system. Click Open Service.", "This will take you to the Data Science Engine service page. At first the page\u2019s status will say the service is loading, but soon the Health swatch will become green and the Status will say Running.", "Now click on the text Services where it says Services > data-science-engine at the top of the white area of the screen. This will take you to the service listing. You should see data-science-engine listed. Click on the launch icon circled in orange in the image below. This will open a connection with JupyterLab. The default Jupyter password is jupyter, but you can set it using the service configuration window we used to launch the service.", "Once you enter the default Jupyter password of jupyter (you can change it in desired_cluster_profile.tfvars), you will see the homepage of JupyterLab. The first page load can take a little while. Tada!", "Now that we\u2019ve booted the cluster and service, let\u2019s exercise it by training a neural network to tag Stack Overflow questions. We treat this as a multi-class, multi-label problem. The training data has been balanced by upsampling the complete dump of questions that have at least one answer, one vote and have at least one tag occurring more than 2,000 times. It is about 600MB. This dataset was previously computed and the files can be found in the paas_blog/data directory of the Github repo.", "You can view the Jupyter Notebook with the code we\u2019ll be running from Github at github.com/rjurney/paas_blog/DCOS_Data_Science_Engine.ipynb. We\u2019ll be opening it using the JupyterLab Github interface, but if you like you can paste its content block-by-block into a new Python 3 notebook.", "JupyterLab\u2019s Github module is super awesome and makes loading the tutorial notebook easy. Click on the Github icon on the far left of the screen, between the file and running man icons. Enter rjurney where it says <Edit User>.", "My public Github projects will come up. Select paas_blog and then double click on the DCOS_Data_Science_Engine.ipynb Jupyter notebook to open it. It uses data on S3, so you shouldn\u2019t have to download any data.", "The first thing to do is to verify that our JupyterLab Python environment on our Data Science Engine EC2 instance is properly configured to work with its onboard GPU. We use tensorflow.test.is_gpu_available and tensorflow.compat.v2.config.experimental.list_physical_devices to verify the GPUs are working with Tensorflow.", "You can load the data for this tutorial using pandas.read_parquet.", "Now we load the indexes to convert back and forth between label indexes and text tags. We\u2019ll use these to view the actual resulting tags predicted at the end of the tutorial.", "Then we verify the number of records loaded:", "We need to join the previously tokenized text back into a string for use in a Tokenizer, which provides useful properties. In addition, making the number of documents a multiple of batch size is a requirement for Tensorflow/Keras to split work among multiple GPUs and to use certain models such as Elmo.", "The data has already been truncated to 200 words per post but the tokenization using the top 10K words reduces this to below 200 in some documents. If any documents vary from 200 words, the data won\u2019t convert properly into a numpy matrix below.", "In addition to converting the text to numeric sequences with a key, Keras\u2019 Tokenizer class is handy for producing the final results of the model via the keras.preprocessing.text.Tokenizer.sequences_to_textsmethod. Then we use Keras\u2019 keras.preprocessing.sequence.pad_sequences method and check the output to ensure the sequences are all 200 items long or they won\u2019t convert properly into a matrix. The string __PAD__ has been used previously to pad the documents, so we reuse it here.", "We need one dataset to train with and one separate dataset to test and validate our model with. The oft used sklearn.model_selection.train_test_split makes it so.", "Although there has already been filtering and up-sampling of the data to restrict it to a sample of questions with at least one tag that occurs more than 2,000 times, there are still uneven ratios between common and uncommon labels. Without class weights, the most common label will be much more likely to be predicted than the least common. Class weights will make the loss function consider uncommon classes more than frequent ones.", "Now we\u2019re ready to train a model to classify/label questions with tag categories. The model is based on Kim-CNN, a commonly used convolutional neural network for sentence and document classification. We use the functional API and we\u2019ve heavily parametrized the code so as to facilitate experimentation.", "In Kim-CNN, we start by encoding the sequences using an Embedding, followed by a Dropout layer to reduce overfitting. Next we split the graph into multiple Conv1D layers with different widths, each followed by MaxPool1D. These are joined by concatenation and are intended to characterize patterns of different size sequence lengths in the documents. There follows another Conv1D/GlobalMaxPool1D layer to summarize the most important of these patterns. This is followed by flattening into a Dense layer and then on to the final sigmoid output layer. Otherwise we use selu throughout.", "Next we compile our model. We use a variety of metrics, because no one metric summarizes model performance, and we need to drill down into the true and false positives and negatives. We also use the ReduceLROnPlateau, EarlyStopping and ModelCheckpoint callbacks to improve performance once we hit a plateau, then to stop early, and to persist only the very best model in terms of the validation categorical accuracy.", "Categorical accuracy is the best fit for gauging our model\u2019s performance because it gives points for each row separately for each class we\u2019re classifying. This means that if we miss one, but get the others right, this is a great result. With binary accuracy, the entire row is scored as incorrect.", "Then it is time to fit the model. We give it the class weights we computed earlier.", "Because we used ModelCheckpoint(save_only_best-True), the best epoch in terms of CategoricalAccuracy is what was saved. We want to use that instead of the last epoch\u2019s model, which is what is stored in model above. So we load the file before evaluating our model.", "Metrics include names like precision_66 which aren\u2019t consistent between runs. We fix these to cleanup our report on training the model. We also add an f1 score, then make a DataFrame to display the log. This could be extended in repeat experiments.", "We want to know the performance at each epoch so that we don\u2019t train needlessly large numbers of epochs.", "It is not enough to know theoretical performance. We need to see the actual output of the tagger at different confidence thresholds.", "Lets look at the sentences with the actual labels and the predicted labels in a DataFrame:", "We can see from these three records that the model is doing fairly well. This tells a different story than performance metrics alone. It is so strange that most machine learning examples just compute performance and don\u2019t actually employ the `predict()` method! At the end of the day statistical performance is irrelevant and what matters is the real world performance \u2014 which is not contained in simple summary statistics!", "That covers how you use the platform manually, but this is about PaaS automation. So how do we speed things up?", "DC/OS\u2019s graphical user interface and CLI together enable easy access to JupyterLab via the Data Science Engine for all kinds of users: non-technical managers trying to view a report in a notebook and dev ops/data engineers looking to automate a process. If the manual GUI process seems involved, we can automate it in a few lines once we have the service configuration as a JSON file by launching the DC/OS cluster via Terraform commands, getting the cluster address from Terraform,then using the DC/OS CLI to authenticate with the cluster and run the service.", "Note: check out the Github page for the DC/OS CLI for more information on how it works.", "The DC/OS CLI is not required to boot a cluster with Terraform and install the Data Science Engine manually in the UI, but it is required to automate the process. If you run into trouble, check out the CLI install documentation. You will need curl.", "Note that you can also download the CLI using the commands that print when you click on the top right dropdown titled jupyter-gpu-xxxx (or whatever you named your cluster) and then click on the Install CLI button inside the orange box below.", "A popup will appear with installation code for Windows, OS X and Linux. Copy/paste that code into a terminal to complete the install.", "Now, if you run dcos you should see the following:", "To automatically install the Data Science Engine package, we first need to get its configuration as a JSON file. This can be done in two ways: via the GUI or via the DC/OS CLI or via the Terraform CLI. We\u2019ll cover both methods.", "Using the GUI, we can get the JSON for the package configuration by downloading it from the Configuration tab of the data-science-engine service page. Click Download Config in purple at the top right under the line.", "By the way, you can edit the Data Science Engine service by clicking the purple Edit button at top right. You can use the menu or JSON editor to change the service \u2014 increase RAM or GPUs for example \u2014 and it will automatically re-deploy with that configuration.", "To use the CLI, we need to authenticate with the cluster. This can be done via Google, or via a username or other method that suits your organization. To do so, first we need the server address from Terraform. We can extract the cluster master IP address(es) via terraform output -json and the jq json utility. Once we have that, we can use whatever method we prefer, including Google, to authenticate. This could also be a username to facilitate automation.", "Check the documentation for dcos cluster setup for information on different methods of authentication.", "Once we\u2019ve authenticated, we can use the CLI to generate the package configuration file for later reuse.", "For Mac OS X the flag for the command base64 is a capital -D:", "For Linux the flag for base64 is a lowercase -d:", "Note that the exported options cover every single option, which isn\u2019t ideal because options can change across different versions of the platform and it is better to rely on the system defaults if you don\u2019t change a value. For example, I\u2019ve edited this export down to:", "You can edit this file to fit your needs or use the GUI to do so and download and edit the config.", "Now we can install the Data Science Engine package using the CLI:", "All together, that makes the entire process using the exported JSON package configuration:", "In this post we booted a DC/OS cluster and deployed the Data Science Engine in a repeatable manner, then executed a test notebook to create a Stack Overflow tagger. This shows how PaaS can be used to enhance the productivity of a data science team. Stay tuned for the next PaaS in the series!", "Note: every step in the tutorial was rigorously tested, so please let me know in the comments if you run into any problems.", "There are some errors I ran into while writing this post that you may run into while working through the example in this post, so I\u2019ve included what those are and how to fix them! :)", "If you forget to run the ssh-agent via eval \"$(ssh-agent -s)\" and then ssh-add your key, you will see the error below. This is especially easy to do if you open a new shell and run terraform commands from it.", "The solution is ssh-add ./my_key from the paas_blog/dcos/terraform directory. To avoid this in the future, you can edit the key field in desired_cluster_profile.tfvars to use a public key in your ~ /.ssh/ directory that you automatically add to the ssh-agent when the shell starts, using ~/.profile, ~/.bash_profile or ~/.bashrc.", "Now verify the key has been added:", "Occasionally things don\u2019t synchronize properly when DC/OS boots up and you\u2019ll get an error about Ansible bootstrap or the load balancer timing out. The thing to do is to destroy and then plan/apply to recreate. It will work the second time.", "If you see either of the following errors, this is what is going on.", "My name is Russell Jurney. I am Principal Data Scientist at Data Syndrome, where I build end-to-end machine learning and visualization products, recommender systems, lead generation systems, data engineering and specialize in weakly supervised learning, or doing more with less data. I blog at http://blog.datasyndrome.com", "I am working on a new book due out next year called Weakly Supervised Learning. Stay tuned for more!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "I am obsessed with knowledge graphs, networks, motifs, graphlets and GNNs. Founder of Graphlet AI: Knowledge Graph Factory"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F656842dd7ba8&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpaas-to-accelerate-data-science-dc-os-data-science-engine-656842dd7ba8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpaas-to-accelerate-data-science-dc-os-data-science-engine-656842dd7ba8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpaas-to-accelerate-data-science-dc-os-data-science-engine-656842dd7ba8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpaas-to-accelerate-data-science-dc-os-data-science-engine-656842dd7ba8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----656842dd7ba8--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----656842dd7ba8--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@rjurney?source=post_page-----656842dd7ba8--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@rjurney?source=post_page-----656842dd7ba8--------------------------------", "anchor_text": "Russell Jurney"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6b00541c45c8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpaas-to-accelerate-data-science-dc-os-data-science-engine-656842dd7ba8&user=Russell+Jurney&userId=6b00541c45c8&source=post_page-6b00541c45c8----656842dd7ba8---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F656842dd7ba8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpaas-to-accelerate-data-science-dc-os-data-science-engine-656842dd7ba8&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F656842dd7ba8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpaas-to-accelerate-data-science-dc-os-data-science-engine-656842dd7ba8&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.amazon.com/Agile-Data-Science-2-0-Applications/dp/1491960116", "anchor_text": "Agile Data Science 2.0"}, {"url": "https://github.com/rjurney/Agile_Data_Code_2", "anchor_text": "my own platform"}, {"url": "https://stedolan.github.io/jq/", "anchor_text": "jq"}, {"url": "https://github.com/rjurney/Agile_Data_Code_2/issues", "anchor_text": "85 issues"}, {"url": "https://github.com/rjurney/paas_blog", "anchor_text": "github.com/rjurney/paas_blog"}, {"url": "https://docs.d2iq.com/mesosphere/dcos/1.13/installing/evaluation/aws/", "anchor_text": "Universal Installer"}, {"url": "https://universe.dcos.io/#/package/data-science-engine/version/latest", "anchor_text": "Data Science Engine package"}, {"url": "https://docs.d2iq.com/mesosphere/dcos/1.13/installing/evaluation/aws/", "anchor_text": "Universal Installer documentation"}, {"url": "https://docs.d2iq.com/mesosphere/dcos/services/data-science-engine/1.0.0/quick-start/", "anchor_text": "Data Science Engine documentation"}, {"url": "https://github.com/yahoo/TensorFlowOnSpark", "anchor_text": "TensorFlowOnSpark"}, {"url": "https://docs.d2iq.com/mesosphere/dcos/1.13/installing/evaluation/aws/", "anchor_text": "DC/OS Universal Installer"}, {"url": "https://www.python.org/about/gettingstarted/", "anchor_text": "Python"}, {"url": "https://pip.pypa.io/en/stable/", "anchor_text": "pip"}, {"url": "https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-welcome.html", "anchor_text": "AWS Command Line Interface (CLI)"}, {"url": "https://www.terraform.io/intro/index.html", "anchor_text": "Terraform"}, {"url": "https://www.anaconda.com/distribution/", "anchor_text": "Anaconda Python"}, {"url": "https://www.anaconda.com/distribution/#download-section", "anchor_text": "here"}, {"url": "https://docs.anaconda.com/anaconda/install/linux/", "anchor_text": "Linux"}, {"url": "https://docs.anaconda.com/anaconda/install/mac-os/", "anchor_text": "Mac OS X"}, {"url": "https://docs.anaconda.com/anaconda/install/windows/", "anchor_text": "Windows"}, {"url": "https://www.terraform.io/docs/providers/aws/index.html", "anchor_text": "Terraform AWS provider"}, {"url": "https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html#cli-quick-configuration", "anchor_text": "AWS CLI"}, {"url": "https://pypi.org/project/awscli/", "anchor_text": "awscli package"}, {"url": "https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html", "anchor_text": "setup"}, {"url": "https://console.aws.amazon.com/iam/home#/security_credentials", "anchor_text": "IAM console"}, {"url": "https://releases.hashicorp.com/terraform/0.11.14/terraform_0.11.14_linux_386.zip", "anchor_text": "https://releases.hashicorp.com/terraform/0.11.14/terraform_0.11.14_linux_386.zip"}, {"url": "https://brew.sh/", "anchor_text": "Homebrew"}, {"url": "https://chocolatey.org/", "anchor_text": "Chocolatey"}, {"url": "https://docs.aws.amazon.com/general/latest/gr/aws_service_limits.html", "anchor_text": "AWS Service Limits"}, {"url": "https://us-west-2.console.aws.amazon.com/servicequotas/home?region=us-west-2#!/services/ec2/quotas", "anchor_text": "https://us-west-2.console.aws.amazon.com/servicequotas/home?region=us-west-2#!/services/ec2/quotas"}, {"url": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html#concepts-available-regions", "anchor_text": "AWS region"}, {"url": "https://console.aws.amazon.com/support/home?region=us-west-2", "anchor_text": "Support Center"}, {"url": "https://docs.d2iq.com/mesosphere/dcos/1.13/installing/evaluation/aws/", "anchor_text": "documentation"}, {"url": "http://docs.d2iq.com", "anchor_text": "d2iq.com"}, {"url": "https://universe.dcos.io/#/package/data-science-engine/version/latest", "anchor_text": "Data Science Engine package"}, {"url": "https://github.com/rjurney/paas_blog", "anchor_text": "Github project"}, {"url": "https://github.com/rjurney/paas_blog/tree/master/dcos/terraform", "anchor_text": "paas_blog/dcos/terraform"}, {"url": "https://docs.python.org/3/library/hashlib.html", "anchor_text": "hashlib"}, {"url": "https://ipython.org/", "anchor_text": "ipython"}, {"url": "https://github.com/rjurney/paas_blog", "anchor_text": "GitHub project"}, {"url": "https://dcos.io/releases/", "anchor_text": "releases page"}, {"url": "https://github.com/dcos-terraform/terraform-aws-dcos", "anchor_text": "Github repo"}, {"url": "https://www.terraform.io/docs/commands/init.html", "anchor_text": "initialize"}, {"url": "https://www.terraform.io/docs/commands/plan.html", "anchor_text": "plan of action"}, {"url": "https://github.com/rjurney/deep_products/blob/master/code/stackoverflow/get_questions.spark.py", "anchor_text": "previously computed"}, {"url": "https://github.com/rjurney/paas_blog/blob/master/DCOS_Data_Science_Engine.ipynb", "anchor_text": "github.com/rjurney/paas_blog/DCOS_Data_Science_Engine.ipynb"}, {"url": "https://keras.io/preprocessing/text/#tokenizer", "anchor_text": "keras.preprocessing.text.Tokenizer.sequences_to_texts"}, {"url": "https://keras.io/preprocessing/sequence/#pad_sequences", "anchor_text": "keras.preprocessing.sequence.pad_sequences"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html", "anchor_text": "sklearn.model_selection.train_test_split"}, {"url": "https://arxiv.org/abs/1408.5882", "anchor_text": "Kim-CNN"}, {"url": "https://github.com/dcos/dcos-cli", "anchor_text": "Github page"}, {"url": "https://docs.d2iq.com/mesosphere/dcos/1.12/cli/install/", "anchor_text": "CLI install documentation"}, {"url": "https://curl.haxx.se/", "anchor_text": "curl"}, {"url": "https://downloads.dcos.io/binaries/cli/linux/x86-64/dcos-1.13/dcos", "anchor_text": "https://downloads.dcos.io/binaries/cli/linux/x86-64/dcos-1.13/dcos"}, {"url": "https://stedolan.github.io/jq/", "anchor_text": "jq"}, {"url": "https://towardsdatascience.com/$CLUSTER_ADDRESS", "anchor_text": "http://$CLUSTER_ADDRESS"}, {"url": "https://docs.d2iq.com/mesosphere/dcos/1.13/cli/command-reference/dcos-cluster/dcos-cluster-setup/", "anchor_text": "documentation"}, {"url": "https://towardsdatascience.com/$CLUSTER_ADDRESS", "anchor_text": "http://$CLUSTER_ADDRESS"}, {"url": "http://datasyndrome.com", "anchor_text": "Data Syndrome"}, {"url": "https://www.snorkel.org/blog/weak-supervision", "anchor_text": "weakly supervised learning"}, {"url": "http://blog.datasyndrome.com", "anchor_text": "http://blog.datasyndrome.com"}, {"url": "https://medium.com/tag/dcos?source=post_page-----656842dd7ba8---------------dcos-----------------", "anchor_text": "DCOS"}, {"url": "https://medium.com/tag/amazon-web-services?source=post_page-----656842dd7ba8---------------amazon_web_services-----------------", "anchor_text": "Amazon Web Services"}, {"url": "https://medium.com/tag/jupyter-notebook?source=post_page-----656842dd7ba8---------------jupyter_notebook-----------------", "anchor_text": "Jupyter Notebook"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----656842dd7ba8---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/paas?source=post_page-----656842dd7ba8---------------paas-----------------", "anchor_text": "Paas"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F656842dd7ba8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpaas-to-accelerate-data-science-dc-os-data-science-engine-656842dd7ba8&user=Russell+Jurney&userId=6b00541c45c8&source=-----656842dd7ba8---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F656842dd7ba8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpaas-to-accelerate-data-science-dc-os-data-science-engine-656842dd7ba8&user=Russell+Jurney&userId=6b00541c45c8&source=-----656842dd7ba8---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F656842dd7ba8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpaas-to-accelerate-data-science-dc-os-data-science-engine-656842dd7ba8&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----656842dd7ba8--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F656842dd7ba8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpaas-to-accelerate-data-science-dc-os-data-science-engine-656842dd7ba8&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----656842dd7ba8---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----656842dd7ba8--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----656842dd7ba8--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----656842dd7ba8--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----656842dd7ba8--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----656842dd7ba8--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----656842dd7ba8--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----656842dd7ba8--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----656842dd7ba8--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@rjurney?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@rjurney?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Russell Jurney"}, {"url": "https://medium.com/@rjurney/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.8K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6b00541c45c8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpaas-to-accelerate-data-science-dc-os-data-science-engine-656842dd7ba8&user=Russell+Jurney&userId=6b00541c45c8&source=post_page-6b00541c45c8--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fe91dbe50a5e3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpaas-to-accelerate-data-science-dc-os-data-science-engine-656842dd7ba8&newsletterV3=6b00541c45c8&newsletterV3Id=e91dbe50a5e3&user=Russell+Jurney&userId=6b00541c45c8&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}