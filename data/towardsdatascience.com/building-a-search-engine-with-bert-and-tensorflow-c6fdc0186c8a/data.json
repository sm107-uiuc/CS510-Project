{"url": "https://towardsdatascience.com/building-a-search-engine-with-bert-and-tensorflow-c6fdc0186c8a", "time": 1682996888.126839, "path": "towardsdatascience.com/building-a-search-engine-with-bert-and-tensorflow-c6fdc0186c8a/", "webpage": {"metadata": {"title": "Building a Search Engine with BERT and TensorFlow | by Denis Antyukhov | Towards Data Science", "h1": "Building a Search Engine with BERT and TensorFlow", "description": "Feature extractors based on deep Neural Probabilistic Language Models such as BERT may extract features that are relevant for a wide array of downstream NLP tasks. For that reason, they are sometimes\u2026"}, "outgoing_paragraph_urls": [{"url": "http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf", "anchor_text": "Neural Probabilistic Language Models", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270", "anchor_text": "BERT", "paragraph_index": 0}, {"url": "https://en.wikipedia.org/wiki/Natural-language_understanding", "anchor_text": "Natural Language Understanding", "paragraph_index": 0}, {"url": "https://en.wikipedia.org/wiki/Instance-based_learning", "anchor_text": "instance-based learning", "paragraph_index": 1}, {"url": "https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm", "anchor_text": "K-NN", "paragraph_index": 1}, {"url": "https://colab.research.google.com/drive/1ra7zPFnB2nWtoAc0U5bLp0rWuPWb6vu4", "anchor_text": "here", "paragraph_index": 6}, {"url": "https://github.com/gaphex/bert_experimental", "anchor_text": "repository", "paragraph_index": 6}, {"url": "https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip", "anchor_text": "English model", "paragraph_index": 7}, {"url": "https://medium.com/p/pre-training-bert-from-scratch-with-cloud-tpu-6e2f71028379?source=email-e6b20e934e90--writer.postDistributed&sk=51c46354668b0fc4255fe8bb7e1e3035", "anchor_text": "other guide", "paragraph_index": 7}, {"url": "https://github.com/hanxiao/bert-as-service", "anchor_text": "bert-as-a-service", "paragraph_index": 8}, {"url": "https://hanxiao.github.io/2019/01/02/Serving-Google-BERT-in-Production-using-Tensorflow-and-ZeroMQ/", "anchor_text": "blog post", "paragraph_index": 14}, {"url": "https://www.tensorflow.org/guide/extend/model_files", "anchor_text": "GraphDef", "paragraph_index": 16}, {"url": "https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator", "anchor_text": "tf.Estimator", "paragraph_index": 17}, {"url": "https://github.com/gaphex/bert_experimental", "anchor_text": "repository", "paragraph_index": 24}, {"url": "https://distill.pub/2016/misread-tsne/", "anchor_text": "T-SNE", "paragraph_index": 26}, {"url": "https://projector.tensorflow.org/?config=https://gist.githubusercontent.com/gaphex/7262af1e151957b1e7c638f4922dfe57/raw/3b946229fc58cbefbca2a642502cf51d4f8e81c5/reuters_proj_config.json", "anchor_text": "Embedding Projector", "paragraph_index": 28}, {"url": "https://en.wikipedia.org/wiki/Nearest_neighbor_search", "anchor_text": "nearest neighbour search", "paragraph_index": 31}, {"url": "https://en.wikipedia.org/wiki/Euclidean_distance", "anchor_text": "Euclidean distance", "paragraph_index": 32}, {"url": "https://en.wikipedia.org/wiki/Norm_(mathematics)#Euclidean_norm", "anchor_text": "L2 norms", "paragraph_index": 42}, {"url": "http://www.cs.cmu.edu/~ark/personas/", "anchor_text": "IMDB dataset", "paragraph_index": 48}], "all_paragraphs": ["Feature extractors based on deep Neural Probabilistic Language Models such as BERT may extract features that are relevant for a wide array of downstream NLP tasks. For that reason, they are sometimes referred to as Natural Language Understanding (NLU) modules.", "These features may also be utilized for computing the similarity between text samples, which is useful for instance-based learning algorithms (e.g. K-NN). To illustrate this, we will build a nearest neighbour search engine for text, using BERT for feature extraction.", "The plan for this experiment is:", "This guide contains implementations of two things: a BERT text feature extractor and a nearest neighbour search engine.", "This guide should be useful for researchers interested in using BERT for natural language understanding tasks. It may also serve as a worked example of interfacing with tf.Estimator API.", "For a reader familiar with TensorFlow it should take around 30 minutes to finish this guide.", "The code for this experiment is available in Colab here. Also, check out the repository I set up for my BERT experiments: it contains bonus stuff!", "We start with a pre-trained BERT checkpoint. For demonstration purposes, I will be using the uncased English model pre-trained by Google engineers. To train a model for a different language, check out my other guide.", "For configuring and optimizing the graph for inference we will make use of the awesome bert-as-a-service repository. This repository allows for serving BERT models for remote clients over TCP.", "Having a remote BERT-server is beneficial in multi-host environments. However, in this part of the experiment we will focus on creating a local (in-process) feature extractor. This is useful if one wishes to avoid additional latency and potential failure modes introduced by a client-server architecture.", "Now, let us download the model and install the package.", "Normally, to modify the model graph we would have to do some low-level TensorFlow programming. However, thanks to bert-as-a-service, we can configure the inference graph using a simple CLI interface.", "There are a couple of parameters there to look out for.", "For each text sample, BERT encoding layers output a tensor of shape [sequence_len, encoder_dim] with one vector per token. We need to apply some sort of pooling if we are to obtain a fixed representation.", "POOL_STRAT parameter defines the pooling strategy applied to the encoder layer number POOL_LAYER. The default value \u2018REDUCE_MEAN\u2019 averages the vectors for all tokens in a sequence. This strategy works best for most sentence-level tasks when the model is not fine-tuned. Another option is NONE, in which case no pooling is applied at all. This is useful for word-level tasks such as Named Entity Recognition or POS tagging. For a detailed discussion of these options check out the Han Xiao\u2019s blog post.", "SEQ_LEN affects the maximum length of sequences processed by the model. Smaller values will increase the model inference speed almost linearly.", "Running the above command will put the model graph and weights into a GraphDef object which will be serialized to a pbtxt file at GRAPH_OUT. The file will often be smaller than the pre-trained model because the nodes and variables required for training will be removed. This results in a very portable solution: for example, the english model only takes 380 MB after serializing.", "Now, we will use the serialized graph to build a feature extractor using the tf.Estimator API. We will need to define two things: input_fn and model_fn", "input_fn manages getting the data into the model. That includes executing the whole text preprocessing pipeline and preparing a feed_dict for BERT.", "First, each text sample is converted into a tf.Example instance containing the necessary features listed in INPUT_NAMES. The bert_tokenizer object contains the WordPiece vocabulary and performs the text preprocessing. After that, the examples are re-grouped by feature name in a feed_dict.", "tf.Estimators have a fun feature which makes them rebuild and reinitialize the whole computational graph at each call to the predict function. So, in order to avoid the overhead, we will pass a generator to the predict function, and the generator will yield the features to the model in a never-ending loop. Ha-ha.", "model_fn contains the specification of the model. In our case, it is loaded from the pbtxt file we saved in the previous step. The features are mapped explicitly to the corresponding input nodes via input_map.", "Now we have almost everything we need to perform inference. Let\u2019s do this!", "Due to our use of generators, consecutive calls to bert_vectorizer will not trigger the model rebuild.", "A standalone version of the feature extractor described above can be found in the repository.", "Now it\u2019s time for a demonstration!", "Using the vectorizer, we will generate embeddings for articles from the Reuters-21578 benchmark corpus.To visualize and explore the embedding vector space in 3D, we will use a dimensionality reduction technique called T-SNE.", "Let\u2019s get the article embeddings first.", "The interactive visualization of generated embeddings is available on the Embedding Projector.", "From the link you can run T-SNE yourself or load a checkpoint using the bookmark in lower-right corner (loading works only on Chrome).", "Building a supervised model with the generated features is straightforward:", "Now, let\u2019s say we have a knowledge base of 50k text samples and we need to answer queries based on this data, fast. How do we retrieve the sample most similar to a query from a text database? The answer is nearest neighbour search.", "Formally, the search problem we will be solving is defined as follows:given a set of points S in vector space M, and a query point Q \u2208 M, find the closest point in S to Q. There are multiple ways to define \u2018closest\u2019 in vector space, we will use Euclidean distance.", "So, to build an Search Engine for text we will follow these steps:", "To make the simple matter of implementing this a bit more exciting, we will do it in pure TensorFlow.", "First, we create the placeholders for Q and S", "Finally, get the most similar sample indices", "Now that we have a basic retrieval algorithm set up, the question is: can we make it run faster? With a tiny bit of math, we can.", "For a pair of vectors p and q, the euclidean distance is defined as follows:", "Which is exactly how we did compute it in Step 4.", "However, since p and q are vectors, we can expand and rewrite this:", "In TensorFlow this can be written as follows:", "By the way, in the formula above PP and QQ are actually squared L2 norms of the respective vectors. If both vectors are L2 normalized, then PP = QQ = 1. That gives an interesting relation between inner product and euclidean distance:", "However, doing L2 normalization discards the information about vector magnitude which in many cases is undesirable .", "Instead, we may notice that as long as the knowledge base does not change, PP, its squared vector norm, also remains constant. So, instead of recomputing it every time we can just do it once and then use the precomputed result further accelerating the distance computation.", "Now let us put it all together.", "Naturally, you could use this implementation with any vectorizer model, not just BERT. It is quite effective at nearest neighbour retrieval, being able to process dozens of requests per second on a 2-core Colab CPU.", "For this example we will use a dataset of movie summaries from IMDB. Using the NLU and Retriever modules, we will build a movie recommendation system that suggests movies with similar plot features.", "First, let\u2019s download and prepare the IMDB dataset.", "Vectorize movie plots with the BERT NLU module:", "Finally, using the L2Retriever, find movies with plot vectors most similar to the query movie, and return it to user.", "In this guide we have built a general-purpose BERT feature extractor. Models built with the features extracted from BERT perform adequately on classification and retrieval tasks. While their performance can be further improved by fine-tuning, the described approach to text feature extraction provides a solid unsupervised baseline for downstream NLP solutions.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fc6fdc0186c8a&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-search-engine-with-bert-and-tensorflow-c6fdc0186c8a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-search-engine-with-bert-and-tensorflow-c6fdc0186c8a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-search-engine-with-bert-and-tensorflow-c6fdc0186c8a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-search-engine-with-bert-and-tensorflow-c6fdc0186c8a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----c6fdc0186c8a--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c6fdc0186c8a--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@gaphex?source=post_page-----c6fdc0186c8a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@gaphex?source=post_page-----c6fdc0186c8a--------------------------------", "anchor_text": "Denis Antyukhov"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe6b20e934e90&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-search-engine-with-bert-and-tensorflow-c6fdc0186c8a&user=Denis+Antyukhov&userId=e6b20e934e90&source=post_page-e6b20e934e90----c6fdc0186c8a---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc6fdc0186c8a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-search-engine-with-bert-and-tensorflow-c6fdc0186c8a&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc6fdc0186c8a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-search-engine-with-bert-and-tensorflow-c6fdc0186c8a&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf", "anchor_text": "Neural Probabilistic Language Models"}, {"url": "https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270", "anchor_text": "BERT"}, {"url": "https://en.wikipedia.org/wiki/Natural-language_understanding", "anchor_text": "Natural Language Understanding"}, {"url": "https://en.wikipedia.org/wiki/Instance-based_learning", "anchor_text": "instance-based learning"}, {"url": "https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm", "anchor_text": "K-NN"}, {"url": "https://colab.research.google.com/drive/1ra7zPFnB2nWtoAc0U5bLp0rWuPWb6vu4", "anchor_text": "here"}, {"url": "https://github.com/gaphex/bert_experimental", "anchor_text": "repository"}, {"url": "https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip", "anchor_text": "English model"}, {"url": "https://medium.com/p/pre-training-bert-from-scratch-with-cloud-tpu-6e2f71028379?source=email-e6b20e934e90--writer.postDistributed&sk=51c46354668b0fc4255fe8bb7e1e3035", "anchor_text": "other guide"}, {"url": "https://github.com/hanxiao/bert-as-service", "anchor_text": "bert-as-a-service"}, {"url": "https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip", "anchor_text": "https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip"}, {"url": "https://hanxiao.github.io/2019/01/02/Serving-Google-BERT-in-Production-using-Tensorflow-and-ZeroMQ/", "anchor_text": "blog post"}, {"url": "https://www.tensorflow.org/guide/extend/model_files", "anchor_text": "GraphDef"}, {"url": "https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator", "anchor_text": "tf.Estimator"}, {"url": "https://github.com/gaphex/bert_experimental", "anchor_text": "repository"}, {"url": "https://distill.pub/2016/misread-tsne/", "anchor_text": "T-SNE"}, {"url": "https://projector.tensorflow.org/?config=https://gist.githubusercontent.com/gaphex/7262af1e151957b1e7c638f4922dfe57/raw/3b946229fc58cbefbca2a642502cf51d4f8e81c5/reuters_proj_config.json", "anchor_text": "Embedding Projector"}, {"url": "https://en.wikipedia.org/wiki/Nearest_neighbor_search", "anchor_text": "nearest neighbour search"}, {"url": "https://en.wikipedia.org/wiki/Euclidean_distance", "anchor_text": "Euclidean distance"}, {"url": "https://en.wikipedia.org/wiki/Norm_(mathematics)#Euclidean_norm", "anchor_text": "L2 norms"}, {"url": "http://www.cs.cmu.edu/~ark/personas/", "anchor_text": "IMDB dataset"}, {"url": "https://towardsdatascience.com/pre-training-bert-from-scratch-with-cloud-tpu-6e2f71028379", "anchor_text": "Pre-training BERT from scratch with cloud TPU"}, {"url": "https://towardsdatascience.com/building-a-search-engine-with-bert-and-tensorflow-c6fdc0186c8a", "anchor_text": "Building a Search Engine with BERT and Tensorflow"}, {"url": "https://towardsdatascience.com/fine-tuning-bert-with-keras-and-tf-module-ed24ea91cff2", "anchor_text": "Fine-tuning BERT with Keras and tf.Module"}, {"url": "https://towardsdatascience.com/improving-sentence-embeddings-with-bert-and-representation-learning-dfba6b444f6b", "anchor_text": "Improving sentence embeddings with BERT and Representation Learning"}, {"url": "https://medium.com/tag/naturallanguageprocessing?source=post_page-----c6fdc0186c8a---------------naturallanguageprocessing-----------------", "anchor_text": "Naturallanguageprocessing"}, {"url": "https://medium.com/tag/information-retrieval?source=post_page-----c6fdc0186c8a---------------information_retrieval-----------------", "anchor_text": "Information Retrieval"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----c6fdc0186c8a---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----c6fdc0186c8a---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc6fdc0186c8a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-search-engine-with-bert-and-tensorflow-c6fdc0186c8a&user=Denis+Antyukhov&userId=e6b20e934e90&source=-----c6fdc0186c8a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc6fdc0186c8a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-search-engine-with-bert-and-tensorflow-c6fdc0186c8a&user=Denis+Antyukhov&userId=e6b20e934e90&source=-----c6fdc0186c8a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc6fdc0186c8a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-search-engine-with-bert-and-tensorflow-c6fdc0186c8a&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c6fdc0186c8a--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fc6fdc0186c8a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-search-engine-with-bert-and-tensorflow-c6fdc0186c8a&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----c6fdc0186c8a---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----c6fdc0186c8a--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----c6fdc0186c8a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----c6fdc0186c8a--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----c6fdc0186c8a--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----c6fdc0186c8a--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----c6fdc0186c8a--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----c6fdc0186c8a--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----c6fdc0186c8a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@gaphex?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@gaphex?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Denis Antyukhov"}, {"url": "https://medium.com/@gaphex/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "345 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe6b20e934e90&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-search-engine-with-bert-and-tensorflow-c6fdc0186c8a&user=Denis+Antyukhov&userId=e6b20e934e90&source=post_page-e6b20e934e90--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fd30ab0a9510a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-search-engine-with-bert-and-tensorflow-c6fdc0186c8a&newsletterV3=e6b20e934e90&newsletterV3Id=d30ab0a9510a&user=Denis+Antyukhov&userId=e6b20e934e90&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}