{"url": "https://towardsdatascience.com/from-cups-to-consciousness-part-3-mapping-your-home-with-slam-8a9129c2ed58", "time": 1683000886.251611, "path": "towardsdatascience.com/from-cups-to-consciousness-part-3-mapping-your-home-with-slam-8a9129c2ed58/", "webpage": {"metadata": {"title": "From Cups to Consciousness (Part 3): Mapping your home with SLAM | by MTank | Towards Data Science", "h1": "From Cups to Consciousness (Part 3): Mapping your home with SLAM", "description": "An introduction to SLAM, an in-depth look at the front-end of SLAM and the sensor suite. Feature vs Direct based SLAM, IMUs, RGBD, VIO, Visual Odometry and more"}, "outgoing_paragraph_urls": [{"url": "http://www.themtank.com/", "anchor_text": "MTank", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/from-cups-to-consciousness-part-1-how-are-cups-related-to-intelligence-8b7c701fa197", "anchor_text": "part 1", "paragraph_index": 6}, {"url": "https://towardsdatascience.com/from-cups-to-consciousness-part-2-from-simulation-to-the-real-world-a9ea1249e233", "anchor_text": "part 2", "paragraph_index": 6}, {"url": "https://towardsdatascience.com/from-cups-to-consciousness-part-2-from-simulation-to-the-real-world-a9ea1249e233", "anchor_text": "last blog", "paragraph_index": 11}, {"url": "https://www.themtank.org/a-year-in-computer-vision", "anchor_text": "Computer Vision report", "paragraph_index": 20}, {"url": "https://twitter.com/Rainmaker1973/status/1185156216315695105", "anchor_text": "doll house", "paragraph_index": 32}, {"url": "https://github.com/IntelRealSense/librealsense/blob/master/doc/depth-from-stereo.md", "anchor_text": "here", "paragraph_index": 37}, {"url": "https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/ismar2011.pdf", "anchor_text": "KinectFusion", "paragraph_index": 38}, {"url": "https://github.com/mp3guy/Kintinuous", "anchor_text": "Kintinuous", "paragraph_index": 38}, {"url": "http://rpg.ifi.uzh.ch/docs/ICRA18_Delmerico.pdf", "anchor_text": "A Benchmark Comparison of Monocular Visual-Inertial Odometry Algorithms for Flying Robots", "paragraph_index": 42}, {"url": "http://www.computervisionblog.com/2016/01/why-slam-matters-future-of-real-time.html", "anchor_text": "The Future of Real-Time SLAM and Deep Learning vs SLAM", "paragraph_index": 48}, {"url": "https://en.wikipedia.org/wiki/Reprojection_error", "anchor_text": "geometric loss", "paragraph_index": 49}, {"url": "https://arxiv.org/abs/1607.02565", "anchor_text": "Direct Sparse Odometry", "paragraph_index": 50}, {"url": "http://rpg.ifi.uzh.ch/docs/TRO17_Forster-SVO.pdf", "anchor_text": "Semi-dense Visual Odometry", "paragraph_index": 51}, {"url": "http://www.themtank.org", "anchor_text": "www.themtank.org", "paragraph_index": 57}], "all_paragraphs": ["At MTank, we work towards two goals:(1) Model and distil knowledge within AI. (2) Make progress towards creating truly intelligent machines. As part of these efforts the MTank team release pieces about our work for people to enjoy and learn from, completely for free. If you like our work, then please show your support. Thanks in advance!", "\u201cAll you need is a plan, the road map, and the courage to press on to your destination\u201d \u2014 Earl Nightingale", "In the previous part of this series we talked about how the road to AGI could be divided into perception and control. Within control, navigation and grasping are a crucial part of the roadmap for building a general robot for household tasks. But as the Cheshire Cat said to Alice in Wonderland,", "\u201cif you don\u2019t know where you want to go, does it really matter which road you take?\u201d", "This is the first of two parts where we talk about how our seemingly kidnapped robots find their bearings. What do we mean? Well, from their point of view, every house is a mysterious place they\u2019ve been thrown into or suddenly awoken in. As one can guess, this process of discovery is almost exclusively in the domain of perception and is where, logically, we should start. One must first perceive before acting accordingly, including understanding your surroundings, mapping your environment, i.e. understanding what is where, where is \u201coccupied\u201d and where one can move to unimpeded.", "Consider this step one in building a robot that can perform general tasks. Forget about having robots do singular tasks, e.g. a roomba to vacuum your floor, alexa to tell you the time, and a toast-robot. To perform general tasks, we need our robot to be aware of its environment, i.e. to recognise what is an obstacle, as well as where things are, and use that information to navigate and complete tasks.", "In part 1, we introduced our philosophical motivations and desires for cup-picking and covered a set of simulated 3D house environments. In part 2, we covered our wrapper for the ai2thor house environment, some reinforcement learning (RL) experiments and two specific physics simulations called PyBullet and Gazebo, enabling us to put the robot Vector onto the moon.", "Knowing that mapping was our next milestone, naturally we began by implementing initial mapping within the PyBullet simulation. Since PyBullet contains a depth and colour RGB image, as well as the pose of the camera (the position and orientation), we can use this depth information from the virtual camera to convert each 2D pixel into its 3D coordinates in space, creating what is known as a point cloud.", "Using that point cloud we can create a grid data structure that defines where the robot can move and where the obstacles surrounding our robot are, i.e. an occupancy grid. This depth image, to point cloud, to occupancy grid pipeline is a very common operation within computer vision and robotics.", "Extending an official PyBullet code example to create a point cloud from depth information, we were able to take all points above the floor and below the roof in our simulated kitchen. From this, we create an occupancy grid with each grid cell showing the likely traversability of an area of 10cm2. This way, each area in the grid is assigned a probability of being occupied. This is, in fact, the most common format and approach for robot navigation.", "This taught us a lot about occupancy grids, sensor models and aggregation of multiple viewpoints into one big occupancy grid. But everything mentioned above was found using the exact pose (position and orientation) of the camera provided by PyBullet. We knew that next it was necessary to find a way to localise the agent\u2019s pose in the world, and specifically, in the real world.", "As we mentioned in our last blog, we decided to move from simulation towards getting algorithms working on real robots first. One simple reason for this; there are many more problems that occur on real cameras compared to virtual scenes.", "In the future, to scale the number of tasks, cups, kettles and environments that our robots can tackle we will most certainly need simulation. But for right now, real-world cup-picking trumps matrix-style every time. As you can see, the road to AGI is long, winding and treacherous. It all begins with first finding yourself.", "\u201cNot until we are lost do we begin to find ourselves\u201d \u2014 Henry David Thoreau", "It is relatively easy to create a map using known localisation (the mapping problem), and it\u2019s also relatively easy to localise within a known map (the localisation problem). Many solutions and variations for both exist, but what if we had neither the robot\u2019s pose, nor a map? That is, how do we create a map of the environment when we don\u2019t know where we are?", "This, relatively speaking, isn\u2019t so easy.", "Return to our kidnapped robot, who has just been placed in an unknown room. Upon opening its eyes how could it understand where it was? Most people will naturally use reference landmarks around them, i.e. a bed, the corner of a TV, an open door, to roughly locate themselves in an environment. Mapping and localisation are inextricably linked, they need each other like toast needs butter and it\u2019s essentially a chicken and egg problem.", "In order to pick up cups at breakneck speeds and to operate in homes the robot has never seen before, it should be able to both localise itself within the environment and at the same time map it, i.e. find the 3D positions of parts of the world. This is why in our journey we inevitably arrived at the field of Simultaneous Localisation and Mapping (SLAM).", "SLAM research can be categorised by the sensor suite used to approach the problem (more on this later). For example, you could use a camera (or a set of cameras) to find the pose of the robot by landmarks detected in the environment. But you may not trust the weaknesses of these cameras when the lights are off, or the visual scene might be confusing, in which case you could use a radar instead. Or, as with most companies building autonomous cars, you could use an expensive LiDAR if you need a more reliable sensor that can find objects hundreds of meters away from you in high detail.", "There is no single sensor to solve all of our problems. We will need to combine many sensors together (i.e. sensor fusion) in our quest to build robots that can understand the world in a way that we can relate to while at the same time, keeping the costs low. To do this we decided to focus our efforts on visual algorithms. That is, camera based SLAM, or as named in the literature, Visual SLAM (V-SLAM).", "Over 2 years ago, we mentioned SLAM in our Computer Vision report and now we\u2019re delighted to have the chance to really dive deep into this fascinating technology. If you are looking for a good review, or details on the state of the art, we recommend:", "A typical Visual SLAM algorithm has two main components that can be easily parallelised, meaning that they can be run independently even though both parts are interconnected. Following the literature, we will refer to these as the \u201cfront-end\u201d and \u201cback-end\u201d.", "The front-end abstracts sensor data into models that are amenable for estimation. It\u2019s in charge of preprocessing the input, as well as the detection and tracking of relevant landmarks to allow for an estimation of the sequence of poses, from which we observed them.", "In the case of VSLAM the algorithm that accomplishes this is visual odometry (VO), which essentially calculates the relative pose (the transformation) between two frames according to salient visual features.", "The most common method is to extract keypoints (e.g. using SIFT, ORB) from a frame and then match these same keypoints in the next frame or track them with optical flow. Moreover, while matching/tracking those observed keypoints from different positions, the nature of these algorithms may incur errors of wrong data association, so usually another algorithm is applied afterwards to remove possible outliers that potentially propagate additional error to our estimations, e.g. random sample consensus (RANSAC).", "Another important consideration is that if we keep tracking every salient point of every image we would consume an immense amount of memory including lots of redundant information. For efficiency reasons, typically only a subset of all images observed, these \u201ckey-frames\u201d are selected by a selection algorithm while matching and tracking the observed features only between keyframes. A simple keyframe selection algorithm would be to take every 5th or 10th frame, but other methods involve only adding a keyframe if the image has changed enough from the previous one (e.g. measuring parallax changed or the distance the keypoints moved).", "Additionally, we could calculate the transformation between two consecutive point clouds (e.g. point cloud registration with the Iterative Closest Point (ICP) algorithm) to localise the agent\u2019s trajectory.", "Also, we can use a lower dimensional representation of features found in the image instead of their full description, for example by using \u201cBag of visual words\u201d methods (DBoW), which creates a dictionary of possible features and transforms an image to a vector formed by a combination of the possible features (or \u201cwords\u201d in the dictionary) encountered for a more compressed representation. This can then be used for place recognition/relocalisation and loop closing.", "The back-end is usually the component that uses all the extracted information from the front-end in order to build, extend and further correct the robot\u2019s trajectory and map. It includes several algorithms like bundle adjustment \u2014 where the goal is to correct errors by enforcing reprojective consistency over more than a pair of frames. It also extends to the generation and optimisation of a graph with the different poses estimated, as well as the comparison of the bags of visual words stored by the front-end to accomplish relocalisation and loop closure.", "Loop closure consists of applying corrections to the graph when the robot recognises a previously seen place. Using this information, we can alleviate possible cumulative \u201cdrift\u201d errors encountered within the whole SLAM process.", "To give you a clearer picture of how these different parts interact with each other, here is a high-level architecture of one of our favourite SLAM systems.", "It goes without saying, that different sensor configurations can make the task of SLAM much easier or much harder. Often, the more informative your sensors are the better but with additional sensors you also need to merge and fuse their information in a clever and principled way. This can become quite expensive computationally-speaking. Ideally by using multiple sensors, their pros and cons cancel each other out and together the system is incredibly robust, but often there are trade-offs involved with this choice.", "Monocular SLAM involves the use of a single camera as an input to the corresponding algorithms for SLAM. This suffers from scale issues, i.e. from a monocular SLAM system\u2019s perspective we couldn\u2019t tell the difference in size between a regular home and a \u2018doll house\u2019 if the camera was scaled accordingly. This scale issue can be solved through various means e.g. a good initialisation procedure or using a known length of an object or real-world distance. Although dealing with these imperfections can be algorithmically more tedious. Having a single sensor is a very simple and elegant solution in terms of robot hardware and architecture. Some famous examples are MonoSLAM, ORB-SLAM and LSD-SLAM.", "From a set of \u201cn\u201d observed points (in this case 4 on the house) in an initial position (from the red image), we move across the scene and capture a second image (highlighted in green). Using matching algorithms we can find the points observed on the first image in the second, and use that information to figure out the motion of the camera (odometry) and the structure of the scene (3D coordinates of the observed points). Structure from Motion (SfM) is another famous field with a lot of similarities to Monocular SLAM. By seeing how much a keypoint moved between two frames we can calculate the extrinsic transformation of the camera in the 2nd frame.", "We found that one of our RGB cameras had a very small Field of View (FoV) of horizontal 69.4 degrees that could potentially lead to losing tracking if moving too fast. A larger FoV theoretically allows to keep track of the same keypoints even after longer displacements. In the future we will also experiment with much wider FoV cameras like the fisheye camera which observes a bigger area at a given time and therefore could potentially make our lives easier in keeping the track of the observed scene at higher speeds than a narrower FoV camera would allow.", "Stereo vision involves the use of two cameras to find the structure of the scene. Having two images from different known positions taken at the same time provides significant advantages. For example, within monocular SLAM the matching happens between two images at different times, which means that between those times any object in the scene could have moved, which would completely ruin the visual odometry calculation, whereas in stereo vision, the matching is done between images taken at the same time, i.e. no movement is needed. However, stereo SLAM and in fact most SLAMs will still have this \u201cdynamic objects problem\u201d since they still have to track the same features taken across multiple frames. Popular stereo SLAMs are VINS-Fusion or SOFT-SLAM.", "Moreover, typically matching algorithms require a computationally expensive search for the matches, but since in this case the relative positions of the cameras is known, we can project both images to an imaginary plane in order to make the search easier and prevent some errors on the matching. This process is called stereographic rectification, and it is illustrated in the image below.", "Passive vs active. Stereo vision doesn\u2019t only have to use passive RGB cameras to calculate the disparity map, it\u2019s also possible to do active stereo. Active stereo projects light-beams onto the world, and by using the deformation patterns, it can help increase the accuracy of the depth. More info on rectification and active vs passive stereo here.", "RGB-D SLAM typically refers to when the input to the SLAM system is both color and depth images. The depth can be achieved by the use of a commercial depth camera which commonly contains a stereo camera but the details of how this is retrieved and optimised is left to internals of the depth camera. RGB-D SLAM is capable of calculating dense maps, i.e. which include all the visible pixels, due to having the depth information of these pixels. Compared to the sparse features mentioned above (i.e. a small number of keypoints), this represents an abundance of information which can be used and displayed in a dense map. In the next blog we\u2019ll cover RTAB-Map (architecture shown earlier) which is an excellent RGB-D SLAM system but other famous examples include KinectFusion and Kintinuous.", "Inertial Measurement Units (IMU) usually contain both an accelerometer and a gyroscope (and sometimes a magnetometer). In the IMU contained in the D435i, the first measures acceleration, the second angular velocity and both measure 3 degrees of freedom (DoF) to form 6 DoF altogether in this 6 DoF IMU. Many other variants are possible, e.g. an included magnetometer would make it 9 DoF, and simpler 3 DoF IMUs are often used in autonomous vehicles.", "However, to extract position from these involves double integration (since the integral of acceleration is velocity and the integral of velocity is position) and even state of the art solutions involve a lot of localisation deviation which will accumulate dramatically over time. This is called drift and why an IMU localisation system usually needs to be combined with visual feedback for less drift.", "The main use of the IMU comes from calculating the orientation of the robot at a high frequency from the raw data. For example, in our D435i, the accelerometer can be set to 250Hz (250 measurements per second) and gyro to 200Hz and a filter (e.g. Kalman or Complementary filter) can be used on the raw data to calculate the orientation of the IMU.", "When you add an IMU to the VO system, it is called Visual Inertial Odometry (VIO). There are also many approaches (loosely-coupled and tightly-coupled) which can output almost \u201cdrift-free\u201d localisation If you choose and carefully tune the correct VIO system. We recommend the \u201cA Benchmark Comparison of Monocular Visual-Inertial Odometry Algorithms for Flying Robots\u201d paper for an overview on the top VIO or SLAMs that use IMUs. In the next blog we\u2019ll talk about some great VIO systems like VINS-Mono and ROVIO.", "All SLAM fanatics likely dream of the ultimate SLAM system which can handle any situation (low light, low texture, fast motion, loop closure, occlusions, dynamic objects and scenes) and still be incredibly robust. We call this mystical mythical creature a \u201cParkour SLAM\u201d.", "While this might be pure hogwash and simply a ridiculous dream, algorithms which are suitable for fast drones, like these VIO systems, come close to the speed and flexibility requirements. So maybe one of these could be the Parkour SLAM we\u2019re looking for, or maybe another day in the future the SLAM of our dreams will arrive. Part 4 of this blog series will be our attempt to find this street urchin SLAM algorithm, test it relentlessly with technical questions, and award one SLAMdog Millionaire a million dollars.", "From the perspective of the Deep Learning (DL) revolution, which has dominated computer vision since 2012, it\u2019s easy to be confused by the fact that most SLAMs use \u201cclassic\u201d keypoint or feature detectors (e.g. SIFT, ORB, FAST, Harris corners). This is surprising given all the hype and good results around DL. So why no deep-SLAM?", "Firstly, it\u2019s important to mention that in \u201cclassic\u201d classification pipelines, keypoint descriptors were used as a feature engineering step before a typical classifier was used to output the class, e.g. a Support Vector Machine (SVM). This paradigm was completely destroyed by DL, but the main takeaway is that keypoints and keypoint descriptors are different: Keypoint descriptors are vectors of values which describe statistical properties of the image patch centred on the keypoint, keypoints refer to the points themselves, i.e. their locations.", "Keypoints are very common and intuitively useful here since SLAM finds the geometric relations and the positions of the 3D points which these detected and tracked 2D keypoints represent. And in fact, this is what VSLAM does in a nutshell.", "SLAM will likely greatly improve with the added use of DL \u2014 for keypoint detection, for semantics or maybe an \u201cend-to-end SLAM\u201d. However, classic features detectors do very well at present. For more on how DL and SLAM could help each other check out Tomasz Malisiewicz\u2019s excellent blog on The Future of Real-Time SLAM and Deep Learning vs SLAM.", "There is another rising paradigm within SLAM that avoids the use of sparse keypoint detectors. These so called \u201cdirect\u201d methods use pixel intensities \u2014 unprocessed RGB values from the image \u2014 to directly estimate the motion of the camera. This minimises a photometric loss whereas feature-based methods usually minimise a geometric loss. Photometric essentially means we are calculating how to projectively warp the first image into the second image using the pixel values directly from both images. We enforce consistency on the transformation from both the raw image, and the transformed synthetic generated image, by minimising the intensity differences.", "There are many variations within direct methods. For example, we could use the sparse points in Direct Sparse Odometry (DSO). The DSO paper in provides a good comparison between all four combinations of dense + direct, dense + indirect, sparse + direct, sparse + indirect. We encourage you to check it out, especially if you find it confusing like we did.", "Another famous paper Semi-dense Visual Odometry (SVO) is a mix of both feature-based and direct methods. Meaning that it only tracks high gradient pixels from edges and corners (using direct photometric error) but relies on feature-based methods for joint optimization of structure and motion. These combinations are why it is deemed to be only \u2018semi-direct\u2019.", "Direct has the potential to be able to track parts of the image with low texture, whereas feature-based SLAM might find this more difficult. Many famous SLAM fanatics believe direct-based SLAM will eventually prevail over indirect. But the results of feature-based SLAM speak for themselves. Only time will tell.", "In summary, we discussed everything from the motivation behind mapping, mapping in simulation, to how localisation and mapping desperately need each other (SLAM); and many related fields (e.g. Visual Odometry, SfM), and the axes of variation between different SLAM algorithms (e.g. their front vs back end, sensor suite, etc.). Not only this, but also the specific methods used, e.g. direct vs feature based, sparse versus dense maps/pointclouds, and whether these systems have global optimisation and loop closure. To illustrate the point of how different SLAM can vary, you can find a comparison between many famous SLAMs in the table below.", "This blog, we hope, should largely introduce and conclude the front-end component of SLAM. In some sense, this component is mostly responsible for calculating the odometry of the camera. In a few cases we saw, it can produce highly accurate localisation without any back-end component needed.", "However, the longer the robot\u2019s trajectory, the more drift will accumulate, even if just 1 millimetre in translation or 1/100 of a rotational degree per 10 kilometres, this will compound. Because of this, in our opinion, a back-end system and the ability to recognise previous places is needed. Fortunately, we reserved the details of this component for the next part of this blog series which will include bundle adjustment, pose graph creation and optimization, as well as loop closure.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "The MTank team creates unique resources on Artificial Intelligence for free and for everyone. Find us @ www.themtank.org"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F8a9129c2ed58&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-cups-to-consciousness-part-3-mapping-your-home-with-slam-8a9129c2ed58&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-cups-to-consciousness-part-3-mapping-your-home-with-slam-8a9129c2ed58&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-cups-to-consciousness-part-3-mapping-your-home-with-slam-8a9129c2ed58&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-cups-to-consciousness-part-3-mapping-your-home-with-slam-8a9129c2ed58&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----8a9129c2ed58--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8a9129c2ed58--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@TheMTank?source=post_page-----8a9129c2ed58--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@TheMTank?source=post_page-----8a9129c2ed58--------------------------------", "anchor_text": "MTank"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa23ef2997f82&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-cups-to-consciousness-part-3-mapping-your-home-with-slam-8a9129c2ed58&user=MTank&userId=a23ef2997f82&source=post_page-a23ef2997f82----8a9129c2ed58---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8a9129c2ed58&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-cups-to-consciousness-part-3-mapping-your-home-with-slam-8a9129c2ed58&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8a9129c2ed58&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-cups-to-consciousness-part-3-mapping-your-home-with-slam-8a9129c2ed58&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "http://www.themtank.com/", "anchor_text": "MTank"}, {"url": "https://towardsdatascience.com/from-cups-to-consciousness-part-1-how-are-cups-related-to-intelligence-8b7c701fa197", "anchor_text": "Part 1: How are cups related to intelligence?"}, {"url": "https://medium.com/@TheMTank/from-cups-to-consciousness-part-2-from-simulation-to-the-real-world-a9ea1249e233", "anchor_text": "Part 2: From simulation to the real world"}, {"url": "https://towardsdatascience.com/from-cups-to-consciousness-part-1-how-are-cups-related-to-intelligence-8b7c701fa197", "anchor_text": "part 1"}, {"url": "https://towardsdatascience.com/from-cups-to-consciousness-part-2-from-simulation-to-the-real-world-a9ea1249e233", "anchor_text": "part 2"}, {"url": "https://towardsdatascience.com/from-cups-to-consciousness-part-2-from-simulation-to-the-real-world-a9ea1249e233", "anchor_text": "last blog"}, {"url": "https://www.themtank.org/a-year-in-computer-vision", "anchor_text": "Computer Vision report"}, {"url": "https://people.eecs.berkeley.edu/~pabbeel/cs287-fa09/readings/Durrant-Whyte_Bailey_SLAM-tutorial-I.pdf", "anchor_text": "part 1"}, {"url": "https://ieeexplore.ieee.org/document/1678144?denied=", "anchor_text": "part 2"}, {"url": "https://link.springer.com/article/10.1007/s40903-015-0032-7", "anchor_text": "An Overview to Visual Odometry and Visual SLAM: Applications to Mobile Robotics"}, {"url": "https://arxiv.org/abs/1606.05830", "anchor_text": "Past, Present, and Future of Simultaneous Localisation And Mapping: Towards the Robust-Perception Age"}, {"url": "https://www.youtube.com/watch?v=8DISRmsO2YQ", "anchor_text": "ORB-SLAM in the KITTI dataset (Sequence 00) YouTube link"}, {"url": "https://github.com/uoip/monoVO-python", "anchor_text": "here"}, {"url": "https://en.wikipedia.org/wiki/Point_set_registration", "anchor_text": "Pointcloud registration"}, {"url": "https://towardsdatascience.com/bag-of-visual-words-in-a-nutshell-9ceea97ce0fb", "anchor_text": "Bag of Visual Words in a Nutshell"}, {"url": "https://introlab.3it.usherbrooke.ca/mediawiki-introlab/images/3/31/Labbe2015ULaval.pdf", "anchor_text": "RTAB-Map slides"}, {"url": "https://twitter.com/Rainmaker1973/status/1185156216315695105", "anchor_text": "doll house"}, {"url": "https://stackoverflow.com/questions/17607312/difference-between-disparity-map-and-disparity-image-in-stereo-matching", "anchor_text": "epipolar lines"}, {"url": "https://github.com/IntelRealSense/librealsense/blob/master/doc/depth-from-stereo.md", "anchor_text": "here"}, {"url": "https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/ismar2011.pdf", "anchor_text": "KinectFusion"}, {"url": "https://github.com/mp3guy/Kintinuous", "anchor_text": "Kintinuous"}, {"url": "http://rpg.ifi.uzh.ch/docs/ICRA18_Delmerico.pdf", "anchor_text": "A Benchmark Comparison of Monocular Visual-Inertial Odometry Algorithms for Flying Robots"}, {"url": "https://www.youtube.com/watch?v=ZMAISVy-6ao", "anchor_text": "ROVIO YouTube"}, {"url": "http://wavelab.uwaterloo.ca/slam/2017-SLAM/Lecture14-Direct_visual_inertial_odometry_and_SLAM/slides.pdf", "anchor_text": "wavelab waterloo slides"}, {"url": "http://www.computervisionblog.com/2016/01/why-slam-matters-future-of-real-time.html", "anchor_text": "The Future of Real-Time SLAM and Deep Learning vs SLAM"}, {"url": "https://en.wikipedia.org/wiki/Reprojection_error", "anchor_text": "geometric loss"}, {"url": "https://ppt-online.org/48421", "anchor_text": "link"}, {"url": "https://arxiv.org/abs/1607.02565", "anchor_text": "Direct Sparse Odometry"}, {"url": "http://rpg.ifi.uzh.ch/docs/TRO17_Forster-SVO.pdf", "anchor_text": "Semi-dense Visual Odometry"}, {"url": "https://link.springer.com/article/10.1186/s41074-017-0027-2", "anchor_text": "Visual SLAM algorithms: a survey from 2010 to 2016"}, {"url": "https://medium.com/tag/computer-vision?source=post_page-----8a9129c2ed58---------------computer_vision-----------------", "anchor_text": "Computer Vision"}, {"url": "https://medium.com/tag/robotics?source=post_page-----8a9129c2ed58---------------robotics-----------------", "anchor_text": "Robotics"}, {"url": "https://medium.com/tag/optimisation?source=post_page-----8a9129c2ed58---------------optimisation-----------------", "anchor_text": "Optimisation"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----8a9129c2ed58---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----8a9129c2ed58---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8a9129c2ed58&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-cups-to-consciousness-part-3-mapping-your-home-with-slam-8a9129c2ed58&user=MTank&userId=a23ef2997f82&source=-----8a9129c2ed58---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8a9129c2ed58&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-cups-to-consciousness-part-3-mapping-your-home-with-slam-8a9129c2ed58&user=MTank&userId=a23ef2997f82&source=-----8a9129c2ed58---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8a9129c2ed58&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-cups-to-consciousness-part-3-mapping-your-home-with-slam-8a9129c2ed58&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8a9129c2ed58--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F8a9129c2ed58&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-cups-to-consciousness-part-3-mapping-your-home-with-slam-8a9129c2ed58&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----8a9129c2ed58---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----8a9129c2ed58--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----8a9129c2ed58--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----8a9129c2ed58--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----8a9129c2ed58--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----8a9129c2ed58--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----8a9129c2ed58--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----8a9129c2ed58--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----8a9129c2ed58--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@TheMTank?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@TheMTank?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "MTank"}, {"url": "https://medium.com/@TheMTank/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "665 Followers"}, {"url": "http://www.themtank.org", "anchor_text": "www.themtank.org"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa23ef2997f82&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-cups-to-consciousness-part-3-mapping-your-home-with-slam-8a9129c2ed58&user=MTank&userId=a23ef2997f82&source=post_page-a23ef2997f82--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fd80e30d7837e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-cups-to-consciousness-part-3-mapping-your-home-with-slam-8a9129c2ed58&newsletterV3=a23ef2997f82&newsletterV3Id=d80e30d7837e&user=MTank&userId=a23ef2997f82&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}