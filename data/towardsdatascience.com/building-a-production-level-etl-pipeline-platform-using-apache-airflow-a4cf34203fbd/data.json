{"url": "https://towardsdatascience.com/building-a-production-level-etl-pipeline-platform-using-apache-airflow-a4cf34203fbd", "time": 1683001080.3841941, "path": "towardsdatascience.com/building-a-production-level-etl-pipeline-platform-using-apache-airflow-a4cf34203fbd/", "webpage": {"metadata": {"title": "Building a Production-Level ETL Pipeline Platform Using Apache Airflow | by Aakash Pydi | Towards Data Science", "h1": "Building a Production-Level ETL Pipeline Platform Using Apache Airflow", "description": "The CernerWorks Enterprise System Management team is responsible for mining systems data from Cerner clients\u2019 systems, providing visibility to the collected data for various teams within Cerner, and\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.cerner.com/solutions/hosting-monitoring", "anchor_text": "CernerWorks", "paragraph_index": 0}, {"url": "https://airflow.apache.org/", "anchor_text": "Apache Airflow", "paragraph_index": 1}, {"url": "https://airflow.apache.org/concepts.html", "anchor_text": "DAG (Directed Acyclic Graph)", "paragraph_index": 1}, {"url": "https://airflow.apache.org/howto/operator/index.html", "anchor_text": "Operator", "paragraph_index": 2}, {"url": "https://github.com/apache/airflow/tree/master/airflow/operators", "anchor_text": "operators", "paragraph_index": 2}, {"url": "https://issues.apache.org/jira/browse/AIRFLOW-289", "anchor_text": "AIRFLOW-289", "paragraph_index": 11}, {"url": "https://hp.cioreview.com/cxoinsight/cerner-advances-big-data-analytic-capabilities-nid-11200-cid-59.html", "anchor_text": "Cerner Advances Big Data Analytics Capabilities. Dan Woicke. Director, Enterprise System Management", "paragraph_index": 12}, {"url": "https://landing.google.com/sre/sre-book/chapters/service-level-objectives/", "anchor_text": "SLA", "paragraph_index": 12}, {"url": "http://www.celeryproject.org/", "anchor_text": "Celery", "paragraph_index": 17}, {"url": "http://site.clairvoyantsoft.com/making-apache-airflow-highly-available/", "anchor_text": "Making Apache Airflow Highly Available", "paragraph_index": 17}, {"url": "https://towardsdatascience.com/a-definitive-compilation-of-apache-airflow-resources-82bc4980c154", "anchor_text": "A Definitive Compilation of Apache Airflow Resources", "paragraph_index": 20}], "all_paragraphs": ["The CernerWorks Enterprise System Management team is responsible for mining systems data from Cerner clients\u2019 systems, providing visibility to the collected data for various teams within Cerner, and building monitoring solutions using the collected data. Our core mission is to help increase the reliability and security of Cerner clients\u2019 systems. About three years ago, our team was at a place where we had developed an effective telemetry framework for systems data collection. At the same time, we were seeing an exponential increase in use-cases where we had to transform the collected systems data in various ways to support our visibility and monitoring efforts. We thereby felt a pressing need to introduce a dedicated ETL pipeline platform to our data architecture.", "After some research, we found that the Apache Airflow open source framework would be a good fit for our requirements as it was designed to implement, schedule and monitor data workflows. In the Airflow context, a data workflow is represented by a DAG (Directed Acyclic Graph) which is a set of tasks with acyclic dependencies as shown below.", "Each task in a DAG is implemented using an Operator. Airflow\u2019s open source codebase provides a set of general operators, however, the framework\u2019s primary appeal to us, was that we could implement custom operators uniquely suited for Cerner\u2019s data workflows. Beyond being able to write custom operators, Airflow as a framework is designed to be heavily customizable.", "Our team worked on customizations motivated by requirements along two fronts (i) minimizing the development overhead for scheduling Cerner data workflows on our platform, and (ii) adopting a robust and reliable production architecture. We named our customized Airflow implementation Jetstream.", "Jetstream\u2019s design pattern for efficiently supporting Cerner\u2019s data workflows.", "The meat of the logic for implementing a given data workflow in Jetstream is within its operators and modules. The modules have logic that might be useful across a wide range of data workflows. For instance, connecting to various databases, connecting to various external and internal APIs, scheduling logic, batching logic, and so on. The operators are a generalized representation for a class of tasks. They load configuration data from the task YAML and utilize the modules to implement a given task. For instance, the DatabaseTransferOperator, allows consumers to transform and move data from one database to another. The task YAML captures the specialized and essential business logic associated with a given task.", "Our team\u2019s systems data visibility and monitoring efforts involve partnerships with a wide variety of teams across Cerner. Many of our requirements for transforming systems data come from these teams. By abstracting away, the meat of the involved logic, we were able to empower these teams to be consumers of our ETL platform with minimal development overhead. This is why the platform has continued to grow significantly over the last three years, and currently has more than 3000 daily tasks. A couple of example task YAMLs are included below,", "Particularly since we introduced Jetstream very early in Airflow\u2019s open source journey, developing a robust and reliable production architecture has been an iterative process. Let\u2019s discuss the design motivations for some of the significant changes to Jetstream\u2019s architecture in the last three years.", "Jetstream Architecture: Improving Scheduling Metadata Reliability and Using Pools", "The picture above captures two significant architecture changes that we made to our initial vanilla proof of concept setup.", "(1) Using our data warehouse to store important task scheduling metadata.", "Having reliable and custom scheduling metadata was an important requirement for our team. We particularly wanted to remove our dependency on Airflow\u2019s native options for storing scheduling metadata. As early adopters of Airflow, we wanted to hedge against significant airflow design changes that might have affected any native scheduling metadata logic that we leveraged. Also, since Cerner has clients all over the world, many of our tasks, rely heavily on logic built on top of our scheduling metadata to process correctly. For instance, Airflow adopted the UTC time standard (AIRFLOW-289) in its 1.9 release which would have broken our logic if we had a hard dependency on Airflow\u2019s native scheduling options.", "The other important consideration was the sheer value of the scheduling metadata. This metadata essentially captures the time intervals for which the data associated with each task was processed. Say our scheduling metadata got corrupted, this could lead to data gaps (missing time intervals) or duplicate data (reprocessed time intervals). Having such critical data on our Jetstream nodes would introduce a high-cost single point of failure. A data warehouse has been a central component of our team\u2019s data architecture (see Cerner Advances Big Data Analytics Capabilities. Dan Woicke. Director, Enterprise System Management). So, our data warehouse\u2019s cluster is closely monitored and thereby has high reliability. By storing this metadata on our data warehouse, even if our Jetstream nodes go down, we can fire them back up and start our jobs without worrying about corrupt, inconsistent or missing scheduling metadata. So, adding this dependency, allowed us to significantly increase the robustness and reliability of our platform. Having this data in the data warehouse also lets us build SLA based alerting logic and develop performance reporting.", "We initially didn\u2019t leverage the execution pools support provided by the Airflow framework. When a DAG\u2019s (Directed Acyclic Graph) task was ready to be scheduled, it would be added to Airflow\u2019s default execution pool, to be picked up for execution by the next available executor. The problem was when we added a DAG with more than a thousand tasks, this DAG\u2019s tasks crowded out the tasks from the other DAGs. We then started to use named execution pools which would limit the number of worker slots available to each DAG. This let us develop a granular control on execution parallelism and prevent DAG\u2019s from being unintentionally crowded out.", "Jetstream Architecture: Setting Up a Jetstream Cluster and Adding Support for Near Real Time Scheduling", "The picture above represents the current production architecture of Jetstream.", "(3) Setting up a Jetstream Cluster.", "We used Airflow\u2019s support for leveraging Celery to setup a production cluster. This let us leverage the benefits of concurrent processing and thereby boost the platform\u2019s robustness and efficiency. Each DAG is created with an associated Celery queue. Each worker node is assigned a set of Celery queues. This means that the tasks from each DAG can be executed on worker nodes \u2018listening\u2019 for the corresponding DAG\u2019s queue. We make sure that every queue has two or more assigned worker nodes to boost the reliability of the platform as we don\u2019t want to have worker nodes that are potential single points of failure. However, notice that the master node is still a single point of failure. See \u2018Making Apache Airflow Highly Available\u2019 for a potential solution to this problem. Addressing this hasn\u2019t been a design priority yet.", "(4) Support for Near Real Time Scheduling.", "We recently had important use cases to do near real time scheduling on our platform. We realized that we needed to store scheduling metadata in-memory in order to support near real time data workflows. So, for our near real time DAGs, we use Redis instead of our data warehouse for storing scheduling metadata.", "For additional reading, see A Definitive Compilation of Apache Airflow Resources.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Code. Debug. Repeat. Currently an SDE II at Amazon AI (AWS SageMaker Hosting). Always promoting curiosity, camaraderie and compassion. All opinions are my own."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fa4cf34203fbd&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-production-level-etl-pipeline-platform-using-apache-airflow-a4cf34203fbd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-production-level-etl-pipeline-platform-using-apache-airflow-a4cf34203fbd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-production-level-etl-pipeline-platform-using-apache-airflow-a4cf34203fbd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-production-level-etl-pipeline-platform-using-apache-airflow-a4cf34203fbd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----a4cf34203fbd--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a4cf34203fbd--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@aakashpydi?source=post_page-----a4cf34203fbd--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@aakashpydi?source=post_page-----a4cf34203fbd--------------------------------", "anchor_text": "Aakash Pydi"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6c7501dfca1b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-production-level-etl-pipeline-platform-using-apache-airflow-a4cf34203fbd&user=Aakash+Pydi&userId=6c7501dfca1b&source=post_page-6c7501dfca1b----a4cf34203fbd---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa4cf34203fbd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-production-level-etl-pipeline-platform-using-apache-airflow-a4cf34203fbd&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa4cf34203fbd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-production-level-etl-pipeline-platform-using-apache-airflow-a4cf34203fbd&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.cerner.com/solutions/hosting-monitoring", "anchor_text": "CernerWorks"}, {"url": "https://airflow.apache.org/", "anchor_text": "Apache Airflow"}, {"url": "https://airflow.apache.org/concepts.html", "anchor_text": "DAG (Directed Acyclic Graph)"}, {"url": "https://airflow.apache.org/howto/operator/index.html", "anchor_text": "Operator"}, {"url": "https://github.com/apache/airflow/tree/master/airflow/operators", "anchor_text": "operators"}, {"url": "https://issues.apache.org/jira/browse/AIRFLOW-289", "anchor_text": "AIRFLOW-289"}, {"url": "https://hp.cioreview.com/cxoinsight/cerner-advances-big-data-analytic-capabilities-nid-11200-cid-59.html", "anchor_text": "Cerner Advances Big Data Analytics Capabilities. Dan Woicke. Director, Enterprise System Management"}, {"url": "https://landing.google.com/sre/sre-book/chapters/service-level-objectives/", "anchor_text": "SLA"}, {"url": "http://www.celeryproject.org/", "anchor_text": "Celery"}, {"url": "http://site.clairvoyantsoft.com/making-apache-airflow-highly-available/", "anchor_text": "Making Apache Airflow Highly Available"}, {"url": "https://www.linkedin.com/in/gunjankaphle/", "anchor_text": "Kaphle,Gunjan."}, {"url": "https://www.linkedin.com/in/adam-agnew-b49068139/", "anchor_text": "Agnew,Adam"}, {"url": "https://www.linkedin.com/in/cfilippelli/", "anchor_text": "Filippelli,Carlo"}, {"url": "https://www.linkedin.com/in/aakash-pydi/", "anchor_text": "Pydi, Aakash"}, {"url": "https://towardsdatascience.com/a-definitive-compilation-of-apache-airflow-resources-82bc4980c154", "anchor_text": "A Definitive Compilation of Apache Airflow Resources"}, {"url": "https://medium.com/tag/data-engineering?source=post_page-----a4cf34203fbd---------------data_engineering-----------------", "anchor_text": "Data Engineering"}, {"url": "https://medium.com/tag/apache-airflow?source=post_page-----a4cf34203fbd---------------apache_airflow-----------------", "anchor_text": "Apache Airflow"}, {"url": "https://medium.com/tag/data-science?source=post_page-----a4cf34203fbd---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/data?source=post_page-----a4cf34203fbd---------------data-----------------", "anchor_text": "Data"}, {"url": "https://medium.com/tag/big-data?source=post_page-----a4cf34203fbd---------------big_data-----------------", "anchor_text": "Big Data"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa4cf34203fbd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-production-level-etl-pipeline-platform-using-apache-airflow-a4cf34203fbd&user=Aakash+Pydi&userId=6c7501dfca1b&source=-----a4cf34203fbd---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa4cf34203fbd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-production-level-etl-pipeline-platform-using-apache-airflow-a4cf34203fbd&user=Aakash+Pydi&userId=6c7501dfca1b&source=-----a4cf34203fbd---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa4cf34203fbd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-production-level-etl-pipeline-platform-using-apache-airflow-a4cf34203fbd&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a4cf34203fbd--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fa4cf34203fbd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-production-level-etl-pipeline-platform-using-apache-airflow-a4cf34203fbd&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----a4cf34203fbd---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----a4cf34203fbd--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----a4cf34203fbd--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----a4cf34203fbd--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----a4cf34203fbd--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----a4cf34203fbd--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----a4cf34203fbd--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----a4cf34203fbd--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----a4cf34203fbd--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@aakashpydi?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@aakashpydi?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Aakash Pydi"}, {"url": "https://medium.com/@aakashpydi/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "184 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6c7501dfca1b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-production-level-etl-pipeline-platform-using-apache-airflow-a4cf34203fbd&user=Aakash+Pydi&userId=6c7501dfca1b&source=post_page-6c7501dfca1b--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fc0d59dd0dae6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-a-production-level-etl-pipeline-platform-using-apache-airflow-a4cf34203fbd&newsletterV3=6c7501dfca1b&newsletterV3Id=c0d59dd0dae6&user=Aakash+Pydi&userId=6c7501dfca1b&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}