{"url": "https://towardsdatascience.com/how-to-enforce-the-outcome-of-your-ml-classifiers-b5f6163d68c2", "time": 1683015456.251359, "path": "towardsdatascience.com/how-to-enforce-the-outcome-of-your-ml-classifiers-b5f6163d68c2/", "webpage": {"metadata": {"title": "How to enforce the outcome of your ML Classifiers | by Aziz Ketari | Towards Data Science", "h1": "How to enforce the outcome of your ML Classifiers", "description": "Calibrating machine learning classifiers for better decision making. Model interpretation using predicted probabilities with python, scikit-learn and pandas"}, "outgoing_paragraph_urls": [{"url": "https://www.scientificamerican.com/article/kahneman-excerpt-thinking-fast-and-slow/", "anchor_text": "Daniel Kahnema", "paragraph_index": 1}, {"url": "https://apps.dtic.mil/dtic/tr/fulltext/u2/a135966.pdf", "anchor_text": "DeGroot & Fienberg (1983)", "paragraph_index": 2}, {"url": "https://www.coursera.org/learn/machine-learning", "anchor_text": "Machine Learning course on Coursera", "paragraph_index": 4}, {"url": "https://en.wikipedia.org/wiki/Ex-ante", "anchor_text": "ex-ante probabilities", "paragraph_index": 5}, {"url": "https://nlp.stanford.edu/IR-book/html/htmledition/support-vector-machines-the-linearly-separable-case-1.html#:~:text=Maximizing%20the%20margin%20seems%20good,the%20classifier%20deciding%20either%20way.&text=By%20construction%2C%20an%20SVM%20classifier,margin%20around%20the%20decision%20boundary.", "anchor_text": "material", "paragraph_index": 11}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html", "anchor_text": "documentation for SVMs on scikit-learn", "paragraph_index": 12}, {"url": "https://www.cs.colorado.edu/~mozer/Teaching/syllabi/6622/papers/Platt1999.pdf", "anchor_text": "paper", "paragraph_index": 13}, {"url": "https://github.com/scikit-learn/scikit-learn/blob/0fb307bf3/sklearn/ensemble/_gb.py#L1200", "anchor_text": "_stagged_raw_predict function in scikit-learn github repo", "paragraph_index": 15}, {"url": "https://www.researchgate.net/publication/228965727_Evaluating_probability_estimates_from_decision_trees", "anchor_text": "N. Chawla in [9]", "paragraph_index": 33}, {"url": "https://apps.dtic.mil/dtic/tr/fulltext/u2/a135966.pdf", "anchor_text": "Comparing Probability Forecasters: Basic Binary Concepts and Multivariate Extensions by DeGroot & Fienberg (1982)", "paragraph_index": 43}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.calibration.calibration_curve.html", "anchor_text": "Sklearn Calibration Curve method", "paragraph_index": 43}, {"url": "https://towardsdatascience.com/introduction-to-reliability-diagrams-for-probability-calibration-ed785b3f5d44", "anchor_text": "A brief introduction to uncertainty calibration and reliability diagrams", "paragraph_index": 43}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.brier_score_loss.html", "anchor_text": "Sklearn Brier Score method", "paragraph_index": 43}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.calibration.CalibratedClassifierCV.html#sklearn.calibration.CalibratedClassifierCV", "anchor_text": "Sklearn Calibration method", "paragraph_index": 43}, {"url": "https://github.com/scikit-learn/scikit-learn/blob/0fb307bf3/sklearn/calibration.py#L33", "anchor_text": "Source code of Sklearn calibration.py", "paragraph_index": 43}, {"url": "http://It is up to the user make sure that the data used for fitting the classifier is disjoint from the data used for fitting the regressor.", "anchor_text": "Sklearn calibration guide", "paragraph_index": 43}, {"url": "https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/_isotonic.pyx", "anchor_text": "PAVA implementation in Scikit-Learn", "paragraph_index": 43}, {"url": "https://www.researchgate.net/publication/228965727_Evaluating_probability_estimates_from_decision_trees", "anchor_text": "Evaluating probability estimates from decision trees by N. Chawla (2006)", "paragraph_index": 43}], "all_paragraphs": ["Making decisions based on your Classifiers\u2019 outputs can be tricky. Luckily, the interpretation of the predicted probabilities has recently become easier through the availability of tools for calibration. I\u2019ve been exposed to this matter because a substantial part of my job requires me to not only interpret models\u2019 output but also to recommend product strategies based on these results. In fact, plenty of businesses rely on predicted probabilities to make decisions (e.g betting odds, ads targeting \u2026). If it\u2019s also your case, this article describes a systemic approach to ensure the calibration of your classifiers with a healthy mix of theory, code and visualizations!", "For people with an engineering background, the term \u201ccalibration\u201d might have excited your system 1 (Daniel Kahneman) and triggered the visualization of a lab room filled with measuring instruments as depicted in this photo. Well, calibrating your ML classifiers is quite similar. Intrigued?", "Throughout this article, it\u2019s fundamental to keep in mind that the goal of calibration is to obtain \u201cpredicted probabilities [that] approach the empirical probability [i.e observed class frequency] as the number of predictions goes to infinity\u201d as defined by DeGroot & Fienberg (1983) [1].", "By the end of this article, the goal is is for you to know how to do 3 things:", "Classification can be understood simply as the separation of data points by a \u201cline\u201d (plane in 2D or hyperplane in >3D+) as explained by Andrew Ng in his first Machine Learning course on Coursera. Algorithms are responsible in finding that optimal \u201cline\u201d. However, these algorithms take different approaches while relying on different assumptions. During inference, classifiers can either predict the label directly or the probability of belonging to each class label.", "The reason why calibration matters is because, ideally, we would like to be able to interpret these probabilities as ex-ante probabilities.", "But, how can you do that when the distribution of the probabilities does NOT match the distribution observed in your data? Answer: Calibration.", "But concretely, what does it mean to you and to your classifiers?", "Let\u2019s look at Logistic Regression. Recall that classifiers try to minimize a loss function (in this case cross entropy) in order to find the best \u201cline\u201d that split the data into 2 parts (binary classification). The minimum of this loss function is found when its derivative is equal to 0. I will spare you the 3\u20134 lines of math formulas and conveniently we end up with the following equality:", "Where p is the class probabilities and y represents the observed class values.", "Assuming that the dataset is linearly separable, this equality seems to tell us that the class probabilities will correspond exactly to the expected values. Therefore, Logistic Regression is very likely to provide calibrated probabilities. This makes it very appealing to use in applied use cases.", "Unfortunately, other (more complex) classifiers don\u2019t present this characteristic. Support Vector Machines (SVMs) are a great example. In fact, SVMs find the optimal \u201cline\u201d by maximizing the margin around the decision boundary (i.e \u201cline\u201d). It does NOT output any likelihood but rather use the concept of margins (here is some material from a course at Stanford).", "At this point, some of you might be confused and opened a new tab with the documentation for SVMs on scikit-learn (if you\u2019re a python user) to double check if you could output probabilities from SVMs. Let me save you some time:", "It seems like the outputted probabilities from the sklearn predict_proba method on Linear SVM estimators are actually coming from a Logistic Regression on the SVM\u2019s scores (a method commonly referred as Platt scaling for the author of the paper that described it).", "Interesting, huh? It seems like scikit-learn already does it for you. But is it true for all classifiers? How about Decision Trees? Let me save you some time again:", "For a single Decision Tree, the outputted probabilities represent a frequency (dictated by your training set, which is why a single decision tree is VERY likely to overfit your training data aka high variance) and not a likelihood.Let me save you even more time, it\u2019s also true with RandomForestClassifiers.In fact, the returned probability is the fraction of trees that predicted the class of interest over all trees. Similar reasoning for GradientBoostingClassifiers (described in the _stagged_raw_predict function in scikit-learn github repo)", "Note:- NO calibration is mentioned for the Tree based classifiers on scikit-learn", "In practice, reliability diagram is a plot of the observed frequency of the class of interest as a function of the predicted probability.", "This graph seems to show a clear case of poor calibration of the Vanilla GB. The curve seems to follow a S-shaped trend. For this graph, I chose to display 10 bins since I had a relatively large dataset (400k+ data points) for calibration.", "However could we have reached the same conclusion with a smaller number of bins \ud83e\udd14 ?", "This is the exact same graph but using 4 bins instead of 10. We can still notice poor calibration in the case of the vanilla GB. However, the shape of the curve is not very pronounced. In this case, it is not detrimental since the dataset is large \u201cenough\u201d for us to calibrate the model using Isotonic Regression. More about it further down.", "After you trained your model and predicted on your test data, it\u2019s now time to construct your reliability diagram. You can either do it manually:", "Step 1: Choose the number of bins Step 2: Bucket the data points and predicted probabilities into the corresponding bins. The size of the bin can be determined by either ensuring that they all contain the same number of data points (uniform strategy) or can follow the a more sophisticated split using quantiles.Step 3: Compute the average probability and the average frequency in the corresponding bin Step 4: Plot the average observed frequency as a function of the the average predicted probabilities", "Or you can choose to use the off-the-shelf method on scikit-learn:", "Tips:- \u26a0\ufe0f WARNING: A common mistake is to plot reliability diagrams using the mean accuracy \ud83d\udc4e instead of the mean predicted probabilities \ud83d\udc4d as noted by Xiang Jiang as well [3]- The width of the bin doesn\u2019t have to be uniform. You can choose to use a split that follows the quantiles distribution- I recommend to use a relatively high number of bins (depending on your dataset size) in order to better visualize the presence of poor calibration.", "Note that the early large fluctuations are due to the small sample size (high variance). Indeed, the more data is used to compute the brier loss the more stable it becomes.", "Note:The previous formula does not include the sample weight. In case you are using the class weights (proportion of data points for the positive and negative class), then the below formula is more appropriate to compute the Brier loss.", "By now, it is understood that calibrating your model can be described as the act of fitting a regressor. The regressor is expected to find a function that maps the output probabilities of the classifier to the observed class frequency.", "In practice, 2 situations might occur:", "Note:- \u26a0\ufe0f When calibrating probabilities from an already fitted classifier, the fitting step is only applied to the regressor used for calibration and not the underlying classifier (as describe in line 237 of the source code of Sklearn calibration file [6])- \u26a0\ufe0f When calibrating probabilities from an already fitted classifier, you must fit the regressor with a disjoint dataset than the one you used to fit the classifier (as describe in the guide [7])", "2. Isotonic Regression is useful for enforcing a monotonic fit to the data. Remember that the definition of perfectly calibrated probabilities is the diagonal line in the reliability diagram. Hence, we know that the best case is when the curve trend is monotonic. Therefore, Isotonic Regression, by design, can act as a powerful smoother for any case where the output from the model is monotonic. Here is a useful visualization to understand its mechanics:", "Fun fact about Isotonic Regression is that there is no parameter to find. It\u2019s a non-parametric approach that leverages \u201csmart\u201d smoothing methods such as Pool Adjacent Violators Algorithm (PAVA). As a matter of fact, PAVA is used in the Isotonic Regression method in scikit-learn [8].", "Note:-\ud83d\udca2 If something is too good to be true, it probably is. In fact, Isotonic Regression has a tendency to overfit, by design. In order to mitigate overfitting, you should consider using Isotonic Regression only with large datasets.", "3. Laplace estimate & m-estimate: You could also choose to change the estimators of your underlying model such as Laplace estimator or m-estimators. N. Chawla in [9] describes an example of this change in the case of RandomForestClassifier:", "\u201c[The predicted probability] can either be the leaf frequency based estimate or [can be] smoothed by Laplace or m-estimate.\u201d", "Unfortunately, his work has not (yet) been implemented into a software package (as far as I know).", "Last but not least, here is the framework that you could use for your cases:", "Step 0: Should you be worried about calibration?Are you using a classifier other than Logistic Regression?If so, proceed to step 1.", "Step 1: Risk Assessment 1.1 Plot your reliability diagram1.2 Compute the Brier loss", "Step 2: Take Action2.1 If the distortion on the reliability diagram looks like a S-shape, then fitting a regressor that leverages a sigmoid function is probably a good idea. You guessed it \u2192 Use Platt Scaling 2.2 Otherwise, before using Isotonic Regression, you should verify that you\u2019re using a large dataset for calibration.", "In this article, we\u2019ve seen that:", "I hope that you found this guide useful. As more ML models gets deployed, it is our (the creators) responsibility to make sure that their outcomes are interpreted correctly. Encourage accountability, always!", "I would love to hear everybody\u2019s opinion about this topic. Don\u2019t be a stranger and drop a comment with your experience/approach.", "[1] Comparing Probability Forecasters: Basic Binary Concepts and Multivariate Extensions by DeGroot & Fienberg (1982)[2] Sklearn Calibration Curve method[3] A brief introduction to uncertainty calibration and reliability diagrams by Xiang Jiang [4] Sklearn Brier Score method[5] Sklearn Calibration method[6] Source code of Sklearn calibration.py[7] Sklearn calibration guide[8] PAVA implementation in Scikit-Learn used for Isotonic Regression[9] Evaluating probability estimates from decision trees by N. Chawla (2006)", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fb5f6163d68c2&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-enforce-the-outcome-of-your-ml-classifiers-b5f6163d68c2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-enforce-the-outcome-of-your-ml-classifiers-b5f6163d68c2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-enforce-the-outcome-of-your-ml-classifiers-b5f6163d68c2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-enforce-the-outcome-of-your-ml-classifiers-b5f6163d68c2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----b5f6163d68c2--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b5f6163d68c2--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://aziz-ketari.medium.com/?source=post_page-----b5f6163d68c2--------------------------------", "anchor_text": ""}, {"url": "https://aziz-ketari.medium.com/?source=post_page-----b5f6163d68c2--------------------------------", "anchor_text": "Aziz Ketari"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Faee91bddfb05&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-enforce-the-outcome-of-your-ml-classifiers-b5f6163d68c2&user=Aziz+Ketari&userId=aee91bddfb05&source=post_page-aee91bddfb05----b5f6163d68c2---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb5f6163d68c2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-enforce-the-outcome-of-your-ml-classifiers-b5f6163d68c2&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb5f6163d68c2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-enforce-the-outcome-of-your-ml-classifiers-b5f6163d68c2&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@nci?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "National Cancer Institute"}, {"url": "https://unsplash.com/s/photos/lab?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://www.scientificamerican.com/article/kahneman-excerpt-thinking-fast-and-slow/", "anchor_text": "Daniel Kahnema"}, {"url": "https://apps.dtic.mil/dtic/tr/fulltext/u2/a135966.pdf", "anchor_text": "DeGroot & Fienberg (1983)"}, {"url": "https://unsplash.com/@camylla93?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Camylla Battani"}, {"url": "https://unsplash.com/s/photos/question?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://www.coursera.org/learn/machine-learning", "anchor_text": "Machine Learning course on Coursera"}, {"url": "https://en.wikipedia.org/wiki/Ex-ante", "anchor_text": "ex-ante probabilities"}, {"url": "https://nlp.stanford.edu/IR-book/html/htmledition/support-vector-machines-the-linearly-separable-case-1.html#:~:text=Maximizing%20the%20margin%20seems%20good,the%20classifier%20deciding%20either%20way.&text=By%20construction%2C%20an%20SVM%20classifier,margin%20around%20the%20decision%20boundary.", "anchor_text": "material"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html", "anchor_text": "documentation for SVMs on scikit-learn"}, {"url": "https://scikit-learn.org/stable/modules/svm.html#scores-probabilities", "anchor_text": "Open Source Library"}, {"url": "https://www.cs.colorado.edu/~mozer/Teaching/syllabi/6622/papers/Platt1999.pdf", "anchor_text": "paper"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier.predict_proba", "anchor_text": "Open Source Library"}, {"url": "https://github.com/scikit-learn/scikit-learn/blob/0fb307bf3/sklearn/ensemble/_gb.py#L1200", "anchor_text": "_stagged_raw_predict function in scikit-learn github repo"}, {"url": "http://www.bom.gov.au/wmo/lrfvs/reliability.shtml#:~:text=Reliability%20diagrams%20(Hartmann%20et%20al,a%20forecast%20probability%20actually%20occurred.", "anchor_text": "Reliability Diagram"}, {"url": "https://en.wikipedia.org/wiki/Brier_score", "anchor_text": "Brier loss"}, {"url": "https://www.cs.colorado.edu/~mozer/Teaching/syllabi/6622/papers/Platt1999.pdf", "anchor_text": "Paper available here"}, {"url": "https://scikit-learn.org/stable/auto_examples/miscellaneous/plot_isotonic_regression.html", "anchor_text": "code"}, {"url": "https://en.wikipedia.org/wiki/BSD_licenses#:~:text=The%20BSD%20license%20is%20a,code%20be%20distributed%20at%20all.", "anchor_text": "BSD license"}, {"url": "https://www.researchgate.net/publication/228965727_Evaluating_probability_estimates_from_decision_trees", "anchor_text": "N. Chawla in [9]"}, {"url": "https://www.cs.cornell.edu/~alexn/papers/calibration.icml05.crc.rev3.pdf", "anchor_text": "Predicting Good Probabilities With Supervised Learning by Niculescu-Mizil & Caruana (2005)"}, {"url": "https://apps.dtic.mil/dtic/tr/fulltext/u2/a135966.pdf", "anchor_text": "Comparing Probability Forecasters: Basic Binary Concepts and Multivariate Extensions by DeGroot & Fienberg (1982)"}, {"url": "https://www.researchgate.net/publication/228965727_Evaluating_probability_estimates_from_decision_trees", "anchor_text": "Evaluating probability estimates from decision trees by N. Chawla (2006)"}, {"url": "https://apps.dtic.mil/dtic/tr/fulltext/u2/a135966.pdf", "anchor_text": "Comparing Probability Forecasters: Basic Binary Concepts and Multivariate Extensions by DeGroot & Fienberg (1982)"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.calibration.calibration_curve.html", "anchor_text": "Sklearn Calibration Curve method"}, {"url": "https://towardsdatascience.com/introduction-to-reliability-diagrams-for-probability-calibration-ed785b3f5d44", "anchor_text": "A brief introduction to uncertainty calibration and reliability diagrams"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.brier_score_loss.html", "anchor_text": "Sklearn Brier Score method"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.calibration.CalibratedClassifierCV.html#sklearn.calibration.CalibratedClassifierCV", "anchor_text": "Sklearn Calibration method"}, {"url": "https://github.com/scikit-learn/scikit-learn/blob/0fb307bf3/sklearn/calibration.py#L33", "anchor_text": "Source code of Sklearn calibration.py"}, {"url": "http://It is up to the user make sure that the data used for fitting the classifier is disjoint from the data used for fitting the regressor.", "anchor_text": "Sklearn calibration guide"}, {"url": "https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/_isotonic.pyx", "anchor_text": "PAVA implementation in Scikit-Learn"}, {"url": "https://www.researchgate.net/publication/228965727_Evaluating_probability_estimates_from_decision_trees", "anchor_text": "Evaluating probability estimates from decision trees by N. Chawla (2006)"}, {"url": "https://medium.com/tag/classification?source=post_page-----b5f6163d68c2---------------classification-----------------", "anchor_text": "Classification"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----b5f6163d68c2---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/model?source=post_page-----b5f6163d68c2---------------model-----------------", "anchor_text": "Model"}, {"url": "https://medium.com/tag/software-development?source=post_page-----b5f6163d68c2---------------software_development-----------------", "anchor_text": "Software Development"}, {"url": "https://medium.com/tag/predictions?source=post_page-----b5f6163d68c2---------------predictions-----------------", "anchor_text": "Predictions"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb5f6163d68c2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-enforce-the-outcome-of-your-ml-classifiers-b5f6163d68c2&user=Aziz+Ketari&userId=aee91bddfb05&source=-----b5f6163d68c2---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb5f6163d68c2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-enforce-the-outcome-of-your-ml-classifiers-b5f6163d68c2&user=Aziz+Ketari&userId=aee91bddfb05&source=-----b5f6163d68c2---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb5f6163d68c2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-enforce-the-outcome-of-your-ml-classifiers-b5f6163d68c2&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b5f6163d68c2--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fb5f6163d68c2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-enforce-the-outcome-of-your-ml-classifiers-b5f6163d68c2&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----b5f6163d68c2---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----b5f6163d68c2--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----b5f6163d68c2--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----b5f6163d68c2--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----b5f6163d68c2--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----b5f6163d68c2--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----b5f6163d68c2--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----b5f6163d68c2--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----b5f6163d68c2--------------------------------", "anchor_text": ""}, {"url": "https://aziz-ketari.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://aziz-ketari.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Aziz Ketari"}, {"url": "https://aziz-ketari.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "66 Followers"}, {"url": "https://github.com/azizketari/", "anchor_text": "https://github.com/azizketari/"}, {"url": "https://www.linkedin.com/in/aziz-ketari", "anchor_text": "https://www.linkedin.com/in/aziz-ketari"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Faee91bddfb05&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-enforce-the-outcome-of-your-ml-classifiers-b5f6163d68c2&user=Aziz+Ketari&userId=aee91bddfb05&source=post_page-aee91bddfb05--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Faee91bddfb05%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-enforce-the-outcome-of-your-ml-classifiers-b5f6163d68c2&user=Aziz+Ketari&userId=aee91bddfb05&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}