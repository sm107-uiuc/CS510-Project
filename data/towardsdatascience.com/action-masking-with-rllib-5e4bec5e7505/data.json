{"url": "https://towardsdatascience.com/action-masking-with-rllib-5e4bec5e7505", "time": 1683013027.314302, "path": "towardsdatascience.com/action-masking-with-rllib-5e4bec5e7505/", "webpage": {"metadata": {"title": "Action Masking with RLlib. RL algorithms learn via trial and\u2026 | by Christian Hubbs | Towards Data Science", "h1": "Action Masking with RLlib", "description": "RL algorithms learn via trial and error. The agent searches the state space early on and takes random actions to learn what leads to a good reward. Pretty straightforward. Unfortunately, this isn\u2019t\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.datahubbs.com/action-masking-with-rllib/", "anchor_text": "how to do this using RLlib", "paragraph_index": 2}, {"url": "https://en.wikipedia.org/wiki/Knapsack_problem", "anchor_text": "knapsack problem", "paragraph_index": 3}, {"url": "https://www.datahubbs.com/what-is-dynamic-programming/", "anchor_text": "dynamic programming", "paragraph_index": 5}, {"url": "https://github.com/hubbs5/or-gym", "anchor_text": "or-gym", "paragraph_index": 9}, {"url": "https://arxiv.org/abs/2008.06319", "anchor_text": "Here, we built a virtual machine assignment environment for", "paragraph_index": 31}, {"url": "https://arxiv.org/abs/2008.06319", "anchor_text": "or-gym", "paragraph_index": 31}, {"url": "https://bit.ly/2scbU1P", "anchor_text": "https://bit.ly/2scbU1P", "paragraph_index": 33}], "all_paragraphs": ["RL algorithms learn via trial and error. The agent searches the state space early on and takes random actions to learn what leads to a good reward. Pretty straightforward.", "Unfortunately, this isn\u2019t terribly efficient, especially if we already know something about what makes a good vs. bad action in some states. Thankfully, we can use action masking \u2014 a simple technique that sets the probability of bad actions to 0 \u2014 to speed learning and improve our policies.", "We enforce constraints via action masking for a knapsack packing environment and show how to do this using RLlib.", "Let\u2019s use the classic knapsack problem to develop a concrete example.", "The knapsack problem (KP) asks you to pack a knapsack to maximize the value in the bag without overloading it. If you have a collection of items like we have shown below, the optimal packing is going to contain three of the yellow boxes and three of the gray boxes for a total of $36 and 15kg (this is the unbounded knapsack problem because you have no limit on how many boxes you can choose).", "Typically, this problem is solved using dynamic programming or math programming. If we set it up following a math program, we can write out the model as follows:", "In this case, x_i\u200b is can be any value \u22650 and symbolizes the number of items i we place into the knapsack. v_i\u200b and w_i\u200b, are the values and weights of the items respectively.", "In plain language, this small model is saying we want to maximize the value in the knapsack (which we call z). We do this by finding the largest number of items (x_i) and their values (v_i\u200b) without exceeding the weight limit of the knapsack (W). This formulation is known as an Integer Program (IP) because we have integer decision variables (we can\u2019t pack parts of items, just full, integer values) and is solved using a solver like CPLEX, Gurobi, or GLPK (the last one is free and open source).", "Enforcing those constraints are built into the model, but it\u2019s not naturally built into RL. The RL model may need to pack the green, 12kg box a few times before learning that it can\u2019t pack that and the yellow, 4kg box, by getting hit with a large, negative reward a few times. The negative reward for over packing is a \u201csoft constraint\u201d because we aren\u2019t explicitly forbidding the algorithm from making these bad decisions. But, if we use action masking, we can ensure that the model doesn\u2019t make dumb choices, which will also help it learn better policies, faster.", "Let\u2019s put this into action by packing a knapsack using the or-gym library, which contains some classic environments from the operations research field that we can use to train RL agents. If you\u2019re familiar with OpenAI Gym, you\u2019ll use this in the same way. You can install it with pip install or-gym.", "Once that\u2019s installed, import it and build the Knapsack-v0 environment, which is the unbounded knapsack problem we described above.", "The default settings for this environment has 200 different items to choose from and has a maximum weight capacity of 200kg.", "This is fine, but 200 items are a little much to see clearly, so we can pass an env_config dictionary to change some of these parameters to match the example above. Additionally, we can turn action masking on and off by passing mask: True or mask: False to the configuration dictionary.", "Now our environment matches the example above. Let\u2019s look at our state briefly.", "When we set the action mask option to True, we get a dictionary output as the state that contains three entries, action_mask, avail_actions, and state. This is the same format for all environments in the or-gym library. The mask is a binary vector where 1 indicates an action is allowed and 0 indicates it is going to break some constraint. In this case, our only constraint is the weight, so if a given item would push the model over weight, it is going to receive a large, negative penalty.", "The available actions correspond to each of the five items the agent can select for packing. The state is the input that gets passed to the neural network. In this case, we have a vector that has concatenated the item weights and values, and has the current weight tacked on the end (0 when you initialize the environment).", "If we go ahead and select the 12kg item to pack, we should see that action mask update to eliminate packing any other item that puts the model over the weight limit.", "If you look at the action_mask, that's exactly what we see. The environment is returning information that we can use to prevent the agent from selecting either the 12kg or the 4kg item because it will violate our constraint.", "The concept here is really straightforward to apply. After you complete the forward pass through your policy network, you use the mask to update the values for the illegal actions so that they become large, negative numbers. That way, when you pass it through the softmax function, the probabilities associated with these are going to be 0.", "Now, let\u2019s turn to using RLlib to train a model to respect these constraints.", "Action masking in RLlib requires building a custom model that handles the logits directly. For a custom environment with action masking, this isn\u2019t as straightforward as I\u2019d like, so I\u2019ll walk you through it step-by-step.", "There are a lot of pieces we\u2019re going to need to import first. ray and our ray.rllib.agents should be obvious if you're familiar with the library, but we'll also need tune, gym.spaces, ModelCatalog, a Tensorflow or PyTorch model (depending on your preference, for this I'll just stick to TF), and a utility in the or_gym library called create_env that we wrote to make this a bit smoother.", "We need to tell our neural network explicitly how to handle the different values in our state dictionary. For this, we\u2019ll build a custom model based on the TFModelV2 module from RLlib. This will enable us to build a custom model class and add a forward method to the model in order to use it. Within the forward method, we apply the masks as shown below:", "To walk through this, we first initialize the model and pass our true_obs_shape, which is going to match the size of the state. If we stick with our reduced KP, that will be a vector with 11 entries. The other value we need to provide is the action_embed_size, which is going to be the size of our action space (5). From here, the model initializes a FullyConnectedNetwork based on the input values we provided and registers these values.", "The actual masking takes place in the forward method where we unpack the mask, actions, and state from the observation dictionary provided by our environment. The state yields our action embeddings which gets combined with our mask to provide logits with the smallest value we can provide. This will get passed to a softmax output which will reduce the probability of selecting these actions to 0, effectively blocking the agent from ever taking these illegal actions.", "Once we have our model in place, we need to register it with the ModelCatalog so RLlib can use it during training.", "Additionally, we need to register our custom environment to be callable with RLlib. Below, I have a little helper function called register_env which we use to wrap our create_env function and tune's register_env function. Tune needs the base class, not an instance of the environment like we get from or_gym.make(env_name) to work with. So we need to pass this to register_env using a lambda function as shown below.", "Finally, we can initialize ray, and pass the model and setup to our trainer.", "To demonstrate that our constraint works, we can mask a given action by setting one of the values to 0.", "We masked action 0, so we shouldn\u2019t see the agent select 0 at all.", "And there we have it! We\u2019ve successfully restricted our output with a custom model in RLlib to enforce constraints. You can use this same setup with tune as well to constrain the action space and provide parametric actions.", "Masking can work very effectively to free an agent from pernicious local minima. Here, we built a virtual machine assignment environment for or-gym where the model with masking quickly found an excellent policy, while the model without masking got stuck in a local optima. We tried a lot with the reward function first to get it out of that rut, but nothing worked until we applied a mask!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "AI/ML researcher writing about technology, economics, and business. Connect with me: https://bit.ly/2scbU1P"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F5e4bec5e7505&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faction-masking-with-rllib-5e4bec5e7505&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faction-masking-with-rllib-5e4bec5e7505&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faction-masking-with-rllib-5e4bec5e7505&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faction-masking-with-rllib-5e4bec5e7505&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----5e4bec5e7505--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5e4bec5e7505--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://christiandhubbs.medium.com/?source=post_page-----5e4bec5e7505--------------------------------", "anchor_text": ""}, {"url": "https://christiandhubbs.medium.com/?source=post_page-----5e4bec5e7505--------------------------------", "anchor_text": "Christian Hubbs"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8c5ed989fb1b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faction-masking-with-rllib-5e4bec5e7505&user=Christian+Hubbs&userId=8c5ed989fb1b&source=post_page-8c5ed989fb1b----5e4bec5e7505---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5e4bec5e7505&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faction-masking-with-rllib-5e4bec5e7505&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5e4bec5e7505&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faction-masking-with-rllib-5e4bec5e7505&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.datahubbs.com/action-masking-with-rllib/", "anchor_text": "how to do this using RLlib"}, {"url": "https://en.wikipedia.org/wiki/Knapsack_problem", "anchor_text": "knapsack problem"}, {"url": "https://www.datahubbs.com/what-is-dynamic-programming/", "anchor_text": "dynamic programming"}, {"url": "https://github.com/hubbs5/or-gym", "anchor_text": "or-gym"}, {"url": "https://arxiv.org/abs/2008.06319", "anchor_text": "Here, we built a virtual machine assignment environment for"}, {"url": "https://arxiv.org/abs/2008.06319", "anchor_text": "or-gym"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----5e4bec5e7505---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----5e4bec5e7505---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/optimization?source=post_page-----5e4bec5e7505---------------optimization-----------------", "anchor_text": "Optimization"}, {"url": "https://medium.com/tag/operations?source=post_page-----5e4bec5e7505---------------operations-----------------", "anchor_text": "Operations"}, {"url": "https://medium.com/tag/data-science?source=post_page-----5e4bec5e7505---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5e4bec5e7505&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faction-masking-with-rllib-5e4bec5e7505&user=Christian+Hubbs&userId=8c5ed989fb1b&source=-----5e4bec5e7505---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5e4bec5e7505&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faction-masking-with-rllib-5e4bec5e7505&user=Christian+Hubbs&userId=8c5ed989fb1b&source=-----5e4bec5e7505---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5e4bec5e7505&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faction-masking-with-rllib-5e4bec5e7505&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5e4bec5e7505--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F5e4bec5e7505&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faction-masking-with-rllib-5e4bec5e7505&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----5e4bec5e7505---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----5e4bec5e7505--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----5e4bec5e7505--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----5e4bec5e7505--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----5e4bec5e7505--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----5e4bec5e7505--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----5e4bec5e7505--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----5e4bec5e7505--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----5e4bec5e7505--------------------------------", "anchor_text": ""}, {"url": "https://christiandhubbs.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://christiandhubbs.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Christian Hubbs"}, {"url": "https://christiandhubbs.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "657 Followers"}, {"url": "https://bit.ly/2scbU1P", "anchor_text": "https://bit.ly/2scbU1P"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8c5ed989fb1b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faction-masking-with-rllib-5e4bec5e7505&user=Christian+Hubbs&userId=8c5ed989fb1b&source=post_page-8c5ed989fb1b--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fd00e5d4c3897&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faction-masking-with-rllib-5e4bec5e7505&newsletterV3=8c5ed989fb1b&newsletterV3Id=d00e5d4c3897&user=Christian+Hubbs&userId=8c5ed989fb1b&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}