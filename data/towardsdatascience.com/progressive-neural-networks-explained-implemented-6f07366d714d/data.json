{"url": "https://towardsdatascience.com/progressive-neural-networks-explained-implemented-6f07366d714d", "time": 1683008490.926791, "path": "towardsdatascience.com/progressive-neural-networks-explained-implemented-6f07366d714d/", "webpage": {"metadata": {"title": "Progressive Neural Networks: Explained & Implemented | by Maxwell .J. Jacobson | Towards Data Science", "h1": "Progressive Neural Networks: Explained & Implemented", "description": "Learn to create Progressive Neural Networks with PyTorch and the Doric library. Learn to use Prognets for inpainting, colorizing, and noise removal."}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/pdf/1802.05365.pdf", "anchor_text": "ELMo", "paragraph_index": 8}, {"url": "https://arxiv.org/pdf/1810.04805.pdf", "anchor_text": "BERT", "paragraph_index": 8}, {"url": "https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf", "anchor_text": "GPT-2", "paragraph_index": 8}, {"url": "https://arxiv.org/pdf/1512.03385.pdf", "anchor_text": "ResNet-50", "paragraph_index": 8}, {"url": "https://arxiv.org/pdf/1409.4842.pdf", "anchor_text": "GoogLeNet", "paragraph_index": 8}, {"url": "https://arxiv.org/pdf/1409.1556.pdf", "anchor_text": "VGG-19", "paragraph_index": 8}, {"url": "https://arxiv.org/pdf/1606.04671.pdf", "anchor_text": "Progressive Neural Networks", "paragraph_index": 10}, {"url": "https://arxiv.org/pdf/1706.03256.pdf", "anchor_text": "Progressive Neural Networks for Transfer Learning in Emotion Recognition", "paragraph_index": 14}, {"url": "https://arxiv.org/pdf/1610.04286.pdf", "anchor_text": "Sim-to-Real Robot Learning from Pixels with Progressive Nets", "paragraph_index": 14}, {"url": "https://github.com/arcosin/Doric", "anchor_text": "here", "paragraph_index": 18}, {"url": "https://towardsdatascience.com/deep-deterministic-policy-gradients-explained-2d94655a9b7b", "anchor_text": "DDPG", "paragraph_index": 27}, {"url": "https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf", "anchor_text": "Towards Data Science article by Irhum Shafket", "paragraph_index": 39}, {"url": "http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html", "anchor_text": "CelebA dataset", "paragraph_index": 41}, {"url": "https://github.com/arcosin/Doric/tree/master/examples", "anchor_text": "Doric repository in the examples directory", "paragraph_index": 42}], "all_paragraphs": ["Learning something new requires that you build on what you already know. Like how a simple hammer can be used to shape metal into much more dynamic and complex tools, prior knowledge can be the foundation to shape ideas and create new knowledge. Research is simply this idea applied upon itself over and over again.", "Transfer Learning is the concept of applying knowledge learned in one task to another related task. Many neural network transfer learning techniques have been used with success. However, the topology of the network is usually fixed, and the parameters used in the old task are likely to be destructively overwritten. These unwanted qualities are addressed by progressive neural networks, which can expand their topology \u2014 transferring important knowledge with it \u2014 while preserving the original parameters for later use.", "This article will discuss transfer learning, catastrophic forgetting, and progressive neural networks. In it, we will learn to use the Doric library along with PyTorch to create prognets; as well as walking through the code behind it, and examining how it implements the functionality discussed in the progressive neural networks paper. Finally, we will run two prognet experiments using Doric, and analyse their results to better understand the benefits and drawbacks of using progressive neural networks for transfer learning.", "Chess is an interesting and complex game, and like many great games its intricacies can take a lifetime to master \u2014 yet its basic mechanics can be taught and understood in a single afternoon. Every chess novice will eventually reach a situation similar to the one illustrated below, and they will quickly learn the optimal move for White: moving the knight to attack both of Black\u2019s rooks at once.", "This maneuver is called a \u201cfork\u201d, putting Black in a lose-lose scenario where they must trade one of their rooks for White\u2019s knight. The skilled chess player will learn that forcing an opponent into a fork can be extremely advantageous.", "Chess is not the only game where this simple tactic appears. Connect-4 is a game in which two players drop tiles into a 6-by-7 upright grid with the goal of completing a run of 4 tiles before their opponent does. Below is a diagram of a connect-4 board. Red has the opportunity to connect three tiles in such a way that Blue can only block the run on one side, allowing Red to win on the next turn.", "Chess and Connect-4 are very different games, but they each include this same strategy of aggressing from two directions at once, forcing the enemy into a no-win situation. In fact, the fork tactic can be found in a multitude of games. By learning the concept of the fork, and applying that concept through the mechanics of a different game, we are able to transfer knowledge across different domains. Learning chess can teach us something about Connect-4.", "And that, dear readers, is the essence of transfer learning.", "Transfer learning is the process of applying knowledge gained from one task to a separate yet related task. In recent years, transfer learning has been applied heavily in the areas of language and image processing. In natural language processing, pretrained embedding models such as ELMo, BERT, and GPT-2 have succeeded in learning the basic structure of the English language such that it can be applied to numerous tasks. In this case, the model is often frozen and optimization is only performed on the task model connected to the head of the embedder. While this approach is ideal for language tasks, it relies on a consistent input following the structure of the language it was trained on. In image processing, fully training a convolutional neural net to accomplish a complex task has become a rarity. Instead, generalist pretrained networks such as ResNet-50, GoogLeNet, and VGG-19 are retrained on new data. This process is known as fine-tuning. While fine-tuning is an effective way of fitting a network to new data, it is not without its drawbacks.", "Catastrophic forgetting is a deleterious event that can occur within a trained neural net during fine-tuning. It occurs when important parameters within the network are changed to fit the new data, compromising the network\u2019s ability to handle the old data. Ideally, a network would retain its competence on prior tasks by encoding new information within sparsely used parameters or by changing important parameters in a way that generalizes to other tasks instead of entirely switching to them. Using standard gradient-based learning methods, this is not often possible. There have been many proposed methods of combating catastrophic forgetting, but the algorithm discussed here does more than that: it completely removes it while still allowing effective general transfer learning.", "A progressive neural network (prognets) is a neural algorithm developed by Deepmind in their paper Progressive Neural Networks (Rusu et al., 2016). Prognets are a simple, powerful, and creative solution to transfer learning \u2014 to quote the paper abstract, they \u201care immune to forgetting and can leverage prior knowledge via lateral connections to previously learned features\u201d.", "Prognets begin their existence with just a single column neural net which will be trained on an initial task. Every column is made of L blocks, each including a layer of neurons W, a set of lateral neurons for each parent column U, and an activation function f. As the initial column has no incoming lateral connections, it will act identically to a standard neural network. This first column learns task 1, and then its parameters are frozen. For task 2, a new column is generated and lateral connections are added. These lateral connections take as input the outputs of the previous block in all prior columns. The lateral layers are then added to the main layer and finally the activation is applied.", "Below is an illustration of a prognet with K = 3 tasks and L = 3 blocks found in the progressive neural nets paper. Note that any given block is a function of its own parameters, the output of the previous block in its own column, and the output of the previous block in all prior columns.", "Another way of understanding prognets is to think of each block as a black-box of computation that needs to decide which information sources are most effective for carrying out its task. For example, in the figure above, output\u2083 has access to the prior hidden layer h\u2082. However, it also has access to the h\u2082 layer for each its parent columns, and can locally fine-tune their outputs with its lateral parameters. If the output\u2083 block finds that h\u2082\u00b9 has all the information it needs, it can \u201czero-out\u201d the other inputs. If it finds that h\u2082\u00b2 and h\u2082\u00b3 each offer part of the needed information, it can ignore h\u2082\u00b9 and act as a function of h\u2082\u00b2 and h\u2082\u00b3. By establishing these lateral connections, we allow the network to easily transfer information between tasks, while also allowing it to ignore unimportant information.", "The authors tested their algorithm on a set of reinforcement learning environments including pong variants, labyrinth variants, and Atari games. They tested against various baseline models and found that prognets outperformed each baseline. Following publication, several other papers have shown the successful use of prognets across various domains; for example, Progressive Neural Networks for Transfer Learning in Emotion Recognition (Gideon et al., 2017) found that progressive neural nets could be used for emotion detection given a set of building-block tasks. Similarly, Sim-to-Real Robot Learning from Pixels with Progressive Nets (Rusu et al., 2018) showed that progressive neural nets could be used to transfer between imperfect robot simulations and the real hardware.", "While the above progressive neural network publications produce convincing results when testing their algorithm on various reinforcement learning tasks, prognets are no panacea for transfer learning. Chiefly they are parameter-wasteful: each lateralized block requires a full lateral layer for each parent column it draws from. Prognets in their current form are also limited to functioning only on data where each record has a corresponding task label. To allow for freezing, each task must be learned sequentially, and the sequence order can drastically affect the quality of the resultant network. This is usually a minor limitation only, but a limitation nonetheless. Additionally, due to freezing, prognets are not flexible if a task changes. Consequently, this means that columns should be extremely adept in their task before freezing.", "In summary, progressive neural networks are immune to catastrophe forgetting because they freeze each finished column, but they also allow for effective transfer learning through lateral connections to new columns. They trade increased model complexity (that is, more parameters to train in each additional column) and flexibility on changing tasks to accomplish this. Many prior publications and experiments support the credibility of this algorithm as a potent transfer learning platform.", "Doric is the tool I developed to implement and extend progressive neural networks. It is a free and open-source library built atop PyTorch. In the remainder of this article, we will be walking through the code behind Doric, running a simple experiment with it to analyze a prognet as it\u2019s being trained, and finally, putting Doric to the test on a difficult set of image processing tasks.", "Note: the version of Doric shown here is simplified. The full version with example code is here.", "First, our library requires a definition of blocks. Each block must contain an activation function, a module to act as the block\u2019s main layer, and a list of lateral layers to implement the U parameters. Doric uses the following abstract class.", "By extending this class and implementing unimplemented methods, a user can easily create their own ProgBlocks for any use imaginable. Let\u2019s add a few simple ones.", "Now we have blocks for simple dense layers, dense layers with batch normalization, and 2D convolutional layers. Note that within our object, any lists containing PyTorch modules are implemented as nn.ModuleLists. This is because it allows PyTorch to properly register these sub-modules. It is also worth noting that because ProgBlocks are a subclass of nn.Module, they can be run like any other PyTorch modules. However, we did not implement the forward method on them, so we will still need to call runActivation and runBlock.", "Now that we have a block object, we need to organize them into column networks. These column nets need to be added every time a new task is specified. However, as we have a good extensible block structure, we won\u2019t need to rely on the user to implement individual columns \u2014 at least not directly.", "Finally, we need the whole progressive neural network. This object will contain a list of column nets and will run each necessary column to produce a result for the given task.", "There are a few important things to note. Firstly, in addColumn, Doric allows the user to directly pass a column net, or the user may create a column generator by extending from the ProgColumnGenerator class.", "This allows the user to define the architecture of every column within the generateColumn method such that the prognet can add columns without being passed one. The msg argument also allows the user to pass information through the addColumn method to generateColumn.", "The second thing of special importance is the forward method. This is in fact the heart of the progressive neural network algorithm. The prognet runs each column in order until reaching the column associated with the given task. This allows the later columns to access inputs for each lateral.", "On its own, this is enough to create progressive nets for any simple sequential architecture, but it breaks down when we require more complicated architectures. Take for example this diagram of a Q-function critic network I wrote as part of a DDPG agent in a different project:", "The easiest way to design a progressive system for a network like this is to allow the use of multi-channel blocks. These blocks must also allow for a straight pass-through on certain channels that are not lateralized.", "Besides that, there is one more feature of Doric we need to address: inert and lambda blocks. Inert blocks are ones that could be lateralized \u2014 not in the initial column and not in the first row \u2014 but are intentionally left un-lateralized. These allow us to include special operations in our network, or to reduce unnecessary parameters in blocks that aren\u2019t greatly benefited by transfer learning.", "To implement both of these changes, we need only define them and make a few small changes to our column network class.", "And that\u2019s it. As of the time of this article\u2019s publication, that is the (simplified) core functionality of Doric. Using these tools, we can implement most neural net architectures as a prognet.", "For this first Doric example, we will be creating a 2-task prognet. Specifically, we will teach our columns to emulate the xor and nand functions. Recall that the truth tables of xor and nand are as below.", "In this simple experiment, our first column will solve the xor task and our second will leverage the first to solve nand. To be clear, this is not a set of tasks that requires transfer learning. A single column network with this architecture could solve either task without much trouble. The value of this experiment is that it will allow us to analyze these networks in-depth, and see how the lateral connections transfer knowledge. This is the code we will use for the experiment.", "After running this program, we receive these results.", "These logs show that the network learned the xor and nand functions, but we will need to look closer if we want to see how it accomplished this. Let\u2019s start with the xor column. After plotting out all of the parameters on a graph structure, we can build the following diagram.", "All of that is good to know, but should be unsurprising. Let\u2019s see what happens when we add the nand column.", "Now, we enumerate all input combinations.", "Autoencoders are a powerful architecture that can be applied to a multitude of tasks with good results (especially image processing tasks). They are also ideal for demonstrating progressive networks. This is for two reasons:", "Variational autoencoders (VAEs) are a variant of autoencoder in which the network learns the abstraction of the mean and standard deviation of the data instead of the data itself. This gives the decoder section powerful generative abilities! This article is long enough without an in-depth explanation of autoencoders and VAEs, so I recommend reading this Towards Data Science article by Irhum Shafket.", "or this experiment, we will build a progressive variational autoencoder using Doric. This network will be able to carry out four different tasks in the domain of image processing. These operations are listed below.", "We will be using the CelebA dataset to run the experiment, a massive set of celebrity face images. This dataset is suited to the parameters of our experiment due to its size, difficulty, and the ease of noticing inconsistencies in a human face.", "The full code for running this experiment can be found in the Doric repository in the examples directory.", "Below are the results from the three experimental columns of the progressive VAE. All are of the format original / input / output. Column 0 (simple reconstruction) is not shown, as it contains no laterals.", "To begin our analysis of the results, I think it is important to first address the weirdness of the autoencoder \u2014 while some images are translated very well, others are not. However, when the VAE fails, it rarely does so by creating a misaligned or unrealistic human face. Instead, it over-generalizes and creates a new human face that is more similar to ones it has seen before. These sorts of errors, and other strange ones, are not due to the prognet and will occur all the same in a standalone VAE.", "That being said, we can begin to analyse the results in relation to the prognet. Starting with the denoising column, we see what we would expect from any normal VAE denoiser. The network did a good job removing the noise and maintaining the quality of the image (the fuzziness is to be expected as the bottleneck is only of size 128). What is notable though is training time; if you run the code, you will likely find that the network learns denoising much faster as a prognet than as a standalone network.", "In the colorizing column, we can see that the results are, once again, about what we would expect. The more notable task is the inpainting performed by column three. With the help of the laterals transferring knowledge from the prior columns, we are able to get spectacular results. With even small parts of the image exposed through a mask corrupted pixels, the network can recreate the image almost as well as with a simple reconstruction.", "Generating quality result images is a good sign for the prognet, but to understand the impact of the lateral parameters, we need to find some way of testing how they affect the result. One way to do this is to run a forward pass with the lateral parameters removed. If the result is still recognizable, the laterals are not doing much. If the result is bright but unrecognizable, the laterals are working together with the main network parameters to create the result. If the result is dark or completely black, the column is only or mostly using the laterals for computation. Here are the results of that forward pass.", "These results show that in columns 1 and 2, most of the computation is being done by the laterals. In the final column, almost all of the work is being done by the laterals. This is a strong indicator that transfer learning is taking place.", "Progressive neural networks are a powerful tool in transfer learning and continual learning. While they are limited by their wasteful memory usage, their need for labeled and well-defined tasks, and their inability to adapt if a previously learned task changes, they are completely immune to catastrophic forgetting, and they offer a new template for width-wise growth of continuously-learning neural systems. The experiments run in the progressive neural net paper, along with those presented by other papers and those within this article, show strong evidence that prognets are a viable tool for transfer learning.", "Likewise, Doric is a useful lightweight library for the development and experimentation of prognets. With user created blocks and the inclusion of multi-channel blocks and inert blocks, almost any standard architecture can be implemented as a progressive network.", "Thank you to Case Wright for help in developing Doric and writing all of our autoencoder tests, to David Stucker for help editing, to Dr. Gustavo Rodriguez-Rivera for guidance and academic support, and to Purdue University.", "All images and other copyrightable material were created by the author unless otherwise specified.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "I am an artificial intelligence / machine learning researcher at Purdue University."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F6f07366d714d&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprogressive-neural-networks-explained-implemented-6f07366d714d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprogressive-neural-networks-explained-implemented-6f07366d714d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprogressive-neural-networks-explained-implemented-6f07366d714d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprogressive-neural-networks-explained-implemented-6f07366d714d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----6f07366d714d--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----6f07366d714d--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@mjacobson130?source=post_page-----6f07366d714d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@mjacobson130?source=post_page-----6f07366d714d--------------------------------", "anchor_text": "Maxwell .J. Jacobson"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5ebf29b4bc65&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprogressive-neural-networks-explained-implemented-6f07366d714d&user=Maxwell+.J.+Jacobson&userId=5ebf29b4bc65&source=post_page-5ebf29b4bc65----6f07366d714d---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6f07366d714d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprogressive-neural-networks-explained-implemented-6f07366d714d&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6f07366d714d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprogressive-neural-networks-explained-implemented-6f07366d714d&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://arxiv.org/pdf/1802.05365.pdf", "anchor_text": "ELMo"}, {"url": "https://arxiv.org/pdf/1810.04805.pdf", "anchor_text": "BERT"}, {"url": "https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf", "anchor_text": "GPT-2"}, {"url": "https://arxiv.org/pdf/1512.03385.pdf", "anchor_text": "ResNet-50"}, {"url": "https://arxiv.org/pdf/1409.4842.pdf", "anchor_text": "GoogLeNet"}, {"url": "https://arxiv.org/pdf/1409.1556.pdf", "anchor_text": "VGG-19"}, {"url": "https://arxiv.org/pdf/1606.04671.pdf", "anchor_text": "Progressive Neural Networks"}, {"url": "https://arxiv.org/pdf/1606.04671.pdf", "anchor_text": "Progressive Neural Networks"}, {"url": "https://arxiv.org/pdf/1706.03256.pdf", "anchor_text": "Progressive Neural Networks for Transfer Learning in Emotion Recognition"}, {"url": "https://arxiv.org/pdf/1610.04286.pdf", "anchor_text": "Sim-to-Real Robot Learning from Pixels with Progressive Nets"}, {"url": "https://arxiv.org/pdf/1610.04286.pdf", "anchor_text": "Sim-to-robot transfer with a manipulator arm."}, {"url": "https://github.com/arcosin/Doric", "anchor_text": "here"}, {"url": "https://towardsdatascience.com/deep-deterministic-policy-gradients-explained-2d94655a9b7b", "anchor_text": "DDPG"}, {"url": "https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf", "anchor_text": "Towards Data Science article by Irhum Shafket"}, {"url": "http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html", "anchor_text": "CelebA dataset"}, {"url": "https://github.com/arcosin/Doric/tree/master/examples", "anchor_text": "Doric repository in the examples directory"}, {"url": "https://arxiv.org/pdf/1606.04671.pdf", "anchor_text": "https://arxiv.org/pdf/1606.04671.pdf"}, {"url": "https://github.com/arcosin/Doric", "anchor_text": "https://github.com/arcosin/Doric"}, {"url": "http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html", "anchor_text": "http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html"}, {"url": "https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf", "anchor_text": "https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf"}, {"url": "https://arxiv.org/pdf/1706.03256.pdf", "anchor_text": "https://arxiv.org/pdf/1706.03256.pdf"}, {"url": "https://arxiv.org/pdf/1610.04286.pdf", "anchor_text": "https://arxiv.org/pdf/1610.04286.pdf"}, {"url": "http://svg_experimenten.deds.nl/chessboard/chess_diagram_maker.html", "anchor_text": "http://svg_experimenten.deds.nl/chessboard/chess_diagram_maker.html"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----6f07366d714d---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/pytorch?source=post_page-----6f07366d714d---------------pytorch-----------------", "anchor_text": "Pytorch"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----6f07366d714d---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/transfer-learning?source=post_page-----6f07366d714d---------------transfer_learning-----------------", "anchor_text": "Transfer Learning"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----6f07366d714d---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "http://creativecommons.org/licenses/by/4.0/", "anchor_text": "Some rights reserved"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6f07366d714d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprogressive-neural-networks-explained-implemented-6f07366d714d&user=Maxwell+.J.+Jacobson&userId=5ebf29b4bc65&source=-----6f07366d714d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6f07366d714d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprogressive-neural-networks-explained-implemented-6f07366d714d&user=Maxwell+.J.+Jacobson&userId=5ebf29b4bc65&source=-----6f07366d714d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6f07366d714d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprogressive-neural-networks-explained-implemented-6f07366d714d&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----6f07366d714d--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F6f07366d714d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprogressive-neural-networks-explained-implemented-6f07366d714d&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----6f07366d714d---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----6f07366d714d--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----6f07366d714d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----6f07366d714d--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----6f07366d714d--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----6f07366d714d--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----6f07366d714d--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----6f07366d714d--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----6f07366d714d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@mjacobson130?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@mjacobson130?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Maxwell .J. Jacobson"}, {"url": "https://medium.com/@mjacobson130/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "10 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5ebf29b4bc65&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprogressive-neural-networks-explained-implemented-6f07366d714d&user=Maxwell+.J.+Jacobson&userId=5ebf29b4bc65&source=post_page-5ebf29b4bc65--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F5ebf29b4bc65%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprogressive-neural-networks-explained-implemented-6f07366d714d&user=Maxwell+.J.+Jacobson&userId=5ebf29b4bc65&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}