{"url": "https://towardsdatascience.com/how-bert-leverage-attention-mechanism-and-transformer-to-learn-word-contextual-relations-5bbee1b6dbdb", "time": 1682994506.6478531, "path": "towardsdatascience.com/how-bert-leverage-attention-mechanism-and-transformer-to-learn-word-contextual-relations-5bbee1b6dbdb/", "webpage": {"metadata": {"title": "How BERT leverage attention mechanism and transformer to learn word contextual relations | by Edward Ma | Towards Data Science", "h1": "How BERT leverage attention mechanism and transformer to learn word contextual relations", "description": "After ELMo (Embeddings from Language Model) and Open AI GPT (Generative Pre-trained Transformer), a new state-of-the-art NLP paper is released by Google. They call this approach as BERT\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/elmo-helps-to-further-improve-your-word-embeddings-c6ed2c9df95f", "anchor_text": "ELMo", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/combining-supervised-learning-and-unsupervised-learning-to-improve-word-vectors-d4dea84ec36b", "anchor_text": "GPT", "paragraph_index": 0}, {"url": "https://github.com/google-research/bert#sentence-and-sentence-pair-classification-tasks", "anchor_text": "official page", "paragraph_index": 22}, {"url": "https://towardsdatascience.com/elmo-helps-to-further-improve-your-word-embeddings-c6ed2c9df95f", "anchor_text": "ELMo", "paragraph_index": 23}, {"url": "https://github.com/google-research/bert#using-bert-to-extract-fixed-feature-vectors-like-elmo", "anchor_text": "official page", "paragraph_index": 24}, {"url": "http://medium.com/@makcedward/", "anchor_text": "Medium Blog", "paragraph_index": 30}, {"url": "https://www.linkedin.com/in/edwardma1026", "anchor_text": "LinkedIn", "paragraph_index": 30}, {"url": "https://github.com/makcedward", "anchor_text": "Github", "paragraph_index": 30}, {"url": "https://arxiv.org/pdf/1810.04805.pdf", "anchor_text": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "paragraph_index": 31}, {"url": "https://towardsdatascience.com/3-silver-bullets-of-word-embedding-in-nlp-10fa8f50cc5a", "anchor_text": "word2vec, glove and fastText Story (Word Embeddings", "paragraph_index": 32}, {"url": "https://makcedward.github.io/", "anchor_text": "https://makcedward.github.io/", "paragraph_index": 34}], "all_paragraphs": ["After ELMo (Embeddings from Language Model) and Open AI GPT (Generative Pre-trained Transformer), a new state-of-the-art NLP paper is released by Google. They call this approach as BERT (Bidirectional Encoder Representations from Transformers).", "Both Open AI GPT and BERT use transformer architecture to learn the text representations. One of the difference is BERT use bidirectional transformer (both left-to-right and right-to-left direction) rather than dictional transformer (left-to-right direction). On the other hand, both ELMo use bidirectional language model to learn the text representations. However, ELMo use shallow concatenation layer while BERT use deep neural network.", "After reading this post, you will understand:", "BERT use three embeddings to compute the input representations. They are token embeddings, segment embeddings and position embeddings. \u201cCLS\u201d is the reserved token to represent the start of sequence while \u201cSEP\u201d separate segment (or sentence). Those inputs are", "After talking about input representation, I will introduce how BERT is trained. It uses two way to achieve it. First training task is masked language model while the second task is predicting next sentence.", "First pre-training tasks is leveraging masked language model (Masked LM). Rather than traditional directional model, BERT use bidirectional as a pre-training objective. If using traditional approach to train a bidirectional model, each word will able to see \u201citself\u201d indirectly. Therefore, BERT use Masked Language Model (MLM) approach. By masking some tokens randomly, using other token to predicted those masked token to learn the representations. Unlike other approaches, BERT predict masked token rather than entire input.", "So the experiment pick 15% of token randomly to be replaced. However, there are some downsides. First disadvantage is that MASK token (actual token will be replaced by this token) will never seen in fine-tuning stage and actual prediction. Therefore, Devlin et al, the selected token for masking will not alway be masked but", "For example, the original sentence is \u201cI am learning NLP\u201d. Assuming \u201cNLP\u201d is a selected token for masking. Then 80% of time, it will show as \u201cI am learning [MASK] (Scenario A). \u201cI am learning OpenCV\u201d in 10% of time (Scenario B). Rest of 10% of time, it will show as original which is \u201cI am learning NLP\u201d (Scenario C). Although random replacement (Scenario B) occur and may harming the meaning of sentence. But it is only 1.5% (Only mask 15% of token out of entire data set and 10% of this 15%) indeed, authors believe that it will not harm the model.", "Another downside is that only 15% token is masked (predicted) per batch, a longer time will take for training.", "Second pre-training task is going to predict next sentence. This approach overcome the issue of first task as it cannot learn the relationship between sentences. The objective is very simple. Only classifying whether second sentence is next sentence or not. For example,", "Input 1: I am learning NLP.", "Input 2: NLG is part of NLP.", "The expected output is either isNextSentence or notNextSentence.", "When generating training data for this tasks, 50% of \u201cnotNextSentence\u201d data will be randomly selected.", "2 phases training is applied in BERT. Using generic data set to perform first training and fine tuning it by providing domain specific data set.", "As described before, two sentences are selected for \u201cnext sentence prediction\u201d pre-training task. 50% of time that another sentence is pickup randomly and marked as \u201cnotNextSentence\u201d wile 50% of time that another sentence is actual next sentence.", "This step done by Google research team and we can leverage this pre-trained model to further fine tuning model based on own data.", "Only some model hyperparameters are changed such as batch size, learning rate and number of training epochs, most mode hyperparameters are kept as same in pre-training phase. During the experiments, the following range of value work well across tasks:", "Fine-tuning procedure is different and it depends on downstream tasks.", "For [CLS] token, it will be feed as the final hidden state. Label (C)probabilities are computed with a softmax. After that it is fine-tuned to maximize the log-probability of the correct label.", "Final hidden representation of token will be feed into the classification layer. Surrounding words will be be considered on the prediction. In other words, the classification only focus on the token itself and no Conditional Random Field (CRF).", "So far, BERT deliver best result when comparing to other state-of-the-art NLP models.", "Before fine-tuning domain specific dataset, I prefer to reproduce experiment result first. You can visit the official page or following instruction for it", "Other than fine-tuning pre-trained model for specific dataset. We can also extract a fixed vectors for downstream tasks which is easier. It is similar to what ELMo did.", "You can visit the official page or following instruction for it", "It will be useful if we understand more about how we can change the parameters. Here is some useful parameter explanation:", "task_name: Specific what task do you use. Specific tasks processors are ready for use. Possible task_name are \u201ccola\u201d, \u201cmnli\u201d, \u201cmrpc\u201d and \u201cxnli\u201d. You can implement your own data processor by extending DataProcessor class.", "do_train: Include training step. Any one of do_train , do_eval or do_test have to been enabled.", "do_eval: Include evaluation step. Any one of do_train , do_eval or do_test have to been enabled.", "do_test: Include test step. Any one of do_train , do_eval or do_test have to been enabled.", "I am Data Scientist in Bay Area. Focusing on state-of-the-art in Data Science, Artificial Intelligence , especially in NLP and platform related. You can reach me from Medium Blog, LinkedIn or Github.", "Devlin J., Chang M. W., Lee K., Toutanova K., 2018. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "word2vec, glove and fastText Story (Word Embeddings)", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Focus in Natural Language Processing, Data Science Platform Architecture. https://makcedward.github.io/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F5bbee1b6dbdb&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-bert-leverage-attention-mechanism-and-transformer-to-learn-word-contextual-relations-5bbee1b6dbdb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-bert-leverage-attention-mechanism-and-transformer-to-learn-word-contextual-relations-5bbee1b6dbdb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-bert-leverage-attention-mechanism-and-transformer-to-learn-word-contextual-relations-5bbee1b6dbdb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-bert-leverage-attention-mechanism-and-transformer-to-learn-word-contextual-relations-5bbee1b6dbdb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----5bbee1b6dbdb--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5bbee1b6dbdb--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@makcedward?source=post_page-----5bbee1b6dbdb--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@makcedward?source=post_page-----5bbee1b6dbdb--------------------------------", "anchor_text": "Edward Ma"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fba547bff904f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-bert-leverage-attention-mechanism-and-transformer-to-learn-word-contextual-relations-5bbee1b6dbdb&user=Edward+Ma&userId=ba547bff904f&source=post_page-ba547bff904f----5bbee1b6dbdb---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5bbee1b6dbdb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-bert-leverage-attention-mechanism-and-transformer-to-learn-word-contextual-relations-5bbee1b6dbdb&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5bbee1b6dbdb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-bert-leverage-attention-mechanism-and-transformer-to-learn-word-contextual-relations-5bbee1b6dbdb&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@weareambitious?utm_source=medium&utm_medium=referral", "anchor_text": "Ambitious Creative Co. - Rick Barrett"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://towardsdatascience.com/elmo-helps-to-further-improve-your-word-embeddings-c6ed2c9df95f", "anchor_text": "ELMo"}, {"url": "https://towardsdatascience.com/combining-supervised-learning-and-unsupervised-learning-to-improve-word-vectors-d4dea84ec36b", "anchor_text": "GPT"}, {"url": "https://towardsdatascience.com/3-silver-bullets-of-word-embedding-in-nlp-10fa8f50cc5a", "anchor_text": "story"}, {"url": "https://towardsdatascience.com/transforming-text-to-sentence-embeddings-layer-via-some-thoughts-b77bed60822c", "anchor_text": "story"}, {"url": "https://unsplash.com/@_louisreed?utm_source=medium&utm_medium=referral", "anchor_text": "Louis Reed"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://github.com/google-research/bert#sentence-and-sentence-pair-classification-tasks", "anchor_text": "official page"}, {"url": "https://gist.github.com/W4ngatang/60c2bdb54d156a41194446737ce03e2e", "anchor_text": "script"}, {"url": "https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip", "anchor_text": "pre-trained"}, {"url": "https://towardsdatascience.com/elmo-helps-to-further-improve-your-word-embeddings-c6ed2c9df95f", "anchor_text": "ELMo"}, {"url": "https://github.com/google-research/bert#using-bert-to-extract-fixed-feature-vectors-like-elmo", "anchor_text": "official page"}, {"url": "http://medium.com/@makcedward/", "anchor_text": "Medium Blog"}, {"url": "https://www.linkedin.com/in/edwardma1026", "anchor_text": "LinkedIn"}, {"url": "https://github.com/makcedward", "anchor_text": "Github"}, {"url": "https://arxiv.org/pdf/1810.04805.pdf", "anchor_text": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"url": "https://github.com/google-research/bert", "anchor_text": "BERT in Tensorflow"}, {"url": "https://github.com/huggingface/pytorch-pretrained-BERT", "anchor_text": "BERT in PyTorch"}, {"url": "https://github.com/soskek/bert-chainer", "anchor_text": "BERT in chainer"}, {"url": "https://towardsdatascience.com/3-silver-bullets-of-word-embedding-in-nlp-10fa8f50cc5a", "anchor_text": "word2vec, glove and fastText Story (Word Embeddings"}, {"url": "https://towardsdatascience.com/transforming-text-to-sentence-embeddings-layer-via-some-thoughts-b77bed60822c", "anchor_text": "Skip-Thoughts Story (Sentence Embeddings)"}, {"url": "https://towardsdatascience.com/elmo-helps-to-further-improve-your-word-embeddings-c6ed2c9df95f", "anchor_text": "ELMo Story"}, {"url": "https://towardsdatascience.com/named-entity-recognition-3fad3f53c91e", "anchor_text": "NER Story"}, {"url": "http://peterbloem.nl/blog/transformers", "anchor_text": "Detail Explanation of Transformer"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----5bbee1b6dbdb---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/python?source=post_page-----5bbee1b6dbdb---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/nlp?source=post_page-----5bbee1b6dbdb---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/naturallanguageprocessing?source=post_page-----5bbee1b6dbdb---------------naturallanguageprocessing-----------------", "anchor_text": "Naturallanguageprocessing"}, {"url": "https://medium.com/tag/data-science?source=post_page-----5bbee1b6dbdb---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5bbee1b6dbdb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-bert-leverage-attention-mechanism-and-transformer-to-learn-word-contextual-relations-5bbee1b6dbdb&user=Edward+Ma&userId=ba547bff904f&source=-----5bbee1b6dbdb---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5bbee1b6dbdb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-bert-leverage-attention-mechanism-and-transformer-to-learn-word-contextual-relations-5bbee1b6dbdb&user=Edward+Ma&userId=ba547bff904f&source=-----5bbee1b6dbdb---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5bbee1b6dbdb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-bert-leverage-attention-mechanism-and-transformer-to-learn-word-contextual-relations-5bbee1b6dbdb&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5bbee1b6dbdb--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F5bbee1b6dbdb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-bert-leverage-attention-mechanism-and-transformer-to-learn-word-contextual-relations-5bbee1b6dbdb&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----5bbee1b6dbdb---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----5bbee1b6dbdb--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----5bbee1b6dbdb--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----5bbee1b6dbdb--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----5bbee1b6dbdb--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----5bbee1b6dbdb--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----5bbee1b6dbdb--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----5bbee1b6dbdb--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----5bbee1b6dbdb--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@makcedward?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@makcedward?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Edward Ma"}, {"url": "https://medium.com/@makcedward/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "3.3K Followers"}, {"url": "https://makcedward.github.io/", "anchor_text": "https://makcedward.github.io/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fba547bff904f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-bert-leverage-attention-mechanism-and-transformer-to-learn-word-contextual-relations-5bbee1b6dbdb&user=Edward+Ma&userId=ba547bff904f&source=post_page-ba547bff904f--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fde3db5912a7c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-bert-leverage-attention-mechanism-and-transformer-to-learn-word-contextual-relations-5bbee1b6dbdb&newsletterV3=ba547bff904f&newsletterV3Id=de3db5912a7c&user=Edward+Ma&userId=ba547bff904f&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}