{"url": "https://towardsdatascience.com/how-to-develop-a-credit-risk-model-and-scorecard-91335fc01f03", "time": 1683012371.039535, "path": "towardsdatascience.com/how-to-develop-a-credit-risk-model-and-scorecard-91335fc01f03/", "webpage": {"metadata": {"title": "How to Develop a Credit Risk Model and Scorecard | Towards Data Science", "h1": "How to Develop a Credit Risk Model and Scorecard", "description": "A detailed walkthrough of statistical credit risk modeling, probability of default prediction, and credit scorecard development with Python"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/swlh/how-to-quantify-risk-and-creditworthiness-c76725bc2380", "anchor_text": "my previous article", "paragraph_index": 0}, {"url": "https://drive.google.com/file/d/1xaF743cmUgI5kc76I86AeZDE84SMkMnt/view?usp=sharing", "anchor_text": "dataset", "paragraph_index": 4}, {"url": "https://www.lendingclub.com/", "anchor_text": "Lending Club", "paragraph_index": 4}, {"url": "https://github.com/finlytics-hub/credit_risk_model/blob/master/Data%20Dictionary.xlsx", "anchor_text": "data dictionary", "paragraph_index": 4}, {"url": "https://towardsdatascience.com/how-to-avoid-potential-machine-learning-pitfalls-a08781f3518e", "anchor_text": "my previous article", "paragraph_index": 13}, {"url": "https://towardsdatascience.com/how-to-find-the-best-predictors-for-ml-algorithms-4b28a71a8a80", "anchor_text": "my previous article", "paragraph_index": 17}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html", "anchor_text": "BaseEstimator", "paragraph_index": 45}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.base.TransformerMixin.html", "anchor_text": "TransformerMixin", "paragraph_index": 45}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html", "anchor_text": "Pipeline", "paragraph_index": 45}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RepeatedStratifiedKFold.html", "anchor_text": "RepeatedStratifiedKFold", "paragraph_index": 48}, {"url": "https://towardsdatascience.com/how-to-effectively-predict-imbalanced-classes-in-python-e8cd3b5720c4", "anchor_text": "my previous article", "paragraph_index": 48}, {"url": "https://en.wikipedia.org/wiki/Youden%27s_J_statistic", "anchor_text": "Youden\u2019s J statistic", "paragraph_index": 64}, {"url": "https://github.com/finlytics-hub/credit_risk_model/blob/master/Credit_Risk_Model_and_Credit_Scorecard.ipynb", "anchor_text": "here", "paragraph_index": 68}, {"url": "https://www.linkedin.com/in/asadmumtaz/", "anchor_text": "me", "paragraph_index": 69}, {"url": "https://www.listendata.com/2015/03/weight-of-evidence-woe-and-information.html", "anchor_text": "Weight of Evidence and Information Value Explained", "paragraph_index": 70}, {"url": "http://www.finltyicshub.com", "anchor_text": "www.finltyicshub.com", "paragraph_index": 75}], "all_paragraphs": ["We are all aware of, and keep track of, our credit scores, don\u2019t we? That all-important number that has been around since the 1950s and determines our creditworthiness. I suppose we all also have a basic intuition of how a credit score is calculated, or which factors affect it. Refer to my previous article for some further details on what a credit score is.", "In this article, we will go through detailed steps to develop a data-driven credit risk model in Python to predict the probabilities of default (PD) and assign credit scores to existing or potential borrowers. We will determine credit scores using a highly interpretable, easy to understand and implement scorecard that makes calculating the credit score a breeze.", "I will assume a working Python knowledge and a basic understanding of certain statistical and credit risk concepts while working through this case study.", "We have a lot to cover, so let\u2019s get started.", "We will use a dataset made available on Kaggle that relates to consumer loans issued by the Lending Club, a US P2P lender. The raw data includes information on over 450,000 consumer loans issued between 2007 and 2014 with almost 75 features, including the current loan status and various attributes related to both borrowers and their payment behavior. Refer to the data dictionary for further details on each column.", "The concepts and overall methodology, as explained here, are also applicable to a corporate loan portfolio.", "Initial data exploration reveals the following:", "We will drop all the above features.", "Based on the data exploration, our target variable appears to be loan_status. A quick look at its unique values and their proportion thereof confirms the same.", "Based on domain knowledge, we will classify loans with the following loan_status values as being in default (or 0):", "All the other values will be classified as good (or 1).", "Let us now split our data into the following sets: training (80%) and test (20%). We will perform Repeated Stratified k Fold testing on the training test to preliminary evaluate our model while the test set will remain untouched till final model evaluation. This approach follows the best model evaluation practice.", "Image 1 above shows us that our data, as expected, is heavily skewed towards good loans. Accordingly, in addition to random shuffled sampling, we will also stratify the train/test split so that the distribution of good and bad loans in the test set is the same as that in the pre-split data. This is achieved through the train_test_split function\u2019s stratify parameter.", "Splitting our data before any data cleaning or missing value imputation prevents any data leakage from the test set to the training set and results in more accurate model evaluation. Refer to my previous article for further details.", "A code snippet for the work performed so far follows:", "Next comes some necessary data cleaning tasks as follows:", "We will define helper functions for each of the above tasks and apply them to the training dataset. Having these helper functions will assist us with performing these same tasks again on the test dataset without repeating our code.", "Next up, we will perform feature selection to identify the most suitable features for our binary classification problem using the Chi-squared test for categorical features and ANOVA F-statistic for numerical features. Refer to my previous article for further details on these feature selection techniques and why different techniques are applied to categorical and numerical variables.", "The p-values, in ascending order, from our Chi-squared test on the categorical features are as below:", "For the sake of simplicity, we will only retain the top four features and drop the rest.", "The ANOVA F-statistic for 34 numeric features shows a wide range of F values, from 23,513 to 0.39. We will keep the top 20 features and potentially come back to select more in case our model evaluation results are not reasonable enough.", "Next, we will calculate the pair-wise correlations of the selected top 20 numerical features to detect any potentially multicollinear variables. A heat-map of these pair-wise correlations identifies two features (out_prncp_inv and total_pymnt_inv) as highly correlated. Therefore, we will drop them also for our model.", "Next, we will simply save all the features to be dropped in a list and define a function to drop them. The code for these feature selection techniques follows:", "Next, we will create dummy variables of the four final categorical variables and update the test dataset through all the functions applied so far to the training dataset.", "Note a couple of points regarding the way we create dummy variables:", "Next up, we will update the test dataset by passing it through all the functions defined so far. Pay special attention to reindexing the updated test dataset after creating dummy variables. Let me explain this by a practical example.", "Consider a categorical feature called grade with the following unique values in the pre-split data: A, B, C, and D. Suppose that the proportion of D is very low, and due to the random nature of train/test split, none of the observations with D in the grade category is selected in the test set. Therefore, grade\u2019s dummy variables in the training data will be grade:A, grade:B, grade:C, and grade:D, but grade:D will not be created as a dummy variable in the test set. We will be unable to apply a fitted model on the test set to make predictions, given the absence of a feature expected to be present by the model. Therefore, we reindex the test set to ensure that it has the same columns as the training data, with any missing columns being added with 0 values. A 0 value is pretty intuitive since that category will never be observed in any of the test samples.", "Creating new categorical features for all numerical and categorical variables based on WoE is one of the most critical steps before developing a credit risk model, and also quite time-consuming. There are specific custom Python packages and functions available on GitHub and elsewhere to perform this exercise. However, I prefer to do it manually as it allows me a bit more flexibility and control over the process.", "Weight of Evidence (WoE) and Information Value (IV) are used for feature engineering and selection and are extensively used in the credit scoring domain.", "WoE is a measure of the predictive power of an independent variable in relation to the target variable. It measures the extent a specific feature can differentiate between target classes, in our case: good and bad customers.", "IV assists with ranking our features based on their relative importance.", "According to Baesens et al.\u00b9 and Siddiqi\u00b2, WOE and IV analyses enable one to:", "The formula to calculate WoE is as follow:", "A positive WoE means that the proportion of good customers is more than that of bad customers and vice versa for a negative WoE value.", "Rules related to combining WoE bins", "The above rules are generally accepted and well documented in academic literature\u00b3.", "Discretization, or binning, of numerical features, is generally not recommended for machine learning algorithms as it often results in loss of data. However, our end objective here is to create a scorecard based on the credit scoring model eventually. A scorecard is utilized by classifying a new untrained observation (e.g., that from the test dataset) as per the scorecard criteria.", "Consider that we don\u2019t bin continuous variables, then we will have only one category for income with a corresponding coefficient/weight, and all future potential borrowers would be given the same score in this category, irrespective of their income. If, however, we discretize the income category into discrete classes (each with different WoE) resulting in multiple categories, then the potential new borrowers would be classified into one of the income categories according to their income and would be scored accordingly.", "WoE binning of continuous variables is an established industry practice that has been in place since FICO first developed a commercial scorecard in the 1960s, and there is substantial literature out there to support it. Some of the other rationales to discretize continuous features from the literature are:", "According to Siddiqi\u00b2, by convention, the values of IV in credit scoring is interpreted as follows:", "Note that IV is only useful as a feature selection and importance technique when using a binary logistic regression model.", "Enough with the theory, let\u2019s now calculate WoE and IV for our training data and perform the required feature engineering. We will define three functions as follows, each one to:", "Sample output of these two functions when applied to a categorical feature, grade, is shown below:", "Once we have calculated and visualized WoE and IV values, next comes the most tedious task to select which bins to combine and whether to drop any feature given its IV. The shortlisted features that we are left with until this point will be treated in one of the following ways:", "Note that for certain numerical features with outliers, we will calculate and plot WoE after excluding them that will be assigned to a separate category of their own.", "Once we have explored our features and identified the categories to be created, we will define a custom \u2018transformer\u2019 class using sci-kit learn\u2019s BaseEstimator and TransformerMixin classes. Like other sci-kit learn\u2019s ML models, this class can be fit on a dataset to transform it as per our requirements. Another significant advantage of this class is that it can be used as part of a sci-kit learn\u2019s Pipeline to evaluate our training data using Repeated Stratified k-Fold Cross-Validation. Using a Pipeline in this structured way will allow us to perform cross-validation without any potential data leakage between the training and test folds.", "Remember that we have been using all the dummy variables so far, so we will also drop one dummy variable for each category using our custom class to avoid multicollinearity.", "The code for our three functions and the transformer class related to WoE and IV follows:", "Finally, we come to the stage where some actual machine learning is involved. We will fit a logistic regression model on our training set and evaluate it using RepeatedStratifiedKFold. Note that we have defined the class_weight parameter of the LogisticRegression class to be balanced. This will force the logistic regression model to learn the model coefficients using cost-sensitive learning, i.e., penalize false negatives more than false positives during model training. Cost-sensitive learning is useful for imbalanced datasets, which is usually the case in credit scoring. Refer to my previous article for further details on imbalanced classification problems.", "Our evaluation metric will be Area Under the Receiver Operating Characteristic Curve (AUROC), a widely used and accepted metric for credit scoring. RepeatedStratifiedKFold will split the data while preserving the class imbalance and perform k-fold validation multiple times.", "After performing k-folds validation on our training set and being satisfied with AUROC, we will fit the pipeline on the entire training set and create a summary table with feature names and the coefficients returned from the model.", "It all comes down to this: apply our trained logistic regression model to predict the probability of default on the test set, which has not been used so far (other than for the generic data cleaning and feature selection tasks). We will save the predicted probabilities of default in a separate dataframe together with the actual classes.", "Next, we will draw a ROC curve, PR curve, and calculate AUROC and Gini. Our AUROC on test set comes out to 0.866 with a Gini of 0.732, both being considered as quite acceptable evaluation scores. Our ROC and PR curves will be something like this:", "Code for predictions and model evaluation on the test set is:", "The final piece of our puzzle is creating a simple, easy-to-use, and implement credit risk scorecard that can be used by any layperson to calculate an individual\u2019s credit score given certain required information about him and his credit history.", "Remember the summary table created during the model training phase? We will append all the reference categories that we left out from our model to it, with a coefficient value of 0, together with another column for the original feature name (e.g., grade to represent grade:A, grade:B, etc.).", "We will then determine the minimum and maximum scores that our scorecard should spit out. As a starting point, we will use the same range of scores used by FICO: from 300 to 850.", "The coefficients returned by the logistic regression model for each feature category are then scaled to our range of credit scores through simple arithmetic. An additional step here is to update the model intercept\u2019s credit score through further scaling that will then be used as the starting point of each scoring calculation.", "At this stage, our scorecard will look like this (the Score-Preliminary column is a simple rounding of the calculated scores):", "Depending on your circumstances, you may have to manually adjust the Score for a random category to ensure that the minimum and maximum possible scores for any given situation remains 300 and 850. Some trial and error will be involved here.", "Once we have our final scorecard, we are ready to calculate credit scores for all the observations in our test set. Remember, our training and test sets are a simple collection of dummy variables with 1s and 0s representing whether an observation belongs to a specific dummy variable. For example, in the image below, observation 395346 had a C grade, owns its own home, and its verification status was Source Verified.", "Accordingly, after making certain adjustments to our test set, the credit scores are calculated as a simple matrix dot multiplication between the test set and the final score for each category. Consider the above observations together with the following final scores for the intercept and grade categories from our scorecard:", "Intuitively, observation 395346 will start with the intercept score of 598 and receive 15 additional points for being in the grade:C category. Similarly, observation 3766583 will be assigned a score of 598 plus 24 for being in the grade:A category. We will automate these calculations across all feature categories using matrix dot multiplication. The final credit score is then a simple sum of individual scores of each feature category applicable for an observation.", "So how do we determine which loans should we approve and reject? What is the ideal credit score cut-off point, i.e., potential borrowers with a credit score higher than this cut-off point will be accepted and those less than it will be rejected? This cut-off point should also strike a fine balance between the expected loan approval and rejection rates.", "To find this cut-off, we need to go back to the probability thresholds from the ROC curve. Remember that a ROC curve plots FPR and TPR for all probability thresholds between 0 and 1. Since we aim to minimize FPR while maximizing TPR, the top left corner probability threshold of the curve is what we are looking for. This ideal threshold is calculated using the Youden\u2019s J statistic that is a simple difference between TPR and FPR.", "The ideal probability threshold in our case comes out to be 0.187. All observations with a predicted probability higher than this should be classified as in Default and vice versa. At first, this ideal threshold appears to be counterintuitive compared to a more intuitive probability threshold of 0.5. But remember that we used the class_weight parameter when fitting the logistic regression model that would have penalized false negatives more than false positives.", "We then calculate the scaled score at this threshold point. As shown in the code example below, we can also calculate the credit scores and expected approval and rejection rates at each threshold from the ROC curve. This can help the business to further manually tweak the score cut-off based on their requirements.", "All the code related to scorecard development is below:", "Well, there you have it \u2014 a complete working PD model and credit scorecard! The complete notebook is available here on GitHub. Feel free to play around with it or comment in case of any clarifications required or other queries.", "As always, feel free to reach out to me if you would like to discuss anything related to data analytics, machine learning, financial analysis, or financial analytics.", "Weight of Evidence and Information Value Explained", "[1] Baesens, B., Roesch, D., & Scheule, H. (2016). Credit risk analytics: Measurement techniques, applications, and examples in SAS. John Wiley & Sons.", "[2] Siddiqi, N. (2012). Credit risk scorecards: developing and implementing intelligent credit scoring. John Wiley & Sons.", "[5] Mironchyk, P. & Tchistiakov, V. (2017). Monotone optimal binning algorithm for credit risk modeling.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "A finance professional by education with a keen interest in data analytics and machine learning. www.finltyicshub.com"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F91335fc01f03&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-develop-a-credit-risk-model-and-scorecard-91335fc01f03&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-develop-a-credit-risk-model-and-scorecard-91335fc01f03&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-develop-a-credit-risk-model-and-scorecard-91335fc01f03&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-develop-a-credit-risk-model-and-scorecard-91335fc01f03&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----91335fc01f03--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----91335fc01f03--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@asad_mumtaz?source=post_page-----91335fc01f03--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@asad_mumtaz?source=post_page-----91335fc01f03--------------------------------", "anchor_text": "Asad Mumtaz"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1f9c6741ffa5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-develop-a-credit-risk-model-and-scorecard-91335fc01f03&user=Asad+Mumtaz&userId=1f9c6741ffa5&source=post_page-1f9c6741ffa5----91335fc01f03---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F91335fc01f03&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-develop-a-credit-risk-model-and-scorecard-91335fc01f03&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F91335fc01f03&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-develop-a-credit-risk-model-and-scorecard-91335fc01f03&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.pexels.com/@lum3n-44775?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels", "anchor_text": "Lum3n"}, {"url": "https://www.pexels.com/photo/art-artistic-blank-book-bindings-616849/?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels", "anchor_text": "Pexels"}, {"url": "https://medium.com/swlh/how-to-quantify-risk-and-creditworthiness-c76725bc2380", "anchor_text": "my previous article"}, {"url": "https://drive.google.com/file/d/1xaF743cmUgI5kc76I86AeZDE84SMkMnt/view?usp=sharing", "anchor_text": "dataset"}, {"url": "https://www.lendingclub.com/", "anchor_text": "Lending Club"}, {"url": "https://github.com/finlytics-hub/credit_risk_model/blob/master/Data%20Dictionary.xlsx", "anchor_text": "data dictionary"}, {"url": "https://towardsdatascience.com/how-to-avoid-potential-machine-learning-pitfalls-a08781f3518e", "anchor_text": "my previous article"}, {"url": "https://towardsdatascience.com/how-to-find-the-best-predictors-for-ml-algorithms-4b28a71a8a80", "anchor_text": "my previous article"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html", "anchor_text": "BaseEstimator"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.base.TransformerMixin.html", "anchor_text": "TransformerMixin"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html", "anchor_text": "Pipeline"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RepeatedStratifiedKFold.html", "anchor_text": "RepeatedStratifiedKFold"}, {"url": "https://towardsdatascience.com/how-to-effectively-predict-imbalanced-classes-in-python-e8cd3b5720c4", "anchor_text": "my previous article"}, {"url": "https://en.wikipedia.org/wiki/Youden%27s_J_statistic", "anchor_text": "Youden\u2019s J statistic"}, {"url": "https://github.com/finlytics-hub/credit_risk_model/blob/master/Credit_Risk_Model_and_Credit_Scorecard.ipynb", "anchor_text": "here"}, {"url": "https://www.linkedin.com/in/asadmumtaz/", "anchor_text": "me"}, {"url": "https://www.listendata.com/2015/03/weight-of-evidence-woe-and-information.html", "anchor_text": "Weight of Evidence and Information Value Explained"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----91335fc01f03---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----91335fc01f03---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/python?source=post_page-----91335fc01f03---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/programming?source=post_page-----91335fc01f03---------------programming-----------------", "anchor_text": "Programming"}, {"url": "https://medium.com/tag/technology?source=post_page-----91335fc01f03---------------technology-----------------", "anchor_text": "Technology"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F91335fc01f03&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-develop-a-credit-risk-model-and-scorecard-91335fc01f03&user=Asad+Mumtaz&userId=1f9c6741ffa5&source=-----91335fc01f03---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F91335fc01f03&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-develop-a-credit-risk-model-and-scorecard-91335fc01f03&user=Asad+Mumtaz&userId=1f9c6741ffa5&source=-----91335fc01f03---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F91335fc01f03&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-develop-a-credit-risk-model-and-scorecard-91335fc01f03&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----91335fc01f03--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F91335fc01f03&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-develop-a-credit-risk-model-and-scorecard-91335fc01f03&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----91335fc01f03---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----91335fc01f03--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----91335fc01f03--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----91335fc01f03--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----91335fc01f03--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----91335fc01f03--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----91335fc01f03--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----91335fc01f03--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----91335fc01f03--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@asad_mumtaz?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@asad_mumtaz?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Asad Mumtaz"}, {"url": "https://medium.com/@asad_mumtaz/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "457 Followers"}, {"url": "http://www.finltyicshub.com", "anchor_text": "www.finltyicshub.com"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1f9c6741ffa5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-develop-a-credit-risk-model-and-scorecard-91335fc01f03&user=Asad+Mumtaz&userId=1f9c6741ffa5&source=post_page-1f9c6741ffa5--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F83025c62db23&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-develop-a-credit-risk-model-and-scorecard-91335fc01f03&newsletterV3=1f9c6741ffa5&newsletterV3Id=83025c62db23&user=Asad+Mumtaz&userId=1f9c6741ffa5&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}