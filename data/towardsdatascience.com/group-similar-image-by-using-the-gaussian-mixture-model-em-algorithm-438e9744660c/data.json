{"url": "https://towardsdatascience.com/group-similar-image-by-using-the-gaussian-mixture-model-em-algorithm-438e9744660c", "time": 1683003225.417969, "path": "towardsdatascience.com/group-similar-image-by-using-the-gaussian-mixture-model-em-algorithm-438e9744660c/", "webpage": {"metadata": {"title": "Group similar Image by using the Gaussian mixture model (EM algorithm) | by AKASH KUMAR | Towards Data Science", "h1": "Group similar Image by using the Gaussian mixture model (EM algorithm)", "description": "Clustering one of the most popular unsupervised machine learning problem. We are already familiar with k-means clustering algorithm but wait there is a problem in this: If you don\u2019t want to\u2026"}, "outgoing_paragraph_urls": [{"url": "https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.multivariate_normal.html?highlight=multivariate_normal#scipy.stats.multivariate_normal", "anchor_text": "check this", "paragraph_index": 37}, {"url": "https://github.com/Aksh-kumar/PYTHONMLAPI", "anchor_text": "PYTHONAPI", "paragraph_index": 56}, {"url": "https://github.com/Aksh-kumar/ML_AlgorithmUI", "anchor_text": "ML_algorithmUI", "paragraph_index": 56}, {"url": "https://github.com/Aksh-kumar/PYTHONMLAPI/tree/master/ML_algorithms/cluster", "anchor_text": "this one", "paragraph_index": 57}], "all_paragraphs": ["Clustering one of the most popular unsupervised machine learning problem. We are already familiar with k-means clustering algorithm but wait there is a problem in this:", "If you don\u2019t want to understand this nasty looking math then please scroll down to go to Implementation Of GMM section where I talk about implementation in python.", "Now maximum likelihood estimation did not work in the presence of latent variables very well. The expectation-maximization algorithm is one method by which can find appropriate model parameters in the presence of latent variables.", "EM is an iterative method to find maximum likelihood or maximum a posteriori estimates of parameters, where the model depends on unobserved latent variables.", "Let simplify it a little bit.", "and a probabilistic model p(X |\u03b8)", "Our goal is to find the value of \u03b8 which maximizes p(X |\u03b8 this is called maximum likelihood estimation.", "IF the model is simple Gaussian distribution then parameter be like", "If this is the case most common approach to solve this problem in ML is gradient descent by minimizing the log-likelihood function which is \u2013log (p(X |\u03b8)) as loss function (remember we actually maximizing the probability function at parameter", "is a simple hack for numerical stability since it changes the product into sum and the minimization of negative of this function is the same as maximizing this function).", "But Gradient descent is a method for solving nonlinear problems, typically for the minimum of some multi-dimensional distribution. Computing the mean (or other moments) of a Gaussian mixture is not a nonlinear problem and so does not require methods designed for nonlinear problems. So is there is any better way to solve this problem.", "There comes the EM algorithm. Let see how the EM algorithm is used in the Gaussian mixture model.", "Maximum Likelihood Estimation (MLE) can be simplified by introducing the Latent variable. A latent variable model makes the assumption that an observation xi is caused by some underlying latent variable. Hmmm\u2026 still not clear ok consider this image", "This type of probabilistic model is called the Gaussian mixture model (GMM), a weighted sum of C Gaussian components in this example C = 3 is", "\u03c0c and \u03a3c are the weight, mean vector and covariance matrix of mixture component Cc, respectively. The weights are non-negative and sum up to 1 i.e.", "denotes the set of all model parameters. If we introduce a discrete latent variable \u2018t\u2019 that determines the assignment of observations to mixture components we can define a joint distribution over observed and latent variables p(x, t| \u03b8) in terms of a conditional distribution p(x| t, \u03b8) and a prior distribution p( t | \u03b8)", "The values of t are one-hot encoded. For example, t2=1 refers to the second mixture component which means t = (0, 1, 0) if there are C=3 components in total. The marginal distribution p (x | \u03b8) is obtained by summing over all possible states of t.", "For each observation xi we have one latent variable ti, which is also called responsibility.", "Considering X be all sets of observation, set of all latent variable T then we can easily maximize complete data log-likelihood p(X, T | \u03b8). Since after finding log-likelihood we then get to know cluster assignments of points that is why we will find marginal log-likelihood or incomplete data log-likelihood p (X | \u03b8).so mathematically,", "Let see step by step how Our Image gets clustered by using a Gaussian Mixture Model. I am using python here for implementing GMM model:", "Let start from our dataset we have given a folder path where all the image resides. We will create a method from which we can extract the average value of R, G B vector for each Image. For single Image the function should be:", "Similarly, we can do this for all of our images and create a pandas Dataframe which contains Image details:", "Now we have the dataset contain features (i.e. [Red, Green, blue] column of the dataset, not all of the rest of the data use for View only). If you want to go for a higher number of k (number of clusters) you can create your own feature extracted from an image but represent a wider range of color code not just r, g, and b. but for sake of simplicity, I took these 3 primary color vector as features.", "Model Generation (Implementing the EM algorithm for Gaussian mixture models):", "Since in GMM we assume all the data points collectively coming from k Gaussian distribution where k is nothing but a number of clusters. Hmmm\u2026 bit tricky let simplify it a bit.", "Consider this 1D probability distribution. If we assume all the data points coming from 2 Gaussian distribution that means we assume k=2(number of clusters) then if we want to know which data came from which Gaussian distribution, all we need to do is to compute the mean and the variance for both the Gaussian distribution. But wait we don\u2019t know yet. But for a second if we assume we know these parameters (i.e. mean and variance of k Gaussian distribution) then we can know the likelihood of each data point from which Gaussian it is coming (this likelihood refers to the responsibilities). Wow, but we don\u2019t know this either. So our conclusion is", "If we know the parameter (i.e. mean and variance of Gaussian) we can calculate the responsibilities and if we know the responsibility then we can calculate the parameters.", "That\u2019s where EM algorithm comes where it starts by placing \u2018k\u2019 Gaussians randomly (generate random mean and variance for the Gaussians) into the picture where it iterates between E and M step until it converges where:", "E- step: assign cluster responsibilities, given current parameters", "M-step: Update parameters, given current cluster responsibilities", "Let\u2019s talk about its implementation in more detail.", "Let\u2019s remember why GMM is used for. Unlike k- mean it does not do hard assignments to the cluster, say an example if an image", "If we are working with k=4 i.e. 4 clusters model i.e. Sunset, forest, cloud, and sky then In k mean algorithm this picture belongs only to cloud this is called a hard assignment but in GMM it belongs to 97% cloud and 3% forest this is called a soft assignment.", "In this step, we compute cluster responsibilities. Let rik represent responsibility of cluster k for data point i. You can assume responsibility as the probability of that data point \u2018i\u2019 belonging to the \u2018k\u2019 cluster. Since it\u2019s a probability that means", "To know how much a cluster is responsible for a given data point, we compute the likelihood of the data point under the particular cluster assignment, multiplied by the weight of the cluster. For data point i and cluster k, mathematically:", "Where N (xi | \u03bck, \u03a3k) is the Gaussian distribution for cluster k (with mean \u03bck and covariance \u03a3k).", "We used \u221d because the quantity N (xi | \u03bck, \u03a3k) is not yet the responsibility we want. To ensure that all responsibilities over each data point add up to 1, we add the normalization constant in the denominator:", "But our data is 3 dimension vector i.e. [red, green, blue]. Scipy provides a convenient function to calculate multivariate normal distribution. check this", "which is multivariate_normal.pdf([data point], mean=[mean vector], cov = [covariance matrix])", "We will talk about later how to calculate the mean vector and covariance matrix in the M step.", "Remember this responsibility is our latent variable.", "Now cluster responsibility is calculated, we have to update our cluster parameters i.e. (weights (\u03c0k), means (\u03bck), and covariances (\u03a3k ) ) associated with each cluster.", "The cluster weight gives how much each cluster represents all data points. Mathematically it is defined as:", "The mean of each cluster is equal to the weighted average of all data points, weighted by the cluster responsibilities. Mathematically for each data point xi and responsibilities for kth cluster rik is Mean of k-th cluster can be defined as:", "The covariance of each cluster is equal to the weighted average of all outer products weighted by the cluster responsibilities. Mathematically it is defined as:", "is the outer product. Let take a simple example of what outer product looks like.", "be the two vector then the outer product of a and b is defined as:", "Transpose is for just to make the column vector to row vector. Since by convention vector represented as a column vector in machine learning.", "But how we will measure our mixture of Gaussian model. For this, we will calculate the log-likelihood of the Gaussian mixture. It Quantifies the probability of observing a given set of data under particular parameters i.e. means, covariance, and weights. We will keep iterating EM algorithms with different sets of parameters and check for its convergence i.e. log-likelihood doesn\u2019t change too much or till fix number of iterations.", "Now It\u2019s time for the actual main method of Implementation of EM algorithms", "So, all set now but what will be our initial parameter. There are many approaches we can go with.", "One way is to initialize all clusters weights equally i.e. (1/k) for each cluster where k is the total number of clusters.", "Initialization of covariance matrix for 1 cluster is the square diagonal matrix of dimension equal to the dimension of data * dimension of data (in this case dimension of data is 3 i.e. [r, g, b]) mathematically", "Where \u03c3xy = is the covariance between x and y", "For mean initialization, it\u2019s a bit tricky. Remember the k-mean algorithm. We use the output of these algorithms to initialize our EM mean parameter. Since by using the k-mean algorithm we push mean co-ordinate to reach the near point of convergence. So if we start from this point EM algorithm will take less time to converge. But there is a problem with how we will initialize initial centroids for k-mean.", "Hmmm. Well for this problem we will go with the k mean++ initialization strategy in which it selects k centroids farthest from each other. Now we have all the three-parameter to start with.", "NOTE: For Full solution look my git-hub link PYTHONAPI you will find full Flask code for this clustering problem End to End and can be consumed by any client-side technology. I select Angular 8 as client-side which code is available on ML_algorithmUI.", "For those who want to check only EM related code, the link is this one which contains two folders Expectation_Maximization and K_Mean which contain the relative code separately. Please go through the code and feel free to ask me if you face any problems.", "Now coming back to code so, driver code of this program", "Now our data is ready and the value of k i.e. number of the cluster is all personal choice. For simplicity I take k = 4 means we are assuming the distribution of data is coming from 4 Gaussian. Since our data and value of k are ready then we are ready to create a model.", "It will take time don\u2019t worry but in the end, you got what you want. Now if you have new observation data you can find out the soft assignments since we already got the correct cluster parameters.", "create your numpy array of observations and call this method with returned parameter by em as shown in (2) and pass it to this function you will get the soft assignment of each cluster. For example, let you have an image with path \u201c../../test_example.jpg\u201d", "note: remember this function takes an array of observation and in this case, we have only one image i.e. one observation so convert this single observation to an array first then call the function.", "Hurray, you got your cluster assignments. I made a web-based result visualization of some of the screenshots.", "Let\u2019s talk about how it removes all disadvantages of k mean:", "conclusion: So coming to end we can say that any image with the primary set of colors can be cluster with this technique. We can use this approach to cluster Image as many numbers as we want. Here I just took a simple example with primary color but according to need feature generation may be altered with multiple shades of colors. GMM is best suited for those cases where you want the probability of image assignment to a particular cluster.", "Now it\u2019s time to close my blog post If you are really interested in this code please visit my git-hub link and feel free to contribute and give suggestions for improvements. Happy learning\u2026.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Full stack Engineer Experienced in Machine learning and python and AI enthusiast.."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F438e9744660c&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgroup-similar-image-by-using-the-gaussian-mixture-model-em-algorithm-438e9744660c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgroup-similar-image-by-using-the-gaussian-mixture-model-em-algorithm-438e9744660c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgroup-similar-image-by-using-the-gaussian-mixture-model-em-algorithm-438e9744660c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgroup-similar-image-by-using-the-gaussian-mixture-model-em-algorithm-438e9744660c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----438e9744660c--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----438e9744660c--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@resakash1498?source=post_page-----438e9744660c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@resakash1498?source=post_page-----438e9744660c--------------------------------", "anchor_text": "AKASH KUMAR"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F467d3605a139&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgroup-similar-image-by-using-the-gaussian-mixture-model-em-algorithm-438e9744660c&user=AKASH+KUMAR&userId=467d3605a139&source=post_page-467d3605a139----438e9744660c---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F438e9744660c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgroup-similar-image-by-using-the-gaussian-mixture-model-em-algorithm-438e9744660c&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F438e9744660c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgroup-similar-image-by-using-the-gaussian-mixture-model-em-algorithm-438e9744660c&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/photos/L68SOwRShLU", "anchor_text": "Image by Shlomo Shalev"}, {"url": "https://fallfordata.com/soft-clustering-with-gaussian-mixture-models-gmm/", "anchor_text": "Image source"}, {"url": "http://ethen8181.github.io/machine-learning/clustering/GMM/GMM.html", "anchor_text": "Image source"}, {"url": "https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.multivariate_normal.html?highlight=multivariate_normal#scipy.stats.multivariate_normal", "anchor_text": "check this"}, {"url": "https://github.com/Aksh-kumar/PYTHONMLAPI", "anchor_text": "PYTHONAPI"}, {"url": "https://github.com/Aksh-kumar/ML_AlgorithmUI", "anchor_text": "ML_algorithmUI"}, {"url": "https://github.com/Aksh-kumar/PYTHONMLAPI/tree/master/ML_algorithms/cluster", "anchor_text": "this one"}, {"url": "https://krasserm.github.io/2019/11/21/latent-variable-models-part-1/", "anchor_text": "https://krasserm.github.io/2019/11/21/latent-variable-models-part-1/"}, {"url": "http://ethen8181.github.io/machine-learning/clustering/GMM/GMM.html", "anchor_text": "http://ethen8181.github.io/machine-learning/clustering/GMM/GMM.html"}, {"url": "https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm", "anchor_text": "https://en.wikipedia.org/wiki/Expectation\u2013maximization_algorithm"}, {"url": "https://developers.google.com/machine-learning/clustering/algorithm/advantages-disadvantages", "anchor_text": "https://developers.google.com/machine-learning/clustering/algorithm/advantages-disadvantages"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----438e9744660c---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/clustering?source=post_page-----438e9744660c---------------clustering-----------------", "anchor_text": "Clustering"}, {"url": "https://medium.com/tag/expectation-maximization?source=post_page-----438e9744660c---------------expectation_maximization-----------------", "anchor_text": "Expectation Maximization"}, {"url": "https://medium.com/tag/gaussian-mixture-model?source=post_page-----438e9744660c---------------gaussian_mixture_model-----------------", "anchor_text": "Gaussian Mixture Model"}, {"url": "https://medium.com/tag/image-clustering?source=post_page-----438e9744660c---------------image_clustering-----------------", "anchor_text": "Image Clustering"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F438e9744660c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgroup-similar-image-by-using-the-gaussian-mixture-model-em-algorithm-438e9744660c&user=AKASH+KUMAR&userId=467d3605a139&source=-----438e9744660c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F438e9744660c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgroup-similar-image-by-using-the-gaussian-mixture-model-em-algorithm-438e9744660c&user=AKASH+KUMAR&userId=467d3605a139&source=-----438e9744660c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F438e9744660c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgroup-similar-image-by-using-the-gaussian-mixture-model-em-algorithm-438e9744660c&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----438e9744660c--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F438e9744660c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgroup-similar-image-by-using-the-gaussian-mixture-model-em-algorithm-438e9744660c&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----438e9744660c---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----438e9744660c--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----438e9744660c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----438e9744660c--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----438e9744660c--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----438e9744660c--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----438e9744660c--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----438e9744660c--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----438e9744660c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@resakash1498?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@resakash1498?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "AKASH KUMAR"}, {"url": "https://medium.com/@resakash1498/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "37 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F467d3605a139&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgroup-similar-image-by-using-the-gaussian-mixture-model-em-algorithm-438e9744660c&user=AKASH+KUMAR&userId=467d3605a139&source=post_page-467d3605a139--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fbaf82a57cbae&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgroup-similar-image-by-using-the-gaussian-mixture-model-em-algorithm-438e9744660c&newsletterV3=467d3605a139&newsletterV3Id=baf82a57cbae&user=AKASH+KUMAR&userId=467d3605a139&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}