{"url": "https://towardsdatascience.com/ai-feynman-2-0-learning-regression-equations-from-data-3232151bd929", "time": 1683010145.2989829, "path": "towardsdatascience.com/ai-feynman-2-0-learning-regression-equations-from-data-3232151bd929/", "webpage": {"metadata": {"title": "AI Feynman 2.0: Learning Regression Equations From Data | by Daniel Shapiro, PhD | Towards Data Science", "h1": "AI Feynman 2.0: Learning Regression Equations From Data", "description": "1. Introduction 2. Code 3. Their Example 4. Our Own Easy Example 5. Symbolic Regression on Noisy Data I recently saw a post on LinkedIn from MIT professor Max Tegmark about a new ML library his lab\u2026"}, "outgoing_paragraph_urls": [{"url": "https://space.mit.edu/home/tegmark/", "anchor_text": "Max Tegmark", "paragraph_index": 1}, {"url": "https://arxiv.org/abs/2006.10782", "anchor_text": "AI Feynman 2.0: Pareto-optimal symbolic regression exploiting graph modularity", "paragraph_index": 1}, {"url": "https://github.com/SJ001", "anchor_text": "Silviu-Marian Udrescu", "paragraph_index": 1}, {"url": "https://en.wikipedia.org/wiki/Symbolic_regression#:~:text=Symbolic%20Regression%20(SR)%20is%20a,starting%20point%20to%20the%20algorithm.", "anchor_text": "Symbolic regression", "paragraph_index": 2}, {"url": "https://en.wikipedia.org/wiki/Decision_tree_learning#:~:text=Decision%20trees%20where%20the%20target,numbers)%20are%20called%20regression%20trees.&text=In%20decision%20analysis%2C%20a%20decision,represent%20decisions%20and%20decision%20making.", "anchor_text": "regression trees", "paragraph_index": 3}, {"url": "https://umap-learn.readthedocs.io/en/latest/", "anchor_text": "UMAP", "paragraph_index": 4}, {"url": "https://www.wolframalpha.com/input/?i=3.000000000000%2Blog%28sqrt%28exp%28%28x2-x1%29%29%29%29", "anchor_text": "see on WolframAlpha", "paragraph_index": 23}, {"url": "https://github.com/SJ001/AI-Feynman", "anchor_text": "AI-Feynman 2.0", "paragraph_index": 35}, {"url": "https://github.com/dcshapiro/AI-Feynman/blob/master/AI_Feynman_2_0.ipynb", "anchor_text": "HERE", "paragraph_index": 35}, {"url": "https://github.com/SJ001", "anchor_text": "Silviu Marian Udrescu", "paragraph_index": 36}, {"url": "https://www.gravity-ai.com/", "anchor_text": "Gravity-ai.com", "paragraph_index": 36}, {"url": "https://medium.com/towards-data-science/how-to-price-an-ai-project-f7270cb630a4", "anchor_text": "How to Price an AI Project", "paragraph_index": 37}, {"url": "https://medium.com/towards-data-science/why-hire-an-ai-consultant-50e155e17b39", "anchor_text": "How to Hire an AI Consultant", "paragraph_index": 37}, {"url": "http://eepurl.com/gdKMVv", "anchor_text": "join the newsletter", "paragraph_index": 37}, {"url": "https://lemay.ai", "anchor_text": "https://lemay.ai", "paragraph_index": 39}], "all_paragraphs": ["1. Introduction2. Code3. Their Example4. Our Own Easy Example5. Symbolic Regression on Noisy Data", "I recently saw a post on LinkedIn from MIT professor Max Tegmark about a new ML library his lab released. I decided to try it out. The paper is AI Feynman 2.0: Pareto-optimal symbolic regression exploiting graph modularity, submitted June 18th, 2020. The first author is Silviu-Marian Udrescu, who was generous enough to hop on a call with me and explain the backstory of this new machine learning library. The library, called AI Feynman 2.0, helps to fit regression formulas to data. More specifically, it helps to fit formulas to data at different levels of complexity (defined in bits). The user can select the operators that the solver will use from sets of operators, and the solver does its thing. Operators are things like exponentiation, cos, arctan, and so on.", "Symbolic regression is a way of stringing together the user-specified mathematical functions to build an equation for the output \u201cy\u201d that best fits the provided dataset. That provided dataset takes the form of sample points (or observations) for each input variable x0, x1, and so forth, along with the corresponding \u201cy\u201d. Since we don\u2019t want to overfit on the data, we need to limit the allowed complexity of the equation or at least have the ability to solve under a complexity constraint. Unlike a neural network, learning one formula with just a few short expressions in it gives you a highly interpretable model, and can lead to insights that you might not get from a neural network model with millions of weights and biases.", "Why is this interesting? Well, science tends to generate lots of observations (data) that scientists want to generalize into underlying rules. These rules are equations that \u201cfit\u201d the observations. Unlike a \u201cusual\u201d machine learning model, equations of the form y=f(x) are very clear, and they can omit some of the variables in the data that are not needed. In the practicing machine learning engineer\u2019s toolbox, regression trees would be the closest concept I can think of that implements this idea of learning an interpretable model that connects observations to a prediction. Having a new way to try and fit a regression model to data is a good addition to the toolbox of stuff you can try on your dataset.", "In this article, I want to explore this new library as a user (how to use it), rather than a scientist (how does it work). AI-Feynman 2.0 reminds me of UMAP, in that it includes very fancy math on the inside of the solver, but does something useful to me in an abstracted way that I can treat as a black box. I understand that the code is going to be updated in stages over the next several months, and so the way the interface to the code looks today may not be the way it works when you are reading this. Hopefully, more documentation will also be added to give you a quick path to trying this on your data. For the moment, I\u2019m including a notebook with this article so that you can dive in and get everything working from one place.", "The library uses machine learning to help with the equation discovery, breaking the problem into subproblems recursively, but let\u2019s not get too far into the weeds. Let\u2019s instead turn our attention to using the library. You are welcome to read the paper to learn more about how the library does what it does to solve the symbolic regression mystery on your data.", "A Google Collab notebook containing all of the code for this article is available here:", "Some notes on the output are important. The solver prints many times the Complexity, RMSE, and Expression. Be aware that the RMSE number is not actually the Root Mean Squared Error. It is the Mean Error Description Length (MEDL) described in the paper, and that message will be changed soon. Also, the Expression printout is not the equation for the dataset, but rather for the sub-problem within the overall problem graph that the solver is currently working on. This is important because you will find that sometimes there is a printout that seems like it has a very low error, but it only applies to some subproblem and is not the equation you are trying to find. The final results are stored in the results folder using the name of the input file.", "Clone the repository and install the dependencies. Next, compile the Fortran code and run the first example dataset from the AI-Feynman repository (example1.txt from the repository).", "The first few steps are listed here:", "Next, put this file into the Code directory and run it with python3:", "The first line of the example1.txt file is:", "Example 1 contains data generated from an equation, where the last column is the regression target, and the rest of the columns are the input data. The following example shows the relationship between the first line of the file example1.txt and the formula used to make the data.", "We can see from running the code snippet above that the target \u201cy\u201d data points in example1.txt are generated using the equation on line 3, where the inputs are all the columns except for the last one, and the equation generates the last column.", "Let\u2019s now run the program. In the folder AI-Feynman/Code/ run the command python3 ai_feynman_magic.py to run the program we wrote above which in turn fits equations to the example1.txt dataset.", "The solver runs for a long time, trying different kinds of equations at different levels of complexity, and assessing the best fit for each one. As it works through the solution, it prints intermediate results. If it hits a super low error you can stop the program and just use the equation. It\u2019s really your call if you let it run to the end. For the input file example1.txt, the results show up in AI-Feynman/Code/results/solution_example1.txt. There are other spots where results are generated, but this is the place we care about right now. That file \u201csolution_\u2026txt\u201d ranks the identified solutions. It\u2019s funny that assuming y is a constant is a common strategy for the solver. Constants have no input variables, and so they have low complexity in terms of the number of bits. In the case of example 1, the equation ((x0-x1)**2 + (x2-x3)**2)**0.5 fit the best.", "In the Collab notebook, I now moved the repository and data to Google Drive so that it will persist. The following code generates 10,000 examples from an equation. This example has 2 \u201cx\u201d variables and 2 duplicated \u201cx\u201d variables. Of course, y is still the output.", "Plotting the first variable against Y we get:", "Now that we took a peek at what our data looks like, let\u2019s ask the solver to find a simple equation that fits our data, using our dataset. The idea is that we want the solver to notice that you don\u2019t need all of the supplied variables in order to fit the data.", "Here is an example of a permissions problem:", "If you have file permission issues when you try to run the code, open up the file permissions like this:", "Below is the command to run the solver. Go get coffee, because this is not going to be fast\u2026", "If you have nothing better to do, watch the solver go. Notice the solver goes through a list of equation types before mixing it up. The initial models it tries out are quickly mapped to x0 and x2 as it \u201crealized\u201d x1 and x3 are duplicates and so not needed. Later on, the solver found the equation 3.000000000000+log(sqrt(exp((x2-x1)))) which is a bit crazy but looks like a plane.", "We can see on WolframAlpha that an equivalent form of this equation is:", "which is what we used to generate the dataset!", "The solver settled on y=log(sqrt(exp(-x1 + x3))) + 3.0 which we know is a correct description of our plane, from the wolfram alpha thing above. The solver ended up using x1 and x3, dropping x0 because it is a copy of x1 and so not needed, and similarly dropping x2 because it is not needed when using x3.", "Now, that worked, but it was a bit of a softball problem. The data has an exact solution, and so it didn\u2019t need to fit noisy data, which is not a realistic real-world situation. Real data is messy. Let\u2019s now add noise to the dataset and see how the library holds up. We don\u2019t need to go as far as introducing missing variables and imputation. Let\u2019s just make the problem a tiny bit harder to mess with the solver.", "The following code creates points on the same plane as the previous example, but this time noise is added.", "The following figure shows how the duplicate columns are now no longer exact duplicates. Will the solver average the points with noise on them to get a better signal? I would average x0 and x1 into a cleaner point, and then average x2 and x3 into a cleaner point. Let\u2019s see what the solver decides to do.", "We now make yet another runner file as follows:", "If you have permissions issues, do the chmod 777 thing, or 775 or whatever. To run the program do this:", "As the solver works through ideas, it comes up with some wild stuff. You can sort of see in the figure below the plane-like shape in one solution the solver tried: 1.885417681639+log((((x1+1)/cos((x0\u20131)))+1)). Unfortunately the 2 variables it tried there are x0 and x1, which are duplicates of each other with a small amount of noise added.", "Nice try solver. Let\u2019s keep it running and see what happens next.", "As I had hoped, the solver figured out that averaging x0 and x1 gets you a cleaner (less noisy) x01, and averaging x2 and x3 similarly results in a less noisy x23. Recall that the original formula used to make \u201cy\u201d was operating on the input data before we added noise to the inputs:", "This is another version of the formula that uses fewer variables in exchange for a slightly less perfect fit to the data (because of the added noise). And so the solver gives you, the user, the option to see the formula that fits the data at different levels of complexity.", "A symbolic regression solver called AI-Feynman 2.0 was tested out in this article, starting with the example that comes with the repo, moving to an example we make ourselves from scratch, and finally challenging the solver by adding some noise. A notebook for reproducing this article can be found HERE.", "Special thanks to Silviu Marian Udrescu for helping me to better understand the code, and for reviewing an earlier draft of this work to make sure I don\u2019t say silly things. This is going to be fun to try on real-world problems. I have been containerizing this library for Gravity-ai.com to apply to real-world datasets. Hopefully you will find it useful and use it for your own work.", "If you liked this article, then have a look at some of my most read past articles, like \u201cHow to Price an AI Project\u201d and \u201cHow to Hire an AI Consultant.\u201d And hey, join the newsletter!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Passionate About Machine Learning R&D and Value Creation. \u270d daniel@lemay.ai \u2b31 https://lemay.ai"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F3232151bd929&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fai-feynman-2-0-learning-regression-equations-from-data-3232151bd929&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fai-feynman-2-0-learning-regression-equations-from-data-3232151bd929&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fai-feynman-2-0-learning-regression-equations-from-data-3232151bd929&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fai-feynman-2-0-learning-regression-equations-from-data-3232151bd929&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----3232151bd929--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3232151bd929--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@lemaysolutions?source=post_page-----3232151bd929--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@lemaysolutions?source=post_page-----3232151bd929--------------------------------", "anchor_text": "Daniel Shapiro, PhD"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe7f791e64e83&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fai-feynman-2-0-learning-regression-equations-from-data-3232151bd929&user=Daniel+Shapiro%2C+PhD&userId=e7f791e64e83&source=post_page-e7f791e64e83----3232151bd929---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3232151bd929&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fai-feynman-2-0-learning-regression-equations-from-data-3232151bd929&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3232151bd929&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fai-feynman-2-0-learning-regression-equations-from-data-3232151bd929&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://pixabay.com/images/search/user:geralt/", "anchor_text": "Gerd Altmann"}, {"url": "https://pixabay.com/illustrations/learn-mathematics-child-girl-2300141/", "anchor_text": "Pixabay"}, {"url": "https://space.mit.edu/home/tegmark/", "anchor_text": "Max Tegmark"}, {"url": "https://arxiv.org/abs/2006.10782", "anchor_text": "AI Feynman 2.0: Pareto-optimal symbolic regression exploiting graph modularity"}, {"url": "https://github.com/SJ001", "anchor_text": "Silviu-Marian Udrescu"}, {"url": "https://en.wikipedia.org/wiki/Symbolic_regression#:~:text=Symbolic%20Regression%20(SR)%20is%20a,starting%20point%20to%20the%20algorithm.", "anchor_text": "Symbolic regression"}, {"url": "https://en.wikipedia.org/wiki/Decision_tree_learning#:~:text=Decision%20trees%20where%20the%20target,numbers)%20are%20called%20regression%20trees.&text=In%20decision%20analysis%2C%20a%20decision,represent%20decisions%20and%20decision%20making.", "anchor_text": "regression trees"}, {"url": "https://umap-learn.readthedocs.io/en/latest/", "anchor_text": "UMAP"}, {"url": "https://github.com/dcshapiro/AI-Feynman/blob/master/AI_Feynman_2_0.ipynb", "anchor_text": "dcshapiro/AI-FeynmanPermalink Dismiss GitHub is home to over 50 million developers working together to host and review code, manage\u2026github.com"}, {"url": "https://www.wolframalpha.com/input/?i=3.000000000000%2Blog%28sqrt%28exp%28%28x2-x1%29%29%29%29", "anchor_text": "WolframAlpha"}, {"url": "https://www.wolframalpha.com/input/?i=3.000000000000%2Blog%28sqrt%28exp%28%28x2-x1%29%29%29%29", "anchor_text": "see on WolframAlpha"}, {"url": "https://www.wolframalpha.com/input/?i=1.885417681639%2Blog%28%28%28%28x1%2B1%29%2Fcos%28%28x0-1%29%29%29%2B1%29%29", "anchor_text": "WolframAlpha"}, {"url": "https://github.com/SJ001/AI-Feynman", "anchor_text": "AI-Feynman 2.0"}, {"url": "https://github.com/dcshapiro/AI-Feynman/blob/master/AI_Feynman_2_0.ipynb", "anchor_text": "HERE"}, {"url": "https://github.com/SJ001", "anchor_text": "Silviu Marian Udrescu"}, {"url": "https://www.gravity-ai.com/", "anchor_text": "Gravity-ai.com"}, {"url": "https://medium.com/towards-data-science/how-to-price-an-ai-project-f7270cb630a4", "anchor_text": "How to Price an AI Project"}, {"url": "https://medium.com/towards-data-science/why-hire-an-ai-consultant-50e155e17b39", "anchor_text": "How to Hire an AI Consultant"}, {"url": "http://eepurl.com/gdKMVv", "anchor_text": "join the newsletter"}, {"url": "https://lemay.ai/", "anchor_text": "Lemay.ai"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----3232151bd929---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----3232151bd929---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----3232151bd929---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/regression?source=post_page-----3232151bd929---------------regression-----------------", "anchor_text": "Regression"}, {"url": "https://medium.com/tag/data-science?source=post_page-----3232151bd929---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3232151bd929&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fai-feynman-2-0-learning-regression-equations-from-data-3232151bd929&user=Daniel+Shapiro%2C+PhD&userId=e7f791e64e83&source=-----3232151bd929---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3232151bd929&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fai-feynman-2-0-learning-regression-equations-from-data-3232151bd929&user=Daniel+Shapiro%2C+PhD&userId=e7f791e64e83&source=-----3232151bd929---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3232151bd929&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fai-feynman-2-0-learning-regression-equations-from-data-3232151bd929&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3232151bd929--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F3232151bd929&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fai-feynman-2-0-learning-regression-equations-from-data-3232151bd929&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----3232151bd929---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----3232151bd929--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----3232151bd929--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----3232151bd929--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----3232151bd929--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----3232151bd929--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----3232151bd929--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----3232151bd929--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----3232151bd929--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@lemaysolutions?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@lemaysolutions?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Daniel Shapiro, PhD"}, {"url": "https://medium.com/@lemaysolutions/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "4.6K Followers"}, {"url": "https://lemay.ai", "anchor_text": "https://lemay.ai"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe7f791e64e83&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fai-feynman-2-0-learning-regression-equations-from-data-3232151bd929&user=Daniel+Shapiro%2C+PhD&userId=e7f791e64e83&source=post_page-e7f791e64e83--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F7c56dcd5dd56&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fai-feynman-2-0-learning-regression-equations-from-data-3232151bd929&newsletterV3=e7f791e64e83&newsletterV3Id=7c56dcd5dd56&user=Daniel+Shapiro%2C+PhD&userId=e7f791e64e83&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}