{"url": "https://towardsdatascience.com/understanding-k-nearest-neighbors-b249969d5265", "time": 1683015664.541337, "path": "towardsdatascience.com/understanding-k-nearest-neighbors-b249969d5265/", "webpage": {"metadata": {"title": "Understanding K-Nearest Neighbors | by Trist'n Joseph | Towards Data Science", "h1": "Understanding K-Nearest Neighbors", "description": "Machine learning (ML) algorithms are often categorized as either supervised or unsupervised, and this broadly refers to whether the dataset being used is labelled or not. Supervised ML algorithms\u2026"}, "outgoing_paragraph_urls": [], "all_paragraphs": ["Machine learning (ML) algorithms are often categorized as either supervised or unsupervised, and this broadly refers to whether the dataset being used is labelled or not. Supervised ML algorithms apply what has been learned in the past to new data by using labelled examples to predict future outcomes. Essentially, the correct answer is known for these types of problems and the estimated model\u2019s performance is judged based on whether or not the predicted output is correct.", "In contrast, unsupervised ML algorithms refer to those developed when the information used to train the model is neither classified nor labelled. These algorithms work by attempting to make sense out of data by extracting features and patterns that can be found within the sample.", "Supervised learning is useful when the given task is a classification or regression problem. Classification problems refer to grouping observations or input data into discrete \u2018classes\u2019 based on particular criteria developed by the model. A typical example of this would be predicting whether a given picture displays a cat or does not. The model would be developed and trained on a dataset containing both cat pictures and non-cat pictures, where each observation is appropriately labelled.", "Although supervised learning is an appropriate method for both classification and regression problems, the two are quite different. Regression problems refer to the process of accepting a set of input data and determining a continuous quantity as the output. That is, the goal of regression problems is to predict a particular number, given an input vector. Whereas classification problems, on the other hand, predict a discrete value (or label) that is associated with an input vector.", "Typically, classification algorithms group observations together based on similarity. For example, suppose we wanted to group reptiles versus non-reptiles. It might be found that the animals which have scales, lay eggs, and are poisonous are more likely to be reptiles than non-reptiles. Therefore, if a new observation which had these features was added to the data set, a similarity analysis would find that this observation is more closely related to reptiles than non-reptiles, and it would be classified as a reptile.", "A common similarity analysis to conduct is that of a distance matrix. This shows how \u2018far apart\u2019 observations are from each other, in terms of what they have in common. Observations which have less distance between them are said to be more similar to each other and have a higher chance of being grouped together. There are multiple methods to determine distance, but a relatively easy method is the Jaccard index.", "This index, also known as the Jaccard similarity coefficient, compares the members for two groups to see which members are shares and which are distinct. The measure of similarity ranges from 0% to 100%, and higher percentages indicate greater similarity. Going back to the example of reptiles versus non-reptiles, suppose the relevant features were scales, being poisonous, egg-laying, having facial hair and having blood. If we compared a viper snake to a human, chances are they would only have 1/5 things in common, producing a similarity score of 20%. Thus, they should not be grouped together.", "Now, this example is one of the simplest approaches to classification, and it is referred to as nearest neighbour. The \u2018learning part\u2019 of this algorithm is fairly trivial because all the algorithm has to do is remember the training data. Then, when a new observation needs to be classified, the algorithm will then go to the training data, see which previous observation this new observation is most similar to, and then predict it as that group.", "At face value, this nearest neighbour approach seems spectacular because it is likely that observations that are most similar to each other should be grouped together. We saw from the example above that viper snakes do not have much in common with humans so there is little chance of them being grouped together as reptiles. But recall that our task was to predict reptiles versus non-reptiles. Fish should be classified within the non-reptile group, but there are poisonous fish that have scales and lay eggs. Based on similarity analysis (and the variables given previously), fish would be classified as replies.", "Therefore, the issue with the nearest neighbour approach is that it is very sensitive to noisy data and could lead to incorrect classifications.", "What is usually done to combat this is a modification of the nearest neighbours approach; k nearest neighbours (kNN). The idea here is that we don\u2019t just take the nearest neighbour, but we take some number of nearest neighbours (usually an odd number) and let them \u2018vote\u2019 on what the predicted classification should be.", "So now the question comes, how do we choose the appropriate k? Well, there are about 3 main things to consider. The first is that as k gets larger, the longer it takes for the algorithm to run because it must conduct many more comparisons. This is also the case as the training set contains more observations. The second is that if k is too small, then there might not be a way for the neighbours to \u2018vote\u2019 and choose a winner. Therefore, k must be sufficiently large. The last is that if k is too large then it runs the risk of getting dominated by the size of the class. That is, suppose there were many types of reptiles within our set and the reptiles outnumbered the non-reptiles. A new observation is then likely to be classified as a reptile, given that k is extraordinarily large.", "Thus, the most robust method of choosing the appropriate k is cross-validation. This is a technique used to evaluate a model\u2019s fit by training several models on various subsets of the sample dataset and then evaluating them on a complementary subset of the training set.", "The advantages of kNN are that the learning is very fast, it does not involve much math, and it is very easy to explain the process or the results derived from the algorithm. The disadvantages, however, are that it is very memory-intensive and the predictions take a long time to be produced (as the data set gets larger) due to the number of comparisons being conducted.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Scientist? Yes. Researcher? Somewhat. Content creator? Sure, why not."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fb249969d5265&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-k-nearest-neighbors-b249969d5265&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-k-nearest-neighbors-b249969d5265&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-k-nearest-neighbors-b249969d5265&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-k-nearest-neighbors-b249969d5265&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----b249969d5265--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b249969d5265--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://trisxcjoseph.medium.com/?source=post_page-----b249969d5265--------------------------------", "anchor_text": ""}, {"url": "https://trisxcjoseph.medium.com/?source=post_page-----b249969d5265--------------------------------", "anchor_text": "Trist'n Joseph"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F32920ce9f4b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-k-nearest-neighbors-b249969d5265&user=Trist%27n+Joseph&userId=32920ce9f4b&source=post_page-32920ce9f4b----b249969d5265---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb249969d5265&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-k-nearest-neighbors-b249969d5265&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb249969d5265&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-k-nearest-neighbors-b249969d5265&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://ocw.mit.edu/courses/sloan-school-of-management/15-097-prediction-machine-learning-and-statistics-spring-2012/lecture-notes/MIT15_097S12_lec06.pdf", "anchor_text": "ocw.mit.edu/courses/sloan-school-of-management/15-097-prediction-machine-learning-and-statistics-spring-2012/lecture-notes/MIT15_097S12_lec06.pdf"}, {"url": "https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-034-artificial-intelligence-fall-2010/tutorials/MIT6_034F10_tutor03.pdf", "anchor_text": "ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-034-artificial-intelligence-fall-2010/tutorials/MIT6_034F10_tutor03.pdf"}, {"url": "https://jmlr.org/papers/volume10/weinberger09a/weinberger09a.pdf", "anchor_text": "jmlr.org/papers/volume10/weinberger09a/weinberger09a.pdf"}, {"url": "https://www.geeksforgeeks.org/find-the-jaccard-index-and-jaccard-distance-between-the-two-given-sets/", "anchor_text": "geeksforgeeks.org/find-the-jaccard-index-and-jaccard-distance-between-the-two-given-sets/"}, {"url": "https://towardsdatascience.com/machine-learning-basics-with-the-k-nearest-neighbors-algorithm-6a6e71d01761", "anchor_text": "towardsdatascience.com/machine-learning-basics-with-the-k-nearest-neighbors-algorithm-6a6e71d01761"}, {"url": "https://www.analyticsvidhya.com/blog/2018/03/introduction-k-neighbours-algorithm-clustering/", "anchor_text": "analyticsvidhya.com/blog/2018/03/introduction-k-neighbours-algorithm-clustering/"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html", "anchor_text": "scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html"}, {"url": "https://medium.com/tag/data-science?source=post_page-----b249969d5265---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----b249969d5265---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----b249969d5265---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/programming?source=post_page-----b249969d5265---------------programming-----------------", "anchor_text": "Programming"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----b249969d5265---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb249969d5265&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-k-nearest-neighbors-b249969d5265&user=Trist%27n+Joseph&userId=32920ce9f4b&source=-----b249969d5265---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb249969d5265&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-k-nearest-neighbors-b249969d5265&user=Trist%27n+Joseph&userId=32920ce9f4b&source=-----b249969d5265---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb249969d5265&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-k-nearest-neighbors-b249969d5265&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b249969d5265--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fb249969d5265&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-k-nearest-neighbors-b249969d5265&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----b249969d5265---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----b249969d5265--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----b249969d5265--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----b249969d5265--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----b249969d5265--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----b249969d5265--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----b249969d5265--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----b249969d5265--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----b249969d5265--------------------------------", "anchor_text": ""}, {"url": "https://trisxcjoseph.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://trisxcjoseph.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Trist'n Joseph"}, {"url": "https://trisxcjoseph.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "385 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F32920ce9f4b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-k-nearest-neighbors-b249969d5265&user=Trist%27n+Joseph&userId=32920ce9f4b&source=post_page-32920ce9f4b--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fcfd73cacf13a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-k-nearest-neighbors-b249969d5265&newsletterV3=32920ce9f4b&newsletterV3Id=cfd73cacf13a&user=Trist%27n+Joseph&userId=32920ce9f4b&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}