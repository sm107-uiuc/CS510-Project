{"url": "https://towardsdatascience.com/legal-applications-of-neural-word-embeddings-556b7515012f", "time": 1683012075.89902, "path": "towardsdatascience.com/legal-applications-of-neural-word-embeddings-556b7515012f/", "webpage": {"metadata": {"title": "Legal Applications of Neural Word Embeddings | by Erin Zhang | Towards Data Science", "h1": "Legal Applications of Neural Word Embeddings", "description": "A fundamental issue with LegalTech is that words \u2014 the basic currency of all legal documentation \u2014 are a form of unstructured data that cannot be intuitively understood by machines. Therefore, in\u2026"}, "outgoing_paragraph_urls": [], "all_paragraphs": ["A fundamental issue with LegalTech is that words \u2014 the basic currency of all legal documentation \u2014 are a form of unstructured data that cannot be intuitively understood by machines. Therefore, in order to process textual documents, words have to be represented by vectors of real numbers.", "Traditionally, methods like bag-of-words (BoW) map word tokens/n-grams to term frequency vectors, which represent the number of times a word has appeared in the document. Using one-hot encoding, each word token/n-gram is represented by a vector element and marked 0, 1, 2 etc depending on whether or the number of times that a word is present in the document. This means that if a word is absent in the corpus vocabulary, the element will be marked 0, and if it is present once, the element will be marked 1 etc.", "The problem was that this produced very sparse matrices (i.e. mostly comprising zeros) in extremely high dimensions. For instance, a corpus with 30,000 unique word tokens would require a matrix with 30,000 rows, which is extremely computationally taxing. Furthermore, this method fails to capture semantic meaning, context, and word relations, as it can only show how frequently a word exists in a document. This inability to represent semantic meaning persisted even as BoW was complemented by the TF-IDF measure, as while the latter was able to represent a measure of how important a word was to a corpus (i.e. an improvement from the plain BoW representations), it was still computed based on the frequency of a word token/n-gram appearing in the corpus.", "In contrast, modern word embeddings (word2vec, GloVE, fasttext etc) rely on neural networks to map the semantic properties of words into dense vector representations with significantly less dimensions.", "As a preliminary note, it should be said that word embeddings are premised off distributional semantics assumptions, i.e. words that are used and occur in the same contexts tend to have similar meanings. This means that the neural network learns the vector representations of the words through the contexts that the words are found in.", "\u201cContext\u201d here is represented by a sliding context window of size n, where n words before the target word and n words after the target word will fall within the context window (e.g. n=2 in the example). The model will then train by using one-hot encoded context vectors to predict one-hot encoded target vectors as the context window moves down the sentence.", "In doing so, word embeddings can capture semantic associations and linguistic contexts not captured by BoW. This article will explore the impact of neural word embeddings in legal AI technologies.", "An important implication of word embeddings is that it captures the semantic relationship between words.", "The ability to map out the relationship between legal terms and objects has exciting implications in its capacity to improve our understanding of legal reasoning. An interesting direction is the potential vectorisation of judicial opinions with doc2vec to identify/cluster judges with similar belief patterns (based on conservativeness of legal opinions, precedents-cited etc).", "Another function is that word embeddings can capture implicit racial and gender biases in judicial opinions, as measured by the Word Embedding Association Test (WEAT). Word embeddings are powerful because they can represent societal biases in mathematical or diagrammatical form. For instance, Bolukbasi (2016) showed that word embeddings trained on Google News articles exhibited significant gender bias, which can be geometrically captured by a direction in the word embedding (i.e. gender-neutral words are linearly separable from gender-neutral words).", "As such, word embeddings may reflect vector relationships like \u201cman is to programmer as woman is to home-maker\u201d, as the word \u201cman\u201d in the Google News corpora co-occurs more frequently alongside words like \u201cprogrammer\u201d or \u201cengineer\u201d, while the word \u201cwoman\u201d will appear more frequently beside \u201chomemaker\u201d or \u201cnurse\u201d.", "Applied to the legal domain, we can tabulate WEAT scores across judicial opinions, and preliminary research in this field has shown interesting trends, such as (i) male judges showing higher gender bias (i.e. higher WEAT scores) than female judges and (ii) white judges showing lower race bias than black judges. More remains to be explored in this domain.", "Word embeddings also has fundamental implications for improving the technology behind legal research platforms (known in machine learning parlance as Legal Information Retrieval (LIR) systems).", "Currently, most LIR systems (e.g. Westlaw and LexisNexis) are still boolean-indexed systems primarily utilising keyword search functionality.", "This means that the system looks for literal matches or variants of the query keywords, usually by using string-based algorithms to measure the similarity between two text strings. However, these types of searches fail to understand the intent behind the solicitor\u2019s query, meaning that search results are often under-inclusive (i.e. missing relevant information that does not contain the keyword, but perhaps variants of it) or over-inclusive (i.e. returning irrelevant information that comprises the keyword).", "Word embeddings, however, enhance the potential of commercially available semantic search LIR . As it allows practitioners to mathematically capture semantic similarity, word embeddings can help LIR systems find results that are not only exact matches of the query string, but also results that might be relevant or semantically close, but differ in certain words.", "For instance, Landthaler shows that effective results can be produced by first summing up the word vectors in each search phrase into a search phrase vector. The document is then sequentially parsed by a window of size n (where n = the number of words in the search phrase) and the cosine similarity of the search phrase vector and accumulated vectors is calculated. This will not only be able to return exact keyword-matching results, but also semantically-related search results, which provides more intuitive results.", "This is especially important since research shows that participants using boolean-indexed search LIR systems which search for the exact matches of query terms on full-text legal documents can have recall rates as low as 20% (i.e. only 20% of relevant documents are retrieved by the LIR). However, on average, these participants estimate their retrieval rates to be up to 75%, which is significantly higher. This means that solicitors can often overlook relevant precedents or case law that may bolster their case, just because the LIR system prioritises string similarity over semantic similarity. Word embeddings hence have the potential to significantly address this shortfall.", "Overall, the field of neural word embeddings is fascinating. Not only is the ability to mathematically capture semantic context and word relations academically intriguing, word embeddings have also been a hugely important driver behind many LegalTech products in the market.", "However, word embeddings are not without limitations, and ML practitioners sometimes turn to newer pre-trained language modelling techniques (e.g. ULMFit, ELMo, OpenAI transformer, and BERT) to overcome some of the inherent problems with word embeddings (e.g. presuming monosemy). Nevertheless, word embeddings remain one of the most fascinating NLP topics today, and the move from sparse, frequency-based vector representations to denser semantically-representative vectors is a crucial step in advancing the NLP subdomain and the field of legal AI.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Hi, I\u2019m Erin! I work as a trainee solicitor in London and am passionate about making legaltech and data science concepts accessible to everyone."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F556b7515012f&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flegal-applications-of-neural-word-embeddings-556b7515012f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flegal-applications-of-neural-word-embeddings-556b7515012f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flegal-applications-of-neural-word-embeddings-556b7515012f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flegal-applications-of-neural-word-embeddings-556b7515012f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----556b7515012f--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----556b7515012f--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@erin.zhangyj?source=post_page-----556b7515012f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@erin.zhangyj?source=post_page-----556b7515012f--------------------------------", "anchor_text": "Erin Zhang"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1f0b81ad9d97&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flegal-applications-of-neural-word-embeddings-556b7515012f&user=Erin+Zhang&userId=1f0b81ad9d97&source=post_page-1f0b81ad9d97----556b7515012f---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F556b7515012f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flegal-applications-of-neural-word-embeddings-556b7515012f&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F556b7515012f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flegal-applications-of-neural-word-embeddings-556b7515012f&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://developers.google.com/machine-learning/crash-course/embeddings/translating-to-a-lower-dimensional-space", "anchor_text": "Google Developers"}, {"url": "https://www.analyticssteps.com/blogs/an-optimum-approach-towards-the-bag-of-words-with-code-illustration-in-python", "anchor_text": "AnalyticSteps"}, {"url": "https://www.semanticscholar.org/paper/Man-is-to-Computer-Programmer-as-Woman-is-to-Word-Bolukbasi-Chang/ccf6a69a7f33bcf052aa7def176d3b9de495beb7", "anchor_text": "https://www.semanticscholar.org/paper/Man-is-to-Computer-Programmer-as-Woman-is-to-Word-Bolukbasi-Chang/ccf6a69a7f33bcf052aa7def176d3b9de495beb7)"}, {"url": "http://mlwiki.org/index.php/Information_Retrieval", "anchor_text": "http://mlwiki.org/index.php/Information_Retrieval"}, {"url": "https://www.ontotext.com/knowledgehub/fundamentals/what-is-semantic-search/", "anchor_text": "https://www.ontotext.com/knowledgehub/fundamentals/what-is-semantic-search/"}, {"url": "https://www.lawtechnologytoday.org/2017/11/legal-consumer-research/", "anchor_text": "https://www.lawtechnologytoday.org/2017/11/legal-consumer-research/"}, {"url": "https://medium.com/tag/legaltech?source=post_page-----556b7515012f---------------legaltech-----------------", "anchor_text": "Legaltech"}, {"url": "https://medium.com/tag/legal-ai?source=post_page-----556b7515012f---------------legal_ai-----------------", "anchor_text": "Legal Ai"}, {"url": "https://medium.com/tag/nlp?source=post_page-----556b7515012f---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/word-embeddings?source=post_page-----556b7515012f---------------word_embeddings-----------------", "anchor_text": "Word Embeddings"}, {"url": "https://medium.com/tag/legal-ai-software-market?source=post_page-----556b7515012f---------------legal_ai_software_market-----------------", "anchor_text": "Legal Ai Software Market"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F556b7515012f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flegal-applications-of-neural-word-embeddings-556b7515012f&user=Erin+Zhang&userId=1f0b81ad9d97&source=-----556b7515012f---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F556b7515012f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flegal-applications-of-neural-word-embeddings-556b7515012f&user=Erin+Zhang&userId=1f0b81ad9d97&source=-----556b7515012f---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F556b7515012f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flegal-applications-of-neural-word-embeddings-556b7515012f&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----556b7515012f--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F556b7515012f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flegal-applications-of-neural-word-embeddings-556b7515012f&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----556b7515012f---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----556b7515012f--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----556b7515012f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----556b7515012f--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----556b7515012f--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----556b7515012f--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----556b7515012f--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----556b7515012f--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----556b7515012f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@erin.zhangyj?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@erin.zhangyj?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Erin Zhang"}, {"url": "https://medium.com/@erin.zhangyj/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "63 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1f0b81ad9d97&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flegal-applications-of-neural-word-embeddings-556b7515012f&user=Erin+Zhang&userId=1f0b81ad9d97&source=post_page-1f0b81ad9d97--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fc328a5522871&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flegal-applications-of-neural-word-embeddings-556b7515012f&newsletterV3=1f0b81ad9d97&newsletterV3Id=c328a5522871&user=Erin+Zhang&userId=1f0b81ad9d97&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}