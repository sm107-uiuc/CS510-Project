{"url": "https://towardsdatascience.com/using-stochastic-gradient-descent-to-train-linear-classifiers-c80f6aeaff76", "time": 1683017722.189454, "path": "towardsdatascience.com/using-stochastic-gradient-descent-to-train-linear-classifiers-c80f6aeaff76/", "webpage": {"metadata": {"title": "Using Stochastic Gradient Descent to Train Linear Classifiers | by Lindo St. Angel | Towards Data Science", "h1": "Using Stochastic Gradient Descent to Train Linear Classifiers", "description": "A guide to using Stochastic Gradient Descent to efficiently train linear classifiers when the number of training examples or features is large"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/stochastic-gradient-descent-clearly-explained-53d239905d31", "anchor_text": "Stochastic Gradient Descent", "paragraph_index": 0}, {"url": "https://en.wikipedia.org/wiki/Linear_classifier", "anchor_text": "linear classifiers", "paragraph_index": 0}, {"url": "https://en.wikipedia.org/wiki/Support_vector_machine", "anchor_text": "Support Vector Machines", "paragraph_index": 0}, {"url": "https://en.wikipedia.org/wiki/Logistic_regression", "anchor_text": "Logistic Regression", "paragraph_index": 0}, {"url": "https://en.wikipedia.org/wiki/Online_machine_learning", "anchor_text": "online learning", "paragraph_index": 1}, {"url": "https://scikit-learn.org/stable/index.html", "anchor_text": "scikit-learn", "paragraph_index": 2}, {"url": "https://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf", "anchor_text": "A Practical Guide to Support Vector Classification", "paragraph_index": 3}, {"url": "https://github.com/goruck/radar-ml", "anchor_text": "radar-ml", "paragraph_index": 4}, {"url": "https://drive.google.com/drive/folders/12QKwuzniZkiqsEVBDx-ho9K6AbOiJz7w?usp=sharing", "anchor_text": "here", "paragraph_index": 4}, {"url": "https://github.com/goruck/radar-ml/blob/master/train.py", "anchor_text": "train.py", "paragraph_index": 12}, {"url": "https://github.com/goruck/radar-ml", "anchor_text": "radar-ml", "paragraph_index": 12}, {"url": "https://github.com/goruck/radar-ml/blob/master/train.py#L324", "anchor_text": "train.py", "paragraph_index": 13}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier", "anchor_text": "sklearn linear_model.SGDClassifier", "paragraph_index": 13}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html", "anchor_text": "sklearn.svm.LinearSVC", "paragraph_index": 14}, {"url": "https://www.csie.ntu.edu.tw/~cjlin/liblinear/", "anchor_text": "LIBLINEAR", "paragraph_index": 14}, {"url": "https://www.csie.ntu.edu.tw/~cjlin/libsvm/", "anchor_text": "LIBSVM", "paragraph_index": 15}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html", "anchor_text": "sklearn svm.SVC", "paragraph_index": 15}, {"url": "https://github.com/goruck/radar-ml/blob/master/train.py#L442", "anchor_text": "train.py", "paragraph_index": 15}, {"url": "https://en.wikipedia.org/wiki/Sequential_minimal_optimization", "anchor_text": "Sequential minimal optimization", "paragraph_index": 15}, {"url": "https://en.wikipedia.org/wiki/Kernel_trick", "anchor_text": "kernelized", "paragraph_index": 15}, {"url": "https://github.com/goruck/radar-ml/blob/master/train.py#L215", "anchor_text": "train.py", "paragraph_index": 16}, {"url": "https://drive.google.com/drive/folders/1k5Xa02aVMp-JnmlOHgUXpqPexbTUeZve?usp=sharing", "anchor_text": "here", "paragraph_index": 21}, {"url": "https://github.com/goruck/radar-ml/blob/master/predict.py#L56", "anchor_text": "predict.py", "paragraph_index": 22}], "all_paragraphs": ["You are probably aware that Stochastic Gradient Descent (SGD) is one of the key algorithms used in training deep neural networks. However, you may not be as familiar with its application as an optimizer for training linear classifiers such as Support Vector Machines and Logistic Regression or when and how to apply it.", "In this article, you will learn that the approach can be effectively used on large-scale data sets (> 10\u2075 samples) or with a large number (> 10\u2075) of features where other methods may lead to extremely long fit times or be infeasible to use beyond a few thousand samples or features. Additionally, SGD allows for online learning, making the algorithm quickly fit new data on an existing classifier.", "You will learn about how to use Python APIs from scikit-learn, an example of a data set that is well-suited for this method captured from radar samples, test results from a classifier fitted with that data using SGD and some drawbacks of using SGD including the need for rather extensive hyperparameter tuning.", "You can use other optimizers to train linear classifiers and, depending on the size of your data set and feature space, the SGD method and linear classifiers in general may not be the best solution. For an overall guide to using SVMs, see A Practical Guide to Support Vector Classification (Hsu, et al., 2016) Some techniques described in this excellent paper were used here.", "In order to help you understand the techniques and code used in this article, a short walk through of the data set is provided in this section. The data set was gathered from radar samples as part of the radar-ml project and found here. This project employs autonomous supervised learning whereby standard camera-based object detection techniques are used to automatically label radar scans of people and objects.", "The data set is a Python dict of the form:", "samples is a list of N radar projection numpy.array tuple samples in the form:", "Where a radar projection is the maximum return signal strength of a scanned target object in 3-D space projected to the x, y and z axis. These 2-D representations are typically sparse since a projection occupies a small part of scanned volume.", "labels is a list of N numpy.array class labels corresponding to each radar projection sample of the form:", "Projections from a typical single sample are shown in the heat map visualization below. Red indicates where the return signal is strongest.", "This data was captured in my house in various locations designed to maximize the variation in detected objects (currently only people, dogs and cats), distance and angle from the radar sensor.", "The data set contains known labeling errors, mostly stemming from the object detector mistaking my cat for my dog which happens (subjectively), at about a 10% error rate. A future effort will attempt to fine-tune the object detector to reduce the error. This error will get propagated to the radar classifier trained from this data set.", "You can use the steps below to train the model on the radar data. The complete Python code that implements these steps can be found in the train.py module of the radar-ml project.", "The Python snippet below from radar-ml\u2019s train.py shows the actual fitting function. This uses the sklearn linear_model.SGDClassifier API with \u2018log\u2019 loss which gives Logistic Regression. You can see the online training aspect which is used to do partial fits on the optimum classifier using augmented data as well as novel data sets \u2014 a very computationally efficient process. You can also see that the grid search tries fits with a number of hyperparameters and getting these values right is key to an accurate classifier.", "Note: the sklearn.svm.LinearSVC API can optimize the same cost function as the SGDClassifier by adjusting the penalty and loss parameters. However, LinearSVC does not allow for online learning. LinearSVC uses the LIBLINEAR library (Fan et al.,2008).", "The train.py module also will fit a model using LIBSVM (Chang and Lin, 2011) via the sklearn svm.SVC API, you can see that used in the Python snippet below from radar-ml\u2019s train.py. LIBSVM implements the Sequential minimal optimization algorithm for kernelized Support Vector Machines which is a very powerful method but does not scale well for large data sets or feature vectors from a fit time perspective.", "Using the test set that was split from the data set in the step above, evaluate the performance of the final classifier. The test set was not used for either model training or calibration validation so these samples are completely new to the classifier. The evaluation function is shown in the Python snippet below which is part of radar-ml\u2019s train.py.", "The evaluation results from using SGDClassifier are shown below.", "The evaluation results from using SVC are shown below.", "You can see that the SGD method gives better overall accuracy (89% vs. 84%) on the test set and moreover completes the training in about seven minutes (including four epochs of augmentation) vs. about 75 minutes as compared to SVC. These results are not apples-to-apples since the SGD classifier accuracy benefits from the data augmentation. Using augmentation with SVC is basically infeasible on my i5 3.4 GHz machine since the training times are a non-linear function of the training set and would take many days. Note that some of the inaccuracies are likely due to the labeling errors highlighted above.", "The resulting SGD-trained linear classifier takes about 250 KB of disk space whereas SVC results in a classifier (RBF kernel) more than two orders of magnitude larger, around 40 MB. This could be an advantage if you use the SDG classifier in a resource limited embedded system.", "You can find the fitted classifiers and training results here.", "Using the classifier to make predictions on new data is straightforward as you can see from the Python snippet below. This is taken from radar-ml\u2019s predict.py.", "You should consider using Stochastic Gradient Descent as an optimizer to efficiently train linear classifiers if you have a large number (many thousands) of training examples or features. Also consider using it for online learning, for example in situations where it is necessary for the algorithm to dynamically adapt to new patterns in the data. Different optimization methods or classifiers may be better in other cases.", "SGD classifiers are sensitive to feature scaling and require fine tuning of a number of hyperparameters including the regularization parameter and the number of iteration for good performance. You should always use feature normalization and a technique like grid search to find the most optimal hyperparameters when using this method. If you intend to use the classifier to predict both a class and a confidence level, you should calibrate it first on a data set disjoint from the training set. Always evaluate the final classifier on a test set disjoint from both the training and validation set.", "You will find prediction using the SGD classifier straightforward via the sklearn APIs and its compact size favors resource limited embedded systems.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "High technology professional at Amazon creating amazing products and services customers love."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fc80f6aeaff76&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-stochastic-gradient-descent-to-train-linear-classifiers-c80f6aeaff76&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-stochastic-gradient-descent-to-train-linear-classifiers-c80f6aeaff76&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-stochastic-gradient-descent-to-train-linear-classifiers-c80f6aeaff76&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-stochastic-gradient-descent-to-train-linear-classifiers-c80f6aeaff76&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----c80f6aeaff76--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c80f6aeaff76--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@lindo.st.angel?source=post_page-----c80f6aeaff76--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@lindo.st.angel?source=post_page-----c80f6aeaff76--------------------------------", "anchor_text": "Lindo St. Angel"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe03a0010d87d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-stochastic-gradient-descent-to-train-linear-classifiers-c80f6aeaff76&user=Lindo+St.+Angel&userId=e03a0010d87d&source=post_page-e03a0010d87d----c80f6aeaff76---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc80f6aeaff76&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-stochastic-gradient-descent-to-train-linear-classifiers-c80f6aeaff76&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc80f6aeaff76&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-stochastic-gradient-descent-to-train-linear-classifiers-c80f6aeaff76&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@snaps_by_clark?utm_source=medium&utm_medium=referral", "anchor_text": "Clark Van Der Beken"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://towardsdatascience.com/stochastic-gradient-descent-clearly-explained-53d239905d31", "anchor_text": "Stochastic Gradient Descent"}, {"url": "https://en.wikipedia.org/wiki/Linear_classifier", "anchor_text": "linear classifiers"}, {"url": "https://en.wikipedia.org/wiki/Support_vector_machine", "anchor_text": "Support Vector Machines"}, {"url": "https://en.wikipedia.org/wiki/Logistic_regression", "anchor_text": "Logistic Regression"}, {"url": "https://en.wikipedia.org/wiki/Online_machine_learning", "anchor_text": "online learning"}, {"url": "https://scikit-learn.org/stable/index.html", "anchor_text": "scikit-learn"}, {"url": "https://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf", "anchor_text": "A Practical Guide to Support Vector Classification"}, {"url": "https://github.com/goruck/radar-ml", "anchor_text": "radar-ml"}, {"url": "https://drive.google.com/drive/folders/12QKwuzniZkiqsEVBDx-ho9K6AbOiJz7w?usp=sharing", "anchor_text": "here"}, {"url": "https://github.com/goruck/radar-ml/blob/master/train.py", "anchor_text": "train.py"}, {"url": "https://github.com/goruck/radar-ml", "anchor_text": "radar-ml"}, {"url": "https://github.com/goruck/radar-ml/blob/master/train.py#L324", "anchor_text": "train.py"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier", "anchor_text": "sklearn linear_model.SGDClassifier"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html", "anchor_text": "sklearn.svm.LinearSVC"}, {"url": "https://www.csie.ntu.edu.tw/~cjlin/liblinear/", "anchor_text": "LIBLINEAR"}, {"url": "https://www.csie.ntu.edu.tw/~cjlin/libsvm/", "anchor_text": "LIBSVM"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html", "anchor_text": "sklearn svm.SVC"}, {"url": "https://github.com/goruck/radar-ml/blob/master/train.py#L442", "anchor_text": "train.py"}, {"url": "https://en.wikipedia.org/wiki/Sequential_minimal_optimization", "anchor_text": "Sequential minimal optimization"}, {"url": "https://en.wikipedia.org/wiki/Kernel_trick", "anchor_text": "kernelized"}, {"url": "https://github.com/goruck/radar-ml/blob/master/train.py#L215", "anchor_text": "train.py"}, {"url": "https://drive.google.com/drive/folders/1k5Xa02aVMp-JnmlOHgUXpqPexbTUeZve?usp=sharing", "anchor_text": "here"}, {"url": "https://github.com/goruck/radar-ml/blob/master/predict.py#L56", "anchor_text": "predict.py"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----c80f6aeaff76---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/stochastic-gradient?source=post_page-----c80f6aeaff76---------------stochastic_gradient-----------------", "anchor_text": "Stochastic Gradient"}, {"url": "https://medium.com/tag/classification?source=post_page-----c80f6aeaff76---------------classification-----------------", "anchor_text": "Classification"}, {"url": "https://medium.com/tag/support-vector-machine?source=post_page-----c80f6aeaff76---------------support_vector_machine-----------------", "anchor_text": "Support Vector Machine"}, {"url": "https://medium.com/tag/logistic-regression?source=post_page-----c80f6aeaff76---------------logistic_regression-----------------", "anchor_text": "Logistic Regression"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc80f6aeaff76&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-stochastic-gradient-descent-to-train-linear-classifiers-c80f6aeaff76&user=Lindo+St.+Angel&userId=e03a0010d87d&source=-----c80f6aeaff76---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc80f6aeaff76&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-stochastic-gradient-descent-to-train-linear-classifiers-c80f6aeaff76&user=Lindo+St.+Angel&userId=e03a0010d87d&source=-----c80f6aeaff76---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc80f6aeaff76&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-stochastic-gradient-descent-to-train-linear-classifiers-c80f6aeaff76&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c80f6aeaff76--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fc80f6aeaff76&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-stochastic-gradient-descent-to-train-linear-classifiers-c80f6aeaff76&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----c80f6aeaff76---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----c80f6aeaff76--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----c80f6aeaff76--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----c80f6aeaff76--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----c80f6aeaff76--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----c80f6aeaff76--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----c80f6aeaff76--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----c80f6aeaff76--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----c80f6aeaff76--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@lindo.st.angel?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@lindo.st.angel?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Lindo St. Angel"}, {"url": "https://medium.com/@lindo.st.angel/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "103 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe03a0010d87d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-stochastic-gradient-descent-to-train-linear-classifiers-c80f6aeaff76&user=Lindo+St.+Angel&userId=e03a0010d87d&source=post_page-e03a0010d87d--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fb32de2b8dd33&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-stochastic-gradient-descent-to-train-linear-classifiers-c80f6aeaff76&newsletterV3=e03a0010d87d&newsletterV3Id=b32de2b8dd33&user=Lindo+St.+Angel&userId=e03a0010d87d&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}