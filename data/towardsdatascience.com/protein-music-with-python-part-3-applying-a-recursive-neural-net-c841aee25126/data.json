{"url": "https://towardsdatascience.com/protein-music-with-python-part-3-applying-a-recursive-neural-net-c841aee25126", "time": 1683007719.689095, "path": "towardsdatascience.com/protein-music-with-python-part-3-applying-a-recursive-neural-net-c841aee25126/", "webpage": {"metadata": {"title": "Turning a Protein into Music (Part 3) \u2014 Recursive Neural Network | by Ford Combs | Towards Data Science", "h1": "Turning a Protein into Music (Part 3) \u2014 Recursive Neural Network", "description": "Protein amino acid sequences, generated with an RNN, and structures, predicted with homology modeling, are made into music using python and sonification."}, "outgoing_paragraph_urls": [{"url": "https://levelup.gitconnected.com/turning-a-protein-into-music-with-python-ba655c694097", "anchor_text": "first article", "paragraph_index": 0}, {"url": "https://levelup.gitconnected.com/turning-a-protein-into-music-with-python-part-2-harmony-a3c2ffa748be", "anchor_text": "second article", "paragraph_index": 0}, {"url": "https://pubs.acs.org/doi/10.1021/acsnano.9b02180", "anchor_text": "paper", "paragraph_index": 1}, {"url": "http://cee.mit.edu/people_individual/markus-j-buehler/", "anchor_text": "Markus Buehler\u2019s", "paragraph_index": 1}, {"url": "http://lamm.mit.edu/", "anchor_text": "Laboratory for Atomistic and Molecular Mechanics", "paragraph_index": 1}, {"url": "https://en.wikipedia.org/wiki/Recurrent_neural_network#Long_short-term_memory", "anchor_text": "recurrent neural network", "paragraph_index": 1}, {"url": "https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21", "anchor_text": "excellent article", "paragraph_index": 2}, {"url": "https://magenta.tensorflow.org/", "anchor_text": "Magenta", "paragraph_index": 2}, {"url": "https://www.tensorflow.org/", "anchor_text": "TensorFlow", "paragraph_index": 2}, {"url": "https://www.rcsb.org/structure/1Q2W", "anchor_text": "PDB ID 1Q2W", "paragraph_index": 5}, {"url": "https://en.wikipedia.org/wiki/Delaunay_triangulation", "anchor_text": "Delaunay Tessellation", "paragraph_index": 6}, {"url": "https://web.expasy.org/protscale/pscale/Hphob.Doolittle.html", "anchor_text": "Kyte and Doolittle", "paragraph_index": 7}, {"url": "https://blast.ncbi.nlm.nih.gov/Blast.cgi?PAGE=Proteins", "anchor_text": "blastp", "paragraph_index": 8}, {"url": "http://www.wwpdb.org/documentation/file-format-content/format33/v3.3.html", "anchor_text": "here", "paragraph_index": 10}, {"url": "https://www.tensorflow.org/tutorials/text/text_generation", "anchor_text": "tutorial", "paragraph_index": 13}, {"url": "https://towardsdatascience.com/all-you-need-to-know-about-rnns-e514f0b00c7c", "anchor_text": "here", "paragraph_index": 15}, {"url": "https://stats.stackexchange.com/questions/153531/what-is-batch-size-in-neural-network", "anchor_text": "here", "paragraph_index": 15}, {"url": "https://www.tensorflow.org/tutorials/text/text_generation", "anchor_text": "here", "paragraph_index": 16}, {"url": "https://en.wikipedia.org/wiki/Cosine_similarity", "anchor_text": "cosine similarity", "paragraph_index": 16}, {"url": "https://en.wikipedia.org/wiki/Euclidean_distance", "anchor_text": "euclidean distance", "paragraph_index": 16}, {"url": "https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21", "anchor_text": "here", "paragraph_index": 16}, {"url": "https://pathmind.com/wiki/word2vec", "anchor_text": "word2vec", "paragraph_index": 16}, {"url": "https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21", "anchor_text": "here", "paragraph_index": 17}, {"url": "https://cwiki.apache.org/confluence/display/MXNET/Multi-hot+Sparse+Categorical+Cross-entropy", "anchor_text": "sparse categorical cross entropy", "paragraph_index": 18}, {"url": "https://arxiv.org/pdf/1412.6980.pdf", "anchor_text": "paper", "paragraph_index": 18}, {"url": "https://www.tensorflow.org/tutorials/text/text_generation#generate_text", "anchor_text": "here", "paragraph_index": 21}, {"url": "https://swissmodel.expasy.org/", "anchor_text": "SWISS-MODEL", "paragraph_index": 26}, {"url": "https://en.wikipedia.org/wiki/Homology_modeling", "anchor_text": "homology modeling", "paragraph_index": 26}, {"url": "http://fatcat.godziklab.org/", "anchor_text": "FATCAT", "paragraph_index": 27}, {"url": "https://magenta.tensorflow.org/", "anchor_text": "Magenta", "paragraph_index": 28}], "all_paragraphs": ["In two previous articles, I demonstrated how to convert proteins into music. The first article explained how the molecular vibrations of individual amino acids, the molecules that make up proteins, can be turned into musical notes. The second article introduced harmony and rhythm; the amino acid neighbors in the protein structure can be played together to produce chords and the secondary structure classification of an amino acid can determine its note length. Here, I show how to create a genre of protein music by training an RNN on a set of similar proteins and then using it to generate new sequences, which can be converted into music.", "A paper from Markus Buehler\u2019s Laboratory for Atomistic and Molecular Mechanics at MIT was my introduction to protein sonification. The authors sonified proteins, using the techniques I describe in my previous articles, and then trained a recurrent neural network (RNN) on the resulting musical sequences. Once trained, the RNN was used to generate new musical sequences. Importantly, these new sequences will contain characteristics of the sequences used in training. This has interesting implications when applied to protein structure research. To understand why, it is useful to understand the basics of RNNs.", "RNNs are one of the many types of neural network architectures. They are typically used with sequential information because they have a form of memory, i.e., they can look back at previous information while performing calculations. In the case of sequences, this means RNNs predict the next character in a sequence by considering what precedes it. There are many ways this can be implemented and I recommend reading this excellent article by Michael Phi to learn more. The authors of the Buehler paper used the Magenta library, which is powered by TensorFlow, to build their RNN that included an LSTM and an attention layers. I based my model on a model for generating text by TensorFlow.", "The function of a protein is determined by its structure, which is determined by its sequence. Proteins perform many functions like transporting molecules, identifying antigens, or signalling responses and these different functions are made possible by different structures. It follows that there are classes of protein structures that have similar functions. In order for an RNN to generate interesting sequences, it needs to be trained on some class of information. By interesting, I mean that the generated sequences contain some information or characteristics that are present in the training data. This is the potential of RNNs for studying protein sequences.", "These are the resources I used to build my data set:", "I needed a set of similar protein structures to train the RNN. I chose the SARS coronavirus main protease, PDB ID 1Q2W, as my starting point. I used blasp to search PDB for similar structures and my training data contained 26 protein structures.", "I previously described how amino acid neighbors in the 3D space of a protein structure can be used to build harmonies in protein music. For the purposes of making music, that works well, but for training an RNN, that produces a problem. An amino acid in a protein typically has ~8 neighbors as defined by Delaunay Tessellation. Since there are 20 standard amino acids, that means that there are 20\u2078, over 25 billion, possible combinations of neighbors. Rather than scrap the harmonic information, I wanted to find a way to characterize the group of neighbors as a whole. This is where hydrophobicity enters.", "A molecule that is attracted to water is known as hydrophilic and one that is repelled is known as hydrophobic. Each of the 20 amino acids has a different affinity toward water and there are different scales for measuring this. The Kyte and Doolittle [5] scale assigns a number for each amino acid. On this scale, the more positive the number, the more hydrophobic, and the more negative the number, the more hydrophilic. Using the Kyte and Doolittle scale, a sum of the hydrophobicity scores for each neighbor can be calculated. This sum holds information about the neighbor set, e.g., if it is very positive, then most of the neighbors are hydrophobic. To further simplify this score, the following transformation function was used; it defines the neighbor set hydrophobicity score as either -1, 0, or 1.", "I chose the protein 1Q2W, the SARS coronavirus main protease, as the target protein for my data set and used blastp[4] to find similar proteins. Below are a few lines of the results of the blastp results. The bold text contains the PDB IDs for similar proteins.", "In order to build the data set, the PDB files for the similar proteins were downloaded. The following code accomplishes that in two steps. First, it pulls out the similar protein IDs from the blastp results and, then, it downloads the PDB files for those proteins.", "Next, the protein sequences, coordinates, and secondary structure classifications can be retrieved. This data is stored in specific lines and positions in PDB files. You can learn more about the PDB format here, but the following code pulls out the relevant data. First, the protein ID is obtained from the HEADER line and, then, secondary structure information is collected. The HELIX and SHEET lines contain the starting and ending positions of all the secondary structure elements. So, for each HELIX or SHEET line, the code fills in the internal positions of each element and adds all the positions to helices and sheets, respectively. Next, the sequence position and coordinates of each atom are pulled from the ATOM lines. In this case, the coordinates for the c-alpha carbon are used for each amino acid. Finally, each position is checked to see if it exists in either helices or sheets. If it does, then an H, helix, or E, sheet, is added to the secondary structure (SS) sequence. If no, then a C, coil, is added. The code below stores this information in the data dictionary.", "Below is a function to define amino acid neighbors by Delaunay Tessellation. It takes in the a set of coordinates, in this case the positions of the c-alpha carbon for each amino acid in the protein, and returns a dictionary where each key is a position in the sequence and each value is the set of neighbors.", "The final part of data collection is to create a long sequence that contains all the relevant information for the proteins in the data set. The following code goes through each protein and creates an entry for each position in the sequence. entry contains a string composed of three elements. The first is the amino acid one-letter symbol. Next, is the secondary structure classification. The final part is the neighbor set hydrophobicity score, as described above. All of the sequence entries for each protein are added together to form one long sequence.", "With sequence, we can now build and train the RNN. I used the model described in this tutorial as my template. First we define alphabet as the unique entries in sequence. Then we create a mapping for each entry in the sequence to a unique integer and use this mapping to create int_sequence, which contains the sequence in integer form.", "For training, sequence needs to be broken down into smaller sequences. I chose seq_length of 100 arbitrarily, but the best value can be determined through experimentation. Using this length, the data set can be split in input examples and targets. First the data is split into chunk of length seq_length + 1. Input examples will contain the first 100 entries and the target will contain entries from 1 to 101. Given an input example, the RNN tries to predict the next entry.", "Next, the training data is lumped into batches. The batch size determines how many training examples to feed the RNN before updating the weights. Updating the weights based on multiple training examples makes the training process more stable. You can learn more about this concept here and there is a nice visualization here.", "Now we can construct the RNN. I used the architecture presented here, which has three layers. There is an embedding layer that creates a dense vector of a given size for each entry in the alphabet. The concept of embeddings can be difficult to grasp at first. I find it helpful to consider that an embedding maps each entry to a location in N-dimensional space, where N is your chosen size of the vector. This is useful because in this N-dimensional space, you can calculate relationships between entries using cosine similarity or euclidean distance. You can learn more about the topic here or by studying the word2vec model, of which embeddings are the core concept.", "The GRU layer is what makes this network an RNN. It provides the network with a short term memory. There is an excellent article on GRUs and a similar architecture, called LSTMs, here. Finally there is the dense layer; this is the output layer of the model. It is the layer which generates a vector the size of the alphabet, which contains logits, or the prediction probabilities. The following code is a handy function for building a network.", "The following code uses the above function to build the model. The first few lines are the parameters of the model, except for batch_size, which was defined earlier. In order to train, the model needs a loss function and an optimizer. The loss function compares the prediction to the true value and calculates the severity of the error. Here, sparse categorical cross entropy is used. Adam is used to optimize the learning rate. You can read more about the Adam optimizer in the authors\u2019 paper.", "The last step before training is to tell the model where to save checkpoints. These are used to load the model weights from different points in the training process.", "I trained the model for 100 epochs, which took under an hour and generated surprisingly good results.", "The following code is a function to generate sequences; it is based on the TensorFlow function described here.", "With the above function, we can prepare the model for sequence generation. The model is rebuilt with the same architecture, except with the batch size set to 1, and the weights from the last checkpoint are loaded.", "Now, the RNN can be used to generate new sequences. In the following code, a seed sequence is chosen randomly and then fed into the network.", "Here are two amino acid sequences generated by the RNN:", "To see if the RNN learned characteristics about the proteins in the data set, I ran blastp on the generated sequences. Most of the top hits for both sequences were for viral proteases or proteinases. And, about 90 of the top 100 hits were for coronavirus proteins. Recall that the RNN was trained on the SARS coronavirus main protease, so these results suggest that the network did learn key characteristics of coronavirus protease.", "This idea is further support by protein structure prediction modeling. The first sequence reported above was modeled using SWISS-MODEL [6], which uses the technique of homology modeling. In this process, a target sequence is submitted to the server, and SWISS-MODEL compares that sequences to a databases of other sequences of proteins with known structure. It is key that the database contains proteins structures, because once a set of similar sequences is found, the structures associated with those sequences can be used as templates. A template is used to construct a structure prediction for the submitted sequence. The SWISS-MODEL prediction for the first generated sequence and the second can be seen above and below, respectively.", "The similarity between the above structures and the structure of the SARS-CoV-2 main protease, shown at the top of the article, is visible. This further supports that the RNN captured sequence characteristics of viral proteases. There are tools like FATCAT that can be used to quantitatively compare these structure, but I will save that for another article.", "I downloaded the PDB files for the generated structures and used the techniques I previously described to create protein music. You can listen to the melodies, harmonies, and combination of the two below. I have two goals for future work. The first goal is to create a much larger data set of similar proteins to be used in training; here the data set was small, but much larger families of proteins could be used. The second goal concerns the type of data used and generated. Here, I used protein sequence data for training and converted the generated sequences into music. In the future, I\u2019m planning to add one more layer of abstraction; I want to use Magenta and train an RNN directly on protein music, so that the output is music. Stay tuned and clap if you enjoyed my work!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Bioinformatics and Computational Biology PhD | Machine Learning and Artificial Intelligence Enthusiast"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fc841aee25126&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprotein-music-with-python-part-3-applying-a-recursive-neural-net-c841aee25126&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprotein-music-with-python-part-3-applying-a-recursive-neural-net-c841aee25126&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprotein-music-with-python-part-3-applying-a-recursive-neural-net-c841aee25126&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprotein-music-with-python-part-3-applying-a-recursive-neural-net-c841aee25126&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://me.dm/@fordcombs", "anchor_text": "Mastodon"}, {"url": "https://towardsdatascience.com/?source=post_page-----c841aee25126--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c841aee25126--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://fordcombs.medium.com/?source=post_page-----c841aee25126--------------------------------", "anchor_text": ""}, {"url": "https://fordcombs.medium.com/?source=post_page-----c841aee25126--------------------------------", "anchor_text": "Ford Combs"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fbb50b09eee87&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprotein-music-with-python-part-3-applying-a-recursive-neural-net-c841aee25126&user=Ford+Combs&userId=bb50b09eee87&source=post_page-bb50b09eee87----c841aee25126---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc841aee25126&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprotein-music-with-python-part-3-applying-a-recursive-neural-net-c841aee25126&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc841aee25126&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprotein-music-with-python-part-3-applying-a-recursive-neural-net-c841aee25126&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "http://alexlenail.me/NN-SVG/index.html", "anchor_text": "NN SVG"}, {"url": "https://levelup.gitconnected.com/turning-a-protein-into-music-with-python-ba655c694097", "anchor_text": "first article"}, {"url": "https://levelup.gitconnected.com/turning-a-protein-into-music-with-python-part-2-harmony-a3c2ffa748be", "anchor_text": "second article"}, {"url": "https://pubs.acs.org/doi/10.1021/acsnano.9b02180", "anchor_text": "paper"}, {"url": "http://cee.mit.edu/people_individual/markus-j-buehler/", "anchor_text": "Markus Buehler\u2019s"}, {"url": "http://lamm.mit.edu/", "anchor_text": "Laboratory for Atomistic and Molecular Mechanics"}, {"url": "https://en.wikipedia.org/wiki/Recurrent_neural_network#Long_short-term_memory", "anchor_text": "recurrent neural network"}, {"url": "https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21", "anchor_text": "excellent article"}, {"url": "https://magenta.tensorflow.org/", "anchor_text": "Magenta"}, {"url": "https://www.tensorflow.org/", "anchor_text": "TensorFlow"}, {"url": "https://pdb101.rcsb.org/learn/videos/coronavirus-main-protease", "anchor_text": "SARS-CoV-2"}, {"url": "https://www.uniprot.org/", "anchor_text": "UniProt"}, {"url": "https://www.rcsb.org/", "anchor_text": "Protein Data Bank"}, {"url": "https://blast.ncbi.nlm.nih.gov/Blast.cgi", "anchor_text": "BLAST"}, {"url": "https://blast.ncbi.nlm.nih.gov/Blast.cgi?PROGRAM=blastp&PAGE_TYPE=BlastSearch&LINK_LOC=blasthome", "anchor_text": "blastp"}, {"url": "https://www.rcsb.org/structure/1Q2W", "anchor_text": "PDB ID 1Q2W"}, {"url": "https://en.wikipedia.org/wiki/Delaunay_triangulation", "anchor_text": "Delaunay Tessellation"}, {"url": "https://web.expasy.org/protscale/pscale/Hphob.Doolittle.html", "anchor_text": "Kyte and Doolittle"}, {"url": "https://blast.ncbi.nlm.nih.gov/Blast.cgi?PAGE=Proteins", "anchor_text": "blastp"}, {"url": "http://www.wwpdb.org/documentation/file-format-content/format33/v3.3.html", "anchor_text": "here"}, {"url": "https://www.tensorflow.org/tutorials/text/text_generation", "anchor_text": "tutorial"}, {"url": "https://towardsdatascience.com/all-you-need-to-know-about-rnns-e514f0b00c7c", "anchor_text": "here"}, {"url": "https://stats.stackexchange.com/questions/153531/what-is-batch-size-in-neural-network", "anchor_text": "here"}, {"url": "https://www.tensorflow.org/tutorials/text/text_generation", "anchor_text": "here"}, {"url": "https://en.wikipedia.org/wiki/Cosine_similarity", "anchor_text": "cosine similarity"}, {"url": "https://en.wikipedia.org/wiki/Euclidean_distance", "anchor_text": "euclidean distance"}, {"url": "https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21", "anchor_text": "here"}, {"url": "https://pathmind.com/wiki/word2vec", "anchor_text": "word2vec"}, {"url": "https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21", "anchor_text": "here"}, {"url": "https://cwiki.apache.org/confluence/display/MXNET/Multi-hot+Sparse+Categorical+Cross-entropy", "anchor_text": "sparse categorical cross entropy"}, {"url": "https://arxiv.org/pdf/1412.6980.pdf", "anchor_text": "paper"}, {"url": "https://www.tensorflow.org/tutorials/text/text_generation#generate_text", "anchor_text": "here"}, {"url": "https://swissmodel.expasy.org/", "anchor_text": "SWISS-MODEL"}, {"url": "https://swissmodel.expasy.org/", "anchor_text": "SWISS-MODEL"}, {"url": "https://en.wikipedia.org/wiki/Homology_modeling", "anchor_text": "homology modeling"}, {"url": "https://swissmodel.expasy.org/", "anchor_text": "SWISS-MODEL"}, {"url": "http://fatcat.godziklab.org/", "anchor_text": "FATCAT"}, {"url": "https://magenta.tensorflow.org/", "anchor_text": "Magenta"}, {"url": "https://doi.org/10.1093/nar/gky1049", "anchor_text": "Nucleic Acids Res. 47: D506\u2013515 (2019)"}, {"url": "http://dx.doi.org/10.1093/nar/28.1.235", "anchor_text": "Nucleic Acids Research, 28: 235\u2013242."}, {"url": "https://www.ncbi.nlm.nih.gov/pubmed/2231712?dopt=Citation", "anchor_text": "PubMed"}, {"url": "https://medium.com/tag/python?source=post_page-----c841aee25126---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----c841aee25126---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/biology?source=post_page-----c841aee25126---------------biology-----------------", "anchor_text": "Biology"}, {"url": "https://medium.com/tag/music?source=post_page-----c841aee25126---------------music-----------------", "anchor_text": "Music"}, {"url": "https://medium.com/tag/sonification?source=post_page-----c841aee25126---------------sonification-----------------", "anchor_text": "Sonification"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc841aee25126&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprotein-music-with-python-part-3-applying-a-recursive-neural-net-c841aee25126&user=Ford+Combs&userId=bb50b09eee87&source=-----c841aee25126---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc841aee25126&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprotein-music-with-python-part-3-applying-a-recursive-neural-net-c841aee25126&user=Ford+Combs&userId=bb50b09eee87&source=-----c841aee25126---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc841aee25126&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprotein-music-with-python-part-3-applying-a-recursive-neural-net-c841aee25126&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c841aee25126--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fc841aee25126&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprotein-music-with-python-part-3-applying-a-recursive-neural-net-c841aee25126&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----c841aee25126---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----c841aee25126--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----c841aee25126--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----c841aee25126--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----c841aee25126--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----c841aee25126--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----c841aee25126--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----c841aee25126--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----c841aee25126--------------------------------", "anchor_text": ""}, {"url": "https://fordcombs.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://fordcombs.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ford Combs"}, {"url": "https://fordcombs.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "62 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fbb50b09eee87&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprotein-music-with-python-part-3-applying-a-recursive-neural-net-c841aee25126&user=Ford+Combs&userId=bb50b09eee87&source=post_page-bb50b09eee87--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fb30850490707&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprotein-music-with-python-part-3-applying-a-recursive-neural-net-c841aee25126&newsletterV3=bb50b09eee87&newsletterV3Id=b30850490707&user=Ford+Combs&userId=bb50b09eee87&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}