{"url": "https://towardsdatascience.com/data-preprocessing-and-interpreting-results-the-heart-of-machine-learning-part-2-pca-feature-92f8f6ec8c8", "time": 1683010117.949204, "path": "towardsdatascience.com/data-preprocessing-and-interpreting-results-the-heart-of-machine-learning-part-2-pca-feature-92f8f6ec8c8/", "webpage": {"metadata": {"title": "Data Preprocessing And Interpreting Results: The Heart Of Machine Learning: Part 2- PCA, Feature Selection And Result Analysis | by Abhijit Roy | Towards Data Science", "h1": "Data Preprocessing And Interpreting Results: The Heart Of Machine Learning: Part 2- PCA, Feature Selection And Result Analysis", "description": "In part 1, we have looked at our dataset and the concepts of the explorative data analysis. In this part, we will take a look at the principal component analysis or PCA and feature selection\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/@myac.abhijit/data-preprocessing-and-interpreting-results-the-heart-of-machine-learning-part-1-eda-49ce99e36655", "anchor_text": "part 1", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5", "anchor_text": "an awesome post", "paragraph_index": 51}, {"url": "https://github.com/abr-98/EDA-and-PCA-demo", "anchor_text": "Github Link", "paragraph_index": 75}, {"url": "https://abhijitroy1998.wixsite.com/abhijitcv", "anchor_text": "https://abhijitroy1998.wixsite.com/abhijitcv", "paragraph_index": 77}], "all_paragraphs": ["In part 1, we have looked at our dataset and the concepts of the explorative data analysis. In this part, we will take a look at the principal component analysis or PCA and feature selection procedures. Let\u2019s start.", "Until now, we have visualized several features singly and the correlations among them. But it is very important for us to visualize the whole data together. For this, we need to project this 30 Dimension data into a 2D feature map. So, a dimension reduction is required. This is where PCA comes in.", "Before we move to PCA let\u2019s explore variance.", "The variance is a measure of how dispersed or spread out the set is, something that the \u201caverage\u201d (mean or median) is not designed to do. If I tell you a data set has a variance of zero, for example, you know that every member of it is the same. If the variance is high, especially as a ratio to the mean, that tells you that the data have some very large dis-similarities amongst its members.", "If the variance is very low, we are basically getting all similar kinds of data from that feature or feature set. The machine learning usually works on dissimilarities in behaviour of Data Points among different classes. So, if it is very low points are expected to cluster around the same points which cause them to be very hard to differentiate. So variance is often regarded as the information in case of ML datasets.", "In PCA we try to create n composite features which can best represent the information contained in the 30 features of our dataset. These n features are called principal components. The value of n depends on the user. These n features are not original features. These features are developed as a combination of different features.", "Before we jump into the understanding and application of PCA, we must take care of one thing. PCA is always conducted on scaled data. So, let\u2019s take a look at scaling.", "The data we use have several features. Now, these features have different ranges depending upon the features like some features have decimal values between 0 to 1 while others have values between 100\u20131000.", "If we use classifier like logistic regression this creates a lot of problems.", "A logistic regression draws a line say represented by this formula. Now, if x1 is from 0 to 1 and x2 from 100\u20131000, evidently x2 will get a lot of stress and weightage. To avoid this scaling is done so that all the values are in a fixed range. It just changes the values to a scale but keeps the distributions intact.", "There are two major types of scaling", "It gives values between -1 to +1.", "2. Min-Max Scaling: It is given by", "It gives values between 0 to 1.", "Let\u2019s return to PCA. We will be using a standard scaler here. Let us first see the application, then we will move to the explanation.", "After dimension reduction, these two components are formed as our n-components is 2. Now we will regard this as our feature set and joining our target set correspondingly we will try to visualize. As this has two data features now it can easily be represented on a 2D plane.", "Now, this is the PCA distribution obtained. This is our full datasets representation if we represent the 30D feature set in 2D in the best possible way. But, this is always not feasible let\u2019s see why.", "PCA is based on an unsupervised algorithm. Now, how are the components obtained, is the question? Here goes the answer.", "We have a 30 Dimensional Dataset. So this means we can plot our points on a 30 Dimensional Plane where each dimension represents a feature. Say, then we color the points based on our target classification. \u201cM\u201d is colored red. \u201cB\u201d is colored Blue. Now, at this moment, we try to draw a 2D plane that is best suited to divide the points based on the target class. We are using 2D because n_components is 2. Now we start taking projections of these 30D points on the 2D plane. So, we are decreasing a 30 length tuple to a 2 length tuple. Now, if we think, we will be very clear that we will lose some information also due to dimensionality reduction. Due to this, the variance decreases. The x and y-axis values are the principal components. So, our target is how to decrease the variance loss. To do this, we usually pick the projectable 2D planes out of a lot of choices.", "Elaborately speaking, first, a 2D plane is chosen that can best differentiate the points. The points are projected and variance loss is calculated, then another plane orthogonal to the previous one is selected and again its variance is calculated. This goes on and on until a lot of choices are obtained and then the plane with least loss is used.", "After all this also, PCA is not always very confident in representing all the information. This can be tested using the variance ratio.", "This does not give a very clear representation as the two components only represent 44+18=62% of the total information represented here. It is not an accurate representation as it only represents 62% of the total information. I,e it can\u2019t be represented using two principal components. It is an estimate but it is said that if the sum of ratios is less than 85%, then the representation is not very correct as a lot of information is missing.", "Still, a lot of information can be extracted from this operation. Like if we see the graph, we can see that if we draw a straight line then it can easily classify the classes. So, for this problem, logistic regression will function much better than KNN type classifiers.", "This is almost all about PCA let\u2019s move to the next topic Feature Selection.", "We sometimes face problems which have a lot of features. They are even large numbers like 99 100 features. Now, some of these features are not very useful in model prediction. But they increase the dimensionality of feature set, thus making it hard to analyze, increases the time for training, and also decrease the accuracy. So such features must be removed. So, in this section, we will be talking about several ways in which we can accomplish the task.", "In this method, we fit the data to a random forest model and obtain the feature importances. Thus obtaining the best features. As we have known the random forests are based on the decision tree algorithm so, they can very easily model even non-linearity and provide a very clear picture of the feature importance. Let\u2019s see the application.", "Thus we fit the decision tree model and obtain the feature importances.", "This is the feature importance obtained from random forest algorithms.", "This is another method that uses the feature importances after fitting the data into a logistic regression model. If the data is linear in nature this method works great. As we know logistic regression\u2019s classification line is given by a linear equation like:", "The weights here are the corresponding feature importances.", "Thus the model is fit and importances are obtained in the weights list.", "This is the feature importance distribution generated by logistic regression.", "This is a very common method that is used to obtain feature importances. This is done by fitting the data to a lasso regression model and obtaining the feature importances. Lasso regression works on the policies of the L1 regularization.", "L1 regularization is used for sparse data where there are a lot of features but all of them are not essential for prediction. In such cases, L1 regularization zeros down the feature importances of the features which are not very essential. Lasso uses the same policy.", "One thing to note is Lasso can\u2019t be used to classify \u201cM\u201d and \u201cB\u201d classes they must be converted to integers.", "So, we fit the data to the lasso model and load the feature importances.", "This is the feature importance list obtained by lasso, we can see a few features weights are reduced to zero, So in the training of the model, they won\u2019t play any part since their weights are 0.", "Recursive Feature Elimination is the best way devised so far to choose the best features. It works on a kind of recursive algorithm. It first takes all the features into account, builds a model predicts, and then finds the least important features. Now, it eliminates those features, rebuilds the model, and again checks the effects of removing the features. This process involves a k fold cross-validation step in order to remove any kind of imbalance problems in the set. We set the estimator or the model that the process uses.", "So, we use a random forest classifier as the estimator. We use a 10 fold cross-validation on each step.", "This is used to obtain the number of features finally obtained after elimination. We can see that after elimination 24 out of 30 features are obtained.", "This obtains the features which have been eliminated in the process. Let\u2019s visualize the results.", "These are the finally left features and their importance after RFE.", "We have covered almost all the important procedures that are used to obtain important features.", "So, let\u2019s move to the next portion, the Result Analysis.", "Another very important part of machine learning is the result analysis or interpretation. Now, normally, we understand results means high accuracy, but it is not so in case of machine learning. In machine learning, accuracy depends on a lot of other factors. The question is why? So, let\u2019s see an example. Say there is a dataset of 100 emails in which 10 are spam. So, we form an 80% train set and a 20% test set. Now, let\u2019s say the train set has 9 spam emails and the test set has 1 spam email. We train our model, but due to the fewer instances of the spam email, it didn\u2019t learn it properly. So, the model classifies everything as not spam. Now, if we use our test data set to evaluate our model then it will give a 90% accuracy because 9 are actually not spam. But our model actually failed. So, we can\u2019t use accuracy only as an evaluation metric.", "There are several methods that have been developed for measuring the results, the most used are the ROC-AUC curve and Precision-Recall.", "ROC or Reciever Operating Characteristic curve is used to visualize the performance of a binary classifier. It shows the trade-off between the true positive rate and false-positive rate.", "True Positive Rate = True Positive / (True Positive+False Negative )", "False Positive Rate = False Positive / (True negative+False Positive)", "Now, how does it work? The classification models actually produce a prediction percentage which denotes how much likely is the event that the given sample belongs to the given class. The prediction percentage can be anything between 0 to 100. Now, it has a threshold value. Below the threshold, the sample is classified as 0 class and above the threshold, it is considered to be 1 class. So, the false positives, true positives everything depends on this threshold. If we decrease the threshold, the number of samples classified as 1 class increases, so false positives increases, else false-negative increases. The ROC curve actually rolls this threshold and points all the True positive and false-positive rates accordingly.", "AUC stands for \u201cArea under the ROC Curve.\u201d That is, AUC measures the entire two-dimensional area underneath the entire ROC curve. AUC provides an aggregate measure of performance across all possible classification thresholds. So, more the AUC more is the chance that the model labels the classes correctly. So, more is the accuracy.", "I found an awesome post, with explaining images that I have used here.", "An excellent model has AUC near to the 1 which means it has good measure of separability. A poor model has AUC near to the 0 which means it has worst measure of separability. In fact it means it is reciprocating the result. It is predicting 0s as 1s and 1s as 0s. And when AUC is 0.5, it means model has no class separation capacity whatsoever.", "Depending upon the model\u2019s capacity there can be 4 cases.", "This is the ideal case where the model predicts every point correctly. In actual data the positive and negative classes are disjoint sets i.e, a point can either be true positive or be true negative but can\u2019t be both. Here we can see that the model covers and classifies all the points correctly. So, the area under the curve is the full 2D box area i.e, 1 and accuracy is 100%", "These graphs show the actual behavior of the models. Here we can see there is an overlap portion where the model predicts a few samples wrong i.e, samples Positive class as a negative class and vice versa. The smaller the overlap portion higher the accuracy of the model. Here we can see that the AUC is 0.7, not 1.", "This is a case where the model has basically not learned anything. So, it keeps predicting two classes on an equal basis. So, half the time it classifies the correct half time it is wrong.", "In this case, the model classifies all positive classes as negatives and negative classes as positives. So, it does a total wrong classification. So, the AUC score is 0, the accuracy is 0%", "This is all about the ROC-AUC curve. Let\u2019s move to the precision-recall.", "These measures are also useful in applied machine learning for evaluating binary classification models.", "Precision= True Positive / (True Positive +False Positive)", "These two metrics are very useful in case of imbalanced datasets.", "If the data set has too many instances of positive class and almost no instances of negative class then the model will be prone to predict positive class and so more false-positive hence low precision. If the data set has too many instances of the negative class and almost no instances of positive class then the model will be prone to predict negative class and so more false-negative hence low recall.", "So, in both cases, these metrics can give us a very clear idea. Let\u2019s see the Precision-Recall curve.", "This is how a precision-recall curve looks like. Precision-Recall does not take into consideration true negatives. So only positive classes are concerned. Now, these graphs have a no skill classifier baseline which serves as the reference. It is a straight line at 0.5. The curve is measured on its basis like if the model classifies best it is a convex and away from the baseline, else it is closer.", "We have studied the different types of performance matrices. So let's implement it on our breast cancer dataset.", "I have used a logistic regression here.", "Results for logistic regression on the dataset.", "So, here the data is fit into our model and the model is ready for prediction.", "Our model gives an accuracy score of 97%. It is a very high accuracy that means our PCA was good enough.", "Let\u2019s obtain the ROC- AUC curve:", "The ROC AUC score or area under the ROC curve is obtained as 0.98", "This is our obtained ROC curve.", "Now, we move to the precision-recall curve.", "This is the precision-recall curve obtained. As we can see it is a lot far away from the baseline. So, our dataset is pretty good and the accuracy is satisfying.", "In this article, we have seen PCA, approaches of feature selection, and the performance metrics. Here is the Github Link. I hope these articles help.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "I am a Computer Science and Technology Graduate from NIT, Durgapur. Find Me at https://abhijitroy1998.wixsite.com/abhijitcv"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F92f8f6ec8c8&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-preprocessing-and-interpreting-results-the-heart-of-machine-learning-part-2-pca-feature-92f8f6ec8c8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-preprocessing-and-interpreting-results-the-heart-of-machine-learning-part-2-pca-feature-92f8f6ec8c8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-preprocessing-and-interpreting-results-the-heart-of-machine-learning-part-2-pca-feature-92f8f6ec8c8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-preprocessing-and-interpreting-results-the-heart-of-machine-learning-part-2-pca-feature-92f8f6ec8c8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----92f8f6ec8c8--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----92f8f6ec8c8--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@myac.abhijit?source=post_page-----92f8f6ec8c8--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@myac.abhijit?source=post_page-----92f8f6ec8c8--------------------------------", "anchor_text": "Abhijit Roy"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4c235a4f4b95&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-preprocessing-and-interpreting-results-the-heart-of-machine-learning-part-2-pca-feature-92f8f6ec8c8&user=Abhijit+Roy&userId=4c235a4f4b95&source=post_page-4c235a4f4b95----92f8f6ec8c8---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F92f8f6ec8c8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-preprocessing-and-interpreting-results-the-heart-of-machine-learning-part-2-pca-feature-92f8f6ec8c8&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F92f8f6ec8c8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-preprocessing-and-interpreting-results-the-heart-of-machine-learning-part-2-pca-feature-92f8f6ec8c8&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@lukechesser?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Luke Chesser"}, {"url": "https://towardsdatascience.com/?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://medium.com/@myac.abhijit/data-preprocessing-and-interpreting-results-the-heart-of-machine-learning-part-1-eda-49ce99e36655", "anchor_text": "part 1"}, {"url": "https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5", "anchor_text": "an awesome post"}, {"url": "https://github.com/abr-98/EDA-and-PCA-demo", "anchor_text": "Github Link"}, {"url": "https://medium.com/tag/principal-component?source=post_page-----92f8f6ec8c8---------------principal_component-----------------", "anchor_text": "Principal Component"}, {"url": "https://medium.com/tag/data-analysis?source=post_page-----92f8f6ec8c8---------------data_analysis-----------------", "anchor_text": "Data Analysis"}, {"url": "https://medium.com/tag/feature-selection?source=post_page-----92f8f6ec8c8---------------feature_selection-----------------", "anchor_text": "Feature Selection"}, {"url": "https://medium.com/tag/roc?source=post_page-----92f8f6ec8c8---------------roc-----------------", "anchor_text": "Roc"}, {"url": "https://medium.com/tag/performance-metrics?source=post_page-----92f8f6ec8c8---------------performance_metrics-----------------", "anchor_text": "Performance Metrics"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F92f8f6ec8c8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-preprocessing-and-interpreting-results-the-heart-of-machine-learning-part-2-pca-feature-92f8f6ec8c8&user=Abhijit+Roy&userId=4c235a4f4b95&source=-----92f8f6ec8c8---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F92f8f6ec8c8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-preprocessing-and-interpreting-results-the-heart-of-machine-learning-part-2-pca-feature-92f8f6ec8c8&user=Abhijit+Roy&userId=4c235a4f4b95&source=-----92f8f6ec8c8---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F92f8f6ec8c8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-preprocessing-and-interpreting-results-the-heart-of-machine-learning-part-2-pca-feature-92f8f6ec8c8&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----92f8f6ec8c8--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F92f8f6ec8c8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-preprocessing-and-interpreting-results-the-heart-of-machine-learning-part-2-pca-feature-92f8f6ec8c8&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----92f8f6ec8c8---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----92f8f6ec8c8--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----92f8f6ec8c8--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----92f8f6ec8c8--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----92f8f6ec8c8--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----92f8f6ec8c8--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----92f8f6ec8c8--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----92f8f6ec8c8--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----92f8f6ec8c8--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@myac.abhijit?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@myac.abhijit?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Abhijit Roy"}, {"url": "https://medium.com/@myac.abhijit/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "458 Followers"}, {"url": "https://abhijitroy1998.wixsite.com/abhijitcv", "anchor_text": "https://abhijitroy1998.wixsite.com/abhijitcv"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4c235a4f4b95&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-preprocessing-and-interpreting-results-the-heart-of-machine-learning-part-2-pca-feature-92f8f6ec8c8&user=Abhijit+Roy&userId=4c235a4f4b95&source=post_page-4c235a4f4b95--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F2ba8066c30a7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-preprocessing-and-interpreting-results-the-heart-of-machine-learning-part-2-pca-feature-92f8f6ec8c8&newsletterV3=4c235a4f4b95&newsletterV3Id=2ba8066c30a7&user=Abhijit+Roy&userId=4c235a4f4b95&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}