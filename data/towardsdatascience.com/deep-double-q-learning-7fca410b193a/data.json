{"url": "https://towardsdatascience.com/deep-double-q-learning-7fca410b193a", "time": 1682993890.676468, "path": "towardsdatascience.com/deep-double-q-learning-7fca410b193a/", "webpage": {"metadata": {"title": "Deep (Double) Q-Learning | Towards Data Science", "h1": "Self Learning AI-Agents III:Deep (Double) Q-Learning", "description": "Double Deep Q-Learning is an algorithm that teaches an AI to behave in discrete action spaces. It shows superior performance compared to vanilla Deep Q-Learning."}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/self-learning-ai-agents-part-ii-deep-q-learning-b5ac60c3f47", "anchor_text": "2nd article", "paragraph_index": 0}, {"url": "https://github.com/artem-oppermann/Deep-Reinforcement-Learning/tree/master/src/double%20q%20learning", "anchor_text": "GitHub Repository", "paragraph_index": 2}, {"url": "https://towardsdatascience.com/self-learning-ai-agents-part-i-markov-decision-processes-baf6b8fc4c5f", "anchor_text": "see the first article", "paragraph_index": 6}, {"url": "https://towardsdatascience.com/self-learning-ai-agents-part-ii-deep-q-learning-b5ac60c3f47", "anchor_text": "second article of the series", "paragraph_index": 7}, {"url": "https://www.deeplearning-academy.com/", "anchor_text": "www.deeplearning-academy.com", "paragraph_index": 13}, {"url": "https://artem-oppermann.medium.com/subscribe", "anchor_text": "https://artem-oppermann.medium.com/subscribe", "paragraph_index": 29}], "all_paragraphs": ["In the 2nd article of the series \u201cSelf Learning AI-Agents\u201d, I presented you Deep Q-Learning as an algorithm that can be used to teach AI to behave and solve tasks in discrete action spaces. However, this approach is not without its shortcomings that can potentiality result in lower performances of the AI agent.", "In the following, I will introduce a common problem of Deep Q-Learning and show you how the vanilla implementation can be extended to what we call Double Deep Q-Learning, which generally leads to better performances of the AI agents.", "This example of OpenAI\u2019s Gym CartPole problem was solved with Double Q-Learning algorithm, presented here \u2014 and some techniques from the last article. The well-documented source code can be found in my GitHub Repository. I have chosen CartPole as an example because the training time for this problem is very low and you can reproduce it yourself very quickly. Clone the repository and execute run_training.py to start the algorithm.", "In the previous two parts of the series, I introduced the action-value function Q(s,a) as the expected return G_t the AI agent would get by starting in state s, taking action a and then following a certain policy \u03c0.", "The right part of the equation is also called the Temporal Difference Target (TD-Target). The TD-Target is the sum of the immediate reward r the agent got for the action a in state s and the discounted value Q(s\u2019,a\u2019), a\u2019 being the action the agent will take in the next state s\u2019.", "Q(s,a) tells the agent the value (or quality) of a possible action a in a particular state s. Given a state s the action-value function calculates the quality/value for each possible action a_i in this state as a scalar value. Higher quality means a better action with regards to the given objective. For an AI agent, a possible objective could be learning how to walk or how to play chess against human players.", "Following a greedy policy w.r.t Q(s,a), meaning taking the actions a\u2019 that result in highest values of Q(s,a\u2019) leads to Bellmann Optimality Equation, which gives a recursive definition for Q(s,a) (see the first article). The Bellman equation can also be used to recursively calculate all values Q(s,a) for any given action or state.", "In the second article of the series, the Temporal Difference Learning was introduced as a better approach to estimate the values Q(s,a). The objective in temporal difference learning was the minimization of the distance between the TD-Target and Q(s,a) which suggests a convergence of Q(s,a) towards its true values in the given environment. This was called Q-Learning.", "We have seen that a neural network approach turns out to be a yet better way to estimate Q(s,a). The main objective stays the same. It is the minimization of the distance between Q(s, a) and TD-Target (or temporal distance of Q(s,a)). This objective can be expressed as minimization of the error loss function:", "In Deep Q-Learning TD-Target y_i and Q(s,a) are estimated separately by two different neural networks, which are often called the Target-, and Q-Networks (Fig. 4). The parameters \u03b8(i-1) (weights, biases) belong to the Target-Network while \u03b8(i) belong to the Q-Network.", "The actions of the AI agents are selected according to the behavior policy \u00b5(a|s). On the other side, the greedy target policy \u03c0(a|s) selects only actions a\u2019 that maximize Q(s, a), that is used to calculate the TD-Target.", "The minimization of the error loss function can be accomplished by usual gradient descent algorithms that are used in deep learning.", "Coming soon: Advanced Deep Learning Education for software developers, data analysts, academics and industry experts to speed up the transition to a career in Artificial Intelligence.", "For more details check out: www.deeplearning-academy.com", "Deep Q-learning is known to sometimes learn unrealistically high action values because it includes a maximization step over estimated action values, which tends to prefer overestimated to underestimated values, as it can be seen in the calculation of the TD-Target y_i.", "It is still a more or less open question whether overestimations negatively affects performances of AI agents in practice. Over-optimistic value estimates are not necessarily a problem in and of themselves. If all values would be uniformly higher then the relative action preferences are preserved and we would not expect the resulting policy to be any worse.", "If, however, the overestimations are not uniform and not concentrated at states about which we wish to learn more, then they might negatively affect the quality of the resulting policy.", "The idea of Double Q-learning is to reduce overestimations by decomposing the max operation in the target into action selection and action evaluation.", "In the vanilla implementation, the action selection and action evaluation are coupled. We are using the Target-Network to select the action and at the same time to estimate the quality of the action. That does this mean?", "The Target-Network calculates Q(s, a_i) for each possible action a_i in state s. The greedy policy decides upon the highest values Q(s, a_i) which action a_i to select. This means that the Target-Network selects the action a_i and at the same time evaluates its quality by calculating Q(s, a_i). Double Q-Learning tries to decouple this both procedures from each other.", "In Double Q-Learning the TD-Target looks as follows:", "As you can see the max operation in the target is gone. While the Target-Network with parameters \u03b8(i-1) evaluates the quality of the action, the action itself is determined by the Q-Network that has parameters \u03b8(i). This procedure is in contrast to the vanilla implementation of Deep Q-Learning where the Target-Network was responsible for action selection and evaluation.", "The calculation of new TD-Target y_i can be summarized in the following steps:", "The process of Double Q-Learning can be once more visualized in a graph for further comprehension (Fig. 2). An AI agent is at the start in state s. He knows based on some previous calculations the qualities Q(s, a_1) and Q(s, a_2) for possible two actions in that states. He decides to take action a_1 and ends up in state s\u2019.", "The Q-Network calculates the qualities Q(s\u2019, a_1') and Q(s, a_2') for possible actions in this new state. Action a_1' is picked because it results in the highest quality according to the Q-Network.", "The new action-value Q(s, a1) for action a_1 in state s can now be calculated with the equation in the Fig. 2 where Q(s\u2019,a_1') is the evaluation of a_1' that is determined by the Target-Network.", "In [1] David Silver et al. tested Deep Q-Networks (DQNs) and Deep Double Q-Networks (Double DQNs) on several Atari 2600 games. The normalized scores achieved by AI agents of those two methods as well as the comparable human performance are shown in the Fig. 3. The figure also contains a tuned version of Double DQN where some hyper parameter optimization was performed. This version of DQN will not be covered here however.", "It can be clearly noticed that these two different versions of Double DQNs achieve better performances in this area than it\u2019s vanilla implementation.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Deep Learning & AI Software Developer | MSc. Physics | https://artem-oppermann.medium.com/subscribe"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F7fca410b193a&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-double-q-learning-7fca410b193a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-double-q-learning-7fca410b193a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-double-q-learning-7fca410b193a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-double-q-learning-7fca410b193a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----7fca410b193a--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----7fca410b193a--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://artem-oppermann.medium.com/?source=post_page-----7fca410b193a--------------------------------", "anchor_text": ""}, {"url": "https://artem-oppermann.medium.com/?source=post_page-----7fca410b193a--------------------------------", "anchor_text": "Artem Oppermann"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F619319ac8220&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-double-q-learning-7fca410b193a&user=Artem+Oppermann&userId=619319ac8220&source=post_page-619319ac8220----7fca410b193a---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7fca410b193a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-double-q-learning-7fca410b193a&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7fca410b193a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-double-q-learning-7fca410b193a&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/self-learning-ai-agents-part-i-markov-decision-processes-baf6b8fc4c5f", "anchor_text": "Part I: Markov Decision Processes"}, {"url": "https://towardsdatascience.com/self-learning-ai-agents-part-ii-deep-q-learning-b5ac60c3f47", "anchor_text": "Part II: Deep Q-Learning"}, {"url": "https://www.linkedin.com/in/artem-oppermann-929154199/?locale=en_US", "anchor_text": "LinkedIn"}, {"url": "https://towardsdatascience.com/self-learning-ai-agents-part-ii-deep-q-learning-b5ac60c3f47", "anchor_text": "2nd article"}, {"url": "https://github.com/artem-oppermann/Deep-Reinforcement-Learning/tree/master/src/double%20q%20learning", "anchor_text": "GitHub Repository"}, {"url": "https://towardsdatascience.com/self-learning-ai-agents-part-i-markov-decision-processes-baf6b8fc4c5f", "anchor_text": "see the first article"}, {"url": "https://towardsdatascience.com/self-learning-ai-agents-part-ii-deep-q-learning-b5ac60c3f47", "anchor_text": "second article of the series"}, {"url": "https://www.deeplearning-academy.com/", "anchor_text": "www.deeplearning-academy.com"}, {"url": "https://www.deeplearning-academy.com/", "anchor_text": "www.deeplearning-academy.com"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----7fca410b193a---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----7fca410b193a---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----7fca410b193a---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/data-science?source=post_page-----7fca410b193a---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----7fca410b193a---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7fca410b193a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-double-q-learning-7fca410b193a&user=Artem+Oppermann&userId=619319ac8220&source=-----7fca410b193a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7fca410b193a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-double-q-learning-7fca410b193a&user=Artem+Oppermann&userId=619319ac8220&source=-----7fca410b193a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7fca410b193a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-double-q-learning-7fca410b193a&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----7fca410b193a--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F7fca410b193a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-double-q-learning-7fca410b193a&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----7fca410b193a---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----7fca410b193a--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----7fca410b193a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----7fca410b193a--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----7fca410b193a--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----7fca410b193a--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----7fca410b193a--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----7fca410b193a--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----7fca410b193a--------------------------------", "anchor_text": ""}, {"url": "https://artem-oppermann.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://artem-oppermann.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Artem Oppermann"}, {"url": "https://artem-oppermann.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "3.8K Followers"}, {"url": "https://artem-oppermann.medium.com/subscribe", "anchor_text": "https://artem-oppermann.medium.com/subscribe"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F619319ac8220&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-double-q-learning-7fca410b193a&user=Artem+Oppermann&userId=619319ac8220&source=post_page-619319ac8220--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fc09555d6711e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-double-q-learning-7fca410b193a&newsletterV3=619319ac8220&newsletterV3Id=c09555d6711e&user=Artem+Oppermann&userId=619319ac8220&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}