{"url": "https://towardsdatascience.com/generating-piano-music-with-dilated-convolutional-neural-networks-d81d02e1dda6", "time": 1683013091.951099, "path": "towardsdatascience.com/generating-piano-music-with-dilated-convolutional-neural-networks-d81d02e1dda6/", "webpage": {"metadata": {"title": "Generating Piano Music with Dilated Convolutional Neural Networks | by Thomas Angsten | Towards Data Science", "h1": "Generating Piano Music with Dilated Convolutional Neural Networks", "description": "Fully convolutional neural networks consisting of dilated 1D convolutions are straightforward to construct, easy to train, and can generate realistic piano music, such as the following: A\u2026"}, "outgoing_paragraph_urls": [{"url": "https://openai.com/blog/musenet/", "anchor_text": "Musenet", "paragraph_index": 1}, {"url": "https://towardsdatascience.com/generate-piano-instrumental-music-by-using-deep-learning-80ac35cdbd2e", "anchor_text": "previous TDS post", "paragraph_index": 1}, {"url": "https://deepmind.com/", "anchor_text": "DeepMind", "paragraph_index": 3}, {"url": "https://arxiv.org/abs/1609.03499", "anchor_text": "WaveNet", "paragraph_index": 3}, {"url": "https://deepmind.com/blog/article/wavenet-generative-model-raw-audio", "anchor_text": "piano music", "paragraph_index": 3}, {"url": "https://github.com/angsten/pianonet", "anchor_text": "Github", "paragraph_index": 7}, {"url": "https://soundcloud.com/tom-angsten", "anchor_text": "SoundCloud", "paragraph_index": 7}, {"url": "https://towardsdatascience.com/understanding-2d-dilated-convolution-operation-with-examples-in-numpy-and-tensorflow-with-d376b3972b25", "anchor_text": "TDS blog post", "paragraph_index": 32}, {"url": "https://arxiv.org/abs/1611.09482", "anchor_text": "Fast Wavenet Generation Algorithm", "paragraph_index": 60}, {"url": "https://github.com/angsten?tab=repositories", "anchor_text": "PianoNet", "paragraph_index": 60}, {"url": "https://github.com/angsten?tab=repositories", "anchor_text": "PianoNet", "paragraph_index": 63}, {"url": "https://github.com/angsten?tab=repositories", "anchor_text": "PianoNet", "paragraph_index": 77}], "all_paragraphs": ["Fully convolutional neural networks consisting of dilated 1D convolutions are straightforward to construct, easy to train, and can generate realistic piano music, such as the following:", "A considerable amount of research has been devoted to training deep neural networks that can compose piano music. For example, Musenet, developed by OpenAI, has trained large-scale transformer models capable of composing realistic piano pieces that are many minutes in length. The model used by Musenet adopts many of the technologies, such as attention layers, that were originally developed for NLP tasks. See this previous TDS post for more details on applying attention-based models to music generation.", "Although NLP-based methods are a fantastic fit for machine-based music generation (after all, music is like a language), the transformer model architecture is somewhat involved, and proper data preparation and training can require great care and experience. This steep learning curve motivates my exploration of simpler approaches to training deep neural networks that can compose piano music. In particular, I\u2019ll focus on fully convolutional neural networks based on dilated convolutions, which require only a handful of lines of code to define, take minimal data preparation, and are easy to train.", "In 2016, DeepMind researchers introduced the WaveNet model architecture,\u00b9 which yielded state-of-the-art performance in speech synthesis. Their research demonstrated that stacked 1D convolutional layers with exponentially growing dilation rates can process sequences of raw audio waveforms extremely efficiently, leading to generative models that can synthesize convincing audio from a variety of sources, including piano music.", "In this post, I build upon DeepMind\u2019s research, with an explicit focus on generating piano music. Instead of feeding the model raw audio from recorded music, I explicitly feed the model sequences of piano notes encoded in Musical Instrument Digital Interface (MIDI) files. This facilitates data collection, drastically reduces computational load, and allows the model to focus entirely on the musical aspects of the data. This efficient data encoding and ease of data collection enables rapid exploration of how well fully-convolutional networks can understand piano music.", "To give a sense of how realistic these models can sound, let\u2019s play an imitation game. Which excerpt below is composed by a human, and which is composed by a model:", "Maybe you anticipated this trick, but both compositions were produced by the model described in this post. The model generating the above two pieces took only four days to train on a single NVIDIA Tesla T4 with 100 hours of classical piano music in the training set.", "I hope the quality of these two performances provides you with motivation to read on and explore how to build your own models for generating piano music. The code described in this project can be found at PianoNet\u2019s Github, and more example compositions can be found at PianoNet\u2019s SoundCloud.", "Now, let\u2019s dive into the details of how to train a model to produce piano music like the above examples.", "When beginning any machine learning project, it\u2019s good practice to clearly define the task we\u2019re trying to accomplish, the experience from which our model will learn, and the performance measure(s) we\u2019ll use to determine if our model is improving at the task.", "Our overarching goal is to produce a model that efficiently approximates the data generating distribution, P(X). This distribution is a function that maps any sequence of piano notes, X, to a real number ranging between 0 and 1. In essence, P(X) assigns larger values to sequences that are more likely to have been created by skilled human composers. For example, if X\u00b9 is a composition consisting of 200 hours of randomly selected notes, and X\u00b2 is a Mozart sonata, then P(X\u00b9) < P(X\u00b2). Further, P(X\u00b9) will be very near zero.", "In practice, the distribution P(X) can never be exactly determined, as this would require gathering all the human composers that could ever exist into a room and making them write piano music for all of eternity. However, the same incompleteness exists for simpler data generating distributions. For example, exactly determining the distribution of human heights requires all possible humans to exist and be measured, but this doesn\u2019t stop us from defining and approximating such a distribution. In this sense, the P(X) defined above is a useful mathematical abstraction that encompasses all possible factors determining how piano music is generated.", "If we estimate P(X) well enough with a model, we can use that model to stochastically sample new, realistic compositions that have never been heard before. This definition is still a little abstract, so let\u2019s apply some sensible approximations to make the estimation of P(X) more tractable.", "Data Encoding: We need to encode piano music in a way that a computer can understand. To do this, we will represent a piano composition as a variable-length time series of binary states, each state tracking whether or not a given note on the keyboard is being pressed down by a finger during a time step:", "The data processing inequality tells us that information can only ever be lost when we process information,\u00b2 and our chosen method of encoding of piano music is no exception. There are two ways information loss can happen in this case. First, resolution in the time direction must be limited by making the time steps finite. Luckily, a rather large step size of 0.02 seconds still leads to negligible reduction in music quality. Second, we do not represent the velocity with which keys are being pressed, and thus, musical dynamics are lost.", "Despite significant approximations, this encoding method captures much of the underlying information in a concise and machine-friendly form. This is because a piano is effectively a big mechanical finite state machine. Efficiently encoding the music of other, more nuanced instruments, such as a guitar or a flute, would likely be much harder.", "Now that we have an encoding scheme, we can represent the data generating distribution in a more concrete form:", "As an example, at t=1, x\u2081 = 1 if the first note on a piano is pressed down during the first time step.", "Factorizing the Distribution: Another simplification involves factorizing the joint probability distribution in (1) using the chain rule of probability:", "Similar to n-gram language models, we can make a Markov assumption that notes having occurred more than N time steps in the past have no impact on whether a note at time t=n is pressed. This allows us to rewrite each of the factors in (2) using, at most, the last N note states:", "Note that, at the beginning of a song, the note history can be padded with up to N zeros so that there is always a history of at least N notes. Also, note that N must be in the hundreds of time steps (many seconds of history) for this approximation to work well for piano music. We will later see that N is determined by the receptive field of our model, and is a key determinant in the quality of performances.", "At last, we have covered enough mathematical background to rigorously define the task of this project: Using a dataset of encoded sequences of real piano music, train an estimator p\u0302 that gives the next note\u2019s probability of being pressed down given the last N note states. We can then repeatedly sample the next note using p\u0302, updating the note history after each sampling, auto-regressively creating an entirely new composition.", "In this project, our estimator p\u0302 will be a fully convolutional deep neural network. But, before we can talk about model architectures or training, we need to collect some data.", "The data our model sees during training will largely determine the quality and style of its generated music. Further, the joint probability distribution we are trying to estimate is very high-dimensional and is prone to issues of data sparsity. This can be overcome by using sufficiently large amounts of training data and appropriate model selection. Regarding the former, piano music is quite easy to collect and preprocess en masse for two reasons.", "First, there is a preponderance of piano music on the internet in the form of MIDI files. These files are essentially sequences of piano key states, and minimal preprocessing is needed to get to the encoding we desire for our training data. Second, the task we are trying to accomplish is self-supervised learning, in which our target labels can be automatically generated from the data without manual labeling:", "Figure 2 shows how training instances for this task are collected. First, a sequence of binary key states is selected from a song. Then, these states are split into two sub-sequences; an input sequence and then a target sequence of equal size, but with indices shifted forward by one note. All said and done, building our dataset is as simple as downloading quality piano midi files from the internet and manipulating arrays.", "It\u2019s important to note that the styles of music we include in our training set will largely determine the subspace of the data generating distribution, P(X), that we end up estimating. If we feed the model sequences of Bach and Mozart, it will obviously not learn to generate the jazz music of Thelonious Monk. Due to data collection considerations, I focus on input data from the most famous classical composers, including Bach, Chopin, Mozart, and Beethoven. However, extending this work to different styles of piano music would only require augmenting the dataset with representative examples of these additional styles.", "PianoNet is essentially a self-supervised binary classifier that is called repeatedly to predict whether the next key state is up or down. For any probabilistic binary classifier, we can maximize the model\u2019s predicted likelihood of the dataset by minimizing Cross-Entropy loss over the training data:", "During training, our model is fed long sequences of note-state inputs and targets sampled from human-composed piano music. At each training step, the model uses a finite note history to predict the probability of the next piano key state being pressed. The model will be strongly penalized according to (4) if the predicted probability is close to zero when the true key state is pressed down, or if the predicted probability is close to one when the true key state is not pressed.", "There is a potential problem with this loss function. Our ultimate goal is not to create a model that predicts a single next note based on past notes in a human-created composition, but rather to create extended sequences of notes based on a history of notes that the model itself generated. In this sense, we are making a very strong assumption: As a model improves at predicting the next piano note\u2019s state when given a history of human-generated note states, it will also generally improve at generating extended performances auto-regressively, using its own output as the note history.", "It turns out, this assumption does not always hold, as models with relatively low validation losses can still sound very unmusical when generating performances. Below, I show that this assumption does not apply to shallow networks, but for sufficiently deep networks, it applies well in practice.", "We now know how to define the task and can collect large amounts of properly encoded data, but what model should we train on the data to perform better at the task of piano composition? As a good starting point, our model should have a few properties that should improve its ability to generalize to unseen data:", "Fully convolutional neural networks composed of stacked dilated convolution layers are a simple but effective choice for satisfying the above properties. The dilated convolution is like a traditional convolution, except that there may be a gap of length one or more between kernel inputs. See this TDS blog post for a more detailed overview of dilated convolutions, although we\u2019re using 1D convolutions in this case, not 2D.", "Similar to the approach used in the WaveNet paper,\u00b9 we will construct our model using an exponentially growing dilation rate within each \u2018block\u2019, and stack multiple blocks to create the full network. Below is example Tensorflow code for constructing a model with two blocks, each block having seven layers. The network gets wider and wider throughout, such that the second block has many more filters in each layer than the first. Finally, the model ends with a single filter whose output is run through a sigmoid activation, and this output will be the predicted probability that the next note state is one.", "The line dilation_rate = 2**i causes the dilation rate in each block to start at one (like a conventional convolution with kernel size two), then exponentially increase with each subsequent layer in the block. Once the second block begins, this dilation rate resets to size one, and starts increasing exponentially as before.", "We could have let the dilation rate continue to increase exponentially without ever resetting, but this would cause the receptive field to grow too rapidly per model depth. We also do not have to increase the number of filters with each new layer. We could instead let the filter count of each layer always be equal to some large number, like 64. However, while experimenting, I found that starting with a small number of filters and slowly increasing the count results in a more statistically efficient model. Figure 3 shows a schematic model with two blocks:", "A final note is required on the padding='valid' argument of each Conv1D layer instantiation. We want our model input to only ever see notes that occurred before the current predicted note - otherwise we would be allowing future information that will not be present at inference time to leak into our predictions. We also do not want to pad an input sequence of notes with zeros (silence). Input sequences may start in the middle of a song, and padding with silence would create an artificial input state where silence is suddenly interrupted by an internal song fragment. The \u2018valid\u2019 setting pads the sequences in a way that satisfies the above two requirements.", "Although it\u2019s easy to procure a large dataset, and the model architectures we\u2019ve discussed are rather simple to construct, training a model that produces realistic music is still an art. Here are some tips to help you to overcome many challenges I was faced with.", "When I began training models, I started with single-block shallow network architectures, having around 15 layers, with large numbers of weights per layer. Although the validation loss showed that these models were learning from the data and not overfitting, I was dismayed to hear performances like the following:", "The shallow network\u2019s performance starts somewhat musically, but as it gets farther and farther from the human-generated seed, it eventually devolves into chaos, eventually giving way to mostly silence.", "Surprisingly, when I increased network depth by adding more blocks, but held the number of parameters constant, the performance quality increased dramatically even when validation loss did not decrease:", "The shallow and deep networks that generated the performances above have nearly the same loss on the validation set, yet the deep network is able to form coherent musical phrases, while the shallow network is\u2026avant-garde. Note that the above two networks are relatively small with narrow receptive fields of 2.6 seconds and 10\u201340 layers. The more advanced models shown at the beginning of this post have closer to 100 layers and a receptive field of almost 10 seconds. Also, these results are not random. I\u2019ve listened to many minutes of music from both the shallow and deep model using many different seeds, and the pattern remains consistent.", "Why doesn\u2019t validation loss tell the whole story? Remember that the loss we\u2019re using measures the model\u2019s ability to predict the next note given a large history of human-generated notes. This will relate to, but not entirely capture, a model\u2019s ability to auto-regress a clean performance using its own generated note history for long stretches without devolving into a space within P(X) for which it has not been given representative training data. The Cross-Entropy loss given in (4) is an intrinsic measure that is convenient for training, but the subjective quality of the performances is the true extrinsic measure.", "I have supported why low validation loss doesn\u2019t imply a good sounding performance, but why does adding depth to our network tend to improve musicality for the same loss? It\u2019s likely that statistical efficiency is the determining factor here. Deep-and-thin convolutional neural networks likely represent the family of functions we are trying to estimate more efficiently than their shallow-and-wide counterparts. The process of composing music, like many human tasks, seems to be better represented as a series of many compositions of simpler functions, as opposed to that of a few relatively complex functions. It is likely that the shallow networks are \u2018memorizing\u2019 common patterns in the note history, while the deeper networks are encoding these patterns into more meaningful representations that vary less with small variations in the input.", "There is one final heuristic regarding depth that I\u2019ve found improves performance and reduces training time. The receptive field of each additional block should shrink by a factor of two relative to the last block. This entails each subsequent block having one fewer layer than the last. This greatly reduces the number of convolution operations, as the widest layers (those near the output layer) have the smallest receptive fields. At the same time, the overall receptive field of the model is not greatly affected if the first layers have rather large receptive fields.", "Each additional block added to the network adds significant depth \u2014 around ten layers each block. Thus, getting these deep networks to train can quickly be tricky. To combat exploding gradients during training, the learning rate needs to be rather small, usually around 0.0001, which means the model must train for many more steps before it converges on a good solution.", "There is a silver lining, however. Because we have access to so much data (billions of note states), it\u2019s difficult for a modest-sized model with a million or so parameters to overfit the dataset. This effectively means that, the longer you let your model train, the better it will sound. The validation loss will decrease less and less as training progresses, but the model continues to learn extremely important elements of the music late into training.", "To prove how important it is to let the model train sufficiently long, here is a set of mini performances showing how a given model learns as training progresses. Each mini performance has the same human-composed seed, consisting of a snippet of Beethoven\u2019s F\u00fcr Elise. With each repetition, the model is given more and more training time before it is asked to complete the seed:", "We see that, it\u2019s not until the last 20% of training wall-time that the model produces musically sensible phrasings.", "To speed up training a bit, I often start with a batch size of one and slowly increase it to 32. This encourages exploration across the parameter space and limits the amount of compute resources required. We can get away with an initial batch size of one because each individual training sample can contain hundreds of predictions across a song segment (see the input and target sequences in Figure 2).", "Most of the time, a piano key is not being pressed. Only the most experimental artists will ever press more than a few keys at any time step. This makes it very important to set the bias correctly in our output layer:", "By setting bias to -3.2, our model predicts the correct base rate of notes being pressed on average (around 0.04) from the outset. The model no longer has to spend many training steps figuring out that piano keys simply aren\u2019t pressed that often. This removes the hockey stick learning curve and speeds up training a bit.", "In the same way that rotating and scaling images can help models generalize better in computer vision tasks, we can stretch and compress our 1D sequences of piano notes by a few percent to further augment the training data. The idea is that a given piano piece could have been played a little faster or a little slower without altering its chance of being sampled from the data generating distribution. This helps considerably in the fight against data sparsity, and it also helps the model learn to be invariant to small changes in tempo. I usually create five clones of each input song, stretched randomly between 15% slower and 15% faster.", "We should not sample random segments of all songs when building the training and validation sets. Piano songs often repeat entire sections, so if we use a random sampling approach, we likely would end up inadvertently training on the validation set.", "To avoid this, we must ensure that the training and validation sets are assigned entirely distinct songs, before splitting them into sample segments. As long as the number of songs sampled from each composer is in the hundreds, we will still will get a reasonable estimate of the model\u2019s generalization error on unseen music.", "On average, different composers will have higher or lower musical entropies. That is, composers\u2019 compositions will be more or less unpredictable, in that there will be more or less uncertainty in what note may follow the previous notes. In general, earlier composers, such as Bach or Haydn, will have lower uncertainty in their data generating distributions, while later composers, such as Prokofiev or Ravel, will have many more possible notes given a history of notes.", "Due to this varying degree of entropy, don\u2019t be surprised if more model capacity, training time, and/or training data is required for your model to reproduce more unpredictable styles. In my experience, the models described in this post often can easily replicate Bach\u2019s works with a hundred songs, while getting a model to produce compelling Chopin music seems to take much more input data.", "Once you have a trained model, you\u2019ll want to use it to generate new compositions. To generate a performance, we start with an input seed equal in length to the model\u2019s receptive field. This seed can be part of a song written by a human, or simply silence. We can then call the model.predict on this input seed to generate a probability that the state of the first key in the next time step is pressed down. If this prediction is 0.01, then we will sample the next key being pressed down with 1% probability.", "If we drop the first note from the input seed, and add the model\u2019s last sampled note state to the end, then we once again have an input vector equal in length to the model\u2019s receptive field. We can use the new input to generate yet another key state, this time using the model\u2019s last output as part of the input. Finally, we can repeat this process indefinitely, at some point the input being entirely generated by our model. This is the core idea behind autoregression.", "Although the above approach will work just fine, it could take hours to generate a minute of music. This is because each model.predict(input) call requires a very large number of successive convolution operations to compute the final output, and the compute time scales poorly with model depth.", "It turns out we can speed up these calculations significantly if we store some information from past convolution operations in a queue. This is the idea behind the Fast Wavenet Generation Algorithm. How this algorithm works is beyond the scope of this post, but rest assured, I\u2019ve implemented a pure Python version in the PianoNet package described in more detail below. If you use this package, even with a very large network containing millions of parameters, generating a minute of piano music will take on the order of five minutes, rather than hours.", "One last detail regarding performance generation involves constraining the model from drifting. Because our architecture is invariant with respect to key changes, it does not encode which octave a key state is in. This means the model treats all octaves on the keyboard as equally likely, and can drift toward the edges of the piano key states. We don\u2019t want this to happen, because composers tend to keep the pressed keys closer to the middle of the piano.", "To combat this drift in the simplest way possible, I\u2019ve applied what I call edge aversion. This approach is a way to bias the model away from the edges during performances without altering the model\u2019s output distribution too much. In essence, edge aversion enforces that the very highest and very lowest notes on a piano keyboard are only played when the model predicts a very high probability. For example, if the model predicts that the highest key on the piano will be pressed with a probability of 0.05, the key will still never be randomly sampled as pressed.", "At this point, you may be excited to start training your own models for generating piano music. To make this process as easy as possible, I\u2019ve created PianoNet, a Python package for easily reproducing the work I describe above.", "For a full end-to-end tutorial on how to use the package, please see the readme file contained in the linked repo. In the below sections, I\u2019ll briefly talk about the abstractions used for controlling data collection, model training, and performance generation. The basic steps for any workflow are always:", "Step one is to find a handful of midi files containing piano performances in the styles you want the model to learn. The internet is a great place to find free piano midi files from all sorts of composers. Once you\u2019ve collected these files, move them all to a directory of your choosing. I'll assume they\u2019re located at /path/to/midi/ in this example.", "To train models using the PianoNet package, we need to extract all of our midi files into training and validation MasterNoteArray instances saved to disk. A MasterNoteArray object is essentially a 1D array of all input songs concatenated in a way that is amenable to training. We can create a data description json file for generating the MasterNoteArray, such as the following:", "Make sure to change the /path/to/midi/ path above to wherever you store your midi files. For more augmentations (stretching of each song), increase the num_augmentations_per_midi_file from five to a larger number.", "Finally, we can generate our input datasets using the following command:", "This script will generate two .mna_jl files, one containing the training data, one containing the validation data.", "Once we have our MasterNoteArray datasets, we can start model training sessions, or runs. Each run sits in its own directory, and has a single run_description.json file describing how the run should be performed:", "It\u2019s important that training_master_note_array_path and validation_master_note_array_path both point to the .mna_jl files you created in the dataset creation step.", "The model_description parameter describes how the model should be generated. Each list in filter_increments controls a block, with each number representing the increase in filter count for the current layer relative to the previous layer\u2019s filter count. Adding more lists would increase the model capacity by adding depth (more blocks), while increasing the filter increment magnitudes would add width to the model. The rest of the descriptor fields are fairly self-explanatory, and they allow you to control hyperparameters like batch size, how many predicted notes each input sample sequence contains, and arguments of the optimizer. The above numbers all represent sensible defaults.", "Once you\u2019ve created the run_description.json file and have located it in the run directory, you can start the training using:", "This will start a training session in you terminal. To view the output logs for the run, start another terminal in the run directory and use tail -f training_output.txt to see the logs in real time as training progresses.", "Models will be periodically saved in the models directory within the run path, with files names like 0_trained_model. If training is stopped, it can be restarted using the above command and will start from the last saved model.", "The last step, once you\u2019ve trained the model sufficiently, is to listen to how it sounds. This involves inputting your trained model\u2019s file path and a seed to the performance tools method. I\u2019ve created a jupyter notebook overviewing how to do this step by step, which can be found at pianonet/examples/pianonet_mini/get_performances.ipynb.", "Fully convolutional neural networks based on dilated convolutions can successfully generate convincing piano music when trained correctly. These models can be fed raw sequences of notes, are straightforward to build, and can be easy to train with the help of a few tricks. Best of all, they can generate music almost realtime if the Fast-WaveNet algorithm is used. PianoNet is a python package that makes it easy to reproduce all of these features.", "Although performances from these fully-convolutional networks sound quite good, they are lacking in some respects compared to the transformer-based architectures mentioned in the introduction. Because fully-convolutional networks are effectively constrained to have receptive fields shorter than twenty seconds, they are incapable of learning song patterns extending beyond this time scale, such as the repeating structure of the sonata form. It\u2019s also difficult for the convolutional models to learn melodic motifs, something that NLP-based transformer models seem to do quite well.", "Despite these shortcomings, the fully-convolutional architectures described in this post are relatively simple, easy to work with, and are a quick way to have some fun exploring the possibilities of machine-generated art.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Machine learning engineer obsessed with exploring the limits of deep learning."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fd81d02e1dda6&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerating-piano-music-with-dilated-convolutional-neural-networks-d81d02e1dda6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerating-piano-music-with-dilated-convolutional-neural-networks-d81d02e1dda6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerating-piano-music-with-dilated-convolutional-neural-networks-d81d02e1dda6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerating-piano-music-with-dilated-convolutional-neural-networks-d81d02e1dda6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----d81d02e1dda6--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d81d02e1dda6--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@tangst10?source=post_page-----d81d02e1dda6--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@tangst10?source=post_page-----d81d02e1dda6--------------------------------", "anchor_text": "Thomas Angsten"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8ec0edfee770&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerating-piano-music-with-dilated-convolutional-neural-networks-d81d02e1dda6&user=Thomas+Angsten&userId=8ec0edfee770&source=post_page-8ec0edfee770----d81d02e1dda6---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd81d02e1dda6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerating-piano-music-with-dilated-convolutional-neural-networks-d81d02e1dda6&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd81d02e1dda6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerating-piano-music-with-dilated-convolutional-neural-networks-d81d02e1dda6&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://openai.com/blog/musenet/", "anchor_text": "Musenet"}, {"url": "https://towardsdatascience.com/generate-piano-instrumental-music-by-using-deep-learning-80ac35cdbd2e", "anchor_text": "previous TDS post"}, {"url": "https://deepmind.com/", "anchor_text": "DeepMind"}, {"url": "https://arxiv.org/abs/1609.03499", "anchor_text": "WaveNet"}, {"url": "https://deepmind.com/blog/article/wavenet-generative-model-raw-audio", "anchor_text": "piano music"}, {"url": "https://github.com/angsten/pianonet", "anchor_text": "Github"}, {"url": "https://soundcloud.com/tom-angsten", "anchor_text": "SoundCloud"}, {"url": "https://towardsdatascience.com/understanding-2d-dilated-convolution-operation-with-examples-in-numpy-and-tensorflow-with-d376b3972b25", "anchor_text": "TDS blog post"}, {"url": "https://arxiv.org/abs/1611.09482", "anchor_text": "Fast Wavenet Generation Algorithm"}, {"url": "https://github.com/angsten?tab=repositories", "anchor_text": "PianoNet"}, {"url": "https://github.com/angsten?tab=repositories", "anchor_text": "PianoNet"}, {"url": "https://github.com/angsten?tab=repositories", "anchor_text": "PianoNet"}, {"url": "https://arxiv.org/abs/1609.03499", "anchor_text": "https://arxiv.org/abs/1609.03499"}, {"url": "https://en.wikipedia.org/wiki/Data_processing_inequality", "anchor_text": "https://en.wikipedia.org/wiki/Data_processing_inequality"}, {"url": "https://arxiv.org/abs/1611.09482", "anchor_text": "https://arxiv.org/abs/1611.09482"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----d81d02e1dda6---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----d81d02e1dda6---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/tensorflow?source=post_page-----d81d02e1dda6---------------tensorflow-----------------", "anchor_text": "TensorFlow"}, {"url": "https://medium.com/tag/wavenet?source=post_page-----d81d02e1dda6---------------wavenet-----------------", "anchor_text": "Wavenet"}, {"url": "https://medium.com/tag/editors-pick?source=post_page-----d81d02e1dda6---------------editors_pick-----------------", "anchor_text": "Editors Pick"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd81d02e1dda6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerating-piano-music-with-dilated-convolutional-neural-networks-d81d02e1dda6&user=Thomas+Angsten&userId=8ec0edfee770&source=-----d81d02e1dda6---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd81d02e1dda6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerating-piano-music-with-dilated-convolutional-neural-networks-d81d02e1dda6&user=Thomas+Angsten&userId=8ec0edfee770&source=-----d81d02e1dda6---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd81d02e1dda6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerating-piano-music-with-dilated-convolutional-neural-networks-d81d02e1dda6&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d81d02e1dda6--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fd81d02e1dda6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerating-piano-music-with-dilated-convolutional-neural-networks-d81d02e1dda6&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----d81d02e1dda6---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----d81d02e1dda6--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----d81d02e1dda6--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----d81d02e1dda6--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----d81d02e1dda6--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----d81d02e1dda6--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----d81d02e1dda6--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----d81d02e1dda6--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----d81d02e1dda6--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@tangst10?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@tangst10?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Thomas Angsten"}, {"url": "https://medium.com/@tangst10/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "54 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8ec0edfee770&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerating-piano-music-with-dilated-convolutional-neural-networks-d81d02e1dda6&user=Thomas+Angsten&userId=8ec0edfee770&source=post_page-8ec0edfee770--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F8ec0edfee770%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerating-piano-music-with-dilated-convolutional-neural-networks-d81d02e1dda6&user=Thomas+Angsten&userId=8ec0edfee770&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}