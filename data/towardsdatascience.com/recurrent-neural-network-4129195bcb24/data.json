{"url": "https://towardsdatascience.com/recurrent-neural-network-4129195bcb24", "time": 1683018138.635629, "path": "towardsdatascience.com/recurrent-neural-network-4129195bcb24/", "webpage": {"metadata": {"title": "Recurrent Neural Network. Clear understanding on when to use\u2026 | by Namrata Kapoor | Towards Data Science", "h1": "Recurrent Neural Network", "description": "In my last blog about NLP I had taken topics of Bag of Words, tokenization, TF-IDF, Word2Vec, all of these had a problem like they don\u2019t store the information of semantics. It is important\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/nlp-easy-explanation-of-common-terms-with-python-dc7c323a4691", "anchor_text": "blog", "paragraph_index": 1}, {"url": "https://towardsdatascience.com/neural-network-its-internal-functioning-and-uses-7adc4d37f3d8", "anchor_text": "forward propagation", "paragraph_index": 15}, {"url": "https://towardsdatascience.com/neural-network-its-internal-functioning-and-uses-7adc4d37f3d8", "anchor_text": "Relu", "paragraph_index": 16}, {"url": "https://towardsdatascience.com/neural-network-its-internal-functioning-and-uses-7adc4d37f3d8", "anchor_text": "Backward Propagation", "paragraph_index": 33}], "all_paragraphs": ["There is a quote which says \u201cA sequence works in a way, a collection never can.\u201d- George Murray", "In my last blog about NLP I had taken topics of Bag of Words, tokenization, TF-IDF, Word2Vec, all of these had a problem like they don\u2019t store the information of semantics.", "Semantics means the sequence of words, like which word was spoken before or after a word.", "It is important information to keep in language processing if we have to interpret it in a right way.", "For example, if I say \u201cYou are beautiful but not intelligent\u201d and we are not able to keep semantics, the sentence may mean differently with only collection of words.", "In RNN we keep the information of sequence so that a meaningful sentence can be made, translated or interpreted.", "1) Inputs and outputs can be of different length in different examples.", "2) The learning across different positions of text is not maintained, hence semantic information is not maintained neither do context. This necessary for speech recognition, text generation, text or voice semantic recognition.", "a) With same number of input and outputs", "b) With different number of inputs and outputs", "Many to Many Architecture of RNN (Equal Inputs and Outputs)", "Examples of problems using such architecture are:", "1) Video classification where we wish to label each frame of the video", "In this architecture, sequence of inputs is maintained and outputs are given simultaneously. Series x is basically a sentence and each word x11,x1\u2026.x14 is fed into a neural network which gives an output y11, y12..y14 as well as o which is again fed into next neural network layer to give a kind of context or history on what is being talked about.", "So next word generation is based on the words that have come in sequence before.", "Here in forward propagation outputs can be represented by :", "Where w1 is weight assigned to o1 and by is bias of y, f is any activation function like Relu, TanH, etc.", "o1 output is context output which is represented by following equations:", "here again f is an activation function , and w is weight given to input x11, which is first word as input, bo is bias applied to output o.", "Examples can be identifying name entities in a sentence like \u201cAdam lists his Manhattan house for $37.5 million\u201d, here Adam is name, Manhattan is location and $37.5 is monitory value.", "Many to Many Architecture of RNN (Unequal Inputs and Outputs)", "Example of problem using such architecture is:", "In this architecture which is basically used in language translation jobs where input and output can be of different numbers for example:", "English Sentence: How are you doing? Can be translated to French as Comment allez vous?", "See the difference between numbers of words, English words are four while French words are three.", "So in architecture above, n is not equal to m.", "While the sequence of words inputs and their context is maintained in encoder and decoder, the number of inputs and outputs can differ.", "Many to One Architecture of RNN (Many inputs but one output)", "Example of problem using such architecture is:", "In this architecture the sequence and context of words are taken into consideration to know what the sentiment of it is. Like if it was a negative or positive sentence.", "If someone reviews a movie as \u201cThe movie was not at all good\u201d. This sentence has to be interpreted in sequence and also context has to taken into consideration for sentiment output, it need not give many outputs but just one of its sentiment, hence many to one relation.", "The above architecture can be explained as:", "here o1 , o2\u2026 are context outputs which have sequence of words maintained, w is weight given to input while w\u2019 is weight given to each output o, f is an activation function.", "Weight Updation w\u2019\u2019, in Backward Propagation at time t4 will be:", "By Chain rule w\u2019\u2019 is dependent on y^", "Weight Updation w w.r.t x14 in Backward Propagation at time t4", "By Chain Rule w is dependent on o4 which in turn is dependent on y^", "Weight Updation w\u2019 w.r.t o3 in Backward Propagation at time t3", "By Chain Rule w is dependent on o4 which in turn is dependent on y^", "With so many multiplication of derivatives which is are very small values can result in vanishing gradient problem.", "Example of problem using such architecture is:", "2) Image captioning takes an image and outputs a sentence of words", "In the architecture above only one input is given which generates a series of outputs.", "Example can be music generation by just giving first note of music which is then generated to a series of outputs, while feeding output as an input at each step.", "Also if an image is given to be interpreted, the output can be many words like a person riding a bike on a sunny day.", "Example of problem using such architecture is:", "In this type of architecture only one input is given like an image of dog or cat and it has to specify or classify which image was it.", "Diagram is simple to understand like x as input of image and y is given out as out as classification of that image after it was trained on dog/cat classification.", "1) It has capacity to process input of any length", "2) The model size does not increase with size of input", "3) It computes historical information as well.", "4) Weights are shared as per time.", "The Short coming or disadvantage of RNN are:", "1) Vanishing gradient problem: As the layers get deeper the dependencies and derivatives which are mostly between values of 0\u20130.25 for sigmoid are multiplied various times, resulting in smaller values which in result do not update the new weights significantly, resulting in no learning.", "No convergence to global minima, i.e. point of minimum loss.", "2) If weight derivates are of value more than 1, the problem will be of exploding gradient which means weights will be so high, that weights will keep moving and not converge to global minima. This can however be resolved by gradient clipping, it is a technique by which maximum value for the gradient can be capped, known as controlled in practice.", "3) In simple RNN for text generation, it can consider only context of last words given to it, not the words further or after it. Which will be resolved by bi-directional RNN.", "5) It has difficulty of accessing information from a long time ago.", "RNNs have many advantages and disadvantages , in my blogs ahead we will talk about how researchers overcame these shortcomings to have an architecture which were optimized and grew to handle more of real life problems. GRUs, LSTM , Bi-directional RNNs being few of them.", "Thanks for reading and happy learning!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Science Professional | Technical Blogger | Artificial Intelligence | NLP | Chatbots and more"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F4129195bcb24&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrent-neural-network-4129195bcb24&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrent-neural-network-4129195bcb24&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrent-neural-network-4129195bcb24&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrent-neural-network-4129195bcb24&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----4129195bcb24--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4129195bcb24--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://namratakapoor1.medium.com/?source=post_page-----4129195bcb24--------------------------------", "anchor_text": ""}, {"url": "https://namratakapoor1.medium.com/?source=post_page-----4129195bcb24--------------------------------", "anchor_text": "Namrata Kapoor"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fae90d36721ac&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrent-neural-network-4129195bcb24&user=Namrata+Kapoor&userId=ae90d36721ac&source=post_page-ae90d36721ac----4129195bcb24---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4129195bcb24&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrent-neural-network-4129195bcb24&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4129195bcb24&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrent-neural-network-4129195bcb24&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/photos/WfSnpsM3O9o?utm_source=unsplash&utm_medium=referral&utm_content=creditShareLink", "anchor_text": "Source"}, {"url": "https://towardsdatascience.com/nlp-easy-explanation-of-common-terms-with-python-dc7c323a4691", "anchor_text": "blog"}, {"url": "https://towardsdatascience.com/neural-network-its-internal-functioning-and-uses-7adc4d37f3d8", "anchor_text": "forward propagation"}, {"url": "https://towardsdatascience.com/neural-network-its-internal-functioning-and-uses-7adc4d37f3d8", "anchor_text": "Relu"}, {"url": "https://towardsdatascience.com/neural-network-its-internal-functioning-and-uses-7adc4d37f3d8", "anchor_text": "Backward Propagation"}, {"url": "https://www.numpyninja.com/post/nlp-word-embedding-made-easy", "anchor_text": "https://www.numpyninja.com"}, {"url": "https://medium.com/tag/nlp?source=post_page-----4129195bcb24---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----4129195bcb24---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----4129195bcb24---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----4129195bcb24---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----4129195bcb24---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4129195bcb24&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrent-neural-network-4129195bcb24&user=Namrata+Kapoor&userId=ae90d36721ac&source=-----4129195bcb24---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4129195bcb24&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrent-neural-network-4129195bcb24&user=Namrata+Kapoor&userId=ae90d36721ac&source=-----4129195bcb24---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4129195bcb24&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrent-neural-network-4129195bcb24&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4129195bcb24--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F4129195bcb24&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrent-neural-network-4129195bcb24&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----4129195bcb24---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----4129195bcb24--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----4129195bcb24--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----4129195bcb24--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----4129195bcb24--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----4129195bcb24--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----4129195bcb24--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----4129195bcb24--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----4129195bcb24--------------------------------", "anchor_text": ""}, {"url": "https://namratakapoor1.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://namratakapoor1.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Namrata Kapoor"}, {"url": "https://namratakapoor1.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "211 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fae90d36721ac&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrent-neural-network-4129195bcb24&user=Namrata+Kapoor&userId=ae90d36721ac&source=post_page-ae90d36721ac--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fccc842effc8c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecurrent-neural-network-4129195bcb24&newsletterV3=ae90d36721ac&newsletterV3Id=ccc842effc8c&user=Namrata+Kapoor&userId=ae90d36721ac&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}