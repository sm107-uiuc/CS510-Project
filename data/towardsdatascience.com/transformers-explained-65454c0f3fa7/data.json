{"url": "https://towardsdatascience.com/transformers-explained-65454c0f3fa7", "time": 1683009018.0215821, "path": "towardsdatascience.com/transformers-explained-65454c0f3fa7/", "webpage": {"metadata": {"title": "Transformers Explained. An exhaustive explanation of Google\u2019s\u2026 | by Rohan Jagtap | Towards Data Science", "h1": "Transformers Explained", "description": "This post is an in-depth elucidation of the Transformer model from the well-known paper \u201cAttention is all you need\u201d by Google Research. This model has been a pioneer to many SOTA (state of the art)\u2026"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "Attention is all you need", "paragraph_index": 0}, {"url": "https://medium.com/@rojagtap/abstractive-text-summarization-using-transformers-3e774cc42453", "anchor_text": "here", "paragraph_index": 34}], "all_paragraphs": ["This post is an in-depth elucidation of the Transformer model from the well-known paper \u201cAttention is all you need\u201d by Google Research. This model has been a pioneer to many SOTA (state of the art) approaches in sequence transduction tasks (any task which involves converting one sequence to another). Following are the contents of this post:", "This is going to be a long one, so sit tight!", "The sequence to sequence encoder-decoder architecture is the base for sequence transduction tasks. It essentially suggests encoding the complete sequence at once and then using this encoding as a context for the generation of decoded sequence or the target sequence.", "One may relate this to the human tendency of first \u2018listening\u2019 to a sentence(sequence) completely and then responding accordingly be it a conversation, translation, or any similar task.", "The seq2seq model consists of separate RNNs at encoder and decoder respectively. The encoded sequence is the hidden state of the RNN at the encoder network. Using this encoded sequence and (usually) word-level generative modeling, seq2seq generates the target sequence. Since encoding is at the word-level, for longer sequences it is difficult to preserve the context at the encoder, hence the well-known attention mechanism was incorporated with seq2seq to \u2018pay attention\u2019 at specific words in the sequence that prominently contribute to the generation of the target sequence. Attention is weighing individual words in the input sequence according to the impact they make on the target sequence generation.", "Sequence to Sequence with RNNs is great, with attention it\u2019s even better. Then what\u2019s so great about Transformers?", "The main issue with RNNs lies in their inability of providing parallelization while processing. The processing of RNN is sequential, i.e. we cannot compute the value of the next timestep unless we have the output of the current. This makes RNN-based approaches slow.", "This issue, however, was addressed by Facebook Research wherein they suggested using a convolution-based approach that allows incorporating parallelization with GPU. These models establish hierarchical representation between words, where the words that occur closer in sequences interact at lower levels while the ones appearing far from each other operate at higher levels in the hierarchy. ConvS2S and ByteNet are two such models. The hierarchy is introduced to address long-term dependencies.", "Although this achieves parallelization, it is still computationally expensive. The number of operations per layer incurred by RNNs and CNNs is way more unreasonable as compared to the quality of results they offer. The original Transformer paper has put forth a comparison of these parameters for the competent models:", "Here, d (or d_model) is the representation dimension or embedding dimension of a word (usually in the range 128\u2013512), n is the sequence length (usually in the range 40\u201370), k is the kernel size of the convolution and r is the attention window-size for restricted self-attention. From the table, we can infer the following:", "The Transformer uses the self-attention mechanism where attention weights are calculated using all the words in the input sequence at once, hence it facilitates parallelization. In addition to that, since the per-layer operations in the Transformer are among words of the same sequence, the complexity does not exceed O(n\u00b2d). Hence, the transformer proves to be effective (since it uses attention) and at the same time, a computationally efficient model.", "In the previous section, we discussed that the Transformer uses self-attention as a means for effective computation. In this section, we will decipher, what exactly is self-attention and how is it used in the Transformer.", "The attention mechanism as a general convention follows a Query, Key, Value pattern. All three of these are words from the input sequence that are meant to operate with each other in a certain pattern. The query and key initially undergo certain operations, the output is then (usually) multiplied with the value. This will get clearer in the next sub-section where we will see a pictorial depiction of how self-attention works.", "As mentioned earlier, self-attention is \u2018attending\u2019 words from the same sequence.", "Superficially speaking, self-attention determines the impact a word has on the sentence", "In the picture above, the working of self-attention is explained with the example of a sentence, \u201cThis is Attention\u201d. The word \u201cThis\u201d is operated with every other word in the sentence. Similarly, the attention weights for all the words are calculated (here, \u201cis\u201d and \u201cAttention\u201d). There is no concept of \u2018hidden state\u2019 here. The inputs are used directly. This removes sequentiality from the architecture, hence allowing parallelization.", "In case of Transformer, Multi-Head Attention is used, which is covered later in the post.", "So far, we have seen mechanisms implemented in the Transformer. Hereafter, we will actually see how these adjoining mechanisms and several components specific to the model are incorporated.", "We will try and build a Transformer bottom-up", "If you observe, the self-attention computations have no notion of ordering of words among the sequences. Although RNNs are slow, their sequential nature ensures that the order of words is preserved. So, to elicit this notion of positioning of words in the sequence, Positional Encodings are added to the regular input embeddings. The dimension of positional encodings is the same as the embeddings (d_model) for facilitating the summation of both. In the paper, positional encodings are obtained using:", "Here, i is the dimension and pos is the position of the word. We use sine for even values (2i) of dimensions and cosine for odd values (2i + 1). There are several choices for positional encodings \u2014 learned or fixed. This is the fixed way as the paper states learned as well as fixed methods achieved identical results.", "The general idea behind this is, for a fixed offset k, PE\u209a\u2092\u209b\u208a\u2096 can be represented as linear function of PE\u209a\u2092\u209b.", "There are two kinds of masks used in the multi-head attention mechanism of the Transformer.", "The following snippet of code explains how padding helps:", "This is the main \u2018Attention Computation\u2019 step that we have previously discussed in the Self-Attention section. This involves a few steps:", "The scaled dot-product attention is a major component of the multi-head attention which we are about to see in the next sub-section.", "Multi-Head Attention is essentially the integration of all the previously discussed micro-concepts.", "In the adjacent figure, h is the number of heads. As far as the math is concerned, the initial inputs to the Multi-Head Attention are split into h parts, each having Queries, Keys, and Values, for max_length words in a sequence, for batch_size sequences. The dimensions of Q, K, V are called depth which is calculated as follows:", "This is the reason why d_model needs to be completely divisible by h. So, while splitting, the d_model shaped vectors are split into h vectors of shape depth. These vectors are passed as Q, K, V to the scaled dot product, and the output is \u2018Concat\u2019 by again reshaping the h vectors into 1 vector of shape d_model. This reformed vector is then passed through a feed-forward neural network layer.", "The Point-wise feed-forward network block is essentially a two-layer linear transformation which is used identically throughout the model architecture, usually after attention blocks.", "For Regularization, a dropout is applied to the output of each sub-layer before it is added to the inputs of the sub-layer and normalized.", "All the components are assembled together to build the Transformer. The Encoder block is the one in the left and Decoder to the right.", "The Encoder and the Decoder blocks may be optionally fine-tuned to N\u2093 units to adjust the model.", "In this post, we\u2019ve seen the issues with RNN-based approaches in sequence transduction tasks and how the revolutionary, Transformer model addresses them. Later, we studied the mechanisms underlying the Transformer and their working. Finally, we saw how various components and the underlying mechanisms work with the Transformer.", "I have covered the step-by-step implementation of Transformer using TensorFlow for Abstractive Text Summarization here.", "Some Great Visualizations of the Transformer:", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Immensely interested in AI Research | I read papers and post my notes on Medium"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F65454c0f3fa7&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-explained-65454c0f3fa7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-explained-65454c0f3fa7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-explained-65454c0f3fa7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-explained-65454c0f3fa7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----65454c0f3fa7--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----65454c0f3fa7--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://rojagtap.medium.com/?source=post_page-----65454c0f3fa7--------------------------------", "anchor_text": ""}, {"url": "https://rojagtap.medium.com/?source=post_page-----65454c0f3fa7--------------------------------", "anchor_text": "Rohan Jagtap"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F39646f947a4c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-explained-65454c0f3fa7&user=Rohan+Jagtap&userId=39646f947a4c&source=post_page-39646f947a4c----65454c0f3fa7---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F65454c0f3fa7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-explained-65454c0f3fa7&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F65454c0f3fa7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-explained-65454c0f3fa7&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@wagner2074?utm_source=medium&utm_medium=referral", "anchor_text": "Christian Wagner"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "Attention is all you need"}, {"url": "https://ai.googleblog.com/2016/09/a-neural-network-for-machine.html", "anchor_text": "Google AI Blog"}, {"url": "https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/#.XreFtRbhU5l", "anchor_text": "Michal Chromiak\u2019s blog"}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "\u201cAttention is all you need\u201d"}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "\u201cAttention is all you need\u201d"}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "\u201cAttention is all you need\u201d"}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "\u201cAttention is all you need\u201d"}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "\u201cAttention is all you need\u201d"}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "\u201cAttention is all you need\u201d"}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "\u201cAttention is all you need\u201d"}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "\u201cAttention is all you need\u201d"}, {"url": "https://medium.com/@rojagtap/abstractive-text-summarization-using-transformers-3e774cc42453", "anchor_text": "here"}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "https://arxiv.org/abs/1706.03762"}, {"url": "https://arxiv.org/abs/1409.3215", "anchor_text": "https://arxiv.org/abs/1409.3215"}, {"url": "https://arxiv.org/abs/1409.0473", "anchor_text": "https://arxiv.org/abs/1409.0473"}, {"url": "https://arxiv.org/abs/1705.03122", "anchor_text": "https://arxiv.org/abs/1705.03122"}, {"url": "https://arxiv.org/abs/1610.10099", "anchor_text": "https://arxiv.org/abs/1610.10099"}, {"url": "https://towardsdatascience.com/self-attention-and-transformers-882e9de5edda", "anchor_text": "Self Attention and TransformersFrom Attention to Self Attention to Transformerstowardsdatascience.com"}, {"url": "https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/#.XreFtRbhU5l", "anchor_text": "The Transformer - Attention is all you need.Transformer - more than meets the eye! Are we there yet? Well... not really, but... How about eliminating recurrence\u2026mchromiak.github.io"}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "The Illustrated TransformerDiscussions: Hacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments) Translations\u2026jalammar.github.io"}, {"url": "https://medium.com/tag/attention?source=post_page-----65454c0f3fa7---------------attention-----------------", "anchor_text": "Attention"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----65454c0f3fa7---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----65454c0f3fa7---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----65454c0f3fa7---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----65454c0f3fa7---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F65454c0f3fa7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-explained-65454c0f3fa7&user=Rohan+Jagtap&userId=39646f947a4c&source=-----65454c0f3fa7---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F65454c0f3fa7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-explained-65454c0f3fa7&user=Rohan+Jagtap&userId=39646f947a4c&source=-----65454c0f3fa7---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F65454c0f3fa7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-explained-65454c0f3fa7&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----65454c0f3fa7--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F65454c0f3fa7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-explained-65454c0f3fa7&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----65454c0f3fa7---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----65454c0f3fa7--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----65454c0f3fa7--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----65454c0f3fa7--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----65454c0f3fa7--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----65454c0f3fa7--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----65454c0f3fa7--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----65454c0f3fa7--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----65454c0f3fa7--------------------------------", "anchor_text": ""}, {"url": "https://rojagtap.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://rojagtap.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Rohan Jagtap"}, {"url": "https://rojagtap.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "465 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F39646f947a4c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-explained-65454c0f3fa7&user=Rohan+Jagtap&userId=39646f947a4c&source=post_page-39646f947a4c--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fe51e2b6202c5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-explained-65454c0f3fa7&newsletterV3=39646f947a4c&newsletterV3Id=e51e2b6202c5&user=Rohan+Jagtap&userId=39646f947a4c&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}