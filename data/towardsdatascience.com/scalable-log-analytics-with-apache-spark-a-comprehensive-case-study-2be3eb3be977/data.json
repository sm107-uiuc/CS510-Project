{"url": "https://towardsdatascience.com/scalable-log-analytics-with-apache-spark-a-comprehensive-case-study-2be3eb3be977", "time": 1682995728.3735979, "path": "towardsdatascience.com/scalable-log-analytics-with-apache-spark-a-comprehensive-case-study-2be3eb3be977/", "webpage": {"metadata": {"title": "Scalable Log Analytics with Apache Spark \u2014 A Comprehensive Case-Study | by Dipanjan (DJ) Sarkar | Towards Data Science", "h1": "Scalable Log Analytics with Apache Spark \u2014 A Comprehensive Case-Study", "description": "One of the most popular and effective enterprise case-studies which leverage analytics today is log analytics. Almost every small and big organization today have multiple systems and infrastructure\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/sql-at-scale-with-apache-spark-sql-and-dataframes-concepts-architecture-and-examples-c567853a702f", "anchor_text": "SQL at scale with Spark", "paragraph_index": 1}, {"url": "http://ita.ee.lbl.gov/html/contrib/NASA-HTTP.html", "anchor_text": "here", "paragraph_index": 5}, {"url": "http://ita.ee.lbl.gov/html/contrib/NASA-HTTP.html", "anchor_text": "website", "paragraph_index": 6}, {"url": "https://github.com/dipanjanS/data_science_for_all/tree/master/tds_scalable_log_analytics", "anchor_text": "my GitHub", "paragraph_index": 7}, {"url": "https://community.cloud.databricks.com", "anchor_text": "Databricks community edition", "paragraph_index": 8}, {"url": "https://www.w3.org/Daemon/User/Config/Logging.html#common-logfile-format", "anchor_text": "Common Log Format", "paragraph_index": 23}, {"url": "http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.regexp_extract", "anchor_text": "regexp_extract()", "paragraph_index": 26}, {"url": "http://regexone.com/lesson/capturing_groups", "anchor_text": "capture groups", "paragraph_index": 26}, {"url": "http://regexone.com/", "anchor_text": "RegexOne web site", "paragraph_index": 27}, {"url": "http://shop.oreilly.com/product/0636920023630.do", "anchor_text": "Regular Expressions Cookbook", "paragraph_index": 27}, {"url": "http://stackoverflow.com/a/33901312", "anchor_text": "excellent answer", "paragraph_index": 41}, {"url": "https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions", "anchor_text": "documentation", "paragraph_index": 65}, {"url": "https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.dayofmonth", "anchor_text": "dayofmonth", "paragraph_index": 84}, {"url": "https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.dayofmonth", "anchor_text": "function", "paragraph_index": 84}, {"url": "https://github.com/dipanjanS/data_science_for_all/tree/master/tds_scalable_log_analytics", "anchor_text": "my GitHub Repository", "paragraph_index": 104}, {"url": "https://nbviewer.jupyter.org/github/dipanjanS/data_science_for_all/blob/master/tds_scalable_log_analytics/Scalable_Log_Analytics_Spark.ipynb", "anchor_text": "Jupyter Notebook", "paragraph_index": 105}, {"url": "https://towardsdatascience.com/", "anchor_text": "TDS", "paragraph_index": 106}, {"url": "https://www.linkedin.com/in/dipanzan/", "anchor_text": "LinkedIn", "paragraph_index": 106}], "all_paragraphs": ["One of the most popular and effective enterprise case-studies which leverage analytics today is log analytics. Almost every small and big organization today have multiple systems and infrastructure running day in and day out. To effectively keep their business running, organizations need to know if their infrastructure is performing to its maximum potential. This involves analyzing system and application logs and maybe even apply predictive analytics on log data. The amount of log data is typically massive, depending on the type of organizational infrastructure and applications running on it. Gone are the days when we were limited by just trying to analyze a sample of data on a single machine due to compute constraints.", "Powered by big data, better and distributed computing, big data processing and open-source analytics frameworks like Spark, we can perform scalable log analytics on potentially millions and billions of log messages daily. The intent of this case-study oriented tutorial is to take a hands-on approach to showcasing how we can leverage Spark to perform log analytics at scale on semi-structured log data. If you are interested in scalable SQL with Spark, feel free to check out SQL at scale with Spark.", "We will be covering the following major topics in this article today.", "While there are a lot of excellent open-source frameworks and tools out there for log analytics including elasticsearch, the intent of this tutorial is to showcase how Spark can be leveraged for analyzing logs at scale. In the real-world, you are free to choose your toolbox when analyzing log data. Let\u2019s get started!", "Like we mentioned before, Apache Spark is an excellent and ideal open-source framework for wrangling, analyzing and modeling on structured and unstructured data \u2014 at scale! In this tutorial, our main objective is focusing on one of the most popular case studies in the industry \u2014 log analytics. Typically, server logs are a very common data source in enterprises and often contain a gold mine of actionable insights and information. Log data comes from many sources in an enterprise, such as the web, client and compute servers, applications, user-generated content, flat files. They can be used for monitoring servers, improving business and customer intelligence, building recommendation systems, fraud detection, and much more.", "Spark allows you to dump and store your logs in files on disk cheaply, while still providing rich APIs to perform data analysis at scale. This hands-on case study will show you how to use Apache Spark on real-world production logs from NASA and learn data wrangling and basic yet powerful techniques in exploratory data analysis. In this case study, we will analyze log datasets from NASA Kennedy Space Center web server in Florida. The full data set is freely available for download here.", "These two datasets contain two months\u2019 worth of all HTTP requests to the NASA Kennedy Space Center WWW server in Florida. You can head over to the website and download the following files as needed (or click on the following links directly).", "Make sure both the files are in the same directory as the notebook containing the tutorial which is available on my GitHub.", "The first step is to make sure you have access to a Spark session and cluster. For this you can use your own local setup or a cloud based setup. Typically most cloud platforms will provide a Spark cluster these days and you also have free options including Databricks community edition. This tutorial assumes you already have Spark setup hence we will not be spending additional time configuring or setting up Spark from scratch.", "Often pre-configured Spark setups already have the necessary environment variables or dependencies pre-loaded when you start your jupyter notebook server. In my case, I can check them using the following commands in my notebook.", "This shows me that my cluster is running Spark 2.4.0 at the moment. We can also check if sqlContext is present using the following code.", "Now in case you don\u2019t have these variables pre-configured and get an error, you can load them up and configure them using the following code. Besides this we also load up some other libraries for working with dataframes and regular expressions.", "Working with regular expressions will be one of the major aspects of parsing log files. Regular expressions are a really powerful pattern matching technique which can be used to extract and find patterns in semi-structured and unstructured data.", "Regular expressions can be extremely effective and powerful, yet they can sometimes be overwhelming or confusing. Not to worry though, with more practice, you can really leveraging its maximum potential. The following example showcases a way of using regular expressions in Python.", "Let\u2019s move on to the next part of our analysis.", "Given that our data is stored in the following mentioned path (in the form of flat files), let\u2019s load it into a DataFrame. We\u2019ll do this in steps. The following code get\u2019s us the log data file names in our disk.", "Now, we\u2019ll use sqlContext.read.text() or spark.read.text() to read the text file. This will produce a DataFrame with a single string column called value.", "This allows us to see the schema for our log data which apparently looks like text data which we shall inspect soon. You can view the type of data structure holding our log data using the following code.", "We will be using Spark DataFrames throughout our tutorial. However if you want, you can also convert a dataframe into an RDD if needed, Spark\u2019s original data structure (resilient distributed datasets).", "Let\u2019s now take a peek at the actual log data in our dataframe.", "This definitely looks like standard server log data which is semi-structured and we will definitely need to do some data processing and wrangling before this can be useful. Do remember accessing data from RDDs is slightly different as seen below.", "Now that we have loaded up and viewed our log data, let\u2019s process and wrangle it.", "In this section, we will try and clean and parse our log dataset to really extract structured attributes with meaningful information from each log message.", "If you\u2019re familiar with web server logs, you\u2019ll recognize that the above displayed data is in Common Log Format.", "The fields are: remotehost rfc931 authuser [date] \"request\" status bytes", "We will need to use some specific techniques to parse, match and extract these attributes from the log data.", "Next, we have to parse our semi-structured log data into individual columns. We\u2019ll use the special built-in regexp_extract() function to do the parsing. This function matches a column against a regular expression with one or more capture groups and allows you to extract one of the matched groups. We\u2019ll use one regular expression for each field we wish to extract.", "You must have heard or used a fair bit of regular expressions by now. If you find regular expressions confusing (and they certainly can be), and you want to learn more about them, we recommend checking out the RegexOne web site. You might also find Regular Expressions Cookbook, by Goyvaerts and Levithan, to be useful as a reference.", "Let\u2019s take a look at the total number of logs we are working with in our dataset.", "Looks like we have a total of approximately 3.46 million log messages. Not a small number! Let\u2019s extract and take a look at some sample log messages.", "Let\u2019s try and write some regular expressions to extract the host name from the logs.", "Let\u2019s now try and use regular expressions to extract the timestamp fields from the logs", "Let\u2019s now try and use regular expressions to extract the HTTP request methods, URIs and Protocol patterns fields from the logs.", "Let\u2019s now try and use regular expressions to extract the HTTP status codes from the logs.", "Let\u2019s now try and use regular expressions to extract the HTTP response content size from the logs.", "Let\u2019s now try and leverage all the regular expression patterns we previously built and use the regexp_extract(...) method to build our dataframe with all the log attributes neatly extracted in their own separate columns.", "Missing and null values are the bane of data analysis and machine learning. Let\u2019s see how well our data parsing and extraction logic worked. First, let\u2019s verify that there are no null rows in the original dataframe.", "All good! Now, if our data parsing and extraction worked properly, we should not have any rows with potential null values. Let\u2019s try and put that to test!", "Ouch! Looks like we have over 33K missing values in our data! Can we handle this?", "Do remember, this is not a regular pandas dataframe which you can directly query and get which columns have null. Our so-called big dataset is residing on disk which can potentially be present in multiple nodes in a spark cluster. So how do we find out which columns have potential nulls?", "We can typically use the following technique to find out which columns have null values.", "(Note: This approach is adapted from an excellent answer on StackOverflow.)", "Well, looks like we have one missing value in the status column and everything else is in the content_size column. Let's see if we can figure out what's wrong!", "Our original parsing regular expression for the status column was:", "Could it be that there are more digits making our regular expression wrong? or is the data point itself bad? Let\u2019s try and find out!", "Note: In the expression below, ~ means \"not\".", "Let\u2019s look at what this bad record looks like!", "Looks like a record with a lot of missing information! Let\u2019s pass this through our log data parsing pipeline.", "Looks like the record itself is an incomplete record with no useful information, the best option would be to drop this record as follows!", "Based on our previous regular expression, our original parsing regular expression for the content_size column was:", "Could there be missing data in our original dataset itself? Let\u2019s try and find out! We first try to find out the records in our base dataframe with potential missing content sizes.", "The number seems to match the number of missing content size values in our processed dataframe. Let\u2019s take a look at the top ten records of our data frame having missing content sizes.", "It is quite evident that the bad raw data records correspond to error responses, where no content was sent back and the server emitted a \u201c-\" for the content_size field.", "Since we don\u2019t want to discard those rows from our analysis, let\u2019s impute or fill them to 0.", "The easiest solution is to replace the null values in logs_df with 0 like we discussed earlier. The Spark DataFrame API provides a set of functions and fields specifically designed for working with null values, among them:", "There are several ways to invoke this function. The easiest is just to replace all null columns with known values. But, for safety, it\u2019s better to pass a Python dictionary containing (column_name, value) mappings. That\u2019s what we\u2019ll do. A sample example from the documentation is depicted below", "Now we use this function and fill all the missing values in the content_size field with 0!", "Look at that, no missing values!", "Now that we have a clean, parsed DataFrame, we have to parse the timestamp field into an actual timestamp. The Common Log Format time is somewhat non-standard. A User-Defined Function (UDF) is the most straightforward way to parse it.", "Let\u2019s now use this function to parse our time column in our dataframe.", "Things seem to be looking good! Let\u2019s verify this by checking the schema of our dataframe.", "Let\u2019s now cache logs_df since we will be using it extensively for our data analysis section in the next part!", "Now that we have a DataFrame containing the parsed and cleaned log file as a data frame, we can perform some interesting exploratory data analysis (EDA) to try and get some interesting insights!", "Let\u2019s compute some statistics about the sizes of content being returned by the web server. In particular, we\u2019d like to know what are the average, minimum, and maximum content sizes.", "We can compute the statistics by calling .describe() on the content_size column of logs_df. The .describe() function returns the count, mean, stddev, min, and max of a given column.", "Alternatively, we can use SQL to directly calculate these statistics. You can explore many useful functions within the pyspark.sql.functions module in the documentation.", "After we apply the .agg() function, we call toPandas() to extract and convert the result into a pandas dataframe which has better formatting on Jupyter notebooks.", "We can validate the results and see they are the same as expected.", "Next, let\u2019s look at the status code values that appear in the log. We want to know which status code values appear in the data and how many times. We again start with logs_df, then group by the status column, apply the .count() aggregation function, and sort by the status column.", "Looks like we have a total of 8 distinct HTTP status codes. Let\u2019s take a look at their occurrences in the form of a frequency table.", "Looks like status code 200 OK is the most frequent code which is a good sign that things have been working normally most of the time. Let\u2019s visualize this.", "Not too bad! But several status codes are almost not visible due to the huge skew in the data. Let\u2019s take a log transform and see if things improve.", "The results definitely look good and seem to have handled the skewness, let\u2019s verify this by visualizing this data.", "This definitely looks much better and less skewed!", "Let\u2019s look at hosts that have accessed the server frequently. We will try to get the count of total accesses by each host and then sort by the counts and display only the top ten most frequent hosts.", "This looks good but let\u2019s inspect the blank record in row number 9 more closely.", "Looks like we have some empty strings as one of the top host names! This teaches us a valuable lesson to not just check for nulls but also potentially empty strings when data wrangling.", "Now, let\u2019s visualize the number of hits to endpoints (URIs) in the log. To perform this task, we start with our logs_df and group by the endpointcolumn, aggregate by count, and sort in descending order like the previous question.", "Not surprisingly GIFs, the home page and some CGI scripts seem to be the most accessed assets.", "What are the top ten endpoints requested which did not have return code 200 (HTTP Status OK)? We create a sorted list containing the endpoints and the number of times that they were accessed with a non-200 return code and show the top ten.", "Looks like GIFs (animated\\static images) are failing to load the most. Do you know why? Well given that these logs are from 1995 and given the internet speed we had back then, I\u2019m not surprised!", "What were the total number of unique hosts who visited the NASA website in these two months? We can find this out with a few transformations.", "For an advanced example, let\u2019s look at a way to determine the number of unique hosts in the entire log on a day-by-day basis. This computation will give us counts of the number of unique daily hosts.", "We\u2019d like a DataFrame sorted by increasing day of the month which includes the day of the month and the associated number of unique hosts for that day.", "Think about the steps that you need to perform to count the number of different hosts that make requests each day. Since the log only covers a single month, you can ignore the month. You may want to use the dayofmonthfunction in the pyspark.sql.functions module (which we have already imported as F.", "host_day_df : A DataFrame with two columns", "There will be one row in this DataFrame for each row in logs_df. Essentially, we are just transforming each row of logs_df. For example, for this row in logs_df:", "your host_day_df should have: unicomp6.unicomp.net 1", "host_day_distinct_df : This DataFrame has the same columns as host_day_df, but with duplicate (day, host) rows removed.", "daily_unique_hosts_df : A DataFrame with two columns:", "This gives us a nice dataframe showing the total number of unique hosts per day. Let\u2019s visualize this!", "In the previous example, we looked at a way to determine the number of unique hosts in the entire log on a day-by-day basis. Let\u2019s now try and find the average number of requests being made per Host to the NASA website per day based on our logs. We\u2019d like a DataFrame sorted by increasing day of the month which includes the day of the month and the associated number of average requests made for that day per Host.", "We can now visualize the average daily requests per host.", "Looks like Day 13 got the maximum number of requests per host.", "Create a DataFrame containing only log records with a 404 status code (Not Found). We make sure to cache() the not_found_df dataframe as we will use it in the rest of the examples here. How many 404 records do you think are in the logs?", "Using the DataFrame containing only log records with a 404 response code that we cached earlier, we will now print out a list of the top twenty endpoints that generate the most 404 errors. Remember, top endpoints should be in sorted order.", "Using the DataFrame containing only log records with a 404 response code that we cached earlier, we will now print out a list of the top twenty hosts that generate the most 404 errors. Remember, top hosts should be in sorted order.", "Gives us a good idea which hosts end up generating the most 404 errors for the NASA webpage.", "Let\u2019s explore our 404 records temporally (by time) now. Similar to the example showing the number of unique daily hosts, we will break down the 404 requests by day and get the daily counts sorted by day in errors_by_date_sorted_df.", "Let\u2019s visualize the total 404 errors per day now.", "Based on the earlier plot, what are the top three days of the month having the most 404 errors? We can leverage our previously created errors_by_date_sorted_df for this.", "Using the DataFrame not_found_df we cached earlier, we will now group and sort by hour of the day in increasing order, to create a DataFrame containing the total number of 404 responses for HTTP requests for each hour of the day (midnight starts at 0). Then we will build a visualization from the DataFrame.", "Looks like total 404 errors occur the most in the afternoon and the least in the early morning. We can now reset the maximum rows displayed by pandas to the default value since we had changed it earlier to display a limited number of rows.", "We took a hands-on approach to data wrangling, parsing, analysis and visualization at scale on a very common yet essential case-study on Log Analytics. While the data we worked on here may not really be traditionally \u2018Big Data\u2019 from a size or volume perspective, the techniques and methodologies are generic enough to scale on larger volumes of data. I hope this case-study gives you a good idea about how open-source frameworks like Apache Spark can be easily leveraged to work with structured and semi-structured data at scale!", "All the code and analyses accompanying this article are available in my GitHub Repository.", "You can find a step-by-step approach in this Jupyter Notebook.", "I solve real-world problems leveraging data science, artificial intelligence, machine learning and deep learning. I also do some consulting, research and mentoring in my spare time. If you need focused consulting, training sessions, want me to speak at events or if you want to publish an article on TDS, feel free to reach out to me on LinkedIn.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F2be3eb3be977&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscalable-log-analytics-with-apache-spark-a-comprehensive-case-study-2be3eb3be977&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscalable-log-analytics-with-apache-spark-a-comprehensive-case-study-2be3eb3be977&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscalable-log-analytics-with-apache-spark-a-comprehensive-case-study-2be3eb3be977&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscalable-log-analytics-with-apache-spark-a-comprehensive-case-study-2be3eb3be977&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----2be3eb3be977--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2be3eb3be977--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://djsarkar.medium.com/?source=post_page-----2be3eb3be977--------------------------------", "anchor_text": ""}, {"url": "https://djsarkar.medium.com/?source=post_page-----2be3eb3be977--------------------------------", "anchor_text": "Dipanjan (DJ) Sarkar"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6278d12b0682&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscalable-log-analytics-with-apache-spark-a-comprehensive-case-study-2be3eb3be977&user=Dipanjan+%28DJ%29+Sarkar&userId=6278d12b0682&source=post_page-6278d12b0682----2be3eb3be977---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2be3eb3be977&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscalable-log-analytics-with-apache-spark-a-comprehensive-case-study-2be3eb3be977&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2be3eb3be977&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscalable-log-analytics-with-apache-spark-a-comprehensive-case-study-2be3eb3be977&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/photos/dPgPoiUIiXk?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Robin Pierre"}, {"url": "https://unsplash.com/collections/291422/night-lights?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://doughenschen.com/2015/06/17/spark-on-fire-why-all-the-hype/", "anchor_text": "Doug Henschen"}, {"url": "https://towardsdatascience.com/sql-at-scale-with-apache-spark-sql-and-dataframes-concepts-architecture-and-examples-c567853a702f", "anchor_text": "SQL at scale with Spark"}, {"url": "http://ita.ee.lbl.gov/html/contrib/NASA-HTTP.html", "anchor_text": "here"}, {"url": "http://ita.ee.lbl.gov/html/contrib/NASA-HTTP.html", "anchor_text": "website"}, {"url": "https://github.com/dipanjanS/data_science_for_all/tree/master/tds_scalable_log_analytics", "anchor_text": "my GitHub"}, {"url": "https://community.cloud.databricks.com", "anchor_text": "Databricks community edition"}, {"url": "https://www.xkcd.com/1171/", "anchor_text": "xkcd"}, {"url": "https://www.w3.org/Daemon/User/Config/Logging.html#common-logfile-format", "anchor_text": "Common Log Format"}, {"url": "http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.regexp_extract", "anchor_text": "regexp_extract()"}, {"url": "http://regexone.com/lesson/capturing_groups", "anchor_text": "capture groups"}, {"url": "http://regexone.com/", "anchor_text": "RegexOne web site"}, {"url": "http://shop.oreilly.com/product/0636920023630.do", "anchor_text": "Regular Expressions Cookbook"}, {"url": "http://stackoverflow.com/a/33901312", "anchor_text": "excellent answer"}, {"url": "http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.fillna", "anchor_text": "fillna()"}, {"url": "http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.na", "anchor_text": "na"}, {"url": "http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameNaFunctions", "anchor_text": "DataFrameNaFunctions"}, {"url": "https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions", "anchor_text": "documentation"}, {"url": "https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.dayofmonth", "anchor_text": "dayofmonth"}, {"url": "https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.dayofmonth", "anchor_text": "function"}, {"url": "https://github.com/dipanjanS/data_science_for_all/tree/master/tds_scalable_log_analytics", "anchor_text": "my GitHub Repository"}, {"url": "https://nbviewer.jupyter.org/github/dipanjanS/data_science_for_all/blob/master/tds_scalable_log_analytics/Scalable_Log_Analytics_Spark.ipynb", "anchor_text": "Jupyter Notebook"}, {"url": "https://towardsdatascience.com/", "anchor_text": "TDS"}, {"url": "https://www.linkedin.com/in/dipanzan/", "anchor_text": "LinkedIn"}, {"url": "https://www.linkedin.com/in/dipanzan/", "anchor_text": "Dipanjan Sarkar - Data Scientist - Red Hat | LinkedInView Dipanjan Sarkar's profile on LinkedIn, the world's largest professional community. Dipanjan has 9 jobs listed on\u2026www.linkedin.com"}, {"url": "https://medium.com/tag/big-data?source=post_page-----2be3eb3be977---------------big_data-----------------", "anchor_text": "Big Data"}, {"url": "https://medium.com/tag/data-science?source=post_page-----2be3eb3be977---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/technology?source=post_page-----2be3eb3be977---------------technology-----------------", "anchor_text": "Technology"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----2be3eb3be977---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----2be3eb3be977---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2be3eb3be977&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscalable-log-analytics-with-apache-spark-a-comprehensive-case-study-2be3eb3be977&user=Dipanjan+%28DJ%29+Sarkar&userId=6278d12b0682&source=-----2be3eb3be977---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2be3eb3be977&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscalable-log-analytics-with-apache-spark-a-comprehensive-case-study-2be3eb3be977&user=Dipanjan+%28DJ%29+Sarkar&userId=6278d12b0682&source=-----2be3eb3be977---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2be3eb3be977&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscalable-log-analytics-with-apache-spark-a-comprehensive-case-study-2be3eb3be977&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2be3eb3be977--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F2be3eb3be977&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscalable-log-analytics-with-apache-spark-a-comprehensive-case-study-2be3eb3be977&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----2be3eb3be977---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----2be3eb3be977--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----2be3eb3be977--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----2be3eb3be977--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----2be3eb3be977--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----2be3eb3be977--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----2be3eb3be977--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----2be3eb3be977--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----2be3eb3be977--------------------------------", "anchor_text": ""}, {"url": "https://djsarkar.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://djsarkar.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Dipanjan (DJ) Sarkar"}, {"url": "https://djsarkar.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "10.4K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6278d12b0682&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscalable-log-analytics-with-apache-spark-a-comprehensive-case-study-2be3eb3be977&user=Dipanjan+%28DJ%29+Sarkar&userId=6278d12b0682&source=post_page-6278d12b0682--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fa34c887aa0f4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscalable-log-analytics-with-apache-spark-a-comprehensive-case-study-2be3eb3be977&newsletterV3=6278d12b0682&newsletterV3Id=a34c887aa0f4&user=Dipanjan+%28DJ%29+Sarkar&userId=6278d12b0682&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}