{"url": "https://towardsdatascience.com/understanding-automatic-text-summarization-2-abstractive-methods-7099fa8656fe", "time": 1683012104.858746, "path": "towardsdatascience.com/understanding-automatic-text-summarization-2-abstractive-methods-7099fa8656fe/", "webpage": {"metadata": {"title": "Understanding Automatic Text Summarization-2: Abstractive Methods | by Abhijit Roy | Towards Data Science", "h1": "Understanding Automatic Text Summarization-2: Abstractive Methods", "description": "This is my second article on Text summarization. In my first article, I have talked about the extractive approaches to summarize text and the metrics used. In this article, we are going to talk about\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/understanding-automatic-text-summarization-1-extractive-methods-8eb512b21ecc", "anchor_text": "first article", "paragraph_index": 0}, {"url": "https://arxiv.org/abs/1602.06023", "anchor_text": "Source", "paragraph_index": 32}, {"url": "https://arxiv.org/abs/1602.06023", "anchor_text": "Source", "paragraph_index": 34}, {"url": "https://arxiv.org/abs/1602.06023", "anchor_text": "Source", "paragraph_index": 36}, {"url": "https://abhijitroy1998.wixsite.com/abhijitcv", "anchor_text": "https://abhijitroy1998.wixsite.com/abhijitcv", "paragraph_index": 47}], "all_paragraphs": ["This is my second article on Text summarization. In my first article, I have talked about the extractive approaches to summarize text and the metrics used. In this article, we are going to talk about abstractive summarization. We are going to see how deep learning can be used to summarize the text. So, let\u2019s dive in.", "Abstractive summarizers are so-called because they do not select sentences from the originally given text passage to create the summary. Instead, they produce a paraphrasing of the main contents of the given text, using a vocabulary set different from the original document. This is very similar to what we as humans do, to summarize. We create a semantic representation of the document in our brains. We then pick words from our general vocabulary (the words we commonly use) that fit in the semantics, to create a short summary that represents all the points of the actual document. As you may notice, developing this kind of summarizer may be difficult as they would need the Natural Language Generation. Let\u2019s look at the most used approach to the problem.", "The approach was proposed in a paper by Ramesh Nallapati, Bowen Zhou, Cicero dos Santos, Caglar Gulcehre, Bing Xiang from IBM. The term \u201csequence to sequence models\u201d is used because the models are designed to create an output sequence of words from an input sequence of words. The input sequence in the considered case is the actual text document and the output sequence is the shortened summary.", "The paper proposes a model inspired by an attentional Recurrent Neural Network encoder-decoder model which was first proposed for machine translation by Dzmitry Bahdanau, Jacob\u2019s University, Germany.", "Though, the problems are a lot different as you can already sense. Firstly for machine translation, we need the translation to be loss-less as we want the exact sentence in a translated form, but for Summary Generation, we need to compress the original document, to create the summary, so it needs to be a bit lossy. Secondly, for a summary generation, the length of the summary does not depend on the original text. These two points are the key challenges in the problem as given by the problem.", "Before jumping into the application details of the paper let\u2019s look at the encoder and decoder networks and the reason for using the attention layer.", "If we consider a general LSTM( Long term short memory) layer, it looks something like the diagram given below. It either produces an output for every input or it creates a feature vector, which is later used by dense neural network layers for classification tasks with the application of softmax layers. For example, sentiment detection, where we pass the whole sentence through RNN and use the feature vectors which are fit to softmax layers for producing the ultimate result.", "But one thing to realize here is, for the current problem, or problems like this including machine translation, more generally speaking, where we can say the problem is a sequence to sequence problem, this particular model approach cannot be applied. The main reason is that the size of the output is independent of the size of the input and both are sequences. To deal with this problem the encoder-decoder network model was introduced.", "The model basic architecture is described by the above diagram.", "The encoder is responsible for taking in the input sentence or original document and generate a final state vector(hidden state and cell state). This is represented by the internal state in the diagram. The Encoder may contain LSTM layers, RNN, or GRU layers. Mostly LSTM layers are used due to the removed Exploding and vanishing gradient problem.", "The above diagram shows our encoder network. In the encoder network, one word is fed at a time step, and finally, after the nth input word is fed to the LSTM layer the hidden state and cell states become our final state or feature vector. The cell state Cn and the hidden state Hn are sent to the first set of the LSTM layer of the decoder.", "This is how our decoder model looks. Now the first layer receives the inputs from the encoder\u2019s final states that are the hidden and cell states activations. The decoder model takes in the inputs and generates the predicted words of the output sequence, given the previous word generated. So, for LSTM at time step 1 for the decoder has 0 vector input, and Y1 is the predicted word generated, for time step 2, Y1 is fed to the LSTM layer as input and Y2 is the generated word and so on. The decoder generates the words time steps by time steps until the <end> tag is faced.", "This might raise a question, how are the words generated?. Well, here is the answer. The encoder-decoder model is trained on a target set or vocabulary of words. Now, in each step, of the decoder, the LSTMs hidden activation is sent through a softmax layer which generates the probabilities for each word in the vocabulary to be predicted as the next word. The word with the maximum probability is chosen as the output at that time step. Now, how does the model know which word exactly suits the semantics? For this, the model trains on a dataset and transforms the problem into a supervised classification problem. Alongside, the models, usually use word embeddings of the words in the vocabulary from well-known embedding vectors like the word2vec by google or the Glove by Standford NLP. The word embeddings help to gain several insights about the word like whether a given word is similar to a given word or not. Some times TFIDF vectorizations are also used to generate meaningfulness of context words.", "Let\u2019s go by example. Say we have a dataset, which has a collection of long-form reports and their human summaries. The information can be used as labels and targets for training our encoder-decoder networks. We will vectorize our labels and targets, form a vocabulary. Next, we will pick the embeddings up from word2vec or Glove for the words in our vocabulary and then fit the labels and targets to our model for training.", "But this particular summarization problem has an issue. The original document can be very large. Say the document is of 100 lines. Now, when we, as humans, are summarizing from 1\u20135 lines, shall we need to consider the 100th line? No right? When we summarize manually we give more attention to the lines 1\u20135 when we are summarizing those lines, and slowly move forward. This aspect could not be implemented by the normal encoder-decoder model, So, the attention mechanism was introduced.", "The mechanism aims to focus on some specific sequences from the input only, rather than the entire input sequence to predict a word. This approach is very similar to the human approach and seems to solve the problem.", "The above diagram shows the implementation of the attention layer by TensorFlow. We can see the for predicting a word each word in the input sequence is being assigned a weight, called the attention weights. The summation of the vectorized attention weights is used to form the context vectors which are used for prediction.", "Let us examine in detail. The paper proposes the encoder to be composed of a bi-directional GRU RNN and the decoder will have unidirectional GRU RNN. Both the encoder and decoder will have the same number of hidden layers and units.", "Let\u2019s see the attention mechanism here.", "The upper green layer shows the decoder, the Y1 and Y2 are the time step outputs. X1, X2 are the inputs to the encoder. \u201caf0\u201d is the forward activation of input 0 and \u201cab0\u201d is the backward activation of the timestep 0 and so on.", "Let\u2019s say \u2018an\u2019 is (\u2018abn\u2019, \u2018afn\u2019) combined activation at timestep n, so, a0=(af0,ab0), i.e, both forward and backward combined. \u201cWmn\u201d determines how much weight should be given to the input at timestep m while predicting the output at time step n of the decoder.", "The Context vector is the weighted sum of the attention weights. It is given by:", "The above equation states context vector for time step n is equal to the sum of weights given to input at timestep m and the combined activation of the timestep m, for all m timesteps in the input i.e, x time steps if there are x words in the input.", "Now, how are the words obtained?", "Where we obtain e using a neural network with a single hidden state. The network takes in S(n-1) and a(m) (Combined activation of timestep m) and gives e(m,n) as output. The neural network for obtaining \u2018e\u2019 is also trained during the training of our encoder-decoder model.", "We use softmax, so the sums of all the weights assigned to all the input timesteps are always equal to 1.", "This is the overall mechanism of the attention model.", "We have discussed previously, the words predicted by the decoder are generated using a softmax. Now, if we use all the words in our vocabulary as our target word set, the softmax will have a huge number of output nodes, and the prediction will be computationally inefficient. The paper proposes the Large Vocabulary trick proposed by Sebastien Jean, University of Montreal to solve this issue. The model is trained in mini-batches. The target word set or the decoder-vocabulary of each minibatch is restricted to the source document of that particular mini-batch. Now, if we use different subsets of decoder-vocabulary for each mini-batch, there is a chance that the target vocabulary becomes unequal in length, which is very obvious, as different samples have different numbers of lines and a different number of words. So, we add the most frequent words in the total vocabulary to the subset vocabularies to make them of fixed size. This decreases the time requirements and fastens the convergence as with the decrease in target set size the softmax layer also shortens.", "Previously, when we discussed the encoder-decoder model, I had mentioned that we normally use word embeddings to represent the words in the documents after vectorization. Now, let\u2019s try to think about what we actually need the words to indicate in order to summarize. We will realize that the embeddings are not enough, because for summarization we need to focus on the context and the keywords in the piece of text. The embeddings help to get a general idea about a word but it has nothing to do with the context of the text. So, the paper proposed to take into consideration factors like part of speech tags, named-entity tags, and TFIDF statistics of a word alongside embeddings to represent a word. We convert the continuous TFIDF values into categorical value, using bins. Finally, we take all of the features and embeddings for a word and create a new embedding for the words. So, basically the TFIDF, POS tags gives us an idea about how important are the words are in the context of the document and the word embeddings give a general idea about the word. Next, we concatenate them into a single long vector and feed to network. One thing to notice is, we use only word embeddings to represent the words in the target side.", "Next, We will look at another very important aspect proposed by the paper.", "In Natural language handling when we train a model using a supervised model, we often need to handle some words which are not present in our vocabulary. Such words are termed as OOV or out-of-vocabulary. In normal cases, we handle them using \u2018UNK\u2019 tags. But in the case of summarizations, it will not be correct because the words may carry some significance in the summary. The paper proposes a switching decoder pointer to handle such cases. T each time step of the decoder, there is a pointer maintained to the input text. Whenever the decoder faces an OOV term, it points to a term in the input and uses the term from the input text directly. So, the decoder has basically two actions at a time step, it can generate a word from the target dictionary or it can point and copy a word. This decision is taken using a switch If the switch is turned on it generates a word else it copies a word from the input.", "Now, the question is how the switch operates? The switch is a sigmoid activation function over the entire context vector at a particular decoder time step. It is given by:", "\u201cwhere P(si = 1) is the probability of the switch turning on at i th time-step of the decoder, hi is the hidden state, E[oi\u22121] is the embedding vector of the emission from the previous time step, ci is the attention-weighted context vector, and Ws h, Ws e, Ws c, bs and vs are the switch parameters\u201d-Source", "The pointer at each time step must point to a word in order to copy the word, this is decided based on the attention weights distribution for that decoder time stamps.", "\u201cIn the above equation, pi is the pointer value at i th word-position in the summary, sampled from the attention distribution Pa i over the document word-positions j \u2208 {1, . . . , Nd}, where P a i (j) is the probability of i th time-step in the decoder pointing to the j th position in the document, and h d j is the encoder\u2019s hidden state at position j\u201d-Source", "The switch is also trained during the training of the neural network. The function given below is optimized during the training.", "\u201cwhere y and x are the summary and document words respectively, gi is an indicator function\u201d-Source. The indicator function is set to 0 whenever an OOV is faced with respect to the decoder vocabulary. This turns off the switch and a word is copied from the input text.", "This is an overview of the sequence-to-sequence model for text summarization. I encourage you to go through the references for more implementational details.", "One thing to note is, the authors proposed hierarchical Attention layers for long documents. If the documents are very very long, we some times may need to identify the key sentences with the keywords. For this, we need a hierarchical attention mechanism. One level to take care of the importance of the sentence and another level for the importance of the words. The two attention layers operate at the two levels simultaneously.", "The other two most notable approaches are:", "This approach was proposed by Alexander Rush, Facebook AI Research in 2015.", "The above diagram describes facebook\u2019s model. It has three encoders:", "Lastly, a beam search is applied to the results in order to obtain the summarized text.", "This model was proposed by Abigii See from Standford university.", "I felt this model is similar to IBM\u2019s model, but the model uses a coverage mechanism to reduce the repetitions problem of the sequence-to-sequence network.", "You can go through the papers for more details. I will provide the links in the reference.", "In this article, we talked about several approaches by which deep learning is used for text summarization.", "I am a Computer Science and Technology Graduate from NIT, Durgapur. Find Me at https://abhijitroy1998.wixsite.com/abhijitcv"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F7099fa8656fe&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-automatic-text-summarization-2-abstractive-methods-7099fa8656fe&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-automatic-text-summarization-2-abstractive-methods-7099fa8656fe&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-automatic-text-summarization-2-abstractive-methods-7099fa8656fe&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-automatic-text-summarization-2-abstractive-methods-7099fa8656fe&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://unsplash.com/@brett_jordan?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Brett Jordan"}, {"url": "https://unsplash.com/s/photos/page?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://medium.com/@myac.abhijit?source=post_page-----7099fa8656fe--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----7099fa8656fe--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@myac.abhijit?source=post_page-----7099fa8656fe--------------------------------", "anchor_text": "Abhijit Roy"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4c235a4f4b95&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-automatic-text-summarization-2-abstractive-methods-7099fa8656fe&user=Abhijit+Roy&userId=4c235a4f4b95&source=post_page-4c235a4f4b95----7099fa8656fe---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----7099fa8656fe--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7099fa8656fe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-automatic-text-summarization-2-abstractive-methods-7099fa8656fe&user=Abhijit+Roy&userId=4c235a4f4b95&source=-----7099fa8656fe---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7099fa8656fe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-automatic-text-summarization-2-abstractive-methods-7099fa8656fe&source=-----7099fa8656fe---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/understanding-automatic-text-summarization-1-extractive-methods-8eb512b21ecc", "anchor_text": "first article"}, {"url": "https://www.tensorflow.org/tutorials/text/nmt_with_attention", "anchor_text": "Source"}, {"url": "https://arxiv.org/abs/1602.06023", "anchor_text": "Source"}, {"url": "https://arxiv.org/abs/1602.06023", "anchor_text": "Source"}, {"url": "https://arxiv.org/abs/1602.06023", "anchor_text": "Sourc"}, {"url": "https://arxiv.org/abs/1602.06023", "anchor_text": "Source"}, {"url": "https://arxiv.org/abs/1602.06023", "anchor_text": "Source"}, {"url": "https://arxiv.org/abs/1602.06023", "anchor_text": "Source"}, {"url": "https://arxiv.org/abs/1602.06023", "anchor_text": "Source"}, {"url": "https://arxiv.org/abs/1602.06023", "anchor_text": "Source"}, {"url": "https://arxiv.org/abs/1602.06023", "anchor_text": "Source"}, {"url": "https://arxiv.org/abs/1509.00685", "anchor_text": "Source"}, {"url": "https://arxiv.org/abs/1704.04368", "anchor_text": "Source"}, {"url": "https://arxiv.org/abs/1602.06023", "anchor_text": "https://arxiv.org/abs/1602.06023"}, {"url": "https://arxiv.org/abs/1704.04368", "anchor_text": "https://arxiv.org/abs/1704.04368"}, {"url": "https://arxiv.org/abs/1509.00685", "anchor_text": "https://arxiv.org/abs/1509.00685"}, {"url": "https://arxiv.org/abs/1409.0473", "anchor_text": "https://arxiv.org/abs/1409.0473"}, {"url": "https://arxiv.org/abs/1412.2007", "anchor_text": "https://arxiv.org/abs/1409.0473"}, {"url": "https://www.tensorflow.org/tutorials/text/nmt_with_attention", "anchor_text": "https://www.tensorflow.org/tutorials/text/nmt_with_attention"}, {"url": "https://medium.com/tag/text-summarization?source=post_page-----7099fa8656fe---------------text_summarization-----------------", "anchor_text": "Text Summarization"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----7099fa8656fe---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/abstractive-summarization?source=post_page-----7099fa8656fe---------------abstractive_summarization-----------------", "anchor_text": "Abstractive Summarization"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7099fa8656fe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-automatic-text-summarization-2-abstractive-methods-7099fa8656fe&user=Abhijit+Roy&userId=4c235a4f4b95&source=-----7099fa8656fe---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7099fa8656fe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-automatic-text-summarization-2-abstractive-methods-7099fa8656fe&user=Abhijit+Roy&userId=4c235a4f4b95&source=-----7099fa8656fe---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7099fa8656fe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-automatic-text-summarization-2-abstractive-methods-7099fa8656fe&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/@myac.abhijit?source=post_page-----7099fa8656fe--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----7099fa8656fe--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4c235a4f4b95&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-automatic-text-summarization-2-abstractive-methods-7099fa8656fe&user=Abhijit+Roy&userId=4c235a4f4b95&source=post_page-4c235a4f4b95----7099fa8656fe---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F2ba8066c30a7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-automatic-text-summarization-2-abstractive-methods-7099fa8656fe&newsletterV3=4c235a4f4b95&newsletterV3Id=2ba8066c30a7&user=Abhijit+Roy&userId=4c235a4f4b95&source=-----7099fa8656fe---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/@myac.abhijit?source=post_page-----7099fa8656fe--------------------------------", "anchor_text": "Written by Abhijit Roy"}, {"url": "https://medium.com/@myac.abhijit/followers?source=post_page-----7099fa8656fe--------------------------------", "anchor_text": "458 Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----7099fa8656fe--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://abhijitroy1998.wixsite.com/abhijitcv", "anchor_text": "https://abhijitroy1998.wixsite.com/abhijitcv"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4c235a4f4b95&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-automatic-text-summarization-2-abstractive-methods-7099fa8656fe&user=Abhijit+Roy&userId=4c235a4f4b95&source=post_page-4c235a4f4b95----7099fa8656fe---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F2ba8066c30a7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-automatic-text-summarization-2-abstractive-methods-7099fa8656fe&newsletterV3=4c235a4f4b95&newsletterV3Id=2ba8066c30a7&user=Abhijit+Roy&userId=4c235a4f4b95&source=-----7099fa8656fe---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/introduction-to-recommender-systems-1-971bd274f421?source=author_recirc-----7099fa8656fe----0---------------------0fa7adc0_9780_4da2_a054_0accb7ad7882-------", "anchor_text": ""}, {"url": "https://medium.com/@myac.abhijit?source=author_recirc-----7099fa8656fe----0---------------------0fa7adc0_9780_4da2_a054_0accb7ad7882-------", "anchor_text": ""}, {"url": "https://medium.com/@myac.abhijit?source=author_recirc-----7099fa8656fe----0---------------------0fa7adc0_9780_4da2_a054_0accb7ad7882-------", "anchor_text": "Abhijit Roy"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----7099fa8656fe----0---------------------0fa7adc0_9780_4da2_a054_0accb7ad7882-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/introduction-to-recommender-systems-1-971bd274f421?source=author_recirc-----7099fa8656fe----0---------------------0fa7adc0_9780_4da2_a054_0accb7ad7882-------", "anchor_text": "Introduction To Recommender Systems- 1: Content-Based Filtering And Collaborative FilteringHow services like Netflix, Amazon, and Youtube recommend items to the users?"}, {"url": "https://towardsdatascience.com/introduction-to-recommender-systems-1-971bd274f421?source=author_recirc-----7099fa8656fe----0---------------------0fa7adc0_9780_4da2_a054_0accb7ad7882-------", "anchor_text": "11 min read\u00b7Jul 29, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F971bd274f421&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-recommender-systems-1-971bd274f421&user=Abhijit+Roy&userId=4c235a4f4b95&source=-----971bd274f421----0-----------------clap_footer----0fa7adc0_9780_4da2_a054_0accb7ad7882-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/introduction-to-recommender-systems-1-971bd274f421?source=author_recirc-----7099fa8656fe----0---------------------0fa7adc0_9780_4da2_a054_0accb7ad7882-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F971bd274f421&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-recommender-systems-1-971bd274f421&source=-----7099fa8656fe----0-----------------bookmark_preview----0fa7adc0_9780_4da2_a054_0accb7ad7882-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----7099fa8656fe----1---------------------0fa7adc0_9780_4da2_a054_0accb7ad7882-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----7099fa8656fe----1---------------------0fa7adc0_9780_4da2_a054_0accb7ad7882-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----7099fa8656fe----1---------------------0fa7adc0_9780_4da2_a054_0accb7ad7882-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----7099fa8656fe----1---------------------0fa7adc0_9780_4da2_a054_0accb7ad7882-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----7099fa8656fe----1---------------------0fa7adc0_9780_4da2_a054_0accb7ad7882-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----7099fa8656fe----1---------------------0fa7adc0_9780_4da2_a054_0accb7ad7882-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----0fa7adc0_9780_4da2_a054_0accb7ad7882-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----7099fa8656fe----1---------------------0fa7adc0_9780_4da2_a054_0accb7ad7882-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----7099fa8656fe----1-----------------bookmark_preview----0fa7adc0_9780_4da2_a054_0accb7ad7882-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----7099fa8656fe----2---------------------0fa7adc0_9780_4da2_a054_0accb7ad7882-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----7099fa8656fe----2---------------------0fa7adc0_9780_4da2_a054_0accb7ad7882-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----7099fa8656fe----2---------------------0fa7adc0_9780_4da2_a054_0accb7ad7882-------", "anchor_text": "Jacob Marks, Ph.D."}, {"url": "https://towardsdatascience.com/?source=author_recirc-----7099fa8656fe----2---------------------0fa7adc0_9780_4da2_a054_0accb7ad7882-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----7099fa8656fe----2---------------------0fa7adc0_9780_4da2_a054_0accb7ad7882-------", "anchor_text": "How I Turned My Company\u2019s Docs into a Searchable Database with OpenAIAnd how you can do the same with your docs"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----7099fa8656fe----2---------------------0fa7adc0_9780_4da2_a054_0accb7ad7882-------", "anchor_text": "15 min read\u00b7Apr 25"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&user=Jacob+Marks%2C+Ph.D.&userId=f7dc0c0eae92&source=-----4f2d34bd8736----2-----------------clap_footer----0fa7adc0_9780_4da2_a054_0accb7ad7882-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----7099fa8656fe----2---------------------0fa7adc0_9780_4da2_a054_0accb7ad7882-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "20"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&source=-----7099fa8656fe----2-----------------bookmark_preview----0fa7adc0_9780_4da2_a054_0accb7ad7882-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/an-introduction-to-gradient-descent-and-backpropagation-81648bdb19b2?source=author_recirc-----7099fa8656fe----3---------------------0fa7adc0_9780_4da2_a054_0accb7ad7882-------", "anchor_text": ""}, {"url": "https://medium.com/@myac.abhijit?source=author_recirc-----7099fa8656fe----3---------------------0fa7adc0_9780_4da2_a054_0accb7ad7882-------", "anchor_text": ""}, {"url": "https://medium.com/@myac.abhijit?source=author_recirc-----7099fa8656fe----3---------------------0fa7adc0_9780_4da2_a054_0accb7ad7882-------", "anchor_text": "Abhijit Roy"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----7099fa8656fe----3---------------------0fa7adc0_9780_4da2_a054_0accb7ad7882-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/an-introduction-to-gradient-descent-and-backpropagation-81648bdb19b2?source=author_recirc-----7099fa8656fe----3---------------------0fa7adc0_9780_4da2_a054_0accb7ad7882-------", "anchor_text": "An Introduction to Gradient Descent and BackpropagationHow the Machine Learns?"}, {"url": "https://towardsdatascience.com/an-introduction-to-gradient-descent-and-backpropagation-81648bdb19b2?source=author_recirc-----7099fa8656fe----3---------------------0fa7adc0_9780_4da2_a054_0accb7ad7882-------", "anchor_text": "13 min read\u00b7Jun 14, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F81648bdb19b2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-gradient-descent-and-backpropagation-81648bdb19b2&user=Abhijit+Roy&userId=4c235a4f4b95&source=-----81648bdb19b2----3-----------------clap_footer----0fa7adc0_9780_4da2_a054_0accb7ad7882-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/an-introduction-to-gradient-descent-and-backpropagation-81648bdb19b2?source=author_recirc-----7099fa8656fe----3---------------------0fa7adc0_9780_4da2_a054_0accb7ad7882-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "5"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F81648bdb19b2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-gradient-descent-and-backpropagation-81648bdb19b2&source=-----7099fa8656fe----3-----------------bookmark_preview----0fa7adc0_9780_4da2_a054_0accb7ad7882-------", "anchor_text": ""}, {"url": "https://medium.com/@myac.abhijit?source=post_page-----7099fa8656fe--------------------------------", "anchor_text": "See all from Abhijit Roy"}, {"url": "https://towardsdatascience.com/?source=post_page-----7099fa8656fe--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://towardsdatascience.com/make-a-text-summarizer-with-gpt-3-f0917a07189e?source=read_next_recirc-----7099fa8656fe----0---------------------2a740420_c0f6_4433_bc5f_2169e3e7bd3c-------", "anchor_text": ""}, {"url": "https://medium.com/@jaypeterman?source=read_next_recirc-----7099fa8656fe----0---------------------2a740420_c0f6_4433_bc5f_2169e3e7bd3c-------", "anchor_text": ""}, {"url": "https://medium.com/@jaypeterman?source=read_next_recirc-----7099fa8656fe----0---------------------2a740420_c0f6_4433_bc5f_2169e3e7bd3c-------", "anchor_text": "Jay Peterman"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----7099fa8656fe----0---------------------2a740420_c0f6_4433_bc5f_2169e3e7bd3c-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/make-a-text-summarizer-with-gpt-3-f0917a07189e?source=read_next_recirc-----7099fa8656fe----0---------------------2a740420_c0f6_4433_bc5f_2169e3e7bd3c-------", "anchor_text": "Make a Text Summarizer with GPT-3Quick tutorial using Python, OpenAI\u2019s GPT-3, and Streamlit"}, {"url": "https://towardsdatascience.com/make-a-text-summarizer-with-gpt-3-f0917a07189e?source=read_next_recirc-----7099fa8656fe----0---------------------2a740420_c0f6_4433_bc5f_2169e3e7bd3c-------", "anchor_text": "\u00b711 min read\u00b7Jan 23"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff0917a07189e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmake-a-text-summarizer-with-gpt-3-f0917a07189e&user=Jay+Peterman&userId=9731dc608e6c&source=-----f0917a07189e----0-----------------clap_footer----2a740420_c0f6_4433_bc5f_2169e3e7bd3c-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/make-a-text-summarizer-with-gpt-3-f0917a07189e?source=read_next_recirc-----7099fa8656fe----0---------------------2a740420_c0f6_4433_bc5f_2169e3e7bd3c-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff0917a07189e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmake-a-text-summarizer-with-gpt-3-f0917a07189e&source=-----7099fa8656fe----0-----------------bookmark_preview----2a740420_c0f6_4433_bc5f_2169e3e7bd3c-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/fine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80?source=read_next_recirc-----7099fa8656fe----1---------------------2a740420_c0f6_4433_bc5f_2169e3e7bd3c-------", "anchor_text": ""}, {"url": "https://skanda-vivek.medium.com/?source=read_next_recirc-----7099fa8656fe----1---------------------2a740420_c0f6_4433_bc5f_2169e3e7bd3c-------", "anchor_text": ""}, {"url": "https://skanda-vivek.medium.com/?source=read_next_recirc-----7099fa8656fe----1---------------------2a740420_c0f6_4433_bc5f_2169e3e7bd3c-------", "anchor_text": "Skanda Vivek"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----7099fa8656fe----1---------------------2a740420_c0f6_4433_bc5f_2169e3e7bd3c-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/fine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80?source=read_next_recirc-----7099fa8656fe----1---------------------2a740420_c0f6_4433_bc5f_2169e3e7bd3c-------", "anchor_text": "Fine-Tune Transformer Models For Question Answering On Custom DataA tutorial on fine-tuning the Hugging Face RoBERTa QA Model on custom data and obtaining significant performance boosts"}, {"url": "https://towardsdatascience.com/fine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80?source=read_next_recirc-----7099fa8656fe----1---------------------2a740420_c0f6_4433_bc5f_2169e3e7bd3c-------", "anchor_text": "\u00b75 min read\u00b7Dec 15, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513eaac37a80&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80&user=Skanda+Vivek&userId=220d9bbb8014&source=-----513eaac37a80----1-----------------clap_footer----2a740420_c0f6_4433_bc5f_2169e3e7bd3c-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/fine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80?source=read_next_recirc-----7099fa8656fe----1---------------------2a740420_c0f6_4433_bc5f_2169e3e7bd3c-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513eaac37a80&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80&source=-----7099fa8656fe----1-----------------bookmark_preview----2a740420_c0f6_4433_bc5f_2169e3e7bd3c-------", "anchor_text": ""}, {"url": "https://pub.towardsai.net/build-chatgpt-like-chatbots-with-customized-knowledge-for-your-websites-using-simple-programming-f393206c6626?source=read_next_recirc-----7099fa8656fe----0---------------------2a740420_c0f6_4433_bc5f_2169e3e7bd3c-------", "anchor_text": ""}, {"url": "https://lucianosphere.medium.com/?source=read_next_recirc-----7099fa8656fe----0---------------------2a740420_c0f6_4433_bc5f_2169e3e7bd3c-------", "anchor_text": ""}, {"url": "https://lucianosphere.medium.com/?source=read_next_recirc-----7099fa8656fe----0---------------------2a740420_c0f6_4433_bc5f_2169e3e7bd3c-------", "anchor_text": "LucianoSphere"}, {"url": "https://pub.towardsai.net/?source=read_next_recirc-----7099fa8656fe----0---------------------2a740420_c0f6_4433_bc5f_2169e3e7bd3c-------", "anchor_text": "Towards AI"}, {"url": "https://pub.towardsai.net/build-chatgpt-like-chatbots-with-customized-knowledge-for-your-websites-using-simple-programming-f393206c6626?source=read_next_recirc-----7099fa8656fe----0---------------------2a740420_c0f6_4433_bc5f_2169e3e7bd3c-------", "anchor_text": "Build ChatGPT-like Chatbots With Customized Knowledge for Your Websites, Using Simple ProgrammingLike ChatGPT but in a form that you can plug into your website and expand with any kind of tailored information by combining basic\u2026"}, {"url": "https://pub.towardsai.net/build-chatgpt-like-chatbots-with-customized-knowledge-for-your-websites-using-simple-programming-f393206c6626?source=read_next_recirc-----7099fa8656fe----0---------------------2a740420_c0f6_4433_bc5f_2169e3e7bd3c-------", "anchor_text": "\u00b711 min read\u00b7Dec 26, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-artificial-intelligence%2Ff393206c6626&operation=register&redirect=https%3A%2F%2Fpub.towardsai.net%2Fbuild-chatgpt-like-chatbots-with-customized-knowledge-for-your-websites-using-simple-programming-f393206c6626&user=LucianoSphere&userId=d28939b5ab78&source=-----f393206c6626----0-----------------clap_footer----2a740420_c0f6_4433_bc5f_2169e3e7bd3c-------", "anchor_text": ""}, {"url": "https://pub.towardsai.net/build-chatgpt-like-chatbots-with-customized-knowledge-for-your-websites-using-simple-programming-f393206c6626?source=read_next_recirc-----7099fa8656fe----0---------------------2a740420_c0f6_4433_bc5f_2169e3e7bd3c-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "14"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff393206c6626&operation=register&redirect=https%3A%2F%2Fpub.towardsai.net%2Fbuild-chatgpt-like-chatbots-with-customized-knowledge-for-your-websites-using-simple-programming-f393206c6626&source=-----7099fa8656fe----0-----------------bookmark_preview----2a740420_c0f6_4433_bc5f_2169e3e7bd3c-------", "anchor_text": ""}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----7099fa8656fe----1---------------------2a740420_c0f6_4433_bc5f_2169e3e7bd3c-------", "anchor_text": ""}, {"url": "https://medium.com/@timothymugayi?source=read_next_recirc-----7099fa8656fe----1---------------------2a740420_c0f6_4433_bc5f_2169e3e7bd3c-------", "anchor_text": ""}, {"url": "https://medium.com/@timothymugayi?source=read_next_recirc-----7099fa8656fe----1---------------------2a740420_c0f6_4433_bc5f_2169e3e7bd3c-------", "anchor_text": "Timothy Mugayi"}, {"url": "https://betterprogramming.pub/?source=read_next_recirc-----7099fa8656fe----1---------------------2a740420_c0f6_4433_bc5f_2169e3e7bd3c-------", "anchor_text": "Better Programming"}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----7099fa8656fe----1---------------------2a740420_c0f6_4433_bc5f_2169e3e7bd3c-------", "anchor_text": "How To Build Your Own Custom ChatGPT With Custom Knowledge BaseFeed your ChatGPT bot with custom data sources"}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----7099fa8656fe----1---------------------2a740420_c0f6_4433_bc5f_2169e3e7bd3c-------", "anchor_text": "\u00b711 min read\u00b7Apr 7"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fbetter-programming%2F4e61ad82427e&operation=register&redirect=https%3A%2F%2Fbetterprogramming.pub%2Fhow-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e&user=Timothy+Mugayi&userId=34774d6cac27&source=-----4e61ad82427e----1-----------------clap_footer----2a740420_c0f6_4433_bc5f_2169e3e7bd3c-------", "anchor_text": ""}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----7099fa8656fe----1---------------------2a740420_c0f6_4433_bc5f_2169e3e7bd3c-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "83"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4e61ad82427e&operation=register&redirect=https%3A%2F%2Fbetterprogramming.pub%2Fhow-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e&source=-----7099fa8656fe----1-----------------bookmark_preview----2a740420_c0f6_4433_bc5f_2169e3e7bd3c-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/translate-with-gpt-3-9903c4a6f385?source=read_next_recirc-----7099fa8656fe----2---------------------2a740420_c0f6_4433_bc5f_2169e3e7bd3c-------", "anchor_text": ""}, {"url": "https://medium.com/@bnjmn_marie?source=read_next_recirc-----7099fa8656fe----2---------------------2a740420_c0f6_4433_bc5f_2169e3e7bd3c-------", "anchor_text": ""}, {"url": "https://medium.com/@bnjmn_marie?source=read_next_recirc-----7099fa8656fe----2---------------------2a740420_c0f6_4433_bc5f_2169e3e7bd3c-------", "anchor_text": "Benjamin Marie"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----7099fa8656fe----2---------------------2a740420_c0f6_4433_bc5f_2169e3e7bd3c-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/translate-with-gpt-3-9903c4a6f385?source=read_next_recirc-----7099fa8656fe----2---------------------2a740420_c0f6_4433_bc5f_2169e3e7bd3c-------", "anchor_text": "Translate with GPT-3Machine translation but without a machine translation system"}, {"url": "https://towardsdatascience.com/translate-with-gpt-3-9903c4a6f385?source=read_next_recirc-----7099fa8656fe----2---------------------2a740420_c0f6_4433_bc5f_2169e3e7bd3c-------", "anchor_text": "\u00b718 min read\u00b7Nov 22, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9903c4a6f385&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftranslate-with-gpt-3-9903c4a6f385&user=Benjamin+Marie&userId=ad2a414578b3&source=-----9903c4a6f385----2-----------------clap_footer----2a740420_c0f6_4433_bc5f_2169e3e7bd3c-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/translate-with-gpt-3-9903c4a6f385?source=read_next_recirc-----7099fa8656fe----2---------------------2a740420_c0f6_4433_bc5f_2169e3e7bd3c-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "4"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9903c4a6f385&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftranslate-with-gpt-3-9903c4a6f385&source=-----7099fa8656fe----2-----------------bookmark_preview----2a740420_c0f6_4433_bc5f_2169e3e7bd3c-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/how-to-build-your-own-custom-chatgpt-bot-cf4af959adcc?source=read_next_recirc-----7099fa8656fe----3---------------------2a740420_c0f6_4433_bc5f_2169e3e7bd3c-------", "anchor_text": ""}, {"url": "https://arslanmirza.medium.com/?source=read_next_recirc-----7099fa8656fe----3---------------------2a740420_c0f6_4433_bc5f_2169e3e7bd3c-------", "anchor_text": ""}, {"url": "https://arslanmirza.medium.com/?source=read_next_recirc-----7099fa8656fe----3---------------------2a740420_c0f6_4433_bc5f_2169e3e7bd3c-------", "anchor_text": "Arslan Mirza"}, {"url": "https://levelup.gitconnected.com/?source=read_next_recirc-----7099fa8656fe----3---------------------2a740420_c0f6_4433_bc5f_2169e3e7bd3c-------", "anchor_text": "Level Up Coding"}, {"url": "https://levelup.gitconnected.com/how-to-build-your-own-custom-chatgpt-bot-cf4af959adcc?source=read_next_recirc-----7099fa8656fe----3---------------------2a740420_c0f6_4433_bc5f_2169e3e7bd3c-------", "anchor_text": "How To Build Your Own Custom ChatGPT BotA step-by-step guide to building and fine-tuning custom ChatGPT models"}, {"url": "https://levelup.gitconnected.com/how-to-build-your-own-custom-chatgpt-bot-cf4af959adcc?source=read_next_recirc-----7099fa8656fe----3---------------------2a740420_c0f6_4433_bc5f_2169e3e7bd3c-------", "anchor_text": "\u00b79 min read\u00b7Mar 29"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fgitconnected%2Fcf4af959adcc&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fhow-to-build-your-own-custom-chatgpt-bot-cf4af959adcc&user=Arslan+Mirza&userId=35aaa5742af7&source=-----cf4af959adcc----3-----------------clap_footer----2a740420_c0f6_4433_bc5f_2169e3e7bd3c-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/how-to-build-your-own-custom-chatgpt-bot-cf4af959adcc?source=read_next_recirc-----7099fa8656fe----3---------------------2a740420_c0f6_4433_bc5f_2169e3e7bd3c-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "5"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcf4af959adcc&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fhow-to-build-your-own-custom-chatgpt-bot-cf4af959adcc&source=-----7099fa8656fe----3-----------------bookmark_preview----2a740420_c0f6_4433_bc5f_2169e3e7bd3c-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----7099fa8656fe--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----7099fa8656fe--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----7099fa8656fe--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----7099fa8656fe--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----7099fa8656fe--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----7099fa8656fe--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----7099fa8656fe--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----7099fa8656fe--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----7099fa8656fe--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----7099fa8656fe--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}