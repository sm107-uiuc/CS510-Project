{"url": "https://towardsdatascience.com/how-to-understand-lstm-by-playing-football-136c8431e451", "time": 1683009934.708253, "path": "towardsdatascience.com/how-to-understand-lstm-by-playing-football-136c8431e451/", "webpage": {"metadata": {"title": "How to understand LSTM by playing football | by Nechu BM | Towards Data Science", "h1": "How to understand LSTM by playing football", "description": "In the series of articles, we have studied the limits of Feed Forward Neural Network (FFNN) when it comes to understanding contexts. In order to solve the problem, we introduced the concept of\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/swlh/introduction-to-recurrent-neural-networks-rnn-c2374305a630", "anchor_text": "introduced the concept of Recurrent Neural Networks (RNN)", "paragraph_index": 0}, {"url": "https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog1402_1", "anchor_text": "Jeffrey L. Elman", "paragraph_index": 1}, {"url": "http://www.bioinf.jku.at/publications/older/2604.pdf", "anchor_text": "Sepp Hochreiter and J\u00fcrgen Schmidhuber", "paragraph_index": 12}, {"url": "https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0", "anchor_text": "following article.", "paragraph_index": 20}, {"url": "https://bit.ly/36vajnu", "anchor_text": "https://bit.ly/36vajnu", "paragraph_index": 33}], "all_paragraphs": ["In the series of articles, we have studied the limits of Feed Forward Neural Network (FFNN) when it comes to understanding contexts. In order to solve the problem, we introduced the concept of Recurrent Neural Networks (RNN) and we defined the different types there exist. However, our previous analysis did not adequately address key questions on RNN such as what does it make a RNN different? What kind of calculations take place inside a cell? How does the RNN learn?", "The purpose of this article is to address all these questions and, in order to do so, it might be beneficial to approach them by starting with an historical perspective. One of the first attempts in creating models capable of understanding context was developed by Jeffrey L. Elman in the early 90\u2019s. This first RNN, sometimes called Vanilla RNN, was one of the first cells in which two units of the same layer would share information, the \u2018state\u2019 would pass from one to the other.", "As suggested by the title of this article, the aim is to understand LSTM by playing football not talking about history, where is the ball?", "Here it comes, the model below represents a simplification of the actions you must take as a football player. The input for the model is an image of what your eyes would see in the field. Then a RNN layer makes its proper calculation of what your next action should be, it can be \u2018pass\u2019, \u2018wait\u2019, \u2018run\u2019\u2026", "At time t you have the ball and you see one of your colleagues is not being defended, so the RNN decides to pass the ball. The next image is your colleague trying to control the ball, you don\u2019t know if he is going to manage to do so, then you wait before going to attack or defend. At time t+2 he has the ball and you offer yourself if he wants to pass the ball back. He is a very good player, so he decides to run towards the football goal, then your action is to run too. At time t+4 he is shooting the ball, so your first thought is to be ready if there is any rebound. At time t+5 you see the ball entering the goal, so you are with your colleagues celebrating the goal and having and awesome moment.", "Once we felt the energizing moment of scoring and celebrating the goal, is time to see some math. We represent below one of the cells in the RNN layer, what are the calculations that occurs inside?", "At time t+1 the cell receives two inputs, the state from the previous cell S\u0305\u209c and the event x\u0304\u209c\u208a\u2081 , in our example would be the image. To calculate the next state of the cell S\u0305\u209c\u208a\u2081 we add the result from the dot product of the event and the state with its respective weight matrices and we apply an activation function \u03c6. Don\u2019t you know what an activation function is? The concept of activation functions will be explored in greater detail in the next section. The resulting vector enter the cell at time t+2 as one of its inputs. Finally, the prediction y\u0305\u209c\u208a\u2081 is obtained by the dot product of S\u0305\u209c\u208a\u2081 with its weight matrix W\u1d67 .", "So, are we saying that human memory can be replicated with just two calculations? Is it that simple for a Neural Network to understand context? The answer is no, the approach of Elman was a very nice work, but it has one obstacle called vanishing gradient problem and therefore we also know this cell as short memory. Computers have not become wiser than human yet.", "In the example of the representation of the RNN for football decision we have just focused on one move. But what is the result? How much time is left? Is it a League or a Champions game? Is the player I am going to pass the ball to even in my team?", "The Elman structure suffers from vanishing gradient problem: this means that the cell is not able to learn any temporal dependency from 8\u201310 time steps away. We can find a more explicit representation of the problem thanks to the previous image of the RNN model. In this case, we are going to change the RNN layer color (green bubble) by a representation of how much data it can remember from previous steps. At each time step new data is added to the RNN represented by different colors. For the first step, we just have the blue colour displayed whilst for the second step the colour orange is added. Then as we continue through time, the information received from previous time steps starts to disappear while the information given by more recent steps becomes more visible.Our model has changed to the following:", "Even though we have not covered the mathematics behind Vanishing Gradient Descent, from this diagram we can clearly understand the problem. The longer the sequence is, the harder it becomes for the neural network to learn from previous steps.", "So, we have seen that our player has short memory. Then he might forget the goal of the game or his teammates; consequently the decisions he makes can prevent him from winning the match. How can we solve then the problem of short term memory?", "In the mid 90\u2019s Long Short-Term Memory or LSTM was introduced by Sepp Hochreiter and J\u00fcrgen Schmidhuber. Innovation introduced by this theory is the idea of gates, some signals from our data can be saved as Long Term Memory (LTM) while other signals can be kept as Shot Term Memory (STM). This differentiation allows using long-term or short-term information when needed depending on the situation. For a better understanding of the difference between Long and Short Memory let\u2019s focus on time step t + 5 right after our colleague has scored the goal. The final diagram we would like to achieve looks as follows:", "As input, we have the event we are analyzing (the image) and instead of just one state or memory we have two, LTM whose aims is to remember general information that would not be updated very often and STM focusing on precise information to be used. Then we apply the LSTM cell and we output a new LTM, a STM and a prediction. In the middle of the image we have again the green bubble that represents the RNN. If we look inside this bubble, there are 4 different calculations we are going to analyze. For a matter of representation, we have converted the bubble into a rectangle. Conceptually speaking the RNN cell looks as follow:", "There are 3 gates plus one cell state. In some articles you might find there exist 4 gates, but this is because they refer to the cell state as a gate too. In the section \u20183-Mathematical Representation\u2019 we will see the difference.", "To better understand the aim of each of the gates, let\u2019s continue with the example on time step t+5 where our teammate has scored.", "Great, up to this point we have studied the limits of Vanilla RNN proposed by Elman. We understood the vanishing gradient problem and how the model would only be able to create short memory. To solve this problem, we have introduced a new structure of RNN cells known as LSTM composed by different gates. The explanation above is a simplified explanation of how an LSTM works, let\u2019s understand at a deeper level the mathematics that\u2019s being used.", "Once we translate the above diagram and gates into calculations this is how it looks like.", "The first time I saw this image I felt a bit overwhelmed, too many functions and lines inside just one cell. If you also felt like I did, there is no need to worry because if you have understood the concepts explained above; it is going to be very easy to understand this structure.", "Before explaining the LSTM in detail, one word about activation functions. To understand activation functions just think of the neurons in our brain, they work with electrical impulses in an on-off fashion. Neurons are interconnected with each other and they exchange information with those impulses. The activation of certain neurons will trigger our actions. Coming back to our RNN the input is measured with numbers rather than impulses so how can we convert such figures into an impulse? This is exactly when activation functions come into play to do this job for us. Let\u2019s focus on two of them in particular:", "For more information about activation functions I recommend the following article.", "We have converted the diagram into a mathematical representation, let\u2019s do the same thing with the explanation. Once we understood the role of each gate explained with words, now it is time to do it with numbers:", "We explained that the role of this gate is to decide thanks to the short-term memory and the new image which information to forget in the LTM. Therefore, we are using a sigmoid function. As we can see in the image, we represent the LTM as a vector, the output of the sigmoid would be a vector of the same shape. Information from Team and Game will be kept because the output of the activation function is 1, while the output is 0 for the Score. Then we forget the score.", "It is composed of two actions, from one side we calculate the new information by applying a tanh and then we decide thanks to the sigmoid from this new information which one we should keep. New information for \u2018Score\u2019, \u2018Team\u2019 and \u2018Game has been calculated but we will just keep the new \u2018Score\u2019.", "Here we can see clearly why this is not a gate. There is one multiplication and one addition, we are not transforming the data with a tanh or selecting what information to keep with a sigmoid. Once we know what information to keep from the previous LTM and what to save from the new input we have the LTM for the next step.", "This gate will take filtered information from the new LTM and thanks to the STM and the new image will get a new STM. It\u2019s worth noting that the output from this gate is at the same time the new STM and the prediction.", "And that\u2019s a Long Short-Term Memory structure, we use the information from the previous time step thanks to LTM and STM combined with the new information, the image. We apply sigmoid to decide what information to keep and tanh to keep the values between -1 and 1. Each gate has its own role and combinations of activation functions.", "We have to take into account that LSTM is not the only structure, for example Gated Recurrent Unit (GRU) is also a well-known structure. It follows the same idea of gates and activation function, the difference lies in the number of gates and the distribution of each of them.", "If you arrived to this point congratulations! you start to become a RNN expert. It means we understand the limits of a FFNN when it comes to understanding contexts and how to apply RNN to solve it.", "It also means we know there exist different cell architectures. The first one is Vanilla RNN which encounter the vanishing gradient problem, in other words the incapacity to retain dependencies over multiple time steps. This problem of short memory can be solved with a new structure of cell called LSTM. And the most important we dove deep in the structure of a LSTM cell to understand how it works.", "Instead of one state like Vanilla RNN, Long-Short Term memory has two states Long-Term Memory and Short-Term Memory. We analyzed the role of the Forget Gate which is to decide what information is no longer useful from the previous time step. Then the Input Gate filter from the new input what information is relevant. The Cell State take the output from the Forget Gate and the Input Gate to generate the new LTM. Finally, the Use Gate combined the new LTM and the input to create the STM.", "So far, we have focused on the cell itself, in the next articles we will explore multiple ways of using LSTM cells to create for example an encoder-decoder structure and how it allows to plant the seed for attention models.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Scientist & Entrepreneur! Learn Artificial Intelligence and Machine Learning to become a Linchpin \u279c https://bit.ly/36vajnu"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F136c8431e451&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-understand-lstm-by-playing-football-136c8431e451&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-understand-lstm-by-playing-football-136c8431e451&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-understand-lstm-by-playing-football-136c8431e451&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-understand-lstm-by-playing-football-136c8431e451&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----136c8431e451--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----136c8431e451--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@dbenzaquenm?source=post_page-----136c8431e451--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@dbenzaquenm?source=post_page-----136c8431e451--------------------------------", "anchor_text": "Nechu BM"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F56da16d481ba&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-understand-lstm-by-playing-football-136c8431e451&user=Nechu+BM&userId=56da16d481ba&source=post_page-56da16d481ba----136c8431e451---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F136c8431e451&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-understand-lstm-by-playing-football-136c8431e451&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F136c8431e451&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-understand-lstm-by-playing-football-136c8431e451&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/swlh/introduction-to-recurrent-neural-networks-rnn-c2374305a630", "anchor_text": "introduced the concept of Recurrent Neural Networks (RNN)"}, {"url": "https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog1402_1", "anchor_text": "Jeffrey L. Elman"}, {"url": "http://www.bioinf.jku.at/publications/older/2604.pdf", "anchor_text": "Sepp Hochreiter and J\u00fcrgen Schmidhuber"}, {"url": "https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0", "anchor_text": "following article."}, {"url": "https://medium.com/swlh/introduction-to-recurrent-neural-networks-rnn-c2374305a630", "anchor_text": "Introduction to Recurrent Neural Networks (RNN)How are computers able to understand context? What is a recurrent neural network and how it can help us? Are there\u2026medium.com"}, {"url": "https://towardsdatascience.com/how-to-build-a-translation-pipeline-with-rnn-and-keras-57c1cf4a8a7", "anchor_text": "How to build a translation pipeline with RNN and KerasHave you ever wondered how a computer is able to learn multiple languages so fast? Follow this step by step guide to\u2026towardsdatascience.com"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----136c8431e451---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----136c8431e451---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/data-science?source=post_page-----136c8431e451---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/ai?source=post_page-----136c8431e451---------------ai-----------------", "anchor_text": "AI"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----136c8431e451---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F136c8431e451&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-understand-lstm-by-playing-football-136c8431e451&user=Nechu+BM&userId=56da16d481ba&source=-----136c8431e451---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F136c8431e451&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-understand-lstm-by-playing-football-136c8431e451&user=Nechu+BM&userId=56da16d481ba&source=-----136c8431e451---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F136c8431e451&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-understand-lstm-by-playing-football-136c8431e451&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----136c8431e451--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F136c8431e451&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-understand-lstm-by-playing-football-136c8431e451&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----136c8431e451---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----136c8431e451--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----136c8431e451--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----136c8431e451--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----136c8431e451--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----136c8431e451--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----136c8431e451--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----136c8431e451--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----136c8431e451--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@dbenzaquenm?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@dbenzaquenm?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Nechu BM"}, {"url": "https://medium.com/@dbenzaquenm/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "153 Followers"}, {"url": "https://bit.ly/36vajnu", "anchor_text": "https://bit.ly/36vajnu"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F56da16d481ba&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-understand-lstm-by-playing-football-136c8431e451&user=Nechu+BM&userId=56da16d481ba&source=post_page-56da16d481ba--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F4f6c9f72bf01&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-understand-lstm-by-playing-football-136c8431e451&newsletterV3=56da16d481ba&newsletterV3Id=4f6c9f72bf01&user=Nechu+BM&userId=56da16d481ba&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}