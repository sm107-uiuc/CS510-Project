{"url": "https://towardsdatascience.com/dragface-training-a-gan-for-drag-queen-transformation-7daf1958e517", "time": 1683001659.3465002, "path": "towardsdatascience.com/dragface-training-a-gan-for-drag-queen-transformation-7daf1958e517/", "webpage": {"metadata": {"title": "DragFace: Training a GAN for Drag Queen Transformation | by Eros Wang | Towards Data Science", "h1": "DragFace: Training a GAN for Drag Queen Transformation", "description": "Drag queens have been gaining popularity among wider and wider populations nowadays. They also play a strong and inspiring role in the LGBTQ community. They usually attract audiences by their\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/sigmoid/a-brief-introduction-to-gans-and-how-to-code-them-2620ee465c30", "anchor_text": "here", "paragraph_index": 3}, {"url": "https://www.kaggle.com/c/generative-dog-images", "anchor_text": "Kaggle competition", "paragraph_index": 7}, {"url": "https://arxiv.org/pdf/1907.10830.pdf", "anchor_text": "Junho Kim et al. on U-GAT-IT", "paragraph_index": 8}, {"url": "https://colab.research.google.com", "anchor_text": "Google Colab", "paragraph_index": 22}, {"url": "https://github.com/eroswang/DragFace", "anchor_text": "GitHub", "paragraph_index": 39}, {"url": "https://medium.com/@erosw7", "anchor_text": "Medium", "paragraph_index": 40}, {"url": "https://www.linkedin.com/in/eroswang7/", "anchor_text": "LinkedIn", "paragraph_index": 40}, {"url": "https://twitter.com/erosw7", "anchor_text": "Twitter", "paragraph_index": 40}], "all_paragraphs": ["Drag queens have been gaining popularity among wider and wider populations nowadays. They also play a strong and inspiring role in the LGBTQ community. They usually attract audiences by their excellent performing skills and extravagant makeup.", "While most drag queens\u2019 humorous hosting style and fabulous dance moves are hard to imitate, their intricate makeup always amaze me more. On average, it takes about 1.5\u20133 hours for a professional drag queen to finish her makeup. The amount of skills needed to create drag makeup is not any less than oil painting. I have always wondered what would my friends and I would look like if we put on drag makeup.", "Since I do not know any drag makeup artists, and cannot afford a couple hundred dollar budget, I turned to Machine Learning for my solution.", "Recently I have been studying GANs (Generative Adversarial Networks). They are an approach to generative modeling using deep learning methods, such as convolution neural networks (CNNs). GANs are an exciting and rapidly changing field. State-of-the-art GANs are able to generate realistic images across a range of problem domains, most notably in image-to-image translation tasks such as translating photos of summer to winter or day to night, and in generating realistic photos of objects, scenes, and people that even humans cannot tell are fake. Thus, they are the perfect models for this project, which ideally could transform any photo with a clear frontal face to have a full-on drag makeup. If you are interested in learning more about GANs, here is a great article that would be helpful.", "The project plan (with my time estimates):", "To collect good quality drag queen photos and regular selfies photos, my initial thought is to scrape photos from Google image search. I wrote a quick python scraper to collect drag queen photos from Google image search. My target was 3000 photos. However, a lot of images on Google image search have broken links and the algorithm stops after clicking through certain amount of search pages. I ended up with around only 700 images. Bummer.", "One day as I was scrolling through my Instagram feed due to my minor social media addiction (2019 problems, am I right?), it struck me that if I want a large amount of good quality image data, there is no better place than Instagram in today\u2019s world. Thus, I created another python scraper and collected drag queen photos and regular selfies from an Instagram image search using tags like \u201cdrag queen\u201d and \u201cselfie\u201d. This time, I ended up with over 3000 photos of drag queens and regular photos within 10 minutes.", "I have some experience in CNN and deep learning, and I\u2019ve participated in a GAN based Kaggle competition, which asked participants to generate puppy photos using GANs. For that competition, I trained a DC-GAN, WGAN-GP and CGAN. Training these models weren\u2019t easy and I struggled with problems like non-convergence and diminished gradient of the generator.", "This time, I decided to do more research before I started to create my model. I read a lot of recent papers on the state-of-the-art GANs, especially on the ones that have success in image-translation tasks which involve human faces. Eventually, a paper by Junho Kim et al. on U-GAT-IT caught my attention. Their GAN model has transformed girls' selfies into anime images with visually appealing results, which is frankly quite similar to my project goal (from selfies to drag).", "Upon reading the paper and their code in detail, I decided to try to implement their model for the following two reasons:", "1.The most outstanding feature from their paper is that they proposed an Adaptive Layer-Instance Normalization (AdaLIN) function to adaptively select a proper ratio between Instance Normalization (IN) and Layer Normalization (LN) instead of only using one of them. During GAN training, if only IN is used, the features of the source domain will be well preserved thanks to channel-wise normalized feature statistics. However, the amount of translation to target domain might be insufficient as the global style might not be captured by Instance Normalization. On the other hand, if only LN is used, the target domain style might be well transferred by virtue of layer-wise normalized feature statistics. But the features of the source domain might be less well preserved. By adaptively selecting a ratio of IN and LN according to source and target domain distributions, the model can produce more visually appealing results.", "The paper illustrated this theory with an example. The picture below shows the comparison from the selfie2anime model. Images in (a) are the source files. (b) has translated images using AdaLIN. (c) has images translated using Instance Normaliztion only. (d) has images translated using Layer Normalization only. It\u2019s clear to see that (b) has the most visually appealing results. (c) did a good job retaining the details of source images such as cheek bones and facial expression but did not blend well with the anime features. (d) captured the anime features well but did not translate enough details from source files.", "2. Instead of using the basic MinMax loss function for GANs, the paper implemented 4 different loss functions: Adversarial loss, Cycle loss, Identity loss and CAM loss.", "Adversarial loss tries to match the generated images with the target distribution. The Cycle loss applies a cycle consistency constraint on the generator. It requires the generator to translate the source image to target domain then successfully translate back to the source domain. This helps with the mode collapse problem, which is a big pain in a lot of GAN model training. The Identity loss helps to ensure that the color distribution of the source and target images are similar. If an image from the target domain is used in the generator to translate the image, the image should not change. The CAM loss helps the generator and discriminator to know where to improve on both domains.", "There are many other features that I like in the paper. Due to the limited length of this blog, please refer to the paper for more details.", "Before jumping into the model building, one crucial step was to prepare the data in the right format. I collected 3000 drag queen photos and 3000 selfies, but not all of them had a clear frontal face and not all of them were cropped properly to only exhibit the facial area. To prepare the data, I leveraged Open-CV\u2019s Cascade Classifier and wrote a python script to identify the faces in the dataset and crop them so that the faces are in the middle. Afterwards, I manually curated my dataset and eliminated bad quality photos. I ended up with 1000 drag photos and 1000 selfies.", "When I collected the regular selfies, initially I was only going to include male selfies, as the majority of traditional drag queens are transformed from male forms. However, I noticed a substantial female-transformed drag photos in my dataset. Thus, I included female selfies in my dataset as well. It would be interesting to explore the effect of different sexes on the model results in the future.", "Another thing that caught my attention was that most of all the drag photos have wigs, even though I tried to only include the face in the final photos. While a lot of selfies, especially male selfies, have short hair. The wigs ended up playing an interesting role in my model training. I will further discuss it in section 4.", "Similar to any other GAN, my model has a generator and discriminator. The generator consists of an encoder, a decoder, and an auxiliary classifier. The design of the generator is illustrated below:", "The auxiliary classifier is trained to learn the weights of each layer of feature map, which then used to calculate a set of domain specific attention feature maps. Then parameters in the residual blocks (AdaLIN) of the decoder can be dynamically computed from the attention map through a fully connected layer.", "Similarly, the discriminator also has an encoder, an auxiliary classifier. However, instead of having a decoder with residual blocks, the discriminator has a classifier which classifies whether an image is from target domain or generated by the model.", "Everyone likes free resources, especially free GPU. Training machine learning models can get expensive quickly. Even though GCP and AWS offer relatively cheap GPUs, training a model for just a few days can cost over $100. And GANs are known for their lengthy training time.", "Luckily, I have a relatively small dataset. I was able to train my model completely on Google Colab by mounting my dataset on Google Drive. If you are interested in machine learning but limited by computing power, I recommend you check out Google Colab. Another great thing about Google Colab is that you can log on to it from any machine. It gives great flexibility if you have a busy daily schedule like me.", "For this specific model, each iteration translates one image and produces one set of generator and discriminator loss values. My target is to train the model for 300,000 iterations and monitor the results along the way. On Colab, it takes about 3 seconds to train one iteration, which means it will take me about 250 hours non-stop to reach 300,000 iterations. Ugh.", "But don\u2019t be upset just yet. There is one neat trick that actually saved my life. In Pytorch, the following code can greatly increase the training efficiency:", "This flag enables the benchmark mode in cudnn. In the benchmark mode, if the input size does not vary in the network, cudnn will look for the optimal set of algorithms for that particular configuration (which takes some time). This usually leads to faster runtime, which in my case reduced runtime per iteration to 1 second. Yay!", "However, take this trick with a grain of salt. If the input sizes changes at each iteration, then cudnn will benchmark every time a new size appears, possibly leading to worse runtime performance.", "As I monitored the model results, I quickly realized that some selfie transformations were worse than others. Digging deeper, the problems reveal themselves. These selfies either have glasses on or have obstacles near the face. I decided to stop my training immediately as I\u2019ve learnt from experience that nothing is more important in machine learning than good quality dataset, no matter how genius your algorithm is. Thus, I re-scanned my dataset carefully, and removed any photos that were unclear, had glasses or had obstacles blocking the face. I included some additional photos as well.", "To ensure my model is training properly, I collected the generator loss and discriminator loss for each iteration and graphed their trends. The following image shows the log transformed loss value graphs up till 295,000 iterations.", "The discriminator loss is relatively consistent and small, which means that the discriminator is always good at telling which drag images are real and which are generated by the model. The generator losses start with large values then gradually reduces as training progress. There is constant volatility in both generator and discriminator loss values throughout training. The volatility is expected since each iteration consists of only one photo, and the difficultly of translation varies a lot based on conditions like lighting, angle and image quality. Thus, different input images are likely to have very different loss values.", "Another way to track training progress is to simply look at the results. Let\u2019s take a look at the results progression for two sample photos.", "Doesn\u2019t look so bad does it! The model appears to capture more outstanding features of drag makeup as training goes: the big smoky eyes, colorful and glossy lips, highlight on the nose, and shadow on the cheeks. Starting from 100,000 iterations, the translated photos already look like professional drag images. As training going further, the makeup stands out more and more. Exactly what I want!", "4.5 A good memory can be bad", "Just when I\u2019m about to sit back and relax and watch the model train further, I noticed some random strokes near the face of translated images. At first I thought it was due to input image quality, but it kept getting worse. At 130,000 iterations, I could only see some light lines which could be mistaken as shadows. At 240,000 iterations, they became clear wave shape patterns, surrounding the face. I then realized that these wave-like patterns are actually generated wigs from the model. Since most drag photos have big colorful wigs, it makes sense for the model to gradually learn to apply wigs to the images as well.", "However, as the model starts to focus more and more on the wigs, the translated wave-like patterns make the photo background look distorted and less visually appealing.", "This may be the results of AdaLIN. AdaLIN helps the model successfully retain large amounts of facial features from source domain, and blend in with drag makeup features from target domain. However, for hair, the model should probably retain as many original features as possible for the image to look realistic. For future development, I could modify the model so that the AdaLIN applies the ratio of instance vs. layer normalization varying as a function of the location on the image (facial area uses more instance normalization and the outer area uses more layer normalization)", "Aside from translation photos of myself and my friends, I translated some celebrity photos with my model. Can you guess who are the following celebrities post their fabulous drag transformation? Please comment their names below of your guesses! (Or better, also comment on what do you think their drag names would be! :D)", "In total, it took me four weeks to finish this project, one week longer than originally estimated. I learned so much from this project and overall happy with the results. Looking back, these are the most valuable things I learned:", "As I mentioned in section 4.5, the next step for me is to look into how to improve the translated wigs appearances. I might also collect more data, since the current training set size is only 1000 per class. I am interested to see if more data would bring the model more realistic results.", "The code for this project is available on my GitHub. Check it out!", "If you enjoyed this, feel free to follow me on Medium or connect on LinkedIn & Twitter!", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F7daf1958e517&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdragface-training-a-gan-for-drag-queen-transformation-7daf1958e517&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdragface-training-a-gan-for-drag-queen-transformation-7daf1958e517&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdragface-training-a-gan-for-drag-queen-transformation-7daf1958e517&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdragface-training-a-gan-for-drag-queen-transformation-7daf1958e517&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----7daf1958e517--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----7daf1958e517--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@erosw7?source=post_page-----7daf1958e517--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@erosw7?source=post_page-----7daf1958e517--------------------------------", "anchor_text": "Eros Wang"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc370c0edc896&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdragface-training-a-gan-for-drag-queen-transformation-7daf1958e517&user=Eros+Wang&userId=c370c0edc896&source=post_page-c370c0edc896----7daf1958e517---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7daf1958e517&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdragface-training-a-gan-for-drag-queen-transformation-7daf1958e517&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7daf1958e517&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdragface-training-a-gan-for-drag-queen-transformation-7daf1958e517&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/sigmoid/a-brief-introduction-to-gans-and-how-to-code-them-2620ee465c30", "anchor_text": "here"}, {"url": "https://www.kaggle.com/c/generative-dog-images", "anchor_text": "Kaggle competition"}, {"url": "https://arxiv.org/pdf/1907.10830.pdf", "anchor_text": "Junho Kim et al. on U-GAT-IT"}, {"url": "https://arxiv.org/pdf/1907.10830.pdf", "anchor_text": "https://arxiv.org/pdf/1907.10830.pdf"}, {"url": "https://arxiv.org/pdf/1907.10830.pdf", "anchor_text": "https://arxiv.org/pdf/1907.10830.pdf"}, {"url": "https://arxiv.org/pdf/1907.10830.pdf", "anchor_text": "https://arxiv.org/pdf/1907.10830.pdf"}, {"url": "https://colab.research.google.com", "anchor_text": "Google Colab"}, {"url": "https://github.com/eroswang/DragFace", "anchor_text": "GitHub"}, {"url": "https://medium.com/@erosw7", "anchor_text": "Medium"}, {"url": "https://www.linkedin.com/in/eroswang7/", "anchor_text": "LinkedIn"}, {"url": "https://twitter.com/erosw7", "anchor_text": "Twitter"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----7daf1958e517---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----7daf1958e517---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/data-scientist?source=post_page-----7daf1958e517---------------data_scientist-----------------", "anchor_text": "Data Scientist"}, {"url": "https://medium.com/tag/computer-vision?source=post_page-----7daf1958e517---------------computer_vision-----------------", "anchor_text": "Computer Vision"}, {"url": "https://medium.com/tag/gans?source=post_page-----7daf1958e517---------------gans-----------------", "anchor_text": "Gans"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7daf1958e517&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdragface-training-a-gan-for-drag-queen-transformation-7daf1958e517&user=Eros+Wang&userId=c370c0edc896&source=-----7daf1958e517---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7daf1958e517&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdragface-training-a-gan-for-drag-queen-transformation-7daf1958e517&user=Eros+Wang&userId=c370c0edc896&source=-----7daf1958e517---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7daf1958e517&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdragface-training-a-gan-for-drag-queen-transformation-7daf1958e517&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----7daf1958e517--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F7daf1958e517&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdragface-training-a-gan-for-drag-queen-transformation-7daf1958e517&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----7daf1958e517---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----7daf1958e517--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----7daf1958e517--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----7daf1958e517--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----7daf1958e517--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----7daf1958e517--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----7daf1958e517--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----7daf1958e517--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----7daf1958e517--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@erosw7?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@erosw7?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Eros Wang"}, {"url": "https://medium.com/@erosw7/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "18 Followers"}, {"url": "https://www.linkedin.com/in/eroswang7/", "anchor_text": "https://www.linkedin.com/in/eroswang7/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc370c0edc896&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdragface-training-a-gan-for-drag-queen-transformation-7daf1958e517&user=Eros+Wang&userId=c370c0edc896&source=post_page-c370c0edc896--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Fc370c0edc896%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdragface-training-a-gan-for-drag-queen-transformation-7daf1958e517&user=Eros+Wang&userId=c370c0edc896&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}