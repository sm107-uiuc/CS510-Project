{"url": "https://towardsdatascience.com/assessing-generalization-in-reward-learning-implementations-and-experiments-de02e1d08c0e", "time": 1683015926.005418, "path": "towardsdatascience.com/assessing-generalization-in-reward-learning-implementations-and-experiments-de02e1d08c0e/", "webpage": {"metadata": {"title": "Assessing Generalization in Reward Learning with Procedurally Generated Games (2/2) | by Max Chiswick | Towards Data Science", "h1": "Assessing Generalization in Reward Learning with Procedurally Generated Games (2/2)", "description": "Note: This is the second of two blog posts (part one). In these posts, we describe a project we undertook to assess the ability of reward learning agents to generalize. The implementation for this\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/@chisness/assessing-generalization-in-reward-learning-intro-and-background-da6c99d9e48", "anchor_text": "one", "paragraph_index": 0}, {"url": "https://github.com/lzil/procedural-generalization", "anchor_text": "available", "paragraph_index": 0}, {"url": "https://support.minitab.com/en-us/minitab-express/1/help-and-how-to/modeling-statistics/regression/supporting-topics/basics/a-comparison-of-the-pearson-and-spearman-correlation-methods/", "anchor_text": "two different types of correlations", "paragraph_index": 14}, {"url": "https://gym.openai.com/envs/CartPole-v0/", "anchor_text": "CartPole", "paragraph_index": 18}, {"url": "https://medium.com/@chisness/assessing-generalization-in-reward-learning-intro-and-background-da6c99d9e48?sk=eac2f5f9d0536546869350c686873728", "anchor_text": "previous blog post", "paragraph_index": 18}, {"url": "https://machinelearningmastery.com/early-stopping-to-avoid-overtraining-neural-network-models/", "anchor_text": "early stopping", "paragraph_index": 25}, {"url": "https://towardsdatascience.com/regularization-in-machine-learning-76441ddcf99a", "anchor_text": "regularization", "paragraph_index": 25}, {"url": "https://chisness.medium.com/assessing-generalization-in-reward-learning-intro-and-background-da6c99d9e48", "anchor_text": "post one", "paragraph_index": 39}], "all_paragraphs": ["Note: This is the second of two blog posts (part one). In these posts, we describe a project we undertook to assess the ability of reward learning agents to generalize. The implementation for this project is available on GitHub.", "In the first post, we reviewed some fundamental background material and described the inspiration for as well as the aims of our project. In doing so, we discussed a number of papers that served as the launching point for our experiments.", "We based our implementations on T-REX [Brown and Goo et al. 2019]. We chose it for its straightforward setup, as well as the open-source implementation provided by the authors, which would save us substantial time. According to the paper, the algorithm showed strong results on Atari games, so we wanted to evaluate how well those results would carry over to Procgen environments, where levels would be randomly generated and thus could not be memorized.", "We ran the T-REX algorithm on four Procgen game environments: CoinRun, FruitBot, StarPilot, and BigFish. As a brief recap: the algorithm is supposed to learn to play the games well without having access to the game score. The game objective must be inferred only from the provided set of ranked demonstrations and the resulting predictions of the reward model.", "These are the algorithm steps that we implemented:", "In order to train anything at all, we needed a dataset. For T-REX, the dataset consists of ranked demonstrations from the environment. How many of them? In the T-REX paper, the authors were able to obtain good results on Atari with just 12 ranked demonstrations. However, we suspected that we would need quite a bit more, as we believed that the Procgen environments could be a good deal more difficult to learn because of the inherently random levels.", "Either way, we needed to produce demonstrations of varying quality from the four Procgen environments we wanted to test on. To do so, we could either play the games ourselves or we could train reinforcement learning agents on those environments, using the true rewards provided by the actual environments themselves.", "We chose the latter; having trained agents allows us to generate demonstrations hundreds of times faster compared to the manual approach. Why is it \u201clegal\u201d to use the true rewards when the purpose of reward learning is to learn without being given that information? During training, the algorithm has access to only the demonstrations, and not the rewards that the demonstrations earned.", "Fortunately, the Procgen paper came with code that we could use in order to train a number of agents to various degrees of performance. Even having this speed up, generating \u201cfresh\u201d demos every time would still take a few minutes of waiting at the beginning of every experiment. This would have substantially reduced our efficiency at iteration and improvement of the code, so we decided to generate all of the demonstrations before running any experiments \u2014 thousands of them for each environment and ranked them based on total reward earned", "This way, we created a robust source of demonstrations for any environment, of any quality, of any length, whenever we wanted them.", "Now that we had our demonstration dataset, it was time to get to training our very first reward models using the T-REX procedure.", "In order to evaluate the quality of the reward models, we could train agents using them with a reinforcement learning algorithm and evaluate how the agents performed, but this is computationally expensive. Thus, we looked for an interim metric by which we could examine how close our trained reward model would be to the true reward model. We decided to use a simple measure, the correlation coefficient. This interim metric is probably a less reliable measure for the reward model. However, introducing this metric reduced the time each experiment took by about 5x, allowing us to get many more data points for assessing the algorithm. We would then select the best reward models and run an RL algorithm on them, instead of having to run it on all of the models.", "Our goal is to have a high correlation over environment episodes between the trained reward model and the true environment reward model, which would imply a trained reward model that is similar to the true (target) model. For each of our four chosen environments, we ran tests that varied the number of demonstrations provided to T-REX. We measured the correlation between the total reward for the game as predicted by the reward model and the actual reward collected in this game.", "Have a look at our first results:", "On the x-axis is the number of demonstrations used in the experiment, and on the y-axis is the correlation coefficient. We show two different types of correlations with orange and blue lines. Each large point in the graph represents the average result of 5 different runs within each number of demos, which are represented by the smaller points.", "We made three main observations from this figure:", "Our next step was to train reinforcement learning agents on Procgen using our reward models that showed the highest correlation in place of true rewards from the environment. Here\u2019s what we got:", "The clips generally look pretty good, but something\u2019s off. If you look at FruitBot, you\u2019ll see that although our trained agent has learned not to hit walls, it hasn\u2019t learned a single thing about avoiding non-fruit items, which collectively constitute half of the game objects. Recall that the mission in the game is to survive while eating as much fruit as possible and avoiding all other foods. Here\u2019s what an ideal agent would look like:", "We conjectured that our trained agents were mainly learning to survive, that is, to keep the episode going for as long as possible (optimizing what\u2019s called the \u201clive-long\u201d objective). An example of an environment where surviving directly leads to reward is CartPole (shown in the previous blog post), where the agent receives a reward of +1 for every second that it keeps a pole from falling over. In FruitBot, a large reward is received for making it to the end of the level, so maybe the agent had learned the importance of this, but not about the importance of avoiding non-fruits.", "Before jumping to conclusions, though, we wanted to do some further testing. We realized that this \u201clive-long\u201d objective makes a great deal of sense to use as a baseline with which to compare our reward models in general \u2014 in other words, a good reward model trained on a Procgen environment should be able to do better than a simple reward model that returns a +1 reward for every timestep the agent is alive, like in CartPole. By using such a baseline, we would be testing for this. So we ran some comparisons.", "Below we show the same correlation plots as above, but now with live-long baselines (represented by red dashed lines), showing that our reward models are generally performing slightly worse or close to the baseline, even with 100 and 200 demonstrations! CoinRun is an exception because the baseline itself is so poorly correlated.", "Although it would seem that a correlation of over 75% between the learned reward model and the true reward model would be quite strong, the high live-long baseline correlations were concerning because they show that it is possible to have a highly correlated reward model that actually has no relation to the environment!", "Deep reinforcement learning is tricky. Results that look encouraging might not be so great upon closer inspection, as we saw in the last section. We focused our efforts on FruitBot, where the issue was most glaring, and looked at how true and predicted reward varied across individual episodes:", "Each graph shows the true (green) and predicted (orange) rewards of an agent playing in different levels. On the x-axis is the timestep of the episode, and on the y-axis is the cumulative reward earned up until that timestep. In FruitBot, if an episode reaches 420 timesteps, the episode automatically ends and the agent is given a large reward (indicated by the green spikes near the end). Ideally, the green and orange lines would look very similar since the learned reward should be similar to the environment\u2019s true reward.", "Unfortunately, they\u2019re not. In fact, other than the spike near the end, the predicted and true rewards seem quite unrelated to each other. What is going on?", "We looked for bugs in our code and tried several ways of improving our reward model. Without getting too much into the details, we tried the standard tried-and-true methods of \u201cmassaging\u201d our training procedure, including elements such as early stopping and regularization, as well as changing the neural network architecture of the reward model.", "None of these changes, however, seemed to make any significant difference, as shown in these updated plots:", "Next, we decided that we would simplify the actual task. If the agent could learn in a simplified version, then we would gradually increase the difficulty of the task to figure out which specific elements were causing the problems. We knew that the algorithm worked in the Brown et al. paper with only 12 demonstrations on Atari games and tried to make the task at least as easy in Procgen.", "First, we would modify Procgen so that game levels go in a fixed sequential order instead of a random order. This is how Atari games work and this makes it much easier for an agent to learn.", "Moreover, we would go far beyond using 12 demonstrations and use 150 demonstrations in the T-REX algorithm to provide much more data for learning. This should lead to a more precise reward model. Yet even after these modifications, the FruitBot reward model wasn\u2019t satisfactory.", "At this point, we were convinced that we had done something incorrectly, because it seemed that our modified tasks were at least as easy as those in the Brown paper, where the T-REX algorithm trained successful agents.", "After not being able to produce good reward models even in the simplified setting, we decided to have a closer look at the original implementation and results reported by the authors to pinpoint exactly where we diverged. Because the authors had originally written the algorithm to work on Atari, we couldn\u2019t make direct comparisons. However, we could look at how the reward models they provided would fare when compared to the live-long baseline we had established earlier. So, we took a T-REX-trained reward model from the Atari game Space Invaders and checked how its predicted reward would compare with the true reward given by the environment:", "The red line should look like the green line, but they are quite different, just as we had seen in our earlier plots for the FruitBot game. It seems like the agent was learning approximately a constant positive reward, similar to that of the live-long baseline.", "Upon further investigation, we noted that the output of the original T-REX algorithm was passed through a sigmoid function in the implementation supplied by the authors of the paper, which would restrict predicted reward values to be between 0 and 1.", "We suspected that this would bias the reward model towards the live-long approach, due to the ease with which a small, constant positive reward could be given.", "We contacted Daniel Brown, the lead author of the T-REX paper, to discuss these concerns, and he was immensely responsive and helpful in answering our questions and debugging possible issues. As it turns out, he had considered this issue himself and even ran the relevant experiments in one of his later papers \u2014 he trained agents with both the T-REX reward model and the live-long objective and reported how their performance compared. According to the reported results, the T-REX algorithm in fact produced a meaningful model that outperformed the baseline.", "We decided to run the corresponding experiments ourselves. As we have mentioned earlier we couldn\u2019t afford to run as many \u201cfull\u201d tests as we wanted due to computational constraints, so we only ran experiments for the FruitBot environment in the easiest (sequential) mode.", "As you can see, in our experiments, we couldn\u2019t get the agents that would outperform those trained on the live-long objective baseline. To be fair, the FruitBot is a tough game for the T-REX algorithm and in other games we did sometimes get agents that would outperform the baseline, but rarely so.", "We are not experienced deep reinforcement learning researchers and it is possible that we are mistaken in our conclusions, but to us it seems that the T-REX algorithm beats the baseline by a quite small margin when it does. We were planning to look at how this margin would change if we introduced the randomness of Procgen levels, and given how close to the baseline we\u2019d start with \u2014 the difference would be very hard to spot.", "After some fairly lengthy discussions, we decided it would be best to move on from T-REX and try another reward learning algorithm. After all, the original inspiration of our project isn\u2019t unique to T-REX \u2014 we would like to investigate reward learning algorithms in general. We initially picked T-REX because it seemed easiest to implement as well as potentially manipulate; however, it might have been better in retrospect to attempt to use a more established reward learning method to start with, especially since we were applying the algorithm to a more difficult setting. Most notably, this suggests implementing code from Christiano et al. 2017 (discussed in post one) that has generally been cited as an important result that helped kick off the use of reward learning in deep reinforcement learning.", "Although implementations of the algorithm from this paper are available on GitHub, none were in a condition that we could integrate very easily into our project. Therefore, the easiest way to move forward would be to re-implement this from scratch.", "Originally, we were interested in tackling the problem of measuring the capability of reward learning agents to generalize. We believed this was an important problem because, in order for an agent to be practically deployed in the real world, it would need to be able to robustly maneuver in unseen situations. Previous tests of generalization have been limited to standard reinforcement learning algorithms, and we hoped to help bridge that gap by applying the same tools, such as Procgen, to reward learning.", "In short, we found that the reward functions we learned were quite brittle. After training a number of reward learning agents on a number of different Procgen environments and under a number of different parameter settings, we were unable to obtain reward functions that could then be used to further robustly train new agents. We experimented with the neural network architecture, but this was difficult as there were few principles to guide our exploration other than intuition. We tried some classic changes \u2014 adding convolutional layers, changing filter sizes, changing the loss function to predict multiple steps, etc. \u2014 and obtained middling results that were sometimes better than what we already had, but not as good as we\u2019d hoped.", "As with any deep learning project, network performance can be subject to precise changes in architecture or training procedure. Although we tried to replicate the method found in the T-REX paper as well as we could, it\u2019s always feasible that there exist some parameter settings that would have produced good reward functions.", "Although our project unfortunately hasn\u2019t led to interesting results, we gained invaluable experience and learned important lessons, some of which we\u2019d like to share:", "This sums up our journey, its results, and lessons learned. We wish every aspiring researcher had a chance to experience what we did throughout this project and we\u2019re infinitely grateful to the people who made this possible."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fde02e1d08c0e&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fassessing-generalization-in-reward-learning-implementations-and-experiments-de02e1d08c0e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fassessing-generalization-in-reward-learning-implementations-and-experiments-de02e1d08c0e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fassessing-generalization-in-reward-learning-implementations-and-experiments-de02e1d08c0e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fassessing-generalization-in-reward-learning-implementations-and-experiments-de02e1d08c0e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://chisness.medium.com/?source=post_page-----de02e1d08c0e--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----de02e1d08c0e--------------------------------", "anchor_text": ""}, {"url": "https://chisness.medium.com/?source=post_page-----de02e1d08c0e--------------------------------", "anchor_text": "Max Chiswick"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F98505f8c082&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fassessing-generalization-in-reward-learning-implementations-and-experiments-de02e1d08c0e&user=Max+Chiswick&userId=98505f8c082&source=post_page-98505f8c082----de02e1d08c0e---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----de02e1d08c0e--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fde02e1d08c0e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fassessing-generalization-in-reward-learning-implementations-and-experiments-de02e1d08c0e&user=Max+Chiswick&userId=98505f8c082&source=-----de02e1d08c0e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fde02e1d08c0e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fassessing-generalization-in-reward-learning-implementations-and-experiments-de02e1d08c0e&source=-----de02e1d08c0e---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/@chisness/assessing-generalization-in-reward-learning-intro-and-background-da6c99d9e48", "anchor_text": "one"}, {"url": "https://github.com/lzil/procedural-generalization", "anchor_text": "available"}, {"url": "https://support.minitab.com/en-us/minitab-express/1/help-and-how-to/modeling-statistics/regression/supporting-topics/basics/a-comparison-of-the-pearson-and-spearman-correlation-methods/", "anchor_text": "two different types of correlations"}, {"url": "https://gym.openai.com/envs/CartPole-v0/", "anchor_text": "CartPole"}, {"url": "https://medium.com/@chisness/assessing-generalization-in-reward-learning-intro-and-background-da6c99d9e48?sk=eac2f5f9d0536546869350c686873728", "anchor_text": "previous blog post"}, {"url": "https://machinelearningmastery.com/early-stopping-to-avoid-overtraining-neural-network-models/", "anchor_text": "early stopping"}, {"url": "https://towardsdatascience.com/regularization-in-machine-learning-76441ddcf99a", "anchor_text": "regularization"}, {"url": "https://www.kaggle.com/general/187778", "anchor_text": "source"}, {"url": "https://chisness.medium.com/assessing-generalization-in-reward-learning-intro-and-background-da6c99d9e48", "anchor_text": "post one"}, {"url": "https://aisafety.camp/", "anchor_text": "AI Safety Camp"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----de02e1d08c0e---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----de02e1d08c0e---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/genewardlearningprocgen?source=post_page-----de02e1d08c0e---------------genewardlearningprocgen-----------------", "anchor_text": "Genewardlearningprocgen"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----de02e1d08c0e---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/ai?source=post_page-----de02e1d08c0e---------------ai-----------------", "anchor_text": "AI"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fde02e1d08c0e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fassessing-generalization-in-reward-learning-implementations-and-experiments-de02e1d08c0e&user=Max+Chiswick&userId=98505f8c082&source=-----de02e1d08c0e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fde02e1d08c0e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fassessing-generalization-in-reward-learning-implementations-and-experiments-de02e1d08c0e&user=Max+Chiswick&userId=98505f8c082&source=-----de02e1d08c0e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fde02e1d08c0e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fassessing-generalization-in-reward-learning-implementations-and-experiments-de02e1d08c0e&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://chisness.medium.com/?source=post_page-----de02e1d08c0e--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----de02e1d08c0e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F98505f8c082&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fassessing-generalization-in-reward-learning-implementations-and-experiments-de02e1d08c0e&user=Max+Chiswick&userId=98505f8c082&source=post_page-98505f8c082----de02e1d08c0e---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F98505f8c082%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fassessing-generalization-in-reward-learning-implementations-and-experiments-de02e1d08c0e&user=Max+Chiswick&userId=98505f8c082&source=-----de02e1d08c0e---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://chisness.medium.com/?source=post_page-----de02e1d08c0e--------------------------------", "anchor_text": "Written by Max Chiswick"}, {"url": "https://chisness.medium.com/followers?source=post_page-----de02e1d08c0e--------------------------------", "anchor_text": "30 Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----de02e1d08c0e--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F98505f8c082&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fassessing-generalization-in-reward-learning-implementations-and-experiments-de02e1d08c0e&user=Max+Chiswick&userId=98505f8c082&source=post_page-98505f8c082----de02e1d08c0e---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F98505f8c082%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fassessing-generalization-in-reward-learning-implementations-and-experiments-de02e1d08c0e&user=Max+Chiswick&userId=98505f8c082&source=-----de02e1d08c0e---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/assessing-generalization-in-reward-learning-intro-and-background-da6c99d9e48?source=author_recirc-----de02e1d08c0e----0---------------------987d790e_8d67_4273_a7d1_a73d739fcb69-------", "anchor_text": ""}, {"url": "https://chisness.medium.com/?source=author_recirc-----de02e1d08c0e----0---------------------987d790e_8d67_4273_a7d1_a73d739fcb69-------", "anchor_text": ""}, {"url": "https://chisness.medium.com/?source=author_recirc-----de02e1d08c0e----0---------------------987d790e_8d67_4273_a7d1_a73d739fcb69-------", "anchor_text": "Max Chiswick"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----de02e1d08c0e----0---------------------987d790e_8d67_4273_a7d1_a73d739fcb69-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/assessing-generalization-in-reward-learning-intro-and-background-da6c99d9e48?source=author_recirc-----de02e1d08c0e----0---------------------987d790e_8d67_4273_a7d1_a73d739fcb69-------", "anchor_text": "Assessing Generalization in Reward Learning: Intro and BackgroundAn overview of reinforcement learning, generalization, and reward learning"}, {"url": "https://towardsdatascience.com/assessing-generalization-in-reward-learning-intro-and-background-da6c99d9e48?source=author_recirc-----de02e1d08c0e----0---------------------987d790e_8d67_4273_a7d1_a73d739fcb69-------", "anchor_text": "16 min read\u00b7Sep 30, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fda6c99d9e48&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fassessing-generalization-in-reward-learning-intro-and-background-da6c99d9e48&user=Max+Chiswick&userId=98505f8c082&source=-----da6c99d9e48----0-----------------clap_footer----987d790e_8d67_4273_a7d1_a73d739fcb69-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/assessing-generalization-in-reward-learning-intro-and-background-da6c99d9e48?source=author_recirc-----de02e1d08c0e----0---------------------987d790e_8d67_4273_a7d1_a73d739fcb69-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fda6c99d9e48&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fassessing-generalization-in-reward-learning-intro-and-background-da6c99d9e48&source=-----de02e1d08c0e----0-----------------bookmark_preview----987d790e_8d67_4273_a7d1_a73d739fcb69-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----de02e1d08c0e----1---------------------987d790e_8d67_4273_a7d1_a73d739fcb69-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----de02e1d08c0e----1---------------------987d790e_8d67_4273_a7d1_a73d739fcb69-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----de02e1d08c0e----1---------------------987d790e_8d67_4273_a7d1_a73d739fcb69-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----de02e1d08c0e----1---------------------987d790e_8d67_4273_a7d1_a73d739fcb69-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----de02e1d08c0e----1---------------------987d790e_8d67_4273_a7d1_a73d739fcb69-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----de02e1d08c0e----1---------------------987d790e_8d67_4273_a7d1_a73d739fcb69-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----987d790e_8d67_4273_a7d1_a73d739fcb69-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----de02e1d08c0e----1---------------------987d790e_8d67_4273_a7d1_a73d739fcb69-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----de02e1d08c0e----1-----------------bookmark_preview----987d790e_8d67_4273_a7d1_a73d739fcb69-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----de02e1d08c0e----2---------------------987d790e_8d67_4273_a7d1_a73d739fcb69-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----de02e1d08c0e----2---------------------987d790e_8d67_4273_a7d1_a73d739fcb69-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----de02e1d08c0e----2---------------------987d790e_8d67_4273_a7d1_a73d739fcb69-------", "anchor_text": "Jacob Marks, Ph.D."}, {"url": "https://towardsdatascience.com/?source=author_recirc-----de02e1d08c0e----2---------------------987d790e_8d67_4273_a7d1_a73d739fcb69-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----de02e1d08c0e----2---------------------987d790e_8d67_4273_a7d1_a73d739fcb69-------", "anchor_text": "How I Turned My Company\u2019s Docs into a Searchable Database with OpenAIAnd how you can do the same with your docs"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----de02e1d08c0e----2---------------------987d790e_8d67_4273_a7d1_a73d739fcb69-------", "anchor_text": "15 min read\u00b7Apr 25"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&user=Jacob+Marks%2C+Ph.D.&userId=f7dc0c0eae92&source=-----4f2d34bd8736----2-----------------clap_footer----987d790e_8d67_4273_a7d1_a73d739fcb69-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----de02e1d08c0e----2---------------------987d790e_8d67_4273_a7d1_a73d739fcb69-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "20"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&source=-----de02e1d08c0e----2-----------------bookmark_preview----987d790e_8d67_4273_a7d1_a73d739fcb69-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----de02e1d08c0e----3---------------------987d790e_8d67_4273_a7d1_a73d739fcb69-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----de02e1d08c0e----3---------------------987d790e_8d67_4273_a7d1_a73d739fcb69-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----de02e1d08c0e----3---------------------987d790e_8d67_4273_a7d1_a73d739fcb69-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----de02e1d08c0e----3---------------------987d790e_8d67_4273_a7d1_a73d739fcb69-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----de02e1d08c0e----3---------------------987d790e_8d67_4273_a7d1_a73d739fcb69-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----de02e1d08c0e----3---------------------987d790e_8d67_4273_a7d1_a73d739fcb69-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----3-----------------clap_footer----987d790e_8d67_4273_a7d1_a73d739fcb69-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----de02e1d08c0e----3---------------------987d790e_8d67_4273_a7d1_a73d739fcb69-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----de02e1d08c0e----3-----------------bookmark_preview----987d790e_8d67_4273_a7d1_a73d739fcb69-------", "anchor_text": ""}, {"url": "https://chisness.medium.com/?source=post_page-----de02e1d08c0e--------------------------------", "anchor_text": "See all from Max Chiswick"}, {"url": "https://towardsdatascience.com/?source=post_page-----de02e1d08c0e--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----de02e1d08c0e----0---------------------e412d5ab_8a95_493b_91e3_ea1942a8dfc6-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----de02e1d08c0e----0---------------------e412d5ab_8a95_493b_91e3_ea1942a8dfc6-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----de02e1d08c0e----0---------------------e412d5ab_8a95_493b_91e3_ea1942a8dfc6-------", "anchor_text": "The PyCoach"}, {"url": "https://artificialcorner.com/?source=read_next_recirc-----de02e1d08c0e----0---------------------e412d5ab_8a95_493b_91e3_ea1942a8dfc6-------", "anchor_text": "Artificial Corner"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----de02e1d08c0e----0---------------------e412d5ab_8a95_493b_91e3_ea1942a8dfc6-------", "anchor_text": "You\u2019re Using ChatGPT Wrong! Here\u2019s How to Be Ahead of 99% of ChatGPT UsersMaster ChatGPT by learning prompt engineering."}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----de02e1d08c0e----0---------------------e412d5ab_8a95_493b_91e3_ea1942a8dfc6-------", "anchor_text": "\u00b77 min read\u00b7Mar 17"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fartificial-corner%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&user=The+PyCoach&userId=fb44e21903f3&source=-----886a50dabc54----0-----------------clap_footer----e412d5ab_8a95_493b_91e3_ea1942a8dfc6-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----de02e1d08c0e----0---------------------e412d5ab_8a95_493b_91e3_ea1942a8dfc6-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "276"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&source=-----de02e1d08c0e----0-----------------bookmark_preview----e412d5ab_8a95_493b_91e3_ea1942a8dfc6-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----de02e1d08c0e----1---------------------e412d5ab_8a95_493b_91e3_ea1942a8dfc6-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----de02e1d08c0e----1---------------------e412d5ab_8a95_493b_91e3_ea1942a8dfc6-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----de02e1d08c0e----1---------------------e412d5ab_8a95_493b_91e3_ea1942a8dfc6-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----de02e1d08c0e----1---------------------e412d5ab_8a95_493b_91e3_ea1942a8dfc6-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----de02e1d08c0e----1---------------------e412d5ab_8a95_493b_91e3_ea1942a8dfc6-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----de02e1d08c0e----1---------------------e412d5ab_8a95_493b_91e3_ea1942a8dfc6-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----1-----------------clap_footer----e412d5ab_8a95_493b_91e3_ea1942a8dfc6-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----de02e1d08c0e----1---------------------e412d5ab_8a95_493b_91e3_ea1942a8dfc6-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----de02e1d08c0e----1-----------------bookmark_preview----e412d5ab_8a95_493b_91e3_ea1942a8dfc6-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----de02e1d08c0e----0---------------------e412d5ab_8a95_493b_91e3_ea1942a8dfc6-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----de02e1d08c0e----0---------------------e412d5ab_8a95_493b_91e3_ea1942a8dfc6-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----de02e1d08c0e----0---------------------e412d5ab_8a95_493b_91e3_ea1942a8dfc6-------", "anchor_text": "Alexander Nguyen"}, {"url": "https://levelup.gitconnected.com/?source=read_next_recirc-----de02e1d08c0e----0---------------------e412d5ab_8a95_493b_91e3_ea1942a8dfc6-------", "anchor_text": "Level Up Coding"}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----de02e1d08c0e----0---------------------e412d5ab_8a95_493b_91e3_ea1942a8dfc6-------", "anchor_text": "Why I Keep Failing Candidates During Google Interviews\u2026They don\u2019t meet the bar."}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----de02e1d08c0e----0---------------------e412d5ab_8a95_493b_91e3_ea1942a8dfc6-------", "anchor_text": "\u00b74 min read\u00b7Apr 13"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fgitconnected%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&user=Alexander+Nguyen&userId=a148fd75c2e9&source=-----dc8f865b2c19----0-----------------clap_footer----e412d5ab_8a95_493b_91e3_ea1942a8dfc6-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----de02e1d08c0e----0---------------------e412d5ab_8a95_493b_91e3_ea1942a8dfc6-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "91"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&source=-----de02e1d08c0e----0-----------------bookmark_preview----e412d5ab_8a95_493b_91e3_ea1942a8dfc6-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/understanding-nerfs-2a082e13c6eb?source=read_next_recirc-----de02e1d08c0e----1---------------------e412d5ab_8a95_493b_91e3_ea1942a8dfc6-------", "anchor_text": ""}, {"url": "https://wolfecameron.medium.com/?source=read_next_recirc-----de02e1d08c0e----1---------------------e412d5ab_8a95_493b_91e3_ea1942a8dfc6-------", "anchor_text": ""}, {"url": "https://wolfecameron.medium.com/?source=read_next_recirc-----de02e1d08c0e----1---------------------e412d5ab_8a95_493b_91e3_ea1942a8dfc6-------", "anchor_text": "Cameron R. Wolfe"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----de02e1d08c0e----1---------------------e412d5ab_8a95_493b_91e3_ea1942a8dfc6-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/understanding-nerfs-2a082e13c6eb?source=read_next_recirc-----de02e1d08c0e----1---------------------e412d5ab_8a95_493b_91e3_ea1942a8dfc6-------", "anchor_text": "Understanding NeRFsA massive breakthrough in scene representation"}, {"url": "https://towardsdatascience.com/understanding-nerfs-2a082e13c6eb?source=read_next_recirc-----de02e1d08c0e----1---------------------e412d5ab_8a95_493b_91e3_ea1942a8dfc6-------", "anchor_text": "\u00b711 min read\u00b73 days ago"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2a082e13c6eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-nerfs-2a082e13c6eb&user=Cameron+R.+Wolfe&userId=28aa6026c553&source=-----2a082e13c6eb----1-----------------clap_footer----e412d5ab_8a95_493b_91e3_ea1942a8dfc6-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/understanding-nerfs-2a082e13c6eb?source=read_next_recirc-----de02e1d08c0e----1---------------------e412d5ab_8a95_493b_91e3_ea1942a8dfc6-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2a082e13c6eb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-nerfs-2a082e13c6eb&source=-----de02e1d08c0e----1-----------------bookmark_preview----e412d5ab_8a95_493b_91e3_ea1942a8dfc6-------", "anchor_text": ""}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----de02e1d08c0e----2---------------------e412d5ab_8a95_493b_91e3_ea1942a8dfc6-------", "anchor_text": ""}, {"url": "https://medium.com/@timothymugayi?source=read_next_recirc-----de02e1d08c0e----2---------------------e412d5ab_8a95_493b_91e3_ea1942a8dfc6-------", "anchor_text": ""}, {"url": "https://medium.com/@timothymugayi?source=read_next_recirc-----de02e1d08c0e----2---------------------e412d5ab_8a95_493b_91e3_ea1942a8dfc6-------", "anchor_text": "Timothy Mugayi"}, {"url": "https://betterprogramming.pub/?source=read_next_recirc-----de02e1d08c0e----2---------------------e412d5ab_8a95_493b_91e3_ea1942a8dfc6-------", "anchor_text": "Better Programming"}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----de02e1d08c0e----2---------------------e412d5ab_8a95_493b_91e3_ea1942a8dfc6-------", "anchor_text": "How To Build Your Own Custom ChatGPT With Custom Knowledge BaseFeed your ChatGPT bot with custom data sources"}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----de02e1d08c0e----2---------------------e412d5ab_8a95_493b_91e3_ea1942a8dfc6-------", "anchor_text": "\u00b711 min read\u00b7Apr 7"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fbetter-programming%2F4e61ad82427e&operation=register&redirect=https%3A%2F%2Fbetterprogramming.pub%2Fhow-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e&user=Timothy+Mugayi&userId=34774d6cac27&source=-----4e61ad82427e----2-----------------clap_footer----e412d5ab_8a95_493b_91e3_ea1942a8dfc6-------", "anchor_text": ""}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----de02e1d08c0e----2---------------------e412d5ab_8a95_493b_91e3_ea1942a8dfc6-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "83"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4e61ad82427e&operation=register&redirect=https%3A%2F%2Fbetterprogramming.pub%2Fhow-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e&source=-----de02e1d08c0e----2-----------------bookmark_preview----e412d5ab_8a95_493b_91e3_ea1942a8dfc6-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----de02e1d08c0e----3---------------------e412d5ab_8a95_493b_91e3_ea1942a8dfc6-------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=read_next_recirc-----de02e1d08c0e----3---------------------e412d5ab_8a95_493b_91e3_ea1942a8dfc6-------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=read_next_recirc-----de02e1d08c0e----3---------------------e412d5ab_8a95_493b_91e3_ea1942a8dfc6-------", "anchor_text": "Wouter van Heeswijk, PhD"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----de02e1d08c0e----3---------------------e412d5ab_8a95_493b_91e3_ea1942a8dfc6-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----de02e1d08c0e----3---------------------e412d5ab_8a95_493b_91e3_ea1942a8dfc6-------", "anchor_text": "Proximal Policy Optimization (PPO) ExplainedThe journey from REINFORCE to the go-to algorithm in continuous control"}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----de02e1d08c0e----3---------------------e412d5ab_8a95_493b_91e3_ea1942a8dfc6-------", "anchor_text": "\u00b713 min read\u00b7Nov 29, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fabed1952457b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fproximal-policy-optimization-ppo-explained-abed1952457b&user=Wouter+van+Heeswijk%2C+PhD&userId=33f45c9ab481&source=-----abed1952457b----3-----------------clap_footer----e412d5ab_8a95_493b_91e3_ea1942a8dfc6-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----de02e1d08c0e----3---------------------e412d5ab_8a95_493b_91e3_ea1942a8dfc6-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fabed1952457b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fproximal-policy-optimization-ppo-explained-abed1952457b&source=-----de02e1d08c0e----3-----------------bookmark_preview----e412d5ab_8a95_493b_91e3_ea1942a8dfc6-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----de02e1d08c0e--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----de02e1d08c0e--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----de02e1d08c0e--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----de02e1d08c0e--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----de02e1d08c0e--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----de02e1d08c0e--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----de02e1d08c0e--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----de02e1d08c0e--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----de02e1d08c0e--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----de02e1d08c0e--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}