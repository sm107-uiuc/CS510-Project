{"url": "https://towardsdatascience.com/what-learning-unsupervised-word-embeddings-across-51-000-journal-articles-can-tell-us-about-d60851fb4c30", "time": 1683005940.100972, "path": "towardsdatascience.com/what-learning-unsupervised-word-embeddings-across-51-000-journal-articles-can-tell-us-about-d60851fb4c30/", "webpage": {"metadata": {"title": "What learning unsupervised word embeddings across 51,000 journal articles can tell us about COVID-19. | by Leon Chlon | Towards Data Science", "h1": "What learning unsupervised word embeddings across 51,000 journal articles can tell us about COVID-19.", "description": "Supervised natural language processing is a good place to start, but reducing entire corpora of publications to a single structured labelled dataset isn\u2019t exactly what anyone wants to spend their\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.nature.com/articles/s41586-019-1335-8", "anchor_text": "Tshitoyan and colleagues", "paragraph_index": 0}, {"url": "https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge", "anchor_text": "COVID-19 Open Research Dataset", "paragraph_index": 11}, {"url": "https://www.nature.com/articles/s41586-019-1335-8", "anchor_text": "Tshitoyan and colleagues", "paragraph_index": 11}, {"url": "https://www.nature.com/articles/s41586-019-1335-8", "anchor_text": "Tshitoyan", "paragraph_index": 12}, {"url": "https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge", "anchor_text": "COVID-19 Open Research Dataset", "paragraph_index": 12}, {"url": "https://github.com/mcs07/ChemDataExtractor", "anchor_text": "ChemDataExtractor", "paragraph_index": 12}, {"url": "https://towardsdatascience.com/understanding-word-n-grams-and-n-gram-probability-in-natural-language-processing-9d9eef0fa058", "anchor_text": "n-grams", "paragraph_index": 13}, {"url": "https://radimrehurek.com/gensim/", "anchor_text": "ensim", "paragraph_index": 13}, {"url": "https://radimrehurek.com/gensim/", "anchor_text": "ensim", "paragraph_index": 14}, {"url": "https://www.accessdata.fda.gov/scripts/cder/daf/", "anchor_text": "database", "paragraph_index": 17}, {"url": "http://www.hkupasteur.hku.hk/publications/pdf/Zhang_BioorgMedChem_2004b.pdf", "anchor_text": "reported as a potential SARS-CoV proteinase inhibitor in 2004", "paragraph_index": 18}, {"url": "https://www.ncbi.nlm.nih.gov/pubmed/18423639", "anchor_text": "lack of clinical efficacy in mouse models in 2008.", "paragraph_index": 18}, {"url": "https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&cad=rja&uact=8&ved=2ahUKEwjGgJ2Yxu3oAhU9aRUIHV4VCGkQFjABegQIAhAB&url=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC5021412%2F&usg=AOvVaw10NPZgmUcU16Ai8n8Y_eBI", "anchor_text": "2016 study", "paragraph_index": 18}, {"url": "https://www.fiercebiotech.com/biotech/gilead-shares-slip-as-a-second-remdesivir-covid-19-trial-halted-china", "anchor_text": "trials", "paragraph_index": 20}, {"url": "https://www.nejm.org/doi/full/10.1056/NEJMoa2001282", "anchor_text": "ritonavir", "paragraph_index": 20}, {"url": "https://www.nature.com/articles/s41586-019-1335-8", "anchor_text": "Tshitoyan and colleagues", "paragraph_index": 23}], "all_paragraphs": ["Supervised natural language processing is a good place to start, but reducing entire corpora of publications to a single structured labelled dataset isn\u2019t exactly what anyone wants to spend their quarantine time doing. Instead, Tshitoyan and colleagues found that they could capture underlying complex materials science concepts using unsupervised information-dense word embeddings. Amazingly, they found that by training a simple Word2Vec network, they could predict the thermoconductive properties of materials years ahead of their discovery!", "With a free afternoon, a fresh cup of coffee and a very naive overview of the world of virology, can we extend this methodology to a corpora (collection of texts) of 51,000 COVID-19 related journal articles?", "When we first learn a language, we are taught a few words that make up the vocabulary, and we then form associations between words in that vocabulary depending on context; we call those sentences. With that extremely diluted overview, let\u2019s try to teach our computer to become a virologist.", "First off we need a way to represent a vocabulary. Computers don\u2019t have an intuitive way of processing words as strings; a common trick is to map each word onto a n-dimensional binary vector, where each row represents a word in vocabulary with n words. When representing a single word like \u201cRome\u201d, the vector will have a 1 in the row corresponding to \u201cRome\u201d and 0 everywhere else. This is called one-hot encoding and is illustrated below.", "Unfortunately this doesn\u2019t tell us anything about the ordering of words, for that we must learn sentence representations. One way to do this is to build a metric for context, or how likely a word is to be \u201cclose\u201d to another word in a sentence. In other words (no pun intended), what kind of model can we build that represents the likelihood of the association between words in a corpora?", "We can train a neural network to do this by attempting to predict words likely to be in a neighbourhood or window centred around a word of interest. A network trained on IMDB reviews with a window size of 1 is more likely to predict \u201cDunder\u201d next to \u201cMifflin\u201d than \u201cDunder\u201d next to \u201cBiryani\u201d; that\u2019s because the phrase \u201cDunder Mifflin\u201d or (\u201cDunder\u201d, \u201cMifflin\u201d) will appear many more times as a training example than \u201cDunder Biryani\u201d or (\u201cDunder\u201d, \u201cBiryani\u201d).", "On the other hand, a window of size 1 would fail to associate \u201cDunder\u201d with \u201cPaper\u201d in \u201cDunder Mifflin Paper Company\u201d, since \u201cPaper\u201d is two words away. To remedy this, we need a window of size 2, which would give rise to many more instances of (\u201cDunder\u201d, \u201cPaper\u201d) as training examples. Choosing an appropriate neighbourhood size is pivotal to capturing the semantics of a complex language.", "One highly popular network that does this is Skip-Gram: a one-hidden-layer network that maps our one-hot-encoded input vector to an output of association probabilities. This network is illustrated below:", "The hidden layer is the prize here; we are mapping a binary vector containing mostly zeroes to a 300-dimensional vector describing its semantic representation with the rest of the corpus. A word in this representation is said to be embedded, the embedded vocabulary are called the embeddings of the corpus.", "What\u2019s even more remarkable is that we can use vector algebra on the embeddings to describe the relationship between words in a language. The cosine similarity score (CSS) between the embedded vectors for \u201cKing\u201d, \u201cKings\u201d or even \u201cEmperor\u201d should be close to 1 since they appear in similar contexts, thus the training pairs they appear in should provide similar updates to the weights in the network forming the embedding. Adding and subtracting the vectors has been shown to capture linguistic regularities such as analogies. For example \u201cMan is to King what Woman is to\u201d is represented via additive and subtraction operators on the embeddings: \u201cKing\u201d \u2014 \u201cMan\u201d + \u201cWoman\u201d; this results in an embedding very close to \u201cQueen\u201d.", "Now that we have a model, we can proceed to learn the language of Covid-19 research from a corpus of publication data.", "In response to the pandemic, the White House and a federation of leading researchers assembled the COVID-19 Open Research Dataset, a resource of over 51,000 scholarly articles around COVID-19, SARS-CoV-2 and other coronaviruses, hosted on Kaggle. In their original paper, Tshitoyan and colleagues focus on learning scientific concepts from the abstract of a paper. The abstract basically provides a tl;dr overview of the methods, results, findings and conclusions of the work done by the authors and therefore a rich contextual setting for the relationships between forming a smaller but denser information corpora on coronavirus research.", "The same protocol used to pre-process the journals in the Tshitoyan et al paper was used to prepare the COVID-19 Open Research Dataset. Sentences were filtered for key words such as \u201cABSTRACT:\u201d and \u201cMETHODS:\u201d and tokenised (split into word arrays) using ChemDataExtractor, a repo that helps to generalise chemical identifiers. Non-technical words were lower cased and de-accented to reduce the overall vocabulary size. Tokens were admitted to the vocabulary if they appeared more than 5 times in the corpus, giving 22399 unique words.", "The authors also enrich their vocabulary with n-grams phrases. These are sequences of n words with a high likelihood of appearing next to one another given the training data. gensim was used to generate 4-gram using phrases that include common terms such as \u201cof\u201d or \u201ca\u201d. Phrases were admitted into the vocabulary if they appeared more than 10 times in the corpus, giving 33504 unique phrases.", "I used the Word2Vec Skip-Gram implementation in gensim with a 200-dimensional embedding and a context window of 8 words with a downsampling threshold of 0.0001 which downsamples the 328 most common words in the corpus. The network was trained using stochastic gradient descent with a linearly decreasing learning rate from 0.01 to 0.0001 over 30 epochs. A collection of 15,000 grammatical science analogies were used to assess network performance, with the score defined as the number of \u201ccorrectly solved\u201d analogies. This score was used to manually tune the network hyperparameters.", "These results were particularly exciting since they allude to some learning of relationships between key concepts in the literature.", "Results \u2014 Drug Name Cosine Similarities and Analogies", "Next I asked what contextual relationships existed between drugs the coronavirae SARS-CoV-2, SARS and MERS-CoV. I downloaded a database of 2542-approved FDA drugs, of which 408 overlapped with drug names present in the vocabulary.", "Notably, hydroxychloroquine was one of the highest ranked drugs by semantic similarity with SARS-CoV-2, which is unsurprising given the attention it has been receiving recently as a possible therapeutic. Another interesting hit is promazine, reported as a potential SARS-CoV proteinase inhibitor in 2004, but showed a lack of clinical efficacy in mouse models in 2008. Imatinib is a cancer treatment drug that was demonstrated to inhibit both SARS-CoV and MERS-CoV in a 2016 study.", "Next I used vector algebra on the embeddings to ask the following question: \u201cribavirin is to SARS as _ is to SARS-CoV-2?\u201d. Below are the top 10 answers by way of cosine similarity:", "Interestingly, remedisivir is currently in trials pending its utility to treat mild to moderate COVID-19 symptoms and a trial of ritonavir just concluded in a cohort of COVID-19 symptoms with no notable benefit.", "An interesting line of inquiry would be how many drugs mapped to SARS-CoV-2 don\u2019t appear within the same abstract, since this would indicate potential contextual analogies based off of patterns learned from papers focusing exclusively on SARS-CoV and MERS-CoV.", "Similar to above, asking \u201cdry_cough is to SARS is as _ is to SARS-CoV-2\u201d returns a comprehensive list of symptoms, many of which are known to be associated with the recent outbreak.", "Extending the methodology by Tshitoyan and colleagues provided several amazing insights into the disease simply by asking a NLP model questions in its own language. This work was one afternoon\u2019s coding and barely scratches the surface of what can be done with the power of NLP and the amazing dataset being assembled by some pretty remarkable people. Much much much more remains to be done. For example, we could replace the Word2Vec model with a more sophisticated context-aware model such as Google\u2019s BERT or ELMO. More technical questions could be asked regarding protein structure analogies and figuring out what questions the network is answering when the key words don\u2019t appear in the same abstract as SARS-CoV-2 publications. No doubt there is much work being done in this area to try and find strategies to fight a disease that has brought our planet to a stand-still.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Research Data Scientist \u2014 Facebook; Past: McKinsey Analytics Consultant | Harvard Medical School Postdoc | University of Cambridge PhD, MPhil"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fd60851fb4c30&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-learning-unsupervised-word-embeddings-across-51-000-journal-articles-can-tell-us-about-d60851fb4c30&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-learning-unsupervised-word-embeddings-across-51-000-journal-articles-can-tell-us-about-d60851fb4c30&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-learning-unsupervised-word-embeddings-across-51-000-journal-articles-can-tell-us-about-d60851fb4c30&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-learning-unsupervised-word-embeddings-across-51-000-journal-articles-can-tell-us-about-d60851fb4c30&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----d60851fb4c30--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d60851fb4c30--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://chleon.medium.com/?source=post_page-----d60851fb4c30--------------------------------", "anchor_text": ""}, {"url": "https://chleon.medium.com/?source=post_page-----d60851fb4c30--------------------------------", "anchor_text": "Leon Chlon"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fff2d21af7d63&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-learning-unsupervised-word-embeddings-across-51-000-journal-articles-can-tell-us-about-d60851fb4c30&user=Leon+Chlon&userId=ff2d21af7d63&source=post_page-ff2d21af7d63----d60851fb4c30---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd60851fb4c30&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-learning-unsupervised-word-embeddings-across-51-000-journal-articles-can-tell-us-about-d60851fb4c30&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd60851fb4c30&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-learning-unsupervised-word-embeddings-across-51-000-journal-articles-can-tell-us-about-d60851fb4c30&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.nature.com/articles/s41586-019-1335-8", "anchor_text": "Tshitoyan and colleagues"}, {"url": "http://mccormickml.com/", "anchor_text": "Chris McCormick"}, {"url": "https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge", "anchor_text": "COVID-19 Open Research Dataset"}, {"url": "https://www.nature.com/articles/s41586-019-1335-8", "anchor_text": "Tshitoyan and colleagues"}, {"url": "https://www.nature.com/articles/s41586-019-1335-8", "anchor_text": "Tshitoyan"}, {"url": "https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge", "anchor_text": "COVID-19 Open Research Dataset"}, {"url": "https://github.com/mcs07/ChemDataExtractor", "anchor_text": "ChemDataExtractor"}, {"url": "https://towardsdatascience.com/understanding-word-n-grams-and-n-gram-probability-in-natural-language-processing-9d9eef0fa058", "anchor_text": "n-grams"}, {"url": "https://radimrehurek.com/gensim/", "anchor_text": "ensim"}, {"url": "https://radimrehurek.com/gensim/", "anchor_text": "ensim"}, {"url": "https://www.accessdata.fda.gov/scripts/cder/daf/", "anchor_text": "database"}, {"url": "http://www.hkupasteur.hku.hk/publications/pdf/Zhang_BioorgMedChem_2004b.pdf", "anchor_text": "reported as a potential SARS-CoV proteinase inhibitor in 2004"}, {"url": "https://www.ncbi.nlm.nih.gov/pubmed/18423639", "anchor_text": "lack of clinical efficacy in mouse models in 2008."}, {"url": "https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&cad=rja&uact=8&ved=2ahUKEwjGgJ2Yxu3oAhU9aRUIHV4VCGkQFjABegQIAhAB&url=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC5021412%2F&usg=AOvVaw10NPZgmUcU16Ai8n8Y_eBI", "anchor_text": "2016 study"}, {"url": "https://www.fiercebiotech.com/biotech/gilead-shares-slip-as-a-second-remdesivir-covid-19-trial-halted-china", "anchor_text": "trials"}, {"url": "https://www.nejm.org/doi/full/10.1056/NEJMoa2001282", "anchor_text": "ritonavir"}, {"url": "https://www.nature.com/articles/s41586-019-1335-8", "anchor_text": "Tshitoyan and colleagues"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----d60851fb4c30---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/tag/coronavirus?source=post_page-----d60851fb4c30---------------coronavirus-----------------", "anchor_text": "Coronavirus"}, {"url": "https://medium.com/tag/nlp?source=post_page-----d60851fb4c30---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/ai?source=post_page-----d60851fb4c30---------------ai-----------------", "anchor_text": "AI"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd60851fb4c30&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-learning-unsupervised-word-embeddings-across-51-000-journal-articles-can-tell-us-about-d60851fb4c30&user=Leon+Chlon&userId=ff2d21af7d63&source=-----d60851fb4c30---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd60851fb4c30&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-learning-unsupervised-word-embeddings-across-51-000-journal-articles-can-tell-us-about-d60851fb4c30&user=Leon+Chlon&userId=ff2d21af7d63&source=-----d60851fb4c30---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd60851fb4c30&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-learning-unsupervised-word-embeddings-across-51-000-journal-articles-can-tell-us-about-d60851fb4c30&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d60851fb4c30--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fd60851fb4c30&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-learning-unsupervised-word-embeddings-across-51-000-journal-articles-can-tell-us-about-d60851fb4c30&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----d60851fb4c30---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----d60851fb4c30--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----d60851fb4c30--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----d60851fb4c30--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----d60851fb4c30--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----d60851fb4c30--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----d60851fb4c30--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----d60851fb4c30--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----d60851fb4c30--------------------------------", "anchor_text": ""}, {"url": "https://chleon.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://chleon.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Leon Chlon"}, {"url": "https://chleon.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "187 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fff2d21af7d63&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-learning-unsupervised-word-embeddings-across-51-000-journal-articles-can-tell-us-about-d60851fb4c30&user=Leon+Chlon&userId=ff2d21af7d63&source=post_page-ff2d21af7d63--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F25471836b4f1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-learning-unsupervised-word-embeddings-across-51-000-journal-articles-can-tell-us-about-d60851fb4c30&newsletterV3=ff2d21af7d63&newsletterV3Id=25471836b4f1&user=Leon+Chlon&userId=ff2d21af7d63&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}