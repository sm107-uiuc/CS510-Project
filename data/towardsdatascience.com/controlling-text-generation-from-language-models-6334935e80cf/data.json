{"url": "https://towardsdatascience.com/controlling-text-generation-from-language-models-6334935e80cf", "time": 1683011076.069637, "path": "towardsdatascience.com/controlling-text-generation-from-language-models-6334935e80cf/", "webpage": {"metadata": {"title": "Controlling Text Generation for Language Models | Towards Data Science", "h1": "Controlling Text Generation for Language Models", "description": "Control text generation for language models like GPT2. We use PPLM, an approach that combines a language model with attribute models to control the style."}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/pdf/1912.02164.pdf", "anchor_text": "PPLM", "paragraph_index": 1}, {"url": "https://www.uber.com/us/en/uberai/", "anchor_text": "Uber AI", "paragraph_index": 1}, {"url": "https://eng.uber.com/pplm/", "anchor_text": "blog post", "paragraph_index": 2}, {"url": "https://towardsdatascience.com/fine-tuning-gpt2-for-text-generation-using-pytorch-2ee61a4f1ba7", "anchor_text": "post", "paragraph_index": 8}, {"url": "https://github.com/itsuncheng/PPLM", "anchor_text": "code available here", "paragraph_index": 13}, {"url": "https://github.com/itsuncheng/PPLM/blob/master/generated_romance.txt", "anchor_text": "repo", "paragraph_index": 18}, {"url": "https://arxiv.org/pdf/1912.02164.pdf", "anchor_text": "Plug and Play Language Models: A Simple Approach to Controlled Text Generation", "paragraph_index": 21}, {"url": "https://eng.uber.com/pplm/", "anchor_text": "Controlling Text Generation with Plug and Play Language Models", "paragraph_index": 22}, {"url": "https://transformer.huggingface.co/doc/pplm", "anchor_text": "Write with Transformer", "paragraph_index": 23}, {"url": "https://www.linkedin.com/in/itsuncheng/", "anchor_text": "https://www.linkedin.com/in/itsuncheng/", "paragraph_index": 25}], "all_paragraphs": ["It is already cool to see how machines are able to generate fluent texts of any kind right now. But what if we can TELL machines what style or form to generate without training separate models for each style? That would cost a LOT of computing power and time.", "What if I tell you we can actually achieve that? Introducing PPLM, which stands for Plug-and-Play Language Models. Thanks to Uber AI, PPLM is an approach that combines a pretrained language model with one or more attribute models that will be able to steer text generation. The best thing about PPLM is that it won\u2019t require language model training or fine-tuning and that there is no limit to what attribute models can be used.", "In the official blog post, the authors portrayed a large language model such as GPT-2 as a mammoth while the attribute model as a mouse. GPT-2 is more computationally expensive to train, just like how mammoth is difficult to move due to its size. The solution is to use a smaller model (like a mouse) that can steer the direction of the GPT-2.", "PPLM uses two kinds of attribute models: Bag-of-Words (BoW) and Discriminator. In Bag-of-Words, a file containing all the related words to a specific topic is given to train the model. The model is attached on top of the base language model (GPT-2) like a classifier. By doing this, GPT-2 will tend to produce those words more frequently than before. The authors included the following BoW as examples to get started with: Legal, Military, Monsters, Politics, Positive Words, Religion, Science, Space, Technology.", "The following image shows the different outputs given the same input tokens \u201cThe issue focused\u201d based on different Bag-of-Words.", "For Discriminator, a topic attribute can be represented by a dataset containing samples of different classes related to that attribute. Then, PPLM can generate corresponding text according to a class provided as the input. The authors included the following Discriminator examples: Positive Sentiment, Negative Sentiment, Clickbait, Non-Clickbait.", "The following image again shows the different outputs given the same input tokens based on the positive and negative discriminator classes.", "Now that we\u2019ve understood how PPLM works, let\u2019s move on to the implementation. For the sake of time, this article will mainly focus on implementing PPLM-BoW.", "In my previous post, I have described how to fine-tune GPT2 to generate any text in your custom dataset. This blog post will use the fine-tuned model from that post, which is trained on book summaries. If you haven\u2019t read that article, you can first go to that article or otherwise, use the pre-trained GPT2 model. Having a fine-tuned model will allow the generation of a more specific domain (e.g. book summaries) rather than just general text.", "We will be using the official repo from Uber Research, you can download their repo via:", "Build a .txt file that contains the words related to a specific topic you want the generation to be about. For example, if we want to generate romance book summaries, we can build a romance BoW containing a list of romance-related words:", "Now, we have constructed the BoW text file, we can use it to run PPLM:", "The above command will run PPLM using /path/to/BoW/romance.txt as BoW classifier on the pretrained model /path/to/model/ . We set the condition text (input text) as \u201cThe novel\u201d to see what it will generate after it. We generate 20 samples of length 150. Note that --stepsize can used to control the intensity of topic, so increasing it will cause the generated text to be included the BoW more frequently. --colorama will color-code the BoW present in the generated text in red, and --sample will make sure the model produces different samples with the same seed. You can also use the pretrained GPT-2, the default without specifying any model would be \u201cgpt2-medium\u201d.", "If you want to be able to save the generated outputs, I have added an extra command-line argument --save_path that will allow you to save (code available here). You can simply run the command below and the outputs will be saved to that folder :", "Here are some of the samples generated with romance BoW with the input condition text \u201cThe novel\u2026\u201d:", "The novel begins with the story of how two young people, the daughter of a rich industrialist in the United Kingdom, and his wife, the daughter of the beautiful daughter of a wealthy woman, are married to the same man. They are married for eternity to be together and they have no children. Their daughter turns out to be a beautiful woman whose husband, after years of having loved her, decides to give her the love of his life for marriage\u2026", "The novel is the tale of the young couple, Liza and David who are married for the second time. They live happily, but their marriage is doomed when David\u2019s father dies. The couple move in with the couple\u2019s widower father and son-in-law, but they have an adulterous affair.", "The novel is written as a series of flashbacks from the life of a young orphan named Tessa, whose mother died in a fire, and who lives in a house in which she works. The house\u2019s owner is a woman of the same name. When Tessa goes to live with her mother in a nearby house for a couple of years, she is surprised to see a handsome, attractive young man. She falls in love with him, but he does not love her anymore and she is pregnant with her first husband\u2019s child\u2026", "More examples can be viewed on my Github repo.", "This article dived into how to control text generation through the implementation of PPLM with the powerful language model, GPT-2. PPLM is powerful in that it allows different combinations of attribute models to produce different styles of text. And that\u2019s it! Hope you guys learned something from this post and looking forward to seeing you guys in the next one!", "Here are more articles I wrote if interested \ud83d\ude0a", "[1] S. Dathathri, A. Madotto, J. Lan, etc., Plug and Play Language Models: A Simple Approach to Controlled Text Generation (2020), The International Conference on Learning Representations 2020", "[2] R. Liu, S. Dathathri, A. Madotto, etc., Controlling Text Generation with Plug and Play Language Models, Uber AI Blog", "[4] Write with Transformer, Huggingface and Uber AI", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Master\u2019s Student at Carnegie Mellon, Top Writer in AI, Top 1000 Writer, Blogging on ML | Data Science | NLP. Linkedin: https://www.linkedin.com/in/itsuncheng/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F6334935e80cf&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcontrolling-text-generation-from-language-models-6334935e80cf&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcontrolling-text-generation-from-language-models-6334935e80cf&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcontrolling-text-generation-from-language-models-6334935e80cf&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcontrolling-text-generation-from-language-models-6334935e80cf&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----6334935e80cf--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----6334935e80cf--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://itsuncheng.medium.com/?source=post_page-----6334935e80cf--------------------------------", "anchor_text": ""}, {"url": "https://itsuncheng.medium.com/?source=post_page-----6334935e80cf--------------------------------", "anchor_text": "Raymond Cheng"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4c697cd55840&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcontrolling-text-generation-from-language-models-6334935e80cf&user=Raymond+Cheng&userId=4c697cd55840&source=post_page-4c697cd55840----6334935e80cf---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6334935e80cf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcontrolling-text-generation-from-language-models-6334935e80cf&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6334935e80cf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcontrolling-text-generation-from-language-models-6334935e80cf&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@jonflobrant?utm_source=medium&utm_medium=referral", "anchor_text": "Jon Flobrant"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://arxiv.org/pdf/1912.02164.pdf", "anchor_text": "PPLM"}, {"url": "https://www.uber.com/us/en/uberai/", "anchor_text": "Uber AI"}, {"url": "https://eng.uber.com/pplm/", "anchor_text": "Uber AI Blog Post"}, {"url": "https://eng.uber.com/pplm/", "anchor_text": "blog post"}, {"url": "https://eng.uber.com/pplm/", "anchor_text": "Uber AI Blog Post"}, {"url": "https://eng.uber.com/pplm/", "anchor_text": "Uber AI Blog Post"}, {"url": "https://towardsdatascience.com/fine-tuning-gpt2-for-text-generation-using-pytorch-2ee61a4f1ba7", "anchor_text": "post"}, {"url": "https://towardsdatascience.com/fine-tuning-gpt2-for-text-generation-using-pytorch-2ee61a4f1ba7", "anchor_text": "Fine-tuning GPT2 for Text Generation Using PytorchFine-tune GPT2 for text generation using Pytorch and Huggingface. We train on the CMU Book Summary Dataset to generate\u2026towardsdatascience.com"}, {"url": "https://github.com/itsuncheng/PPLM", "anchor_text": "code available here"}, {"url": "https://github.com/itsuncheng/PPLM/blob/master/generated_romance.txt", "anchor_text": "repo"}, {"url": "https://towardsdatascience.com/bert-text-classification-using-pytorch-723dfb8b6b5b", "anchor_text": "BERT Text Classification Using PytorchText classification is a common task in NLP. We apply BERT, a popular Transformer model, on fake news detection using\u2026towardsdatascience.com"}, {"url": "https://towardsdatascience.com/lstm-text-classification-using-pytorch-2c6c657f8fc0", "anchor_text": "LSTM Text Classification Using PytorchA step-by-step guide teaching you how to build a bidirectional LSTM in Pytorch!towardsdatascience.com"}, {"url": "https://arxiv.org/pdf/1912.02164.pdf", "anchor_text": "Plug and Play Language Models: A Simple Approach to Controlled Text Generation"}, {"url": "https://eng.uber.com/pplm/", "anchor_text": "Controlling Text Generation with Plug and Play Language Models"}, {"url": "https://github.com/uber-research/PPLM", "anchor_text": "PPLM Code"}, {"url": "https://transformer.huggingface.co/doc/pplm", "anchor_text": "Write with Transformer"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----6334935e80cf---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----6334935e80cf---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----6334935e80cf---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/data-science?source=post_page-----6334935e80cf---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/technology?source=post_page-----6334935e80cf---------------technology-----------------", "anchor_text": "Technology"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6334935e80cf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcontrolling-text-generation-from-language-models-6334935e80cf&user=Raymond+Cheng&userId=4c697cd55840&source=-----6334935e80cf---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6334935e80cf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcontrolling-text-generation-from-language-models-6334935e80cf&user=Raymond+Cheng&userId=4c697cd55840&source=-----6334935e80cf---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6334935e80cf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcontrolling-text-generation-from-language-models-6334935e80cf&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----6334935e80cf--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F6334935e80cf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcontrolling-text-generation-from-language-models-6334935e80cf&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----6334935e80cf---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----6334935e80cf--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----6334935e80cf--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----6334935e80cf--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----6334935e80cf--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----6334935e80cf--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----6334935e80cf--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----6334935e80cf--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----6334935e80cf--------------------------------", "anchor_text": ""}, {"url": "https://itsuncheng.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://itsuncheng.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Raymond Cheng"}, {"url": "https://itsuncheng.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "731 Followers"}, {"url": "https://www.linkedin.com/in/itsuncheng/", "anchor_text": "https://www.linkedin.com/in/itsuncheng/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4c697cd55840&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcontrolling-text-generation-from-language-models-6334935e80cf&user=Raymond+Cheng&userId=4c697cd55840&source=post_page-4c697cd55840--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fa637a6d3749b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcontrolling-text-generation-from-language-models-6334935e80cf&newsletterV3=4c697cd55840&newsletterV3Id=a637a6d3749b&user=Raymond+Cheng&userId=4c697cd55840&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}