{"url": "https://towardsdatascience.com/nvidia-gave-me-a-15k-data-science-workstation-heres-what-i-did-with-it-70cfb069fc35", "time": 1683004207.8730292, "path": "towardsdatascience.com/nvidia-gave-me-a-15k-data-science-workstation-heres-what-i-did-with-it-70cfb069fc35/", "webpage": {"metadata": {"title": "Nvidia gave me a $15K Data Science Workstation \u2014 here\u2019s what I did with it | by Kyle Gallatin | Towards Data Science", "h1": "Nvidia gave me a $15K Data Science Workstation \u2014 here\u2019s what I did with it", "description": "When NVIDIA asked if I wanted to try one of the latest data science workstations, I was stoked. However, a sobering thought followed the excitement: what in the world should I use this for? As a\u2026"}, "outgoing_paragraph_urls": [{"url": "https://mbr.nlm.nih.gov/Download/Baselines/2019/", "anchor_text": "bulk download as XML files", "paragraph_index": 9}, {"url": "https://www.microway.com/preconfiguredsystems/data-science-whisperstation-nvidia-data-science-workstation/", "anchor_text": "Data Science WhisperStation", "paragraph_index": 12}, {"url": "https://rapids.ai/", "anchor_text": "RAPIDS", "paragraph_index": 15}, {"url": "https://gist.github.com/kylegallatin/0860a1b51101c7bd9c2fcc0d7b6f0906", "anchor_text": "full CPU implementation here", "paragraph_index": 28}, {"url": "https://github.com/rapidsai/cuml/issues/1266", "anchor_text": "this Github issue", "paragraph_index": 29}, {"url": "https://github.com/hanxiao/bert-as-service", "anchor_text": "BERT as a service", "paragraph_index": 36}, {"url": "https://www.elastic.co/jp/blog/text-similarity-search-with-vectors-in-elasticsearch", "anchor_text": "support for vector scoring", "paragraph_index": 57}, {"url": "https://twitter.com/kylegallatin", "anchor_text": "my Twitter game", "paragraph_index": 59}, {"url": "https://www.linkedin.com/in/kylegallatin/", "anchor_text": "LinkedIn", "paragraph_index": 59}], "all_paragraphs": ["When NVIDIA asked if I wanted to try one of the latest data science workstations, I was stoked. However, a sobering thought followed the excitement: what in the world should I use this for?", "As a machine learning engineer, I do a lot of deep learning, but I\u2019m definitely no Google Brain researcher. I could run benchmarking tests, time jobs etc\u2026but I don\u2019t work at Nvidia and honestly, it didn\u2019t sound too fun.", "I kept pitching myself ideas and started to think about the true value of powerful compute for a data scientist. Well-engineered GPU compute can lead to cost savings, low latency serving, and the easy training of large models \u2014 but what I was most interested in was rapid iteration.", "Data science is a field grounded in experimentation. With big data or large models, the number of times a scientist can try out new configurations or parameters is limited without massive resources. Everyone knows the pain of starting a computationally-intensive process, only be blindsided by an unforeseen error literal hours into running it. Then you have to correct it and start all over again.", "I thought back to my first data science project: a massive, multilingual search engine for medical literature. If I had access to the compute and GPU libraries I have now in 2020 back in 2017, what might I have been able to accomplish? How much faster would I have accomplished it?", "And so I tried to answer that question by building a Pubmed search engine using only GPU resources.", "The initial project came from a team in China, where there exists a burden on pharmaceutical companies to provide non-branded medical information to health care practitioners (HCPs). The team wanted to build a novel search tool which needed to be:", "My job was to engineer a solution that would meet and support all of these requirements. Moreover, this solution would need a relatively quick turn around through iterations so that we could rapidly test, evaluate, and update the system based on input from subject matter experts (SMEs).", "However, as a data scientist I had one big issue\u2026", "For those who don\u2019t know, Pubmed is a database of biomedical literature containing more than 30 million citations. While not all full-text articles are open source, each citation has an abstract. All of this information is available via API or in bulk download as XML files, which, unfortunately for me, is about 300GB spread across a thousand files.", "After you parse each XML to extract useful fields (title, abstract, publication year, etc\u2026), the data size is reduced down to an amount closer to 25GB. However, this still isn\u2019t super manageable locally. Like I said before, data science is about experimentation \u2014 and I needed to do a lot of that. As a somewhat new data scientist (and awful engineer), I basically had one resource: Python. There were no 20 node Spark or Elasticsearch clusters coming to my rescue.", "But, what if I had a GPU or two?", "The Data Science WhisperStation I was granted access to by Microway & Nvidia had the following features:", "What we really care about are these last two bullets. As you\u2019re likely aware, GPU compute is hugely popular in data science. Running workflows with GPU libraries can speed up code by orders of magnitude \u2014 which can mean hours instead of weeks with every experiment run.", "Additionally, if you\u2019ve ever set up a data science environment from scratch you know it can really suck. Having Docker, RAPIDs, tensorflow, pytorch and everything else installed and configured out-of-the-box saved hours in setup time.", "Until lately, GPUs were mostly used for deep learning in data science. However, not so long ago Nvidia released RAPIDS \u2014 a general purpose data science library for GPUs. RAPIDS is composed of:", "With these general-purpose data science libraries offering massive computational enhancements for traditionally CPU-bound processes (data loading, cleansing, feature engineering, linear models, etc\u2026), the path is paved to entirely new frontier of data science.", "In modern-day, the simple search is a fairly straightforward process. Words can be represented as numerical vectors, and then the distance between those vectors can be computed to see how \u201csimilar\u201d the passages are. The simplest methodology for this uses cosine similarity with TF-IDF word vectors.", "However, to \u201cvectorize\u201d our text, we first need to actually read it in and preprocess it. Assume the XML \u2192 csv preprocessing has already been completed, and now we just need to read these in as dataframes and perform the associated preprocessing.", "Now, we have about 23GB of data and only two GPUs. I have no illusions about being able to fit all of this in the GPU memory with Python. Fortunately, as is the case with scientific literature, only recent articles tend to be relevant. To evaluate the accuracy of the search with experts, I really only need the last 10 years of Pubmed data \u2014 which, with my dataset, was around 8 million articles.", "Using CPU-bound processes would be pretty taxing, but speeding things up with cuda makes all the difference. I want to:", "This is where cudf comes in. I literally wrote pandas code, did a find/replace and I had GPU accelerated code!", "This processes the dataframes much faster than they would be processed locally. Here is a sample output locally using pandas:", "And here is output from the process on the workstation using cudf:", "Each file is being processed more than twice as fast and the code only needs one GPU! Even better, we can use all the memory in both GPUs by creating a cuda cluster and employing dask.", "If I want to just read in all the abstracts and do something else with them, dask makes this highly efficient with minimal lines of code.", "The code above produced the following output on my subset of Pubmed data (all of Pubmed throws a memory error \u2014 no surprise).", "Checking the output of the GPU usage in a separate window with watch -n 0.5 nvidia-smi, you can watch your processes run and monitor the memory usage.", "Since I now know I can load the last ten years of Pubmed data into the GPU memory, I can move on to the fun part: the actual TF-IDF vectorization. In scikit-learn this is pretty easy, see my full CPU implementation here. Using cuml we should be able to just find and replace like we did with pandas, but unfortunately\u2026.", "According to this Github issue, the text feature extraction libraries for cuml are still in the works at the time of writing (but once finished I\u2019ll update with code!). This means our vectorizer still needs to be implemented with scikit-learn, and we can\u2019t yet get GPU acceleration on this TF-IDF task. This is just for the training step, but it means that our TF-IDF vectorizer will remain CPU bound and therefore, inefficient.", "However, this wasn\u2019t over just yet. Even if the training itself is inefficient, that step really only needs to happen one time. Fortunately, the output sklearn\u2019s TF-IDF vectorizer is just a sparse matrix \u2014 and when we\u2019re back to dealing with matrices, we can get help from some classic tensor libraries. I decided to go with tensorflow.", "As one would expect, matrix multiplication is an implicit part of any tensor library. After training my vectorizer in sklearn, I could port the actual vectors back over to the GPU with tensorflow to perform the matrix multiplication.", "Now, in theory, this worked great with small portions of Pubmed \u2014 but it doesn\u2019t scale. In all of Pubmed (and even our subset), there are quite a few unique words. Since we also have one vector for every citation in Pubmed after 2009, our sparse matrices become massive. I think it became roughly about 8 million by 1 million.", "Thwarted not by the hardware but software. Trying to move back and forth from sklearn and tensorflow was leading to a host of issues. Realizing this approach would take more time and skill than I had readily available, it was time to move on or become a better engineer. It was time to move on to deep learning representations.", "Recent advancements with transformers in NLP have shown massive improvements in a variety of tasks. While numerous models have come since, the origin of this revolution Google\u2019s BERT. Like some other DL based models, BERT produces a contextual vector for sentences. The number of dimensions (length of the vector) is equal to the hidden layer size, which in the latest recommended BERT-large model is 1024.", "This is huge. Even if we can\u2019t use sparse matrices anymore, the size of our vector goes from millions x millions \u2192 million x thousands. On GPUs where space can be somewhat limited, this makes all the difference.", "Normally BERT is used for classification tasks, but in our case, we just want to use it to extract the vectorized representation of our Pubmed abstracts so they can be indexed and searched. Thanks to Tencent Research, we already have a well engineered and GPU capable library: BERT as a service.", "You can follow the instructions in the repo to actually install the service. Once you have it available in your environment, all you have to do is download your preferred BERT model and start it up.", "Now that you have the service running, simple Python can be invoked to get the vectors for any text you want the BERT representation for.", "Easy enough. With the BERT service using the two GPUs on the workstation, large amount of abstracts are passed through the model blazingly fast. Below is the output when I time it for each csv:", "Even with the workstation this process takes a while \u2014 which gives you an idea of how long it takes without it. It\u2019s also worth noting this time includes reading in the data with cudf. To illustrate just how large the gap between GPU acceleration and local compute is, here\u2019s the same process using my personal laptop instead:", "30 minutes. It took almost 30 minutes to vectorize half of the abstracts I process on the workstation in < 60 seconds. Even to just get the vectors from such a large model, GPU compute saves me full days of twiddling my thumbs while my code runs.", "This time, instead of doing matrix multiplication myself I\u2019m going to hand it off to a well engineered fast index library. Facebook\u2019s faiss is easy to use, and GPU capable making it the perfect tool with which to index our BERT vectors. To create a flat GPU-based in faiss, we only need ~10 lines of code.", "Once you have the index itself, all you have to do is toss the vectors in. To save GPU memory, I recommend vectorizing the text using the BERT service separately first and saving to disk. Then, you can load and index the vectors without the service also cruising in the background. However, you can also do it all at once if you choose.", "After creating the index itself, searches can be done in a single line. But will this scale? If I wanted to use this code to retrieve results or even put a model into production, I want to make sure that searches are run as quickly as possible. I benchmarked searches up to ~3 million abstracts and searches results still took < 0.1 seconds.", "Finally? Sanity check. Those whole time I\u2019ve been performing searches under the assumptions that SMEs will be able to evaluate them for accuracy. However, if the searches are so bad it\u2019s pointless I\u2019d have to start all over again or completely refactor my approach. Fortunately, this isn\u2019t the case.", "A quick look shows that aa contextual search for \u201cParkinson\u2019s Disease\u201d returns relevant abstracts in the field (to my layman\u2019s evaluation).", "So, let\u2019s look back at the requirements and see if this approach solved all of the requirements for this project:", "Available via multiple channels (web/WeChat) \u2705: Wrap it up in an API and serve away.", "Customizable (able to tune algorithm for optimal results)\u2705: I used BERT base, but it\u2019s possible to use Bio-BERT or any other fine-tuned BERT here. Additionally, we can stack lightweight classification algorithms or heuristics on these results to improve accuracy even more.", "Low latency (fast search results) \u2705: Using almost 1/3 of the Pubmed abstracts, latency was still < 0.1 seconds and looked to be scaling reasonably.", "Support large data (all Pubmed abstracts and more) \u2705: We only used citations \u2265 the year 2009 for validation, but with more GPUs and a better engineer you could easily scale this to all of Pubmed.", "Accurate (better search results than Pubmed) \ud83e\udd14: Remains to be seen. SMEs would have to rate and compare search results with Pubmed search, and tune the algorithm over time. However, with such a short turnover on ~7 millions abstracts the workstation makes this very feasible to do with relatively quick turnaround. Additionally, while the sanity check lacked scale, it at least shows this approach may be worth exploring.", "Information retrieval is huge in large corporations that are overflowing with disorganized documents. Intelligent solutions to retrieve these documents are in high demand. While many vendors offer robust enterprise-grade solutions, to organize information at such grand scale in such a short period of time is only possible now through the hardware and software advances of the late 2010s.", "I created, iterated, and revised my approach to this problem in my spare time over a few weeks. Thanks to the power of the workstation and open source, I actually managed to accomplish my goal in that time. Rather than wait weeks for code to run, I received constant feedback and tackled errors early. As a result, my code and this personal project progressed exponentially faster.", "Since I\u2019m basically already working in a production environment, it\u2019s also easy to transition to more managed cloud hosts for deployment. While my code was nowhere near production code, using Docker allowed me to ensure everything I built could be prepackaged and shipped off to whatever image registry and deployment scheme I liked.", "Obviously, $15K is a lot to put down on some hardware. But, if you\u2019re an enterprise organization looking for quick experimentation and turnover, it makes sense. As a comparison, here\u2019s a quote for a dedicated AWS p3.8x large (4 Tesla V100s). $75K for 1 year and the headache of installing of installing all the libraries and tools yourself.", "There are solutions to this problem that don\u2019t involve GPU. Since elasticsearch now has support for vector scoring, you can deploy the same solution on a 20 node cluster pretty easily with a lot more bells and whistles than the ~30 lines of code I used here.", "However, the efficiency and scale that was achieved on just two DGXs here should show what\u2019s in the works using GPU. Accessibility through high-level Python APIs now enables the average data scientist to perform highly optimized tasks with minimal effort. Thanks for reading, and by all means please improve on these solutions!", "Shameless plug: working on my Twitter game and feel free to connect on LinkedIn!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Software Engineer for ML Infra. Building scalable, operationalized machine learning services. I don\u2019t represent my employer."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F70cfb069fc35&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnvidia-gave-me-a-15k-data-science-workstation-heres-what-i-did-with-it-70cfb069fc35&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnvidia-gave-me-a-15k-data-science-workstation-heres-what-i-did-with-it-70cfb069fc35&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnvidia-gave-me-a-15k-data-science-workstation-heres-what-i-did-with-it-70cfb069fc35&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnvidia-gave-me-a-15k-data-science-workstation-heres-what-i-did-with-it-70cfb069fc35&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----70cfb069fc35--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----70cfb069fc35--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@kylegallatin?source=post_page-----70cfb069fc35--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@kylegallatin?source=post_page-----70cfb069fc35--------------------------------", "anchor_text": "Kyle Gallatin"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F51ff4b76ebf4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnvidia-gave-me-a-15k-data-science-workstation-heres-what-i-did-with-it-70cfb069fc35&user=Kyle+Gallatin&userId=51ff4b76ebf4&source=post_page-51ff4b76ebf4----70cfb069fc35---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F70cfb069fc35&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnvidia-gave-me-a-15k-data-science-workstation-heres-what-i-did-with-it-70cfb069fc35&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F70cfb069fc35&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnvidia-gave-me-a-15k-data-science-workstation-heres-what-i-did-with-it-70cfb069fc35&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/tagged/video-tutorial", "anchor_text": "Video Tutorial"}, {"url": "https://www.youtube.com/watch?v=WCz8AF-wT1I", "anchor_text": "Watch on YouTube"}, {"url": "https://github.com/kylegallatin/gpu-accerelated-search-notebook", "anchor_text": "associated git repo"}, {"url": "https://www.nvidia.com/content/dam/en-zz/Solutions/deep-learning/deep-learning-solutions/data-science/data-science-laptop-workstation-4c25-p@2x.jpg", "anchor_text": "Damn"}, {"url": "https://mbr.nlm.nih.gov/Download/Baselines/2019/", "anchor_text": "bulk download as XML files"}, {"url": "https://www.microway.com/preconfiguredsystems/data-science-whisperstation-nvidia-data-science-workstation/", "anchor_text": "Data Science WhisperStation"}, {"url": "https://rapids.ai/", "anchor_text": "RAPIDS"}, {"url": "https://www.google.com/url?sa=i&url=https%3A%2F%2Fdevblogs.nvidia.com%2Fgpu-accelerated-analytics-rapids%2F&psig=AOvVaw3tl2r6V5aeZNUhBe_5KKnC&ust=1582231062758000&source=images&cd=vfe&ved=0CAIQjRxqFwoTCLiuvZa83ucCFQAAAAAdAAAAABAD", "anchor_text": "RAPIDs"}, {"url": "https://gist.github.com/kylegallatin/0860a1b51101c7bd9c2fcc0d7b6f0906", "anchor_text": "full CPU implementation here"}, {"url": "https://github.com/rapidsai/cuml/issues/1266", "anchor_text": "this Github issue"}, {"url": "https://github.com/hanxiao/bert-as-service", "anchor_text": "BERT as a service"}, {"url": "https://www.docker.com/sites/default/files/social/docker_facebook_share.png'", "anchor_text": "I love this lil dude so much, they have saved me hours of headaches"}, {"url": "https://www.elastic.co/jp/blog/text-similarity-search-with-vectors-in-elasticsearch", "anchor_text": "support for vector scoring"}, {"url": "https://www.nvidia.com/content/dam/en-zz/Solutions/deep-learning/dgx-saturnv/nvidia-dgx-saturnv-real-world-quality-assurance-4c25-m@2x.jpg", "anchor_text": "Nvidia DGX"}, {"url": "https://twitter.com/kylegallatin", "anchor_text": "my Twitter game"}, {"url": "https://www.linkedin.com/in/kylegallatin/", "anchor_text": "LinkedIn"}, {"url": "https://medium.com/tag/gpu?source=post_page-----70cfb069fc35---------------gpu-----------------", "anchor_text": "Gpu"}, {"url": "https://medium.com/tag/nvidia?source=post_page-----70cfb069fc35---------------nvidia-----------------", "anchor_text": "Nvidia"}, {"url": "https://medium.com/tag/data-science?source=post_page-----70cfb069fc35---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----70cfb069fc35---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/video-tutorial?source=post_page-----70cfb069fc35---------------video_tutorial-----------------", "anchor_text": "Video Tutorial"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F70cfb069fc35&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnvidia-gave-me-a-15k-data-science-workstation-heres-what-i-did-with-it-70cfb069fc35&user=Kyle+Gallatin&userId=51ff4b76ebf4&source=-----70cfb069fc35---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F70cfb069fc35&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnvidia-gave-me-a-15k-data-science-workstation-heres-what-i-did-with-it-70cfb069fc35&user=Kyle+Gallatin&userId=51ff4b76ebf4&source=-----70cfb069fc35---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F70cfb069fc35&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnvidia-gave-me-a-15k-data-science-workstation-heres-what-i-did-with-it-70cfb069fc35&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----70cfb069fc35--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F70cfb069fc35&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnvidia-gave-me-a-15k-data-science-workstation-heres-what-i-did-with-it-70cfb069fc35&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----70cfb069fc35---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----70cfb069fc35--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----70cfb069fc35--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----70cfb069fc35--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----70cfb069fc35--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----70cfb069fc35--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----70cfb069fc35--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----70cfb069fc35--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----70cfb069fc35--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@kylegallatin?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@kylegallatin?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Kyle Gallatin"}, {"url": "https://medium.com/@kylegallatin/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "2.4K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F51ff4b76ebf4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnvidia-gave-me-a-15k-data-science-workstation-heres-what-i-did-with-it-70cfb069fc35&user=Kyle+Gallatin&userId=51ff4b76ebf4&source=post_page-51ff4b76ebf4--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F1e64fc6ee18e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnvidia-gave-me-a-15k-data-science-workstation-heres-what-i-did-with-it-70cfb069fc35&newsletterV3=51ff4b76ebf4&newsletterV3Id=1e64fc6ee18e&user=Kyle+Gallatin&userId=51ff4b76ebf4&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}