{"url": "https://towardsdatascience.com/understanding-transformers-the-programming-way-f8ed22d112b2", "time": 1683014991.138745, "path": "towardsdatascience.com/understanding-transformers-the-programming-way-f8ed22d112b2/", "webpage": {"metadata": {"title": "Understanding Transformers, the Programming Way | by Rahul Agarwal | Towards Data Science", "h1": "Understanding Transformers, the Programming Way", "description": "Transformers have become the defacto standard for NLP tasks nowadays. They started being used in NLP but they are now being used in Computer Vision and sometimes to generate music as well. I am sure\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/understanding-transformers-the-data-science-way-e4670a4ee076", "anchor_text": "last post", "paragraph_index": 1}, {"url": "https://pytorch.org/docs/master/generated/torch.nn.Transformer.html", "anchor_text": "Transformer", "paragraph_index": 22}, {"url": "https://towardsdatascience.com/understanding-transformers-the-data-science-way-e4670a4ee076", "anchor_text": "last post", "paragraph_index": 40}, {"url": "https://github.com/MLWhiz/data_science_blogs/tree/master/transformers", "anchor_text": "GitHub", "paragraph_index": 42}, {"url": "https://click.linksynergy.com/link?id=lVarvwc5BD0&offerid=467035.11503135394&type=2&murl=https%3A%2F%2Fwww.coursera.org%2Flearn%2Flanguage-processing", "anchor_text": "Natural Language Processing", "paragraph_index": 43}, {"url": "https://medium.com/@rahul_agarwal", "anchor_text": "Medium", "paragraph_index": 44}, {"url": "https://mlwhiz.ck.page/a9b8bda70c", "anchor_text": "blog", "paragraph_index": 44}, {"url": "https://lionbridge.ai/articles/transformers-in-nlp-creating-a-translator-model-from-scratch/", "anchor_text": "here", "paragraph_index": 46}, {"url": "http://ko-fi.com/rahulagarwal", "anchor_text": "ko-fi.com/rahulagarwal", "paragraph_index": 48}], "all_paragraphs": ["Transformers have become the defacto standard for NLP tasks nowadays. They started being used in NLP but they are now being used in Computer Vision and sometimes to generate music as well. I am sure you would all have heard about the GPT3 Transformer or the jokes thereof.", "But everything aside, they are still hard to understand as ever. In my last post, I talked in quite a detail about transformers and how they work on a basic level. I went through the encoder and decoder architecture and the whole data flow in those different pieces of the neural network.", "But as I like to say we don\u2019t really understand something before we implement it ourselves. So in this post, we will implement an English to German language translator using Transformers.", "We want to create a translator that uses transformers to convert English to German. So, if we look at it as a black-box, our network takes as input an English sentence and returns a German sentence.", "To train our English-German translation Model, we will need translated sentence pairs between English and German.", "Fortunately, there is pretty much a standard way to get these with the IWSLT(International Workshop on Spoken Language Translation) dataset which we can access using torchtext.datasets. This machine translation dataset is sort of the defacto standard used for translation tasks and contains the translation of TED and TEDx talks on various topics in different languages.", "Also, before we really get into the whole coding part, let us understand what we need as input and output to the model while training. We will actually need two matrices to be input to our Network:", "So now as we know how to preprocess our data we will get into the actual code for preprocessing steps.", "Please note, that it really doesn\u2019t matter here if you preprocess using other methods too. What eventually matters is that in the end, you need to send the sentence source and targets to your model in a way that's intended to be used by the transformer. i.e. source sentences should be padded with blank token and target sentences need to have a start token, an end token and rest padded by blank tokens.", "We start by loading the Spacy Models which provides tokenizers to tokenize German and English text.", "We also define some special tokens we will use for specifying blank/padding words, and beginning and end of sentences as discussed above.", "We can now define a preprocessing pipeline for both our source and target sentences using data.field from torchtext. You can notice that while we only specify pad_token for source sentence, we mention pad_token, init_token and eos_token for the target sentence. We also define which tokenizers to use.", "If you notice till now we haven\u2019t seen any data. We now use IWSLT data from torchtext.datasets to create a train, validation, and test dataset. We also filter our sentences using the MAX_LEN parameter so that our code runs a lot faster. Notice that we are getting the data with .en and .de extensions. and we specify the preprocessing steps using the fields parameter.", "So now since we have got our train data, let's see how it looks like:", "You might notice that while the data.field object has done the tokenization, it has not yet applied the start, end, and pad tokens and that is intentional. This is because we don\u2019t have batches yet and the number of pad tokens will inherently depend on the maximum length of a sentence in the particular batch.", "As mentioned in the start, we also create a Source and Target Language vocabulary by using the built-in function in data.field object. We specify a MIN_FREQ of 2 so that any word that doesn\u2019t occur at least twice doesn\u2019t get to be a part of our vocabulary.", "Once we are done with this, we can simply use data.Bucketiterator which is used to giver batches of similar lengths to get our train iterator and validation iterator. Note that we use a batch_size of 1 for our validation data. It is optional to do this but done so that we don\u2019t do padding or do minimal padding while checking validation data performance.", "Before we proceed, it is always a good idea to see how our batch looks like and what we are sending to the model as an input while training.", "And here is our target matrix:", "So in the first batch, the src_matrix contains 350 sentences of length 20 and the trg_matrix is 350 sentences of length 22. Just so we are sure of our preprocessing, let\u2019s see what some of these numbers represent in the src_matrix and the trg_matrix.", "Just as expected. The opposite method, i.e. string to index also works well.", "So, now that we have a way to send the source sentence and the shifted target to our transformer, we can look at creating the Transformer.", "A lot of the blocks here are taken from Pytorch nn module. Infact, Pytorch has a Transformer module too but it doesn\u2019t include a lot of functionalities present in the paper like the embedding layer, and the PositionalEncoding layer. So this is sort of a more complete implementation that takes in a lot from pytorch implementation as well.", "We create our Transformer particularly using these various blocks from Pytorch nn module:", "Also, note that whatever is happening in the layers is actually just matrix functions as I mentioned in the explanation post for transformers. See in particular how the decoder stack takes memory from encoder as input. We also create a positional encoding layer which lets us add the positional embedding to our word embedding.", "If you want, you can look at the source code of all these blocks also which I have already linked. I had to look many times into the source code myself to make sure that I was giving the right inputs to these layers.", "Now, we can initialize the transformer and the optimizer using:", "In the paper, the authors used an Adam optimizer with a scheduled learning rate but here I just using a normal Adam optimizer to keep things simple.", "Now, we can train our transformer using the train function below. What we are necessarily doing in the training loop is:", "We can now run our training using:", "Below is the output of the training loop (shown only for some epochs):", "We can see how our model starts with a gibberish translation \u2014 \u201cUnd die der der der der der der der der der der der der der der der der der der der der der der der\u201d and starts giving us something by the end of a few iterations.", "We can plot the training and validation losses using Plotly express.", "If we want to deploy this model we can load it simply using:", "and predict for any source sentence using the greeedy_decode_sentence function, which is:", "This function does piecewise predictions. The greedy search would start with:", "Now we can translate any sentence using this:", "Since I don\u2019t have a German Translator at hand, I will use the next best thing to see how our model is performing. Let us take help of google translate service to understand what this german sentence means.", "There seem to be some mistakes in the translation as \u201cNatural Language Processing\u201d is not there(Ironic?) but it seems like a good enough translation to me as the neural network is somehow able to understand the structure of both the languages with just an hour of training.", "We might have achieved better results if we did everything in the same way the paper did:", "I discussed all of these in my last post and all of these are easy to implement additions. But this simple implementation was meant to understand how a transformer works so I didn\u2019t include all these so as not to confuse the readers. There have actually been quite a lot of advancements on top of transformers that have allowed us to have much better models for translation. We will discuss those advancements and how they came about in the upcoming post, where I will talk about BERT, one of the most popular NLP models that utilizes a Transformer at its core.", "In this post, We created an English to German translation network almost from scratch using the transformer architecture.", "For a closer look at the code for this post, please visit my GitHub repository where you can find the code for this post as well as all my posts.", "If you want to learn more about NLP, I would like to call out an excellent course on Natural Language Processing from the Advanced Machine Learning Specialization. Do check it out.", "I am going to be writing more of such posts in the future too. Let me know what you think about them. Should I write on heavily technical topics or more beginner level articles? The comment section is your friend. Use it. Also, follow me up at Medium or Subscribe to my blog.", "And, finally, a small disclaimer \u2014 There are some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.", "This story was first published here.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "4M Views. Bridging the gap between Data Science and Intuition. MLE@FB, Ex-WalmartLabs, Citi. Connect on Twitter @mlwhiz \u2615\ufe0f ko-fi.com/rahulagarwal"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Ff8ed22d112b2&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-transformers-the-programming-way-f8ed22d112b2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-transformers-the-programming-way-f8ed22d112b2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-transformers-the-programming-way-f8ed22d112b2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-transformers-the-programming-way-f8ed22d112b2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----f8ed22d112b2--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f8ed22d112b2--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://mlwhiz.medium.com/?source=post_page-----f8ed22d112b2--------------------------------", "anchor_text": ""}, {"url": "https://mlwhiz.medium.com/?source=post_page-----f8ed22d112b2--------------------------------", "anchor_text": "Rahul Agarwal"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe8cce06956c9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-transformers-the-programming-way-f8ed22d112b2&user=Rahul+Agarwal&userId=e8cce06956c9&source=post_page-e8cce06956c9----f8ed22d112b2---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff8ed22d112b2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-transformers-the-programming-way-f8ed22d112b2&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff8ed22d112b2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-transformers-the-programming-way-f8ed22d112b2&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://wallpaperaccess.com/transformers", "anchor_text": "Wallpaperaccess"}, {"url": "https://towardsdatascience.com/understanding-transformers-the-data-science-way-e4670a4ee076", "anchor_text": "last post"}, {"url": "https://pytorch.org/docs/master/generated/torch.nn.Transformer.html", "anchor_text": "Transformer"}, {"url": "https://pytorch.org/docs/master/generated/torch.nn.TransformerEncoderLayer.html", "anchor_text": "TransformerEncoderLayer"}, {"url": "https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html", "anchor_text": "TransformerEncoder"}, {"url": "https://pytorch.org/docs/stable/generated/torch.nn.TransformerDecoderLayer.html", "anchor_text": "TransformerDecoderLayer"}, {"url": "https://pytorch.org/docs/stable/generated/torch.nn.TransformerDecoder.html", "anchor_text": "TransformerDecoder"}, {"url": "https://towardsdatascience.com/understanding-transformers-the-data-science-way-e4670a4ee076", "anchor_text": "last post"}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "Attention Is All You Need"}, {"url": "https://nlp.seas.harvard.edu/2018/04/03/attention.html", "anchor_text": "The Annotated Transformer"}, {"url": "https://github.com/MLWhiz/data_science_blogs/tree/master/transformers", "anchor_text": "GitHub"}, {"url": "https://click.linksynergy.com/link?id=lVarvwc5BD0&offerid=467035.11503135394&type=2&murl=https%3A%2F%2Fwww.coursera.org%2Flearn%2Flanguage-processing", "anchor_text": "Natural Language Processing"}, {"url": "https://medium.com/@rahul_agarwal", "anchor_text": "Medium"}, {"url": "https://mlwhiz.ck.page/a9b8bda70c", "anchor_text": "blog"}, {"url": "https://lionbridge.ai/articles/transformers-in-nlp-creating-a-translator-model-from-scratch/", "anchor_text": "here"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----f8ed22d112b2---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----f8ed22d112b2---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/programming?source=post_page-----f8ed22d112b2---------------programming-----------------", "anchor_text": "Programming"}, {"url": "https://medium.com/tag/data-science?source=post_page-----f8ed22d112b2---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/learning?source=post_page-----f8ed22d112b2---------------learning-----------------", "anchor_text": "Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff8ed22d112b2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-transformers-the-programming-way-f8ed22d112b2&user=Rahul+Agarwal&userId=e8cce06956c9&source=-----f8ed22d112b2---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff8ed22d112b2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-transformers-the-programming-way-f8ed22d112b2&user=Rahul+Agarwal&userId=e8cce06956c9&source=-----f8ed22d112b2---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff8ed22d112b2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-transformers-the-programming-way-f8ed22d112b2&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f8ed22d112b2--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Ff8ed22d112b2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-transformers-the-programming-way-f8ed22d112b2&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----f8ed22d112b2---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----f8ed22d112b2--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----f8ed22d112b2--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----f8ed22d112b2--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----f8ed22d112b2--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----f8ed22d112b2--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----f8ed22d112b2--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----f8ed22d112b2--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----f8ed22d112b2--------------------------------", "anchor_text": ""}, {"url": "https://mlwhiz.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://mlwhiz.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Rahul Agarwal"}, {"url": "https://mlwhiz.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "13.9K Followers"}, {"url": "http://ko-fi.com/rahulagarwal", "anchor_text": "ko-fi.com/rahulagarwal"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe8cce06956c9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-transformers-the-programming-way-f8ed22d112b2&user=Rahul+Agarwal&userId=e8cce06956c9&source=post_page-e8cce06956c9--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Ff41165c9f72f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-transformers-the-programming-way-f8ed22d112b2&newsletterV3=e8cce06956c9&newsletterV3Id=f41165c9f72f&user=Rahul+Agarwal&userId=e8cce06956c9&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}