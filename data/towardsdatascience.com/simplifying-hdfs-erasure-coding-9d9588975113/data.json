{"url": "https://towardsdatascience.com/simplifying-hdfs-erasure-coding-9d9588975113", "time": 1683006382.983113, "path": "towardsdatascience.com/simplifying-hdfs-erasure-coding-9d9588975113/", "webpage": {"metadata": {"title": "HDFS Erasure Coding. Reduce storage overhead significantly\u2026 | by Prathamesh Nimkar | Towards Data Science", "h1": "HDFS Erasure Coding", "description": "Understand HDFS' Erasure Coding framework, it's inner workings, advantages and limitations."}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/hadoop-distributed-file-system-b09946738555", "anchor_text": "Blocks & Replication Factor, Storage & Replication and Rack Awareness", "paragraph_index": 0}, {"url": "https://www.youtube.com/watch?v=2wn79fy2XyQ&list=PLg-UKERBljNyTX8RGRxKnHP7GZ0UP3sST&index=8&t=0s", "anchor_text": "ISA-L", "paragraph_index": 21}, {"url": "https://blog.cloudera.com/introduction-to-hdfs-erasure-coding-in-apache-hadoop/", "anchor_text": "Introduction to HDFS Erasure Coding in Apache Hadoop", "paragraph_index": 23}, {"url": "http://web.eecs.utk.edu/~jplank/plank/papers/2013-02-11-FAST-Tutorial.pdf", "anchor_text": "Tutorial on Erasure Coding for Storage Applications, Part 1", "paragraph_index": 24}, {"url": "https://www.linkedin.com/in/prathameshnimkar/", "anchor_text": "https://www.linkedin.com/in/prathameshnimkar/", "paragraph_index": 26}], "all_paragraphs": ["You\u2019ve gone through and internalized Hadoop Distributed File System or HDFS\u2019 basic concepts \u2014 Blocks & Replication Factor, Storage & Replication and Rack Awareness", "Hadoop Distributed File System (HDFS) blocks and replication methodology has two key concepts, i.e. \u201cBlock Size\u201d and \u201cReplication Factor\u201d. Each file that enters HDFS is broken down into several chunks or \u201cblocks\u201d. The number of blocks is dependent on the maximum Block Size allocated, generally 128 MB. Once the blocks are created they are replicated across HDFS\u2019s cluster. The number of replicas is determined by the Replication Factor (RF), typically configured as 3 (1 original and 2 copies). This redundancy helps to build resilience and fault tolerance i.e. when a block fails we have another 2 from which the data can be safely restored.", "The above image is a quick demonstration of the split that we have just discussed. A 700 MB file is broken down into 6 blocks. 5 parts of 128 MB and 1 part of 60 MB. With a replication factor of 3 it will consume (6*3) = 18 blocks and (700 MB * 3) = 2100 MB of storage. Consider scaling this to the Petabyte scale, and you will quickly realize the significant under-utilization of available space due to redundancy brought by data replication.", "So, for a 3-n/3-way replication (i.e. RF = 3), where n = number of replicas, we have 2 replicated data blocks i.e. 200% storage overhead with a (1/n) or (1/3) efficiency or 33% and n-1 = 2 as fault tolerance. Not to mention, the network and I/O usage to create, maintain and perform BAU activities.", "While access to cheap storage and excellent network bandwidth is a reality in today\u2019s world, this method of building redundancy for fault-tolerance and resilience is still highly inefficient. This has led to a new paradigm in securing data with significantly higher efficiency.", "Erasure Coding (EC) was introduced to Hadoop in version 3.x through a joint effort between the good folks at Intel and Cloudera.", "Continuing with the same example, once the data is split into several blocks in HDFS, it is passed on to EC as input, which returns a number of parity blocks. This process is known as encoding and the (data + parity block/s) is known as an encoding group. In case of a failure aka erasure, the data can be reconstructed from this encoding group, known as decoding.", "In Exclusive-OR or XOR encoding, denoted as \u2295, the data (INs) are passed through a mathematical operation, called XOR\u2019ing, and results (OUT) in a single parity block, as can be seen above. Furthermore, XOR has 2 beautiful mathematical properties:", "Meaning \u2014 Arrangement of inputs doesn\u2019t change the output.", "As a corollary, we can scale up to our example, with 6 data blocks as input we will consume a total of 7 blocks of storage on the disk, 6 data blocks and 1 parity block. This increases efficiency significantly as compared to HDFS\u2019 3-way replication. And if any 1 data block fails, for example if B2 fails, the remaining blocks are XOR\u2019ed i.e. B1\u2295P1\u2295B3\u2295B4\u2295B5\u2295B6 to recover the missing data, B2. However, \u22652 simultaneous failures and the data is irrecoverable. Thus, XOR can tolerate at most 1 failure with an efficiency of ((n-1)/n ) or ~83%, where n = total number of data blocks. Since XOR suffers from low fault-tolerance, it is simply unacceptable for HDFS\u2019 requirements.", "Reed-Solomon (RS) encoding addresses HDFS\u2019 requirement of handling multiple simultaneous failures by using complex linear algebra to generate multiple parity blocks. RS encoding, denoted as RS\u208d\u2096,\u2098\u208e, works with 2 parameters, where k is a vector of data blocks and m is the number of parity blocks. This is generated by multiplying the data blocks vector k with a Generator Matrix (G\u1d40) to get a Codeword\u208d\u2096\u208a\u2098\u208e vector with length = (k+m) or length = (data + parity) as can be seen in the image above.", "When a failure occurs, recovery is conducted by multiplying the remaining blocks (or survivors) with the inverse of the Generator Matrix, provided that k number of blocks are always available. Thus, m number of failures at max can be tolerated. In our example, using best practice parameters for k & m i.e. RS(6,3), the file with 6 blocks will consume 9 blocks of storage on the disk (i.e. 6 data and 3 parity) or 1 parity block for every 2 data blocks. Thus, requiring only 50% of storage overhead with an efficiency of (k/(k+m)) or ~67%.", "There is another popular RS configuration i.e. RS(10,4) that gives an efficiency of (k/(k+m)) or ~71% with the same storage overhead of ~50%.", "I\u2019m hoping by now that you are all rooting for HDFS with Reed Solomon Erasure Coding with either (6,3) or (10,4) configuration.", "In Contiguous block layout, you have a simpler implementation over HDFS base. Consider HDFS EC RS(10,4) deployed over a contiguous block layout with block size fixed at 128 MB. Whether you have 10 data blocks or 1 data block to write, 4 parity blocks are always written. So, for a single data block (think of B5), storage efficiency overhead is now at ~400% which is worse than 3-way replication. All our storage efficiency gains are wiped out. In order to justify the switch to EC with contiguous layout from HDFS 3-way replication, all 10 data blocks need to be written. Similarly for RS(6,3) with the same block layout, we\u2019d need to have 6 written data blocks to ensure storage efficiency.", "Through personal experience over several projects and Cloudera\u2019s industry research (referenced below), ~70\u201395% of overall data stored in HDFS is of size less than 1 block or 128 MB. In the Striping block layout, the file is \u201cstriped\u201d to a smaller size, typically 64 KB or 1 MB data \u201ccells\u201d. Thus, whether we use RS(10,4) or RS(6,3) the parity cells created don\u2019t affect storage overhead much.", "Most importantly, HDFS allows different replication and erasure coding policies to be configured on a directory or file-level in a cluster. Since file size is the determining factor for block layout, the ability to apply the erasure coding policy on a file/directory-level is highly beneficial from a data durability and storage efficiency perspective.", "Certain architectural changes were made in order to handle EC with a striped layout.", "The logical block or the byte range of a file has been separated from the storage block which stores the actual chunks of data in the data node. The HDFS NameNode now maps it\u2019s block ID to the data node\u2019s storage block and logical block separately. This creates a significant storage overhead on the Name Node of ~250\u2013440%.", "So, a new hierarchical block naming protocol was introduced to tackle this exorbitant increase in space usage. Instead of sequentially allocating the block IDs based on timestamp, we divide the block maps into 3 sections as above. Note, for contiguous block layout it is 2 sections only. Thus, the NN is able to manage the logical and storage blocks in the form of a summary-detail or hierarchical protocol and limit the overhead to ~21\u201376%.", "Extending DFSInputStream (Java data type) to DFSStripedInputStream and DFSOutputStream (Java data type) to DFSStripedOutputStream to accommodate data striping and EC. Since the logical and storage blocks are now separated, using *striped* data types we can enable parallel creation/processing of blocks in HDFS.", "The NN identifies missing blocks and assigns the recovery to data nodes. This is handled by a new component, namely, ECW or ErasureCodingWorker, which performs three tasks. First, it sends a read request to all the non-failed data sources. Second, using Intel\u2019s optimized Reed Solomon Erasure Codec Framework, ISA-L, the ECW decodes the data. And third, it pushes the recovered data to the failed data nodes.", "Lastly, let\u2019s sum up the pros and cons of what we\u2019ve discussed so far:", "[2] A.Wang, Z. Zhang, K. Zheng, U. Maheshwara and V. Kumar, Introduction to HDFS Erasure Coding in Apache Hadoop (2015), Cloudera", "[3] Prof. J. Plank, Tutorial on Erasure Coding for Storage Applications, Part 1 (2013), EECS Department, University of Tennessee", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Tech Enthusiast \u2014 Data Engineering | Data Analytics | LinkedIN: https://www.linkedin.com/in/prathameshnimkar/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F9d9588975113&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimplifying-hdfs-erasure-coding-9d9588975113&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimplifying-hdfs-erasure-coding-9d9588975113&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimplifying-hdfs-erasure-coding-9d9588975113&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimplifying-hdfs-erasure-coding-9d9588975113&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----9d9588975113--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----9d9588975113--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@prathamesh.nimkar?source=post_page-----9d9588975113--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@prathamesh.nimkar?source=post_page-----9d9588975113--------------------------------", "anchor_text": "Prathamesh Nimkar"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6f0e324a756e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimplifying-hdfs-erasure-coding-9d9588975113&user=Prathamesh+Nimkar&userId=6f0e324a756e&source=post_page-6f0e324a756e----9d9588975113---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9d9588975113&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimplifying-hdfs-erasure-coding-9d9588975113&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9d9588975113&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimplifying-hdfs-erasure-coding-9d9588975113&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/hadoop-distributed-file-system-b09946738555", "anchor_text": "Blocks & Replication Factor, Storage & Replication and Rack Awareness"}, {"url": "https://www.usenix.org/legacy/event/fast09/tech/full_papers/plank/plank.pdf", "anchor_text": "Source"}, {"url": "https://www.youtube.com/watch?v=2wn79fy2XyQ&list=PLg-UKERBljNyTX8RGRxKnHP7GZ0UP3sST&index=8&t=0s", "anchor_text": "ISA-L"}, {"url": "https://hadoop.apache.org/docs/r3.0.0/hadoop-project-dist/hadoop-hdfs/HDFSErasureCoding.html", "anchor_text": "HDFS Erasure Coding"}, {"url": "https://blog.cloudera.com/introduction-to-hdfs-erasure-coding-in-apache-hadoop/", "anchor_text": "Introduction to HDFS Erasure Coding in Apache Hadoop"}, {"url": "http://web.eecs.utk.edu/~jplank/plank/papers/2013-02-11-FAST-Tutorial.pdf", "anchor_text": "Tutorial on Erasure Coding for Storage Applications, Part 1"}, {"url": "https://medium.com/@prathamesh.nimkar/big-data-analytics-using-the-hadoop-ecosystem-411d629084d3", "anchor_text": "Big Data Analytics Pipeline using the Hadoop EcosystemLanding Pagemedium.com"}, {"url": "https://medium.com/tag/hdfs?source=post_page-----9d9588975113---------------hdfs-----------------", "anchor_text": "Hdfs"}, {"url": "https://medium.com/tag/hdfs-architecture?source=post_page-----9d9588975113---------------hdfs_architecture-----------------", "anchor_text": "Hdfs Architecture"}, {"url": "https://medium.com/tag/hadoop?source=post_page-----9d9588975113---------------hadoop-----------------", "anchor_text": "Hadoop"}, {"url": "https://medium.com/tag/encoding?source=post_page-----9d9588975113---------------encoding-----------------", "anchor_text": "Encoding"}, {"url": "https://medium.com/tag/cloudera?source=post_page-----9d9588975113---------------cloudera-----------------", "anchor_text": "Cloudera"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9d9588975113&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimplifying-hdfs-erasure-coding-9d9588975113&user=Prathamesh+Nimkar&userId=6f0e324a756e&source=-----9d9588975113---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9d9588975113&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimplifying-hdfs-erasure-coding-9d9588975113&user=Prathamesh+Nimkar&userId=6f0e324a756e&source=-----9d9588975113---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9d9588975113&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimplifying-hdfs-erasure-coding-9d9588975113&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----9d9588975113--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F9d9588975113&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimplifying-hdfs-erasure-coding-9d9588975113&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----9d9588975113---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----9d9588975113--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----9d9588975113--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----9d9588975113--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----9d9588975113--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----9d9588975113--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----9d9588975113--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----9d9588975113--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----9d9588975113--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@prathamesh.nimkar?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@prathamesh.nimkar?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Prathamesh Nimkar"}, {"url": "https://medium.com/@prathamesh.nimkar/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "186 Followers"}, {"url": "https://www.linkedin.com/in/prathameshnimkar/", "anchor_text": "https://www.linkedin.com/in/prathameshnimkar/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6f0e324a756e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimplifying-hdfs-erasure-coding-9d9588975113&user=Prathamesh+Nimkar&userId=6f0e324a756e&source=post_page-6f0e324a756e--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F4018850c89c8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsimplifying-hdfs-erasure-coding-9d9588975113&newsletterV3=6f0e324a756e&newsletterV3Id=4018850c89c8&user=Prathamesh+Nimkar&userId=6f0e324a756e&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}