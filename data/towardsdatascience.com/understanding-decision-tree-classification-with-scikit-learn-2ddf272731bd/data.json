{"url": "https://towardsdatascience.com/understanding-decision-tree-classification-with-scikit-learn-2ddf272731bd", "time": 1683002451.329401, "path": "towardsdatascience.com/understanding-decision-tree-classification-with-scikit-learn-2ddf272731bd/", "webpage": {"metadata": {"title": "Decision Tree and Gini Impurity | Towards Data Science", "h1": "Understanding Decision Tree Classification with Scikit-Learn", "description": "Understand and implement a Decision tree using Scikit-Learn. Gini impurity, feature importance are discussed in detail thorough examples."}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/Corrado_Gini", "anchor_text": "Corrado Gini", "paragraph_index": 3}, {"url": "https://www.kaggle.com/fredericobreno/play-tennis#play_tennis.csv", "anchor_text": "simple data-set", "paragraph_index": 3}, {"url": "https://www.amazon.com/Python-Machine-Learning-Sebastian-Raschka-ebook/dp/B00YSILNL0", "anchor_text": "\u2018Python Machine Learning\u2019", "paragraph_index": 8}, {"url": "https://archive.ics.uci.edu/ml/datasets/Bank+Marketing", "anchor_text": "Bank Marketing Data-Set", "paragraph_index": 9}, {"url": "https://seaborn.pydata.org/generated/seaborn.countplot.html", "anchor_text": "Seaborn countplot", "paragraph_index": 12}, {"url": "https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.hist.html", "anchor_text": "Matplotlib Hist", "paragraph_index": 13}, {"url": "https://seaborn.pydata.org/generated/seaborn.heatmap.html", "anchor_text": "Seaborn Heatmap", "paragraph_index": 14}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html", "anchor_text": "StandardScaler", "paragraph_index": 17}, {"url": "https://sebastianraschka.com/faq/docs/decision-tree-binary.html", "anchor_text": "Binary Classification with Decision Trees", "paragraph_index": 26}, {"url": "https://courses.cs.washington.edu/courses/cse546/13au/slides/decision-trees.pdf", "anchor_text": "Decision Tree:", "paragraph_index": 27}, {"url": "https://sebastianraschka.com/faq/docs/decisiontree-error-vs-entropy.html", "anchor_text": "Why Use Entropy to Grow Decision Tree?", "paragraph_index": 28}, {"url": "https://www.amazon.com/Introduction-Machine-Learning-Python-Scientists/dp/1449369413", "anchor_text": "Introduction to Machine Learning with Python", "paragraph_index": 29}, {"url": "https://github.com/suvoooo/Machine_Learning/tree/master/DecsTree", "anchor_text": "Link for all Codes and Images.", "paragraph_index": 30}], "all_paragraphs": ["Decision trees are one of the most fundamental Machine Learning tools which are used for both classification and regression tasks. In this post, I will cover:", "Idea of Decision Tree (hereafter DT) algorithm is to learn a set of if/else questions that can lead to decision. Here is a very naive example of classifying a person. If he/she claps a blog post after reading, then the person is considered as awesome, else, little less awesome. This tree is based on yes/no question. But the idea would remain same even for the numeric data. A decision tree can combine both numeric and categorical data. Some terminologies used in decision tree are shown in the picture below \u2014", "Here we see the how the nodes are divided depending on the position of them in the DT. First we need to learn how to choose the root node and here we need to learn one of the criteria to decide the nodes, Gini Impurity.", "Gini Impurity is named after the Italian statistician Corrado Gini. Gini impurity can be understood as a criterion to minimize the probability of misclassification. To understand the definition (as shown in the figure) and exactly how we can build up a decision tree, let\u2019s get started with a very simple data-set, where depending on various weather conditions, we decide whether to play an outdoor game or not. From the definition, a data-set containing only one class will have 0 Gini Impurity. In building up the decision tree our idea is to choose the feature with least Gini Impurity as root node and so on... Let\u2019s get started with the simple data-set \u2014", "Here we see that depending on 4 features (Outlook, Temperature, Humidity, Wind), decision is made on whether to play tennis or not. So what feature will be on the root node? To decide we will make use of Gini Impurity. Let\u2019s start with the feature \u2018Outlook\u2019. Important to note that when \u2018Outlook\u2019 is overcast, we always go out to play tennis. That node has only one class of samples (as shown in figure below).", "Since these are categorical variables if we want to apply decision tree classifier and fit to data, first we need to create dummy variables.", "Here we can be sure of one thing, that once we create a decision tree the root node will definitely be the feature \u2018Outlook_Overcast\u2019. Let\u2019s see the decision tree (shown in figure below). When \u2018Outlook_Overcast\u2019 \u22640.5 is False, i.e. when \u2018Outlook Overcast\u2019 is 1, we have a leaf node of pure samples with Gini impurity of 0.", "For the root node let\u2019s calculate the Gini Impurity. Since we have 9 ones (\u2018yes) and 5 zeroes (\u2018no\u2019), so Gini Impurity is ~ 0.459. Next node is \u2018Humidity_High\u2019 as that feature will give us the least Gini Impurity. For a small data-set like this one, we can always use Pandas data-frame features to check this and calculate Gini Impurity for each features. Once we have \u2018Outlook_Overcast\u2019 as root node we get 4 samples (\u2018yes\u2019) in a leaf nodes. And from the remaining 10 samples, we have 5 of each \u2018yes\u2019 and \u2018no\u2019. Then \u2018Humidity_High\u2019 is selected as the feature and the Gini Impurity of the node is 0.5 and so on\u2026", "Gini Impurity calculation could have a little advantage over the Entropy in a sense that it may take little less time to build a decision tree for a large data-set as, Entropy needs computation of log [1] [2]. Sebastian Raschka, author of the book \u2018Python Machine Learning\u2019 has a fantastic blog on why we use Entropy to build the decision tree instead of classification error [3]. Check that out! Let\u2019s move to the next section to implement Decision Tree algorithm for a realistic data-set.", "Here I\u2019ll be using the Bank Marketing Data-Set, available in the UC Irvine Machine Learning Repository. Abstract of the data-set as stated in the website is", "Abstract: The data is related with direct marketing campaigns (phone calls) of a Portuguese banking institution. The classification goal is to predict if the client will subscribe a term deposit (variable y).", "Let\u2019s load the data-set using Pandas", "So there are 16 features including categorical and numerical variables and total number of samples are 11162. First we check how the labels (\u2018yes\u2019, \u2018no\u2019) are distributed. We can use Seaborn countplot as below \u2014", "The data-set is slightly biased towards more number of rejections (\u2018no\u2019). So, later on when the data-set will be split into train and test set, we will use stratify. We can also check distribution of some numeric variables using Matplotlib Hist as shown below \u2014", "Correlation plot (plotted using Seaborn Heatmap)of numerical variables show very little correlation between the features. Once we have played around enough with the data-set, let\u2019s prepare our data-set for applying DT algorithm. Since there are several categorical variables, we need to convert them to dummy variables. I dropped the feature \u2018duration\u2019 because as mentioned in the description of the data-set this feature highly affects the target variable (when duration=0, y= \u2018no\u2019).", "Next step is to select features and labels \u2014", "Next step is to split the data-set in train and test set \u2014", "Next, I created a pipeline of StandardScaler (standardize the features) and DT Classifier (see a note below regarding Standardization of features). We can import DT classifier as from sklearn.tree import DecisionTreeClassifier from Scikit-Learn. To determine the best parameters (criterion of split and maximum tree depth) for DT classifier, I also used Grid Search Cross Validation. The code snippet below is self-explanatory.", "Next, I\u2019ve applied 3, 4, 5 fold cross-validation to determine the best parameters \u2014", "Here we have seen, how to successfully apply decision tree classifier within grid search cross validation, to determine and optimize the best fit parameters. Since this particular example has 46 features, it is very difficult to visualize the tree here in a Medium page. So, I made the data-frame simpler by dropping the \u2018month\u2019 feature (since it creates maximum number of dummy variables, 12) and did the fitting procedure one more time with number of features now is 35.", "Let\u2019s plot the decision tree with maximum depth 6 and \u2018Gini\u2019 as criterion. Visualizing the tree using Scikit Learn needs some coding \u2014", "Let\u2019s see the root and first few nodes of the tree in more detail \u2014", "Here we see \u2018contanct_unknown\u2019 is selected as the feature for root node. Total number of training samples were 8929, and the Gini Impurity is ~ 0.5. Next depth we see one numerical variable \u2018pdays\u2019 is selected as attribute to split the samples and so on\u2026 With so many features and specially widely distributed numerical features it is extremely difficult to build a tree manually and this scenario can be compared with the previously used much simpler data-set of playing tennis. We can also plot the which features are important in building up the tree using feature_importance_ attribute of the DecisionTreeClassifier class. The figure is shown below \u2014", "As expected from the tree \u2018contanct_unknown\u2019 which is the root node of the tree has the highest importance and many features have almost zero or negligible role to play. A feature with low or zero feature importance could imply that there is (are) another feature(s) which encode(s) the same information.", "Note: Another relevant info regarding this post is that even though I have used Standardization, DT algorithm is completely invariant to the scaling of the data [4]. As every feature is processed separately, the scaling of the features don\u2019t have any effect on the splitting.", "So to conclude, we have learned the basics of building a DT using Gini Impurity as a criterion to split. We also implemented a Grid Search Cross Validation to select the best parameters for our model to classify a realistic data-set. Hopefully, you have found all these little helpful!", "[1] Binary Classification with Decision Trees; Sebastian Raschka", "[2] Decision Tree: Carlos Guestrin, University of Washington, Lecture Notes.", "[3] Why Use Entropy to Grow Decision Tree?; Sebastian Raschka", "[4] Introduction to Machine Learning with Python; A. Muller, S.Guido.", "[6] Link for all Codes and Images.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F2ddf272731bd&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-decision-tree-classification-with-scikit-learn-2ddf272731bd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-decision-tree-classification-with-scikit-learn-2ddf272731bd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-decision-tree-classification-with-scikit-learn-2ddf272731bd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-decision-tree-classification-with-scikit-learn-2ddf272731bd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----2ddf272731bd--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2ddf272731bd--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://saptashwa.medium.com/?source=post_page-----2ddf272731bd--------------------------------", "anchor_text": ""}, {"url": "https://saptashwa.medium.com/?source=post_page-----2ddf272731bd--------------------------------", "anchor_text": "Saptashwa Bhattacharyya"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9a3c3c477239&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-decision-tree-classification-with-scikit-learn-2ddf272731bd&user=Saptashwa+Bhattacharyya&userId=9a3c3c477239&source=post_page-9a3c3c477239----2ddf272731bd---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2ddf272731bd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-decision-tree-classification-with-scikit-learn-2ddf272731bd&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2ddf272731bd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-decision-tree-classification-with-scikit-learn-2ddf272731bd&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html", "anchor_text": "GridSearchCV"}, {"url": "https://en.wikipedia.org/wiki/Corrado_Gini", "anchor_text": "Corrado Gini"}, {"url": "https://www.kaggle.com/fredericobreno/play-tennis#play_tennis.csv", "anchor_text": "simple data-set"}, {"url": "https://www.amazon.com/Python-Machine-Learning-Sebastian-Raschka-ebook/dp/B00YSILNL0", "anchor_text": "\u2018Python Machine Learning\u2019"}, {"url": "https://archive.ics.uci.edu/ml/datasets/Bank+Marketing", "anchor_text": "Bank Marketing Data-Set"}, {"url": "https://seaborn.pydata.org/generated/seaborn.countplot.html", "anchor_text": "Seaborn countplot"}, {"url": "https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.hist.html", "anchor_text": "Matplotlib Hist"}, {"url": "https://seaborn.pydata.org/generated/seaborn.heatmap.html", "anchor_text": "Seaborn Heatmap"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html", "anchor_text": "StandardScaler"}, {"url": "https://sebastianraschka.com/faq/docs/decision-tree-binary.html", "anchor_text": "Binary Classification with Decision Trees"}, {"url": "https://courses.cs.washington.edu/courses/cse546/13au/slides/decision-trees.pdf", "anchor_text": "Decision Tree:"}, {"url": "https://sebastianraschka.com/faq/docs/decisiontree-error-vs-entropy.html", "anchor_text": "Why Use Entropy to Grow Decision Tree?"}, {"url": "https://www.amazon.com/Introduction-Machine-Learning-Python-Scientists/dp/1449369413", "anchor_text": "Introduction to Machine Learning with Python"}, {"url": "https://scikit-learn.org/stable/modules/tree.html#tree", "anchor_text": "Scikit-Learn Decision Tree Manual."}, {"url": "https://github.com/suvoooo/Machine_Learning/tree/master/DecsTree", "anchor_text": "Link for all Codes and Images."}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----2ddf272731bd---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----2ddf272731bd---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/scikit-learn?source=post_page-----2ddf272731bd---------------scikit_learn-----------------", "anchor_text": "Scikit Learn"}, {"url": "https://medium.com/tag/python?source=post_page-----2ddf272731bd---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/classification?source=post_page-----2ddf272731bd---------------classification-----------------", "anchor_text": "Classification"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2ddf272731bd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-decision-tree-classification-with-scikit-learn-2ddf272731bd&user=Saptashwa+Bhattacharyya&userId=9a3c3c477239&source=-----2ddf272731bd---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2ddf272731bd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-decision-tree-classification-with-scikit-learn-2ddf272731bd&user=Saptashwa+Bhattacharyya&userId=9a3c3c477239&source=-----2ddf272731bd---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2ddf272731bd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-decision-tree-classification-with-scikit-learn-2ddf272731bd&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2ddf272731bd--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F2ddf272731bd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-decision-tree-classification-with-scikit-learn-2ddf272731bd&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----2ddf272731bd---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----2ddf272731bd--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----2ddf272731bd--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----2ddf272731bd--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----2ddf272731bd--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----2ddf272731bd--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----2ddf272731bd--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----2ddf272731bd--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----2ddf272731bd--------------------------------", "anchor_text": ""}, {"url": "https://saptashwa.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://saptashwa.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Saptashwa Bhattacharyya"}, {"url": "https://saptashwa.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "3.3K Followers"}, {"url": "https://www.linkedin.com/in/saptashwa", "anchor_text": "https://www.linkedin.com/in/saptashwa"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9a3c3c477239&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-decision-tree-classification-with-scikit-learn-2ddf272731bd&user=Saptashwa+Bhattacharyya&userId=9a3c3c477239&source=post_page-9a3c3c477239--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F423a8008308d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-decision-tree-classification-with-scikit-learn-2ddf272731bd&newsletterV3=9a3c3c477239&newsletterV3Id=423a8008308d&user=Saptashwa+Bhattacharyya&userId=9a3c3c477239&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}