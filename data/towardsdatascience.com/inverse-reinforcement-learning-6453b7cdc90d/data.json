{"url": "https://towardsdatascience.com/inverse-reinforcement-learning-6453b7cdc90d", "time": 1682994379.618149, "path": "towardsdatascience.com/inverse-reinforcement-learning-6453b7cdc90d/", "webpage": {"metadata": {"title": "Inverse Reinforcement Learning. Introduction and Main Issues | by Alexandre Gonfalonieri | Towards Data Science", "h1": "Inverse Reinforcement Learning", "description": "This article is based on the work of Johannes Heidecke, Jacob Steinhardt, Owain Evans, Jordan Alexander, Prasanth Omanakuttan, Bilal Piot, Matthieu Geist, Olivier Pietquin and other influencers in\u2026"}, "outgoing_paragraph_urls": [{"url": "https://thinkingwires.com/posts/2018-02-13-irl-tutorial-1.html", "anchor_text": "Johannes Heidecke", "paragraph_index": 0}, {"url": "https://jsteinhardt.wordpress.com/2017/02/07/model-mis-specification-and-inverse-reinforcement-learning/", "anchor_text": "Jacob Steinhardt", "paragraph_index": 0}, {"url": "https://www.lesswrong.com/posts/cnC2RMWEGiGpJv8go/model-mis-specification-and-inverse-reinforcement-learning", "anchor_text": ", Owain Evans,", "paragraph_index": 0}, {"url": "https://thegradient.pub/learning-from-humans-what-is-inverse-reinforcement-learning/", "anchor_text": "Jordan Alexander", "paragraph_index": 0}, {"url": "https://medium.com/@pomanakuttan9642?source=post_header_lockup", "anchor_text": "Prasanth Omanakuttan", "paragraph_index": 0}, {"url": "http://www.lifl.fr/~pietquin/pdf/TNNLS_2016_BPMGOP.pdf", "anchor_text": "Bilal Piot", "paragraph_index": 0}, {"url": "http://www.lifl.fr/~pietquin/pdf/TNNLS_2016_BPMGOP.pdf", "anchor_text": "Matthieu Geist", "paragraph_index": 0}, {"url": "http://www.lifl.fr/~pietquin/pdf/TNNLS_2016_BPMGOP.pdf", "anchor_text": "Olivier Pietquin", "paragraph_index": 0}, {"url": "https://thinkingwires.com/posts/2018-02-13-irl-tutorial-1.html", "anchor_text": "source", "paragraph_index": 4}, {"url": "https://thegradient.pub/learning-from-humans-what-is-inverse-reinforcement-learning/", "anchor_text": "source", "paragraph_index": 8}, {"url": "https://medium.com/@pomanakuttan9642/what-is-inverse-reinforcement-learning-e333228af146", "anchor_text": "source", "paragraph_index": 9}, {"url": "http://www.lifl.fr/~pietquin/pdf/TNNLS_2016_BPMGOP.pdf", "anchor_text": "source", "paragraph_index": 15}, {"url": "https://thinkingwires.com/posts/2018-02-13-irl-tutorial-1.html", "anchor_text": "source", "paragraph_index": 17}, {"url": "https://jsteinhardt.wordpress.com/2017/02/07/model-mis-specification-and-inverse-reinforcement-learning/", "anchor_text": "source", "paragraph_index": 20}, {"url": "https://www.lesswrong.com/posts/cnC2RMWEGiGpJv8go/model-mis-specification-and-inverse-reinforcement-learning", "anchor_text": "source", "paragraph_index": 25}, {"url": "https://www.lesswrong.com/posts/cnC2RMWEGiGpJv8go/model-mis-specification-and-inverse-reinforcement-learning", "anchor_text": "source", "paragraph_index": 31}, {"url": "https://thinkingwires.com/posts/2018-02-13-irl-tutorial-1.html", "anchor_text": "source", "paragraph_index": 33}, {"url": "http://www.lifl.fr/~pietquin/pdf/TNNLS_2016_BPMGOP.pdf", "anchor_text": "source", "paragraph_index": 37}, {"url": "https://alexandregonfalonieri.medium.com/membership", "anchor_text": "https://alexandregonfalonieri.medium.com/membership", "paragraph_index": 39}], "all_paragraphs": ["This article is based on the work of Johannes Heidecke, Jacob Steinhardt, Owain Evans, Jordan Alexander, Prasanth Omanakuttan, Bilal Piot, Matthieu Geist, Olivier Pietquin and other influencers in the field of Inverse Reinforcement Learning. I used their words to help people understand IRL.", "Inverse reinforcement learning is a recently developed machine-learning framework that can solve the inverse problem of RL.", "Basically, IRL is about learning from humans.", "Inverse reinforcement learning (IRL) is the field of learning an agent\u2019s objectives, values, or rewards by observing its behavior.", "Johannes Heidecke said \u201cWe might observe the behavior of a human in some specific task and learn which states of the environment the human is trying to achieve and what the concrete goals might be.\u201d (source)", "\u201cIRL is a paradigm relying on Markov Decision Processes (MDPs), where the goal of the apprentice agent is to find a reward function from the expert demonstrations that could explain the expert behavior.\u201d Bilal Piot, Matthieu Geist and Oliver Pietquin, Bridging the Gap between Imitation Learning and Inverse Reinforcement Learning", "In the case that one day some artificial intelligence reaches super-human capabilities, IRL might be one approach to understand what humans want and to hopefully work towards these goals.", "Jordan Alexander said \u201cThe goal is to learn a decision process to produce behavior that maximizes some predefined reward function. Basically, the goal is to extract the reward function from the observed behavior of an agent.", "For instance, consider the task of autonomous driving. One approach would be to create a reward function that captures the desired behavior of a driver, like stopping at red lights, avoiding pedestrians, etc. However, this would require an exhaustive list of every behavior we\u2019d want to consider, as well as a list of weights describing how important each behavior is.\u201d (source)", "Prasanth Omanakuttan, AI Researcher, said \u201cHowever, through IRL, the task is to take a set of human-generated driving data and extract an approximation of that human\u2019s reward function for the task. Still, much of the information necessary for solving a problem is captured within the approximation of the true reward function. Once we have the right reward function, the problem is reduced to finding the right policy, and can be solved with standard reinforcement learning methods.\u201d (source)", "\u201cThe main problem when converting a complex task into a simple reward function is that a given policy may be optimal for many different reward functions. That is, even though we have the actions from an expert, there exist many different reward functions that the expert might be attempting to maximize.\u201d Jordan Alexander, Stanford Univerisity, Learning from humans: what is inverse reinforcement learning?", "Bilal Piot, Matthieu Geist and Olivier Pietquin have said \u201cIn other words, our goal is to model an agent taking actions in a given environment. We therefore suppose that we have a state space S (the set of states the agent and environment can be in), an action space A (the set of actions the agent can take), and a transition function T(s\u2032|s,a), which gives the probability of moving from state s to state s\u2032 when taking action a. For instance, for an AI learning to control a car, the state space would be the possible locations and orientations of the car, the action space would be the set of control signals that the AI could send to the car, and the transition function would be the dynamics model for the car. The tuple of (S,A,T) is called an MDP\u2216R, which is a Markov Decision Process without a reward function. (The MDP\u2216R will either have a known horizon or a discount rate \u03b3 but we\u2019ll leave these out for simplicity.)", "The inference problem for IRL is to infer a reward function R given an optimal policy \u03c0\u2217:S\u2192A for the MDP\u2216R. We learn about the policy \u03c0\u2217 from samples (s,a) of states and the corresponding action according to \u03c0\u2217 (which may be random). Typically, these samples come from a trajectory, which records the full history of the agent\u2019s states and actions in a single episode:", "In the car example, this would correspond to the actions taken by an expert human driver who is demonstrating desired driving behaviour (where the actions would be recorded as the signals to the steering wheel, brake, etc.).", "Given the MDP\u2216R and the observed trajectory, the goal is to infer the reward function R. In a Bayesian framework, if we specify a prior on R we have:", "The likelihood P(ai|si,R) is just \u03c0R(s)[ai], where \u03c0R is the optimal policy under the reward function R. Note that computing the optimal policy given the reward is in general non-trivial; except in simple cases, we typically approximate the policy using reinforcement learning. Due to the challenges of specifying priors, computing optimal policies and integrating over reward functions, most work in IRL uses some kind of approximation to the Bayesian objective.\u201d (source)", "Johannes Heidecke said \u201cIn most reinforcement learning tasks there is no natural source for the reward signal. Instead, it has to be hand-crafted and carefully designed to accurately represent the task.", "Often, it is necessary to manually tweak the rewards of the RL agent until desired behavior is observed. A better way of finding a well fitting reward function for some objective might be to observe a (human) expert performing the task in order to then automatically extract the respective rewards from these observations.\u201d (source)", "The biggest motivation for IRL is that it is often immensely difficult to manually specify a reward function for a task.", "Johannes Steinhardt said \u201cIRL is a promising approach to learning human values in part because of the easy availability of data. For supervised learning, humans need to produce many labeled instances specialized for a task. IRL, by contrast, is an unsupervised/semi-supervised approach where any record of human behavior is a potential data source. Facebook\u2019s logs of user behavior, YouTube videos etc. provide many data-points on human behavior.", "However, while there is lots of existing data that is informative about human preferences, exploiting this data for IRL is difficult with current techniques.\u201d(source)", "Another element mentioned by Johannes Steinhardt is about the issue of data. He said \u201cThe records of human behaviour in books and videos are difficult for IRL algorithms to use. However, Data from Facebook seems promising: we can store the state and each human action (clicks and scrolling).", "While this covers a broad range of tasks, there are obvious limitations. Some kinds of human preferences seem hard to learn about from behaviour on a computer.\u201d", "Human actions depend both on their preferences and their beliefs.", "Owain Evans and Johannes Steinhardt said \u201cThe beliefs, like the preferences, are never directly observed. For narrow tasks (e.g. people choosing their favorite photos from a display), we can model humans as having full knowledge of the state. But for most real-world tasks, humans have limited information and their information changes over time. If IRL assumes the human has full information, then the model is misspecified and generalizing about what the human would prefer in other scenarios can be mistaken. Here are some examples:", "The above arises from humans forgetting information \u2014 even if the information is only a short string of characters. This is one way in which humans systematically deviate from rational Bayesian agents.\u201d (source)", "Another element given by Owain Evans and Johannes Steinhardt is long-term plans. Indeed, they said \u201cAgents will often take long series of actions that generate negative utility for them in the moment in order to accomplish a long-term goal. Such long-term plans can make IRL more difficult for a few reasons. Let\u2019s focus on two:", "To make inferences based on long-term plans, it would be helpful to have coherent data about a single agent\u2019s actions over a long period of time. But in practice, we will likely have substantially more data consisting of short snapshots of a large number of different agents (because many websites or online services already record user interactions, but it is uncommon for a single person to be exhaustively tracked and recorded over an extended period of time even while they are offline).", "On the other hand, there are some services that do have extensive data about individual users across a long period of time. However, this data has another issue: it is incomplete in a very systematic way (since it only tracks online behaviour). For instance, someone might go online most days to read course notes and Wikipedia for a class; this is data that would likely be recorded. However, it is less likely that one would have a record of that person taking the final exam, passing the class and then getting an internship based on their class performance. Of course, some pieces of this sequence would be inferable based on some people\u2019s e-mail records, etc., but it would likely be under-represented in the data relative to the record of Wikipedia usage. In either case, some non-trivial degree of inference would be necessary to make sense of such data.", "Next, we discuss another potential issue \u2014 fragility to model misspecification.", "Suppose someone spends 99 days doing a boring task to accomplish an important goal on day 100. A system that is only trying to correctly predict actions will be right 99% of the time if it predicts that the person inherently enjoys boring tasks. Of course, a system that understands the goal and how the tasks lead to the goal will be right 100% of the time, but even minor errors in its understanding could bring the accuracy back below 99%.", "Basically, large changes in the model of the agent might only lead to small changes in the predictive accuracy of the model, and the longer the time horizon on which a goal is realized, the more this might be the case. This means that even slight misspecifications in the model could tip the scales back in favor of a (very) incorrect reward function. One solution could be to identify \u201cimportant\u201d predictions that seem closely tied to the reward function, and focus particularly on getting those predictions right.\u201d (source)", "In the case of even slight model mis-specification, the \u201ccorrect\u201d model might actually perform worse under typical metrics such as predictive accuracy. Therefore, more careful methods of constructing a model might be necessary.", "Johannes Heidecke, AI Researcher, said \u201cIn IRL, we are given some agent\u2019s policy or a history of behavior and we try to find a reward function that explains the given behavior. Under the assumption that our agent acted optimally, i.e. always picks the best possible action for its reward function, we try to estimate a reward function that could have led to this behavior.\u201d (source)", "How to find a reward function under which observed behavior is optimal. This comes with two main problems:", "Important: IRL seeks the reward functions that \u2018explains\u2019 the demonstrations. Do not confuse this with Apprenticeship learning (AL) where the primary interest is a policy which can generate the seen demonstrations.", "For Bilal Piot, Matthieu Geist and Oliver Pietquin, \u201cIRL relies on the assumption that the expert\u2019s policy is optimal with respect to an unknown reward function. In this case, the first aim of the apprentice is to learn a reward function that explains the observed expert behavior. Then, using direct reinforcement learning, it optimizes its policy according to this reward and hopefully behaves as well as the expert. Learning a reward has some advantages over learning a policy immediately. First, the reward can be analyzed so as to better understand the expert\u2019s behavior. Second, it allows adapting to perturbations in the dynamics of the environment .", "In other words, it is transferable to other environments. Third, it allows improving with time through real interactions and without requiring new demonstrations. However, a major issue is that an MDP must be solved to obtain the optimal policy with respect to the learned reward. Another issue is that the IRL problem is ill-posed as every policy is optimal for the null reward (which is obviously not the reward one is looking for).\u201d (source)", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "AI Consultant \u2014 Working on Brain-computer interface and new AI business models \u2014 Support my writing: https://alexandregonfalonieri.medium.com/membership"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F6453b7cdc90d&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finverse-reinforcement-learning-6453b7cdc90d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finverse-reinforcement-learning-6453b7cdc90d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finverse-reinforcement-learning-6453b7cdc90d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finverse-reinforcement-learning-6453b7cdc90d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----6453b7cdc90d--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----6453b7cdc90d--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://alexandregonfalonieri.medium.com/?source=post_page-----6453b7cdc90d--------------------------------", "anchor_text": ""}, {"url": "https://alexandregonfalonieri.medium.com/?source=post_page-----6453b7cdc90d--------------------------------", "anchor_text": "Alexandre Gonfalonieri"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F94354d6ab94d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finverse-reinforcement-learning-6453b7cdc90d&user=Alexandre+Gonfalonieri&userId=94354d6ab94d&source=post_page-94354d6ab94d----6453b7cdc90d---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6453b7cdc90d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finverse-reinforcement-learning-6453b7cdc90d&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6453b7cdc90d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finverse-reinforcement-learning-6453b7cdc90d&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://thinkingwires.com/posts/2018-02-13-irl-tutorial-1.html", "anchor_text": "Johannes Heidecke"}, {"url": "https://jsteinhardt.wordpress.com/2017/02/07/model-mis-specification-and-inverse-reinforcement-learning/", "anchor_text": "Jacob Steinhardt"}, {"url": "https://www.lesswrong.com/posts/cnC2RMWEGiGpJv8go/model-mis-specification-and-inverse-reinforcement-learning", "anchor_text": ", Owain Evans,"}, {"url": "https://thegradient.pub/learning-from-humans-what-is-inverse-reinforcement-learning/", "anchor_text": "Jordan Alexander"}, {"url": "https://medium.com/@pomanakuttan9642?source=post_header_lockup", "anchor_text": "Prasanth Omanakuttan"}, {"url": "http://www.lifl.fr/~pietquin/pdf/TNNLS_2016_BPMGOP.pdf", "anchor_text": "Bilal Piot"}, {"url": "http://www.lifl.fr/~pietquin/pdf/TNNLS_2016_BPMGOP.pdf", "anchor_text": "Matthieu Geist"}, {"url": "http://www.lifl.fr/~pietquin/pdf/TNNLS_2016_BPMGOP.pdf", "anchor_text": "Olivier Pietquin"}, {"url": "https://thinkingwires.com/posts/2018-02-13-irl-tutorial-1.html", "anchor_text": "source"}, {"url": "https://thegradient.pub/learning-from-humans-what-is-inverse-reinforcement-learning/", "anchor_text": "source"}, {"url": "https://medium.com/@pomanakuttan9642/what-is-inverse-reinforcement-learning-e333228af146", "anchor_text": "source"}, {"url": "http://www.miubiq.cs.titech.ac.jp/modeling-risk-anticipation-and-defensive-driving-on-residential-roads-using-inverse-reinforcement-learning/", "anchor_text": "Source"}, {"url": "https://jsteinhardt.wordpress.com/2017/02/07/model-mis-specification-and-inverse-reinforcement-learning/", "anchor_text": "Source"}, {"url": "http://www.lifl.fr/~pietquin/pdf/TNNLS_2016_BPMGOP.pdf", "anchor_text": "source"}, {"url": "https://thinkingwires.com/posts/2018-02-13-irl-tutorial-1.html", "anchor_text": "source"}, {"url": "https://jsteinhardt.wordpress.com/2017/02/07/model-mis-specification-and-inverse-reinforcement-learning/", "anchor_text": "source"}, {"url": "https://www.lesswrong.com/posts/cnC2RMWEGiGpJv8go/model-mis-specification-and-inverse-reinforcement-learning", "anchor_text": "source"}, {"url": "https://www.lesswrong.com/posts/cnC2RMWEGiGpJv8go/model-mis-specification-and-inverse-reinforcement-learning", "anchor_text": "source"}, {"url": "https://thinkingwires.com/posts/2018-02-13-irl-tutorial-1.html", "anchor_text": "source"}, {"url": "http://www.lifl.fr/~pietquin/pdf/TNNLS_2016_BPMGOP.pdf", "anchor_text": "source"}, {"url": "https://thinkingwires.com/posts/2018-02-13-irl-tutorial-1.html", "anchor_text": "https://thinkingwires.com/posts/2018-02-13-irl-tutorial-1.html"}, {"url": "https://www.lesswrong.com/posts/cnC2RMWEGiGpJv8go/model-mis-specification-and-inverse-reinforcement-learning", "anchor_text": "https://www.lesswrong.com/posts/cnC2RMWEGiGpJv8go/model-mis-specification-and-inverse-reinforcement-learning"}, {"url": "http://www.lifl.fr/~pietquin/pdf/TNNLS_2016_BPMGOP.pdf", "anchor_text": "http://www.lifl.fr/~pietquin/pdf/TNNLS_2016_BPMGOP.pdf"}, {"url": "https://medium.com/@pomanakuttan9642/what-is-inverse-reinforcement-learning-e333228af146", "anchor_text": "https://medium.com/@pomanakuttan9642/what-is-inverse-reinforcement-learning-e333228af146"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----6453b7cdc90d---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----6453b7cdc90d---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/technology?source=post_page-----6453b7cdc90d---------------technology-----------------", "anchor_text": "Technology"}, {"url": "https://medium.com/tag/data-science?source=post_page-----6453b7cdc90d---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----6453b7cdc90d---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6453b7cdc90d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finverse-reinforcement-learning-6453b7cdc90d&user=Alexandre+Gonfalonieri&userId=94354d6ab94d&source=-----6453b7cdc90d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6453b7cdc90d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finverse-reinforcement-learning-6453b7cdc90d&user=Alexandre+Gonfalonieri&userId=94354d6ab94d&source=-----6453b7cdc90d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6453b7cdc90d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finverse-reinforcement-learning-6453b7cdc90d&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----6453b7cdc90d--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F6453b7cdc90d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finverse-reinforcement-learning-6453b7cdc90d&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----6453b7cdc90d---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----6453b7cdc90d--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----6453b7cdc90d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----6453b7cdc90d--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----6453b7cdc90d--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----6453b7cdc90d--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----6453b7cdc90d--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----6453b7cdc90d--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----6453b7cdc90d--------------------------------", "anchor_text": ""}, {"url": "https://alexandregonfalonieri.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://alexandregonfalonieri.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Alexandre Gonfalonieri"}, {"url": "https://alexandregonfalonieri.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "4.5K Followers"}, {"url": "https://alexandregonfalonieri.medium.com/membership", "anchor_text": "https://alexandregonfalonieri.medium.com/membership"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F94354d6ab94d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finverse-reinforcement-learning-6453b7cdc90d&user=Alexandre+Gonfalonieri&userId=94354d6ab94d&source=post_page-94354d6ab94d--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F91e2af9e409c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finverse-reinforcement-learning-6453b7cdc90d&newsletterV3=94354d6ab94d&newsletterV3Id=91e2af9e409c&user=Alexandre+Gonfalonieri&userId=94354d6ab94d&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}