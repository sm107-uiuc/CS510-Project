{"url": "https://towardsdatascience.com/illustrating-the-reformer-393575ac6ba0", "time": 1683003567.211713, "path": "towardsdatascience.com/illustrating-the-reformer-393575ac6ba0/", "webpage": {"metadata": {"title": "\ud83d\udca1Illustrating the Reformer. \ud83d\ude8a \ufe0f The efficient Transformer | by Alireza Dirafzoon | Towards Data Science", "h1": "\ud83d\udca1Illustrating the Reformer", "description": "\ud83c\udfa5 If you have been developing machine learning algorithms for processing sequential data \u2014 such as text in language processing, speech signals, or videos \u2014 you probably have heard about or used the\u2026"}, "outgoing_paragraph_urls": [{"url": "https://lbourdois.github.io/blog/nlp/Reformer/", "anchor_text": "French", "paragraph_index": 0}, {"url": "https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html", "anchor_text": "Transformer", "paragraph_index": 1}, {"url": "https://ai.googleblog.com/2020/01/reformer-efficient-transformer.html", "anchor_text": "Reformer", "paragraph_index": 2}, {"url": "https://arxiv.org/pdf/2001.04451.pdf", "anchor_text": "Reformer: The efficient Transforme", "paragraph_index": 2}, {"url": "https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf", "anchor_text": "sequence-to-sequence learning problem", "paragraph_index": 4}, {"url": "http://Neural machine translation by jointly learning to align and translate.", "anchor_text": "attention mechanism", "paragraph_index": 4}, {"url": "https://arxiv.org/pdf/1609.08144.pdf", "anchor_text": "Google\u2019s Neural Machine Translation System", "paragraph_index": 4}, {"url": "https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html", "anchor_text": "Transformer model", "paragraph_index": 5}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "Attention is all you need", "paragraph_index": 5}, {"url": "https://openai.com/blog/better-language-models/", "anchor_text": "Open AI\u2019s GPT2 language model", "paragraph_index": 5}, {"url": "https://github.com/huggingface/transformers", "anchor_text": "Transformers library", "paragraph_index": 5}, {"url": "https://transformer.huggingface.co/", "anchor_text": "Write with transformers", "paragraph_index": 5}, {"url": "https://talktotransformer.com/", "anchor_text": "Talk to Transformers", "paragraph_index": 5}, {"url": "https://magenta.tensorflow.org/music-transformer", "anchor_text": "generating music", "paragraph_index": 5}, {"url": "https://ai.google/research/pubs/pub46840/", "anchor_text": "images", "paragraph_index": 5}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "The Illustrated Transformer", "paragraph_index": 6}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "The Illustrated Transformer", "paragraph_index": 9}, {"url": "https://en.wikipedia.org/wiki/Locality-sensitive_hashing", "anchor_text": "locality-sensitive-hashing", "paragraph_index": 15}, {"url": "https://arxiv.org/abs/1707.04585", "anchor_text": "reversible residual layers", "paragraph_index": 15}, {"url": "https://arxiv.org/pdf/1512.03385.pdf", "anchor_text": "paper", "paragraph_index": 31}, {"url": "https://en.wikipedia.org/wiki/Vanishing_gradient_problem", "anchor_text": "vanishing gradient problem", "paragraph_index": 31}, {"url": "https://papers.nips.cc/paper/6816-the-reversible-residual-network-backpropagation-without-storing-activations.pdf", "anchor_text": "reversible residual network (RevNet", "paragraph_index": 32}, {"url": "https://github.com/google/trax/tree/master/trax/models/reformer", "anchor_text": "code", "paragraph_index": 43}, {"url": "https://github.com/google/trax/tree/master/trax/", "anchor_text": "Trax", "paragraph_index": 43}, {"url": "https://colab.research.google.com/github/google/trax/blob/master/trax/models/reformer/image_generation.ipynb", "anchor_text": "image generation", "paragraph_index": 43}, {"url": "https://colab.research.google.com/github/google/trax/blob/master/trax/models/reformer/text_generation.ipynb", "anchor_text": "text generation", "paragraph_index": 43}, {"url": "https://medium.com/u/cbc32bf7cff7?source=post_page-----393575ac6ba0--------------------------------", "anchor_text": "Abraham Kang", "paragraph_index": 44}, {"url": "https://github.com/alirezadir", "anchor_text": "https://github.com/alirezadir", "paragraph_index": 46}], "all_paragraphs": ["See also: Translation in \ud83c\uddeb\ud83c\uddf7 French", "\ud83c\udfa5 If you have been developing machine learning algorithms for processing sequential data \u2014 such as text in language processing, speech signals, or videos \u2014 you probably have heard about or used the Transformer model, and you probably know it\u2019s different from the one that twitter thinks:", "\ud83d\udd0a\ud83d\uddde Recently, Google introduced the Reformer architecture, a Transformer model designed to efficiently handle processing very long sequences of data (e.g. up to 1 million words in a language processing). Execution of Reformer requires much lower memory consumption and achieves impressive performance even when running on only a single GPU. The paper Reformer: The efficient Transformer will be presented in ICLR 2020 (and has received a near-perfect score in the reviews). Reformer model is expected to have a significant impact on the filed by going beyond language applications (e.g. music, speech, image and video generation).", "\ud83d\udca1 In this post, we will try to dive into the Reformer model and try to understand it with some visual guides. Ready? \ud83d\udcaa", "\ud83c\udfac A class of tasks in NLP (e.g. machine translation, text generation, question answering) can be formulated as a sequence-to-sequence learning problem. Long short term memory (LSTM) neural networks, later equipped with an attention mechanism, were a prominent architecture used to build prediction models for such problems \u2014 e.g. in Google\u2019s Neural Machine Translation System. However, the inherently sequential nature of recurrence in LSTMs, was the biggest obstacle in parallelization of computation over the sequence of data (in terms of speed and vanishing gradients), and as a result, those architectures could not take advantage of the context over long sequences.", "\ud83d\ude80 The more recent Transformer model \u2014 introduced in the paper Attention is all you need \u2014 achieved state of the art performance in a number of tasks by getting rid of recurrence and instead introducing multi-head self-attention mechanism. The main novelty of the transformer was its capability of parallel processing, which enabled processing long sequences (with context windows of thousands of words) resulting in superior models such as the remarkable Open AI\u2019s GPT2 language model with less training time. \ud83e\udd17 Huggingface\u2019s Transformers library \u2014 with over 32+ pre-trained models in 100+ languages and interoperability between TensorFlow and PyTorch \u2014 is a fantastic open-source effort for building state-of-the-art NLP systems. \ud83c\udfae Write with transformers and Talk to Transformers are some of the fun demos to play with. The Transformer has been used for applications beyond text as well such as generating music and images.", "\ud83d\udea6Before diving deep in the reformer, let\u2019s review what is challenging about the transformer model. This requires some understanding of the transformer architecture itself, which we cannot go through in this post. However, if you already don\u2019t know, Jay Alamar\u2019s The Illustrated Transformer post is the greatest visual explanation so far, and I highly encourage reading his post before going through the rest of this post.", "\ud83e\udd14 Although transformer models yield great results being used on increasingly long sequences \u2014 e.g. 11K long text examples in (Liu et al., 2018) \u2014 many of such large models can only be trained in large industrial compute platforms and even cannot be fine-tuned on a single GPU even for a single training step due to their memory requirements. For example, the full GPT-2 model consists of roughly 1.5B parameters. The number of parameters in the largest configuration reported in (Shazeer et al., 2018) exceeds 0.5B per layer, while the number of layers goes up to 64 in (Al-Rfou et al., 2018).", "\ud83d\udca1 Let\u2019s look at a simplified overview of the Transformer model:", "\ud83d\ude15 If this model doesn\u2019t look familiar or seems hard to understand, I urge you to pause here and review \u27a1\ufe0f The Illustrated Transformer post.", "You may notice there exist some \ud83d\udc53\u2019s in the diagram with 3 different colors. Each of these unique \ud83d\udc53\u2019s represents a part of the Transformer model that the Reformer authors looked at as sources of computation and memory issues:", "Computing attention on sequences of length L is O(L\u00b2) (both time and memory). Imagine what happens if we have a sequence of length 64K.", "A model with N layers consumes N-times larger memory than a single-layer model, as activations in each layer need to be stored for back-propagation.", "The depth of intermediate feed-forward layers is often much larger than the depth of attention activations.", "The Reformer model addresses the above three main sources of memory consumption in the Transformer and improves upon them in such a way that the Reformer model can handle context windows of up to 1 million words, all on a single accelerator and using only 16GB of memory.", "In a nutshell, the Reformer model combines two techniques to solve the problems of attention and memory allocation: locality-sensitive-hashing (LSH) to reduce the complexity of attending over long sequences, and reversible residual layers to more efficiently use the memory available.", "Below we go into further details.", "Attention in deep learning is a mechanism that enables the network to focus attentively on different parts of a the context based on their relativeness to the current timestep. There exist 3 types of attention mechanism in the transformer model as below:", "The standard attention used in the Transformer is the scaled dot-product, formulated as:", "From the above equation and the figure below, it can be observed that the computational and memory cost of multiplication QK\u1d40 (with the shape [L, L]) are both in O(L\u00b2), which is the main memory bottleneck.", "\u2753But is it necessary to compute and store the full matrix QK\u1d40 ? The answer is no, as we are only interested in softmax(QK\u1d40 ), which is dominated by the largest elements in a typically sparse matrix. Hence, as you can see in the above example, for each query q we only need to pay attention to the keys k that are closest to q. For example, if K is of length 64K, for each q we could only consider a small subset of the 32 or 64 closest keys. So the attention mechanism finds the nearest neighbor keys of a query but in an inefficient manner. \ud83d\udca1Does this remind you of nearest neighbors' search?", "The first novelty in the reformer comes from replacing dot-product attention with locality-sensitive hashing (LSH) to change the complexity from O(L\u00b2) to O(L log L).", "LSH is a well-known algorithm for an efficient and approximate way of nearest neighbors search in high dimensional datasets. The main idea behind LSH is to select hash functions such that for two points \u2018p\u2019 and \u2018q\u2019, if \u2018q\u2019 is close to \u2018p\u2019 then with good enough probability we have \u2018hash(q) == hash(p)\u2019.", "The simplest way to achieve this is to keep cutting space by random hyperplanes and append sign(p\u1d40H) as to the hash code of each point. Let\u2019s look at an example below:", "Once we find hash codes of a desired length, we divide the points into buckets based on their hash codes \u2014 in the above example, \u2018a\u2019 and \u2018b\u2019 belong to the same bucket since hash(a) == hash(b). Now the search space to find the nearest neighbors of each point reduces dramatically from the whole data set into the bucket where it belongs to.", "\ud83d\uddd2 Angular LSH: A variant of the plain LSH algorithm, referred to as angular LSH, projects the points on a unit sphere which has been divided into predefined regions each with a distinct code. Then a series of random rotations of points define the bucket the points belong to. Let\u2019s illustrate this through a simplified 2D example, taken from the Reformer paper:", "Here we have two points that are projected onto a unit circle and rotated randomly 3 times with different angles. We can observe that they are unlikely to share the same hash bucket. In the next example, however, we see the two points that are pretty close to each other will end up sharing the same hash buckets after 3 random rotations:", "Now the basic idea behind LSH attention is as follows. Looking back into the standard attention formula above, instead of computing attention over all of the vectors in Q and K matrices, we do the following:", "Muti-round LSH attention: Repeat the above procedure a few times to increase the probability that similar items do not fall in different buckets.", "The animation below illustrates a simplified version of LSH Attention based on the figure from the paper.", "Now we are ready to solve the second and third issues in the Transformer, i.e. a large number of (N) encoder and decoder layers and the depth of the feedforward layers.", "Paying close attention to the encoder and decoder blocks in Fig. 2, we realize that each attention layer and feedforward layer is wrapped into a residual block (similar to what we see in Fig. 6 (left)). Residual networks(ResNets) \u2014 introduced in this paper\u2014 are powerful component used in NN architectures to help with vanishing gradient problem in deep networks (with many layers). However, memory consumption in ResNets is a bottleneck as one needs to store the activations in each layer in memory in order to calculate gradients during backpropagation. The memory cost is proportional to the number of units in the network.", "To resolve this issue, the reversible residual network (RevNet) which are composed of a series of reversible blocks. In Revnet, each layer\u2019s activations can be reconstructed exactly from the subsequent layer\u2019s activations, which enables us to perform backpropagation without storing the activations in memory. Fig. 6. illustrates residual blocks and reversible residual blocks. Note how we can compute the inputs of the block (X\u2081, X\u2082 ) from its outputs (Y\u2081, Y\u2082).", "Going back to our second problem, the issue was dealing with memory requirements of N-layer Transformer network \u2014 with potentially pretty large N.", "Reformer applies the RevNet idea to the Transformer by combining the attention and feed-forward layers inside the RevNet block. In Fig. 6, now F becomes an attention layer and G becomes the feed-forward layer:", "\ud83c\udf89 Now using reversible residual layers instead of standard residuals enables storing activations only once during the training process instead of N times.", "The last portion of efficiency improvements in the Reformer deal with the 3rd problem, i.e. high dimensional intermediate vectors of the feed-forward layers \u2014 that can go up to 4K and higher in dimensions.", "Due to the fact that computations in feed-forward layers are independent across positions in a sequence, the computations for the forward and backward passes as well as the reverse computation can be all split into chunks. For example, for the forward pass we will have:", "The authors conducted experiments on two tasks: the image generation task imagenet64 (with sequences of length 12K) and the text task enwik8 (with sequences of length 64K), and evaluated the effect of reversible Transformer and LSH hashing on the memory, accuracy, and speed.", "\ud83c\udf89 Reversible Transformer matches baseline: Their experiment results showed that the reversible Transformer saves memory without sacrificing accuracy:", "\ud83c\udf89 LSH attention matches baseline:\ud83d\udcd4Note that as LSH attention is an approximation of full attention, its accuracy improves as the hash value increases. When the hash value is 8, LSH attention is almost equivalent to full attention:", "\ud83c\udf89 They also demonstrated that the conventional attention slows down as the sequence length increases, while LSH attention speed remains steady, and it runs on sequences of length ~100K at usual speed on 8GB GPUs:", "The final Reformer model performed similarly compared to the Transformer model, but showed higher storage efficiency and faster speed on long sequences.", "\ud83e\udd16 The code for the Reformer has been released as part of the new Trax library. Trax is a modular deep learning training and inference library which is aimed to allow you to understand deep learning from scratch. The Reformer code includes several examples that you can train and infer on image generation and text generation tasks.", "I would like to thank \u0141ukasz Kaiser for his vivid presentation of the Reformer and providing supplementary material. I would also like to thank Abraham Kang for his deep review and constructive feedback.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "@Facebook \ud83d\udde3\ud83d\udcac\ud83e\udd16 | Machine Learning Enthusiast, Into Conversational AI, VR/AR, Robotics | Github: https://github.com/alirezadir"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F393575ac6ba0&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fillustrating-the-reformer-393575ac6ba0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fillustrating-the-reformer-393575ac6ba0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fillustrating-the-reformer-393575ac6ba0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fillustrating-the-reformer-393575ac6ba0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----393575ac6ba0--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----393575ac6ba0--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@alirezadir?source=post_page-----393575ac6ba0--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@alirezadir?source=post_page-----393575ac6ba0--------------------------------", "anchor_text": "Alireza Dirafzoon"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F608d07f103b8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fillustrating-the-reformer-393575ac6ba0&user=Alireza+Dirafzoon&userId=608d07f103b8&source=post_page-608d07f103b8----393575ac6ba0---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F393575ac6ba0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fillustrating-the-reformer-393575ac6ba0&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F393575ac6ba0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fillustrating-the-reformer-393575ac6ba0&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://lbourdois.github.io/blog/nlp/Reformer/", "anchor_text": "French"}, {"url": "https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html", "anchor_text": "Transformer"}, {"url": "https://ai.googleblog.com/2020/01/reformer-efficient-transformer.html", "anchor_text": "Reformer"}, {"url": "https://arxiv.org/pdf/2001.04451.pdf", "anchor_text": "Reformer: The efficient Transforme"}, {"url": "https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf", "anchor_text": "sequence-to-sequence learning problem"}, {"url": "http://Neural machine translation by jointly learning to align and translate.", "anchor_text": "attention mechanism"}, {"url": "https://arxiv.org/pdf/1609.08144.pdf", "anchor_text": "Google\u2019s Neural Machine Translation System"}, {"url": "https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html", "anchor_text": "Transformer model"}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "Attention is all you need"}, {"url": "https://openai.com/blog/better-language-models/", "anchor_text": "Open AI\u2019s GPT2 language model"}, {"url": "https://github.com/huggingface/transformers", "anchor_text": "Transformers library"}, {"url": "https://transformer.huggingface.co/", "anchor_text": "Write with transformers"}, {"url": "https://talktotransformer.com/", "anchor_text": "Talk to Transformers"}, {"url": "https://magenta.tensorflow.org/music-transformer", "anchor_text": "generating music"}, {"url": "https://ai.google/research/pubs/pub46840/", "anchor_text": "images"}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "The Illustrated Transformer"}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "The Illustrated Transformer"}, {"url": "https://en.wikipedia.org/wiki/Locality-sensitive_hashing", "anchor_text": "locality-sensitive-hashing"}, {"url": "https://arxiv.org/abs/1707.04585", "anchor_text": "reversible residual layers"}, {"url": "https://arxiv.org/pdf/1512.03385.pdf", "anchor_text": "paper"}, {"url": "https://en.wikipedia.org/wiki/Vanishing_gradient_problem", "anchor_text": "vanishing gradient problem"}, {"url": "https://papers.nips.cc/paper/6816-the-reversible-residual-network-backpropagation-without-storing-activations.pdf", "anchor_text": "reversible residual network (RevNet"}, {"url": "https://github.com/google/trax/tree/master/trax/models/reformer", "anchor_text": "code"}, {"url": "https://github.com/google/trax/tree/master/trax/", "anchor_text": "Trax"}, {"url": "https://colab.research.google.com/github/google/trax/blob/master/trax/models/reformer/image_generation.ipynb", "anchor_text": "image generation"}, {"url": "https://colab.research.google.com/github/google/trax/blob/master/trax/models/reformer/text_generation.ipynb", "anchor_text": "text generation"}, {"url": "https://medium.com/u/cbc32bf7cff7?source=post_page-----393575ac6ba0--------------------------------", "anchor_text": "Abraham Kang"}, {"url": "https://ai.googleblog.com/2020/01/reformer-efficient-transformer.html", "anchor_text": "Reformer: The Efficient TransformerUnderstanding sequential data - such as language, music or videos - is a challenging task, especially when there is\u2026ai.googleblog.com"}, {"url": "https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html", "anchor_text": "Transformer: A Novel Neural Network Architecture for Language UnderstandingNeural networks, in particular recurrent neural networks (RNNs), are now at the core of the leading approaches to\u2026ai.googleblog.com"}, {"url": "https://arxiv.org/pdf/2001.04451.pdf", "anchor_text": "Reformer: The efficient Transforme"}, {"url": "https://github.com/google/trax/tree/master/trax/", "anchor_text": "Google/Trax deep learning library"}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "The Illustrated Transformer"}, {"url": "https://github.com/huggingface/transformers", "anchor_text": "Huggingface/Transformers NLP library"}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "Attention is all you need"}, {"url": "https://openai.com/blog/better-language-models/", "anchor_text": "Open AI\u2019s GPT2 language model"}, {"url": "https://transformer.huggingface.co/", "anchor_text": "Write with transformers"}, {"url": "https://talktotransformer.com/", "anchor_text": "Talk to Transformers"}, {"url": "https://arxiv.org/pdf/1609.08144.pdf", "anchor_text": "Google\u2019s Neural Machine Translation System"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----393575ac6ba0---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/nlp?source=post_page-----393575ac6ba0---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/ai?source=post_page-----393575ac6ba0---------------ai-----------------", "anchor_text": "AI"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----393575ac6ba0---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----393575ac6ba0---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F393575ac6ba0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fillustrating-the-reformer-393575ac6ba0&user=Alireza+Dirafzoon&userId=608d07f103b8&source=-----393575ac6ba0---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F393575ac6ba0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fillustrating-the-reformer-393575ac6ba0&user=Alireza+Dirafzoon&userId=608d07f103b8&source=-----393575ac6ba0---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F393575ac6ba0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fillustrating-the-reformer-393575ac6ba0&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----393575ac6ba0--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F393575ac6ba0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fillustrating-the-reformer-393575ac6ba0&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----393575ac6ba0---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----393575ac6ba0--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----393575ac6ba0--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----393575ac6ba0--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----393575ac6ba0--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----393575ac6ba0--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----393575ac6ba0--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----393575ac6ba0--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----393575ac6ba0--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@alirezadir?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@alirezadir?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Alireza Dirafzoon"}, {"url": "https://medium.com/@alirezadir/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "105 Followers"}, {"url": "https://github.com/alirezadir", "anchor_text": "https://github.com/alirezadir"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F608d07f103b8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fillustrating-the-reformer-393575ac6ba0&user=Alireza+Dirafzoon&userId=608d07f103b8&source=post_page-608d07f103b8--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F608d07f103b8%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fillustrating-the-reformer-393575ac6ba0&user=Alireza+Dirafzoon&userId=608d07f103b8&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}