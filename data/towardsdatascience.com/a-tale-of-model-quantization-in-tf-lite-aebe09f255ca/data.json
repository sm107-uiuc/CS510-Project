{"url": "https://towardsdatascience.com/a-tale-of-model-quantization-in-tf-lite-aebe09f255ca", "time": 1683006968.481951, "path": "towardsdatascience.com/a-tale-of-model-quantization-in-tf-lite-aebe09f255ca/", "webpage": {"metadata": {"title": "A Tale of Model Quantization in TF Lite | by Sayak Paul | Towards Data Science", "h1": "A Tale of Model Quantization in TF Lite", "description": "State of the art machine learning models are often bulky which often makes them inefficient for deployment in resource-constrained environments, like mobile phones, Raspberry Pis, microcontrollers\u2026"}, "outgoing_paragraph_urls": [{"url": "https://app.wandb.ai/sayakpaul/tale-of-quantization", "anchor_text": "here", "paragraph_index": 0}, {"url": "https://github.com/sayakpaul/Adventures-in-TensorFlow-Lite", "anchor_text": "Check out the code on GitHub", "paragraph_index": 3}, {"url": "https://www.youtube.com/watch?v=27Zx-4GOQA8", "anchor_text": "TensorFlow Lite: ML for mobile and IoT devices (TF Dev Summit \u201820)", "paragraph_index": 6}, {"url": "https://colab.research.google.com/", "anchor_text": "Colab", "paragraph_index": 13}, {"url": "https://www.tensorflow.org/lite/performance/post_training_quantization", "anchor_text": "Here", "paragraph_index": 18}, {"url": "https://www.tensorflow.org/api_docs/python/tf/lite/TFLiteConverter", "anchor_text": "here", "paragraph_index": 22}, {"url": "https://www.tensorflow.org/lite/performance/post_training_quantization", "anchor_text": "Here", "paragraph_index": 22}, {"url": "https://www.tensorflow.org/model_optimization/guide/quantization/training_comprehensive_guide", "anchor_text": "Here", "paragraph_index": 26}, {"url": "https://www.tensorflow.org/lite/performance/gpu", "anchor_text": "GPU delegates", "paragraph_index": 35}, {"url": "https://www.tensorflow.org/lite/performance/post_training_float16_quant", "anchor_text": "float16 quantization", "paragraph_index": 35}, {"url": "https://www.tensorflow.org/lite/performance/post_training_quantization", "anchor_text": "This is where", "paragraph_index": 38}, {"url": "https://www.tensorflow.org/lite/examples", "anchor_text": "there are a number of example applications", "paragraph_index": 40}, {"url": "https://www.tensorflow.org/lite/guide", "anchor_text": "here", "paragraph_index": 40}, {"url": "https://github.com/sayakpaul/Adventures-in-TensorFlow-Lite", "anchor_text": "Check out the code on GitHub \u2192", "paragraph_index": 42}, {"url": "https://sayak.dev/", "anchor_text": "https://sayak.dev/", "paragraph_index": 44}], "all_paragraphs": ["Interact with the dashboard of results here.", "State of the art machine learning models are often bulky which often makes them inefficient for deployment in resource-constrained environments, like mobile phones, Raspberry Pis, microcontrollers, and so on. Even if you think that you might get around this problem by hosting your model on the Cloud and using an API to serve results \u2014 think of constrained environments where internet bandwidths might not be always high, or where data must not leave a particular device.", "We need a set of tools that make the transition to on-device machine learning seamless. In this report, I will show you how TensorFlow Lite (TF Lite) can really shine in situations like this. We\u2019ll cover model optimization strategies and quantization techniques supported by TensorFlow.", "Check out the code on GitHub \u2192", "Thanks to Arun, Khanh, and Pulkit (Google) for sharing incredibly useful tips for this report.", "In this article, we\u2019ll cover the following topics \u2013", "In their talk TensorFlow Lite: ML for mobile and IoT devices (TF Dev Summit \u201820), Tim Davis and T.J. Alumbaugh emphasize the following:", "To make the heavy-weight ML models deployable on tiny devices we need to optimize them, for instance, to fit a 1.9GB model into a 2GB application. To help ML developers and mobile application developers, the TensorFlow team has come up with two solutions:", "TensorFlow, via TensorFlow Lite and the Model Optimization Toolkit, supports the following model optimization strategies today -", "In this article, we will focus on quantization.", "Generally, our machine learning models operate in float32 precision format. All the model parameters are stored in this precision format, which often leads to heavier models. The heaviness of a model has a direct correlation to the speed at which the model makes predictions. So, it might occur to you naturally that what if we could reduce the precision in which our models would operate, we could cut down on prediction times. That is what quantization does - it reduces the precision to lower forms like float16, int8, etc to represent the parameters of a model.", "Quantization can be applied to a model in two flavors -", "We will see both these flavors in this report. Let\u2019s get started!", "All of the experiments that we do in this report were performed on Colab. I used the flowers dataset for the experiments and fine-tuned a pre-trained MobileNetV2 network to start off with. Here\u2019s the code that defines the network architecture -", "The networks were trained for 10 epochs with a batch size of 32.", "We see the network trains reasonably well and comes in at 35.6 MB.", "After you have trained a model intf.keras, the quantization part is just a matter of a few lines of code. So, the way you would do that is as follows -", "You are first loading your model into a TFLiteConverter converter class, then specifying an optimization policy, and finally, you ask TFLite to convert your model with the optimization policy. Serializing the converted TF Lite file is straight-forward -", "This form of quantization is also referred to as post-training dynamic range quantization. It quantizes the weights of your model to 8-bits of precision. Here you can find more details about this and other post-training quantization schemes.", "TF Lite allows us to specify a number of different configurations when converting our models. We saw once such configuration in the aforementioned code, where we specified the optimization policy.", "Apart from tf.lite.Optimize.DEFAULT, there are other two policies available - tf.lite.Optimize.OPTIMIZE_FOR_SIZE & tf.lite.Optimize.OPTIMIZE_FOR_LATENCY. From the names, you can see that, based on the choice of policy, TF Lite will try to optimize the models accordingly.", "We can specify other things like -", "Learn more about the TFLiteConverter class here. It's important to note that these different configuration options allow us to maintain trade-offs between a model's prediction speed and it's accuracy. Here, you can find a number of trade-offs with respect to different post-training quantization schemes available in TF Lite.", "Below we can see some useful statistics on this converted model.", "We see a substantial reduction in the size of the model but that came with the cost of accuracy. Ideally, we wouldn\u2019t want an accuracy loss this big in our converted model. This suggests that we need to explore other quantization schemes to further improve the accuracy of the converted model.", "A good first approach here is to train your model in a way in which it would learn to compensate for the information loss that might be induced from quantization. With quantization-aware training, we can do just that. To train our network in a quantization-aware manner, we just add the following lines of code -", "Now, you can train qat_model in the same way you would train a tf.keras model. Here you can find comprehensive coverage of QAT.", "Below, we can see that this quantization aware model does slightly better than our previous model.", "In terms of model size, the QAT model is similar to the non-QAT model:", "But in terms of model training time, we see that the QAT model takes more time. This is because during QAT, fake quantization nodes are introduced in the model to compensate for the information loss, which makes the QAT model takes more time to converge.", "This is important to keep in mind in cases where you are optimizing for time to convergence. If your training model takes a long time to train, then introducing QAT will further increase this time.", "Quantizing a QAT model is exactly the same (we will use the same quantization configurations) that what we saw in the section above.", "Let\u2019s now compare the performance of the quantized version of the QAT model.", "In the following table, we see that the quantized version of the QAT model indeed performs better than the previous model.", "We clearly see that the model trained with QAT does not cause any accuracy drop. In the next section, we will keep both the models\u2019 parameters as floats to see how far we can push the trade-off between model size and accuracy.", "To quantize our models to float precision, we just need to discard this line \u2014 converter.optimizations = [tf.lite.Optimize.DEFAULT]. This policy is particularly helpful if you were to take advantage of GPU delegates. Note that, float16 quantization is also supported in TensorFlow Lite. In the table below, we can see the size and accuracy of the models quantized using this scheme.", "Although the size of these models has increased, we see the original performance of the models remains high. Note that converting a QAT model using this scheme is not recommended since, during QAT, the fake quantization ops that are inserted are in int precision. So, when we quantize a QAT model using this scheme the converted model can show inconsistencies.", "Additionally, hardware accelerators, like the Edge TPU USB Accelerator, will not support float models.", "There are other post-training quantization techniques available as well, such as full integer quantization, float16 quantization, etc. This is where you can learn more about them. Keep in mind that the full integer quantization scheme might not always be compatible with a QAT model.", "There are a number of SoTA pre-trained TF Lite models hosted for the developers to use for their applications and they can be found here:", "For mobile developers who are looking to integrate machine learning in their applications, there are a number of example applications in TF Lite worth checking out. TensorFlow Lite also provides tooling for embedded systems and microcontrollers and you can learn more about it from here.", "If you\u2019d like to reproduce the results of this analysis, you can \u2013", "Check out the code on GitHub \u2192", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "ML at \ud83e\udd17 | Netflix Nerd | Personal site: https://sayak.dev/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Faebe09f255ca&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-tale-of-model-quantization-in-tf-lite-aebe09f255ca&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-tale-of-model-quantization-in-tf-lite-aebe09f255ca&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-tale-of-model-quantization-in-tf-lite-aebe09f255ca&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-tale-of-model-quantization-in-tf-lite-aebe09f255ca&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----aebe09f255ca--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----aebe09f255ca--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://sayakpaul.medium.com/?source=post_page-----aebe09f255ca--------------------------------", "anchor_text": ""}, {"url": "https://sayakpaul.medium.com/?source=post_page-----aebe09f255ca--------------------------------", "anchor_text": "Sayak Paul"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc7edfd638e74&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-tale-of-model-quantization-in-tf-lite-aebe09f255ca&user=Sayak+Paul&userId=c7edfd638e74&source=post_page-c7edfd638e74----aebe09f255ca---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Faebe09f255ca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-tale-of-model-quantization-in-tf-lite-aebe09f255ca&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Faebe09f255ca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-tale-of-model-quantization-in-tf-lite-aebe09f255ca&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/tagged/production-ml", "anchor_text": "Machine Learning in Production"}, {"url": "https://app.wandb.ai/sayakpaul/tale-of-quantization", "anchor_text": "here"}, {"url": "https://github.com/sayakpaul/Adventures-in-TensorFlow-Lite", "anchor_text": "Check out the code on GitHub"}, {"url": "https://app.wandb.ai/sayakpaul/tale-of-quantization", "anchor_text": "here"}, {"url": "https://www.youtube.com/watch?v=27Zx-4GOQA8", "anchor_text": "TensorFlow Lite: ML for mobile and IoT devices (TF Dev Summit \u201820)"}, {"url": "https://www.tensorflow.org/lite", "anchor_text": "TensorFlow Lite"}, {"url": "https://www.tensorflow.org/model_optimization/", "anchor_text": "TensorFlow Model Optimization Toolkit"}, {"url": "https://colab.research.google.com/", "anchor_text": "Colab"}, {"url": "https://app.wandb.ai/sayakpaul/tale-of-quantization/", "anchor_text": "here"}, {"url": "https://app.wandb.ai/sayakpaul/tale-of-quantization/runs/normal-training-fine-crctd/files?workspace=user-sayakpaul", "anchor_text": "here"}, {"url": "https://www.tensorflow.org/lite/performance/post_training_quantization", "anchor_text": "Here"}, {"url": "https://www.tensorflow.org/api_docs/python/tf/lite/TFLiteConverter", "anchor_text": "here"}, {"url": "https://www.tensorflow.org/lite/performance/post_training_quantization", "anchor_text": "Here"}, {"url": "https://app.wandb.ai/sayakpaul/tale-of-quantization", "anchor_text": "here"}, {"url": "https://www.tensorflow.org/model_optimization/guide/quantization/training_comprehensive_guide", "anchor_text": "Here"}, {"url": "https://app.wandb.ai/sayakpaul/tale-of-quantization", "anchor_text": "here"}, {"url": "https://app.wandb.ai/sayakpaul/tale-of-quantization", "anchor_text": "here"}, {"url": "https://www.tensorflow.org/lite/performance/gpu", "anchor_text": "GPU delegates"}, {"url": "https://www.tensorflow.org/lite/performance/post_training_float16_quant", "anchor_text": "float16 quantization"}, {"url": "https://app.wandb.ai/sayakpaul/tale-of-quantization", "anchor_text": "here"}, {"url": "https://www.tensorflow.org/lite/performance/post_training_quantization", "anchor_text": "This is where"}, {"url": "https://www.tensorflow.org/lite/guide/hosted_models", "anchor_text": "https://www.tensorflow.org/lite/guide/hosted_models"}, {"url": "https://tfhub.dev/s?deployment-format=lite&publisher=tensorflow&q=lite", "anchor_text": "https://tfhub.dev/s?deployment-format=lite&publisher=tensorflow&q=lite"}, {"url": "https://www.tensorflow.org/lite/examples", "anchor_text": "there are a number of example applications"}, {"url": "https://www.tensorflow.org/lite/guide", "anchor_text": "here"}, {"url": "https://github.com/sayakpaul/Adventures-in-TensorFlow-Lite", "anchor_text": "Check out the code on GitHub \u2192"}, {"url": "https://medium.com/tag/production-ml?source=post_page-----aebe09f255ca---------------production_ml-----------------", "anchor_text": "Production Ml"}, {"url": "https://medium.com/tag/tensorflow?source=post_page-----aebe09f255ca---------------tensorflow-----------------", "anchor_text": "TensorFlow"}, {"url": "https://medium.com/tag/keras?source=post_page-----aebe09f255ca---------------keras-----------------", "anchor_text": "Keras"}, {"url": "https://medium.com/tag/model-optimization?source=post_page-----aebe09f255ca---------------model_optimization-----------------", "anchor_text": "Model Optimization"}, {"url": "https://medium.com/tag/wandb?source=post_page-----aebe09f255ca---------------wandb-----------------", "anchor_text": "Wandb"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Faebe09f255ca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-tale-of-model-quantization-in-tf-lite-aebe09f255ca&user=Sayak+Paul&userId=c7edfd638e74&source=-----aebe09f255ca---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Faebe09f255ca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-tale-of-model-quantization-in-tf-lite-aebe09f255ca&user=Sayak+Paul&userId=c7edfd638e74&source=-----aebe09f255ca---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Faebe09f255ca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-tale-of-model-quantization-in-tf-lite-aebe09f255ca&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----aebe09f255ca--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Faebe09f255ca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-tale-of-model-quantization-in-tf-lite-aebe09f255ca&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----aebe09f255ca---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----aebe09f255ca--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----aebe09f255ca--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----aebe09f255ca--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----aebe09f255ca--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----aebe09f255ca--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----aebe09f255ca--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----aebe09f255ca--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----aebe09f255ca--------------------------------", "anchor_text": ""}, {"url": "https://sayakpaul.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://sayakpaul.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Sayak Paul"}, {"url": "https://sayakpaul.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "743 Followers"}, {"url": "https://sayak.dev/", "anchor_text": "https://sayak.dev/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc7edfd638e74&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-tale-of-model-quantization-in-tf-lite-aebe09f255ca&user=Sayak+Paul&userId=c7edfd638e74&source=post_page-c7edfd638e74--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F253c32626149&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-tale-of-model-quantization-in-tf-lite-aebe09f255ca&newsletterV3=c7edfd638e74&newsletterV3Id=253c32626149&user=Sayak+Paul&userId=c7edfd638e74&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}