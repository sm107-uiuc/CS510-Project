{"url": "https://towardsdatascience.com/xlm-cross-lingual-language-model-33c1fd1adf82", "time": 1683013818.9230468, "path": "towardsdatascience.com/xlm-cross-lingual-language-model-33c1fd1adf82/", "webpage": {"metadata": {"title": "XLM: Cross-Lingual Language Model | by Rohan Jagtap | Towards Data Science", "h1": "XLM: Cross-Lingual Language Model", "description": "Models like BERT (Devlin et. al.) or GPT (Radford et. al.) have achieved the state of the art in language understanding. However, these models are pre-trained only on one language. Recently, efforts\u2026"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "Devlin et. al.", "paragraph_index": 0}, {"url": "https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf", "anchor_text": "Radford et. al.", "paragraph_index": 0}, {"url": "https://arxiv.org/abs/1901.07291", "anchor_text": "Cross-lingual Language Model Pretraining", "paragraph_index": 1}, {"url": "https://medium.com/dataseries/openai-gpt-generative-pre-training-for-language-understanding-bbbdb42b7ff4", "anchor_text": "GPT", "paragraph_index": 6}, {"url": "https://medium.com/swlh/openai-gpt-2-language-models-are-multitask-learners-1c6d42d406ae", "anchor_text": "GPT-2", "paragraph_index": 6}, {"url": "https://medium.com/swlh/bert-pre-training-of-transformers-for-language-understanding-5214fba4a9af", "anchor_text": "BERT", "paragraph_index": 8}, {"url": "https://medium.com/dataseries/roberta-robustly-optimized-bert-pretraining-approach-d033464bd946", "anchor_text": "RoBERTa", "paragraph_index": 8}, {"url": "https://ai.googleblog.com/2016/09/a-neural-network-for-machine.html", "anchor_text": "refer to this blog", "paragraph_index": 18}, {"url": "https://github.com/facebookresearch/XLM", "anchor_text": "Here is a link", "paragraph_index": 23}, {"url": "https://huggingface.co/transformers/model_doc/xlm.html", "anchor_text": "Here is a link", "paragraph_index": 24}], "all_paragraphs": ["Models like BERT (Devlin et. al.) or GPT (Radford et. al.) have achieved the state of the art in language understanding. However, these models are pre-trained only on one language. Recently, efforts have been made towards mitigating monolingual representations and building universal cross-lingual models that would be capable of encoding any sentence into a shared embedding space.", "In this article, we will be discussing the paper, Cross-lingual Language Model Pretraining, proposed by Facebook AI. The authors propose 2 approaches for cross-lingual language modeling:", "In this section, we will discuss the approaches proposed for training the XLM.", "The model uses the same shared vocabulary for all the languages. This helps in establishing a common embedding space for tokens from all languages. Hence, it is evident that languages that have the same script (alphabets), or similar words map better to this common embedding space.", "For tokenizing the corpora, Byte-Pair Encoding (BPE) is used.", "This is the regular Language Modeling objective where we maximize the probability of a token x_t to appear at the \u2018t\u2019th position in a given sequence given all the tokens x_<t (all the tokens preceding the \u2018t\u2019th token) in that sequence. i.e.", "OpenAI\u2019s GPT and GPT-2 are trained on this objective. You can refer to my articles on GPT and GPT-2 if you\u2019re interested in the details of this objective.", "This is a type of the Denoising Autoencoding objective, also known as the Cloze task. Here, we maximize the probability of a given masked token x_t to appear at the \u2018t\u2019th position in a given sequence given all the tokens in that sequence, x_hat. i.e.", "BERT and RoBERTa are trained on this objective. You can refer to my articles on BERT and RoBERTa if you\u2019re interested in the details of this objective.", "Note that the only difference between BERT\u2019s and XLM\u2019s approach is that BERT uses pairs of sentences whereas XLM uses streams of an arbitrary number of sentences and truncate once the length is 256.", "The CLM and MLM tasks work well on monolingual corpora, however, they do not take advantage of the available parallel translation data. Hence, the authors propose a Translation Language Modeling objective wherein we take a sequence of parallel sentences from the translation data and randomly mask tokens from the source as well as from the target sentence. For example, in the figure above, we have masked words from English as well as from the French sentence. All the words in the sequence contribute to the prediction of a given masked word, hence establishing a cross-lingual mapping among the tokens.", "In this work, we consider cross-lingual language model pretraining with either CLM, MLM, or MLM used in combination with TLM.", "In this section, we\u2019ll discuss how XLM Pre-training is leveraged for downstream tasks like:", "Just like in any other Transformer-based monolingual model, XLM too, is fine-tuned on the XNLI dataset for obtaining the cross-lingual classification.", "A classification layer is added on top of XLM and it is trained on the English NLI training dataset. Then the model is evaluated on 15 XNLI languages.", "Since the model hasn\u2019t been tuned to classify sentences from any of these languages, it is a zero-shot learning example.", "For this task, the authors propose pre-training a complete encoder-decoder architecture with a cross-lingual language modeling objective. The model is evaluated on several translation benchmarks including WMT\u201914 English-French, WMT\u201916 English-German, and WMT\u201916 English-Romanian.", "Here, the encoder and decoder are loaded with pre-trained weights from XLM and then fine-tuned over the supervised translation dataset. This essentially achieves multi-lingual language translation.", "For more on multi-lingual NMT, refer to this blog.", "Here\u2019s where \u201clanguages with the same script or similar words provide better mapping\u201d comes into the picture. For example, there are 100k sentences written in Nepali on Wikipedia and about 6 times more in Hindi. Moreover, these languages have 80% of tokens in common.", "Hence, a cross-lingual language model will be evidently beneficial for a language model in Nepali as it is trained on relatively more data of similar correspondence.", "Finally, since we have a shared vocabulary, the lookup table (or embedding matrix) of the XLM model gives us the cross-lingual word embeddings.", "In this article, we discussed how a cross-lingual language model is beneficial not only for obtaining better results in generic downstream tasks but also for the fact that it improves the quality of the model for low-resource languages by training on similar high-resource languages, hence getting exposure to more relevant data.", "Here is a link to the original XLM GitHub repository.", "Here is a link to huggingface\u2019s XLM architecture implementation and pre-trained weights.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Immensely interested in AI Research | I read papers and post my notes on Medium"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F33c1fd1adf82&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxlm-cross-lingual-language-model-33c1fd1adf82&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxlm-cross-lingual-language-model-33c1fd1adf82&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxlm-cross-lingual-language-model-33c1fd1adf82&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxlm-cross-lingual-language-model-33c1fd1adf82&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----33c1fd1adf82--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----33c1fd1adf82--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://rojagtap.medium.com/?source=post_page-----33c1fd1adf82--------------------------------", "anchor_text": ""}, {"url": "https://rojagtap.medium.com/?source=post_page-----33c1fd1adf82--------------------------------", "anchor_text": "Rohan Jagtap"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F39646f947a4c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxlm-cross-lingual-language-model-33c1fd1adf82&user=Rohan+Jagtap&userId=39646f947a4c&source=post_page-39646f947a4c----33c1fd1adf82---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F33c1fd1adf82&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxlm-cross-lingual-language-model-33c1fd1adf82&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F33c1fd1adf82&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxlm-cross-lingual-language-model-33c1fd1adf82&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@leookubo?utm_source=medium&utm_medium=referral", "anchor_text": "Leonardo Toshiro Okubo"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "Devlin et. al."}, {"url": "https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf", "anchor_text": "Radford et. al."}, {"url": "https://arxiv.org/abs/1901.07291", "anchor_text": "Cross-lingual Language Model Pretraining"}, {"url": "https://arxiv.org/abs/1906.08237", "anchor_text": "XLNet Paper"}, {"url": "https://medium.com/dataseries/openai-gpt-generative-pre-training-for-language-understanding-bbbdb42b7ff4", "anchor_text": "GPT"}, {"url": "https://medium.com/swlh/openai-gpt-2-language-models-are-multitask-learners-1c6d42d406ae", "anchor_text": "GPT-2"}, {"url": "https://arxiv.org/abs/1901.07291", "anchor_text": "XLM Paper"}, {"url": "https://arxiv.org/abs/1906.08237", "anchor_text": "XLNet Paper"}, {"url": "https://medium.com/swlh/bert-pre-training-of-transformers-for-language-understanding-5214fba4a9af", "anchor_text": "BERT"}, {"url": "https://medium.com/dataseries/roberta-robustly-optimized-bert-pretraining-approach-d033464bd946", "anchor_text": "RoBERTa"}, {"url": "https://arxiv.org/abs/1901.07291", "anchor_text": "XLM Paper"}, {"url": "https://arxiv.org/abs/1901.07291", "anchor_text": "XLM Paper"}, {"url": "https://ai.googleblog.com/2016/09/a-neural-network-for-machine.html", "anchor_text": "refer to this blog"}, {"url": "https://github.com/facebookresearch/XLM", "anchor_text": "Here is a link"}, {"url": "https://huggingface.co/transformers/model_doc/xlm.html", "anchor_text": "Here is a link"}, {"url": "https://arxiv.org/abs/1901.07291", "anchor_text": "Cross-lingual Language Model PretrainingRecent studies have demonstrated the efficiency of generative pretraining for English natural language understanding\u2026arxiv.org"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----33c1fd1adf82---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----33c1fd1adf82---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----33c1fd1adf82---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----33c1fd1adf82---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----33c1fd1adf82---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F33c1fd1adf82&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxlm-cross-lingual-language-model-33c1fd1adf82&user=Rohan+Jagtap&userId=39646f947a4c&source=-----33c1fd1adf82---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F33c1fd1adf82&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxlm-cross-lingual-language-model-33c1fd1adf82&user=Rohan+Jagtap&userId=39646f947a4c&source=-----33c1fd1adf82---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F33c1fd1adf82&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxlm-cross-lingual-language-model-33c1fd1adf82&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----33c1fd1adf82--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F33c1fd1adf82&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxlm-cross-lingual-language-model-33c1fd1adf82&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----33c1fd1adf82---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----33c1fd1adf82--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----33c1fd1adf82--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----33c1fd1adf82--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----33c1fd1adf82--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----33c1fd1adf82--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----33c1fd1adf82--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----33c1fd1adf82--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----33c1fd1adf82--------------------------------", "anchor_text": ""}, {"url": "https://rojagtap.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://rojagtap.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Rohan Jagtap"}, {"url": "https://rojagtap.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "465 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F39646f947a4c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxlm-cross-lingual-language-model-33c1fd1adf82&user=Rohan+Jagtap&userId=39646f947a4c&source=post_page-39646f947a4c--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fe51e2b6202c5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxlm-cross-lingual-language-model-33c1fd1adf82&newsletterV3=39646f947a4c&newsletterV3Id=e51e2b6202c5&user=Rohan+Jagtap&userId=39646f947a4c&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}