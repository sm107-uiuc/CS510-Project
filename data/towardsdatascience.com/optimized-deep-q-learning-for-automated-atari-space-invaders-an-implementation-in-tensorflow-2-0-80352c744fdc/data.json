{"url": "https://towardsdatascience.com/optimized-deep-q-learning-for-automated-atari-space-invaders-an-implementation-in-tensorflow-2-0-80352c744fdc", "time": 1683002427.652411, "path": "towardsdatascience.com/optimized-deep-q-learning-for-automated-atari-space-invaders-an-implementation-in-tensorflow-2-0-80352c744fdc/", "webpage": {"metadata": {"title": "Optimized Space Invaders using Deep Q-learning: An Implementation in Tensorflow 2.0. | by Adrian Yijie Xu | Towards Data Science", "h1": "Optimized Space Invaders using Deep Q-learning: An Implementation in Tensorflow 2.0.", "description": "Over the past few articles on GradientCrescent, we\u2019ve spent a significant period of time exploring the field of online learning, a highly reactive family of reinforcement learning algorithms behind\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-illustrating-online-learning-through-temporal-differences-ec4833b6b06a", "anchor_text": "online learning", "paragraph_index": 0}, {"url": "https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-automating-pong-in-using-a-policy-model-an-implementation-b71f64c158ff", "anchor_text": "sample-based learning", "paragraph_index": 0}, {"url": "https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-understanding-blackjack-strategy-through-monte-carlo-88c9b85194ed", "anchor_text": "offline counterparts", "paragraph_index": 0}, {"url": "https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-navigating-cliffworld-with-sarsa-and-q-learning-cc3c36eb5830", "anchor_text": "theory", "paragraph_index": 1}, {"url": "https://towardsdatascience.com/automating-pac-man-with-deep-q-learning-an-implementation-in-tensorflow-ca08e9891d9c", "anchor_text": "practical implementations", "paragraph_index": 1}, {"url": "https://towardsdatascience.com/automating-pac-man-with-deep-q-learning-an-implementation-in-tensorflow-ca08e9891d9c", "anchor_text": "past articles", "paragraph_index": 1}, {"url": "https://towardsdatascience.com/automating-pac-man-with-deep-q-learning-an-implementation-in-tensorflow-ca08e9891d9c", "anchor_text": "previous implementation of OpenAI\u2019s Miss Pacman gym environment", "paragraph_index": 2}, {"url": "https://danieltakeshi.github.io/2016/11/25/frame-skipping-and-preprocessing-for-deep-q-networks-on-atari-2600-games/", "anchor_text": "Deepmind team", "paragraph_index": 4}, {"url": "https://github.com/EXJUSTICE/GradientCrescent", "anchor_text": "GradientCrescent Github.", "paragraph_index": 6}, {"url": "https://towardsdatascience.com/automating-pac-man-with-deep-q-learning-an-implementation-in-tensorflow-ca08e9891d9c", "anchor_text": "our previous implementation for Miss Pacman.", "paragraph_index": 7}, {"url": "https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-automating-pong-in-using-a-policy-model-an-implementation-b71f64c158ff", "anchor_text": "Pong automation implementation", "paragraph_index": 12}, {"url": "https://medium.com/gradientcrescent", "anchor_text": "GradientCrescent", "paragraph_index": 31}], "all_paragraphs": ["Over the past few articles on GradientCrescent, we\u2019ve spent a significant period of time exploring the field of online learning, a highly reactive family of reinforcement learning algorithms behind many of the latest achievements in general AI. Online learning belongs to the sample-based learning class of approaches, reliant allow for the determination of state values simply through repeated observations, eliminating the need for transition dynamics. Unlike their offline counterparts, online learning approaches allow for the incremental updates of the values of states and actions during an environmental episode, allowing for constant, incremental performance improvements to be observed.", "Beyond Temporal Difference learning (TD), we\u2019ve discussed the theory and practical implementations of Q-learning, an evolution of TD designed to allow for incremental estimations and improvement of state-action values. Q-learning has been made famous as becoming the backbone of reinforcement learning approaches to simulated game environments, such as those observed in OpenAI\u2019s gyms. As we\u2019ve already covered theoretical aspects of Q-learning in past articles, they will not be repeated here.", "In our previous implementation of OpenAI\u2019s Miss Pacman gym environment, we relied on a set of observation instances (states) of individual game frames as the inputs for our training process. However, this method is partially flawed, as it fails to take into account many of the idiosyncrasies of the Atari game environment, including:", "Together, these problems can serve to drastically reduce agent performance, as some instances of data essentially become out-of-domain, or completely irrelevant to the actual game environment. Furthermore, these problems would only become compounded with more complex game environments and real-world applications, such as in autonomous driving. This was observed in our previous implementation as a high level of variance and flat-lining in performance during training.", "To overcome these problems, we can utilize a couple of techniques first introduced by the Deepmind team in 2015.", "Let\u2019s implement examine the effects of these techniques on the more complex Atari Space Invader\u2019s environment.", "Our Google Colaboratory implementation is written in Python utilizing Tensorflow Core, and can be found on the GradientCrescent Github. We\u2019ve converted our code to be TF2 compliant, using the new compat package. To start with, let\u2019s briefly go over the actions required for our Q-learning implementation.", "For reference, let\u2019s first demonstrate the results using the vanilla data input approach, which is essentially the same as that observed in our previous implementation for Miss Pacman. After training our agent for 800 episodes, we observe the following reward distribution.", "Note how the variation in performance exhibits high variation, with a very limited amount of improvement observed after 650 episodes.", "Likewise, the performance of our agent is less than stellar, with almost no evasion behavior being detected. If you look closely, you\u2019ll notice the blinking in both the outgoing and incoming laser trails \u2014 this is an intentional part of the game environment, and results in certain frames having no projectiles in them at all, or only one set of projectiles visible. This means that elements of our input data are highly misleading, and negatively affect agent performance.", "Let\u2019s go over our improved implementation.", "We start by importing all of the necessary packages, including the OpenAI gym environments and Tensorflow core.", "Next, we define a preprocessing function to crop the images from our gym environment and convert them into one-dimensional tensors. We\u2019ve seen this before in our Pong automation implementation.", "Next, let\u2019s initialize the gym environment, and inspect a few screens of gameplay, and also understand the 9 actions available within the gamespace. Naturally, this information is not available to our agent.", "We can take this chance to compare our original and preprocessed input images:", "Next, we introduce input stacking and input composition into our preprocessing pipeline. Upon a new episode, we start off by taking two of our input frames, and returning an element-wise maximum summation maxframe of the two (note that technically this is not necessary, as the two frames are the same, but serves for good practice). The stacked frames are stored in a deque, which automatically removes older entries as new entries are introduced. Initially, we copy the preprocessed maxframe to fill out our deque. As the episode progresses, we create new maxframes by taking the new frame, element-wise maximum summing it with the most recent entry in our deque, and then appending the new maxframe to our deque. We then stack the frames at the very end of the process.", "Next, let\u2019s define our model, a deep Q-network. This is essentially a three layer convolutional network that takes preprocessed input images, flattens and feeds them to a fully-connected layer, and outputs the probabilities of taking each action in the game space. As previously mentioned, there\u2019s no activation layer here, as the presence of one would result in a binary output distribution.", "Let\u2019s also take this chance to define our hyperparameters for our model and training process. Note that the X_shape is now (None, 88, 80, 4), on account of our stacked frames.", "Recall, that Q-learning requires us to select actions with the highest action values. To ensure that we still visit every single possible state-action combination, we\u2019ll have our agent follow an epsilon-greedy policy, with an exploration rate of 5%. We\u2019ll set this exploration rate to decay with time, as we eventually assume all combinations have already been explored \u2014 any exploration after that point would simply result in the forced selection of sub-optimal actions.", "Recall from the equations above, that the update function for Q-learning requires the following:", "To supply these parameters in meaningful quantities, we need to evaluate our current policy following a set of parameters and store all of the variables in a buffer, from which we\u2019ll draw data in minibatches during training. Let\u2019s go ahead and create our buffer and a simple sampling function:", "Next, let\u2019s copy the weight parameters of our original network into a target network. This dual-network approach allows us to generate data during the training process using an existing policy while still optimizing our parameters for the next policy iteration.", "Finally, we\u2019ll also define our loss. This is simply the squared difference of our target action (with the highest action value) and our predicted action. We\u2019ll use an ADAM optimizer to minimize our loss during training.", "With all of our code defined, let\u2019s run our network and go over the training process. We\u2019ve defined most of this in the initial summary, but let\u2019s recall for posterity.", "Once training is complete, we can plot the reward distribution against incremental episodes. The first 800 episodes are shown below:", "Notice how the core variation in reward distribution has decreased significantly, allowing for a much more consistent inter-episode distribution to be observed, and increases in performance to become more statistically significant. A visible increase in performance can be observed from 550 episodes onwards, a full 100 episodes earlier than that of the vanilla data approach, validating our hypothesis.", "To evaluate our results within the confinement of the Colaboratory environment, we can record an entire episode and display it within a virtual display using a wrapped based on the IPython library:", "We then run a new session of our environment using our model and record it.", "Let\u2019s inspect a few rounds of gameplay.", "Our agent has learned to behave both defensively and offensively, utilizing cover and evasion effectively. The drastic difference in behavior between the two episodes can be attributed to how Q-learning works \u2014actions selected earlier on gain a Q-value, which tend to be favoured with an epsilon-greedy policy. With further training, we would expect these two behaviours to converge.", "That wraps up this introduction to optimizing Q-learning. In our next article, we\u2019ll take all we\u2019ve learned and move on from the world of Atari to tackling one of the most well known FPS games in the world.", "We hope you enjoyed this article, and hope you check out the many other articles on GradientCrescent, covering applied and theoretical aspects of AI. To stay up to date with the latest updates on GradientCrescent, please consider following the publication and following our Github repository.", "White et. al, Fundamentals of Reinforcement Learning, University of Alberta", "Silva et. al, Reinforcement Learning, UCL", "Ravichandiran et. al, Hands-On Reinforcement Learning with Python", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F80352c744fdc&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimized-deep-q-learning-for-automated-atari-space-invaders-an-implementation-in-tensorflow-2-0-80352c744fdc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimized-deep-q-learning-for-automated-atari-space-invaders-an-implementation-in-tensorflow-2-0-80352c744fdc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimized-deep-q-learning-for-automated-atari-space-invaders-an-implementation-in-tensorflow-2-0-80352c744fdc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimized-deep-q-learning-for-automated-atari-space-invaders-an-implementation-in-tensorflow-2-0-80352c744fdc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----80352c744fdc--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----80352c744fdc--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@adrianitsaxu?source=post_page-----80352c744fdc--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@adrianitsaxu?source=post_page-----80352c744fdc--------------------------------", "anchor_text": "Adrian Yijie Xu"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc834a59b6354&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimized-deep-q-learning-for-automated-atari-space-invaders-an-implementation-in-tensorflow-2-0-80352c744fdc&user=Adrian+Yijie+Xu&userId=c834a59b6354&source=post_page-c834a59b6354----80352c744fdc---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F80352c744fdc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimized-deep-q-learning-for-automated-atari-space-invaders-an-implementation-in-tensorflow-2-0-80352c744fdc&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F80352c744fdc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimized-deep-q-learning-for-automated-atari-space-invaders-an-implementation-in-tensorflow-2-0-80352c744fdc&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-illustrating-online-learning-through-temporal-differences-ec4833b6b06a", "anchor_text": "online learning"}, {"url": "https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-automating-pong-in-using-a-policy-model-an-implementation-b71f64c158ff", "anchor_text": "sample-based learning"}, {"url": "https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-understanding-blackjack-strategy-through-monte-carlo-88c9b85194ed", "anchor_text": "offline counterparts"}, {"url": "https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-navigating-cliffworld-with-sarsa-and-q-learning-cc3c36eb5830", "anchor_text": "theory"}, {"url": "https://towardsdatascience.com/automating-pac-man-with-deep-q-learning-an-implementation-in-tensorflow-ca08e9891d9c", "anchor_text": "practical implementations"}, {"url": "https://towardsdatascience.com/automating-pac-man-with-deep-q-learning-an-implementation-in-tensorflow-ca08e9891d9c", "anchor_text": "past articles"}, {"url": "https://towardsdatascience.com/automating-pac-man-with-deep-q-learning-an-implementation-in-tensorflow-ca08e9891d9c", "anchor_text": "previous article"}, {"url": "https://towardsdatascience.com/automating-pac-man-with-deep-q-learning-an-implementation-in-tensorflow-ca08e9891d9c", "anchor_text": "previous implementation of OpenAI\u2019s Miss Pacman gym environment"}, {"url": "https://danieltakeshi.github.io/2016/11/25/frame-skipping-and-preprocessing-for-deep-q-networks-on-atari-2600-games/", "anchor_text": "Deepmind team"}, {"url": "https://github.com/EXJUSTICE/GradientCrescent", "anchor_text": "GradientCrescent Github."}, {"url": "https://towardsdatascience.com/automating-pac-man-with-deep-q-learning-an-implementation-in-tensorflow-ca08e9891d9c", "anchor_text": "our previous implementation for Miss Pacman."}, {"url": "https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-automating-pong-in-using-a-policy-model-an-implementation-b71f64c158ff", "anchor_text": "Pong automation implementation"}, {"url": "https://medium.com/gradientcrescent", "anchor_text": "GradientCrescent"}, {"url": "https://danieltakeshi.github.io/2016/11/25/frame-skipping-and-preprocessing-for-deep-q-networks-on-atari-2600-games/", "anchor_text": "Github"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----80352c744fdc---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----80352c744fdc---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----80352c744fdc---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/atari?source=post_page-----80352c744fdc---------------atari-----------------", "anchor_text": "Atari"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----80352c744fdc---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F80352c744fdc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimized-deep-q-learning-for-automated-atari-space-invaders-an-implementation-in-tensorflow-2-0-80352c744fdc&user=Adrian+Yijie+Xu&userId=c834a59b6354&source=-----80352c744fdc---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F80352c744fdc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimized-deep-q-learning-for-automated-atari-space-invaders-an-implementation-in-tensorflow-2-0-80352c744fdc&user=Adrian+Yijie+Xu&userId=c834a59b6354&source=-----80352c744fdc---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F80352c744fdc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimized-deep-q-learning-for-automated-atari-space-invaders-an-implementation-in-tensorflow-2-0-80352c744fdc&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----80352c744fdc--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F80352c744fdc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimized-deep-q-learning-for-automated-atari-space-invaders-an-implementation-in-tensorflow-2-0-80352c744fdc&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----80352c744fdc---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----80352c744fdc--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----80352c744fdc--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----80352c744fdc--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----80352c744fdc--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----80352c744fdc--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----80352c744fdc--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----80352c744fdc--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----80352c744fdc--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@adrianitsaxu?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@adrianitsaxu?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Adrian Yijie Xu"}, {"url": "https://medium.com/@adrianitsaxu/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "604 Followers"}, {"url": "https://github.com/EXJUSTICE/", "anchor_text": "https://github.com/EXJUSTICE/"}, {"url": "https://www.linkedin.com/in/yijie-xu-0174a325/", "anchor_text": "https://www.linkedin.com/in/yijie-xu-0174a325/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc834a59b6354&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimized-deep-q-learning-for-automated-atari-space-invaders-an-implementation-in-tensorflow-2-0-80352c744fdc&user=Adrian+Yijie+Xu&userId=c834a59b6354&source=post_page-c834a59b6354--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F362de3a1de04&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimized-deep-q-learning-for-automated-atari-space-invaders-an-implementation-in-tensorflow-2-0-80352c744fdc&newsletterV3=c834a59b6354&newsletterV3Id=362de3a1de04&user=Adrian+Yijie+Xu&userId=c834a59b6354&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}