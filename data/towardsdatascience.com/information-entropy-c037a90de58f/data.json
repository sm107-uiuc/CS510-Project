{"url": "https://towardsdatascience.com/information-entropy-c037a90de58f", "time": 1682995692.690221, "path": "towardsdatascience.com/information-entropy-c037a90de58f/", "webpage": {"metadata": {"title": "Information Entropy. A layman\u2019s introduction to information\u2026 | by A S | Towards Data Science", "h1": "Information Entropy", "description": "A layman\u2019s introduction to information theory"}, "outgoing_paragraph_urls": [], "all_paragraphs": ["If you were to watch me cross the street, or watch me play Russian roulette, which one would be more exciting? The possibilities are the same- me living or dying, but we can all agree that the crossing of the street is a bit boring, and the Russian roulette\u2026 maybe too exciting. This is partially because we pretty much know what will happen when I cross the street, but we don\u2019t really know what will happen in Russian roulette.", "Another way of looking at this, is to say we gain less information observing the result of crossing the street than we do from Russian roulette. A formal way of putting that is to say the game of Russian roulette has more \u2018entropy\u2019 than crossing the street. Entropy is defined as \u2018lack of order and predictability\u2019, which seems like an apt description of the difference between the two scenarios.", "Information is only useful when it can be stored and/or communicated. We have all learned this lesson the hard way when we have forgotten to save a document we were working on. In a digital form, information is stored in \u2018bits\u2019, or a series of numbers that can either be 0 or 1. The letters in your keyboard are stores in a \u2018byte\u2019, which is 8 bits, which allows for 2\u2078 =256 combinations. It is important to know that information storage and communication are almost the same thing, as you can think of storage as communication with a hard disk.", "The mathematician Claude Shannon had the insight that the more predictable some information is, the less space is required to store it. Crossing the street is more predictable than Russian roulette, therefore you would need to store more information about the game of Russian roulette. Shannon had a mathematical formula for the \u2018entropy\u2019 of a probability distribution, which outputs the minimum number of bits required, on average, to store its outcomes.", "Above is the formula for calculating the entropy of a probability distribution. It involves summing P*log(p) with base 2, for all the possible outcomes in a distribution. Here is a function to do this in Python:", "If we were to quantify the crossing the street example as having a 1 in a billion chance of death, and Russian roulette as 1 in 2, we\u2019d get entropy([1, 999_999_999]) \u2248 3.1*10^-8 bits , and entropy([50,50])=1 bit, respectively. This means that if we repeated both experiments a trillion times, it would take at least 31,000 bits to store the results of crossing the street, and 1 trillion bits to store the results of Russian roulette, in line with our earlier intuition.", "The English language has 26 letters, if you assume each letter has a probability of 1/26 of being next, the language has an entropy of 4.7 bits. However, some letters are more common than other letters, and some letters appear often together, so through clever \u2018guessing\u2019 (i.e. not assigning probabilities of 1/26), we can be much more efficient.", "Random guessing on average takes us 13.5 guesses to get the correct letter. Let us say we are given the first letter of every word in this sentence:", "It would be very bad if it took us 13.5*16=216 guesses to fill in the 16 blanks. It would likely take us less than an average of two guesses per blank to figure out the sentence is \u201cHow are you doing my friend?\u201d. So even if we exhaustively guessed the first letter and it took us 13.5 guesses, it would take us roughly 5.1 guesses/letter to fill in all the blanks, a huge improvement on random guessing.", "Experiments by Shannon showed that English has an entropy between 0.6 and 1.3 bits. To put that into perspective, a 3 sided die has an entropy of 1.58 bits, and takes on average 2 guesses to predict. Also, note that the encoding system on your keyboard uses 8 bits per letter. So it could theoretically make all files in only the English language at least 6 times smaller!", "Shannon\u2019s work found uses in data storage, spaceship communication, and even communication over the internet. . Even if we are not working in any of those fields, \u2018KL divergence\u2019 is an idea derived from Shannon\u2019s work, that is frequently used in data science. It tells you how good one distribution is at estimating another by comparing their entropies.", "Communication and storage of information is what has made humans great, and Shannon\u2019s work revolutionised the way we do so in the digital age.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Maths graduate, former algorithmic trader, aspiring data scientist, played online poker."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fc037a90de58f&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finformation-entropy-c037a90de58f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finformation-entropy-c037a90de58f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finformation-entropy-c037a90de58f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finformation-entropy-c037a90de58f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----c037a90de58f--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c037a90de58f--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@asquant?source=post_page-----c037a90de58f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@asquant?source=post_page-----c037a90de58f--------------------------------", "anchor_text": "A S"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F77c9d1ca99db&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finformation-entropy-c037a90de58f&user=A+S&userId=77c9d1ca99db&source=post_page-77c9d1ca99db----c037a90de58f---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc037a90de58f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finformation-entropy-c037a90de58f&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc037a90de58f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finformation-entropy-c037a90de58f&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/tag/data-science?source=post_page-----c037a90de58f---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/information-theory?source=post_page-----c037a90de58f---------------information_theory-----------------", "anchor_text": "Information Theory"}, {"url": "https://medium.com/tag/uncertainty?source=post_page-----c037a90de58f---------------uncertainty-----------------", "anchor_text": "Uncertainty"}, {"url": "https://medium.com/tag/probability?source=post_page-----c037a90de58f---------------probability-----------------", "anchor_text": "Probability"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc037a90de58f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finformation-entropy-c037a90de58f&user=A+S&userId=77c9d1ca99db&source=-----c037a90de58f---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc037a90de58f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finformation-entropy-c037a90de58f&user=A+S&userId=77c9d1ca99db&source=-----c037a90de58f---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc037a90de58f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finformation-entropy-c037a90de58f&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c037a90de58f--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fc037a90de58f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finformation-entropy-c037a90de58f&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----c037a90de58f---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----c037a90de58f--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----c037a90de58f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----c037a90de58f--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----c037a90de58f--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----c037a90de58f--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----c037a90de58f--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----c037a90de58f--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----c037a90de58f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@asquant?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@asquant?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "A S"}, {"url": "https://medium.com/@asquant/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "112 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F77c9d1ca99db&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finformation-entropy-c037a90de58f&user=A+S&userId=77c9d1ca99db&source=post_page-77c9d1ca99db--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F85d08d9c8f11&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finformation-entropy-c037a90de58f&newsletterV3=77c9d1ca99db&newsletterV3Id=85d08d9c8f11&user=A+S&userId=77c9d1ca99db&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}