{"url": "https://towardsdatascience.com/ols-linear-regression-gauss-markov-blue-and-understanding-the-math-453d7cc630a5", "time": 1683008537.984972, "path": "towardsdatascience.com/ols-linear-regression-gauss-markov-blue-and-understanding-the-math-453d7cc630a5/", "webpage": {"metadata": {"title": "OLS Regression, Gauss-Markov, BLUE, and understanding the math | by Andrew Rothman | Towards Data Science", "h1": "OLS Regression, Gauss-Markov, BLUE, and understanding the math", "description": "For anyone pursuing study in Statistics or Machine Learning, Ordinary Least Squares (OLS) Linear Regression is one of the first and most \u201csimple\u201d methods one is exposed to. Though I have noticed in\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/generalized-least-squares-gls-mathematical-derivations-intuition-2b7466832c2c", "anchor_text": "Generalized Least Squares", "paragraph_index": 22}, {"url": "http://www.linkedin.com/in/andrew-rothman-49739630", "anchor_text": "LinkedIn", "paragraph_index": 24}, {"url": "https://anr248.medium.com/", "anchor_text": "follow me here on Medium", "paragraph_index": 24}], "all_paragraphs": ["For anyone pursuing study in Statistics or Machine Learning, Ordinary Least Squares (OLS) Linear Regression is one of the first and most \u201csimple\u201d methods one is exposed to. Though I have noticed in the field, and on this platform, a good deal of confusion over the OLS estimator. Confusion over what assumptions are \u201crequired\u201d for the valid OLS estimation, and how it relates to other estimators. I\u2019m writing this article to serve as a fairly in-depth mathematically driven explanation of OLS, the Gauss-Markov theorem, and the required assumptions needed to meet different conditions. Note, that in this article I am working from a Frequentist paradigm (as opposed to a Bayesian paradigm), mostly as a matter of convenience. So, let\u2019s jump in:", "You\u2019ve probably at some point been asked the following question:", "I\u2019m now going to make a comment that some (at first) may find controversial or counter-intuitive. My comment is this:", "The question is unanswerable as stated for two main reasons:", "The desired properties above are all different, and require different assumption to hold to fulfill them. And these are but a few desired properties and scenarios we may be interested in, there are certainly more. Therefore, when specifying a statistical model and asking ourselves \u201cwhat assumptions are required\u201d, we need to first answer the question \u201cwhat are we going to use this statistical model for, and what do I need from it?\u201d. Questioning what the \u201crequired assumptions\u201d of a statistical model are without this context will always be a fundamentally ill-posed question.", "We\u2019re going to spend a good deal of time diving into the OLS estimator, learning about it\u2019s properties under different conditions, and how it relates to other estimators. The outline is as follows:", "Before jumping into recovering the OLS estimator itself, let\u2019s talk about the Gauss-Markov Theorem.", "The Gauss-Markov (GM) theorem states that for an additive linear model, and under the \u201dstandard\u201d GM assumptions that the errors are uncorrelated and homoscedastic with expectation value zero, the Ordinary Least Squares (OLS) estimator has the lowest sampling variance within the class of linear unbiased estimators. That may seem like a bit of a mouthful. Let\u2019s start to tease out the above more specifically.", "There are two notes I would like to make about notation:", "The notation we have used thus far, and will use throughout the rest of this article besides this short section, is the \u201cclassical\u201d notation from the Probability Theory and Mathematical Statistics fields. However, this same material on OLS estimation is often taught in introductory Machine Learning (ML) courses through Computer Science departments, and is typically presented using the alternative notation shown directly above in note #2. Careful study of this alternative notation reveals several of the matrix and vector dimensions are transposed as compared to the \u201cclassical\u201d notation, the intercept term lies outside the parameter vector, and the parameter vector and data matrix are switched in the order of their inner product.", "As someone who is trained as a statistician, I\u2019m not a particularly big fan of this alternative notation, nor have I ever fully understood why it exists? Examining the history of this material, it\u2019s not clear to me why the Computer Science arm of the ML community established a \u201cmirrored\u201d and slightly backwards notation to what already existed in the Statistics community for decades? There was already a great deal of work (proofs, theorems, etc) solved using the \u201cclassical\u201d notation prior to ML becoming a field; theorems which are quite useful and valuable in AI and ML. Converting said proofs and theorems to match the alternative notation above is (I feel) a lot of extra work and unnecessarily confusing.", "Anyway, enough of my ranting. I want you the reader to at least be aware of this alternative notation, given those with interests in AI and ML will absolutely come across it. Though, for the rest of this article, we will be using the \u201cclassical\u201d notation.", "Now that we\u2019ve covered the Gauss-Markov Theorem, let\u2019s recover the OLS estimator.", "Note that we solved for the OLS estimator above analytically, given the OLS estimator happens to have a closed form solution. However, when fitting our model to data in practice, we could have alternatively used an iterative numerical technique (like Gradient Descent or Newton-Raphson) to recover empirical estimates of the parameters of the model we specified. If we implemented these numerical techniques correctly and ran them to convergence, they would recover identical parameter estimates to the closed-form solution we solved for above. In a future article, I will give an in-depth overview of iterative numerical techniques for fitting statistical models, so stay tuned.", "Let\u2019s next prove under exactly what conditions the OLS estimator is an unbiased estimator:", "So as we can see above, not all of the \u201cstandard\u201d Gauss-Markov assumptions are required for the OLS estimator to be an unbiased estimator. The error terms do not need be homoscedastic nor uncorrelated. Now, among the universe of all unbiased linear estimators, will the OLS estimator be the unbiased estimator with smallest variance? Not necessarily. That\u2019s something we cover in section #5.", "Let\u2019s next recover the variance of the OLS estimator under the \u201cstandard\u201d GM assumptions:", "Now, what if not all the standard GM assumptions hold? What if the only assumption that holds is the expected value of all the error terms is zero?", "Under the GM assumptions, the OLS estimator is the BLUE (Best Linear Unbiased Estimator). Meaning, if the standard GM assumptions hold, of all linear unbiased estimators possible the OLS estimator is the one with minimum variance and is, therefore, most efficient. Let\u2019s prove this:", "In many introductory statistics courses, it is often (poorly) taught that a required hard assumption for OLS linear regression is that the error terms must be normally distributed and identically and independently distributed (i.i.d.). However, as we have seen in most of this article, we managed to derive quite a bit of theory and proofs for OLS without making any explicit normality or i.i.d. assumptions? So where do these additional assumptions come into play?", "These normality and i.i.d. assumptions are the \u201cbridge\u201d between OLS and Maximum Likelihood Estimation (MLE). If we have a linear additive model, and our n error terms are all normally distributed and are i.i.d., then the Maximum Likelihood Estimator mathematically reduces to the OLS estimator. In other words, these are the unique set of conditions where MLE and OLS become the same thing. I\u2019m not going to show the proof here, but in a future article I plan to go in-depth into Maximum Likelihood Estimation, so stay tuned.", "As a quick wrap-up, the table below provides an overview of some of things we\u2019ve discussed in this article regarding additive linear estimators (desired properties, and the necessary requirements to fulfill them).", "For a primer on methods and analysis when relaxing some of the classic Gauss-Markov Assumptions, please see my piece on Generalized Least Squares.", "I hope this article serves to clear up misconceptions about OLS linear regression, and at a higher level, provides context for why it\u2019s critical to think about the context of the problem you\u2019re working on and what types of properties you need from a model before jumping into \u201crequired assumptions\u201d. Always ask yourself \u201crequired assumptions to do what?\u201d. If you can\u2019t clearly answer the \u201cto do what\u201d part of the question, then you\u2019re not ready to list any required assumptions.", "I hope the above is insightful. As I\u2019ve mentioned in some of my previous pieces, it\u2019s my opinion not enough folks take the time to go through these types of exercises. For me, this type of theory-based insight leaves me more comfortable using methods in practice. A personal goal of mine is to encourage others in the field to take a similar approach. I\u2019m planning on writing based pieces in the future, so feel free to connect with me on LinkedIn, and follow me here on Medium for updates!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Principal Data/ML Scientist @ The Cambridge Group | Harvard trained Statistician and Machine Learning Scientist | Expert in Statistical ML & Causal Inference"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F453d7cc630a5&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fols-linear-regression-gauss-markov-blue-and-understanding-the-math-453d7cc630a5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fols-linear-regression-gauss-markov-blue-and-understanding-the-math-453d7cc630a5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fols-linear-regression-gauss-markov-blue-and-understanding-the-math-453d7cc630a5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fols-linear-regression-gauss-markov-blue-and-understanding-the-math-453d7cc630a5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----453d7cc630a5--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----453d7cc630a5--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://anr248.medium.com/?source=post_page-----453d7cc630a5--------------------------------", "anchor_text": ""}, {"url": "https://anr248.medium.com/?source=post_page-----453d7cc630a5--------------------------------", "anchor_text": "Andrew Rothman"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4688574fc42a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fols-linear-regression-gauss-markov-blue-and-understanding-the-math-453d7cc630a5&user=Andrew+Rothman&userId=4688574fc42a&source=post_page-4688574fc42a----453d7cc630a5---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F453d7cc630a5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fols-linear-regression-gauss-markov-blue-and-understanding-the-math-453d7cc630a5&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F453d7cc630a5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fols-linear-regression-gauss-markov-blue-and-understanding-the-math-453d7cc630a5&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.researchgate.net/figure/Geometry-of-the-Gauss-Markov-theorem_fig1_276081257", "anchor_text": "researchgate.net"}, {"url": "https://towardsdatascience.com/generalized-least-squares-gls-mathematical-derivations-intuition-2b7466832c2c", "anchor_text": "Generalized Least Squares"}, {"url": "http://www.linkedin.com/in/andrew-rothman-49739630", "anchor_text": "LinkedIn"}, {"url": "https://anr248.medium.com/", "anchor_text": "follow me here on Medium"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----453d7cc630a5---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/tag/data-science?source=post_page-----453d7cc630a5---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/math?source=post_page-----453d7cc630a5---------------math-----------------", "anchor_text": "Math"}, {"url": "https://medium.com/tag/statistics?source=post_page-----453d7cc630a5---------------statistics-----------------", "anchor_text": "Statistics"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----453d7cc630a5---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F453d7cc630a5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fols-linear-regression-gauss-markov-blue-and-understanding-the-math-453d7cc630a5&user=Andrew+Rothman&userId=4688574fc42a&source=-----453d7cc630a5---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F453d7cc630a5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fols-linear-regression-gauss-markov-blue-and-understanding-the-math-453d7cc630a5&user=Andrew+Rothman&userId=4688574fc42a&source=-----453d7cc630a5---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F453d7cc630a5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fols-linear-regression-gauss-markov-blue-and-understanding-the-math-453d7cc630a5&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----453d7cc630a5--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F453d7cc630a5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fols-linear-regression-gauss-markov-blue-and-understanding-the-math-453d7cc630a5&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----453d7cc630a5---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----453d7cc630a5--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----453d7cc630a5--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----453d7cc630a5--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----453d7cc630a5--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----453d7cc630a5--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----453d7cc630a5--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----453d7cc630a5--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----453d7cc630a5--------------------------------", "anchor_text": ""}, {"url": "https://anr248.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://anr248.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Andrew Rothman"}, {"url": "https://anr248.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.3K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4688574fc42a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fols-linear-regression-gauss-markov-blue-and-understanding-the-math-453d7cc630a5&user=Andrew+Rothman&userId=4688574fc42a&source=post_page-4688574fc42a--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F9aece5122d3b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fols-linear-regression-gauss-markov-blue-and-understanding-the-math-453d7cc630a5&newsletterV3=4688574fc42a&newsletterV3Id=9aece5122d3b&user=Andrew+Rothman&userId=4688574fc42a&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}