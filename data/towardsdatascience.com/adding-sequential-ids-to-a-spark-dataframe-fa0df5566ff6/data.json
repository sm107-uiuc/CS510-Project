{"url": "https://towardsdatascience.com/adding-sequential-ids-to-a-spark-dataframe-fa0df5566ff6", "time": 1683000715.623851, "path": "towardsdatascience.com/adding-sequential-ids-to-a-spark-dataframe-fa0df5566ff6/", "webpage": {"metadata": {"title": "Adding sequential IDs to a Spark Dataframe | by Maria Karanasou | Towards Data Science", "h1": "Adding sequential IDs to a Spark Dataframe", "description": "Coming from traditional relational databases, like MySQL, and non-distributed data frames, like Pandas, one may be used to working with ids (auto-incremented usually) for identification of course but\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.mysql.com", "anchor_text": "MySQL", "paragraph_index": 1}, {"url": "https://pandas.pydata.org", "anchor_text": "Pandas", "paragraph_index": 1}, {"url": "https://spark.apache.org/docs/latest/rdd-programming-guide.html", "anchor_text": "RDDs", "paragraph_index": 4}, {"url": "https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.row_number", "anchor_text": "row_number", "paragraph_index": 9}, {"url": "http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.monotonically_increasing_id", "anchor_text": "documentation", "paragraph_index": 12}, {"url": "http://buymeacoffee.com/mkaranasou", "anchor_text": "buymeacoffee.com/mkaranasou", "paragraph_index": 27}], "all_paragraphs": ["Adding sequential unique IDs to a Spark Dataframe is not very straight-forward, especially considering the distributed nature of it. You can do this using either zipWithIndex() or row_number() (depending on the amount and kind of your data) but in every case there is a catch regarding performance.", "Coming from traditional relational databases, like MySQL, and non-distributed data frames, like Pandas, one may be used to working with ids (auto-incremented usually) for identification of course but also the ordering and constraints you can have in data by using them as reference. For example, ordering your data by id (which is usually an indexed field) in a descending order, will give you the most recent rows first etc.", "Depending on the needs, we might be found in a position where we would benefit from having a (unique) auto-increment-ids\u2019-like behavior in a spark dataframe. When the data is in one table or dataframe (in one machine), adding ids is pretty straigth-forward. What happens though when you have distributed data, split into partitions that might reside in different machines like in Spark?", "Throughout this post, we will explore the obvious and not so obvious options, what they do, and the catch behind using them.", "One option is to fall back to RDDs", "resilient distributed dataset (RDD), which is a collection of elements partitioned across the nodes of the cluster that can be operated on in parallel", "The ordering is first based on the partition index and then theordering of items within each partition. So the first item inthe first partition gets index 0, and the last item in the lastpartition receives the largest index.", "This method needs to trigger a spark job when this RDD containsmore than one partitions.", "*You cannot really update or add to a dataframe, since they are immutable but you could for example join one with another and end up with a dataframe that has more rows than the original.", "If you can order your data by one of the columns, let\u2019s say column1 in our example, then you can use the row_number() function to provide, well, row numbers:", "row_number() is a windowing function, which means it operates over predefined windows / groups of data.", "If your data is NOT sortable \u2014 or you don\u2019t want to change the current order of your data", "Another option, is to combine row_number() with monotonically_increasing_id(), which according to the documentation creates:", "> A column that generates monotonically increasing 64-bit integers.", "> The generated ID is guaranteed to be monotonically increasing and unique, but not consecutive. The current implementation puts the partition ID in the upper 31 bits, and the record number within each partition in the lower 33 bits. The assumption is that the data frame has less than 1 billion partitions, and each partition has less than 8 billion records.", "The monotonically increasing and unique, but not consecutive is the key here. Which means you can sort by them but you cannot trust them to be sequential. In some cases, where you only need sorting, monotonically_increasing_id() comes in very handy and you don\u2019t need the row_number() at all. But in this case, let\u2019s say we absolutely need to have consequent ids.", "Again, resuming from where we left things in code:", "There are of course different ways (semantically) to go about it. For example, you could use a temp view (which has no obvious advantage other than you can use the pyspark SQL syntax):", "In order to use row_number(), we need to move our data into one partition. The Window in both cases (sortable and not sortable data) consists basically of all the rows we currently have so that the row_number() function can go over them and increment the row number. This can cause performance and memory issues \u2014 we can easily go OOM, depending on how much data and how much memory we have. So, my suggestion would be to really ask yourself if you need an auto-increment/ indexing like behavior for your data or if you can do things another way and avoid this, because it will be expensive. Especially if you process arbitrary amounts of data each time, so careful memory amount consideration cannot be done (e.g. processing streaming data in groups or windows).", "Spark will give you the following warning whenever you use Window without providing a way to partition your data:", "Well, probably not. In my experience, if you find yourself needing this kind of functionality, then you should take a good look at your needs and the transformation process you have and figure out ways around it if possible. Even if you use zipWithIndex() the performance of your application will probably still suffer \u2014 but it seems like a safer option to me.", "But if you cannot avoid it, at least be aware of the mechanism behind it, the risks and plan accordingly.", "I hope this was helpful. Any thoughts, questions, corrections and suggestions are very welcome :)", "The indexes when using row_number() start from 1. To have them start from 0 we can simply deduct 1 from the row_num column:", "This creates (or replaces if that view name already exists) a lazily evaluated \u201cview\u201d of you data, which means that if you don\u2019t cache/ persist it, each time you access the view any calculations will run again. In general, you can then use like a hive table in Spark SQL.", "Understanding your Machine Learning model\u2019s predictions:", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "A mom and a Software Engineer who loves to learn new things & all about ML & Big Data. Buy me a coffee to help me keep going buymeacoffee.com/mkaranasou"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Ffa0df5566ff6&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadding-sequential-ids-to-a-spark-dataframe-fa0df5566ff6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadding-sequential-ids-to-a-spark-dataframe-fa0df5566ff6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadding-sequential-ids-to-a-spark-dataframe-fa0df5566ff6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadding-sequential-ids-to-a-spark-dataframe-fa0df5566ff6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://me.dm/@karanasou", "anchor_text": "Mastodon"}, {"url": "https://towardsdatascience.com/?source=post_page-----fa0df5566ff6--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----fa0df5566ff6--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://karanasou.medium.com/?source=post_page-----fa0df5566ff6--------------------------------", "anchor_text": ""}, {"url": "https://karanasou.medium.com/?source=post_page-----fa0df5566ff6--------------------------------", "anchor_text": "Maria Karanasou"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1570cd236e2d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadding-sequential-ids-to-a-spark-dataframe-fa0df5566ff6&user=Maria+Karanasou&userId=1570cd236e2d&source=post_page-1570cd236e2d----fa0df5566ff6---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffa0df5566ff6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadding-sequential-ids-to-a-spark-dataframe-fa0df5566ff6&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffa0df5566ff6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadding-sequential-ids-to-a-spark-dataframe-fa0df5566ff6&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@markusspiske?utm_source=medium&utm_medium=referral", "anchor_text": "Markus Spiske"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://www.mysql.com", "anchor_text": "MySQL"}, {"url": "https://pandas.pydata.org", "anchor_text": "Pandas"}, {"url": "https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-rdd-partitions.html", "anchor_text": "here"}, {"url": "http://spark.apache.org/docs/latest/quick-start.html", "anchor_text": "PySpark"}, {"url": "https://towardsdatascience.com/explaining-technical-stuff-in-a-non-techincal-way-apache-spark-274d6c9f70e9#b88f-81d3a1ffe447", "anchor_text": "short intro"}, {"url": "https://spark.apache.org/docs/latest/rdd-programming-guide.html", "anchor_text": "RDDs"}, {"url": "https://stackoverflow.com/questions/37088484/whats-the-performance-impact-of-converting-between-dataframe-rdd-and-back", "anchor_text": "can be quite expensive"}, {"url": "https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.row_number", "anchor_text": "row_number"}, {"url": "http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.monotonically_increasing_id", "anchor_text": "documentation"}, {"url": "https://towardsdatascience.com/explaining-technical-stuff-in-a-non-techincal-way-apache-spark-274d6c9f70e9", "anchor_text": "Explaining technical stuff in a non-technical way \u2014 Apache SparkWhat is Spark and PySpark and what can I do with it?towardsdatascience.com"}, {"url": "https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html", "anchor_text": "A Tale of Three Apache Spark APIs: RDDs vs DataFrames and DatasetsIn summation, the choice of when to use RDD or DataFrame and/or Dataset seems obvious. While the former offers you\u2026databricks.com"}, {"url": "https://spark.apache.org/docs/latest/rdd-programming-guide.html", "anchor_text": "RDD Programming GuideSpark 2.4.4 is built and distributed to work with Scala 2.12 by default. (Spark can be built to work with other\u2026spark.apache.org"}, {"url": "https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.createOrReplaceTempView", "anchor_text": "pyspark.sql module - PySpark 2.4.4 documentationschema - a pyspark.sql.types.DataType or a datatype string or a list of column names, default is . The data type string\u2026spark.apache.org"}, {"url": "https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.row_number", "anchor_text": "pyspark.sql module - PySpark 2.4.4 documentationschema - a pyspark.sql.types.DataType or a datatype string or a list of column names, default is . The data type string\u2026spark.apache.org"}, {"url": "https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html", "anchor_text": "Introducing Window Functions in Spark SQLIn this blog post, we introduce the new window function feature that was added in Apache Spark 1.4. Window functions\u2026databricks.com"}, {"url": "https://medium.com/mlearning-ai/machine-learning-interpretability-shapley-values-with-pyspark-16ffd87227e3", "anchor_text": "Machine Learning Interpretability \u2014 Shapley Values with PySparkInterpreting Isolation Forest\u2019s predictions \u2014 and not onlymedium.com"}, {"url": "https://medium.com/tag/big-data?source=post_page-----fa0df5566ff6---------------big_data-----------------", "anchor_text": "Big Data"}, {"url": "https://medium.com/tag/pyspark?source=post_page-----fa0df5566ff6---------------pyspark-----------------", "anchor_text": "Pyspark"}, {"url": "https://medium.com/tag/apache-spark?source=post_page-----fa0df5566ff6---------------apache_spark-----------------", "anchor_text": "Apache Spark"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----fa0df5566ff6---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/tag/python?source=post_page-----fa0df5566ff6---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffa0df5566ff6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadding-sequential-ids-to-a-spark-dataframe-fa0df5566ff6&user=Maria+Karanasou&userId=1570cd236e2d&source=-----fa0df5566ff6---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffa0df5566ff6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadding-sequential-ids-to-a-spark-dataframe-fa0df5566ff6&user=Maria+Karanasou&userId=1570cd236e2d&source=-----fa0df5566ff6---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffa0df5566ff6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadding-sequential-ids-to-a-spark-dataframe-fa0df5566ff6&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----fa0df5566ff6--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Ffa0df5566ff6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadding-sequential-ids-to-a-spark-dataframe-fa0df5566ff6&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----fa0df5566ff6---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----fa0df5566ff6--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----fa0df5566ff6--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----fa0df5566ff6--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----fa0df5566ff6--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----fa0df5566ff6--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----fa0df5566ff6--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----fa0df5566ff6--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----fa0df5566ff6--------------------------------", "anchor_text": ""}, {"url": "https://karanasou.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://karanasou.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Maria Karanasou"}, {"url": "https://karanasou.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.4K Followers"}, {"url": "http://buymeacoffee.com/mkaranasou", "anchor_text": "buymeacoffee.com/mkaranasou"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1570cd236e2d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadding-sequential-ids-to-a-spark-dataframe-fa0df5566ff6&user=Maria+Karanasou&userId=1570cd236e2d&source=post_page-1570cd236e2d--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F399ab963554c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadding-sequential-ids-to-a-spark-dataframe-fa0df5566ff6&newsletterV3=1570cd236e2d&newsletterV3Id=399ab963554c&user=Maria+Karanasou&userId=1570cd236e2d&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}