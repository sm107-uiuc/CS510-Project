{"url": "https://towardsdatascience.com/exploring-simclr-a-simple-framework-for-contrastive-learning-of-visual-representations-158c30601e7e", "time": 1683004149.790719, "path": "towardsdatascience.com/exploring-simclr-a-simple-framework-for-contrastive-learning-of-visual-representations-158c30601e7e/", "webpage": {"metadata": {"title": "Exploring SimCLR: A Simple Framework for Contrastive Learning of Visual Representations | by Thalles Silva | Towards Data Science", "h1": "Exploring SimCLR: A Simple Framework for Contrastive Learning of Visual Representations", "description": "For quite some time now, we know about the benefits of transfer learning in Computer Vision (CV) applications. Nowadays, pre-trained Deep Convolution Neural Networks (DCNNs) are the first go-to\u2026"}, "outgoing_paragraph_urls": [{"url": "https://camelyon17.grand-challenge.org/", "anchor_text": "CAMELYON dataset", "paragraph_index": 9}, {"url": "https://arxiv.org/abs/2002.05709", "anchor_text": "SimCLR", "paragraph_index": 20}, {"url": "https://sthalles.github.io/simple-self-supervised-learning/#1", "anchor_text": "A Simple Framework for Contrastive Learning of Visual Representations", "paragraph_index": 21}, {"url": "https://arxiv.org/abs/1912.01991", "anchor_text": "PIRL", "paragraph_index": 27}, {"url": "https://arxiv.org/abs/1911.05722", "anchor_text": "MOCO", "paragraph_index": 27}, {"url": "https://arxiv.org/abs/1512.03385", "anchor_text": "ResNet-50", "paragraph_index": 29}, {"url": "https://github.com/sthalles/SimCLR", "anchor_text": "implementation here", "paragraph_index": 32}, {"url": "http://ai.stanford.edu/~acoates/stl10/", "anchor_text": "STL-10 dataset", "paragraph_index": 33}, {"url": "https://github.com/sthalles/SimCLR", "anchor_text": "Jupyter Notebook", "paragraph_index": 39}, {"url": "https://sthalles.github.io/", "anchor_text": "https://sthalles.github.io/", "paragraph_index": 44}], "all_paragraphs": ["For quite some time now, we know about the benefits of transfer learning in Computer Vision (CV) applications. Nowadays, pre-trained Deep Convolution Neural Networks (DCNNs) are the first go-to pre-solutions to learn a new task. These large models are trained on huge supervised corpora, like the ImageNet. And most important, their features are known to adapt well to new problems.", "This is particularly interesting when annotated training data is scarce. In situations like this, we take the models\u2019 pre-trained weights, append a new classifier layer on top of it, and retrain the network. This is called transfer learning and is one of the most used techniques in CV. Aside from a few tricks when performing fine-tuning (if the case), it has been shown (many times) that:", "If training for a new task, models initialized with pre-trained weights tend to learn faster and be more accurate then training from scratch using random initialization.", "However, as one might guess, there is a bottleneck in this process. Most of the current transfer learning methods rely on models trained on supervised corpora. But the problem is that annotating data is not cheap.", "If we look around, data, in an unsupervised way, is abundant. Thus, it makes sense to use unlabeled data to learn representations that could be used as a proxy to train better supervised models. In fact, that is a long-standing problem, and as we will see, current research on unsupervised representation learning is finally catching up with supervised methods.", "Unsupervised representation learning is concerned to address the following issue:", "How can we learn good representations from unlabeled data?", "Besides the question of what a good representation is, learning from unlabeled data has great potential. It can unlock a number of applications that current transfer learning hasn\u2019t been able to address. Historically, however, unsupervised representation learning has been a much harder problem than its supervised counterpart.", "As a simple example, let\u2019s consider the task of breast cancer detection. Currently, all the best solutions use ImageNet pre-trained models as a starting point in the optimization process. Interestingly, even though there is a significant difference between breast cancer slide images and regular ImageNet samples, the transfer learning assumptions still hold to some extent.", "To have an idea, most supervised datasets for breast cancer detection, like the CAMELYON dataset, do not compare in size and variability with common Computer Vision supervised datasets. On the other hand, we have a massive number of non-annotated slide images of breast cancer. Thus, if we could learn good representations from the unsupervised (much larger corpora) it would certainly help to learn more specific downstream tasks that have limited annotated data.", "Fortunately, visual unsupervised representation learning has shown great promise. More specifically, visual representations learned using contrastive based techniques are now reaching the same level of those learned via supervised methods \u2014 in some self-supervised benchmarks.", "Let\u2019s explore how unsupervised contrastive learning works and have a closer look at one major work on the area.", "Contrastive methods aim to learn representations by enforcing similar elements to be equal and dissimilar elements to be different.", "In recent months, we have seen an explosion of unsupervised Deep Learning methods based on these principles. In fact, some self-supervised contrastive-based representations already match supervised-based features in linear classification benchmarks.", "The core of contrastive learning is the Noise Contrastive Estimator (NCE) loss.", "In the equation above, you can think of x+ as a data point similar to the input x. In other words, the observations x and x+ are correlated and the pair (x, x+) represents a positive example. Usually, x+ is the result of some transformation on x. This can be a geometric transform aimed to change the size, shape or orientation of x, or any type of data augmentation technique. Some examples include rotation, sheer, resize, cutout and more.", "On the other hand, x- are examples dissimilar to x. The pair (x, x-) form a negative example and they are meant to be uncorrelated. Here, the NCE loss will enforce them to be different from the positive pairs. Note that for each positive pair (x,x+) we have a set of K negatives. Indeed, empirical results have shown that a large number of negatives is required to obtain good representations.", "The sim(.) function is a similarity (distance) metric. It is responsible for minimizing the difference between the positives while maximizing the difference between positive and negatives. Often, sim(.) is defined in terms of dot products or cosine similarities.", "Lastly, g(.) is a convolution neural network encoder. Specifically, recent contrastive learning architectures use siamese networks to learn embeddings for positive and negative examples. These embeddings are then passed as input to the contrastive loss.", "In simple terms, we can think of the contrastive task as trying to identify the positive example among a bunch of negatives.", "SimCLR uses the same principles of contrastive learning described above. In the proposed paper, the method achieves SOTA in self-supervised and semi-supervised learning benchmarks. It introduces a simple framework to learn representations from unlabeled images based on heavy data augmentation. To put it simply, SimCLR uses contrastive learning to maximize agreement between 2 augmented versions of the same image.", "Credits: A Simple Framework for Contrastive Learning of Visual Representations", "To understand SimCLR, let\u2019s explore how it builds on the core components of the contrastive learning framework.", "Given an input image, we create 2 correlated copies of it, by applying 2 separate data augmentation operators. The transformations include (1) random crop and resize, (2) random color distortions, and (3) random Gaussian blur.", "The order of the operations is kept fixed, but since each operation has its own uncertainty, it makes the resulting views visually different. Note that since we apply 2 distinct augmentation functions on the same image, if we sample 5 images, we end up with 2 \u00d7 5 = 10 augmented observations in the batch. See the visual concept below.", "To maximize the number of negatives, the idea is to pair each image (indexed i) in the batch with all other images (indexed j). Note that we avoid pairing an observation i with itself, and with its augmented version. As a result, for each image in the batch, we get 2 \u00d7(N-1) negative pairs \u2014 where N is the batch size.", "Note that the same method is applied to both augmented version of a given observation. This way, the number of negative pairs is increased even more.", "Moreover, by arranging negative samples in this way, SimCLR has the advantage of not needing extra logic to mine negatives. To have an idea, recent implementations like PIRL and MOCO, uses a Memory Bank and a Queue, respectively, to store and sample large batches of negatives.", "In fact, in the original implementation, SimCLR is trained with batch sizes as large as 8192. By following these ideas, this batch size produces 16382 negative examples per positive pair. In addition, the authors also showed that larger batches (hence more negatives) tend to produce better results.", "SimCLR uses ResNet-50 as the main ConvNet backbone. The ResNet receives an augmented image of shape (224,224,3) and outputs a 2048-dimensional embedding vector h. Then, a projection head g(.) is applied to the embedding vector h which produces a final representation z = g(h). The projection head g(.) is a Multilayer Perceptron (MLP) with 2 dense layers. Both layers have 2048 units and the hidden layer has a non-linearity (ReLU) activation function.", "For the similarity function, the authors use the cosine similarity. It measures the cosine of the angle between 2 non-zero vectors in a d-dimensional space. If the angle between 2 vectors is 0 degrees, the cosine similarity is 1. Otherwise, it outputs a number smaller than 1 all the way down to -1. Note that the contrastive learning loss operates on the latent space mapped by the projection head g(.) \u2014 the z embedding vectors.", "Once the system is trained, we can through away the projection head g(.) and use the representations h (straight from the ResNet) to learn new downstream tasks.", "Once the components of the contrastive learning objective are in place, training the system is straight forward. You can have a look at my implementation here.", "To train the model, I used the STL-10 dataset. It contains 10 different classes with a reasonable small number of observations per class. Most importantly, it contains a larger unsupervised set with 100000 unlabeled images \u2014 that is the bulk of images used for training.", "For this implementation, I used a ResNet-18 as the ConvNet backbone. It receives images of shape (96,96,3), regular STL-10 dimensions, and outputs vector representations of size 512. The projection head g(.) has 2 fully-connected layers. Each layer has 512 units and produces the final 64-dimensional feature representation z.", "To train SimCLR, I took the train + unlabeled portions of the dataset \u2014 that gives a total of 105000 images.", "After training, we need a way to evaluate the quality of the representations learned by SimCLR. One standard way is to use a linear evaluation protocol.", "The idea is to train linear classifiers on fixed representations from the SimCLR encoder. To do that, we take the training data, pass it through the pre-trained SimCLR model, and store the output representations. Note that at this point, we do not need the projection head g(.) anymore.", "These fixed representations are then used to train a Logistic Regression model using the training labels as targets. Then, we can measure the testing accuracy, and use it as a measure of feature quality.", "This Jupyter Notebook shows the evaluation protocol. Using the SimCLR fixed representations as training signals, we reach a test accuracy of 64%. To have an idea, performing PCA on the training data and keeping the most important principal components, we get a test accuracy of only 36%. This emphasizes the quality of the features leaned by SimCLR.", "The original SimCLR paper also provides other interesting results. These include:", "I encourage you to have a look at the paper for more details.", "-He, Kaiming, et al. \u201cDeep residual learning for image recognition.\u201d Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Computer Vision & Deep Learning. Personal blog: https://sthalles.github.io/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F158c30601e7e&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexploring-simclr-a-simple-framework-for-contrastive-learning-of-visual-representations-158c30601e7e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexploring-simclr-a-simple-framework-for-contrastive-learning-of-visual-representations-158c30601e7e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexploring-simclr-a-simple-framework-for-contrastive-learning-of-visual-representations-158c30601e7e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexploring-simclr-a-simple-framework-for-contrastive-learning-of-visual-representations-158c30601e7e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----158c30601e7e--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----158c30601e7e--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@thalles.silva?source=post_page-----158c30601e7e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@thalles.silva?source=post_page-----158c30601e7e--------------------------------", "anchor_text": "Thalles Silva"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff8db098eb9ca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexploring-simclr-a-simple-framework-for-contrastive-learning-of-visual-representations-158c30601e7e&user=Thalles+Silva&userId=f8db098eb9ca&source=post_page-f8db098eb9ca----158c30601e7e---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F158c30601e7e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexploring-simclr-a-simple-framework-for-contrastive-learning-of-visual-representations-158c30601e7e&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F158c30601e7e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexploring-simclr-a-simple-framework-for-contrastive-learning-of-visual-representations-158c30601e7e&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://github.com/sthalles/SimCLR", "anchor_text": "PyTorch Implementation of SimCLR"}, {"url": "https://camelyon17.grand-challenge.org/", "anchor_text": "CAMELYON dataset"}, {"url": "https://arxiv.org/abs/2002.05709", "anchor_text": "SimCLR"}, {"url": "https://sthalles.github.io/simple-self-supervised-learning/#1", "anchor_text": "A Simple Framework for Contrastive Learning of Visual Representations"}, {"url": "https://arxiv.org/abs/1912.01991", "anchor_text": "PIRL"}, {"url": "https://arxiv.org/abs/1911.05722", "anchor_text": "MOCO"}, {"url": "https://arxiv.org/abs/1512.03385", "anchor_text": "ResNet-50"}, {"url": "https://github.com/sthalles/SimCLR", "anchor_text": "implementation here"}, {"url": "http://ai.stanford.edu/~acoates/stl10/", "anchor_text": "STL-10 dataset"}, {"url": "https://github.com/sthalles/SimCLR", "anchor_text": "Jupyter Notebook"}, {"url": "https://sthalles.github.io/simple-self-supervised-learning/", "anchor_text": "https://sthalles.github.io"}, {"url": "https://medium.com/tag/unsupervised-learning?source=post_page-----158c30601e7e---------------unsupervised_learning-----------------", "anchor_text": "Unsupervised Learning"}, {"url": "https://medium.com/tag/self-supervised-learning?source=post_page-----158c30601e7e---------------self_supervised_learning-----------------", "anchor_text": "Self Supervised Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----158c30601e7e---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/representation-learning?source=post_page-----158c30601e7e---------------representation_learning-----------------", "anchor_text": "Representation Learning"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----158c30601e7e---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F158c30601e7e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexploring-simclr-a-simple-framework-for-contrastive-learning-of-visual-representations-158c30601e7e&user=Thalles+Silva&userId=f8db098eb9ca&source=-----158c30601e7e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F158c30601e7e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexploring-simclr-a-simple-framework-for-contrastive-learning-of-visual-representations-158c30601e7e&user=Thalles+Silva&userId=f8db098eb9ca&source=-----158c30601e7e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F158c30601e7e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexploring-simclr-a-simple-framework-for-contrastive-learning-of-visual-representations-158c30601e7e&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----158c30601e7e--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F158c30601e7e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexploring-simclr-a-simple-framework-for-contrastive-learning-of-visual-representations-158c30601e7e&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----158c30601e7e---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----158c30601e7e--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----158c30601e7e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----158c30601e7e--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----158c30601e7e--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----158c30601e7e--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----158c30601e7e--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----158c30601e7e--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----158c30601e7e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@thalles.silva?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@thalles.silva?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Thalles Silva"}, {"url": "https://medium.com/@thalles.silva/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.4K Followers"}, {"url": "https://sthalles.github.io/", "anchor_text": "https://sthalles.github.io/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff8db098eb9ca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexploring-simclr-a-simple-framework-for-contrastive-learning-of-visual-representations-158c30601e7e&user=Thalles+Silva&userId=f8db098eb9ca&source=post_page-f8db098eb9ca--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fea9a35433442&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fexploring-simclr-a-simple-framework-for-contrastive-learning-of-visual-representations-158c30601e7e&newsletterV3=f8db098eb9ca&newsletterV3Id=ea9a35433442&user=Thalles+Silva&userId=f8db098eb9ca&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}