{"url": "https://towardsdatascience.com/the-official-definition-of-degrees-of-freedom-in-regression-faa04fd3c610", "time": 1683012908.9723341, "path": "towardsdatascience.com/the-official-definition-of-degrees-of-freedom-in-regression-faa04fd3c610/", "webpage": {"metadata": {"title": "The Official Definition of Degrees of Freedom in Regression | by Ravi Charan | Towards Data Science", "h1": "The Official Definition of Degrees of Freedom in Regression", "description": "Degrees of Freedom is rarely defined in many contexts. We define it for regression as the effective number of parameters and compute it for Ridge Regression"}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)", "anchor_text": "wikipedia", "paragraph_index": 2}, {"url": "https://en.wikipedia.org/wiki/Trace_(linear_algebra)", "anchor_text": "Trace", "paragraph_index": 18}, {"url": "https://en.wikipedia.org/wiki/Singular_value_decomposition", "anchor_text": "singular value decomposition", "paragraph_index": 21}, {"url": "https://towardsdatascience.com/the-singular-value-decomposition-without-algebra-ae10147aab4c", "anchor_text": "earlier article", "paragraph_index": 21}, {"url": "https://en.wikipedia.org/wiki/Akaike_information_criterion", "anchor_text": "AIC", "paragraph_index": 38}, {"url": "https://medium.com/@rmcharan/regression-geometry-61fdd5515ab7", "anchor_text": "linear regression", "paragraph_index": 42}, {"url": "https://towardsdatascience.com/the-singular-value-decomposition-without-algebra-ae10147aab4c", "anchor_text": "singular value decomposition", "paragraph_index": 42}, {"url": "https://web.stanford.edu/~hastie/ElemStatLearn/", "anchor_text": "Elements of Statistical Learning", "paragraph_index": 43}], "all_paragraphs": ["Back in middle and high school you likely learned to calculate the mean and standard deviation of a dataset. And your teacher probably told you that there are two kinds of standard deviation: population and sample. The formulas for the two are just small variations on one another:", "where \u03bc is the population mean and x-bar is the sample mean. Typically, one just learns the formulas and is told when to use them. If you ask why, the answer is something vague like \u201cthere was one degree of freedom used up when estimating the sample mean.\u201d without a true definition of a \u201cdegree of freedom.\u201d", "Degrees of freedom also show up in several other places in statistics, for example: when doing t-tests, F-tests, \u03c7\u00b2 tests, and generally studying regression problems. Depending on the circumstance, degrees of freedom can mean subtly different things (the wikipedia article lists at least 9 closely-related definitions by my count\u00b9).", "In this article, we\u2019ll focus on the meaning of degrees of freedom in a regression context. Specifically we\u2019ll use the sense in which \u201cdegrees of freedom\u201d is the \u201ceffective number of parameters\u201d for a model. We\u2019ll see how to compute the number of degrees of freedom of the standard deviation problem above alongside linear regression, ridge regression, and k-nearest neighbors regression. As we go we\u2019ll also briefly discuss the relation to statistical inference (like a t-test) and model selection (how to compare two different models using their effective degrees of freedom).", "In the regression context we have N samples each with a real-valued outcome value y. For each sample, we have a vector of covariates x, usually taken to include a constant. In other words, the first entry of the x-vector is 1 for each sample. We have some sort of model or procedure (which could be parametric or non-parametric) that is fit to the data (or otherwise uses the data) to produce predictions about what we think the value of y should be given an x-vector (which could be out-of-sample or not).", "The result is the predicted value, y-hat, for each of the N samples. We\u2019ll define the degrees of freedom, which we denote as \u03bd (nu):", "And we\u2019ll interpret the degrees of freedom as the \u201ceffective number of parameters\u201d of the model. Now let\u2019s see some examples.", "Let\u2019s return to the school-age problem we started with. Computing the mean of a sample is just making the prediction that every data point has value equal to the mean (after all, that\u2019s the best guess you can make under the circumstances). In other words:", "Note that estimating the mean is equivalent to running a linear regression with only one covariate, a constant: x = [1]. Hopefully this makes it clear why we can re-cast the problem as a prediction problem.", "Now it\u2019s simple to compute the degrees of freedom. Unsurprisingly we get 1 degree of freedom:", "To understand the relationship to the standard deviation, we have to use another closely related definition of degrees of freedom (which we won\u2019t go into depth on). If our samples were independent and identically distributed, then we can say, informally, that we started out with N degrees of freedom. We lost one in estimating the mean, leaving N\u20131 left over for the standard deviation.", "Now let\u2019s expand this into the context of regular old linear regression. In this context, we like to collect the sample data into a vector Y and matrix X. Throughout this article we will use p to denote the number of covariates for each sample (the length of the x-vector).", "It shouldn\u2019t come as a spoiler that the number of degrees of freedom will end up being p. But the method used to calculate this will pay off for us when we turn to Ridge Regression.", "The count of p covariates includes a constant if we include one in our model as we usually do. Each row in the X matrix corresponds to the x-vector for each observation in our sample:", "The model is that there are p parameters collected into a vector \u03b2. Y = X\u03b2 plus an error term. We\u2019ll go through the derivation because it will be useful for us later. We pick an estimate for \u03b2 that minimizes the sum of squares of the errors. In other words our loss function is", "The first sum is in terms of each sample with row-vectors x labeled by i. To optimize L, we differentiate with respect to the vector \u03b2, obtaining a p\u2a091 vector of derivatives.", "Set it equal to 0 and solve for \u03b2", "We call the matrix H for \u201chat matrix\u201d because it \u201cputs the hat\u201d on the Y (producing our fitted/predicted values). The hat matrix is an N\u2a09N matrix. We are assuming that the y\u2019s are independent, so we can compute the effective degrees of freedom:", "where the second sum is over the diagonal terms in the matrix. If you write out the matrix and write out the formula for the predicted value of sample 1, you will see that these derivatives are in fact just the diagonal entries of the hat matrix. The sum of the diagonals of a matrix is called the Trace of matrix and we have denoted that in the second line.", "Now we turn to computing the trace of H. We had better hope it is p!", "There is a simple way to compute the trace of H using the cyclicality of the trace. But we\u2019ll take another approach that will be generalized when we discuss ridge regression.", "We use the singular value decomposition of X. (See my earlier article for a geometric explanation of the singular value decomposition and the linear algebra we are about to do). The trace of a matrix is a basis-independent number, so we can choose whatever basis we want for the vector space containing Y. Similarly, we can choose whatever basis we want for the vector space containing the parameters \u03b2. The singular value decomposition says that there exists a basis for each such that the matrix X is diagonal. The entries on the diagonal in the first p rows are called the singular values. The \u201cno perfect multi-collinearity\u201d assumption for linear regression means that none of the singular values are 0. The remaining N\u2013p rows of the matrix are all full of 0s.", "Now it\u2019s easy to compute H. You can just multiply the versions of X by hand and get a diagonal matrix with the first p diagonal entries all 1 and the rest 0. The entries not shown (the off-diagonal ones) are all 0 as well.", "In our previous example (mean and standard deviation), we computed the standard deviation after computing the mean, using n\u20131 for the denominator because we \u201clost 1 degree of freedom\u201d to estimate the mean.", "In this context, the standard deviation gets renamed as the \u201cstandard error\u201d but the formula should look analogous:", "Just as before, we compare the sum of squares of the difference between each measured value y and its predicted value. We used up p degrees of freedom to compute the estimate, so only N\u2013p are left.", "In Ridge Regression, we add a regularization term to our loss function. Done properly, this increases bias in our coefficient but decreases variance to result in overall lower error in our predictions.", "Using our definition of degrees of freedom, we can compute the effective number of parameters in a ridge regression. We would expect the regularization to decrease this below the original number p of parameters (since they no longer freely vary).", "We go through the same steps to compute the hat matrix as in linear regression.", "2. We take the derivative, set it equal to 0, and solve for \u03b2. I is the identity matrix here", "3. We compute the fitted values and extract the hat matrix H. The formula is the same as last time except that we add \u03bb to each diagonal entry of the matrix in parentheses.", "4. We use the singular value decomposition to choose bases for the vector spaces containing Y and \u03b2 so that we can write X as a diagonal matrix, and compute H.", "Which leaves us with the following formulas for the degrees of freedom of regular (\u03bb = 0) regression and Ridge regression (\u03bb>0) in terms of the singular values d, indexed by i.", "The above calculations using the singular value decomposition give us a good perspective on Ridge Regression.", "First of all, if the design matrix is perfectly (multi-)collinear, one of its singular values will be 0. A common case where this happens is if there are more covariates than samples. This is a problem in a regular regression because it means the term in parentheses in the hat matrix isn\u2019t invertible (the denominators are 0 in the formula above). Ridge regression fixes this problem by adding a positive term to each squared singular value.", "Second, we can see that the coefficient shrinkage is high for terms with a small singular value. Such terms correspond to components of the \u03b2 estimate that have high variance in a regular regression (due to high correlation between regressor). On the other hand, for terms with a higher singular value, the shrinkage is comparatively smaller.", "The degrees of freedom calculation we have done perfectly encapsulates this shrinkage to give us an estimate for the effective number of parameters we actually used. Note also that the singular values are a function of the design matrix X and not of Y. That means that you could, in theory, choose \u03bb by computing the number of effective parameters you want and finding \u03bb to achieve that.", "In our vanilla regression examples we saw that the standard error (or standard deviation) can be computed by assuming that we started with N degrees of freedom and subtracting out the number of effective parameters we used. This doesn\u2019t necessarily make as much sense with the Ridge, which gives a biased estimator for the coefficients (albeit with lower mean-squared error for correctly chosen \u03bb). In particular, the residuals are no longer nicely distributed.", "Instead, we can use our effective number of parameters to plug into the AIC (Akaike Information Criterion), an alternative to cross-validation for model selection. The AIC penalizes models for having more parameters and approximates the expected test error if we were to use a held-out test set. Then choosing \u03bb to optimize it can replace cross-validation, provided we use the effective degrees of freedom in the formula for the AIC. Note, however, that if we choose \u03bb adaptively before computing the AIC, then there are extra effective degrees of freedom added.", "As a final example, consider k-nearest neighbors regression. It should be apparent that the fitted value for each data point is the average of k nearby points, including itself. This means that the degrees of freedom is", "This enables us to do model comparison between different types of models (for example, comparing k-nearest neighbors to a ridge regression using the AIC as above).", "I hope you see that the degrees of freedom is a very general measure and can be applied to all sorts of regression models (kernel regression, splines, etc.).", "Hopefully this article will give you a more solid understanding of degrees of freedom and make the whole concept less of a vague statistical idea. I mentioned that there are other, closely related, definitions of degrees of freedom. The other main version is a geometric idea. If you want to know more about that, read my article about the geometric approach to linear regression. If you want to understand more of the algebra we did to compute the degrees of freedom, read a non-algebraic approach to the singular value decomposition.", "You can find a discussion of this material in the Elements of Statistical Learning (Hastie, Tibshirani, and Friedman 2009). Section 7.4 covers optimism, which is the relationship between what we have discussed and model selection. Section 3.4 covers ridge regression and the computations we did there as well discussing degrees of freedom.", "[1] Specifically I\u2019m counting (a) the geometric definition for a random vector; (b) the closely related degrees of freedom of probability distributions; (c) 4 formulas for the regression degrees of freedom; and (d) 3 formulas for the residual degrees of freedom. You might count differently but will hopefully see my point that there are several closely related ideas and formulas with subtle differences.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Scientist, Mathematician. Formerly @MIT, @McKinsey, currently teaching computers to read"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Ffaa04fd3c610&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-official-definition-of-degrees-of-freedom-in-regression-faa04fd3c610&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-official-definition-of-degrees-of-freedom-in-regression-faa04fd3c610&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-official-definition-of-degrees-of-freedom-in-regression-faa04fd3c610&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-official-definition-of-degrees-of-freedom-in-regression-faa04fd3c610&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----faa04fd3c610--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----faa04fd3c610--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@rmcharan?source=post_page-----faa04fd3c610--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@rmcharan?source=post_page-----faa04fd3c610--------------------------------", "anchor_text": "Ravi Charan"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F393ce2bbf82c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-official-definition-of-degrees-of-freedom-in-regression-faa04fd3c610&user=Ravi+Charan&userId=393ce2bbf82c&source=post_page-393ce2bbf82c----faa04fd3c610---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffaa04fd3c610&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-official-definition-of-degrees-of-freedom-in-regression-faa04fd3c610&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffaa04fd3c610&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-official-definition-of-degrees-of-freedom-in-regression-faa04fd3c610&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@insolitus?utm_source=medium&utm_medium=referral", "anchor_text": "Rowan Heuvel"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)", "anchor_text": "wikipedia"}, {"url": "https://en.wikipedia.org/wiki/Trace_(linear_algebra)", "anchor_text": "Trace"}, {"url": "https://en.wikipedia.org/wiki/Singular_value_decomposition", "anchor_text": "singular value decomposition"}, {"url": "https://towardsdatascience.com/the-singular-value-decomposition-without-algebra-ae10147aab4c", "anchor_text": "earlier article"}, {"url": "https://en.wikipedia.org/wiki/Akaike_information_criterion", "anchor_text": "AIC"}, {"url": "https://medium.com/@rmcharan/regression-geometry-61fdd5515ab7", "anchor_text": "linear regression"}, {"url": "https://towardsdatascience.com/the-singular-value-decomposition-without-algebra-ae10147aab4c", "anchor_text": "singular value decomposition"}, {"url": "https://web.stanford.edu/~hastie/ElemStatLearn/", "anchor_text": "Elements of Statistical Learning"}, {"url": "https://medium.com/tag/editors-pick?source=post_page-----faa04fd3c610---------------editors_pick-----------------", "anchor_text": "Editors Pick"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----faa04fd3c610---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----faa04fd3c610---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----faa04fd3c610---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----faa04fd3c610---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffaa04fd3c610&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-official-definition-of-degrees-of-freedom-in-regression-faa04fd3c610&user=Ravi+Charan&userId=393ce2bbf82c&source=-----faa04fd3c610---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffaa04fd3c610&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-official-definition-of-degrees-of-freedom-in-regression-faa04fd3c610&user=Ravi+Charan&userId=393ce2bbf82c&source=-----faa04fd3c610---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffaa04fd3c610&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-official-definition-of-degrees-of-freedom-in-regression-faa04fd3c610&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----faa04fd3c610--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Ffaa04fd3c610&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-official-definition-of-degrees-of-freedom-in-regression-faa04fd3c610&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----faa04fd3c610---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----faa04fd3c610--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----faa04fd3c610--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----faa04fd3c610--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----faa04fd3c610--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----faa04fd3c610--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----faa04fd3c610--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----faa04fd3c610--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----faa04fd3c610--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@rmcharan?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@rmcharan?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ravi Charan"}, {"url": "https://medium.com/@rmcharan/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "599 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F393ce2bbf82c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-official-definition-of-degrees-of-freedom-in-regression-faa04fd3c610&user=Ravi+Charan&userId=393ce2bbf82c&source=post_page-393ce2bbf82c--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F6bca6dd641ca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-official-definition-of-degrees-of-freedom-in-regression-faa04fd3c610&newsletterV3=393ce2bbf82c&newsletterV3Id=6bca6dd641ca&user=Ravi+Charan&userId=393ce2bbf82c&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}