{"url": "https://towardsdatascience.com/basic-nlp-on-the-texts-of-harry-potter-topic-modeling-with-latent-dirichlet-allocation-f3c00f77b0f5", "time": 1682994108.9700422, "path": "towardsdatascience.com/basic-nlp-on-the-texts-of-harry-potter-topic-modeling-with-latent-dirichlet-allocation-f3c00f77b0f5/", "webpage": {"metadata": {"title": "LDA on the Texts of Harry Potter. Topic Modeling with Latent Dirichlet\u2026 | by Greg Rafferty | Towards Data Science", "h1": "LDA on the Texts of Harry Potter", "description": "In this post, I\u2019ll describe topic modeling with Latent Dirichlet Allocation and compare different algorithms for it, through the lens of Harry Potter."}, "outgoing_paragraph_urls": [{"url": "https://github.com/raffg/harry_potter_nlp", "anchor_text": "github", "paragraph_index": 0}, {"url": "https://github.com/raffg/basic_nlp", "anchor_text": "Github repo", "paragraph_index": 13}, {"url": "https://radimrehurek.com/gensim/models/ldamodel.html", "anchor_text": "ldamodel", "paragraph_index": 16}, {"url": "https://radimrehurek.com/gensim/models/ldamallet.html", "anchor_text": "Latent Dirichlet Allocation via Mallet", "paragraph_index": 17}, {"url": "http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip", "anchor_text": "http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip", "paragraph_index": 17}], "all_paragraphs": ["I\u2019m Greg Rafferty, a data scientist in the Bay Area. You can check out the code for this project on my github. Feel free to contact me with any questions!", "In this post, I\u2019ll describe topic modeling with Latent Dirichlet Allocation and compare different algorithms for it, through the lens of Harry Potter. Upcoming posts will cover two more NLP tasks: text summarization and sentiment analysis.", "Recently, I was put on a new project with a team who were unanimously shocked and disappointed that I\u2019d never read nor seen the movies about a certain fictional wizard named Harry Potter. In order to fit in with the team, and obviously save my career from an early end, it quickly became evident that I was going to have to take a crash-course in the happenings at Hogwarts. Armed with my ebook reader and seven shiny new pdf files, I settled in to see what the fuss was all about. Meanwhile, I had started working on a side project composed of a bunch of unrelated NLP tasks. I needed a good sized set of text documents and I thought all of these shiny new pdfs would be a great source.", "And somewhere around the middle of the third book, I suddenly realized that LDA was basically just an algorithmic Sorting Hat.", "LDA, or Latent Dirichlet Allocation, is a generative probabilistic model of (in NLP terms) a corpus of documents made up of words and/or phrases. The model consists of two tables; the first table is the probability of selecting a particular word in the corpus when sampling from a particular topic, and the second table is the probability of selecting a particular topic when sampling from a particular document.", "Here\u2019s an example. Let\u2019s say I\u2019ve got these three (rather non-sensical) documents:", "Here\u2019s the term-frequency matrix for these documents:", "Just from glancing at this, it seems pretty obvious that document 0 is mostly about Harry, a little bit about magic, and partly about wand. Document 1 is also a little bit about magic, but mostly about Hermione and robe. And document 2 is again partly about magic, but mostly about Malfoy and spell. Document 3 is equally about Harry, Hermione, and Malfoy. It\u2019s usually not so easy to see this because a more practical corpus would consists of thousands or ten-of-thousands of words, so let\u2019s see what the LDA algorithm chooses for topics:", "And that\u2019s roughly what we predicted just by going with term frequencies and our gut. The number of topics is a hyperparameter you\u2019ll need to choose and tune carefully, and I\u2019ll go into that later, but for this example I chose 4 topics to make my point. The upper table shows words versus topics and the lower table shows documents versus topics. Each column in the upper table and each row in the lower table must sum to 1. These tables are telling us that if we were to randomly sample a word from Topic 0, there\u2019s a 70.9% chance we\u2019d grab \u201cHarry\u201d. If we chose a word from Topic 3, it\u2019s near certain that we\u2019d pick \u201cmagic\u201d. If we sampled Document 3, there\u2019s an equal chance that we would pick Topic 0, 1, or 2.", "It\u2019s up to us as smart humans who can infer meaning from a bag of words to name these topics. In these examples with a very limited vocabulary, the topics quite obviously correspond to single words. If we had run LDA on, say, a couple thousand restaurant descriptions, we might find topics corresponding to cuisine type or atmosphere. It\u2019s important to note that LDA, unlike typical clustering algorithms such as Kmeans, allows a document to exist in multiple topics. So in those restaurant descriptions, we might find one restaurant placed in the \u201cItalian\u201d, \u201cdate-night\u201d, and \u201ccafe\u201d topics.", "So how is all of this like the Sorting Hat? All new students at Hogwarts go through a ceremony when they arrive on day one to determine which house they\u2019ll be in (I\u2019m probably the only person who didn\u2019t know this up until a few weeks ago). The Sorting Hat, once placed on someone\u2019s head, understands what is in their thoughts, dreams, and experiences. This is a bit like LDA building the term-frequency matrix and understanding what words and N-grams are contained within each document.", "The Sorting Hat then compares the student\u2019s attributes with the attributes of the various houses (bravery goes to Gryffindor, loyalty to Hufflepuff, wisdom to Ravenclaw, and sneaky, shifty sleazeballs go to Slytherin (ok, just a quick aside \u2014 can ANYONE explain to me why Slytherin has persisted for the thousand-year history of this school? It\u2019s like that one fraternity which finds itself in yet another ridiculously obscene scandal every damn year!)). This is where LDA creates the word-topic table and begins to associate the attributes of the topics.", "Harry\u2019s placement was notably split between Gryffindor and Slytherin due to his combination of courage, intelligence, talent, and ambition, but Gryffindor just slightly edged out ahead and Harry Potter became the hero of an entire generation of young millennials instead of its villain. This is where LDA creates the document-topic table and finally determines which is the dominant topic for each document.", "OK, so now that we know roughly what LDA does, let\u2019s look at two different implementations in Python. Check out my Github repo for all of the nitty-gritty details.", "First of all, one of the best ways to determine how many topics you should model is with an elbow plot. This is the same technique often used to determine how many clusters to choose with the clustering algorithms. In this case, we\u2019ll plot the coherence score against the number of topics:", "You\u2019ll generally want to pick the lowest number of topics where the coherence score begins to level off. This is why it\u2019s called an elbow plot \u2014 you pick the elbow between steep gains and shallow gains. In this case (and it\u2019s a remarkably spiky case; usually the curves are a little bit smoother than this), I\u2019d go with somewhere around 20 topics.", "The first model I used is Gensim\u2019s ldamodel. At 20 topics, Gensim had a coherence score of 0.319. This is not great; indeed the Mallet algorithm which we\u2019ll look at next almost always outperforms Gensim\u2019s. However, one really cool thing with Gensim is the pyLDAvis, an interactive chart you can run in a Jupyter notebook. It plots the clusters with two principal components and shows the proportion of words in each cluster:", "The next implementation I looked at was Mallet (MAchine Learning for LanguagE Toolkit), a Java-based package put out by UMASS Amherst. The difference between Mallet and Gensim\u2019s standard LDA is that Gensim uses a Variational Bayes sampling method which is faster but less precise that Mallet\u2019s Gibbs Sampling. Fortunately for those who prefer to code in Python, Gensim has a wrapper for Mallet: Latent Dirichlet Allocation via Mallet. In order to use it, you need to download the Mallet Java package from here http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip and also install the Java Development Kit. Once everything is set up, implementing the model is pretty much the same as Gensim\u2019s standard model. Using Mallet, the coherence score for the 20-topic model increased to 0.375 (remember, Gensim\u2019s standard model output 0.319). It\u2019s a modest increase, but usually persists with a variety of data sources so although Mallet is slightly slower, I prefer it for its increase in return.", "Finally, I built a Mallet model on the 192 chapters of all 7 books in the Harry Potter series. Here are the top 10 keywords the model output for each latent topic. How would you name these clusters?", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Ff3c00f77b0f5&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbasic-nlp-on-the-texts-of-harry-potter-topic-modeling-with-latent-dirichlet-allocation-f3c00f77b0f5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbasic-nlp-on-the-texts-of-harry-potter-topic-modeling-with-latent-dirichlet-allocation-f3c00f77b0f5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbasic-nlp-on-the-texts-of-harry-potter-topic-modeling-with-latent-dirichlet-allocation-f3c00f77b0f5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbasic-nlp-on-the-texts-of-harry-potter-topic-modeling-with-latent-dirichlet-allocation-f3c00f77b0f5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----f3c00f77b0f5--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f3c00f77b0f5--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@raffg?source=post_page-----f3c00f77b0f5--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@raffg?source=post_page-----f3c00f77b0f5--------------------------------", "anchor_text": "Greg Rafferty"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3c8205f57821&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbasic-nlp-on-the-texts-of-harry-potter-topic-modeling-with-latent-dirichlet-allocation-f3c00f77b0f5&user=Greg+Rafferty&userId=3c8205f57821&source=post_page-3c8205f57821----f3c00f77b0f5---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff3c00f77b0f5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbasic-nlp-on-the-texts-of-harry-potter-topic-modeling-with-latent-dirichlet-allocation-f3c00f77b0f5&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff3c00f77b0f5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbasic-nlp-on-the-texts-of-harry-potter-topic-modeling-with-latent-dirichlet-allocation-f3c00f77b0f5&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://github.com/raffg/harry_potter_nlp", "anchor_text": "github"}, {"url": "https://github.com/raffg/basic_nlp", "anchor_text": "Github repo"}, {"url": "https://radimrehurek.com/gensim/models/ldamodel.html", "anchor_text": "ldamodel"}, {"url": "https://radimrehurek.com/gensim/models/ldamallet.html", "anchor_text": "Latent Dirichlet Allocation via Mallet"}, {"url": "http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip", "anchor_text": "http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip"}, {"url": "https://medium.com/tag/data-science?source=post_page-----f3c00f77b0f5---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/nlp?source=post_page-----f3c00f77b0f5---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/python?source=post_page-----f3c00f77b0f5---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----f3c00f77b0f5---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-journalism?source=post_page-----f3c00f77b0f5---------------data_journalism-----------------", "anchor_text": "Data Journalism"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff3c00f77b0f5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbasic-nlp-on-the-texts-of-harry-potter-topic-modeling-with-latent-dirichlet-allocation-f3c00f77b0f5&user=Greg+Rafferty&userId=3c8205f57821&source=-----f3c00f77b0f5---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff3c00f77b0f5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbasic-nlp-on-the-texts-of-harry-potter-topic-modeling-with-latent-dirichlet-allocation-f3c00f77b0f5&user=Greg+Rafferty&userId=3c8205f57821&source=-----f3c00f77b0f5---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff3c00f77b0f5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbasic-nlp-on-the-texts-of-harry-potter-topic-modeling-with-latent-dirichlet-allocation-f3c00f77b0f5&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f3c00f77b0f5--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Ff3c00f77b0f5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbasic-nlp-on-the-texts-of-harry-potter-topic-modeling-with-latent-dirichlet-allocation-f3c00f77b0f5&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----f3c00f77b0f5---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----f3c00f77b0f5--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----f3c00f77b0f5--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----f3c00f77b0f5--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----f3c00f77b0f5--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----f3c00f77b0f5--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----f3c00f77b0f5--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----f3c00f77b0f5--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----f3c00f77b0f5--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@raffg?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@raffg?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Greg Rafferty"}, {"url": "https://medium.com/@raffg/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3c8205f57821&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbasic-nlp-on-the-texts-of-harry-potter-topic-modeling-with-latent-dirichlet-allocation-f3c00f77b0f5&user=Greg+Rafferty&userId=3c8205f57821&source=post_page-3c8205f57821--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fb0cdd9173271&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbasic-nlp-on-the-texts-of-harry-potter-topic-modeling-with-latent-dirichlet-allocation-f3c00f77b0f5&newsletterV3=3c8205f57821&newsletterV3Id=b0cdd9173271&user=Greg+Rafferty&userId=3c8205f57821&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}