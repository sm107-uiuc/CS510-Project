{"url": "https://towardsdatascience.com/regularization-in-gradient-point-of-view-manual-back-propagation-in-tensorflow-b7811ad297da", "time": 1682994513.348366, "path": "towardsdatascience.com/regularization-in-gradient-point-of-view-manual-back-propagation-in-tensorflow-b7811ad297da/", "webpage": {"metadata": {"title": "Regularization in Gradient Point of View [ Manual Back Propagation in Tensorflow ] | by Jae Duk Seo | Towards Data Science", "h1": "Regularization in Gradient Point of View [ Manual Back Propagation in Tensorflow ]", "description": "Regularization is just like the cat shown above when some of the weights want to be \u2018big\u2019 in magnitude, we penalize them. And today I wanted to see what kind of changes does regularization term\u2026"}, "outgoing_paragraph_urls": [{"url": "http://rohanvarma.me/Regularization/", "anchor_text": "blog post", "paragraph_index": 4}, {"url": "https://blog.alexlenail.me/what-is-the-difference-between-ridge-regression-the-lasso-and-elasticnet-ec19c71c9028", "anchor_text": "blog post", "paragraph_index": 5}, {"url": "https://cs.stanford.edu/~acoates/stl10/", "anchor_text": "STL Data Set", "paragraph_index": 6}, {"url": "https://colab.research.google.com/drive/1C13_mV9zChz6eQwZgZrPZHLvQ_b2I6C7", "anchor_text": "click here", "paragraph_index": 31}, {"url": "https://github.com/JaeDukSeo/Daily-Neural-Network-Practice-2/blob/master/Class%20Stuff/Sparsity%20in%20Gradient%20Point%20of%20View/blog/a%20blog.ipynb", "anchor_text": "click here", "paragraph_index": 31}, {"url": "http://www.scs.ryerson.ca/~bruce/", "anchor_text": "Dr. Bruce", "paragraph_index": 35}, {"url": "https://arxiv.org/abs/0811.4706", "anchor_text": "Comparing Measures of Sparsity", "paragraph_index": 35}, {"url": "https://jaedukseo.me/", "anchor_text": "website", "paragraph_index": 35}], "all_paragraphs": ["Regularization is just like the cat shown above when some of the weights want to be \u2018big\u2019 in magnitude, we penalize them. And today I wanted to see what kind of changes does regularization term brings on the gradient. Below is the list of different regularization terms that we are going to compare. (\u03b8 is the weight of each layer.).", "In order to prevent over-fitting, or control the degree of over-fitting (severeness of over-fitting) we use regularization, such as drop out or adding an L1 or L2 regularization term. The general idea behind regularization (except for drop-out), is to minimize the magnitude of the weight while minimizing the objective function, and of course, some regularization aims to create \u2018sparse\u2019 weights, meaning that most of the weight values are zero.", "Moving on, for any training procedures in neural network architectures, there are common elements such as the values before and after activation. Gradient respect to the weight, and gradient that gets to be passed along to the previous layers. The weights themselves, and when using the Adam optimizer the momentum value for every weight.", "So putting the two and two together, what kind of effect does the regularization term have on those values? Their mean, standard deviation, skewness, kurtosis, and the number of non-zero elements, would they increase, decrease or stay the same with base-line performance?", "One of the most interesting viewpoints on how to interpret the regularization term was to view them as Bayesian prior to the weights. This blog post describes in details on the different point of views in which we can understand the regularization term. (And I suspect the degree of regularization differs depending on how well the prior is informed/uninformed also I think as training proceeds eventually the likelihood will have a greater effect to the weights.).", "Additionally, if anyone is interested in the basic intuition behind L1 and L2 regularization as well as some naming history please check this blog post out.", "Blue Sphere \u2192 Input Image from STL Data Set shape of (20,96,96,3)Yellow Rectangle\u2192 Convolution Layer with ReLU activationRed Square \u2192 Cross-Entropy loss term", "The whole experiment is just a classification task, and depending on which regularization terms we are adding the yellow rectangles will have an additional term when calculating the gradient. And before moving on I wish to clarify the meaning of each word that I am going to use.", "When we view the accuracy plots for both training and testing data, we can observe that\u2026..Highest Training Accuracy is Achieved when adding: sqrt(\u03b8\u00b2)/\u03b8Highest Testing Accuracy is Achieved when adding: -tanh(\u03b8)Lowest Performance is Achieved when adding: \u03b8", "When we add the term \u03b8 the derivative value to each weight just becomes one, and I suspect this may cause an overshooting.", "Average Statistic for gradientp (gradient that gets pass along to previous layer)", "When we view the gradient values that gets pass along to the previous layer, one color stands out, grey. Adding the term -log(1+\u03b8\u00b2) seems to make the gradient values vary a lot.", "Average Statistic for gradientw (gradient respect to weight)", "When we focus our attention on the number of non-zero elements we observe that without any regularization terms, gradient respect to each weight is mostly made up of zeros. (this may also contribute to the reason for slow convergence.).", "And when we focus our attention on average standard deviation values, we notice that in the final regions of training, the pink and yellow line meets. However, case H (pink) gave much better performance on the train/test data than case B (yellow).", "Average Statistic for layer (layer value before activation)", "When we view the values of each layer (before activation function) we notice right away that average value for the case C (green) keeps going down as training continues. I don\u2019t think this decrease itself is a bad thing, however, when compared to other cases, it is extreme, additionally please note that we are using ReLU activation, this means that any value that is smaller than zero, are dropped and replaced by zero.", "Average Statistic for layera (layer value after activation)", "We already know, that adding -tanh(\u03b8) (pink line) gave us the best results on test images, and when we focus our attention on the number of non-zero elements after the activation layer, case H (pink line), is in its own region.", "While other cases are either above or below 60,000, strangely case H seems to converge in that region. The internal representation of data is sparser when compared to other cases.", "Average Statistic for weights (weight values for each case)", "Due to case C (green line) overshooting, we get a distorted graph, in which other values are squeeze just to accommodate the decrease of weight in case C, so in the next plot, I have removed case C from the average mean weight plot.", "After removal, we can see a clear picture of how different weights values change over time. Again, we notice that case H (pink line) is located in its own region, I have no idea why it does that, also it doesn\u2019t make sense, since the point of regularization is to minimize the weights, why is the value higher than other cases?", "Average Statistic for moment (momentum portion of Adam Optimizer)", "From the above plot, we (might be able to) conclude that case C (green line) have overshot. It seems like all? or most of the values at the momentum is zero. And the standard deviation is all zero, indicating that there are not many variations within the momentum.", "I want to start this section by saying,", "I don\u2019t conclude anything, this was just ONE experiment, hence it would be wrong to conclude anything from one sample. Additionally, one critical flaw of my experiment is the fact that all of the weights were not initialized equally! I gave the exact same seed value as well as standard deviation (as seen below) but I am not sure if all of the weights were equally initialized.", "From all of this experiments, it seems like there is no rule that guarantees that a model will do well on the test data. (or it will have higher generalization power). However, there seems to be a unique pattern for a model to generalize well.", "I\u2019ll only focus my attention on case H (pink), generally, we understand regularization as limiting the growth of weights magnitude, however, from the graph above we can clearly see that the mean value of case H weights were higher than other ones\u2026.so\u2026? The question becomes did it regularize well? Would using the tanh term may be the reason behind this?", "But additionally, we noticed that after ReLU activation, case H\u2019s layera value was sparser than other ones. Meaning a healthy dose amount of zeros are mixed within the internal representation. I really wonder why case H gave the best test accuracy, the hand-wavy explanation of \u2018because it was well regularized\u2019 can do for now, but I know that there is more to it.", "In conclusion, the right balance between weight magnitude as well as sparsity seems to give good generalization performance. (in hand-wavy fashion).", "To access the code in google collab please click here, to access the code in my GitHub please click here.", "While doing these experiments, I can\u2019t help but think that these type of regularization have one (not so critical) flaw.", "They are all passive method of regularization.", "More specifically, every weight in the network, despite where they are located, what kind of features they tend to capture, they are all being regularized. I certainly believe that this is an interesting area of study, for example, we can have a simple conditional statement, that says \u201cif the magnitude of the weight is greater than x perform regularization\u201d or more complex one \u201cif the mutual information between this weight and the class label is already high, perform regularization\u201d. I wonder how those methods will change the overall dynamics of training.", "Finally, I want to thank my supervisor, Dr. Bruce, for recommending me the paper \u201cComparing Measures of Sparsity\u201d. For more articles please visit my website.", "Appendix (GIF animation for each layer for each case)", "Case Z: In order of gradientp, gradientw, layer, layera, moment, weight", "Case A: In order of gradientp, gradientw, layer, layera, moment, weight", "Case B: In order of gradientp, gradientw, layer, layera, moment, weight", "Case C: In order of gradientp, gradientw, layer, layera, moment, weight", "Case D: In order of gradientp, gradientw, layer, layera, moment, weight", "Case E: In order of gradientp, gradientw, layer, layera, moment, weight", "Case F: In order of gradientp, gradientw, layer, layera, moment, weight", "Case G: In order of gradientp, gradientw, layer, layera, moment, weight", "Case H: In order of gradientp, gradientw, layer, layera, moment, weight", "Case I: In order of gradientp, gradientw, layer, layera, moment, weight", "Case J: In order of gradientp, gradientw, layer, layera, moment, weight", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Exploring the intersection of AI, deep learning, and art. Passionate about pushing the boundaries of multi-media production and beyond. #AIArt"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fb7811ad297da&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-in-gradient-point-of-view-manual-back-propagation-in-tensorflow-b7811ad297da&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-in-gradient-point-of-view-manual-back-propagation-in-tensorflow-b7811ad297da&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-in-gradient-point-of-view-manual-back-propagation-in-tensorflow-b7811ad297da&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-in-gradient-point-of-view-manual-back-propagation-in-tensorflow-b7811ad297da&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----b7811ad297da--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b7811ad297da--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@jdseo?source=post_page-----b7811ad297da--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@jdseo?source=post_page-----b7811ad297da--------------------------------", "anchor_text": "Jae Duk Seo"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F70eb2d57a447&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-in-gradient-point-of-view-manual-back-propagation-in-tensorflow-b7811ad297da&user=Jae+Duk+Seo&userId=70eb2d57a447&source=post_page-70eb2d57a447----b7811ad297da---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb7811ad297da&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-in-gradient-point-of-view-manual-back-propagation-in-tensorflow-b7811ad297da&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb7811ad297da&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-in-gradient-point-of-view-manual-back-propagation-in-tensorflow-b7811ad297da&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://giphy.com/gifs/cat-money-cash-ND6xkVPaj8tHO", "anchor_text": "website"}, {"url": "https://web.stanford.edu/~hastie/Papers/B67.2%20%282005%29%20301-320%20Zou%20&%20Hastie.pdf", "anchor_text": "Elastic Net"}, {"url": "https://arxiv.org/abs/0811.4706", "anchor_text": "Comparing Measures of Sparsity"}, {"url": "https://arxiv.org/abs/0811.4706", "anchor_text": "Comparing Measures of Sparsity"}, {"url": "https://arxiv.org/abs/0811.4706", "anchor_text": "Comparing Measures of Sparsity"}, {"url": "https://arxiv.org/abs/0811.4706", "anchor_text": "Comparing Measures of Sparsity"}, {"url": "https://arxiv.org/abs/0811.4706", "anchor_text": "Comparing Measures of Sparsity"}, {"url": "https://arxiv.org/abs/0811.4706", "anchor_text": "Comparing Measures of Sparsity"}, {"url": "http://rohanvarma.me/Regularization/", "anchor_text": "blog post"}, {"url": "https://blog.alexlenail.me/what-is-the-difference-between-ridge-regression-the-lasso-and-elasticnet-ec19c71c9028", "anchor_text": "blog post"}, {"url": "https://cs.stanford.edu/~acoates/stl10/", "anchor_text": "STL Data Set"}, {"url": "https://colab.research.google.com/drive/1C13_mV9zChz6eQwZgZrPZHLvQ_b2I6C7", "anchor_text": "click here"}, {"url": "https://github.com/JaeDukSeo/Daily-Neural-Network-Practice-2/blob/master/Class%20Stuff/Sparsity%20in%20Gradient%20Point%20of%20View/blog/a%20blog.ipynb", "anchor_text": "click here"}, {"url": "http://www.scs.ryerson.ca/~bruce/", "anchor_text": "Dr. Bruce"}, {"url": "https://arxiv.org/abs/0811.4706", "anchor_text": "Comparing Measures of Sparsity"}, {"url": "https://jaedukseo.me/", "anchor_text": "website"}, {"url": "http://www.scs.ryerson.ca/~bruce/", "anchor_text": "http://www.scs.ryerson.ca/~bruce/"}, {"url": "https://arxiv.org/abs/0811.4706", "anchor_text": "https://arxiv.org/abs/0811.4706"}, {"url": "https://www.python-course.eu/matplotlib_multiple_figures.php", "anchor_text": "https://www.python-course.eu/matplotlib_multiple_figures.php"}, {"url": "https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.kurtosis.html", "anchor_text": "https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.kurtosis.html"}, {"url": "https://docs.scipy.org/doc/scipy-0.13.0/reference/generated/scipy.stats.skew.html", "anchor_text": "https://docs.scipy.org/doc/scipy-0.13.0/reference/generated/scipy.stats.skew.html"}, {"url": "https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.count_nonzero.html", "anchor_text": "https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.count_nonzero.html"}, {"url": "https://stackoverflow.com/questions/42916330/efficiently-count-zero-elements-in-numpy-array", "anchor_text": "https://stackoverflow.com/questions/42916330/efficiently-count-zero-elements-in-numpy-array"}, {"url": "https://stackoverflow.com/questions/16327405/syntaxerror-unexpected-eof-while-parsing", "anchor_text": "https://stackoverflow.com/questions/16327405/syntaxerror-unexpected-eof-while-parsing"}, {"url": "https://matplotlib.org/api/_as_gen/matplotlib.pyplot.legend.html", "anchor_text": "https://matplotlib.org/api/_as_gen/matplotlib.pyplot.legend.html"}, {"url": "https://www.google.com/search?q=row+vs+column&rlz=1C1CHBF_enCA771CA771&oq=row+vs+col&aqs=chrome.0.35i39j69i60j69i57j0l3.2289j1j7&sourceid=chrome&ie=UTF-8", "anchor_text": "https://www.google.com/search?q=row+vs+column&rlz=1C1CHBF_enCA771CA771&oq=row+vs+col&aqs=chrome.0.35i39j69i60j69i57j0l3.2289j1j7&sourceid=chrome&ie=UTF-8"}, {"url": "https://matplotlib.org/users/legend_guide.html", "anchor_text": "https://matplotlib.org/users/legend_guide.html"}, {"url": "https://medium.com/@SeoJaeDuk/archived-post-random-notes-for-derivative-for-regularization-terms-1859b1faada", "anchor_text": "https://medium.com/@SeoJaeDuk/archived-post-random-notes-for-derivative-for-regularization-terms-1859b1faada"}, {"url": "https://stackoverflow.com/questions/16261240/releasing-memory-of-huge-numpy-array-in-ipython", "anchor_text": "https://stackoverflow.com/questions/16261240/releasing-memory-of-huge-numpy-array-in-ipython"}, {"url": "https://ipython.readthedocs.io/en/stable/interactive/magics.html", "anchor_text": "https://ipython.readthedocs.io/en/stable/interactive/magics.htm"}, {"url": "http://superfluoussextant.com/making-gifs-with-python.html", "anchor_text": "http://superfluoussextant.com/making-gifs-with-python.html"}, {"url": "https://blog.alexlenail.me/what-is-the-difference-between-ridge-regression-the-lasso-and-elasticnet-ec19c71c9028", "anchor_text": "https://blog.alexlenail.me/what-is-the-difference-between-ridge-regression-the-lasso-and-elasticnet-ec19c71c9028"}, {"url": "https://web.stanford.edu/~hastie/Papers/B67.2%20%282005%29%20301-320%20Zou%20&%20Hastie.pdf", "anchor_text": "https://web.stanford.edu/~hastie/Papers/B67.2%20%282005%29%20301-320%20Zou%20&%20Hastie.pdf"}, {"url": "https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c", "anchor_text": "https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c"}, {"url": "https://blog.alexlenail.me/what-is-the-difference-between-ridge-regression-the-lasso-and-elasticnet-ec19c71c9028", "anchor_text": "https://blog.alexlenail.me/what-is-the-difference-between-ridge-regression-the-lasso-and-elasticnet-ec19c71c9028"}, {"url": "https://cs.stanford.edu/~acoates/stl10/", "anchor_text": "https://cs.stanford.edu/~acoates/stl10/"}, {"url": "https://stackoverflow.com/questions/3899980/how-to-change-the-font-size-on-a-matplotlib-plot", "anchor_text": "https://stackoverflow.com/questions/3899980/how-to-change-the-font-size-on-a-matplotlib-plot"}, {"url": "https://stackoverflow.com/questions/34291260/how-can-i-plot-multiple-figure-in-the-same-line-with-matplotlib", "anchor_text": "https://stackoverflow.com/questions/34291260/how-can-i-plot-multiple-figure-in-the-same-line-with-matplotlib"}, {"url": "https://stackoverflow.com/questions/35119831/ipython-notebook-keyboard-shortcut-search-for-text", "anchor_text": "https://stackoverflow.com/questions/35119831/ipython-notebook-keyboard-shortcut-search-for-text"}, {"url": "https://zulko.github.io/moviepy/getting_started/videoclips.html", "anchor_text": "https://zulko.github.io/moviepy/getting_started/videoclips.html"}, {"url": "http://zulko.github.io/blog/2014/01/23/making-animated-gifs-from-video-files-with-python/", "anchor_text": "http://zulko.github.io/blog/2014/01/23/making-animated-gifs-from-video-files-with-python"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----b7811ad297da---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----b7811ad297da---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/math?source=post_page-----b7811ad297da---------------math-----------------", "anchor_text": "Math"}, {"url": "https://medium.com/tag/python?source=post_page-----b7811ad297da---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----b7811ad297da---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb7811ad297da&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-in-gradient-point-of-view-manual-back-propagation-in-tensorflow-b7811ad297da&user=Jae+Duk+Seo&userId=70eb2d57a447&source=-----b7811ad297da---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb7811ad297da&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-in-gradient-point-of-view-manual-back-propagation-in-tensorflow-b7811ad297da&user=Jae+Duk+Seo&userId=70eb2d57a447&source=-----b7811ad297da---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb7811ad297da&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-in-gradient-point-of-view-manual-back-propagation-in-tensorflow-b7811ad297da&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b7811ad297da--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fb7811ad297da&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-in-gradient-point-of-view-manual-back-propagation-in-tensorflow-b7811ad297da&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----b7811ad297da---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----b7811ad297da--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----b7811ad297da--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----b7811ad297da--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----b7811ad297da--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----b7811ad297da--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----b7811ad297da--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----b7811ad297da--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----b7811ad297da--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@jdseo?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@jdseo?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Jae Duk Seo"}, {"url": "https://medium.com/@jdseo/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "5.2K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F70eb2d57a447&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-in-gradient-point-of-view-manual-back-propagation-in-tensorflow-b7811ad297da&user=Jae+Duk+Seo&userId=70eb2d57a447&source=post_page-70eb2d57a447--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fd9ea20dd433a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregularization-in-gradient-point-of-view-manual-back-propagation-in-tensorflow-b7811ad297da&newsletterV3=70eb2d57a447&newsletterV3Id=d9ea20dd433a&user=Jae+Duk+Seo&userId=70eb2d57a447&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}