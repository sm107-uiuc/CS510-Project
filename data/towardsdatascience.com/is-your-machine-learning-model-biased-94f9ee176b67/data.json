{"url": "https://towardsdatascience.com/is-your-machine-learning-model-biased-94f9ee176b67", "time": 1682994902.801871, "path": "towardsdatascience.com/is-your-machine-learning-model-biased-94f9ee176b67/", "webpage": {"metadata": {"title": "Is your Machine Learning Model Biased? | by Parul Pandey | Towards Data Science", "h1": "Is your Machine Learning Model Biased?", "description": "ProPublica, an independent, investigative journalism outfit, came out with a disturbing story on May 23, 2016, titled Machine Bias. It highlighted a significant bias in the US judicial system, citing\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.propublica.org/about/", "anchor_text": "ProPublica", "paragraph_index": 1}, {"url": "https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing", "anchor_text": "Machine Bias", "paragraph_index": 1}, {"url": "http://www.northpointeinc.com/files/technical_documents/FieldGuide2_081412.pdf", "anchor_text": "COMPAS", "paragraph_index": 2}, {"url": "https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm", "anchor_text": "ProPublica\u2019s analysis", "paragraph_index": 4}, {"url": "https://www.breastcancer.org/symptoms/understand_bc/statistics", "anchor_text": "Breast Cancer Statistics", "paragraph_index": 11}, {"url": "https://medium.com/p/94f9ee176b67/#5725", "anchor_text": "CASE 1", "paragraph_index": 24}, {"url": "https://medium.com/p/94f9ee176b67/#7e4e", "anchor_text": "CASE 2", "paragraph_index": 27}, {"url": "https://www.civisanalytics.com/", "anchor_text": "Civis Analytics", "paragraph_index": 30}, {"url": "https://arxiv.org/pdf/1809.09245.pdf", "anchor_text": "case study", "paragraph_index": 30}, {"url": "https://www.youtube.com/watch?v=V3tmxMf2UH8", "anchor_text": "Measuring Model Fairness", "paragraph_index": 33}, {"url": "https://medium.com/u/654ed03ca126?source=post_page-----94f9ee176b67--------------------------------", "anchor_text": "Civis Analytics", "paragraph_index": 33}], "all_paragraphs": ["Machine learning models are being increasingly used to make decisions that affect people\u2019s lives. With this power comes a responsibility to ensure that the model predictions are fair and not discriminating.", "ProPublica, an independent, investigative journalism outfit, came out with a disturbing story on May 23, 2016, titled Machine Bias. It highlighted a significant bias in the US judicial system, citing an example of an 18-year-old girl named Brisha Borden. In 2014, Brisha was arrested for the theft of a bicycle. She was charged with burglary amounting to $80. Just a year ago, 41-year-old Vernon Prater was picked up for shoplifting $86.35 worth of tools from a nearby Home Depot store. Prater was a seasoned offender and had been previously convicted of many thefts and armed robberies in the past. Brisha, on the other hand, had committed some petty offences when she was a juvenile, nothing serious though.", "COMPAS, which stands for Correctional Offender Management Profiling for Alternative Sanctions is an algorithm used across U.S states to assess a criminal defendant\u2019s likelihood of becoming a recidivist \u2014 a term used to describe criminals who re-offends. According to COMPAS scores, Borden \u2014 who is black \u2014 was rated at higher risk. Prater \u2014 who is white \u2014 was rated at low risk.", "Two years later, Borden had not been charged with any new crime while Prater is serving an 8 -year prison term for another offence. Did the algorithm get it all wrong?", "ProPublica\u2019s analysis of the COMPAS tool found that black defendants were far more likely than white defendants to be incorrectly judged to be at a higher risk of recidivism, while white defendants were more likely than black defendants to be incorrectly flagged at low risk.", "Northpointe, the company behind the tool responded by saying that the model was not unfair because it had several similar overall performances for both white people and black people. To prove this, they looked at the AUC scores", "The table above presents the results from each model for the outcomes of any arrest for African American and White men. The AUCs for African American men range from .64 to .73 while for the whites, it ranges from .69 to .75. Northpoint hence concluded that since the AUC results for White men are quite similar to the results for African American men, their algorithm is entirely fair.", "This leaves us with a very important question: How do we decide which measure of fairness is appropriate? Before answering that let\u2019s learn a bit about how the models get biased in the first place.", "Wikipedia states, \u201c\u2026 bias is an error from erroneous assumptions in the learning algorithm. High bias can cause an algorithm to miss the relevant relations between features and target outputs \u201d.", "Today Machine learning models are being employed in almost every field. They have the capacity to decide whether a person is entitled to a home loan or not, what kind of movies or news one would be interested in, what is a person\u2019s likelihood of responding to an advertisement and so on. Machine learning models have also made their foray into the criminal justice field and are being used to decide how long an offender stays in jail waiting for parole. But how do we go about making sure that these models are not biased and aren\u2019t unfairly discriminating against people?", "Let us consider three different situations :", "Consider the following U.S. Breast Cancer Statistics:", "Now, if we were to build a classifier to predict the occurrence of breastcancer, according to these statistics, women will get breast cancer about two orders of magnitude more frequently than men do, and the ratio of probability will be something like 12% vs 0.1 %. So, in this case, there are very legitimate differences in the ground truth positive rates between the different genders which will impact the result of the classifier.", "Datasets can contain label bias when a protected attribute( something like gender, race, age) are assigned within the data set. Let\u2019s see how?", "In 2011, a paper was released, which reviewed the documented patterns of office discipline referrals in 364 elementary and middle schools in the US. The authors found mentioned that :", "Descriptive and logistic regression analyses indicate that students from African American families are 2.19 (elementary) to 3.78 (middle) times as likely to be referred to the office for problem behavior as their White peers. In addition, the results indicate that students from African American and Latino families are more likely than their White peers to receive expulsion or out of school suspension as consequences for the same or similar problem behavior.", "So if we have a data set like this and we set out to build a classification model that predicts whether a student will have a behaviour problem in future and uses \u2018has been suspended\u2019 for its label then we\u2019re operating with label bias because different groups are getting assigned labels differently within the model, and it\u2019s not an accurate reflection of actual student problem behaviour.", "If we are making a Punitive model, the consequences of the model\u2019sdecisions will be negative, which may be like punishing somebody. In such a case, one might make a case for caring more about FalsePositives to avoid punishing the innocent based on the output of the model.", "On the other hand, When a model is Assistive, the consequences of the model\u2019s decisions will be positive, we would be more concerned about False Negatives so that we don\u2019t waste our resource or resources on people whodon\u2019t need the benefit that we\u2019re handing out.", "We need to carefully think about the consequences of our model that can inform what kind of errors matter to us. Biases can get introduced into our datasets easily and we need to pick the appropriate fairness metric to tackle different situations.", "It appears that there are specific fairness metrics, but then, how do we decide which measure of fairness is necessary in a particular case? How should we define fairness in the context of the model that we\u2019re building or the problem that we\u2019re working on? Well, a lot of fairness metrics are being employed today to make sure that the resulting model is not biased, but unfortunately, they have their limitations.", "This metric is also sometimes called \u2018Statistical Parity\u2019. In U.S. law, unintentional bias is encoded via disparate impact, which occurs when a selection process has widely different outcomes for different groups, even as it appears to be neutral. This legal determination hinges on a definition of a protected class (ethnicity, gender, religious practice) and an explicit description of the process.", "In other words, we take the probability of a positive classification for both groups and then take their ratio. Let\u2019s understand this metric with the help of an example where we need to decide if there is a disparate impact during hiring in a company.", "Since 60% is less than 80%, hence there is Disparate Impact", "Trade-Off: As we can see, this metric implies that the ground truth positive rates would have to be the same. However, this metric of fairness is going to fall flat in CASE 1 discussed above, since there are two orders of magnitude difference between the ground truth positive rates between the different genders.", "This is another popular metric which compares the True Positive rates between protected groups. It is a criterion for discrimination against a specified sensitive attribute in supervised learning, where the goal is to predict some target based on available features. An excerpt from the paper defining equal opportunity:", "In the binary case, we often think of the outcome Y = 1 as the \u201cadvantaged\u201d outcome, such as \u201cnot defaulting on a loan\u201d, \u201cadmission to a college\u201d or \u201creceiving a promotion\u201d. A possible relaxation of equalized odds is to require non-discrimination only within the \u201cadvantaged\u201d outcome group. That is, to require that people who pay back their loan, have an equal opportunity of getting the loan in the first place (without specifying any requirement for those that will ultimately default). This leads to a relaxation of our notion that we call \u201cequal opportunity\u201d.", "Trade-Off: Again, this metric doesn\u2019t help to address the situation in CASE 2. Where there is a high chance of the dataset containing label bias. The \u2018Equal Opportunity\u2019 metric is based on the agreement with the labels, and if the labels themselves are biased, then we\u2019re not addressing the fairness question.", "What we gather from the discussion above is that \u2018Bias\u2019 and \u2018Fairness\u2019 of models have underlying ethical implications and so we still need some human judgment to make an appropriate decision about how we define and measure fairness in our context.", "When Machine Learning models first started being used to make decisions, people assumed that since they relied on math and computation, they ought to be fair. But we know with the examples above how that myth has been debunked. Now people say, \u201cAdding this constraint will make my model fair\u201d which is still not right. We can\u2019t just rely on the math; we still need a human person applying human judgements.", "It is not easy to detect bias in real-world data, and there is no one-size-fits-all solution. Civis Analytics, a Data Science and Analytics company, conducted a case study which examines the ability of six different fairness metrics to detect unfair bias in predictions generated by models trained on datasets containing known, artificial bias. One of their findings was, \u201cWhen evaluating fairness in machine learning settings, practitioners must carefully consider both the imbalances which may be present in the ground truth they hope to model and the origins of the bias in the datasets they will use to create those models\u201d.", "Some of the best practices that can be employed when building a machine learning model are:", "Machine learning has proved its mettle in a lot of applications and areas. However, one of the key hurdles for industrial applications of machine learning models is to determine whether the raw input data used to train the model contains discriminatory bias or not. This is an important question and may have ethical and moral implications. However, there isn\u2019t one single solution to this. For cases where the output of a model affects the people, it is wise to put fairness ahead of profits.", "Reference: This article was inspired by the wonderful talk titled \u2018 Measuring Model Fairness\u2019 given by J. Henry Hinnefeld of Civis Analytics. A big thanks to him for highlighting such an important aspect of AI and Machine learning.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Principal Data Scientist @H2O.ai | Author of Machine Learning for High-Risk Applications"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F94f9ee176b67&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-your-machine-learning-model-biased-94f9ee176b67&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-your-machine-learning-model-biased-94f9ee176b67&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-your-machine-learning-model-biased-94f9ee176b67&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-your-machine-learning-model-biased-94f9ee176b67&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----94f9ee176b67--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----94f9ee176b67--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://pandeyparul.medium.com/?source=post_page-----94f9ee176b67--------------------------------", "anchor_text": ""}, {"url": "https://pandeyparul.medium.com/?source=post_page-----94f9ee176b67--------------------------------", "anchor_text": "Parul Pandey"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7053de462a28&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-your-machine-learning-model-biased-94f9ee176b67&user=Parul+Pandey&userId=7053de462a28&source=post_page-7053de462a28----94f9ee176b67---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F94f9ee176b67&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-your-machine-learning-model-biased-94f9ee176b67&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F94f9ee176b67&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-your-machine-learning-model-biased-94f9ee176b67&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@rawpixel?utm_source=medium&utm_medium=referral", "anchor_text": "rawpixel"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://www.propublica.org/about/", "anchor_text": "ProPublica"}, {"url": "https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing", "anchor_text": "Machine Bias"}, {"url": "http://www.northpointeinc.com/files/technical_documents/FieldGuide2_081412.pdf", "anchor_text": "COMPAS"}, {"url": "https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm", "anchor_text": "ProPublica\u2019s analysis"}, {"url": "https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing", "anchor_text": "Source"}, {"url": "http://www.northpointeinc.com/files/publications/Criminal-Justice-Behavior-COMPAS.pdf", "anchor_text": "http://www.northpointeinc.com/files/publications/Criminal-Justice-Behavior-COMPAS.pdf"}, {"url": "https://www.breastcancer.org/symptoms/understand_bc/statistics", "anchor_text": "Breast Cancer Statistics"}, {"url": "https://www.breastcancer.org/symptoms/understand_bc/statistics", "anchor_text": "https://www.breastcancer.org/symptoms/understand_bc/statistics"}, {"url": "https://arxiv.org/abs/1412.3756)", "anchor_text": "1. Disparate Impact"}, {"url": "https://medium.com/p/94f9ee176b67/#5725", "anchor_text": "CASE 1"}, {"url": "https://arxiv.org/pdf/1610.02413.pdf", "anchor_text": "2. Equal Opportunity"}, {"url": "https://arxiv.org/pdf/1610.02413.pdf", "anchor_text": "https://arxiv.org/pdf/1610.02413.pdf"}, {"url": "https://medium.com/p/94f9ee176b67/#7e4e", "anchor_text": "CASE 2"}, {"url": "https://www.civisanalytics.com/", "anchor_text": "Civis Analytics"}, {"url": "https://arxiv.org/pdf/1809.09245.pdf", "anchor_text": "case study"}, {"url": "https://arxiv.org/pdf/1809.09245.pdf", "anchor_text": "https://arxiv.org/pdf/1809.09245.pdf"}, {"url": "https://arxiv.org/pdf/1412.3756.pdf", "anchor_text": "https://arxiv.org/pdf/1412.3756.pdf"}, {"url": "https://arxiv.org/pdf/1806.08010.pdf", "anchor_text": "https://arxiv.org/pdf/1806.08010.pdf"}, {"url": "https://www.youtube.com/watch?v=V3tmxMf2UH8", "anchor_text": "Measuring Model Fairness"}, {"url": "https://medium.com/u/654ed03ca126?source=post_page-----94f9ee176b67--------------------------------", "anchor_text": "Civis Analytics"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----94f9ee176b67---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----94f9ee176b67---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/bias?source=post_page-----94f9ee176b67---------------bias-----------------", "anchor_text": "Bias"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----94f9ee176b67---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----94f9ee176b67---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F94f9ee176b67&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-your-machine-learning-model-biased-94f9ee176b67&user=Parul+Pandey&userId=7053de462a28&source=-----94f9ee176b67---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F94f9ee176b67&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-your-machine-learning-model-biased-94f9ee176b67&user=Parul+Pandey&userId=7053de462a28&source=-----94f9ee176b67---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F94f9ee176b67&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-your-machine-learning-model-biased-94f9ee176b67&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----94f9ee176b67--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F94f9ee176b67&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-your-machine-learning-model-biased-94f9ee176b67&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----94f9ee176b67---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----94f9ee176b67--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----94f9ee176b67--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----94f9ee176b67--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----94f9ee176b67--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----94f9ee176b67--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----94f9ee176b67--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----94f9ee176b67--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----94f9ee176b67--------------------------------", "anchor_text": ""}, {"url": "https://pandeyparul.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://pandeyparul.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Parul Pandey"}, {"url": "https://pandeyparul.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "20K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7053de462a28&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-your-machine-learning-model-biased-94f9ee176b67&user=Parul+Pandey&userId=7053de462a28&source=post_page-7053de462a28--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F5be6ccf82bc8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-your-machine-learning-model-biased-94f9ee176b67&newsletterV3=7053de462a28&newsletterV3Id=5be6ccf82bc8&user=Parul+Pandey&userId=7053de462a28&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}