{"url": "https://towardsdatascience.com/how-to-compress-a-neural-network-427e8dddcc34", "time": 1683014850.841964, "path": "towardsdatascience.com/how-to-compress-a-neural-network-427e8dddcc34/", "webpage": {"metadata": {"title": "How to compress a neural network. An introduction to weight pruning\u2026 | by Tivadar Danka | Towards Data Science", "h1": "How to compress a neural network", "description": "Modern state-of-the-art neural network architectures are HUGE. For instance, you have probably heard about GPT-3, OpenAI\u2019s newest revolutionary NLP model, capable of writing poetry and interactive\u2026"}, "outgoing_paragraph_urls": [{"url": "https://play.aidungeon.io", "anchor_text": "interactive storytelling", "paragraph_index": 0}, {"url": "https://arxiv.org/pdf/2005.14165.pdf", "anchor_text": "According to the original paper", "paragraph_index": 3}, {"url": "https://www.technologyreview.com/2019/06/06/239031/training-a-single-ai-model-can-emit-as-much-carbon-as-five-cars-in-their-lifetimes/", "anchor_text": "Training large models can emit more CO2 than a car in its entire lifetime.", "paragraph_index": 8}, {"url": "http://papers.nips.cc/paper/250-optimal-brain-damage.pdf", "anchor_text": "Optimal Brain Damage", "paragraph_index": 15}, {"url": "http://yann.lecun.org/exdb/publis/pdf/lecun-89e.pdf", "anchor_text": "LeNet for MNIST classification", "paragraph_index": 16}, {"url": "https://arxiv.org/abs/1803.03635", "anchor_text": "The Lottery Ticket Hypothesis", "paragraph_index": 19}, {"url": "https://arxiv.org/abs/2006.05467", "anchor_text": "In their paper, Hidenori Tanaka, Daniel Kunin, Daniel L. K. Yamins, and Surya Ganguli", "paragraph_index": 26}, {"url": "https://arxiv.org/abs/2009.08576", "anchor_text": "Jonathan Frankle and co-authors point out in their very recent paper", "paragraph_index": 35}, {"url": "https://xkcd.com/1838/", "anchor_text": "bunch of linear algebra", "paragraph_index": 38}, {"url": "https://arxiv.org/abs/1503.02531", "anchor_text": "Distilling the Knowledge in a Neural Network", "paragraph_index": 50}, {"url": "https://www.tivadardanka.com/blog", "anchor_text": "If you love taking machine learning concepts apart and understanding what makes them tick, we have a lot in common. Check out my blog, where I frequently publish technical posts like this!", "paragraph_index": 64}], "all_paragraphs": ["Modern state-of-the-art neural network architectures are HUGE. For instance, you have probably heard about GPT-3, OpenAI\u2019s newest revolutionary NLP model, capable of writing poetry and interactive storytelling.", "Well, GPT-3 has around 175 billion parameters.", "To give you a perspective about how large this number is, consider the following. A $100 bill is approximately 6.14 inches wide. If you start laying down the bills right next to each other, the line will stretch 169,586 miles. For comparison, Earth\u2019s circumference is 24,901 miles, measured along the equator. So, it would take ~6.8 round trips until we ran out of the money.", "Unfortunately, as opposed to money, more is sometimes not better when it comes to the number of parameters. Sure, more parameters seem to mean better results, but also more massive costs. According to the original paper, GPT-3 required 3.14E+23 flops of training time, and the computing cost itself is in the millions of dollars.", "GPT-3 is so large that it cannot be easily moved to other machines. It is currently accessible through the OpenAI API, so you can\u2019t just clone a GitHub repository and run it on your computer.", "However, this is just the tip of the iceberg. Deploying much smaller models can also present a significant challenge for machine learning engineers. In practice, small and fast models are much better than cumbersome ones.", "Because of this, researchers and engineers have put significant energy into compressing models. Out of these efforts, several methods have emerged to deal with the problem.", "If we revisit GPT-3 for a minute, we can see how the number of parameters and the training time influences the performance.", "The trend seems clear: more parameters lead to better performance and higher computational costs. The latter not only impacts the training time but the server costs and the environmental effects as well. (Training large models can emit more CO2 than a car in its entire lifetime.) However, training is only the first part of the life cycle of a neural network. In the long run, inference costs take over.", "To optimize these costs by compressing the models, three main methods have emerged:", "In this article, my goal is to introduce you to these and give an overview of how they work.", "One of the oldest methods for reducing a neural network\u2019s size is weight pruning, eliminating specific connections between neurons. In practice, elimination means that the removed weight is replaced with zero.", "At first glance, this idea might be surprising. Wouldn\u2019t this eliminate the knowledge learned by the neural network?", "Sure, removing all of the connections would undoubtedly result in losing all that is learned. On the other part of the spectrum, pruning only one connection probably wouldn\u2019t mean any decrease in accuracy.", "The question is, how much can you remove until the predictive performance starts to suffer?", "The first ones to study this question were Yann LeCun, John S. Denker, and Sara A. Solla, in their paper Optimal Brain Damage from 1990. They have developed the following iterative method.", "During their experiments with pruning the LeNet for MNIST classification, they found that a significant portion of the weights can be removed without a noticeable increase in the loss.", "However, retraining was necessary after the pruning. This proved to be quite tricky since a smaller model means a smaller capacity. Besides, as mentioned above, training amounts for a significant portion of the computational costs. This compression only helps in inference time.", "Is there a method requiring less post-pruning training, but still reaching the unpruned model\u2019s predictive performance?", "One essential breakthrough was made in 2008 by researchers from MIT. In their paper titled The Lottery Ticket Hypothesis, Jonathan Frankle and Michael Carbin stated that in their hypothesis that", "A randomly-initialized, dense neural network contains a subnetwork that is initialized such that \u2014 when trained in isolation \u2014 it can match the test accuracy of the original network after training for at most the same number of iterations.", "Such subnetworks are called winning lottery tickets. To see why let\u2019s consider that you buy 10\u00b9\u2070\u2070\u2070 lottery tickets. (This is more than the number of atoms in the observable universe, but we\u2019ll let this one slide.) Because you have so many, there is a tiny probability that none of them are winners. This is similar to training a neural network, where we randomly initialize weights.", "If this hypothesis is true, and such subnetworks can be found, training could be done much faster and cheaper, since a single iteration step would take less computation.", "The question is, does the hypothesis hold, and if so, how can we find such subnetworks? The authors proposed the following iterative method.", "On simple architectures trained on simple datasets, such that LeNet on MNIST, this method offered significant improvement, as shown in the figure below.", "However, although it showed promise, it did not perform well on more complex architectures like ResNets. Moreover, pruning still happens after training, which is a significant problem.", "The most recent algorithm to prune before training was published in 2020. (Which is the year I am writing this.) In their paper, Hidenori Tanaka, Daniel Kunin, Daniel L. K. Yamins, and Surya Ganguli from Stanford developed a method that goes much further and does the pruning without training.", "First, they introduce the concept of layer collapse,", "the premature pruning of an entire layer making a network untrainable,", "which plays a significant part in the theory. Any pruning algorithm should avoid layer collapse. The hard part is identifying a class of algorithms that satisfies this criterion.", "For this purpose, the authors introduce the synaptic saliency score for a given weight in the network defined by", "where L is the loss function given by the network\u2019s output, and w is a weight parameter. Each neuron conserves this quantity: under certain constraints for the activation functions, the sum of incoming synaptic salience scores equal to the sum of outgoing synaptic saliency scores.", "This score is used to select which weights are pruned. (Recall that for this purpose, the Optimal Brain Damage method used a perturbation-based quantity, while the authors of the Lottery Ticket Hypothesis paper used the magnitude.)", "It turns out that synaptic saliency scores are conserved between layers, and roughly speaking, if an iterative pruning algorithm respects this layer-wise conservation, layer collapse can be avoided.", "The SynFlow algorithm is an iterative pruning algorithm similar to the previous ones, but the selection is based on the synaptic saliency scores.", "However, the work is far from done. As Jonathan Frankle and co-authors point out in their very recent paper, there is no universal state of the art solution. Each method shines in specific scenarios but outperformed in others. Moreover, the pre-training pruning methods outperform the baseline random pruning, they still don\u2019t perform as well as some post-training algorithms, especially magnitude-based pruning.", "Pruning is available both in TensorFlow and PyTorch.", "Next, we are going to take a look at another tool for neural network compression: quantization.", "In essence, a neural network is just a bunch of linear algebra and some other operations. By default, most systems use float32 types to represent the variables and weights.", "However, in general, computations in other formats such as int8 can be faster than in float32, with less memory footprint. (Of course, these can depend on the hardware, but we are not trying to be extra specific here.)", "Neural network quantization is the suite of methods aiming to take advantage of this. For instance, if we would like to go from float32 to int8 as mentioned, and our values are in the range [-a, a] for some real number a, we could use the transformation", "to convert the weights and proceed with the computations in the new form.", "Of course, things are not that simple. Multiplying two int8 numbers can easily overflow to int16, and so on. During quantization, care must be taken to avoid errors due to this.", "As with all compression methods, this comes with a loss of information and possibly predictive performance. The problem is the same as before: to find an optimal trade-off.", "Quantization has two primary flavors: post-training quantization and quantization-aware training. The former is more straightforward but can result in more significant accuracy loss than the latter.", "As you can see in the table above, this can cut the inference time in half in some instances. However, converting from float32 to int8 is not a smooth transformation; thus, it can lead to suboptimal results when the gradient landscape is wild.", "With quantization-aware training, this method has the potential to improve training time as well.", "Similarly to weight pruning, quantization is also available both in TensorFlow and PyTorch.", "At the time of the writing, the feature is experimental in PyTorch, which means that it is subject to change. So, you should expect breaking changes in the upcoming versions.", "So far, the methods we have seen share the same principle: train the network and discard some information to compress it. As we will see, the third one, knowledge distillation, differs from these significantly.", "Although quantization and pruning can be effective, they are destructive in the end. An alternative approach was developed by Geoffrey Hinton, Oriol Vinyals, and Jeff Dean in their paper Distilling the Knowledge in a Neural Network.", "Their idea is simple: train a big model (teacher) to achieve top performance and use its predictions to train a smaller one (student).", "Their work showed that this way, large ensemble models can be compressed with simpler architectures, more suitable towards production.", "Knowledge distillation improves the inference time of the distilled models, not the training time. This is an essential distinction between the other two methods since training time often has a high cost. (If we think back to the GPT-3 example, it was millions of dollars.)", "You might ask, why not just use a compact architecture from the start? The secret sauce is to teach the student model to generalize like the teacher by using its predictions. Here, the student model not only sees the training data for the big one, but new data as well, where it is fitted to approximate the output of the teacher.", "The smaller a model is, the more training data it needs to generalize well. Thus, it might require a complex architecture such as an ensemble model to reach the state of the art performance on challenging tasks. Still, its knowledge can be used to push the student model\u2019s performance beyond the baseline.", "One of the first use cases for knowledge distillation was compressing ensembles and making them suitable for production. Ensembles were notorious in Kaggle competitions. Several winning models were composed of several smaller ones, offering outstanding results but being unusable in practice.", "Since then, it was applied successfully for other architectures, most notably BERT, the famous transformer model for NLP.", "Besides the baseline distillation approach by Hinton et al., there are several other ones, trying to push the state of the art. If you would like to get an overview of those, I recommend the survey paper below.", "Since knowledge distillation does not require the manipulation of weights like pruning or quantization, it can be performed in any framework of your choice.", "Here are some examples to get you started!", "As neural networks are getting larger and larger, compressing the models are becoming even more critical. As the complexity of the problems and architectures increases, so does the computational cost and the environmental impact.", "This trend only seems to accelerate: GPT-3 contains 175 billion parameters, which is a 10x jump in the magnitude, compared to previous giant models. Thus, compressing these networks is a fundamental problem, which will become even more important in the future.", "Are you ready to tackle this challenge?", "If you love taking machine learning concepts apart and understanding what makes them tick, we have a lot in common. Check out my blog, where I frequently publish technical posts like this!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "I want to democratize machine learning. Math PhD with an INTJ personality. Chaotic good."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F427e8dddcc34&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-compress-a-neural-network-427e8dddcc34&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-compress-a-neural-network-427e8dddcc34&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-compress-a-neural-network-427e8dddcc34&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-compress-a-neural-network-427e8dddcc34&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----427e8dddcc34--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----427e8dddcc34--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@tivadar.danka?source=post_page-----427e8dddcc34--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@tivadar.danka?source=post_page-----427e8dddcc34--------------------------------", "anchor_text": "Tivadar Danka"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F26fd873de5f2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-compress-a-neural-network-427e8dddcc34&user=Tivadar+Danka&userId=26fd873de5f2&source=post_page-26fd873de5f2----427e8dddcc34---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F427e8dddcc34&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-compress-a-neural-network-427e8dddcc34&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F427e8dddcc34&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-compress-a-neural-network-427e8dddcc34&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://play.aidungeon.io", "anchor_text": "interactive storytelling"}, {"url": "https://arxiv.org/pdf/2005.14165.pdf", "anchor_text": "According to the original paper"}, {"url": "https://arxiv.org/abs/2005.14165", "anchor_text": "Language Models are Few-Shot Learners by Tom B. Brown et al."}, {"url": "https://www.technologyreview.com/2019/06/06/239031/training-a-single-ai-model-can-emit-as-much-carbon-as-five-cars-in-their-lifetimes/", "anchor_text": "Training large models can emit more CO2 than a car in its entire lifetime."}, {"url": "http://papers.nips.cc/paper/250-optimal-brain-damage.pdf", "anchor_text": "Optimal Brain Damage"}, {"url": "http://yann.lecun.org/exdb/publis/pdf/lecun-89e.pdf", "anchor_text": "LeNet for MNIST classification"}, {"url": "http://papers.nips.cc/paper/250-optimal-brain-damage.pdf", "anchor_text": "Optimal Brain Damage"}, {"url": "https://arxiv.org/abs/1803.03635", "anchor_text": "The Lottery Ticket Hypothesis"}, {"url": "https://arxiv.org/abs/1803.03635", "anchor_text": "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks"}, {"url": "https://arxiv.org/abs/2006.05467", "anchor_text": "In their paper, Hidenori Tanaka, Daniel Kunin, Daniel L. K. Yamins, and Surya Ganguli"}, {"url": "https://arxiv.org/abs/2006.05467", "anchor_text": "Pruning neural networks without any data by iteratively conserving synaptic flow"}, {"url": "https://arxiv.org/abs/2009.08576", "anchor_text": "Jonathan Frankle and co-authors point out in their very recent paper"}, {"url": "https://pytorch.org/tutorials/intermediate/pruning_tutorial.html", "anchor_text": "Pruning Tutorial - PyTorch Tutorials 1.6.0 documentationAuthor: Michela Paganini State-of-the-art deep learning techniques rely on over-parametrized models that are hard to\u2026pytorch.org"}, {"url": "https://blog.tensorflow.org/2019/05/tf-model-optimization-toolkit-pruning-API.html?hl=es-uy", "anchor_text": "TensorFlow Model Optimization Toolkit - Pruning APIMay 14, 2019 - Since we introduced the Model Optimization Toolkit - a suite of techniques that developers, both novice\u2026blog.tensorflow.org"}, {"url": "https://xkcd.com/1838/", "anchor_text": "bunch of linear algebra"}, {"url": "https://www.tensorflow.org/lite/", "anchor_text": "TensorFlow Lite"}, {"url": "https://www.tensorflow.org/lite/performance/model_optimization", "anchor_text": "TensorFlow Lite documentation"}, {"url": "https://www.tensorflow.org/lite/performance/model_optimization", "anchor_text": "Model optimization | TensorFlow LiteEdge devices often have limited memory or computational power. Various optimizations can be applied to models so that\u2026www.tensorflow.org"}, {"url": "https://pytorch.org/docs/stable/quantization.html", "anchor_text": "Quantization - PyTorch 1.6.0 documentationWarning Quantization is in beta and subject to change. Quantization refers to techniques for performing computations\u2026pytorch.org"}, {"url": "https://arxiv.org/abs/1503.02531", "anchor_text": "Distilling the Knowledge in a Neural Network"}, {"url": "https://arxiv.org/abs/1503.02531", "anchor_text": "Distilling the Knowledge in a Neural Network"}, {"url": "https://medium.com/huggingface/distilbert-8cf3380435b5", "anchor_text": "\ud83c\udfce Smaller, faster, cheaper, lighter: Introducing DilBERT, a distilled version of BERTYou can find the code to reproduce the training of DilBERT along with pre-trained weights for DilBERT here.medium.com"}, {"url": "https://arxiv.org/abs/2006.05525v1", "anchor_text": "Knowledge Distillation: A SurveyIn recent years, deep neural networks have been very successful in the fields of both industry and academia, especially\u2026arxiv.org"}, {"url": "https://keras.io/examples/vision/knowledge_distillation/", "anchor_text": "Keras documentation: Knowledge DistillationAuthor: Kenneth Borup Date created: 2020/09/01 Last modified: 2020/09/01 Description: Implementation of classical\u2026keras.io"}, {"url": "https://github.com/AberHu/Knowledge-Distillation-Zoo", "anchor_text": "AberHu/Knowledge-Distillation-ZooPytorch implementation of various Knowledge Distillation (KD) methods. This repository is a simple reference, mainly\u2026github.com"}, {"url": "https://www.tivadardanka.com/blog", "anchor_text": "If you love taking machine learning concepts apart and understanding what makes them tick, we have a lot in common. Check out my blog, where I frequently publish technical posts like this!"}, {"url": "https://towardsdatascience.com/can-you-remove-99-of-a-neural-network-without-losing-accuracy-915b1fab873b", "anchor_text": "Can you remove 99% of a neural network without losing accuracy?An introduction to weight pruningtowardsdatascience.com"}, {"url": "https://towardsdatascience.com/how-to-accelerate-and-compress-neural-networks-with-quantization-edfbbabb6af7", "anchor_text": "How to accelerate and compress neural networks with quantizationGoing from floats to integerstowardsdatascience.com"}, {"url": "https://towardsdatascience.com/can-a-neural-network-train-other-networks-cf371be516c6", "anchor_text": "Can a neural network train other networks?An introduction to knowledge distillationtowardsdatascience.com"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----427e8dddcc34---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----427e8dddcc34---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----427e8dddcc34---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----427e8dddcc34---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/ai?source=post_page-----427e8dddcc34---------------ai-----------------", "anchor_text": "AI"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F427e8dddcc34&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-compress-a-neural-network-427e8dddcc34&user=Tivadar+Danka&userId=26fd873de5f2&source=-----427e8dddcc34---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F427e8dddcc34&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-compress-a-neural-network-427e8dddcc34&user=Tivadar+Danka&userId=26fd873de5f2&source=-----427e8dddcc34---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F427e8dddcc34&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-compress-a-neural-network-427e8dddcc34&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----427e8dddcc34--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F427e8dddcc34&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-compress-a-neural-network-427e8dddcc34&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----427e8dddcc34---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----427e8dddcc34--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----427e8dddcc34--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----427e8dddcc34--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----427e8dddcc34--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----427e8dddcc34--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----427e8dddcc34--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----427e8dddcc34--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----427e8dddcc34--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@tivadar.danka?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@tivadar.danka?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Tivadar Danka"}, {"url": "https://medium.com/@tivadar.danka/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "3.2K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F26fd873de5f2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-compress-a-neural-network-427e8dddcc34&user=Tivadar+Danka&userId=26fd873de5f2&source=post_page-26fd873de5f2--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F471a6c88bdce&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-compress-a-neural-network-427e8dddcc34&newsletterV3=26fd873de5f2&newsletterV3Id=471a6c88bdce&user=Tivadar+Danka&userId=26fd873de5f2&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}