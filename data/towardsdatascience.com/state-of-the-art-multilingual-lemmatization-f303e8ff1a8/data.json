{"url": "https://towardsdatascience.com/state-of-the-art-multilingual-lemmatization-f303e8ff1a8", "time": 1682995361.371577, "path": "towardsdatascience.com/state-of-the-art-multilingual-lemmatization-f303e8ff1a8/", "webpage": {"metadata": {"title": "State-of-the-art Multilingual Lemmatization | by Erick Fonseca | Towards Data Science", "h1": "State-of-the-art Multilingual Lemmatization", "description": "I find lemmatization an intriguing NLP task, in which we must find ways to deal with the richness and some arbitrariness of human languages. In this post, I summarize the impressive current\u2026"}, "outgoing_paragraph_urls": [{"url": "http://universaldependencies.org/conll17/", "anchor_text": "CoNLL 2017", "paragraph_index": 12}, {"url": "http://universaldependencies.org/conll18/", "anchor_text": "2018", "paragraph_index": 12}, {"url": "https://medium.com/@devnag/seq2seq-the-clown-car-of-deep-learning-f88e1204dac3", "anchor_text": "One of them", "paragraph_index": 16}, {"url": "https://stanfordnlp.github.io/stanfordnlp/", "anchor_text": "StanfordNLP", "paragraph_index": 21}, {"url": "https://turkunlp.org/Turku-neural-parser-pipeline/", "anchor_text": "Turku Parser", "paragraph_index": 21}, {"url": "https://arxiv.org/abs/1902.00972", "anchor_text": "this paper", "paragraph_index": 23}, {"url": "https://github.com/CoNLL-UD-2018/UDPipe-Future", "anchor_text": "UDPipe Future", "paragraph_index": 26}, {"url": "https://github.com/360er0/COMBO", "anchor_text": "Combo parser", "paragraph_index": 26}, {"url": "http://universaldependencies.org/conll18/results-lemmas.html", "anchor_text": "CoNLL evaluation", "paragraph_index": 29}, {"url": "http://universaldependencies.org/conll18/results-lemmas.html", "anchor_text": "CoNLL 2018 scoreboard", "paragraph_index": 33}, {"url": "https://gist.github.com/erickrf/b3ede1e817c58d0df71dc81ad6f5d8dc", "anchor_text": "at Github", "paragraph_index": 44}], "all_paragraphs": ["I find lemmatization an intriguing NLP task, in which we must find ways to deal with the richness and some arbitrariness of human languages. In this post, I summarize the impressive current state-of-the-art for multilingual lemmatizers, with some useful hints for practitioners and newcomers to the field, and make some points on their limitations.", "When we are dealing with text data, sometimes there is the need to reduce vocabulary: if you are searching for \u201cused car prices\u201d, documents containing both car price and price of cars are likely to be relevant.", "A common strategy for this issue is stemming, the process of removing prefixes and suffixes from words until we are left with its stem, which carries most of its meaning. So, cars is stemmed to car, replayed to play, and taxi continues being taxi. It is carried out just by string editing, without any kind of preprocessing or machine learning.", "Stemming has been used successfully by many search engines due to its simplicity. This same simplicity, however, is a limiting factor: there is not even a precise definition of what the stem of a word is. The stem of replay might be play, but the stem of rehearse is certainly not hearse. It makes sense to think that the stem of caring is care, not car; but to accomplish that, we would need a more complex algorithm that knows when to insert a final -e. This can get even more confusing for other languages.", "The bottom line is that any method based only on string editing lacks linguistic consistency. When this is an issue, we turn to lemmatization.", "Lemmatization is the process of determining what is the lemma (i.e., the dictionary form) of a given word. Taking on the previous example, the lemma of cars is car, and the lemma of replay is replay itself. This is a well-defined concept, but unlike stemming, requires a more elaborate analysis of the text input.", "It is reasonably simple for English (as in stemming), a language with little word inflection. But for most languages, lemmatization is both more important and difficult. See this example sentence in Czech:", "Studie proveditelnosti odhaduje, \u017ee by v kabin\u011b cestuj\u00edc\u00edm trvalo p\u0159ekro\u010dit \u0159eku Potomac asi \u010dty\u0159i minuty.", "Replacing each word by its lemma gives us the following:", "Studie proveditelnost odhadovat, \u017ee b\u00fdt v kabina cestuj\u00edc\u00ed trvat p\u0159ekro\u010dit \u0159eka Potomac asi \u010dty\u0159i minuta.", "(in case you\u2019re wondering, it means \u201cThe feasibility study estimates that it would take passengers about four minutes to cross the Potomac River on the gondola.\u201d)", "In our Czech example, 8 out of 15 words differ from the lemmas! That\u2019s almost all content words in the sentence. Most research in NLP uses English data, but as you can see, it is a poor language choice for exemplifying lemmatization, not reflecting its real challenges.", "With the CoNLL 2017 and then 2018 shared tasks, however, we have a lot of data published for over 50 languages with lemma annotation. Participants were asked to, among other things, come up with lemmatizer models that worked for this wide range of languages. These data provide a great benchmark for evaluating lemmatizers.", "Word inflection has a lot of regularities, which might make you think of developing a set of rules for recovering the lemma of a word. But that can be surprisingly difficult because of some factors:", "As you can see, designing hand-written lemmatization rules can be troublesome even for English. So, as usual, we turn to machine learning for help. More specifically, to the ever-present neural networks.", "There is more than one way of lemmatizing words with neural networks, but let\u2019s stick to the one with the best results, which also happens to be the simplest. It uses sequence-to-sequence (seq2seq) neural networks, which reads words character by character, and then outputs their lemmas also character by character. This is the model used, among others, by the Turku NLP group that had the best results on lemmatization in the CoNLL 2018.", "I shall not go into details of how seq2seq works. My focus here is on the practical usage of lemmatizers; there are a lot of great posts explaining seq2seq out there. One of them amusingly calls it the clown car of deep learning because it can fit a lot of information in a seemingly small number of parameters.", "The end symbol, also represented as </s>, is used to indicate when the model finished producing its output. Without it, a seq2seq would generate characters indefinitely.", "The model input doesn\u2019t need to be only words. We can also include some metadata, such as POS tags, which can help determine which is the correct lemma for ambiguous forms. Remember that each character is encoded as an embedding vector, which is learned together with other neural network parameters. So, as long as we can encode our tags in vectors of the same dimensionality, we are good to go.", "Let\u2019s see an example where POS tags are important. In Portuguese, olho can be a noun, meaning eye, or an inflected verb, meaning to look; in the latter case, it should be lemmatized to olhar.", "This allows a lot of flexibility. The model can learn that certain word forms look nothing like their lemmas (e.g., was / be), some never change (grammar words such as prepositions and conjunctions), while for most others it is a matter of changing prefixes, infixes or suffixes. It can also learn to associate certain inflection patterns with certain POS tags.", "Most current state-of-the-art implementations, however, don\u2019t completely solve the problem of lemma ambiguity. Models such as the StanfordNLP and the Turku Parser lemmatize each word independently, taking into account its POS and morphology tags (such as gender, number, tense, case, etc.), but not the other words in the sentence. It is enough to solve cases like the example above, in which noun and verb have different lemmas, but not when the same combination of word, POS tag and morphological inflection still can be mapped to more than one lemma \u2014 like geh\u00f6rt, which I mentioned earlier.", "By the way, do not mistake lemma ambiguity with word sense ambiguity! In lemmatization, we are only interested in the written lemma, not meaning. Thus, you don\u2019t need to know if bats refer to the flying animal or the club used in baseball as long as you know its lemma is bat.", "But why are these models like that? Basically, because these cases are very rare. In this paper, researchers from Turku argue that for most treebanks, this phenomenon happens for less than 1% of the tokens. Even for languages in which it is common, such as Spanish (14%), Hindi (22%) and Urdu (36%), there is one lemma much more frequent than the others. For example, the form fue appears dozens of times tagged as AUX (auxiliary verb) and having ser (to be) as lemma, against a few occurrences with the same tag but the lemma ir (to go).", "On top of that, many of these ambiguities might be annotation errors. I checked the Spanish treebank and found that many ambiguous word/tag combinations actually had typos or were not correctly lemmatized. Unfortunately I don't know any Urdu to repeat the check, but I wouldn't be surprised to find the same.", "This doesn\u2019t mean it is not worth detecting the difference between the two \u2014 on the contrary. The problem is that with so few examples, statistical models have a hard time learning the correct lemmas. So if we really want to prepare our models to disambiguate all the fue, geh\u00f6rt and the like, we need to prepare datasets targeted at those words.", "There are implementations capable of disambiguating them, though \u2014 at least theoretically. The UDPipe Future and the Combo parser are two of them, and despite not using seq2seq, their rationale is compatible with it. The figure below depicts the UDPipe architecture, but the important thing here is the usage of a recurrent neural network (RNN), also employed by Combo.", "The bidirectional LSTM, a common choice of RNN, reads the whole input sentence and produces context-sensitive vectors to encode each word. After that, a lemmatizer MLP classifies each word into one of the automatically generated lemmatization rules, which consist of removing, adding and replacing substrings.", "This kind of representation is not exclusive to their models; it is pretty standard in NLP. The Turku and Stanford systems do have BiLSTMs of their own, but they only use them for the other CoNLL tasks (POS tagging, morphological tagging, and parsing; I intend to talk more about them in an upcoming post).", "It turns out, however, that context awareness doesn\u2019t necessarily translate into better lemmatization performance. In the CoNLL evaluation, the Turku and Stanford systems got the best results in the treebanks that had a reasonable amount of data. Even for the treebanks in which ambiguous tokens are more common, results aren\u2019t very clear:", "Considering how rare ambiguous token/tag combinations are, the capability of distinguishing them played a very marginal role in the results. It is more likely that the seq2seq architecture is simply more efficient in general for this kind of task, in comparison to the classifier used in UDPipe. We still could, of course, combine the best of both worlds and design a seq2seq lemmatizer that includes a context representation in the same way as the extra tag metadata:", "The first sentence in the figure above means I haven\u2019t heard anything, and the second It belonged to the firm. In each case, a bidirectional LSTM would be capable of capturing the particular sentence context, providing more information to the seq2seq lemmatizer, which would, in turn, be able \u2014 at least theoretically \u2014 to predict the correct lemma. Without the context vector, the lemmatizer could never produce two different lemmas for this same word.", "But again, we would hit the data bottleneck. With ambiguous examples being so rare, this improved model wouldn\u2019t have enough examples to learn from and take advantage of its architecture.", "Now that we\u2019ve seen this neat lemmatizer architecture, the next natural question is how good it actually is. The CoNLL 2018 scoreboard I mentioned before gives us a good overview, but these numbers should be read with some caution.", "The first table, All treebanks, includes results for treebanks that have only a couple of thousand words. This is insufficient to learn proper lemmatization rules, and performance on these treebanks can have a high variance depending on random initialization.", "The second table, Big treebanks only, filters out those extremely small treebanks (indeed, the big here can be misleading, as some of these treebanks are still quite small; but Not extremely small treebanks doesn\u2019t sound like a good name). Lemmatizers trained on these remaining treebanks are more viable for production usage.", "The numbers there look pretty good: they are in the high 90\u2019s for most languages. You can also notice that although the Turku model is the best overall, for some languages other models do better. Keep in mind, though, that these values are inflated by the large number of words in every language that do not inflect at all, such as conjunctions and prepositions. Remember the Czech example at the beginning of this post? Well, Czech has a lot of inflection, but in that sentence 7 out of 15 words already looked exactly like their lemma.", "At any rate, the performance of the lemmatizers I mentioned here seem great for production usage \u2014 at least if the texts you are working with look like the treebanks the lemmatizers were trained on. So here go a few more words of caution and practical hints if you need to use one of them.", "The biggest challenge for machine learning based lemmatizers is irregular forms. That\u2019s no surprise: they are challenging for humans as well, sometimes even in their native language, exactly because of their unpredictability. So if your model never saw in the training data that forbade is the past tense of forbid, it will maybe think that forbade sounds like a good lemma for itself.", "If you really want to improve your lemmatizer\u2019s performance, having a backoff list of irregular words in the language you\u2019re working with is a good idea. However, this list should have tuples of (inflected form, POS tag, lemma), or you may run into ambiguity issues. While word/POS/lemma ambiguity is very rare, as I mentioned before, only word/lemma is an issue even in English: think of the noun-verb ambiguity in saw, thought, shot, for example.", "Now, some inflection rules might be perfectly regular, but if they never appear in the training data, no machine learning based system will ever learn them. Since many corpora in the UD datasets come from newspaper texts, a common feature is that verbs in the second person are pretty rare overall. In the German training treebank, for example, with almost 30 thousand verbs, only 13 are in the second person. One of the Finnish treebanks, roughly the same size, is slightly better with 462 \u2014 but that\u2019s only 1,5% of the total!", "So, if you use a lemmatizer trained on CoNLL data and notice it performs very badly in some specific inflection, it\u2019s likely that it didn\u2019t have enough examples to learn. If that is an issue, you will have to either come up with more representative data to retrain the model or implement some manual rules to override the automatic output.", "I've shown that state-of-the-art lemmatizers have developed very interesting and efficient architectures. Cases with ambiguous combinations of word/POS tag/morphological tags are still an issue, but they are so rare as to be hardly a concern in practice. Dealing with irregular words and inflections absent in the training data is more worrying, though.", "Still, if you speak some language with rich morphology, try one of the systems I mentioned here just to be amazed at their capacity of undoing inflections!", "The script I wrote to check the Spanish UD treebank is available at Github. You can use it to check other treebanks for ambiguous lemmas and possible annotation errors.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Scientist at Kaufland, Germany. Doing Natural Language Processing stuff."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Ff303e8ff1a8&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstate-of-the-art-multilingual-lemmatization-f303e8ff1a8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstate-of-the-art-multilingual-lemmatization-f303e8ff1a8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstate-of-the-art-multilingual-lemmatization-f303e8ff1a8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstate-of-the-art-multilingual-lemmatization-f303e8ff1a8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----f303e8ff1a8--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f303e8ff1a8--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@erickrfonseca?source=post_page-----f303e8ff1a8--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@erickrfonseca?source=post_page-----f303e8ff1a8--------------------------------", "anchor_text": "Erick Fonseca"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F232a4085d8e9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstate-of-the-art-multilingual-lemmatization-f303e8ff1a8&user=Erick+Fonseca&userId=232a4085d8e9&source=post_page-232a4085d8e9----f303e8ff1a8---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff303e8ff1a8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstate-of-the-art-multilingual-lemmatization-f303e8ff1a8&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff303e8ff1a8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstate-of-the-art-multilingual-lemmatization-f303e8ff1a8&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "http://universaldependencies.org/conll17/", "anchor_text": "CoNLL 2017"}, {"url": "http://universaldependencies.org/conll18/", "anchor_text": "2018"}, {"url": "https://medium.com/@devnag/seq2seq-the-clown-car-of-deep-learning-f88e1204dac3", "anchor_text": "One of them"}, {"url": "https://stanfordnlp.github.io/stanfordnlp/", "anchor_text": "StanfordNLP"}, {"url": "https://turkunlp.org/Turku-neural-parser-pipeline/", "anchor_text": "Turku Parser"}, {"url": "https://arxiv.org/abs/1902.00972", "anchor_text": "this paper"}, {"url": "https://github.com/CoNLL-UD-2018/UDPipe-Future", "anchor_text": "UDPipe Future"}, {"url": "https://github.com/360er0/COMBO", "anchor_text": "Combo parser"}, {"url": "http://universaldependencies.org/conll18/results-lemmas.html", "anchor_text": "CoNLL evaluation"}, {"url": "http://universaldependencies.org/conll18/results-lemmas.html", "anchor_text": "CoNLL 2018 scoreboard"}, {"url": "https://gist.github.com/erickrf/b3ede1e817c58d0df71dc81ad6f5d8dc", "anchor_text": "at Github"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----f303e8ff1a8---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/naturallanguageprocessing?source=post_page-----f303e8ff1a8---------------naturallanguageprocessing-----------------", "anchor_text": "Naturallanguageprocessing"}, {"url": "https://medium.com/tag/lemmatization?source=post_page-----f303e8ff1a8---------------lemmatization-----------------", "anchor_text": "Lemmatization"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff303e8ff1a8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstate-of-the-art-multilingual-lemmatization-f303e8ff1a8&user=Erick+Fonseca&userId=232a4085d8e9&source=-----f303e8ff1a8---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff303e8ff1a8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstate-of-the-art-multilingual-lemmatization-f303e8ff1a8&user=Erick+Fonseca&userId=232a4085d8e9&source=-----f303e8ff1a8---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff303e8ff1a8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstate-of-the-art-multilingual-lemmatization-f303e8ff1a8&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f303e8ff1a8--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Ff303e8ff1a8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstate-of-the-art-multilingual-lemmatization-f303e8ff1a8&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----f303e8ff1a8---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----f303e8ff1a8--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----f303e8ff1a8--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----f303e8ff1a8--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----f303e8ff1a8--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----f303e8ff1a8--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----f303e8ff1a8--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----f303e8ff1a8--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----f303e8ff1a8--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@erickrfonseca?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@erickrfonseca?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Erick Fonseca"}, {"url": "https://medium.com/@erickrfonseca/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "146 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F232a4085d8e9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstate-of-the-art-multilingual-lemmatization-f303e8ff1a8&user=Erick+Fonseca&userId=232a4085d8e9&source=post_page-232a4085d8e9--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F232a4085d8e9%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstate-of-the-art-multilingual-lemmatization-f303e8ff1a8&user=Erick+Fonseca&userId=232a4085d8e9&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}