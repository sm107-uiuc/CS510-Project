{"url": "https://towardsdatascience.com/evasion-attacks-on-machine-learning-or-adversarial-examples-12f2283e06a1", "time": 1682997173.140243, "path": "towardsdatascience.com/evasion-attacks-on-machine-learning-or-adversarial-examples-12f2283e06a1/", "webpage": {"metadata": {"title": "Evasion attacks on Machine Learning (or \u201cAdversarial Examples\u201d) | by ilmoi | Towards Data Science", "h1": "Evasion attacks on Machine Learning (or \u201cAdversarial Examples\u201d)", "description": "Machine learning is exciting. However, just like any new technology or invention, not only does ML enable new amazing capabilities \u2014 but also, unfortunately, new vulnerabilities. Previously I\u2019ve\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/@iljamoisejevs/what-everyone-forgets-about-machine-learning-974752543849", "anchor_text": "new amazing capabilities \u2014 but also, unfortunately, new vulnerabilities", "paragraph_index": 0}, {"url": "https://medium.com/@iljamoisejevs/will-my-machine-learning-be-attacked-6295707625d8", "anchor_text": "think about these vulnerabilities in a structured way", "paragraph_index": 1}, {"url": "https://arxiv.org/pdf/1712.03141.pdf", "anchor_text": "A paper", "paragraph_index": 4}, {"url": "https://arxiv.org/abs/1312.6199", "anchor_text": "intriguing property", "paragraph_index": 5}, {"url": "https://www.cs.cmu.edu/~sbhagava/papers/face-rec-ccs16.pdf", "anchor_text": "fooling face detection with adversarial glasses", "paragraph_index": 7}, {"url": "https://www.semanticscholar.org/paper/Generating-Natural-Language-Adversarial-Examples-Alzantot-Sharma/c68fbc1f4aa72d30974f8a3071054e3b227137fd", "anchor_text": "sentiment analysis and textual entailment", "paragraph_index": 8}, {"url": "https://arxiv.org/pdf/1801.08917.pdf", "anchor_text": "evading a malware classifier using RL.", "paragraph_index": 9}, {"url": "https://arxiv.org/abs/1312.6199", "anchor_text": "Szegedy\u2019s own paper", "paragraph_index": 11}, {"url": "https://arxiv.org/abs/1412.6572", "anchor_text": "Goodfellow", "paragraph_index": 12}, {"url": "https://arxiv.org/abs/1608.07690", "anchor_text": "tilted boundary", "paragraph_index": 13}, {"url": "https://arxiv.org/abs/1905.02175", "anchor_text": "A recent paper from MIT", "paragraph_index": 15}, {"url": "http://gradientscience.org/adv/", "anchor_text": "11min blog summary", "paragraph_index": 16}, {"url": "https://arxiv.org/abs/1605.07277", "anchor_text": "Previous work", "paragraph_index": 17}, {"url": "https://medium.com/@iljamoisejevs/will-my-machine-learning-be-attacked-6295707625d8", "anchor_text": "capability", "paragraph_index": 18}, {"url": "https://calypsoai.com", "anchor_text": "Calypso", "paragraph_index": 19}, {"url": "https://medium.com/@iljamoisejevs/will-my-machine-learning-be-attacked-6295707625d8", "anchor_text": "WhiteBox attacks", "paragraph_index": 20}, {"url": "https://arxiv.org/abs/1705.07263", "anchor_text": "research", "paragraph_index": 21}, {"url": "https://en.wikipedia.org/wiki/Security_through_obscurity", "anchor_text": "security through obscurity", "paragraph_index": 21}, {"url": "https://medium.com/@iljamoisejevs/will-my-machine-learning-be-attacked-6295707625d8", "anchor_text": "BlackBox type", "paragraph_index": 23}, {"url": "https://arxiv.org/abs/1712.04248", "anchor_text": "Boundary Attack", "paragraph_index": 25}, {"url": "https://arxiv.org/abs/1511.04508", "anchor_text": "Defensive distillation", "paragraph_index": 40}, {"url": "https://arxiv.org/pdf/1607.04311.pdf", "anchor_text": "proved to be false", "paragraph_index": 40}, {"url": "https://arxiv.org/abs/1412.5068", "anchor_text": "autoencoders", "paragraph_index": 42}, {"url": "https://arxiv.org/abs/1712.02976", "anchor_text": "high level representational denoisers", "paragraph_index": 42}, {"url": "https://arxiv.org/abs/1704.01155", "anchor_text": "color bit depth reduction, smoothing", "paragraph_index": 42}, {"url": "https://arxiv.org/abs/1805.06605", "anchor_text": "reforming GANs", "paragraph_index": 42}, {"url": "https://arxiv.org/abs/1608.00853", "anchor_text": "JPEG compression", "paragraph_index": 42}, {"url": "https://arxiv.org/abs/1511.06292", "anchor_text": "foveation", "paragraph_index": 42}, {"url": "https://arxiv.org/abs/1801.08926", "anchor_text": "pixel deflection", "paragraph_index": 42}, {"url": "https://arxiv.org/abs/1803.10840", "anchor_text": "general basis function transformations", "paragraph_index": 42}, {"url": "https://arxiv.org/abs/1711.05929", "anchor_text": "a few", "paragraph_index": 43}, {"url": "https://arxiv.org/abs/1705.09064", "anchor_text": "examples", "paragraph_index": 43}, {"url": "https://arxiv.org/abs/1702.04267", "anchor_text": "here", "paragraph_index": 44}, {"url": "http://calypsoai.com", "anchor_text": "Calypso", "paragraph_index": 45}, {"url": "https://arxiv.org/abs/1703.04318", "anchor_text": "paper", "paragraph_index": 46}, {"url": "https://nicholas.carlini.com/papers/2018_icml_obfuscatedgradients.pdf", "anchor_text": "quickly broken", "paragraph_index": 47}, {"url": "https://github.com/EthicalML/awesome-production-machine-learning#adversarial-robustness-libraries", "anchor_text": "adversarial robustness", "paragraph_index": 49}, {"url": "https://medium.com/@iljamoisejevs/will-my-machine-learning-be-attacked-6295707625d8", "anchor_text": "ML security threat model", "paragraph_index": 50}, {"url": "https://medium.com/@iljamoisejevs/poisoning-attacks-on-machine-learning-1ff247c254db", "anchor_text": "poisoning", "paragraph_index": 50}, {"url": "https://medium.com/@iljamoisejevs/privacy-attacks-on-machine-learning-a1a25e474276", "anchor_text": "privacy", "paragraph_index": 50}], "all_paragraphs": ["Machine learning is exciting. However, just like any new technology or invention, not only does ML enable new amazing capabilities \u2014 but also, unfortunately, new vulnerabilities.", "Previously I\u2019ve discussed how to think about these vulnerabilities in a structured way (or how to develop a \u201cthreat model\u201d for your ML). This time I\u2019d like to dive deep into how your ML system can be exploited during inference time through what is known as an evasion attack.", "With no time to waste, let\u2019s get started.", "An evasion attack happens when the network is fed an \u201cadversarial example\u201d \u2014 a carefully perturbed input that looks and feels exactly the same as its untampered copy to a human \u2014 but that completely throws off the classifier.", "Despite all the hype around adversarial examples being a \u201cnew\u201d phenomenon \u2014 they\u2019re not actually that new. A paper by one of the leading names in Adversarial ML, Battista Biggio, pointed out that the field of attacking machine learning dates back as far as 2004. Back then adversarial examples were studied in the context of spam filtering, showing that linear classifiers could be easily tricked by few carefully-crafted changes in the content of spam emails.", "The \u201cnewness\u201d of adversarial examples really came from them being discovered in the context of deep learning. In 2013 Christian Szegedy of Google AI was working on understanding how neural nets \u201cthink\u201d but instead discovered an intriguing property that all of them seem to possess \u2014 being easily fooled by small perturbations. Given how big deep learning has become \u2014 adversarial example and the field of Adversarial ML have ended up right in the spotlight.", "When talking about adversarial examples I will mainly use computer vision papers \u2014 but really they\u2019re equally applicable to any type of machine learning system, from complicated to not. Some of my personal favorites below:", "Image: fooling face detection with adversarial glasses.", "Language: sentiment analysis and textual entailment.", "Malware: evading a malware classifier using RL.", "It\u2019s a good question. As of today there is no consensus in the community as to why that might be. A number of hypotheses exist.", "The first and original hypothesis trying to explain adversarial examples came from Szegedy\u2019s own paper, where they argued that they exist due to the presence of low-probability \u201cpockets\u201d in the manifold (ie too much non-linearity) and poor regularization of networks.", "Later an opposing theory emerged, pioneered by Goodfellow, arguing that in fact adversarial examples occurred due to too much linearity in modern machine learning and especially deep learning systems. Goodfellow argued that activations functions like ReLU and Sigmoid are basically straight lines in the middle (where, it just so happens we prefer to keep our gradients to prevent them from exploding or vanishing). And so really inside a neural net you have a ton of linear functions, all perpetuating one another\u2019s input, all in the same direction. If you then add tiny perturbations to some of the inputs (a few pixels here and there) that accumulates into massive difference on the other end of the network and it spits out gibberish.", "The third and perhaps most commonly adopted hypothesis today is the tilted boundary. In a nutshell, the authors argue that because the model never fits the data perfectly (otherwise test set accuracy would have always been 100%) \u2014 there will always be adversarial pockets of inputs that exist between the boundary of the classifier and the actual sub-manifold of sampled data. They also empirically debunk the previous two approaches, so if you are technical this paper is worth a read.", "Then there\u2019s a few others, very mathy explanations:", "And finally there\u2019s my personal favorite. A recent paper from MIT argues that adversarial examples are not a bug \u2014 they\u2019re a feature of how neural networks see the world. Just because we humans are limited to 3 dimensions and can\u2019t distinguish noise patterns from one another doesn\u2019t mean those noise patterns are not good features. Our eyes are just bad sensors. So what we\u2019re really dealing with here is a pattern recognition machine more sophisticated than ourselves \u2014 and instead of calling patterns that we don\u2019t understand \u201cadversarial examples\u201d we should just accept there\u2019s more to the world than meets the eye (get it?).", "If the entire paper is too long \u2014 I highly recommend this 11min blog summary.", "That paper, btw, uniquely explains another really interesting phenomenon related to adversarial examples \u2014 their transferability. Previous work pointed out that adversarial example easily transfer between both different models and different datasets. That\u2019s right \u2014 you could have trained a model on dogs vs cats, but then can fool a completely different model trained on hotdogs vs not hotdogs with the same adversarial noise added to the input. Because the MIT paper points out that noise is actually a feature, it makes sense that other images in the image realm (be they cats, dogs, hotdogs or not hotdogs) all share it. Fascinating stuff.", "Attacker\u2019s knowledge of the target system (or their \u201ccapability\u201d) is important. The more they know about your model and how its built \u2014 the easier it is for them to mount an attack on it.", "At Calypso we\u2019ve broken down evasion attacks into five separate classes:", "Gradient-based attacks by definition require access to the model\u2019s gradients and are therefore a type of WhiteBox attacks. These are hands down the most powerful, because the attacker can use their detailed understanding of how the model thinks (its gradients) to mathematically optimize the attack.", "This is also the technique that works best for attacking hardened models. In fact, research has shown that if the attacker has access to the model\u2019s gradients \u2014 they will always be able to craft a new set of adversarial examples to fool the model. This is why adversarial examples are such a big deal \u2014 security through obscurity aside it\u2019s actually pretty hard to defend against them (more on this later).", "The three most powerful gradient-based attacks as of today are:", "Confidence score attacks use the outputted classification confidence to estimate the gradients of the model, and then perform similar smart optimization to gradient-based attacks above. This approach doesn\u2019t require the attacker to know anything about the model and hence is of the BlackBox type.", "The three most powerful confidence-based attacks as of today are:", "Hard label attacks rely solely on the label outputted by the model (\u201ccat\u201d, \u201cdog\u201d, \u201chotdog\u201d) and don\u2019t require the confidence scores. This makes the attack more dumb \u2014 but arguably more realistic (how many public endpoints do you know of that output a classification score?). The most powerful attack in this category remains the Boundary Attack.", "Surrogate model attacks are very similar to gradient-based attacks, except they require an extra step. When the adversary doesn\u2019t have access to the model\u2019s internals but still wants to mount a WhiteBox attack, they can try to first rebuild the target\u2019s model on their machine. They have a few options:", "Finally, there are brute-force attacks. These attacks use no optimization at all to generate adversarial examples and instead resort to simple things like:", "In short \u2014 it\u2019s hard. We\u2019re really between a rock and a hard place here. Let\u2019s examine the two broad categories of defenses \u2014 empirical defenses and formal methods.", "Let\u2019s start with formal methods because they are quicker to discuss. Those of you who worked in industries like chip design or aerospace & defense will be familiar with formal methods. For everyone else \u2014 formal methods are a mathematical technique used to guarantee the robustness of software / hardware systems. In most of software, if you design or build the thing wrong it might lead to some downtime and a few angry customer reviews \u2014 but it probably won\u2019t kill anyone. In some industries, however, such flaws are unacceptable. You can\u2019t manufacture 100m chips only to realize some part of the chip is flawed \u2014 and you can\u2019t put a plane in the air unless you\u2019ve mathematically verified every component works as intended.", "The way formal methods work, very simply, is by attempting every possible scenario and seeing how it plays out. In the world of evasion attacks that means trying to generate every possible adversarial example within a certain radius of perturbation. As an example, imagine you have an image with just two grayscale pixels \u2014 let\u2019s say 180 and 80. Then let\u2019s say you decide on a perturbation range of 3 in each direction. That gives you (3+1+3)\u00b2 or 49 combinations to try out \u2014 and if you wanted to formally verify none of those are adversarial you would have to put each one through your model and see what comes out there other end.", "49 combinations for an image with 2 dimensions and no color. What about a 1000 x 1000px colored image with same perturbation range of 3 in each direction? That\u2019s (3+1+3)^(3*1000*1000) combinations to check for (excel refused to produce a number)!", "Hopefully you can quickly see the problem with formal methods \u2014 they\u2019re not cheap and often completely intractable from a computational perspective. In fact, as applied to neural networks, state of the art formal method techniques today can\u2019t verify a network more than a few layers deep. So for now it\u2019s a worthy \u2014 but elusive aspiration.", "If you did want to read more into formal methods for neural networks, here are some good papers:", "Otherwise let\u2019s move on to empirical defenses. Empirical defenses are, as the name suggests, relying on experiments to demonstrate the effectiveness of a defense. Eg you might attack a model twice \u2014 first a normal, undefended version, then a hardened version \u2014 and observe how easy each is to fool (hopefully the hardened model performs better). So where formal methods are trying to compute every possible scenario and in doing so verify no adversarial examples exist \u2014 empirical methods take the approach of \u201cyou see it works, why do the math\u201d.", "Let\u2019s go through some of the more popular types:", "Adversarial training \u2014 hands down the most talked-about and arguably most effective defense today. During adversarial training the defender retrains the model with adversarial examples included in the training pool, but labelled with correct labels. This teaches the model to ignore the noise and only learn from \u201crobust\u201d features.", "The problem with adversarial training is that it only defends your model against the same attacks used to craft the examples originally included in the training pool. Thus if someone then mounts an attack optimized using a different algorithm, or mounts an adaptive attack (ie a whitebox attack on a defended model) \u2014 they are able to fool the classifier anew, as if no defense was in place.", "You might think that, well, you could keep retraining your model including newly forged adversarial examples over and over again \u2014 but at some point you\u2019ve inserted so much fake data into the training set that the boundary the model learns basically becomes useless.", "That said, if the goal is to simply make it harder for the attacker to bypass the classifier \u2014 then adversarial training is a solid choice. There\u2019s also a lot of creative approaches to actually doing it:", "Next, gradient masking. It\u2019s basically a non-defense. For a while the Adversarial ML community thought that because gradients are needed to compute powerful attacks on models, hiding the gradients should solve the issue. Defensive distillation was very popular \u2014 but quickly proved to be false.", "The reason gradient masking doesn\u2019t work as a defense tactic is because of adversarial examples\u2019 transferability property. Even if you succeed at hiding the gradients of your model \u2014 the attacker can still build a surrogate, attack it, then transfer the examples.", "Input modification happens when an input, before being passed to the model, is in some way \u201ccleaned\u201d to get rid of adversarial noise. Examples include all sorts of denoising solutions (autoencoders, high level representational denoisers), color bit depth reduction, smoothing, reforming GANs, JPEG compression, foveation, pixel deflection, general basis function transformations, and many others.", "Next, detection. Some detection methods are closely related to input modification \u2014 as once an input has been cleaned its prediction can be compared with the original prediction, and if the two are far apart it\u2019s likely that the input has been tampered with. Here are a few examples.", "Others are different. Eg here they actually train a separate detection network whose only job is to decide if an input is adversarial. Then there is a number of detection defenses that examine raw statistics computed at various points in the input feeding / prediction process:", "In general input modification & detection methods are great because they can be applied to an already trained model and don\u2019t require the data scientist to go back to square 0. We are big fans of these kinds of defenses here at Calypso.", "Finally, there is the extra (NULL) class approach. The idea is simple. Classifiers are trained on a very particular data distribution and by definition are clueless when taken outside the bounds of that. So instead of forcing the classifier to guess the label when it clearly doesn\u2019t know what it is \u2014 why not give it the option of abstaining. That\u2019s what the NULL class enables (paper).", "All in all empirical defenses are imperfect \u2014 but they work, and sometimes only take a few lines of code. Yes, admittedly, they are a never ending cat-and-mouse game with the attacker and new ones are often quickly broken \u2014 but if you think about attackers like electricity (always take the path of least resistance) \u2014 our job is to just make their lives just hard enough.", "Phew! This has bee a long post \u2014 but hopefully you found it interesting and useful.", "Final note, if you\u2019re an engineer rather than a mathematician and prefer to learn through doing \u2014 Ethical Institute for AI & Machine Learning has an excellent collection of open source tools related to adversarial robustness. Go ahead and play around.", "Otherwise, if you\u2019re done with evasion attacks but want to know more about ML security \u2014 I recommend reading my other posts on the ML security threat model and poisoning and privacy attacks.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F12f2283e06a1&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fevasion-attacks-on-machine-learning-or-adversarial-examples-12f2283e06a1&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fevasion-attacks-on-machine-learning-or-adversarial-examples-12f2283e06a1&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fevasion-attacks-on-machine-learning-or-adversarial-examples-12f2283e06a1&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fevasion-attacks-on-machine-learning-or-adversarial-examples-12f2283e06a1&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----12f2283e06a1--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----12f2283e06a1--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://ilmoi.medium.com/?source=post_page-----12f2283e06a1--------------------------------", "anchor_text": ""}, {"url": "https://ilmoi.medium.com/?source=post_page-----12f2283e06a1--------------------------------", "anchor_text": "ilmoi"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb7592f20afda&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fevasion-attacks-on-machine-learning-or-adversarial-examples-12f2283e06a1&user=ilmoi&userId=b7592f20afda&source=post_page-b7592f20afda----12f2283e06a1---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F12f2283e06a1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fevasion-attacks-on-machine-learning-or-adversarial-examples-12f2283e06a1&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F12f2283e06a1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fevasion-attacks-on-machine-learning-or-adversarial-examples-12f2283e06a1&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/tagged/security-for-ml", "anchor_text": "Security for Machine Learning"}, {"url": "https://thumbs.gfycat.com/CrazySpanishCero-mobile.jpg", "anchor_text": "Source."}, {"url": "https://medium.com/@iljamoisejevs/what-everyone-forgets-about-machine-learning-974752543849", "anchor_text": "new amazing capabilities \u2014 but also, unfortunately, new vulnerabilities"}, {"url": "https://medium.com/@iljamoisejevs/will-my-machine-learning-be-attacked-6295707625d8", "anchor_text": "think about these vulnerabilities in a structured way"}, {"url": "https://arxiv.org/abs/1412.6572", "anchor_text": "Source."}, {"url": "https://arxiv.org/pdf/1712.03141.pdf", "anchor_text": "A paper"}, {"url": "https://arxiv.org/abs/1312.6199", "anchor_text": "intriguing property"}, {"url": "https://www.cs.cmu.edu/~sbhagava/papers/face-rec-ccs16.pdf", "anchor_text": "fooling face detection with adversarial glasses"}, {"url": "https://github.com/advboxes/AdvBox/blob/master/applications/StealthTshirt/README.md", "anchor_text": "stealth T-shirt at DEFCON"}, {"url": "https://nicholas.carlini.com/code/audio_adversarial_examples/", "anchor_text": "speech-to-text transcription."}, {"url": "https://www.semanticscholar.org/paper/Generating-Natural-Language-Adversarial-Examples-Alzantot-Sharma/c68fbc1f4aa72d30974f8a3071054e3b227137fd", "anchor_text": "sentiment analysis and textual entailment"}, {"url": "https://openai.com/blog/faulty-reward-functions/", "anchor_text": "boat racing"}, {"url": "https://arxiv.org/pdf/1801.08917.pdf", "anchor_text": "evading a malware classifier using RL."}, {"url": "https://arxiv.org/abs/1312.6199", "anchor_text": "Szegedy\u2019s own paper"}, {"url": "https://arxiv.org/abs/1412.6572", "anchor_text": "Goodfellow"}, {"url": "https://arxiv.org/abs/1608.07690", "anchor_text": "tilted boundary"}, {"url": "https://arxiv.org/abs/1804.11285", "anchor_text": "Lack of sufficient training data"}, {"url": "https://arxiv.org/abs/1805.10204", "anchor_text": "Computational intractability of ever building robust classifiers"}, {"url": "https://arxiv.org/abs/1905.02175", "anchor_text": "A recent paper from MIT"}, {"url": "https://arxiv.org/abs/1905.02175", "anchor_text": "Source."}, {"url": "http://gradientscience.org/adv/", "anchor_text": "11min blog summary"}, {"url": "https://arxiv.org/abs/1605.07277", "anchor_text": "Previous work"}, {"url": "https://medium.com/@iljamoisejevs/will-my-machine-learning-be-attacked-6295707625d8", "anchor_text": "capability"}, {"url": "https://calypsoai.com", "anchor_text": "Calypso"}, {"url": "https://medium.com/@iljamoisejevs/will-my-machine-learning-be-attacked-6295707625d8", "anchor_text": "WhiteBox attacks"}, {"url": "https://arxiv.org/abs/1705.07263", "anchor_text": "research"}, {"url": "https://en.wikipedia.org/wiki/Security_through_obscurity", "anchor_text": "security through obscurity"}, {"url": "https://arxiv.org/abs/1709.04114", "anchor_text": "EAD"}, {"url": "https://arxiv.org/abs/1705.07263", "anchor_text": "C&W"}, {"url": "https://arxiv.org/abs/1706.06083", "anchor_text": "Madry"}, {"url": "https://medium.com/@iljamoisejevs/will-my-machine-learning-be-attacked-6295707625d8", "anchor_text": "BlackBox type"}, {"url": "https://arxiv.org/abs/1708.03999", "anchor_text": "ZOO"}, {"url": "https://arxiv.org/abs/1802.05666", "anchor_text": "SPSA"}, {"url": "https://arxiv.org/abs/1804.08598", "anchor_text": "NES"}, {"url": "https://arxiv.org/abs/1712.04248", "anchor_text": "Boundary Attack"}, {"url": "https://medium.com/@iljamoisejevs/privacy-attacks-on-machine-learning-a1a25e474276", "anchor_text": "here"}, {"url": "https://arxiv.org/abs/1712.02779", "anchor_text": "Randomly rotating / translating the image"}, {"url": "https://arxiv.org/abs/1807.01697", "anchor_text": "Applying common perturbations"}, {"url": "https://arxiv.org/abs/1901.10513", "anchor_text": "Adding Gaussian noise with large SD"}, {"url": "https://arxiv.org/abs/1702.01135", "anchor_text": "Reluplex: An Efficient SMT Solver for Verifying Deep Neural Networks"}, {"url": "https://arxiv.org/abs/1610.06940", "anchor_text": "Safety Verification of Deep Neural Networks"}, {"url": "https://arxiv.org/abs/1711.07356", "anchor_text": "Evaluating Robustness of Neural Networks with Mixed Integer Programming"}, {"url": "https://arxiv.org/abs/1705.07204", "anchor_text": "Ensemble AT"}, {"url": "https://arxiv.org/abs/1708.02582", "anchor_text": "Cascade AT"}, {"url": "https://arxiv.org/abs/1706.06083", "anchor_text": "Robust optimization approach to AT"}, {"url": "https://arxiv.org/abs/1811.07457", "anchor_text": "AT via spectral normalization"}, {"url": "https://arxiv.org/abs/1511.04508", "anchor_text": "Defensive distillation"}, {"url": "https://arxiv.org/pdf/1607.04311.pdf", "anchor_text": "proved to be false"}, {"url": "https://arxiv.org/abs/1412.5068", "anchor_text": "autoencoders"}, {"url": "https://arxiv.org/abs/1712.02976", "anchor_text": "high level representational denoisers"}, {"url": "https://arxiv.org/abs/1704.01155", "anchor_text": "color bit depth reduction, smoothing"}, {"url": "https://arxiv.org/abs/1805.06605", "anchor_text": "reforming GANs"}, {"url": "https://arxiv.org/abs/1608.00853", "anchor_text": "JPEG compression"}, {"url": "https://arxiv.org/abs/1511.06292", "anchor_text": "foveation"}, {"url": "https://arxiv.org/abs/1801.08926", "anchor_text": "pixel deflection"}, {"url": "https://arxiv.org/abs/1803.10840", "anchor_text": "general basis function transformations"}, {"url": "https://arxiv.org/abs/1711.05929", "anchor_text": "a few"}, {"url": "https://arxiv.org/abs/1705.09064", "anchor_text": "examples"}, {"url": "https://arxiv.org/abs/1702.04267", "anchor_text": "here"}, {"url": "https://arxiv.org/abs/1702.06280", "anchor_text": "input itself"}, {"url": "https://arxiv.org/abs/1612.07767", "anchor_text": "convolutional filters"}, {"url": "https://arxiv.org/abs/1704.00103", "anchor_text": "ReLU activations"}, {"url": "https://arxiv.org/abs/1803.06373", "anchor_text": "logits"}, {"url": "http://calypsoai.com", "anchor_text": "Calypso"}, {"url": "https://arxiv.org/abs/1703.04318", "anchor_text": "paper"}, {"url": "https://nicholas.carlini.com/papers/2018_icml_obfuscatedgradients.pdf", "anchor_text": "quickly broken"}, {"url": "https://github.com/EthicalML/awesome-production-machine-learning#adversarial-robustness-libraries", "anchor_text": "adversarial robustness"}, {"url": "https://medium.com/@iljamoisejevs/will-my-machine-learning-be-attacked-6295707625d8", "anchor_text": "ML security threat model"}, {"url": "https://medium.com/@iljamoisejevs/poisoning-attacks-on-machine-learning-1ff247c254db", "anchor_text": "poisoning"}, {"url": "https://medium.com/@iljamoisejevs/privacy-attacks-on-machine-learning-a1a25e474276", "anchor_text": "privacy"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----12f2283e06a1---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/security-for-ml?source=post_page-----12f2283e06a1---------------security_for_ml-----------------", "anchor_text": "Security For Ml"}, {"url": "https://medium.com/tag/security?source=post_page-----12f2283e06a1---------------security-----------------", "anchor_text": "Security"}, {"url": "https://medium.com/tag/adversarial-example?source=post_page-----12f2283e06a1---------------adversarial_example-----------------", "anchor_text": "Adversarial Example"}, {"url": "https://medium.com/tag/adversarial-attack?source=post_page-----12f2283e06a1---------------adversarial_attack-----------------", "anchor_text": "Adversarial Attack"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F12f2283e06a1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fevasion-attacks-on-machine-learning-or-adversarial-examples-12f2283e06a1&user=ilmoi&userId=b7592f20afda&source=-----12f2283e06a1---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F12f2283e06a1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fevasion-attacks-on-machine-learning-or-adversarial-examples-12f2283e06a1&user=ilmoi&userId=b7592f20afda&source=-----12f2283e06a1---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F12f2283e06a1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fevasion-attacks-on-machine-learning-or-adversarial-examples-12f2283e06a1&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----12f2283e06a1--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F12f2283e06a1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fevasion-attacks-on-machine-learning-or-adversarial-examples-12f2283e06a1&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----12f2283e06a1---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----12f2283e06a1--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----12f2283e06a1--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----12f2283e06a1--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----12f2283e06a1--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----12f2283e06a1--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----12f2283e06a1--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----12f2283e06a1--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----12f2283e06a1--------------------------------", "anchor_text": ""}, {"url": "https://ilmoi.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://ilmoi.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "ilmoi"}, {"url": "https://ilmoi.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "248 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb7592f20afda&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fevasion-attacks-on-machine-learning-or-adversarial-examples-12f2283e06a1&user=ilmoi&userId=b7592f20afda&source=post_page-b7592f20afda--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F773be8569eaf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fevasion-attacks-on-machine-learning-or-adversarial-examples-12f2283e06a1&newsletterV3=b7592f20afda&newsletterV3Id=773be8569eaf&user=ilmoi&userId=b7592f20afda&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}