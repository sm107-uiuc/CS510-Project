{"url": "https://towardsdatascience.com/probability-learning-vi-hidden-markov-models-fab5c1f0a31d", "time": 1683001271.868553, "path": "towardsdatascience.com/probability-learning-vi-hidden-markov-models-fab5c1f0a31d/", "webpage": {"metadata": {"title": "Probability Learning VI: Hidden Markov Models | by James Thorn | Towards Data Science", "h1": "Probability Learning VI: Hidden Markov Models", "description": "Hello again friends! This is post number six of our Probability Learning series, listed here in case you have missed any of the previous articles: I deeply encourage you to read them, as they are fun\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/probability-learning-iii-maximum-likelihood-e78d5ebea80c", "anchor_text": "Maximum Likelihood estimation, which was fully described in one of my previous articles", "paragraph_index": 23}, {"url": "https://ieeexplore.ieee.org/document/480098", "anchor_text": "Real-time on-line unconstrained handwriting recognition using statistical methods", "paragraph_index": 43}, {"url": "https://www.intechopen.com/books/hidden-markov-models-theory-and-applications/modeling-of-speech-parameter-sequence-considering-global-variance-for-hmm-based-speech-synthesis", "anchor_text": "Modelling of Speech Parameter Sequence Considering Global Variance for HMM-Based Speech Synthesis", "paragraph_index": 46}, {"url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5172443/#bib36", "anchor_text": "computational biology", "paragraph_index": 48}, {"url": "https://medium.com/@jaimezornoza", "anchor_text": "follow me on Medium", "paragraph_index": 49}, {"url": "https://www.linkedin.com/in/jaime-zornoza/", "anchor_text": "LinkedIn", "paragraph_index": 50}, {"url": "https://medium.com/@jaimezornoza", "anchor_text": "here", "paragraph_index": 50}, {"url": "https://howtolearnmachinelearning.com/books/machine-learning-books/", "anchor_text": "How to Learn Machine Learning", "paragraph_index": 52}, {"url": "https://aigents.co/", "anchor_text": "AIgents.co \u2014 A career community for Data Scientists & Machine Learning Engineers", "paragraph_index": 52}], "all_paragraphs": ["Hello again friends! This is post number six of our Probability Learning series, listed here in case you have missed any of the previous articles:", "I deeply encourage you to read them, as they are fun and full of useful information about probabilistic Machine Learning. However, if you don\u00b4t want to read them, that is absolutely fine, this article can be understood without having devoured the rest with only a little knowledge of probability.", "Also, do not fear, I will not include any complex math in this article: it\u2019s intention is to lay the theoretical background of hidden Markov models, show how they can be used, and talk about some of its applications.", "We have already met Reverend Bayes, and today we are going to meet another very influential individual in the world of game theory and probability. This is no other than Andr\u00e9i M\u00e1rkov, they guy who put the Markov in Hidden Markov models, Markov Chains\u2026", "Hidden Markov models are a branch of the probabilistic Machine Learning world, that are very useful for solving problems that involve working with sequences, like Natural Language Processing problems, or Time Series. In a moment, we will see just why this is, but first, lets get to know Markov a little bit.", "Lastly, before we start, here you have some additional resources to skyrocket your Machine Learning career:", "Andrei Markov (1856\u20131922) was a Russian mathematician who taught probability theory in the University of St Petersburg, and was also a very politically active individual. He worked with continuous fractions, the central limit theorem, and other mathematical endeavours, however, he will mostly be remembered because of his work on probability theory, specifically on the study of stochastic processes; the Markov Chains that we will discuss in just a moment.", "Lets start with the most basic element of Markov\u00b4s proposal: the Markov Chain. In probability theory, a Markov Chain or Markov Model is an special type of discrete stochastic process in which the probability of an event occurring only depends on the immediately previous event.", "The underlying assumption is that the \u201cfuture is independent of the past given the present\u201d. In other words, if we know the present state or value of a system or variable, we do not need any past information to try to predict the future states or values.", "Markov chains are generally defined by a set of states and the transition probabilities between each state. In the example above, a two state Markov Chain is displayed: We have states A and B and four transition probabilities: from A to A again, from A to B, from B to A and from B to B again. These transition probabilities are usually represented in the form of a Matrix, called the Transition Matrix, also called the Markov Matrix.", "The element ij is the probability of transiting from state j to state i. In some cases transposed notation is used, so that element ij represents the probability of going from state i to state j. Because of this I added the \u2018to\u2019 and \u2018from\u2019 just to clarify. Overall, the system would look something like this:", "How do we calculate these probabilities? The answer is one that you\u00b4ve probably heard already a million times: from data.", "Imagine the states we have in our Markov Chain are Sunny and Rainy. To calculate the transition probabilities from one to another we just have to collect some data that is representative of the problem that we want to address, count the number of transitions from one state to another, and normalise the measurements. The following figure shows how this would be done for our example.", "At the moment Markov Chains look just like any other state machine, in which we have states and transitions in between them. However, later in this article we will see just how special they are.", "Okay, now that we know what a Markov Chain is, and how to calculate the transitions probabilities involved, lets carry on and learn about Hidden Markov Models.", "Hidden Markov Models are probabilistic models that attempt to find the value or the probability of certain hidden variables having a certain value, based on some other observed variables. These variables are commonly referred to as hidden states and observed states.", "The state of a system might only be partially observable, or not observable at all, and we might have to infer its characteristics based on another fully observable system or variable.", "Imagine, using the previous example, that we add the following information. Every day, there is a probability that we get a phone call from our best friend, John who lives in a different continent, and this probability depends on the weather conditions of such day. Using the latter information (if we get a phone call or not -the observed variables) we would like to infer the former (the weather in the continent where John lives \u2014 the hidden variables)", "The probabilities shown here, that define how likely is John to call us on a given day depending on the weather of such day are called emission probabilities. They define the probability of seeing certain observed variable given a certain value for the hidden variables.", "Knowing these probabilities, along with the transition probabilities we calculated before, and the prior probabilities of the hidden variables (how likely it is to be sunny or rainy), we could try to find out what the weather of a certain period of time was, knowing in which days John gave us a phone call.", "Lets see how we would solve this problem with simple statistics: Imagine John did not phone us for two days in a row. What is the most likely weather scenario? For this, we first need to calculate the prior probabilities (that is, the probability of being sunny or rainy previous to any actual observation), which we obtain from the same observations as the transitions probabilities.", "Now, we are ready to solve our problem: for two days in a row, we did not get a single sign that John is alive. What is the most likely weather scenario then? As we can see in the image below, we have 4 possible situations to consider: sunny followed by sunny, sunny followed by rainy, rainy followed by sunny and lastly rainy followed by rainy.", "In the image above, we have chosen the second option (sunny and then rainy) and using the prior probability (probability of the first day being sunny without any observation), the transition probability from sunny to rainy, and the emission probabilities of not getting phoned on both conditions, we have calculated the probability of the whole thing happening by simply multiplying all these aforementioned probabilities.", "We would have to do this for every possible weather scenario (3 left in our case) and at the end we would choose the one that yields the highest probability. (This is called Maximum Likelihood estimation, which was fully described in one of my previous articles).", "For a sequence of two days we would have to calculate four possible scenarios. For three days, we would have eight scenarios. For Four days sixteen. If we wanted to calculate the weather for a full week, we would have one hundred and twenty eight different scenarios. With this exponential growth in the number of possible situations, it is easy to see how this can get out of hand, driving us towards the use of more practical and intelligent techniques.", "It is not only that we have more scenarios, but in each scenario we have more calculations, as there are more transitions and more emission probabilities present in the chain.", "This is where Markov Chains come in handy.", "Lets refresh the fundamental assumption of a Markov Chain: \u201cfuture is independent of the past given the present\u201d.", "Knowing this, the operating principle of a Hidden Markov model is that instead of calculating the probabilities of many different scenarios, it gradually stores the probabilities of chains of scenarios starting from a length 1 to the n-1, being n the length of the chain for which we want to infer the hidden states.", "What does this mean? Imagine we want to calculate the weather conditions for a whole week knowing the days John has called us. To calculate the weather conditions for the last day, we would calculate the probability of that day being sunny given the best path leading up to a sunny Sunday, do the same for a rainy Sunday and just pick the highest one.", "This largely simplifies the previous problem.", "Recursively, to calculate the probability of Saturday being sunny and rainy, we would do the same, considering the best path up to one day less. This means that on any given day, to calculate the probabilities of the possible weather scenarios for the next day we would only be considering the best of the probabilities reached on that single day \u2014 no previous information.", "In practice this is done by starting in the first time step, calculating the probabilities of observing the hidden states, and picking the best one. Then, using that best one we do the same for the following day and so on. Lets see how this is done for our particular example.", "Using the prior probabilities and the emission probabilities we calculate how likely it is to be sunny or rainy for the first day.", "Lets see how we would carry on for the next day: using the best previously calculated probabilities for sunny and rainy, we would calculate the same for the next day, but instead of using the priors we used last time, we will use the best calculated probability for sunny and for rainy.", "To do this we first see what the actual observation is: lets say Monday was sunny. That happened with a probability of 0,375. Now, lets go to Tuesday being sunny: we have to multiply the probability of Monday being sunny times the transition probability from sunny to sunny, times the emission probability of having a sunny day and not being phoned by John. This gives us a probability value of 0,1575.", "Now, lets say Monday was rainy. What is the chance that Tuesday will be sunny? For this we multiply the highest probability of rainy Monday (0.075) times the transition probability from rainy to sunny (0.4) times the emission probability of being sunny and not receiving a phone call, just like last time. This results in a probability of 0.018, and because the previous one we calculated (Monday sunny and Tuesday sunny) was higher (it was 0.1575), we will keep the former one.", "We would have to do the same for a rainy Tuesday now, keeping the highest of both calculated probabilities. If we continue this chain, calculating the probabilities for Wednesday now:", "If we do this for the whole week, we get the most likely weather conditions for the seven days, shown in the following figure:", "With this procedure, we can infer the most likely weather conditions for any time period, knowing only if John has called us and some prior information coming from historical data.", "That is it! Now that you know the basic principals behind Hidden Markov Models, lets see some of its actual applications.", "As mentioned previously, HMMs are very good when working with sequences. Because of this, they are widely used in Natural Language Processing, where phrases can be considered sequences of words.", "HMMs are used for many NLP applications, but lets cite a few to consolidate the idea in your minds with some concrete examples.", "The paper \u00b4Real-time on-line unconstrained handwriting recognition using statistical methods\u00b4 speaks about the use of HMMs for translating hand written documents into digital text.", "I have an app on my phone called \u2018Pen to Print\u2019 that does exactly this. It takes a handwritten text as an input, breaks it down into different lines and then converts the whole thing into a digital format. The following image shows an example of this.", "There are lots of apps like this and, and are most times they use some probabilistic approach like the Hidden Markov Models we have seen.", "Another paper, \u00b4Modelling of Speech Parameter Sequence Considering Global Variance for HMM-Based Speech Synthesis\u00b4 does something similar but with speech instead of text.", "Think that they way all of our virtual assistants like Siri, Alexa, Cortana and so on work with under the following process: you wake them up with a certain \u00b4call to action\u00b4phrase, and they start actively listening (or so they say). After this, anything that you say, like a request for certain kind of music, gets picked up by the microphone and translated from speech to text. Then this texts gets processed and we get the desired output.", "Other uses of HMMs range from computational biology to online marketing or discovering purchase causality for online stores.", "We have seen what Hidden Markov models are, and various applications where they are used to tackle real problems. More Probability Learning posts will come in the future so to check them out follow me on Medium, and stay tuned!", "That is all, I hope you liked the post. Feel Free to connect with me on LinkedIn or follow me on Twitter at @jaimezorno. Also, you can take a look at my other posts on Data Science and Machine Learning here. Have a good read!", "In case you want to learn a little bit more, clarify your learning from this post, or go deep into the maths of HMMs, I have left some information here which I think could be of great use.", "For further resources on Machine Learning and Data Science check out the following repository: How to Learn Machine Learning! For career resources (jobs, events, skill tests) go to AIgents.co \u2014 A career community for Data Scientists & Machine Learning Engineers.", "Enjoy and feel free to contact me with any doubts!", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Ffab5c1f0a31d&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprobability-learning-vi-hidden-markov-models-fab5c1f0a31d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprobability-learning-vi-hidden-markov-models-fab5c1f0a31d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprobability-learning-vi-hidden-markov-models-fab5c1f0a31d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprobability-learning-vi-hidden-markov-models-fab5c1f0a31d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----fab5c1f0a31d--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----fab5c1f0a31d--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://james-thorn.medium.com/?source=post_page-----fab5c1f0a31d--------------------------------", "anchor_text": ""}, {"url": "https://james-thorn.medium.com/?source=post_page-----fab5c1f0a31d--------------------------------", "anchor_text": "James Thorn"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1fd70d25ff14&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprobability-learning-vi-hidden-markov-models-fab5c1f0a31d&user=James+Thorn&userId=1fd70d25ff14&source=post_page-1fd70d25ff14----fab5c1f0a31d---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffab5c1f0a31d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprobability-learning-vi-hidden-markov-models-fab5c1f0a31d&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffab5c1f0a31d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprobability-learning-vi-hidden-markov-models-fab5c1f0a31d&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/probability-learning-i-bayes-theorem-708a4c02909a", "anchor_text": "Probability Learning I: Bayes\u2019 Theorem"}, {"url": "https://towardsdatascience.com/probability-learning-ii-how-bayes-theorem-is-applied-in-machine-learning-bd747a960962", "anchor_text": "Probability Learning II: How Bayes\u2019 Theorem is applied in Machine Learning"}, {"url": "https://towardsdatascience.com/probability-learning-iii-maximum-likelihood-e78d5ebea80c", "anchor_text": "Probability Learning III: Maximum Likelihood"}, {"url": "https://towardsdatascience.com/probability-learning-iv-the-math-behind-bayes-bfb94ea03dd8", "anchor_text": "Probability Learning IV: The Math Behind Bayes"}, {"url": "https://towardsdatascience.com/probability-learning-v-naive-bayes-7f1d0466f5f1", "anchor_text": "Probability Learning V: Naive Bayes"}, {"url": "https://howtolearnmachinelearning.com/books/machine-learning-books/", "anchor_text": "How to Learn Machine Learning"}, {"url": "https://aigents.co/", "anchor_text": "AIgents.co \u2014 A career community for Data Scientists & Machine Learning Engineers"}, {"url": "https://towardsdatascience.com/probability-learning-iii-maximum-likelihood-e78d5ebea80c", "anchor_text": "Maximum Likelihood estimation, which was fully described in one of my previous articles"}, {"url": "https://ieeexplore.ieee.org/document/480098", "anchor_text": "Real-time on-line unconstrained handwriting recognition using statistical methods"}, {"url": "https://www.intechopen.com/books/hidden-markov-models-theory-and-applications/modeling-of-speech-parameter-sequence-considering-global-variance-for-hmm-based-speech-synthesis", "anchor_text": "Modelling of Speech Parameter Sequence Considering Global Variance for HMM-Based Speech Synthesis"}, {"url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5172443/#bib36", "anchor_text": "computational biology"}, {"url": "https://medium.com/@jaimezornoza", "anchor_text": "follow me on Medium"}, {"url": "https://www.linkedin.com/in/jaime-zornoza/", "anchor_text": "LinkedIn"}, {"url": "https://medium.com/@jaimezornoza", "anchor_text": "here"}, {"url": "http://setosa.io/ev/markov-chains/", "anchor_text": "Great interactive explanation of Markov Chains"}, {"url": "https://www.youtube.com/watch?v=kqSzLo9fenk", "anchor_text": "Youtube video on Hidden Markov Models"}, {"url": "https://medium.com/@jonathan_hui/machine-learning-hidden-markov-model-hmm-31660d217a61", "anchor_text": "Medium post describing the maths behind HMMs"}, {"url": "https://howtolearnmachinelearning.com/online-courses/statistics-and-probability-courses/", "anchor_text": "The best statistics and probability courses reviewed"}, {"url": "https://howtolearnmachinelearning.com/books/machine-learning-books/", "anchor_text": "How to Learn Machine Learning"}, {"url": "https://aigents.co/", "anchor_text": "AIgents.co \u2014 A career community for Data Scientists & Machine Learning Engineers"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----fab5c1f0a31d---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----fab5c1f0a31d---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----fab5c1f0a31d---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----fab5c1f0a31d---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/startup?source=post_page-----fab5c1f0a31d---------------startup-----------------", "anchor_text": "Startup"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffab5c1f0a31d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprobability-learning-vi-hidden-markov-models-fab5c1f0a31d&user=James+Thorn&userId=1fd70d25ff14&source=-----fab5c1f0a31d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffab5c1f0a31d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprobability-learning-vi-hidden-markov-models-fab5c1f0a31d&user=James+Thorn&userId=1fd70d25ff14&source=-----fab5c1f0a31d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffab5c1f0a31d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprobability-learning-vi-hidden-markov-models-fab5c1f0a31d&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----fab5c1f0a31d--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Ffab5c1f0a31d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprobability-learning-vi-hidden-markov-models-fab5c1f0a31d&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----fab5c1f0a31d---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----fab5c1f0a31d--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----fab5c1f0a31d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----fab5c1f0a31d--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----fab5c1f0a31d--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----fab5c1f0a31d--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----fab5c1f0a31d--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----fab5c1f0a31d--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----fab5c1f0a31d--------------------------------", "anchor_text": ""}, {"url": "https://james-thorn.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://james-thorn.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "James Thorn"}, {"url": "https://james-thorn.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "3K Followers"}, {"url": "https://howtolearnmachinelearning.com/", "anchor_text": "https://howtolearnmachinelearning.com/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1fd70d25ff14&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprobability-learning-vi-hidden-markov-models-fab5c1f0a31d&user=James+Thorn&userId=1fd70d25ff14&source=post_page-1fd70d25ff14--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F5ad84c2cef18&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fprobability-learning-vi-hidden-markov-models-fab5c1f0a31d&newsletterV3=1fd70d25ff14&newsletterV3Id=5ad84c2cef18&user=James+Thorn&userId=1fd70d25ff14&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}