{"url": "https://towardsdatascience.com/patient-knowledge-distillation-88969fffb4ba", "time": 1683003819.007024, "path": "towardsdatascience.com/patient-knowledge-distillation-88969fffb4ba/", "webpage": {"metadata": {"title": "Patient Knowledge Distillation. Making large language models smaller\u2026 | by Rohit Pillai | Towards Data Science", "h1": "Patient Knowledge Distillation", "description": "With the advent of deep learning, newer and more complex models are constantly improving performance on a variety of tasks. However, this improvement comes at the cost of computational and storage\u2026"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/pdf/1908.09355.pdf", "anchor_text": "link", "paragraph_index": 8}, {"url": "http://aka.ms/mmai", "anchor_text": "here", "paragraph_index": 8}, {"url": "https://medium.com/@rohit.rameshp", "anchor_text": "https://medium.com/@rohit.rameshp", "paragraph_index": 10}], "all_paragraphs": ["With the advent of deep learning, newer and more complex models are constantly improving performance on a variety of tasks. However, this improvement comes at the cost of computational and storage resources. Most of these models have millions or billions of parameters and take days to train with extremely powerful machines that have several GPUs. For example, the work by Krizhevsky et al. achieved breakthrough results in the 2012 ImageNet Challenge using a network containing 60 million parameters with five convolutional layers and three fully connected layers.", "This gave rise to a new challenge: putting these models on devices like phones, which typically have significantly less memory and computational resources than the servers that models are trained on. To solve this, researchers started exploring the idea of model compression, which focused on reducing the model size without losing too much accuracy. Several generic techniques were devised for this including quantization (reducing the number of bits required to store each weight), parameter sharing (this reduces the overall parameters needed by the model) and knowledge distillation (creating a new \u201cstudent\u201d model with fewer layers but similar accuracy as the original \u201cteacher\u201d model).", "In this work, the Microsoft Dynamics 365 AI Research team focuses mainly on compressing state of the art language models like GPT and BERT, which are huge but form the basis of most NLP model. For instance, BERT-base has 109 million parameters, and BERT-large has around 330 million parameters. To make these models smaller, they propose a new method of Knowledge Distillation (KD) to compress the model thereby saving running time and memory without losing too much accuracy.", "When a language model is compressed by traditional KD techniques, it loses a lot of accuracy when applied to downstream tasks like sentence classification. This is because previous KD methods focused on teaching the student model the final probability logit that the teacher predicted without considering any of the hidden layer values. This is analogous to a real-life scenario where a child learns what the final answer to a question is but has no clue how they got to that answer, the child is more likely to make a mistake when confronted with a slightly different problem. This is true of the student model that is learned using traditional KD methods as well. Based on this assumption, our paper proposes a loss function that not only tries to match the student model\u2019s final probabilities to those of the teacher model, but also tries to match their hidden representations, thereby making the student model\u2019s generalization ability stronger. The model is called \u201cPatient Knowledge Distillation\u201d (PKD).", "This work focuses specifically on the BERT language model being used for sentence classification. In this setting, the model\u2019s prediction is based on the feature representation of the [CLS] characters (a special character which captures the contextual sentence encoding) at every layer, such as adding two layers of full connections to this feature. Therefore, PKD proposes a new loss function that enables the model to learn the feature representation of each layers\u2019 [CLS] character at the same time:", "where M is the number of layers of the student (such as 3, 6), N is the number of layers of the teacher model (such as 12, 24), h is the representation of [CLS] hidden layers in the model, and i, j represent the student-teacher The corresponding relationship of the hidden layers is shown in the figure below. For example, when a 6-layer student model is being trained using a 12-layer teacher model, the student model could learn the representation of every alternate hidden layer of the teacher model (2, 4, 6, 8, 10 in this case) called PKD-skip, or it could learn the representations of the last few layers of the teacher model (7,8,9,10,11 in this case), called PKD-last. We don\u2019t consider learning the final (12th) layer of the teacher model but instead directly learn the final probability predictions, which encompasses the final layer\u2019s hidden representation as well.", "The team applies this technique to compress a 12-layer BERT-base model to both a 6 layer and 3 layer BERT model using both the methods outlined above. They find that the PKD-skip method outperforms the PKD-last method, so all the results discussed are using the PKD-skip method. Testing both these compressed models on the GLUE benchmark show that PKD beats traditional KD and doesn\u2019t cause a very large drop in accuracy. The 6-layer BERT is anywhere between 0.2% and 3.6% lower than the 12-layer BERT depending on the task while the 3-layer BERT is 2.8% \u2014 11.9% lower. This drop is expected because there are significantly fewer parameters in the smaller models, making it harder for them to capture all the information the 12-layer model is able to capture.", "When tested on the RACE benchmark, PKD is once again able to significantly outperform traditional KD and the 6-layer model is only 5% lower in accuracy than the 12-layer model. The biggest benefit to this technique is that it speeds up the prediction pipeline (1.94 times faster for 6-layer, 3.73 times faster for 3-layer) and reduces the storage required by the model (1.64 times less memory for 6-layer, 2.4 times less for 3-layer). Thus through PKD, large language models can be made significantly smaller and faster for use on small devices like laptops and phones.", "Here\u2019s a link to the paper if you want to go into more detail about PKD and click here to see more publications and other work from the team.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "I\u2019m an engineer at Microsoft Dynamics 365 AI Research and I\u2019ll post our new NLP, CV and Multimodal research . Check out https://medium.com/@rohit.rameshp"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F88969fffb4ba&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpatient-knowledge-distillation-88969fffb4ba&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpatient-knowledge-distillation-88969fffb4ba&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpatient-knowledge-distillation-88969fffb4ba&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpatient-knowledge-distillation-88969fffb4ba&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----88969fffb4ba--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----88969fffb4ba--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://conversationalaiteam.medium.com/?source=post_page-----88969fffb4ba--------------------------------", "anchor_text": ""}, {"url": "https://conversationalaiteam.medium.com/?source=post_page-----88969fffb4ba--------------------------------", "anchor_text": "Rohit Pillai"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7f2959a1653&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpatient-knowledge-distillation-88969fffb4ba&user=Rohit+Pillai&userId=7f2959a1653&source=post_page-7f2959a1653----88969fffb4ba---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F88969fffb4ba&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpatient-knowledge-distillation-88969fffb4ba&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F88969fffb4ba&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpatient-knowledge-distillation-88969fffb4ba&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@gentlestache?utm_source=medium&utm_medium=referral", "anchor_text": "bonneval sebastien"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://arxiv.org/pdf/1908.09355.pdf", "anchor_text": "link"}, {"url": "http://aka.ms/mmai", "anchor_text": "here"}, {"url": "https://arxiv.org/search/cs?searchtype=author&query=Cheng%2C+Y", "anchor_text": "Yu Cheng"}, {"url": "https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+D", "anchor_text": "Duo Wang"}, {"url": "https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+P", "anchor_text": "Pan Zhou"}, {"url": "https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+T", "anchor_text": "Tao Zhang"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----88969fffb4ba---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/naturallanguageprocessing?source=post_page-----88969fffb4ba---------------naturallanguageprocessing-----------------", "anchor_text": "Naturallanguageprocessing"}, {"url": "https://medium.com/tag/knowledge-distillation?source=post_page-----88969fffb4ba---------------knowledge_distillation-----------------", "anchor_text": "Knowledge Distillation"}, {"url": "https://medium.com/tag/compression?source=post_page-----88969fffb4ba---------------compression-----------------", "anchor_text": "Compression"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F88969fffb4ba&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpatient-knowledge-distillation-88969fffb4ba&user=Rohit+Pillai&userId=7f2959a1653&source=-----88969fffb4ba---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F88969fffb4ba&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpatient-knowledge-distillation-88969fffb4ba&user=Rohit+Pillai&userId=7f2959a1653&source=-----88969fffb4ba---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F88969fffb4ba&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpatient-knowledge-distillation-88969fffb4ba&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----88969fffb4ba--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F88969fffb4ba&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpatient-knowledge-distillation-88969fffb4ba&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----88969fffb4ba---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----88969fffb4ba--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----88969fffb4ba--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----88969fffb4ba--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----88969fffb4ba--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----88969fffb4ba--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----88969fffb4ba--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----88969fffb4ba--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----88969fffb4ba--------------------------------", "anchor_text": ""}, {"url": "https://conversationalaiteam.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://conversationalaiteam.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Rohit Pillai"}, {"url": "https://conversationalaiteam.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "68 Followers"}, {"url": "https://medium.com/@rohit.rameshp", "anchor_text": "https://medium.com/@rohit.rameshp"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7f2959a1653&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpatient-knowledge-distillation-88969fffb4ba&user=Rohit+Pillai&userId=7f2959a1653&source=post_page-7f2959a1653--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F6630fb9ccb26&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpatient-knowledge-distillation-88969fffb4ba&newsletterV3=7f2959a1653&newsletterV3Id=6630fb9ccb26&user=Rohit+Pillai&userId=7f2959a1653&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}