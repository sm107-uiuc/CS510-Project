{"url": "https://towardsdatascience.com/i-finally-understood-backpropagation-and-you-can-too-44f7dd98ff52", "time": 1683010170.499514, "path": "towardsdatascience.com/i-finally-understood-backpropagation-and-you-can-too-44f7dd98ff52/", "webpage": {"metadata": {"title": "I Finally Understood Backpropagation: And you can too\u2026 | by Kofi Asiedu Brempong | Towards Data Science", "h1": "I Finally Understood Backpropagation: And you can too\u2026", "description": "The backpropagation algorithm is one of the key ingredients for the training of neural networks, but it\u2019s also probably the most difficult concept to grasp in understanding how things really work. At\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives", "anchor_text": "Khan Academy: Multivariable calculus", "paragraph_index": 25}, {"url": "https://www.khanacademy.org/math/linear-algebra/vectors-and-spaces/dot-cross-products/v/vector-dot-product-and-vector-length", "anchor_text": "Khan Academy: Linear Algebra", "paragraph_index": 25}, {"url": "https://www.youtube.com/watch?v=WUvTyaaNkzM&list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr", "anchor_text": "3Blue1Brown", "paragraph_index": 25}], "all_paragraphs": ["The backpropagation algorithm is one of the key ingredients for the training of neural networks, but it\u2019s also probably the most difficult concept to grasp in understanding how things really work. At least in my own experience, it\u2019s been the one thing that I\u2019ve struggled to gain a deep understanding of. After juggling between a plethora of materials on the subject, I got my own eureka moment and the pieces fell into place. Like me, you may also be self-taught through online courses and may have done a couple of projects but you still feel that twinge of half-knowledge, of only vaguely understanding what is actually going on when you are training neural networks (or any other machine learning model).", "In this blog post, I would be letting you in on the intuition that I\u2019ve developed, hoping that you would build on this to develop an even better understanding of this all-important concept. I also provide links to the resources that have been of great help to me.", "But why?Because that was the most glossed over bit for me. Many online courses would just tell you that gradient descent finds the partial derivative of the loss function with respect to the weights (ie. the gradient) and takes a step in the direction opposite to this gradient; Because the gradient points in the direction of steepest ascent, taking a step in the opposite direction would mean we move in the direction of steepest descent. But they never state why, I mean the why of the why. We take a step in the direction opposite to the gradient because it points in the direction of steepest ascent but the question (for me) really is: Why is the gradient the direction of steepest ascent?", "The gradient is just a vector containing all the partial derivatives of a function. Hence the key idea here is really the concept of partial derivatives. Partial derivatives tell us how much a function would change when we keep all but one of its input variables constant and move a slight nudge in the direction of the one variable that is not fixed.", "As an aside, typical neural networks contain thousands of parameters but for simplicity and ease of visualization, we would be considering functions with two variables: \ud835\udc53(\ud835\udc65,\ud835\udc66). Fortunately, everything we do here would generalize nicely to any number of dimensions.", "For our two-variable case, the partial derivative tells us how much the output of the function would change if we keep the \ud835\udc66 variable constant and move a slight nudge in the \ud835\udc65 direction and vice versa.", "We are at the point (2, 3) in our input space which corresponds to a particular point t in the output plane, that is the output of our function is t for an input of (2, 3). The partial derivative with respect to \ud835\udc65 tells us how much change would result in the output if we keep \ud835\udc66 fixed at 3 and move slightly in the \ud835\udc65 direction. Similarly, the partial derivative with respect to \ud835\udc66 measures the resulting change in output when \ud835\udc65 is fixed at 2 and we move a little nudge in the \ud835\udc66 direction.", "Let\u2019s consider the function: \ud835\udc53(\ud835\udc65,\ud835\udc66) = x\u00b2y. The partial derivative with respect to x, \u2202f/\u2202x is 2xy, ie. we keep the y as a constant and differentiate the whole term. Likewise, the partial derivative with respect to y, \u2202f/\u2202y\u200b is x\u00b2.", "Remembering that the gradient packs together all the partial derivative into a vector, the gradient of this function would be: \u2207\ud835\udc53 = [2\ud835\udc65\ud835\udc66, \ud835\udc65\u00b2]. At point (2, 3), the gradient would be \u2207\ud835\udc53=[12, 4]. So a slight nudge purely in the x direction would cause a change by a factor of 12 in the output of the function while a similar change in the y direction would change the output of the function by a factor of 4.", "The problem with partial derivatives is that they only tell us how things would change if we move in only one direction. Partial derivatives are partial because neither of them tells us the full story of how our function f(x,y) changes when it's inputs changes. However, we do not only want to know how things change when we move in either the x or y direction, but we also want to know how much things would change if we move in any arbitrary direction within the input space. That's exactly what directional derivatives are for.", "The directional derivative in a direction say \ud835\udc64\u20d7 tells us how much the output of the function would change if we move a slight nudge in the direction of the vector \ud835\udc64\u20d7. The directional derivative is found by taking the dot product of the gradient of the function and \ud835\udc64\u20d7 ie. the direction in which we want to move.", "Evaluating the directional derivative at the point (2,3):", "What this means is that given that we are at the point (2, 3) in our input plane, taking a tiny step in the direction of the vector (3, 5) would change the output of our function by a factor of 56. Another way to look at this is to consider that in our input plane (ie. the x,y plane), any point or direction in this plane can be thought of as a combination of movements in the \ud835\udc65 and \ud835\udc66 directions.", "In the image above, \ud835\udc64\u20d7 =[3, 5] is a combination of 3 steps in the \ud835\udc65 direction and 5 steps in the \ud835\udc66 direction. So intuitively, taking a step along some arbitrary direction causes a change along the x-axis as well as along the y-axis. Taking the dot product for the directional derivative sums the changes along the x and y-axis.", "So now we have directional derivatives which are essentially a generalization of partial derivatives to deal with any arbitrary direction in our input plane. When training neural networks, the problem we seek to solve is that: given that we are at a point say (2, 3) which corresponds to a loss (the output of our function) of t, we want to know the direction that would result in the greatest increase in our loss? Once we know this direction we take a step in the opposite direction which would lead to the greatest reduction in the loss. Notice that I'm placing an intentional emphasis on the word direction. We are looking for the best direction and lucky enough we already have a tool that gives a measure of how good (or bad) a particular direction is, which as you may have guessed is the directional derivative.", "With directional derivatives, one way we could solve this problem is to find the directional derivative of all possible directions in which we could move. The best direction would be the direction with the largest directional derivative. But that would be too slow to compute, think about the number of possible directions in which we could move, the list is endless. However, the idea is good, we just need a simpler way to find the direction with the maximum directional derivative.", "Our objective now is to find:", "Notice that the vectors in the equation above have a magnitude or length of 1. In a sense, this ensures that we don\u2019t end up picking the wrong vector just because it has a larger magnitude than the rest and hence would maximize the dot product even though it\u2019s pointing in the wrong direction.", "The directional derivative as has already been stated is found by taking the dot product of the gradient and the vector pointing in our desired direction. The dot product possesses a very nice property that would allow us to find the direction that maximizes the directional derivative without having to consider all the possible directions. The dot product measures the similarity between two vectors. It assigns a score to how much the two vectors are traveling in the same direction. Formally, the dot product between two vectors \ud835\udc62\u20d7 and \ud835\udc63\u20d7 is:", "Where \ud835\udf03 is the angle between the two vectors.", "In the illustration above, when \ud835\udc62\u20d7 =[0, 1] and \ud835\udc63\u20d7 =[1, 0], their dot product is 0 because there is no similarity between them, \ud835\udc62\u20d7 is pointing purely in the \ud835\udc65 direction (it has no \ud835\udc66 component whiles \ud835\udc63\u20d7 also points solely in the \ud835\udc66 direction. The angle between them is 90\u00b0 (they are perpendicular) and \ud835\udc50\ud835\udc5c\ud835\udc60(90\u00b0) = 0. When, \ud835\udc62\u20d7 =[0, 1] and \ud835\udc63\u20d7 =[0, 1], the dot product is maximized because they are pointing in the same direction. The dot product, in this case, is 1 because the angle between them is 0\u00b0.", "The magnitude of the vectors does not have an impact on the result of the dot product because both vectors have a magnitude of 1. Hence the result of the dot product using the formulation above depends on the angle between the two vectors. From the foregoing, it is not difficult to grasp the fact that we are driving at, which is that, for unit length vectors, the dot product is maximized when the two vectors are parallel, that is they point in the same direction or that the angle between them is 0\u00b0.", "To remind us of our objective, we want to find the vector that maximizes the directional derivative:", "The directional derivative is also a dot product and so it flows naturally from our understanding of dot products that the vector that would maximize the directional derivative and result in the greatest increase in our function is the vector that points in the same direction as the gradient which is the gradient itself. This is why the gradient is the direction of steepest ascent. Gradient descent takes a step in the opposite direction because our objective in training is to minimize, not to maximize the loss function.", "Hopefully, you\u2019ve gained some useful insights from this post to help you solidify your understanding of the foundations of neural networks and other machine learning algorithms. The next blogpost will take a look at the chain rule which is the other major concept behind backpropagation.", "Partial and Directional derivatives: Khan Academy: Multivariable calculusDot products: Khan Academy: Linear AlgebraCalculus: 3Blue1Brown", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "\u2026 on the highway to mastery"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F44f7dd98ff52&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fi-finally-understood-backpropagation-and-you-can-too-44f7dd98ff52&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fi-finally-understood-backpropagation-and-you-can-too-44f7dd98ff52&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fi-finally-understood-backpropagation-and-you-can-too-44f7dd98ff52&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fi-finally-understood-backpropagation-and-you-can-too-44f7dd98ff52&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----44f7dd98ff52--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----44f7dd98ff52--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@kaybrempong?source=post_page-----44f7dd98ff52--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@kaybrempong?source=post_page-----44f7dd98ff52--------------------------------", "anchor_text": "Kofi Asiedu Brempong"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe23c41ff2a82&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fi-finally-understood-backpropagation-and-you-can-too-44f7dd98ff52&user=Kofi+Asiedu+Brempong&userId=e23c41ff2a82&source=post_page-e23c41ff2a82----44f7dd98ff52---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F44f7dd98ff52&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fi-finally-understood-backpropagation-and-you-can-too-44f7dd98ff52&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F44f7dd98ff52&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fi-finally-understood-backpropagation-and-you-can-too-44f7dd98ff52&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@thisisengineering?utm_source=medium&utm_medium=referral", "anchor_text": "ThisisEngineering RAEng"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives", "anchor_text": "Khan Academy: Multivariable calculus"}, {"url": "https://www.khanacademy.org/math/linear-algebra/vectors-and-spaces/dot-cross-products/v/vector-dot-product-and-vector-length", "anchor_text": "Khan Academy: Linear Algebra"}, {"url": "https://www.youtube.com/watch?v=WUvTyaaNkzM&list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr", "anchor_text": "3Blue1Brown"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----44f7dd98ff52---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----44f7dd98ff52---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----44f7dd98ff52---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/gradient-descent?source=post_page-----44f7dd98ff52---------------gradient_descent-----------------", "anchor_text": "Gradient Descent"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----44f7dd98ff52---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F44f7dd98ff52&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fi-finally-understood-backpropagation-and-you-can-too-44f7dd98ff52&user=Kofi+Asiedu+Brempong&userId=e23c41ff2a82&source=-----44f7dd98ff52---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F44f7dd98ff52&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fi-finally-understood-backpropagation-and-you-can-too-44f7dd98ff52&user=Kofi+Asiedu+Brempong&userId=e23c41ff2a82&source=-----44f7dd98ff52---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F44f7dd98ff52&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fi-finally-understood-backpropagation-and-you-can-too-44f7dd98ff52&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----44f7dd98ff52--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F44f7dd98ff52&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fi-finally-understood-backpropagation-and-you-can-too-44f7dd98ff52&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----44f7dd98ff52---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----44f7dd98ff52--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----44f7dd98ff52--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----44f7dd98ff52--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----44f7dd98ff52--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----44f7dd98ff52--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----44f7dd98ff52--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----44f7dd98ff52--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----44f7dd98ff52--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@kaybrempong?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@kaybrempong?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Kofi Asiedu Brempong"}, {"url": "https://medium.com/@kaybrempong/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "65 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe23c41ff2a82&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fi-finally-understood-backpropagation-and-you-can-too-44f7dd98ff52&user=Kofi+Asiedu+Brempong&userId=e23c41ff2a82&source=post_page-e23c41ff2a82--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Fe23c41ff2a82%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fi-finally-understood-backpropagation-and-you-can-too-44f7dd98ff52&user=Kofi+Asiedu+Brempong&userId=e23c41ff2a82&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}