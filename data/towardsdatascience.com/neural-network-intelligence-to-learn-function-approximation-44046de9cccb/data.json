{"url": "https://towardsdatascience.com/neural-network-intelligence-to-learn-function-approximation-44046de9cccb", "time": 1683001603.445835, "path": "towardsdatascience.com/neural-network-intelligence-to-learn-function-approximation-44046de9cccb/", "webpage": {"metadata": {"title": "Neural Architecture Search with NNI | Towards Data Science", "h1": "Neural Architecture Search with NNI", "description": "We use an AutoML tool Neural Network Intelligence (NNI) to do neural architecture search. We build a neural network to solve a function approximation problem and use NNI to optimize the network."}, "outgoing_paragraph_urls": [{"url": "https://github.com/microsoft/nni", "anchor_text": "Neural Network Intelligence", "paragraph_index": 1}, {"url": "http://neuralnetworksanddeeplearning.com/chap4.html", "anchor_text": "nice, accessible explanation", "paragraph_index": 2}, {"url": "https://github.com/jing-IG/nni-function-approximator/blob/master/func_approx.py", "anchor_text": "script", "paragraph_index": 7}, {"url": "https://github.com/jing-IG/nni-function-approximator/blob/master/func_approx_nni.py", "anchor_text": "such a training script", "paragraph_index": 27}, {"url": "https://github.com/jing-IG/nni-function-approximator", "anchor_text": "here", "paragraph_index": 46}], "all_paragraphs": ["Neural architecture search (NAS) is a difficult challenge in deep learning. Many of us have experienced that for a given dataset, a network may initially struggle to learn. But with a simple change of a hyper-parameter, the learning can become very effective. Manually tweaking hyper-parameters including architecture is time-consuming and challenging even though to some it can also be a lot of fun. Recently, automatic hyper-parameter tuning has become more and more popular, as it provides an efficient mechanism to solve NAS at scale.", "In this post, we will show how to perform hyper-parameter search using an automated machine learning (AutoML) tool \u2014 NNI (for Neural Network Intelligence) open-sourced by Microsoft. I just got started to play with NNI and I have liked it so far. Here I want to share how I use NNI to search for optimal hyper-parameters and architectures.", "We will build a neural network to approximate math functions, as an example. Neural networks with one or more hidden layers are known as function approximators for continuous functions [1], assuming certain non-linearity conditions of activation functions are met. Shallow networks such as the ones with a single hidden layer are not as efficient as width-bound deep networks (for example [2]). Michael Nielsen has given a nice, accessible explanation of how neural nets can approximate functions.", "We begin with a simple nonlinear target function y = x\u00b2. We use a fully connected network with a few hidden layers to learn this function within the range [0, 1].", "Below is code to implement this function as a PyTorch dataset. Input data and ground-truth labels are represented by tensors of shape (1,1), where the two components are channels and data dimension, respectively. They are both 1 since x and y hold only scalar values. If you have experience in image-based neural networks, you can think of the data as a single-channel, single-pixel image.", "We build a fully-connected network of up to three hidden layers. The network is flexible to be configured to have hidden-layer depth to range from 1 to 3.", "To get the training going, we need to make many choices. We must decide number of hidden layers, each layer\u2019s size, activation function, optimizer, learning rate, etc. We start with a network of a single hidden layer of 16 neurons, and the following choices:", "Run the script for 250 epochs, you will get a learned function shown in the graph below. Red dots are network output, which follow the actual quadratic function (blue curve) roughly. But obviously, there is room for improvements.", "Note we have fixed seeds for random number generation in Python, so that every network always starts with the same set of initial weights/biases and results will be reproducible.", "As a fun experiment, we can make the network architecture wider or deeper to see if it improves the approximation. The following graph shows results from a \u201cwider\u201d model of a 256-neuron single hidden layer. The performance is about the same as the 16-neuron version.", "But if we use a \u201cdeeper\u201d model of three hidden layers of 16 neurons each, the network output matches the target function very well (see the graph below). More specifically, the network can be described as {hidden_layer_1_size: 16, hidden_layer_2_size: 16, hidden_layer_3_size: 16}. To facilitate our discussion of various network architectures, we will use a shorthand of three numbers {n1, n2, n3} to represent a network. In this case, the model is {16, 16, 16}. The maximum approximation error between the target function and network output is ~0.02. The error reaches its maximum near the upper bound of x (1.0). Errors elsewhere are much less than that.", "Now let us try a more challenging function. The function goes up and down several times, with magnitude of change slowly decaying. I name the function oscillator, for the lack of a better name. The oscillator is continuous so it is possible for neural networks to approximate. However, it is not smooth, with sudden \u201cjumps\u201d (amazingly neural networks can still be able to handle these, as we see later).", "The function can be created by building a series of points (x, y), with x uniformly distributed between [0,1] with a spacing of 0.1, and y starting at 0.5 and each subsequent y value is calculated from its last value.", "Now it is not trivial to figure out all the hyper-parameters. For example, if we use the same {16, 16, 16} network, and run for over 1000 epochs, the results are:", "Note that we train it over more epochs since the target function is more complex than the simple quadratic function. The learned function (red) does not approximate the actual function (blue) as well as for the quadratic function, even though the network has \u201crecognized\u201d the most prominent features in the function and approximated them.", "As for the quadratic function before, we can make the network architecture wider or deeper to see if it improves the approximation. But it will be time-consuming to find the most efficient model because it involves a lot of trials and errors.", "In the next section, we\u2019ll do the search automatically with NNI.", "NNI is a flexible and powerful hyper-parameter search tool. I have found the NNI documentation and workflow straightforward to understand and follow, unlike other automated machine learning options out there. NNI can be easily configured to run on a local machine. For our purpose, doing search locally is sufficient since each training session takes no more than a few minutes without even a GPU.", "An experiment in NNI is composed of many trials, each evaluating a unique combination of searched parameters. In each trial, a model is constructed from scratch using the selected parameters and trained. All the performance metrics are logged and can be examined using the provided WebUI at the end.", "There are two ways to use NNI: via NNI API or via NNI Python Annotation. The main difference is where to specify search parameter choices. The NNI API method expects the choices to be listed in a JSON file, and the Python Annotation method allows you to embed choices as specially-formatted code comment right next to a parameter definition in your script. The latter provides a better support to neural architecture search (named NNI Programming Interface for Neural Architecture Search), allowing you specify different architectural options conveniently inside an annotation. However, at the time of writing, this is still an experimental feature. In this post, we will use NNI API to do hyper-parameter search including architectures. This requires us to write a bit more code to manage architecture search compared to NNI Programming Interface, but it is still manageable for our purpose.", "There are a lot of hyper-parameters that may potentially affect our model performance but searching for all of them at the same time is very time consuming. Suppose we want to explore 7 parameters, each with 4 values to try. The total number of unique parameter combinations is 4\u2077 = 16384. Running through so many trials will take a long while, and even analyzing final results may become tricky. As shown later in this post, we will use the NNI WebUI to visualize and review the results. But this will not work well if we have too many data points.", "Instead, we will break down the search into two steps. Remember the four parameters (activation function, learning rate, optimizer and loss function) that we have chosen values for earlier in this post? The first step is trying to discover good values for these parameters that are not related to network architecture. Once we get those uncertainties out of our way, then we can focus on architecture search. Note that the activation function could be regarded as one architectural parameter. But it is not related to topology of a network. In our experiments, we group it with other non-architectural parameters so that later we can focus on architecture search from the topological perspective.", "Note that this two-step approach is a compromise. Here we have implicitly assumed that optimization of the non-architecture parameters such as the loss function is not dependent on network topology, so we can decouple them and optimize separately. But in reality, topology, non-architectural hyper-parameters, and initial network weights/biases are most likely intertwined and their optimizations all depend on each other to some degree. We take this practical approach to search for a very good solution to our problem, but we can not guarantee that it is the best.", "We use a model with a {16, 16, 16} network for this experiment. It may not be the best architecture (which we do not know yet) but it will allow us to gain insight into other hyper-parameters first.", "We will first walk through how NNI works. The workflow of running a search experiment using NNI API is:", "1. Define search space. A search space describes all the choices for each parameter to be searched. The choices are to be listed in a JSON file like this:", "Here since learning rate is a continuous variable, we set it to change in a range of [0.005, 005] with a spacing of 0.01.", "2. Adapt model code to use NNI API. This step involves calling nni.get_next_parameter() before each trial to get a unique combination of all the search parameters, calling nni.report_intermediate_result() during training to log intermediate performance metric as a time series to NNI, and calling nni.report_final_result() after each training session to log the final metric. An example of such a training script is provided in the repo for this post.", "3. Create a YAML configuration file and enable NNI API in it.", "Notice we set useAnnotation to false to enable NNI API. And we use GridSearch for the tuner parameter builtinTunerName to search all possible combinations of parameter values. There are other options for the NNI tuner to use different search algorithms to reduce search space and save search time. At this time of writing, I do not yet have a good understanding of trade-offs being made by these search algorithms. You need to give them a try in order to see which works better for you.", "If everything goes well, this will start a NNI search experiment and tell you the URL to view results. Open the URL with a browser and you can see the NNI WebUI. This nice UI tool is very easy to use. It reports job run status, and when the experiment is done, it presents results from all the trials in an intuitive graph. This is the fun part \u2014 to see which parameter combinations give you best results.", "The following hyper-parameter view highlights the top performers (in red) of this experiment.", "We can see that a combination of the SmoothL1Loss loss function, the Adam optimizer and the LeakyReLU activation function tends to work very well, and the performance is not very sensitive to learning rates.", "A great feature of the hyper-parameter view is that we can move a parameter line left or right. In the following screenshot, we swap the \u201closs\u201d parameter with the \u201cact_func\u201d parameter. Now it is easy to see ReLU is a bad choice regardless of other parameters, since it is not linked to any good metric values that are close to or above 2.0 (which are colored in red or orange).", "Similarly, if we move the \u201coptimizer\u201d parameter close to the \u201cdefault metric\u201d line, we can see RMSprop and SGD are in general linked to lower metric values (in green or yellow) and can not compete with Adam.", "Interestingly, the top performer stands out from the crowd by a big margin. This difference may not be very significant, as some models\u2019 metrics fluctuate a lot during training. The top performer has the following parameters:", "If we adopt these parameter values and train the same {16, 16, 16} network, the network output matches the target function extremely well (compared with Figure 5):", "Our architecture search is to find optimal number of hidden layers and size of each hidden layer, as listed below in another JSON file:", "Putting zero as an option for hidden_size_2 and hidden_size_3 effectively cover cases where hidden layer 2 or 3 is not used. For example, setting both hidden_size_2 and hidden_size_3 to zero results in a single hidden-layer network.", "Search results are presented in the graph below. Top performers are highlighted in red or orange. We can see that the number of neurons in the first hidden layer is roughly correlated with performance metric. The more neuron in this layer, the better results we get. In general, this is also true for other hidden layers, which makes sense. The more parameters we have at a network\u2019s disposal, the easier for it to learn.", "In terms of the network topology, there is no obvious patterns. There are networks with number of neurons in each layer decreasing as we go deeper, or remaining roughly the same, or actually increasing.", "We are more interested in a simple network with minimum number of layers and minimum number of neurons. This is the goal of neural architecture search \u2014 finding efficient networks with little or no compromise in performance. Performance of all the single-hidden-layer networks is highlighted in this graph:", "The {64, 0, 0} model can give very decent results, with maximum approximation error around 0.006:", "If a network of a 64-neuron single hidden layer can work here, why does the network of a 128-neuron single hidden layer not perform even better in comparison? On the WebUI, we can easily examine each network\u2019s performance over time. It turns out the {128, 0, 0} model\u2019s metric fluctuates more than the {64, 0, 0} model. Since we report only minimum of the last 5 metrics as the final result, the value for the {128, 0, 0} model happens to be lower. But the upper bound of the last 5 metrics from the {128, 0, 0} model is as competitive as other top performers in this experiment. Why such a big fluctuation during training? It is possible that in later stages of training, network performance is sensitive to minor parameter adjustments, and for a bigger network, it is more challenging to keep all the neurons updated in a synchronized fashion. Therefore there are larger swings in performance from time to time.", "Another interesting observation is that for all the networks with a single hidden layer, those with no more than 32 neurons do not apparently learn from the target function. In fact, we can make sense of this with a rough analysis. The minimum parameters required to approximate the oscillator function are 11*2 = 22, as we need to know all the 11 key points in the range [0, 1] and each requires two values (x and y) to specify. Networks with parameters around 22 would be composed of a single hidden layer of either 4 or 8 neurons. But this does not mean a {8, 0, 0} network could be the most efficient architecture, since we know that neural networks by default do not automatically learn to be efficient and slim. That is, a network does not know to utilize every parameter most effectively. Therefore, the network required for our task may be a little bigger than the {8, 0, 0} network. Indeed, if we try to train the network of a single hidden layer with fewer than 16 neurons, we cannot find a decent solution even after more than 5000 epochs.", "NNI is a lightweight, flexible AutoML tool that can help with Neural Architecture Search. In this post, we have used NNI to optimize a neural network that approximates functions. This experiment can be done on a desktop with average computing powers. So it is easy to get started with the tool, compared to other NAS frameworks. We only explored basic ways of using NNI. I hope they are interesting and give you a good sense of what NNI can do.", "Full source code for this post is here.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F44046de9cccb&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-network-intelligence-to-learn-function-approximation-44046de9cccb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-network-intelligence-to-learn-function-approximation-44046de9cccb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-network-intelligence-to-learn-function-approximation-44046de9cccb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-network-intelligence-to-learn-function-approximation-44046de9cccb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----44046de9cccb--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----44046de9cccb--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@afterbeta?source=post_page-----44046de9cccb--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@afterbeta?source=post_page-----44046de9cccb--------------------------------", "anchor_text": "Jing Wang"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F47ede7f3c59e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-network-intelligence-to-learn-function-approximation-44046de9cccb&user=Jing+Wang&userId=47ede7f3c59e&source=post_page-47ede7f3c59e----44046de9cccb---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F44046de9cccb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-network-intelligence-to-learn-function-approximation-44046de9cccb&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F44046de9cccb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-network-intelligence-to-learn-function-approximation-44046de9cccb&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@thebeardbe?utm_source=medium&utm_medium=referral", "anchor_text": "Filip Bunkens"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://github.com/microsoft/nni", "anchor_text": "Neural Network Intelligence"}, {"url": "http://neuralnetworksanddeeplearning.com/chap4.html", "anchor_text": "nice, accessible explanation"}, {"url": "https://github.com/jing-IG/nni-function-approximator/blob/master/func_approx.py", "anchor_text": "script"}, {"url": "https://github.com/jing-IG/nni-function-approximator/blob/master/func_approx_nni.py", "anchor_text": "such a training script"}, {"url": "https://github.com/jing-IG/nni-function-approximator", "anchor_text": "here"}, {"url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.441.7873&rep=rep1&type=pdf", "anchor_text": "\u201cApproximations by superpositions of sigmoidal functions\u201d"}, {"url": "https://arxiv.org/abs/1610.04161", "anchor_text": "Why deep neural networks for function approximation?"}, {"url": "https://medium.com/tag/function-approximation?source=post_page-----44046de9cccb---------------function_approximation-----------------", "anchor_text": "Function Approximation"}, {"url": "https://medium.com/tag/automl?source=post_page-----44046de9cccb---------------automl-----------------", "anchor_text": "Automl"}, {"url": "https://medium.com/tag/nas?source=post_page-----44046de9cccb---------------nas-----------------", "anchor_text": "Nas"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----44046de9cccb---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----44046de9cccb---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F44046de9cccb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-network-intelligence-to-learn-function-approximation-44046de9cccb&user=Jing+Wang&userId=47ede7f3c59e&source=-----44046de9cccb---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F44046de9cccb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-network-intelligence-to-learn-function-approximation-44046de9cccb&user=Jing+Wang&userId=47ede7f3c59e&source=-----44046de9cccb---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F44046de9cccb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-network-intelligence-to-learn-function-approximation-44046de9cccb&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----44046de9cccb--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F44046de9cccb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-network-intelligence-to-learn-function-approximation-44046de9cccb&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----44046de9cccb---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----44046de9cccb--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----44046de9cccb--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----44046de9cccb--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----44046de9cccb--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----44046de9cccb--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----44046de9cccb--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----44046de9cccb--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----44046de9cccb--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@afterbeta?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@afterbeta?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Jing Wang"}, {"url": "https://medium.com/@afterbeta/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "5 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F47ede7f3c59e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-network-intelligence-to-learn-function-approximation-44046de9cccb&user=Jing+Wang&userId=47ede7f3c59e&source=post_page-47ede7f3c59e--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F47ede7f3c59e%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-network-intelligence-to-learn-function-approximation-44046de9cccb&user=Jing+Wang&userId=47ede7f3c59e&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}