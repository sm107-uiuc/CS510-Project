{"url": "https://towardsdatascience.com/keeping-up-with-the-berts-5b7beb92766", "time": 1683015684.585143, "path": "towardsdatascience.com/keeping-up-with-the-berts-5b7beb92766/", "webpage": {"metadata": {"title": "BERT Explained: What it is and how does it work? | Towards Data Science", "h1": "Keeping up with the BERTs", "description": "BERT stands for Bidirectional Encoder Representations from Transformers and is a language representation model by Google. It uses two steps, pre-training and fine-tuning, to create state-of-the-art models for a wide range of tasks."}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/transformers-89034557de14", "anchor_text": "my previous article", "paragraph_index": 4}, {"url": "https://peltarion.com/knowledge-center/documentation/modeling-view/build-an-ai-model/blocks/bert-encoder", "anchor_text": "this", "paragraph_index": 4}, {"url": "https://towardsdatascience.com/evaluation-of-an-nlp-model-latest-benchmarks-90fd8ce6fae5", "anchor_text": "Question Answering (QA) and Natural Language Inference (NLI)", "paragraph_index": 14}, {"url": "https://towardsdatascience.com/transformers-89034557de14", "anchor_text": "Transformers", "paragraph_index": 30}, {"url": "https://gluebenchmark.com/leaderboard", "anchor_text": "the GLUE Leaderboard", "paragraph_index": 32}, {"url": "https://medium.com/@ria.kulshrestha16", "anchor_text": "here", "paragraph_index": 34}, {"url": "https://twitter.com/ree_____ree", "anchor_text": "Twitter", "paragraph_index": 35}], "all_paragraphs": ["If you even slightly follow the NLP world, or even the ML news you have most likely come across Google\u2019s BERT model or one of its relatives. If you haven\u2019t and still somehow have stumbled across this article, let me have the honor of introducing you to BERT \u2014 the powerful NLP beast.", "BERT stands for Bidirectional Encoder Representations from Transformers and is a language representation model by Google. It uses two steps, pre-training and fine-tuning, to create state-of-the-art models for a wide range of tasks.", "Its distinctive feature is the unified architecture across different downstream tasks \u2014 what these are, we will discuss soon. That means that the same pre-trained model can be fine-tuned for a variety of final tasks that might not be similar to the task model was trained on and give close to state-of-the-art results.", "As you can see, we first train the model on the pre-training tasks simultaneously. Once the pre-training is complete, the same model can be fine-tuned for a variety of downstream tasks. Note that a separate model is fine-tuned for a specific downstream task. So single pre-trained models can generate multiple downstream task specific models post fine tuning.", "Simply put, it is a stack of Transformer\u2019s Encoder. You can read about Transformers in details in my previous article. Or if you have some faint idea about it already, check out this absolutely bomb 3D diagram of the Encoder block used in BERT. Seriously you can\u2019t miss this!", "Now let\u2019s look at some numbers that none of us will ever remember, but our understanding will feel incomplete without them, so here goes nothing:", "L = Number of layers (i.e., #Transformer encoder blocks in the stack).H = Hidden size (i.e. the size of q, k and v vectors).A = Number of attention heads.", "We usually create a language model by training it on some unrelated task but tasks that help develop a contextual understanding of words in a model. More often than not such tasks involve predicting the next word or words in close vicinity of each other. Such training methods can\u2019t be extended and used for bidirectional models because it would allow each word to indirectly \u201csee itself\u201d \u2014 when you would approach the same sentence again but from opposite direction, you kind of already know what to expect. A case of data leakage.", "In such a situation, model could trivially predict the target word. Additionally, we can\u2019t guarantee that the model, if completely trained, has learnt the contextual meaning of the words to some extent and not just focused on optimizing the trivial predictions.", "So how does BERT manage to pre-train bidirectionally? It does so by using a procedure called Masked LM. More details on it later, so read on, my friend.", "The BERT model is trained on the following two unsupervised tasks.", "This task enables the deep bidirectional learning aspect of the model. In this task, some percentage of the input tokens are masked (Replaced with [MASK] token) at random and the model tries to predict these masked tokens \u2014 not the entire input sequence. The predicted tokens from the model are then fed into an output softmax over the vocabulary to get the final output words.", "This, however creates a mismatch between the pre-training and fine-tuning tasks because the latter does not involve predicting masked words in most of the downstream tasks. This is mitigated by a subtle twist in how we mask the input tokens.", "Approximately 15% of the words are masked while training, but all of the masked words are not replaced by the [MASK] token.", "The LM doesn\u2019t directly capture the relationship between two sentences which is relevant in many downstream tasks such as Question Answering (QA) and Natural Language Inference (NLI). The model is taught sentence relationships by training on binarized NSP task.", "In this task, two sentences \u2014 A and B \u2014 are chosen for pre-training.", "The model is trained on both above mentioned tasks simultaneously. This is made possible by clever usage of inputs and outputs.", "The model needs to take input for both a single sentence or two sentences packed together unambiguously in one token sequence. Authors note that a \u201csentence\u201d can be an arbitrary span of contiguous text, rather than an actual linguistic sentence. A [SEP] token is used to separate two sentences as well as a using a learnt segment embedding indicating a token as a part of segment A or B.", "Problem #1: All the inputs are fed in one step \u2014 as opposed to RNNs in which inputs are fed sequentially, the model is not able to preserve the ordering of the input tokens. The order of words in every language is significant, both semantically and syntactically.", "Problem #2: In order to perform Next Sentence Prediction task properly we need to be able to distinguish between sentences A and B. Fixing the lengths of sentences can be too restrictive and a potential bottleneck for various downstream tasks.", "Both of these problems are solved by adding embeddings containing the required information to our original tokens and using the result as the input to our BERT model. The following embeddings are added to token embeddings:", "How does one predict output for two different tasks simultaneously? The answer is by using different FFNN + Softmax layer built on top of output(s) from the last encoder, corresponding to desired input tokens. We will refer to the outputs from last encoder as final states.", "The first input token is always a special classification [CLS] token. The final state corresponding to this token is used as the aggregate sequence representation for classification tasks and used for the Next Sentence Prediction where it is fed into a FFNN + Softmax layer that predicts probabilities for the labels \u201cIsNext\u201d or \u201cNotNext\u201d.", "The final states corresponding to [MASK] tokens is fed into FFNN+Softmax to predict the next word from our vocabulary.", "Fine-tuning on various downstream tasks is done by swapping out the appropriate inputs or outputs. In the general run of things, to train task-specific models, we add an extra output layer to existing BERT and fine-tune the resultant model \u2014 all parameters, end to end. A positive consequence of adding layers \u2014 input/output and not changing the BERT model is that only a minimal number of parameters need to be learned from scratch making the procedure fast, cost and resource efficient.", "Just to give you an idea of how fast and efficient it is, the authors claim that all the results in the paper can be replicated in at most 1 hour on a single Cloud TPU, or a few hours on a GPU, starting from the exact same pre-trained model.", "In Sentence Pair Classification and Single Sentence Classification, the final state corresponding to [CLS] token is used as input for the additional layers that makes the prediction.", "In QA tasks, a start (S) and an end (E) vector are introduced during fine tuning. The question is fed as sentence A and the answer as sentence B. The probability of word i being the start of the answer span is computed as a dot product between Ti (final state corresponding to ith input token) and S (start vector) followed by a softmax over all of the words in the paragraph. A similar method is used for end span. The score of a candidate span from position i to position j is defined as S\u00b7Ti + E\u00b7Tj, and the maximum scoring span where j \u2265 i is used as a prediction", "Is BERT the only model that is producing these ground breaking results? No. Another model by OpenAI, called GPT has been making quite the buzz on internet.", "But what many people don\u2019t realize that these two models have something in common, that is both these model reuse a Transformer component. As stated earlier BERT stacks the encoder part of the Transformer as its building block. Meanwhile, GPT uses the decoder part of the Transformer as its building block.", "Note that the bidirectional connections in BERT due to encoder\u2019s bidirectional self-attention. Meanwhile, the connections in GPT are only in a single direction, from left-to-right, due to decoder design to prevent looking at future predictions \u2014 refer Transformers for more info.", "It wouldn\u2019t be 21st century if we didn\u2019t take something that works well and try to recreate or modify it. BERT architecture is no different. These are some of the most popular variants of it:", "You can check out more BERT inspired models at the GLUE Leaderboard.", "I\u2019m glad you made it till the end of this article. \ud83c\udf89I hope your reading experience was as enriching as the one I had writing this. \ud83d\udc96", "Do check out my other articles here.", "If you want to reach out to me, my medium of choice would be Twitter.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F5b7beb92766&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fkeeping-up-with-the-berts-5b7beb92766&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fkeeping-up-with-the-berts-5b7beb92766&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fkeeping-up-with-the-berts-5b7beb92766&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fkeeping-up-with-the-berts-5b7beb92766&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----5b7beb92766--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5b7beb92766--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://ria-kulshrestha.medium.com/?source=post_page-----5b7beb92766--------------------------------", "anchor_text": ""}, {"url": "https://ria-kulshrestha.medium.com/?source=post_page-----5b7beb92766--------------------------------", "anchor_text": "Ria Kulshrestha"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F406aa3cbd38e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fkeeping-up-with-the-berts-5b7beb92766&user=Ria+Kulshrestha&userId=406aa3cbd38e&source=post_page-406aa3cbd38e----5b7beb92766---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5b7beb92766&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fkeeping-up-with-the-berts-5b7beb92766&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5b7beb92766&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fkeeping-up-with-the-berts-5b7beb92766&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@leookubo?utm_source=medium&utm_medium=referral", "anchor_text": "Leonardo Toshiro Okubo"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "The paper"}, {"url": "https://towardsdatascience.com/transformers-89034557de14", "anchor_text": "my previous article"}, {"url": "https://peltarion.com/knowledge-center/documentation/modeling-view/build-an-ai-model/blocks/bert-encoder", "anchor_text": "this"}, {"url": "https://towardsdatascience.com/transformers-89034557de14", "anchor_text": "TransformersOr as I like to call it Attention on Steroids. \ud83d\udc89\ud83d\udc8atowardsdatascience.com"}, {"url": "https://towardsdatascience.com/evaluation-of-an-nlp-model-latest-benchmarks-90fd8ce6fae5", "anchor_text": "Question Answering (QA) and Natural Language Inference (NLI)"}, {"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "The paper"}, {"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "The paper"}, {"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "The paper."}, {"url": "https://towardsdatascience.com/transformers-89034557de14", "anchor_text": "Transformers"}, {"url": "https://arxiv.org/abs/1909.11942", "anchor_text": "ALBERT"}, {"url": "https://arxiv.org/abs/1907.11692", "anchor_text": "RoBERTa"}, {"url": "https://arxiv.org/abs/1904.09223", "anchor_text": "ERNIE"}, {"url": "https://arxiv.org/abs/1910.01108", "anchor_text": "DistilBERT"}, {"url": "https://gluebenchmark.com/leaderboard", "anchor_text": "the GLUE Leaderboard"}, {"url": "https://towardsdatascience.com/transformers-89034557de14", "anchor_text": "Transformers"}, {"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "The paper"}, {"url": "http://jalammar.github.io/illustrated-bert/", "anchor_text": "Jay Alammar\u2019s Blog"}, {"url": "https://github.com/google-research/bert", "anchor_text": "Official GitHub repo."}, {"url": "https://towardsdatascience.com/2019-year-of-bert-and-transformer-f200b53d05b9", "anchor_text": "More BERT Models"}, {"url": "https://medium.com/@ria.kulshrestha16", "anchor_text": "here"}, {"url": "https://twitter.com/ree_____ree", "anchor_text": "Twitter"}, {"url": "https://medium.com/tag/nlp?source=post_page-----5b7beb92766---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/bert?source=post_page-----5b7beb92766---------------bert-----------------", "anchor_text": "Bert"}, {"url": "https://medium.com/tag/naturallanguageprocessing?source=post_page-----5b7beb92766---------------naturallanguageprocessing-----------------", "anchor_text": "Naturallanguageprocessing"}, {"url": "https://medium.com/tag/google?source=post_page-----5b7beb92766---------------google-----------------", "anchor_text": "Google"}, {"url": "https://medium.com/tag/ai?source=post_page-----5b7beb92766---------------ai-----------------", "anchor_text": "AI"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5b7beb92766&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fkeeping-up-with-the-berts-5b7beb92766&user=Ria+Kulshrestha&userId=406aa3cbd38e&source=-----5b7beb92766---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5b7beb92766&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fkeeping-up-with-the-berts-5b7beb92766&user=Ria+Kulshrestha&userId=406aa3cbd38e&source=-----5b7beb92766---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5b7beb92766&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fkeeping-up-with-the-berts-5b7beb92766&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5b7beb92766--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F5b7beb92766&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fkeeping-up-with-the-berts-5b7beb92766&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----5b7beb92766---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----5b7beb92766--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----5b7beb92766--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----5b7beb92766--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----5b7beb92766--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----5b7beb92766--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----5b7beb92766--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----5b7beb92766--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----5b7beb92766--------------------------------", "anchor_text": ""}, {"url": "https://ria-kulshrestha.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://ria-kulshrestha.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ria Kulshrestha"}, {"url": "https://ria-kulshrestha.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "768 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F406aa3cbd38e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fkeeping-up-with-the-berts-5b7beb92766&user=Ria+Kulshrestha&userId=406aa3cbd38e&source=post_page-406aa3cbd38e--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fccdcb0ec19a4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fkeeping-up-with-the-berts-5b7beb92766&newsletterV3=406aa3cbd38e&newsletterV3Id=ccdcb0ec19a4&user=Ria+Kulshrestha&userId=406aa3cbd38e&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}