{"url": "https://towardsdatascience.com/dismantling-neural-networks-to-understand-the-inner-workings-with-math-and-pytorch-beac8760b595", "time": 1683008644.143462, "path": "towardsdatascience.com/dismantling-neural-networks-to-understand-the-inner-workings-with-math-and-pytorch-beac8760b595/", "webpage": {"metadata": {"title": "Dismantling Neural Networks to Understand the Inner Workings with Math and Pytorch | by Mehdi Amine | Towards Data Science", "h1": "Dismantling Neural Networks to Understand the Inner Workings with Math and Pytorch", "description": "As a child, you might have dismantled a toy in a moment of frenetic curiosity. You were drawn perhaps towards the source of the sound it made. Or perhaps it was a tempting colorful light from a diode\u2026"}, "outgoing_paragraph_urls": [{"url": "https://pytorch.org/docs/stable/torch.html#torch.clamp", "anchor_text": "clamp", "paragraph_index": 12}, {"url": "https://numpy.org/doc/stable/reference/generated/numpy.clip.html", "anchor_text": "clip", "paragraph_index": 12}, {"url": "https://pytorch.org/docs/stable/nn.functional.html#relu", "anchor_text": "relu", "paragraph_index": 12}, {"url": "https://pytorch.org/docs/stable/nn.functional.html#softmax", "anchor_text": "softmax", "paragraph_index": 29}, {"url": "https://pytorch.org/docs/stable/nn.functional.html#cross-entropy", "anchor_text": "cross_entropy", "paragraph_index": 35}, {"url": "https://course.fast.ai/videos/?lesson=9", "anchor_text": "lecture 9", "paragraph_index": 40}, {"url": "https://pytorch.org/docs/stable/nn.functional.html#cross-entropy", "anchor_text": "cross_entropy", "paragraph_index": 43}, {"url": "https://pytorch.org/docs/stable/autograd.html#torch.autograd.backward", "anchor_text": "backward", "paragraph_index": 43}, {"url": "https://drive.google.com/drive/folders/1EkhYkKE74Kz2rYCvGM1BMTJ5ih7Q-toA?usp=sharing", "anchor_text": "The Inner Workings of Neural Networks, My Colab Notebooks", "paragraph_index": 51}, {"url": "https://gist.github.com/Mehdi-Amine", "anchor_text": "The Inner Workings of Neural Networks, My Gists", "paragraph_index": 51}, {"url": "https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/", "anchor_text": "Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow", "paragraph_index": 51}, {"url": "https://sgugger.github.io/a-simple-neural-net-in-numpy.html", "anchor_text": "A simple neural net in numpy", "paragraph_index": 51}, {"url": "https://course.fast.ai/videos/?lesson=9", "anchor_text": "Fast.ai: Deep Learning from the Foundations Lesson 9", "paragraph_index": 51}, {"url": "https://pytorch.org/docs/stable/index.html", "anchor_text": "The Pytorch documentation", "paragraph_index": 51}, {"url": "https://www.linkedin.com/in/mehdi-amine", "anchor_text": "https://www.linkedin.com/in/mehdi-amine", "paragraph_index": 53}], "all_paragraphs": ["As a child, you might have dismantled a toy in a moment of frenetic curiosity. You were drawn perhaps towards the source of the sound it made. Or perhaps it was a tempting colorful light from a diode that called you forth, moved your hands into cracking the plastic open.", "Sometimes you may have felt deceived that the inside was nowhere close to what the shiny outside led you to imagine. I hope you have been lucky enough to open the right toys. Those filled with enough intricacies to make breaking them open worthwhile. Maybe you found a futuristic looking DC-motor. Or maybe a curious looking speaker with a strong magnet on its back that you tried on your fridge. I am sure it felt just right when you discovered what made your controller vibrate.", "We are going to do exactly the same. We are dismantling a neural network with math and with Pytorch. It will be worthwhile, and our toy won\u2019t even break. Maybe you feel discouraged. That\u2019s understandable. There are so many different and complex parts in a neural network. It is overwhelming. It is the rite of passage to a wiser state.", "So to help ourselves we will need a reference, some kind of Polaris to ensure we are on the right course. The pre-built functionalities of Pytorch will be our Polaris. They will tell us the output we must get. And it will fall upon us to find the logic that will lead us to the correct output. If differentiations sound like forgotten strangers that you once might have been acquainted with, fret not! We will make introductions again and it will all be mighty jovial. I hope you will enjoy.", "The value of a neuron depends on its inputs, weights, and bias. To compute this value for all neurons in a layer, we calculate the dot product of the matrix of inputs with the matrix of weights, and we add the bias vector. We represent this concisely when we write:", "Conciseness in mathematical equations however, is achieved with abstraction of the inner workings. The price we pay for conciseness is making it harder to understand and mentally visualize the steps involved. And to be able to code and debug such intricate structures as Neural Networks we need both deep understanding and clear mental visualization. To that end, we favor verbosity:", "Now the equation is grounded with constraints imposed by a specific case: one neuron, three inputs, three weights, and a bias. We have moved away from abstraction to something more concrete, something we can easily implement:", "To calculate z, we have moved forward from a layer of inputs to the next layer of neurons. When a neural network steps all the way forward through its layers and acquires knowledge, it needs to know how to go backwards to adjust its previous layers. We can achieve this backward propagation of knowledge through derivatives. Simply put, if we differentiate z with respect to each of its parameters (the weights and the bias), we can get the values of the input layer x.", "If you have forgotten how to differentiate, rest assured: you won\u2019t be told to go brush up on an entire branch of calculus. We will recall differentiations rules as we need them. The partial derivative of z with respect to a parameter tells you to consider that parameter as a variable, and all other parameters as constants. The derivative of a variable is equal to its coefficient. And the derivative of a constant is equal to zero:", "Similarly, you can differentiate z with respect to w1, w2, and b (with b having the invisible coefficient of 1). You will find that every partial derivative of z is equal to the coefficient of the parameter with respect to which it is differentiated. With this in mind, we can use Pytorch Autograd to evaluate the correctness of our math.", "We introduce non-linearity with activation functions. This enables neural networks to be universal function approximators. There are various types of activations, each one fulfills a different purpose and produces a different effect. We will go through the formula and differentiation of ReLU, Sigmoid, and Softmax.", "The Rectified Linear Unit function compares the value of a neuron with zero and outputs the maximum. We can think of ReLU labeling all nonpositive neurons as equally inactive.", "To implement our own ReLU, we could compare z with 0 and output whichever is greater. But the clamp method provided in the Torch package can already do this for us. In Numpy, the equivalent function is called clip. The following code implements a clamp-based ReLU, before using Pytorch\u2019s relu to evaluate its output.", "The differentiation of ReLU is straightforward:", "Let\u2019s translate our understanding into Python code. We will implement our own ReLU\u2019(z) before comparing it with the automatic differentiation of Pytorch\u2019s ReLU.", "Why are we giving a tensor of ones to backward()? backward() defaults to the case of being called on a single scalar and uses the default argument torch.tensor(1.) This was previously the case when we called z.backward(). Since torch_relu is not a single scalar we need to explicitly provide a tensor of ones equal in shape to torch_relu.", "The sigmoid activation function produces the effect of mapping z from \u211d to the range [0,1]. When performing binary classification, we typically label instances belonging to the target class with the value 1, and all else with the value 0. We interpret the output of sigmoid as the probability that an instance belongs to the target class.", "Quiz: The task of a neural network is to perform binary classification. The output layer of this network consists of a single neuron with a sigmoid activation equal to 0.1. Among the following interpretations, which one(s) are correct?", "Solution: only 1 and 3 are correct. It is important to understand that a sigmoid-activated neuron with some output p, is implicitly giving an output of 1-p for the non-targeted class. It is also important to keep in mind that p is the probability associated with the target class (usually labeled as 1), while 1-p is the probability associated with the non-targeted class (usually labeled as 0).", "Observe: that the sum of p and (1-p) is equal to 1. This seems too obvious to point out at this stage, but it will be useful for us to keep it in mind when we discuss Softmax.", "Once again, we translate the math in Python then we check our results with the Pytorch implementation of sigmoid:", "There is something graceful about the differentiation of sigmoid. It does, however, take a sinuous path to reach its grace. Once we recall a few differentiation rules, we will have all that we need to saunter our way down the sinuous path.", "Having understood how to differentiate sigmoid, we can now implement the math and evaluate it with Pytorch\u2019s Autograd.", "Nowadays, ReLU has been widely adopted as a replacement for sigmoid. But sigmoid is still lingering around, hiding under the name of its more generalized form: Softmax.", "We think of sigmoid for binary classification, and softmax for multi-class classification. This association while correct, misleads many of us into thinking that sigmoid and softmax are two different functions. This is emphasized by the fact that when we look at the equations of sigmoid and softmax, it does not seem like there is an apparent link between them.", "Once again, the abstraction of the formula makes it anything but intuitive at first glance. An example will make it more concrete. We take a case of two output neurons, the first one (z0) outputs the probability that instances belong to a class labeled 0, the second one (z1) outputs the probability that instances belong to a class labeled 1. In fewer words, for z0 the target class is labeled 0, and for z1 the target class is labeled 1. To activate z0 and z1 with softmax we compute:", "Now we can remediate the seeming lack of an apparent link between sigmoid and softmax. We will do this by simply rewriting sigmoid:", "It is more common to see the first mentioned version of sigmoid than it is to see the second one. This is because the latter version is more expensive computationally. Its advantage, however, remains in helping us understand softmax.", "With only two neurons in the output layer, and given the fact that softmax makes all output neurons sum up to 1: we always know that Softmax(z0) is going to be equal to 1-Softmax(z1). Hence for binary classification, it makes sense to consider z0 equal to 0, and to only compute the activation of z1 using sigmoid.", "The following code implements softmax and tests it with an example of three output neurons. Then it compares our result with the result of Pytorch\u2019s softmax.", "We differentiate softmax activations with respect to each neuron. Keeping the same example of an output layer with two neurons, we get four softmax differentiations:", "Regardless of the number of output neurons, there are only two formulas for softmax differentiation. The first formula is applied when we differentiate the softmax of a neuron with respect to itself (top left and bottom right differentiations in the Jacobian). The second formula is applied when we differentiate the softmax of a neuron with respect to some other neuron (top right and bottom left differentiations in the Jacobian).", "To understand the steps involved in the differentiation of softmax, we need to recall one more differentiation rule:", "The following differentiations contain detailed steps. And although they might seem intimidating by the fact that they look dense, I assure you that they are much easier than they look, and I encourage you to redo them on paper.", "The implementation of the softmax differentiation requires us to iterate through the list of neurons and differentiate with respect to each neuron. Hence two loops are involved. Keep in mind that the purpose of these implementations is not to be performant, but rather to explicitly translate the math and arrive at the same results achieved by the built-in methods of Pytorch.", "In the sequence of operations involved in a neural network, softmax is generally followed by the cross-entropy loss. In fact, the two functions are so closely connected that in Pytorch the method cross_entropy combines both functions in one.", "I remember my first impression when I saw the formula for the cross-entropy loss. It was close to admiring hieroglyphs. After deciphering it, I hope you will share my awe towards how simple ideas can sometimes have the most complex representations.", "The variables involved in calculating the cross-entropy loss are p, y, m, and K. Both i and k are used as counters to iterate from 1 to m and K respectively.", "Let\u2019s say we are performing a multi-class classification task where the number of possible classes is three (K=3). Each instance can only belong to one class. Therefore each instance is assigned to a vector of labels with two zeros and a one. For example y=[0,0,1] means that the instance of y belongs to class 2. Similarly, y=[1,0,0] means that the instance of y belongs to class 0. The index of the 1 refers to the class to which the instance belongs. We say that the labels are one-hot encoded.", "We combine the above steps and observations in our implementation of cross-entropy. As usual, we will also go through the Pytorch equivalent method, before comparing both outputs.", "Note: Instead of storing the one-hot encoding of the labels, we simply store the index of the 1. For example, the previous y becomes [2,0]. Notice, at index 0 the value of y is 2, and at index 1 the value of y is 0. Using the indices of y and their values, we can directly retrieve the negative logs for the target classes. This is done by accessing -log(activations) at row 0 column 2, and at row 1 column 0. This allows us to avoid the wasteful multiplications and additions of zeros in steps 3 and 4. This trick is called integer array indexing and is explained by Jeremy Howard in his Deep Learning From The Foundations lecture 9 at 34:57", "If going forward through the layers of a neural network can be seen as its journey to acquire some kind of knowledge, this is the place where that knowledge can be found. Using the differentiation of a loss function can inform the neural network of how much it erred on each instance. Taking this error backwards, a neural network can adjust itself.", "We go through the differentiation steps of cross-entropy after we recall a couple differentiation rules:", "We will not be able to evaluate the following implementation with the output of Pytorch Autograd just yet. The reason goes back to Pytorch\u2019s cross_entropy combining softmax with cross-entropy. Consequently, using backward would also involve the differentiation of softmax in the chain rule. We discuss and implement this in the next section, Backpropagation. For now, here is our implementation of cross-entropy\u2019:", "With every function we discussed, we made one step forward in the layers of a neural network, and we also made the equivalent step backward using the differentiation of the functions. Since neural networks move all the way forward before retracing their steps all the way backward, we need to discuss how to link our functions.", "Going all the way forward, a neural network with one hidden layer starts by feeding input to a linear function, then feeds its output to a non-linear function, then feeds its output to a loss function. The following is an example with an instance x, its corresponding label y, three linear neurons z, each neuron computed using its three weights w and a bias b, followed by a softmax activation layer, and a cross-entropy loss.", "Going all the way backward, the same neural network starts by taking the same input given to the loss function, and feeds it instead to the derivative of that loss function. The output of the derivative of the loss function is the error, what we called the acquired knowledge. To adjust its parameters, the neural network has to carry this error another step backwards to the non-linear layer, and from there another step backwards to the linear layer.", "The next step backward is not as simple as feeding the error to the derivative of the non-linear function. We need to use the chain rule (which we previously recalled in the differentiation of sigmoid), and we also need to pay attention to the input we should give to each derivative.", "Key rules for feedforward and backpropagation:", "My impression is that a lot of people with backgrounds from different disciplines are curious and enthusiastic towards Machine Learning. Unfortunately, there is a justifiable trend towards acquiring the know-how while trying to keep away from the intimidating math. I consider this unfortunate because I believe many people are actually eager to deepen their understanding; If only they could find more resources that appeal to the fact that they come from different backgrounds and might need a little reminder and a little encouragement here and there.", "This article contained my attempt towards writing reader-friendly math. By which I mean math that reminds the reader of the rules required to follow along. And by which I also mean math with equations that avoid skipping so many steps and making us ponder what happened between one line and the next. Because sometimes we really need someone to take our hand and walk with us through the fields of unfamiliar concepts. My sincere hope that I was able to reach your hand.", "M. Amine, The Inner Workings of Neural Networks, My Colab Notebooks (2020).M. Amine, The Inner Workings of Neural Networks, My Gists, (2020).A. G\u00e9ron, Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, (2019).S. Gugger, A simple neural net in numpy, (2018).J. Howard, Fast.ai: Deep Learning from the Foundations Lesson 9, (2019).The Pytorch documentation.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Writer. AI Researcher. MSc Advanced Computer Science Intelligent Systems. BSc Computer Science and Mathematics. https://www.linkedin.com/in/mehdi-amine"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fbeac8760b595&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdismantling-neural-networks-to-understand-the-inner-workings-with-math-and-pytorch-beac8760b595&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdismantling-neural-networks-to-understand-the-inner-workings-with-math-and-pytorch-beac8760b595&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdismantling-neural-networks-to-understand-the-inner-workings-with-math-and-pytorch-beac8760b595&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdismantling-neural-networks-to-understand-the-inner-workings-with-math-and-pytorch-beac8760b595&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----beac8760b595--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----beac8760b595--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@m.e.mehdi.amine?source=post_page-----beac8760b595--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@m.e.mehdi.amine?source=post_page-----beac8760b595--------------------------------", "anchor_text": "Mehdi Amine"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff6182caa5a35&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdismantling-neural-networks-to-understand-the-inner-workings-with-math-and-pytorch-beac8760b595&user=Mehdi+Amine&userId=f6182caa5a35&source=post_page-f6182caa5a35----beac8760b595---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbeac8760b595&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdismantling-neural-networks-to-understand-the-inner-workings-with-math-and-pytorch-beac8760b595&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbeac8760b595&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdismantling-neural-networks-to-understand-the-inner-workings-with-math-and-pytorch-beac8760b595&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@florianklauer?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Florian Klauer"}, {"url": "https://unsplash.com/?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://pytorch.org/docs/stable/torch.html#torch.clamp", "anchor_text": "clamp"}, {"url": "https://numpy.org/doc/stable/reference/generated/numpy.clip.html", "anchor_text": "clip"}, {"url": "https://pytorch.org/docs/stable/nn.functional.html#relu", "anchor_text": "relu"}, {"url": "https://pytorch.org/docs/stable/nn.functional.html#softmax", "anchor_text": "softmax"}, {"url": "https://pytorch.org/docs/stable/nn.functional.html#cross-entropy", "anchor_text": "cross_entropy"}, {"url": "https://course.fast.ai/videos/?lesson=9", "anchor_text": "lecture 9"}, {"url": "https://pytorch.org/docs/stable/nn.functional.html#cross-entropy", "anchor_text": "cross_entropy"}, {"url": "https://pytorch.org/docs/stable/autograd.html#torch.autograd.backward", "anchor_text": "backward"}, {"url": "https://drive.google.com/drive/folders/1EkhYkKE74Kz2rYCvGM1BMTJ5ih7Q-toA?usp=sharing", "anchor_text": "The Inner Workings of Neural Networks, My Colab Notebooks"}, {"url": "https://gist.github.com/Mehdi-Amine", "anchor_text": "The Inner Workings of Neural Networks, My Gists"}, {"url": "https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/", "anchor_text": "Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow"}, {"url": "https://sgugger.github.io/a-simple-neural-net-in-numpy.html", "anchor_text": "A simple neural net in numpy"}, {"url": "https://course.fast.ai/videos/?lesson=9", "anchor_text": "Fast.ai: Deep Learning from the Foundations Lesson 9"}, {"url": "https://pytorch.org/docs/stable/index.html", "anchor_text": "The Pytorch documentation"}, {"url": "https://medium.com/tag/data-science?source=post_page-----beac8760b595---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----beac8760b595---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----beac8760b595---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----beac8760b595---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/python?source=post_page-----beac8760b595---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbeac8760b595&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdismantling-neural-networks-to-understand-the-inner-workings-with-math-and-pytorch-beac8760b595&user=Mehdi+Amine&userId=f6182caa5a35&source=-----beac8760b595---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbeac8760b595&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdismantling-neural-networks-to-understand-the-inner-workings-with-math-and-pytorch-beac8760b595&user=Mehdi+Amine&userId=f6182caa5a35&source=-----beac8760b595---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbeac8760b595&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdismantling-neural-networks-to-understand-the-inner-workings-with-math-and-pytorch-beac8760b595&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----beac8760b595--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fbeac8760b595&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdismantling-neural-networks-to-understand-the-inner-workings-with-math-and-pytorch-beac8760b595&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----beac8760b595---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----beac8760b595--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----beac8760b595--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----beac8760b595--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----beac8760b595--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----beac8760b595--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----beac8760b595--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----beac8760b595--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----beac8760b595--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@m.e.mehdi.amine?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@m.e.mehdi.amine?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Mehdi Amine"}, {"url": "https://medium.com/@m.e.mehdi.amine/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "70 Followers"}, {"url": "https://www.linkedin.com/in/mehdi-amine", "anchor_text": "https://www.linkedin.com/in/mehdi-amine"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff6182caa5a35&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdismantling-neural-networks-to-understand-the-inner-workings-with-math-and-pytorch-beac8760b595&user=Mehdi+Amine&userId=f6182caa5a35&source=post_page-f6182caa5a35--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F32a2916fde12&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdismantling-neural-networks-to-understand-the-inner-workings-with-math-and-pytorch-beac8760b595&newsletterV3=f6182caa5a35&newsletterV3Id=32a2916fde12&user=Mehdi+Amine&userId=f6182caa5a35&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}