{"url": "https://towardsdatascience.com/beyond-classification-with-transformers-and-hugging-face-d38c75f574fb", "time": 1683012221.001652, "path": "towardsdatascience.com/beyond-classification-with-transformers-and-hugging-face-d38c75f574fb/", "webpage": {"metadata": {"title": "Beyond Classification With Transformers and Hugging Face | by Nikhil Dharap | Towards Data Science", "h1": "Beyond Classification With Transformers and Hugging Face", "description": "As I started diving into the world of Transformers, and eventually into BERT and its siblings, a common theme that I came across was the Hugging Face library (link). It reminds me of scikit-learn\u2026"}, "outgoing_paragraph_urls": [{"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "here", "paragraph_index": 0}, {"url": "http://jalammar.github.io/illustrated-bert/", "anchor_text": "here", "paragraph_index": 0}, {"url": "https://huggingface.co/transformers/", "anchor_text": "link", "paragraph_index": 1}, {"url": "https://www.youtube.com/watch?v=FKlPCK1uFrc&list=PLam9sigHPGwOBuH4_4fr-XvDbe5uneaf6", "anchor_text": "here", "paragraph_index": 7}, {"url": "https://arxiv.org/pdf/1906.05714.pdf", "anchor_text": "here", "paragraph_index": 10}, {"url": "https://github.com/tensorflow/tensor2tensor", "anchor_text": "here", "paragraph_index": 10}, {"url": "https://github.com/nidharap/Notebooks/blob/master/Attentions_in_BERT.ipynb", "anchor_text": "here", "paragraph_index": 11}, {"url": "https://huggingface.co/transformers/model_doc/bert.html#bertmodel", "anchor_text": "here", "paragraph_index": 12}, {"url": "https://github.com/nidharap/Notebooks/blob/master/Attentions_in_BERT.ipynb", "anchor_text": "here", "paragraph_index": 17}, {"url": "https://samiraabnar.github.io/articles/2020-04/attention_flow", "anchor_text": "article", "paragraph_index": 19}, {"url": "https://en.wikipedia.org/wiki/Multidimensional_scaling", "anchor_text": "Link", "paragraph_index": 23}, {"url": "https://github.com/nidharap/Notebooks/blob/master/Word_Embeddings_BERT.ipynb", "anchor_text": "here", "paragraph_index": 24}, {"url": "https://huggingface.co/transformers/model_doc/bert.html#bertmodel", "anchor_text": "source", "paragraph_index": 31}, {"url": "http://proceedings.mlr.press/v37/kusnerb15.pdf", "anchor_text": "here", "paragraph_index": 37}, {"url": "https://github.com/src-d/wmd-relax", "anchor_text": "here", "paragraph_index": 37}, {"url": "https://stats.stackexchange.com/questions/303050/word-movers-distance-calculation-between-word-pairs-of-two-documents", "anchor_text": "here", "paragraph_index": 37}, {"url": "https://github.com/nidharap/Notebooks/blob/master/Sentence_Embeddings_RoBERTa.ipynb", "anchor_text": "here", "paragraph_index": 38}, {"url": "https://github.com/src-d/wmd-relax", "anchor_text": "WMD-relax", "paragraph_index": 39}, {"url": "https://github.com/nidharap/Notebooks/blob/master/IMDB_Classification.ipynb", "anchor_text": "here", "paragraph_index": 45}], "all_paragraphs": ["In this post, I plan to explore aspects of cutting edge architectures in NLP like BERT/Transformers. I assume that the readers are familiar with the Transformer architectures. To learn more about these, refer to Jay Alammar\u2019s post here and here . I am also going to explore a couple of BERT variations like BERT-base and RoBERTa-base models, but these techniques can be very easily extended to more recent architectures, thanks to Hugging Face!", "As I started diving into the world of Transformers, and eventually into BERT and its siblings, a common theme that I came across was the Hugging Face library (link). It reminds me of scikit-learn, which provides practitioners with easy access to almost every algorithm, and with a consistent interface. The Hugging Face library has accomplished the same kind of consistent and easy-to-use interface, but this time with deep learning based algorithms/architectures in the NLP world. We will dig into the architectures with the help of these interfaces provided by the this library.", "One of the main components of any Machine Learning (ML) projects is Exploratory Data Analysis (EDA). Every ML project I have been part of in the past years has started with me and my team doing EDA, iteratively, which helped understand and formulate a concrete problem statement. Figure 1 below demonstrates the typical ML process with an iterative EDA phase, which aims at answering questions about the data to help make decisions, typically about methods to leverage the data to solve specific business problems (say via modeling).", "With the advent of Deep Learning (DL), especially with the option of transfer-learning, the exploration phase now extends beyond looking at the data. It also entails another cycle of exploration for models, let\u2019s call that EMA (Exploratory Model Analysis), which involves understanding the model architectures, their pre-training process, the data and assumptions that went into the pre-training, the architecture\u2019s limitations (e.g. input size, model bias, types of problems they cannot solve), and the extent to which they can be fine-tuned for a downstream task. In other words, analyze where do they lie in the spectrum of re-training, few-shots fine-tuning to zero-shot (as they say in the GPT-3 world).", "In this article, I would like to focus more on EMA on BERT (and the likes), to understand what can it provide beyond fine-tuning for classification or Q&A. As I stated earlier, the Hugging Face library can provide us with the tools necessary to peek into the model, and explore various aspects of the model. More specifically, I would like to use the library to answer the following questions -", "These questions spawn from two pain points, limited availability of labelled data, and interpretability. Most of real world projects I have worked on, unlike Kaggle competitions, do not give us a nice labelled dataset, and the challenge is to justify the cost of creating labelled data. The second challenge, in some those projects, is also the ability to provide explanation of the model behavior, to hedge some flavor of risk.", "Without further ado, let\u2019s dive in! :)", "All the transformer based architectures today are based on attention mechanisms. I found that understanding the basics of how attention works helped me explore how that could be used as a tool for interpretation. I plan to describe the layers at a high level in this post, and focus more on how to extract them using the Transformers library from Hugging Face. If you need to understand the concept of attention in depth, I would suggest you go through Jay Alammar\u2019s blog (link provided earlier) or watch this playlist by Chris McCormick and Nick Ryan here.", "The Hugging Face library provides us with a way access the attention values across all attention heads in all hidden layers. In the BERT base model, we have 12 hidden layers, each with 12 attention heads. Each attention head has an attention weight matrix of size NxN (N is number of tokens from the tokenization process). In other words, we have a total of 144 matrices (12x12), each of size NxN. The final embedding size of each token at every layer input or output is 768 (which comes from 64 dimensional vectors from each attention head i.e. 64x12 = 768). This will be clear as you move to figure 4 below.", "Figure 3 provides the architecture for an encoder layer. Figure 4 below drills into the Attention block from Figure 3, and provides a simplified, high-level flow of one sentence through one attention layer of the BERT base model (ignoring the batch_size for simplicity). These diagrams hopefully provides clarity on what matrix will be returned when you turn the output_attentions flag to true via the library.", "Note: I found tools here and here, which enable us to visualize attentions. These tools are either deprecated or do not implement all the latest architectures. Further, instead of leveraging the well maintained APIs like Hugging Face, one of them re-implements the architectures within, which hampers the chance to run things for newer architectures.", "Let\u2019s quickly walkthrough the code (the full notebook can be found here). All sthe code here, except fine-tuning, can be run without a GPU", "The code above creates a Bert config with output_attentions=True, and uses this config to initialize the Bert base model. We put the model into eval mode since we just care about doing a forward pass through the architecture for this task. The code then goes onto tokenize and do a forward pass. The shape of the output is based on the config passed as described the documentation here. The first two items are last_hidden_state for the last layers and the pooled_output that can be used for fine tuning. The next input is what we are interested in, which is the attentions. As you can see from the last three statements, we can reach any layer, and any attention head, each of which will give us a NxN matrix we are interested in.", "We can quickly plot a heatmap for any of the 144 matrices like below", "The code above has two simple functions:", "\u201cget_attentions\u201d navigates to the particular layer and attention head, and grabs the NxN matrix to be visualized", "\u201cplt_attentions\u201d plots the matrix passed as a heat map", "As part of my EDA in the full notebook here, I plotted all the 144 heatmaps as a grid, and skimmed through them to spot some that had a good variation of attention weights. One of them in particular in Figure 3 below shows the relation between the words \u2018it\u2019 and \u2018animal\u2019 in the sentence \u201cThe animal didn\u2019t cross the street because it was too tired\u201d.", "As you can see, with some basic understanding of architecture, the transformers library by Hugging Face makes it extremely easy to pull out raw weights from any attention head. Now that we discussed how to pull out raw weights, let\u2019s talk a bit about whether we should use them directly to interpret what the model has learnt. A recent paper \u201cQuantifying Attention Flow in Transformers\u201d discussed exactly this aspect. They state \u201cacross layers of the Transformer, information originating from different tokens gets increasingly mixed\u201d", "This means reading too much into these weights to interpret how the model deconstructs the input text may not be very useful. They go on to device a strategy to help interpret the impact of inputs on outputs. I won\u2019t dive into the full paper here, but in short, they discuss building a Directed Acyclic Graph (DAG) on top of the architecture, which will help track paths and information flow between pairs of inputs and the hidden tokens. They discuss two approaches \u201cattention rollout\u201d and \u201cattention flow\u201d, that could be used to interpret attention weights as the relative relevance of the input tokens. In simple terms, instead of looking at only the raw attentions in a particular layer, you should consider a weighted flow of information all the way from the input embedding to the particular hidden output. If you are interested to know more, you can also refer to this article, that explains the paper with examples.", "In summary, to interpret the effect of specific inputs, instead of looking at only the raw attentions independently in each layer, we should take it a step further by using them to track the contribution all the way from the input embedding to specific outputs.", "As we discussed, it is quite easy to access the attention layers and the corresponding weights. The Hugging Face library also provides us with easy access to outputs from each layer. This allows us to generate word vectors, and potentially sentence vectors.", "Figure 6 below shows a few different ways we can extract word level vectors. We could average/sum/concat the last few layers to get a vector. The hypothesis would be that the initial layers (closer to the inputs) would learn low level features (like in a CNN), and the final few layers (closer to the output) would have a much richer representation of the words. We could also just extract the last or the second to last layer as a word/token vector. There is no consensus on what one should use, and it would really depend on the requirement of the downstream task, and it may not even matter so much if the task we are trying to run downstream is simple enough.", "Once we have word vectors, we are ready to use them in a downstream task like similarity. We can either directly visualize them using dimensionality reduction techniques like PCA or tSNE. Instead, I generally try to find distances between these vectors first in the higher dimension, and then use techniques like Multi Dimensional Scaling (MDS; ref Link) to visualize the distance matrix. Figure 7 below summarizes this approach (we can do this for sentences as well, as long as we plan to access to sentence vectors directly, more on this later in the post):", "Let\u2019s start looking through some code that implements the above flow. The full colab notebook can be found here", "We will follow the same process as we used to initialize and visualize attention weights, except this time we use \u201coutput_hidden_states=True\u201d while initializing the model.", "We also switch to encode_plus instead of encode, which will help us add CLS/SEP/PAD tokens, with a lot less effort. We build some additional logic around it to get attention masks that mask the PADs and also the CLS/SEPs. Notice the return values in the below function", "Let\u2019s define the model and call it on the sample sentences.", "Thats it! We are now ready to visualize these vectors in different configurations. Let\u2019s look at one of them, concatenating the word vectors from the last 4 layers, and visualizing. We pass two sentences", "And then call the function to plot the similarity", "The encoder based models provide us with a couple of options to get sentence vectors from the architecture. Figure 9 shows us these options:", "A quick note from the authors of BERT: This output is usually not a good summary of the semantic content of the input (source).", "2. Average token vectors across the sentence. The token vectors themselves, as we discussed above, could come from concatenating/averaging over last N-layers or directly from the Nth layer.", "Similarity of two sentences is very subjective. Two sentences could be very similar in one context, and could be treated as opposites in other contexts. For example, two sentences could be called similar because they are talking about certain a topic, and could be discussing both positive as well as negative aspects of the topic. Those sentences could be considered similar because they are talking about a common topic, but would be considered opposites, if the focus is polarity of the sentence. Most of these architectures are trained on an independent objectives like Masked Language Model(MLM) and Next Sentence Prediction (NSP), and are trained on large, varied datasets. What we get out-of-the-box in terms of similarity, may or may not be relevant, based on task at hand.", "Irrespective, it is important to look at our options of understanding similarity. Figure 11 below summarizes our options with sentence similarity in a flow diagram below.", "Option #1 and Option #2 above extract and try to create sentence vectors, which can then use the same pipeline we built for word vector similarity.", "Option #3 tries to compute similarity between two sentences directly from word vectors, instead of attempting to create a sentence vectors explicitly. This uses a special metric called Word Movers Distance (WMD).", "You can read the original paper for WMD here, but in short, it is based on EMD (Earth Movers Distance) and tries to move the words from one sentence to other using the word vectors. An example directly from the paper is show in Figure 10. There is a nice implementation of this here, and an awesome explanation here", "The code to run through sentence similarity options is available as a colab notebook here here", "Most of the code is very similar to the word embeddings piece we discussed earlier, except calculating the word movers distance. The class below calculates the word movers distance using WMD-relax library. it needs access to an embedding lookup, that yields a vector when a word is passed.", "I came across two levels of transfer learning for Transformer based models:", "The problems I work on generally do not have domain specific labeled datasets. This led me to explore the third option:", "To understand if this method of transfer learning works, let\u2019s do a quick experiment. Let\u2019s pick sentiment/polarity as the objective for our experimentation.", "We take the pre-trained RoBERTa model, fine-tune the model on the training set from the IMDB 50K movie reviews, and then pick our evaluation dataset as follows:", "We then extract and visualize sentence similarity on both the base pre-trained model (RoBERTa architecture in this case), and on the fine-tuned model on the same architecture", "The code to fine-tune BERT is available here.", "Legend: The yellow circles are positive, while the pink circles are negative", "Figure 11 below shows the the visualization run on the pre-trained model:", "Figure 12 below shows the same plot has Figure 11 but zoomed in (notice the axes have a different range). This shows that while the distance between the vectors is really small, after zooming in, we see that the model does attempt to place the sentences discussing the subject (food) closer to each other, favoring the subject of discussion more than the polarity of the sentence (see the bottom two circles, one yellow and one pink)", "Now let\u2019s look at the same sentences passed through the fine-tuned model in Figure 13 below. Notice how the sentences are much closer based on the polarity. As you can notice, it has moved its focus from the subject of discussion (food/movie) to polarity (positive vs negative): The positive sentence from the movie review data is closer to the positive sentence from the food review dataset, as is the case with the negative examples. This is exactly what we needed ! We wanted to use a dataset from a completely different domain, but with a similar labeling objective, to transfer learn!", "2. Let\u2019s do our second round of comparison by visualizing the Word Movers Distance (WMD) calculated on word vectors averaged from the last four layers.", "Figure 14 below shows similar characteristics as the models above. The subject of discussion (food) still seems to be the focus. One main difference here is that Word Movers Distance (WMD) is able to tease out the distances better than the earlier method (applying cosine distance on averaging word vectors across sentences), even on the base model i.e. there was no need to zoom in here. This shows that averaging of word vectors may have unintended consequences for a downstream task.", "Figure 15 below shows the same WMD metric on the four sentences using the Fine-tuned model, which again shows that we were able to \u201cshift\u201d the focus of the model towards polarity.", "In summary, the ability to transfer-learn from unrelated domain datasets will open more avenues for projects that struggle with data today. The power of transformers now can enable companies to leverage previously unusable datasets, and not have to spend time creating labelled datasets.", "I hope this article was helpful. There are few other topics like data augmentation in NLP, model interpretation via techniques like LIME (Local interpretable model-agnostic explanations) etc. that interest me and I do plan to explore them in future posts. Until then, thanks for reading!", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fd38c75f574fb&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-classification-with-transformers-and-hugging-face-d38c75f574fb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-classification-with-transformers-and-hugging-face-d38c75f574fb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-classification-with-transformers-and-hugging-face-d38c75f574fb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-classification-with-transformers-and-hugging-face-d38c75f574fb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----d38c75f574fb--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d38c75f574fb--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://ndharap.medium.com/?source=post_page-----d38c75f574fb--------------------------------", "anchor_text": ""}, {"url": "https://ndharap.medium.com/?source=post_page-----d38c75f574fb--------------------------------", "anchor_text": "Nikhil Dharap"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffdefd04bcc7a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-classification-with-transformers-and-hugging-face-d38c75f574fb&user=Nikhil+Dharap&userId=fdefd04bcc7a&source=post_page-fdefd04bcc7a----d38c75f574fb---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd38c75f574fb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-classification-with-transformers-and-hugging-face-d38c75f574fb&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd38c75f574fb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-classification-with-transformers-and-hugging-face-d38c75f574fb&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "here"}, {"url": "http://jalammar.github.io/illustrated-bert/", "anchor_text": "here"}, {"url": "https://huggingface.co/transformers/", "anchor_text": "link"}, {"url": "https://www.youtube.com/watch?v=FKlPCK1uFrc&list=PLam9sigHPGwOBuH4_4fr-XvDbe5uneaf6", "anchor_text": "here"}, {"url": "https://arxiv.org/pdf/1906.05714.pdf", "anchor_text": "here"}, {"url": "https://github.com/tensorflow/tensor2tensor", "anchor_text": "here"}, {"url": "https://github.com/nidharap/Notebooks/blob/master/Attentions_in_BERT.ipynb", "anchor_text": "here"}, {"url": "https://huggingface.co/transformers/model_doc/bert.html#bertmodel", "anchor_text": "here"}, {"url": "https://github.com/nidharap/Notebooks/blob/master/Attentions_in_BERT.ipynb", "anchor_text": "here"}, {"url": "https://samiraabnar.github.io/articles/2020-04/attention_flow", "anchor_text": "article"}, {"url": "https://en.wikipedia.org/wiki/Multidimensional_scaling", "anchor_text": "Link"}, {"url": "https://github.com/nidharap/Notebooks/blob/master/Word_Embeddings_BERT.ipynb", "anchor_text": "here"}, {"url": "https://huggingface.co/transformers/model_doc/bert.html#bertmodel", "anchor_text": "source"}, {"url": "http://proceedings.mlr.press/v37/kusnerb15.pdf", "anchor_text": "here"}, {"url": "https://github.com/src-d/wmd-relax", "anchor_text": "here"}, {"url": "https://stats.stackexchange.com/questions/303050/word-movers-distance-calculation-between-word-pairs-of-two-documents", "anchor_text": "here"}, {"url": "https://github.com/nidharap/Notebooks/blob/master/Sentence_Embeddings_RoBERTa.ipynb", "anchor_text": "here"}, {"url": "https://github.com/src-d/wmd-relax", "anchor_text": "WMD-relax"}, {"url": "https://github.com/nidharap/Notebooks/blob/master/IMDB_Classification.ipynb", "anchor_text": "here"}, {"url": "https://huggingface.co/transformers/model_doc/bert.html#bertmodel", "anchor_text": "BERT - transformers 3.0.2 documentationThe BERT model was proposed in BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding by\u2026huggingface.co"}, {"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "BERT: Pre-training of Deep Bidirectional Transformers for Language UnderstandingWe introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations\u2026arxiv.org"}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "The Illustrated TransformerDiscussions: Hacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments) Translations\u2026jalammar.github.io"}, {"url": "https://arxiv.org/abs/2005.00928", "anchor_text": "Quantifying Attention Flow in TransformersIn the Transformer model, \"self-attention\" combines information from attended embeddings into the representation of the\u2026arxiv.org"}, {"url": "https://samiraabnar.github.io/articles/2020-04/attention_flow", "anchor_text": "Quantifying Attention Flow in Transformers | Samira AbnarAttention has become the key building block of neural sequence processing models, and visualising attention weights is\u2026samiraabnar.github.io"}, {"url": "https://seaborn.pydata.org/generated/seaborn.heatmap.html", "anchor_text": "seaborn.heatmap - seaborn 0.10.1 documentationseaborn. heatmap( data, vmin=None, vmax=None, cmap=None, center=None, robust=False, annot=None, fmt='.2g'\u2026seaborn.pydata.org"}, {"url": "https://github.com/src-d/wmd-relax", "anchor_text": "src-d/wmd-relaxCalculates Word Mover's Distance as described in From Word Embeddings To Document Distances by Matt Kusner, Yu Sun\u2026github.com"}, {"url": "https://www.kaggle.com/snap/amazon-fine-food-reviews", "anchor_text": "Amazon Fine Food ReviewsAnalyze ~500,000 food reviews from Amazonwww.kaggle.com"}, {"url": "https://www.kaggle.com/atulanandjha/imdb-50k-movie-reviews-test-your-bert", "anchor_text": "IMDB 50K Movie Reviews (TEST your BERT)Pytorch-nlp Movie Dataset for Language Translation Models (ULMFiT, BERT, OpenAI)www.kaggle.com"}, {"url": "https://en.wikipedia.org/wiki/Multidimensional_scaling", "anchor_text": "Multidimensional scalingMultidimensional scaling ( MDS) is a means of visualizing the level of similarity of individual cases of a dataset. MDS\u2026en.wikipedia.org"}, {"url": "https://stats.stackexchange.com/questions/303050/word-movers-distance-calculation-between-word-pairs-of-two-documents", "anchor_text": "Word Mover's distance calculation between word pairs of two documentsbegingroup$ Each word in a document is matched against all other words, but weighted by a flow matrix $\\bf{T}$ (with\u2026stats.stackexchange.com"}, {"url": "https://arxiv.org/pdf/1906.05714.pdf", "anchor_text": "https://arxiv.org/pdf/1906.05714.pdf"}, {"url": "https://medium.com/tag/bert?source=post_page-----d38c75f574fb---------------bert-----------------", "anchor_text": "Bert"}, {"url": "https://medium.com/tag/attention?source=post_page-----d38c75f574fb---------------attention-----------------", "anchor_text": "Attention"}, {"url": "https://medium.com/tag/nlp?source=post_page-----d38c75f574fb---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/hugging-face?source=post_page-----d38c75f574fb---------------hugging_face-----------------", "anchor_text": "Hugging Face"}, {"url": "https://medium.com/tag/sentence-similarity?source=post_page-----d38c75f574fb---------------sentence_similarity-----------------", "anchor_text": "Sentence Similarity"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd38c75f574fb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-classification-with-transformers-and-hugging-face-d38c75f574fb&user=Nikhil+Dharap&userId=fdefd04bcc7a&source=-----d38c75f574fb---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd38c75f574fb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-classification-with-transformers-and-hugging-face-d38c75f574fb&user=Nikhil+Dharap&userId=fdefd04bcc7a&source=-----d38c75f574fb---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd38c75f574fb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-classification-with-transformers-and-hugging-face-d38c75f574fb&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d38c75f574fb--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fd38c75f574fb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-classification-with-transformers-and-hugging-face-d38c75f574fb&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----d38c75f574fb---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----d38c75f574fb--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----d38c75f574fb--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----d38c75f574fb--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----d38c75f574fb--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----d38c75f574fb--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----d38c75f574fb--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----d38c75f574fb--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----d38c75f574fb--------------------------------", "anchor_text": ""}, {"url": "https://ndharap.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://ndharap.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Nikhil Dharap"}, {"url": "https://ndharap.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "76 Followers"}, {"url": "http://www.linkedin.com/in/nikhil-dharap-61985416", "anchor_text": "www.linkedin.com/in/nikhil-dharap-61985416"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffdefd04bcc7a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-classification-with-transformers-and-hugging-face-d38c75f574fb&user=Nikhil+Dharap&userId=fdefd04bcc7a&source=post_page-fdefd04bcc7a--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fd9b4ee639552&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbeyond-classification-with-transformers-and-hugging-face-d38c75f574fb&newsletterV3=fdefd04bcc7a&newsletterV3Id=d9b4ee639552&user=Nikhil+Dharap&userId=fdefd04bcc7a&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}