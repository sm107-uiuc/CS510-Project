{"url": "https://towardsdatascience.com/animating-ganime-with-stylegan-part-1-4cf764578e", "time": 1683001094.384768, "path": "towardsdatascience.com/animating-ganime-with-stylegan-part-1-4cf764578e/", "webpage": {"metadata": {"title": "Animating gAnime with StyleGAN: Part 1 | by Nolan Kent | Towards Data Science", "h1": "Animating gAnime with StyleGAN: Part 1", "description": "This is a technical blog about a project I worked on using Generative Adversarial Networks. As this was a personal project, I played around with an anime dataset that I wouldn\u2019t normally use in a\u2026"}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/Generative_adversarial_network", "anchor_text": "Generative Adversarial Networks", "paragraph_index": 0}, {"url": "https://en.wikipedia.org/wiki/Generative_model", "anchor_text": "generative models", "paragraph_index": 5}, {"url": "https://arxiv.org/abs/1812.04948", "anchor_text": "StyleGAN", "paragraph_index": 5}, {"url": "https://github.com/NVlabs/stylegan", "anchor_text": "official code", "paragraph_index": 9}, {"url": "https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2", "anchor_text": "feature maps", "paragraph_index": 10}, {"url": "https://arxiv.org/pdf/1206.5538.pdf", "anchor_text": "disentangle", "paragraph_index": 19}, {"url": "https://arxiv.org/abs/1812.02230", "anchor_text": "formal definition of disentangment", "paragraph_index": 19}, {"url": "https://arxiv.org/abs/1905.12506", "anchor_text": "limited experimental evidence that it is useful for downstream tasks", "paragraph_index": 19}, {"url": "https://arxiv.org/abs/1906.00446", "anchor_text": "several", "paragraph_index": 19}, {"url": "https://arxiv.org/abs/1807.03039", "anchor_text": "other", "paragraph_index": 19}, {"url": "https://www.gwern.net/Danbooru2018", "anchor_text": "https://www.gwern.net/Danbooru2018", "paragraph_index": 20}, {"url": "https://arxiv.org/abs/1511.06434", "anchor_text": "DCGAN", "paragraph_index": 24}, {"url": "https://arxiv.org/abs/1812.04948", "anchor_text": "paper", "paragraph_index": 29}, {"url": "https://arxiv.org/abs/1710.10196", "anchor_text": "PGGAN", "paragraph_index": 30}, {"url": "https://arxiv.org/abs/1801.04406", "anchor_text": "R1 regularization", "paragraph_index": 35}, {"url": "https://arxiv.org/abs/1610.09585", "anchor_text": "ACGAN", "paragraph_index": 38}, {"url": "https://arxiv.org/abs/1802.05637", "anchor_text": "cGan with Projection Discriminator", "paragraph_index": 38}, {"url": "https://www.gwern.net/Danbooru2018", "anchor_text": "https://www.gwern.net/Danbooru2018", "paragraph_index": 42}, {"url": "https://github.com/horovod/horovod", "anchor_text": "Horovod", "paragraph_index": 48}, {"url": "https://arxiv.org/abs/1907.10786", "anchor_text": "https://arxiv.org/abs/1907.10786", "paragraph_index": 49}, {"url": "http://www.hiew.ru/", "anchor_text": "Hiew", "paragraph_index": 51}], "all_paragraphs": ["This is a technical blog about a project I worked on using Generative Adversarial Networks. As this was a personal project, I played around with an anime dataset that I wouldn\u2019t normally use in a professional environment. Here\u2019s a link to the dataset along with a detailed write-up about models that use the dataset:", "Most of the work I did was purely for learning purposes, but some of the more interesting results I ended up with were mouth animations for rectangular portraits:", "I\u2019ll go into the technical details of what I did and some of the lessons I learned. Part of the project was a tool to make interacting with and learning about generative adversarial networks easier, but at this point it is not user friendly. If I continue with this project one of my goals will be to publish a version of that tool that anyone can immediately start using to create animations like Figure 2, but right now it is primarily a tool for research:", "I\u2019ve found that incorporating experimental code into a tool like this instead of working with a jupyter notebook makes it much easier to repeat experiments with different settings. Some concepts only really stick through repetition, so without the tool I feel like I would have missed out on several of the insights mentioned in the blog. If you\u2019re just interested example animations and not the technical details, you can skip to the Results: Animation section.", "One of the main problems with personal projects is that they only involve a single perspective. My goal for this blog is to get other\u2019s perspective on the topic, detail my experience working on the project and receive constructive criticism/corrections.", "I\u2019ve been in the habit of regularly reimplementing papers on generative models for a couple years, so I started this project around the time the StyleGAN paper was published and have been working on it on and off since then. It includes 3 main parts:", "1. A reimplementation of StyleGAN with some tweaks", "2. Models trained using that implementation", "3. A tool used to visualize and interact with the models", "I started out reimplementing StyleGAN as a learning exercise, and because at the time the official code was not available. The results were so much better than the other models I worked with that I wanted to go more in depth. One application of generative models that I\u2019m excited about is automatic asset creation for video games. StyleGAN is the first model I\u2019ve implemented that had results that would acceptable to me in a video game, so my initial step was to try and make a game engine such as Unity load the model. To do this I made a .NET DLL which could interact with the model and theoretically be imported into Unity. To test the DLL I created a harness to interact with it. I ended up adding more and more features to the harness as I thought of them, until it became one of the biggest parts of the project. Here\u2019s the overall architecture:", "I\u2019m a fan of using tools to visualize and interact with digital objects that might otherwise be opaque (such as malware and deep learning models), so one feature I added was visualization of feature maps (Figure 5) and the ability to modify them. Observing where feature maps at various layers are most active across different images helped me understand what the model was doing, and made it simple to automatically locate some facial features. When it came to modification, I noticed that adding or subtracting values from feature maps in specific locations could be used to make meaningful changes \u2014 for example opening and closing a mouth (Figure 2). Combined with automatic facial feature detection, this can be used to make consistent, meaningful modifications in all images generated without needing labels (Figure 9,10).", "To summarize: a feature uses feature maps to modify facial features.", "Here\u2019s how the rest of the blog is structured:", "Overall, there are several applications that make me interested in generative models:", "Better procedural generation for game assets", "I am excited about the possibility of generative models bringing procedural generation to the next level. As far as I\u2019m aware, modern rules based procedural generation techniques cannot randomly create samples from highly complex distributions. For example, a subsection of a procedurally generated level can be mostly independent from the rest of the level and still be acceptable to a player. Randomly generating graphics such as character portraits is much more difficult, as good-looking images tend to reflect the real world which has so many inter-dependencies that each pixel needs to be considered in the context of every other pixel. Even stylized/drawn images need consistent lighting, anatomy, textures, perspective, style, etc. in order to look good. This also applies to automatically generating audio, dialog, animations, and story-lines. Generative models are currently the best way I\u2019m aware of to reliably create samples from such complicated distributions.", "Making the creation of art easier and faster", "In the context of generative models, interactive tools also have the potential to allow laypersons to create images that would otherwise require an experienced artist, or allow artists to speed up the more routine parts of their work. I don\u2019t think generative models are going to remove the need for creative artists any time soon, as generative models (along with most machine learning models) focus on modeling a specific distribution. This makes creating high quality images that are different from any samples in the training distribution (creative images) difficult. Tools like the one used in this blog that allow humans to add custom alterations can help to produce more unique images however, especially if the model has learned some fundamental graphical concepts such as lighting and perspective.", "The fundamental potential of unsupervised learning combined with disentanglement of generative factors", "As unlabeled data vastly outnumbers labeled data and deep learning is extremely data hungry, I think unsupervised/semi-supervised learning has the potential gradually replace supervised methods in the coming years. Some approaches to deep generative models in particular have been made to disentangle the factors of variation in a dataset: see Figure 7, 8 for examples of how StyleGAN can do this (at least partially). There isn\u2019t actually an agreed upon formal definition of disentangment, and my understanding is that there is limited experimental evidence that it is useful for downstream tasks. However, working with GANs has made me optimistic that it will be useful. While the generator can not classically produce internal representations for images it did not generate, there are several other types of generative models that can compete with GANs in visual quality and may be better suited for downstream tasks.", "The best model I was able to train was on an anime dataset (https://www.gwern.net/Danbooru2018). I\u2019ll go into the advantages and disadvantages of that in the data section, one of the main disadvantages being a lack of diversity: it is harder to produce male images. All of the examples below were generated with the tool. I originally created images like these in a jupyter notebook, but having a dedicated tool sped things up significantly and gave me a different perspective into how the model works. The images below are roughly ordered by how complicated it is to make them: Figures 7/8 require more work than Figure 6, and without a tool Figures 9/10 would have been harder to create than Figures 7/8.", "Figure 6 is an example of interpolation between the intermediate latent variables of several images. A cool result that GANs can achieve is ensuring the interpolated images have quality similar to the end points.", "Figure 7 is an example of modifying an image by locating a vector in the intermediate latent space with a certain meaning (in this case hair or eye color) and moving in that direction. For example, the vector for black hair can be found by taking many images of faces with black hair and averaging their latent values, and subtracting the result from the average of all other images. I\u2019ll discuss this more in the Training/Postprocessing section.", "Figure 8 shows the same concept used to generate Figure 7 when applied to the attribute \u2018mouth open\u2019. This works to some extent, but the attribute is not perfectly disentangled: changes can be seen in every part of the image, not just at the mouth. Common sense tells us that a person can move their mouth without significantly changing much else. A simple hack could be to paste the animated mouth on an otherwise static image, but this won\u2019t work in cases where changing the \u2018mouth open\u2019 vector also changes skin tone or image style.", "Figure 9 shows an example of modifying a specific feature map at a spatial location close to the character\u2019s mouth to produce a talking (or chewing) animation that does not cause global changes. With a little bit of manual work, the feature map(s) that need to be modified to produce this change can be found without needing more than a couple examples of labeled data. All of this can be done through the tool. The process is inspired by how the DCGAN paper demonstrated window deletion by modifying feature maps. The modifications are simple additions or subtractions to local areas of specific feature maps. I\u2019ll show how this can be done with the tool in a future blog.", "Figure 10 shows the same thing as Figure 9, except applied to the presence/absence of a hairband. This technique can be applied to many different attributes.", "Given my limited free time and that my primary goal was learning, I deprioritized a few aspects that are important to most projects:", "With these disclaimers out of the way, here\u2019re the gitub links. They are under active development so there may be breaking bugs in some commits:", "In this section I\u2019ll go into the technical details of what I did to reimplement StyleGAN and train a model with it", "I started reimplementing StyleGAN shortly after the paper came out, which was a few months before the official code was released. I\u2019ll discuss some of the challenges I encountered and approaches I took in this section, which assumes familiarity with the StyleGAN paper and TensorFlow.", "StyleGAN is based on PGGAN, which I had already reimplemented. These models use \u2018progressive growing\u2019, where the discriminator and generator grow during training to handle higher and higher resolutions. Growing a model is somewhat uncommon \u2014 all other models I\u2019ve implemented have never needed to change their architecture during training. Fortunately, TensorFlow has some convenient functions, such as the ability to load saved weights for only parts of a model and randomly initialize the rest. This is also used in transfer learning.", "I really like the TensorFlow 2.0 style of creating models as classes that inherit from tf.keras.Model. I created most of the layers, the generator, the discriminator, and the mapping network this way. I also tried to make it an option to switch between eager execution and traditional graph-based execution. Eager execution allows for much easier debugging, which I view as a way to better understand a program (a common technique in malware analysis). Unfortunately, at the time eager execution seemed to be significantly slower than running in graph mode, so eventually I stopped keeping that functionality up to date. One of the nice things about using tf.keras.Model is that it works for both eager and graph mode, so theoretically it should not be too difficult to get eager working again. In the mean time I\u2019ve just used the tfdebug command line interface and TensorBoard which I\u2019m fairly comfortable with at this point.", "StyleGAN has a few important differences from PGGAN. There\u2019s the titular method of supplying the image-specific latent data as a style (non-spatial attribute) to the feature maps while using an \u201cadaptive instance normalization\u201d operation. Conceptually this is straight forward to implement as detailed in the paper, but my choice to use tf.nn.moments to calculate mean and variance did not work as well as the official implementation\u2019s version which calculated these values with lower level operations. My first guess was that this was due to numerical issues, which was not something I felt like debugging at the time so I did not end up looking into it more. I\u2019m usually happy to look deeper into issues like this as they\u2019re obvious chances to learn more, but as this project is a hobby I have to prioritize to make the best use of my time.", "StyleGAN also uses an intermediate latent space which hypothetically (with some empirical evidence presented in the paper) promotes disentanglement by adding flexibility and dependencies to the range of latent values. For example, if we assume a population where men never have long hair, a latent vector that corresponds to hair length should never be in the \u2018long hair\u2019 region when another latent that corresponds to gender is in the \u2018male\u2019 region. If the latent vectors are independent, which is what happens without the mapping network, and we end up sampling \u2018long hair\u2019 alongside \u2018male\u2019, the generator will have to create an image that is either not male or has short hair in order to fool the discriminator. This means even if the hair length latent is in the \u2018long hair\u2019 region, we may end up creating an image with short hair when other latent values are in normal parts of their range. Note that some definitions of disentanglement require axis-alignment (modifying a single latent value results in a meaningful change), while my understanding is that the mapping network of StyleGAN encourages the intermediate latent space to be a rotation of axis-aligned disentanglement (modifying a vector of latent values results in a meaningful change).", "In my opinion the use of intermediate latent values is just as interesting as incorporating information into the network via styles. It is also simple to implement. When I refer to latent value throughout this blog series I will be referring to the intermediate latent value unless otherwise specified.", "It is annoyingly common for what initially looks like a minor detail in a paper to turn out to be the most difficult part to implement. This was the case for StyleGAN \u2014 one difference between StyleGAN and PGGAN is the use of bilinear upsampling (and downsampling) and of R1 regularization (a gradient penalty on the discriminator). Both of these are simple to implement individually, however when I tried to combine them it turned out that TensorFlow did not have a way to compute the second derivative of the tf.nn.depthwise_conv2d operation. Depth-wise convolutions are used to apply a filter individually to each channel. This is not normally needed in convolutional neural networks because (outside some CNNs meant for mobile devices) each filter connects to all channels in the previous layer. The blur filter that is commonly used to implement bilinear interpolation operates on one feature map at a time, so it requires a depthwise convolution. Without a second derivative implemented, I couldn\u2019t compute the R1 penalty which requires taking the gradient of a gradient. At the time I didn\u2019t know enough about automatic differentiation to easily implement the second derivative myself. I spent some time trying to get a better understanding, however at that point I had everything finished except for this part, and a short time later the official code was released. The team at Nvidia solved the problem nicely using two tf.custom_gradient functions in the blur filter.", "I made several experimental tweaks to StyleGAN with varying degrees of success.", "1. I tested out rectangular images by changing the initial resolution to 8x4 and growing from there", "2. I tried conditioning StyleGAN using ACGAN and cGan with Projection Discriminator", "Rectangular images worked well. Conditioning on labels with ACGAN and cGan did not. This may have been due to my hyperparameter selections, but generally the results were worse than training without conditioning and then finding vectors in latent space that corresponded to meaningful features (discussed in Training/Postprocessing section).", "The FFHQ dataset \u2014 a collection of 70,000 high resolution images \u2014 was released alongside the StyleGAN paper. To get closer to the goal of generating entire people, I tried modifying the data extraction script supplied by Nvidia to extract images with an 8:4 (h:w) aspect ratio. In addition to extracting the face, this extracted the same amount of data from below the face. Doubling both the height and the width would require accounting for 4x the input dimensions, but just doubling the height requires 2x the dimensions. Data below a person\u2019s face should also have less variance (variance mostly comes from different clothing) than background data, and not capturing background data means the network does not need to dedicate capacity to data that isn\u2019t part of the person.", "Unfortunately, only 20,000 of the 70,000 images in the FFHQ dataset had enough data below the person\u2019s face to create images with the desired aspect ratio. As can be seen in Figure 11, I was not able to get particularly high quality results with this small dataset, but there may be ways to improve the quality (such as expanding the criteria for including an image).", "I am also interested in the ability of GANs to perform on stylized drawings, and I saw that https://www.gwern.net/Danbooru2018 had recently been released. This dataset has a large number of high resolution images with very rich tag data. It does have some potential downsides, such as a disproportionately small number of male images which significantly reduces the quality of images in that class (Figure 12). The male images in this figure were the highest quality images cherry-picked out of around 1000 total images generated. I do think there is a lot of potential for improvement here, particularly using the truncation trick around the average male image.", "The dataset also contains a significant portion of NSFW images, though I think one potential use of generative models is automatically modifying media to make it more suitable for different audiences.", "To select candidate images to include, I was able to take advantage of metadata including \u2018favorites\u2019 (how many people favorited an image), resolution, tags, and creation date. To reduce variance and increase quality, I excluded images with few favorites and images that were created more than 6 years ago. I also only kept images with a resolution of at least 512x256 (HxW), my target resolution for the model. Finally, I filtered out images with tags that would increase variance in a portrait-style dataset, such as \u2018lying_down\u2019 and \u2018from_side\u2019, or that implied very NSFW images.", "To generate the data I used a combination of the following two tools which I modified to extract the images of the desired aspect ratio:", "These tools did not always extract images correctly, so I used illustration2vec to filter the result, as images where a person could not be detected were likely bad.", "I also created a tool that would display a large grid of images from which I could manually delete bad images quickly, but this became too time consuming for datasets above 30,000 images. I ended up with several different datasets by including images of various qualities and tags which ranged from 40,000 to 160,000 images. All of these resulted in much better models than the 20,000 image FFHQ dataset I built.", "I trained the model using as many as 80 epochs per resolution, which breaks down to 40 epochs for the PGGAN transition phase and 40 epochs for the stabilization phase. For the 160,000 image dataset this took over a month, and was probably overkill. I used Horovod to distribute training across two Nvidia Titan RTX graphics cards, which allowed large batch sizes in the early steps, and kept me from needing to ever go below 16 samples per batch.", "To gather attributes I generated hundreds of thousands of images and scanned them with illustration2vec to get attribute estimates. For each attribute with more than 1000 corresponding images, I averaged the latent values together and subtracted them from the overall average image. This creates a vector that, when added to a latent, should promote the expression of the desired attribute. While this worked pretty well (which I interpret as further evidence the mapping network disentangles attributes), I\u2019m interested in improving the process, potentially by using techniques described in https://arxiv.org/abs/1907.10786. In some cases I tried to manually disentangle correlated attributes: for example if blonde hair is correlated with blue eyes, subtracting a vector that corresponds to blue eyes from the blonde hair vector may help a bit. I then exported these attributes vectors into a csv file that could be loaded by the tool.", "I\u2019ve gained a lot of experience by trying to reimplement papers that rely on fundamentals I don\u2019t understand and then learning those fundamentals with a clear context for how they are used. The process feels a lot like backpropagation, and means I\u2019ve spent 30+ hours trying to fully understand a single paper early on. This is the first time I\u2019ve tried creating a tool to enhance that understanding, and I think it may become a standard strategy for me going forward.", "I believe visual tools are a great way to improve understanding of a topic. Coming from a malware analyst background, tools like Hiew were essential in shaping how I understand malware, even though it was not initially intuitive how visual analysis can be helpful for windows executables. Given the bandwidth and processing power available to the human visual system, my hypothesis is that we can quickly gain a lot of insight from most data with spatially local structures when represented as an image. This is also the type of data that convolutional neural networks seem to work well with (not too surprising given their biological inspiration). The concept also applies to convolutional networks themselves, which is one part of why I worked on this project: hopefully visualizing the feature maps of convolutional neural networks might help me understand them better.", "In the next part of this blog series I go into more detail about the tool and how it can be used to create animations. I also share a compiled version of the tool and a trained model to interact with, as the process to do that with just the source code is complicated at the moment."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F4cf764578e&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanimating-ganime-with-stylegan-part-1-4cf764578e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanimating-ganime-with-stylegan-part-1-4cf764578e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanimating-ganime-with-stylegan-part-1-4cf764578e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanimating-ganime-with-stylegan-part-1-4cf764578e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/@nkent?source=post_page-----4cf764578e--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4cf764578e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@nkent?source=post_page-----4cf764578e--------------------------------", "anchor_text": "Nolan Kent"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F352fdc0a3e4c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanimating-ganime-with-stylegan-part-1-4cf764578e&user=Nolan+Kent&userId=352fdc0a3e4c&source=post_page-352fdc0a3e4c----4cf764578e---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----4cf764578e--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4cf764578e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanimating-ganime-with-stylegan-part-1-4cf764578e&user=Nolan+Kent&userId=352fdc0a3e4c&source=-----4cf764578e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4cf764578e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanimating-ganime-with-stylegan-part-1-4cf764578e&source=-----4cf764578e---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://en.wikipedia.org/wiki/Generative_adversarial_network", "anchor_text": "Generative Adversarial Networks"}, {"url": "https://www.gwern.net/Danbooru2018", "anchor_text": "Danbooru2018: A Large-Scale Crowdsourced and Tagged Anime Illustration DatasetDeep learning for computer revision relies on large annotated datasets. Classification/categorization has benefited\u2026www.gwern.net"}, {"url": "https://www.gwern.net/Faces", "anchor_text": "Making Anime Faces With StyleGANGenerative neural networks, such as GANs, have struggled for years to generate decent-quality anime faces, despite\u2026www.gwern.net"}, {"url": "https://en.wikipedia.org/wiki/Generative_model", "anchor_text": "generative models"}, {"url": "https://arxiv.org/abs/1812.04948", "anchor_text": "StyleGAN"}, {"url": "https://github.com/NVlabs/stylegan", "anchor_text": "official code"}, {"url": "https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2", "anchor_text": "feature maps"}, {"url": "https://arxiv.org/pdf/1206.5538.pdf", "anchor_text": "disentangle"}, {"url": "https://arxiv.org/abs/1812.02230", "anchor_text": "formal definition of disentangment"}, {"url": "https://arxiv.org/abs/1905.12506", "anchor_text": "limited experimental evidence that it is useful for downstream tasks"}, {"url": "https://arxiv.org/abs/1906.00446", "anchor_text": "several"}, {"url": "https://arxiv.org/abs/1807.03039", "anchor_text": "other"}, {"url": "https://www.gwern.net/Danbooru2018", "anchor_text": "https://www.gwern.net/Danbooru2018"}, {"url": "https://arxiv.org/abs/1511.06434", "anchor_text": "DCGAN"}, {"url": "https://github.com/nolan-dev/stylegan_reimplementation", "anchor_text": "nolan-dev/stylegan_reimplementationThis is a reimplementation of NVidia's stylegan https://github.com/NVlabs/stylegan I did for learning purposes. My\u2026github.com"}, {"url": "https://github.com/nolan-dev/GANInterface", "anchor_text": "nolan-dev/GANInterfaceThis is a tool for interacting with images generated by a StyleGAN model. It has 3 parts: TensorflowInterface: Native\u2026github.com"}, {"url": "https://arxiv.org/abs/1812.04948", "anchor_text": "paper"}, {"url": "https://arxiv.org/abs/1710.10196", "anchor_text": "PGGAN"}, {"url": "https://arxiv.org/abs/1801.04406", "anchor_text": "R1 regularization"}, {"url": "https://arxiv.org/abs/1610.09585", "anchor_text": "ACGAN"}, {"url": "https://arxiv.org/abs/1802.05637", "anchor_text": "cGan with Projection Discriminator"}, {"url": "https://www.gwern.net/Danbooru2018", "anchor_text": "https://www.gwern.net/Danbooru2018"}, {"url": "https://github.com/qhgz2013/anime-face-detector", "anchor_text": "qhgz2013/anime-face-detectorA Faster-RCNN based anime face detector. This detector in trained on 6000 training samples and 641 testing samples\u2026github.com"}, {"url": "https://github.com/nagadomi/lbpcascade_animeface", "anchor_text": "nagadomi/lbpcascade_animefaceThe face detector for anime/manga using OpenCV. Original release since 2011 at\u2026github.com"}, {"url": "https://github.com/rezoo/illustration2vec", "anchor_text": "rezoo/illustration2vecillustration2vec (i2v) is a simple library for estimating a set of tags and extracting semantic feature vectors from\u2026github.com"}, {"url": "https://github.com/horovod/horovod", "anchor_text": "Horovod"}, {"url": "https://arxiv.org/abs/1907.10786", "anchor_text": "https://arxiv.org/abs/1907.10786"}, {"url": "http://www.hiew.ru/", "anchor_text": "Hiew"}, {"url": "https://towardsdatascience.com/animating-ganime-with-stylegan-the-tool-c5a2c31379d", "anchor_text": "Animating gAnime with StyleGAN: The ToolIn-depth tutorial for an open-source GAN research tooltowardsdatascience.com"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----4cf764578e---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/anime?source=post_page-----4cf764578e---------------anime-----------------", "anchor_text": "Anime"}, {"url": "https://medium.com/tag/generative-art?source=post_page-----4cf764578e---------------generative_art-----------------", "anchor_text": "Generative Art"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----4cf764578e---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----4cf764578e---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4cf764578e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanimating-ganime-with-stylegan-part-1-4cf764578e&user=Nolan+Kent&userId=352fdc0a3e4c&source=-----4cf764578e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4cf764578e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanimating-ganime-with-stylegan-part-1-4cf764578e&user=Nolan+Kent&userId=352fdc0a3e4c&source=-----4cf764578e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4cf764578e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanimating-ganime-with-stylegan-part-1-4cf764578e&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/@nkent?source=post_page-----4cf764578e--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4cf764578e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F352fdc0a3e4c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanimating-ganime-with-stylegan-part-1-4cf764578e&user=Nolan+Kent&userId=352fdc0a3e4c&source=post_page-352fdc0a3e4c----4cf764578e---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F352fdc0a3e4c%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanimating-ganime-with-stylegan-part-1-4cf764578e&user=Nolan+Kent&userId=352fdc0a3e4c&source=-----4cf764578e---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/@nkent?source=post_page-----4cf764578e--------------------------------", "anchor_text": "Written by Nolan Kent"}, {"url": "https://medium.com/@nkent/followers?source=post_page-----4cf764578e--------------------------------", "anchor_text": "183 Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----4cf764578e--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F352fdc0a3e4c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanimating-ganime-with-stylegan-part-1-4cf764578e&user=Nolan+Kent&userId=352fdc0a3e4c&source=post_page-352fdc0a3e4c----4cf764578e---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F352fdc0a3e4c%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanimating-ganime-with-stylegan-part-1-4cf764578e&user=Nolan+Kent&userId=352fdc0a3e4c&source=-----4cf764578e---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/malware-analysis-with-visual-pattern-recognition-5a4d087c9d26?source=author_recirc-----4cf764578e----0---------------------4eb3c2cd_f076_496e_94e8_572e61af5563-------", "anchor_text": ""}, {"url": "https://medium.com/@nkent?source=author_recirc-----4cf764578e----0---------------------4eb3c2cd_f076_496e_94e8_572e61af5563-------", "anchor_text": ""}, {"url": "https://medium.com/@nkent?source=author_recirc-----4cf764578e----0---------------------4eb3c2cd_f076_496e_94e8_572e61af5563-------", "anchor_text": "Nolan Kent"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----4cf764578e----0---------------------4eb3c2cd_f076_496e_94e8_572e61af5563-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/malware-analysis-with-visual-pattern-recognition-5a4d087c9d26?source=author_recirc-----4cf764578e----0---------------------4eb3c2cd_f076_496e_94e8_572e61af5563-------", "anchor_text": "Malware Analysis with Visual Pattern RecognitionThe secret to quickly reverse-engineering binary files"}, {"url": "https://towardsdatascience.com/malware-analysis-with-visual-pattern-recognition-5a4d087c9d26?source=author_recirc-----4cf764578e----0---------------------4eb3c2cd_f076_496e_94e8_572e61af5563-------", "anchor_text": "18 min read\u00b7May 27, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5a4d087c9d26&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmalware-analysis-with-visual-pattern-recognition-5a4d087c9d26&user=Nolan+Kent&userId=352fdc0a3e4c&source=-----5a4d087c9d26----0-----------------clap_footer----4eb3c2cd_f076_496e_94e8_572e61af5563-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/malware-analysis-with-visual-pattern-recognition-5a4d087c9d26?source=author_recirc-----4cf764578e----0---------------------4eb3c2cd_f076_496e_94e8_572e61af5563-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5a4d087c9d26&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmalware-analysis-with-visual-pattern-recognition-5a4d087c9d26&source=-----4cf764578e----0-----------------bookmark_preview----4eb3c2cd_f076_496e_94e8_572e61af5563-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----4cf764578e----1---------------------4eb3c2cd_f076_496e_94e8_572e61af5563-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----4cf764578e----1---------------------4eb3c2cd_f076_496e_94e8_572e61af5563-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----4cf764578e----1---------------------4eb3c2cd_f076_496e_94e8_572e61af5563-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----4cf764578e----1---------------------4eb3c2cd_f076_496e_94e8_572e61af5563-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----4cf764578e----1---------------------4eb3c2cd_f076_496e_94e8_572e61af5563-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----4cf764578e----1---------------------4eb3c2cd_f076_496e_94e8_572e61af5563-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----4eb3c2cd_f076_496e_94e8_572e61af5563-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----4cf764578e----1---------------------4eb3c2cd_f076_496e_94e8_572e61af5563-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----4cf764578e----1-----------------bookmark_preview----4eb3c2cd_f076_496e_94e8_572e61af5563-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----4cf764578e----2---------------------4eb3c2cd_f076_496e_94e8_572e61af5563-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----4cf764578e----2---------------------4eb3c2cd_f076_496e_94e8_572e61af5563-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----4cf764578e----2---------------------4eb3c2cd_f076_496e_94e8_572e61af5563-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----4cf764578e----2---------------------4eb3c2cd_f076_496e_94e8_572e61af5563-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----4cf764578e----2---------------------4eb3c2cd_f076_496e_94e8_572e61af5563-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----4cf764578e----2---------------------4eb3c2cd_f076_496e_94e8_572e61af5563-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----2-----------------clap_footer----4eb3c2cd_f076_496e_94e8_572e61af5563-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----4cf764578e----2---------------------4eb3c2cd_f076_496e_94e8_572e61af5563-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----4cf764578e----2-----------------bookmark_preview----4eb3c2cd_f076_496e_94e8_572e61af5563-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/animating-ganime-with-stylegan-the-tool-c5a2c31379d?source=author_recirc-----4cf764578e----3---------------------4eb3c2cd_f076_496e_94e8_572e61af5563-------", "anchor_text": ""}, {"url": "https://medium.com/@nkent?source=author_recirc-----4cf764578e----3---------------------4eb3c2cd_f076_496e_94e8_572e61af5563-------", "anchor_text": ""}, {"url": "https://medium.com/@nkent?source=author_recirc-----4cf764578e----3---------------------4eb3c2cd_f076_496e_94e8_572e61af5563-------", "anchor_text": "Nolan Kent"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----4cf764578e----3---------------------4eb3c2cd_f076_496e_94e8_572e61af5563-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/animating-ganime-with-stylegan-the-tool-c5a2c31379d?source=author_recirc-----4cf764578e----3---------------------4eb3c2cd_f076_496e_94e8_572e61af5563-------", "anchor_text": "Animating gAnime with StyleGAN: The ToolIn-depth tutorial for an open-source GAN research tool"}, {"url": "https://towardsdatascience.com/animating-ganime-with-stylegan-the-tool-c5a2c31379d?source=author_recirc-----4cf764578e----3---------------------4eb3c2cd_f076_496e_94e8_572e61af5563-------", "anchor_text": "24 min read\u00b7Nov 7, 2019"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc5a2c31379d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanimating-ganime-with-stylegan-the-tool-c5a2c31379d&user=Nolan+Kent&userId=352fdc0a3e4c&source=-----c5a2c31379d----3-----------------clap_footer----4eb3c2cd_f076_496e_94e8_572e61af5563-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/animating-ganime-with-stylegan-the-tool-c5a2c31379d?source=author_recirc-----4cf764578e----3---------------------4eb3c2cd_f076_496e_94e8_572e61af5563-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc5a2c31379d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanimating-ganime-with-stylegan-the-tool-c5a2c31379d&source=-----4cf764578e----3-----------------bookmark_preview----4eb3c2cd_f076_496e_94e8_572e61af5563-------", "anchor_text": ""}, {"url": "https://medium.com/@nkent?source=post_page-----4cf764578e--------------------------------", "anchor_text": "See all from Nolan Kent"}, {"url": "https://towardsdatascience.com/?source=post_page-----4cf764578e--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----4cf764578e----0---------------------66f755bd_bfc3_43cc_8a57_72fdc91d8fb1-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----4cf764578e----0---------------------66f755bd_bfc3_43cc_8a57_72fdc91d8fb1-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----4cf764578e----0---------------------66f755bd_bfc3_43cc_8a57_72fdc91d8fb1-------", "anchor_text": "The PyCoach"}, {"url": "https://artificialcorner.com/?source=read_next_recirc-----4cf764578e----0---------------------66f755bd_bfc3_43cc_8a57_72fdc91d8fb1-------", "anchor_text": "Artificial Corner"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----4cf764578e----0---------------------66f755bd_bfc3_43cc_8a57_72fdc91d8fb1-------", "anchor_text": "You\u2019re Using ChatGPT Wrong! Here\u2019s How to Be Ahead of 99% of ChatGPT UsersMaster ChatGPT by learning prompt engineering."}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----4cf764578e----0---------------------66f755bd_bfc3_43cc_8a57_72fdc91d8fb1-------", "anchor_text": "\u00b77 min read\u00b7Mar 17"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fartificial-corner%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&user=The+PyCoach&userId=fb44e21903f3&source=-----886a50dabc54----0-----------------clap_footer----66f755bd_bfc3_43cc_8a57_72fdc91d8fb1-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----4cf764578e----0---------------------66f755bd_bfc3_43cc_8a57_72fdc91d8fb1-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "276"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&source=-----4cf764578e----0-----------------bookmark_preview----66f755bd_bfc3_43cc_8a57_72fdc91d8fb1-------", "anchor_text": ""}, {"url": "https://pub.towardsai.net/getting-started-with-stable-diffusion-f343639e4931?source=read_next_recirc-----4cf764578e----1---------------------66f755bd_bfc3_43cc_8a57_72fdc91d8fb1-------", "anchor_text": ""}, {"url": "https://youssefraafat57.medium.com/?source=read_next_recirc-----4cf764578e----1---------------------66f755bd_bfc3_43cc_8a57_72fdc91d8fb1-------", "anchor_text": ""}, {"url": "https://youssefraafat57.medium.com/?source=read_next_recirc-----4cf764578e----1---------------------66f755bd_bfc3_43cc_8a57_72fdc91d8fb1-------", "anchor_text": "Youssef Hosni"}, {"url": "https://pub.towardsai.net/?source=read_next_recirc-----4cf764578e----1---------------------66f755bd_bfc3_43cc_8a57_72fdc91d8fb1-------", "anchor_text": "Towards AI"}, {"url": "https://pub.towardsai.net/getting-started-with-stable-diffusion-f343639e4931?source=read_next_recirc-----4cf764578e----1---------------------66f755bd_bfc3_43cc_8a57_72fdc91d8fb1-------", "anchor_text": "Getting Started With Stable DiffusionStable Diffusion is a text-to-image latent diffusion model created by researchers and engineers from CompVis, Stability AI, and LAION. It\u2019s\u2026"}, {"url": "https://pub.towardsai.net/getting-started-with-stable-diffusion-f343639e4931?source=read_next_recirc-----4cf764578e----1---------------------66f755bd_bfc3_43cc_8a57_72fdc91d8fb1-------", "anchor_text": "\u00b712 min read\u00b7Nov 11, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-artificial-intelligence%2Ff343639e4931&operation=register&redirect=https%3A%2F%2Fpub.towardsai.net%2Fgetting-started-with-stable-diffusion-f343639e4931&user=Youssef+Hosni&userId=859af34925b7&source=-----f343639e4931----1-----------------clap_footer----66f755bd_bfc3_43cc_8a57_72fdc91d8fb1-------", "anchor_text": ""}, {"url": "https://pub.towardsai.net/getting-started-with-stable-diffusion-f343639e4931?source=read_next_recirc-----4cf764578e----1---------------------66f755bd_bfc3_43cc_8a57_72fdc91d8fb1-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff343639e4931&operation=register&redirect=https%3A%2F%2Fpub.towardsai.net%2Fgetting-started-with-stable-diffusion-f343639e4931&source=-----4cf764578e----1-----------------bookmark_preview----66f755bd_bfc3_43cc_8a57_72fdc91d8fb1-------", "anchor_text": ""}, {"url": "https://medium.com/geekculture/stop-doing-this-on-chatgpt-get-ahead-99-users-ai-artificial-intelligence-productivity-prompt-engineering-4-f3441bf7a25a?source=read_next_recirc-----4cf764578e----0---------------------66f755bd_bfc3_43cc_8a57_72fdc91d8fb1-------", "anchor_text": ""}, {"url": "https://medium.com/@rfeers?source=read_next_recirc-----4cf764578e----0---------------------66f755bd_bfc3_43cc_8a57_72fdc91d8fb1-------", "anchor_text": ""}, {"url": "https://medium.com/@rfeers?source=read_next_recirc-----4cf764578e----0---------------------66f755bd_bfc3_43cc_8a57_72fdc91d8fb1-------", "anchor_text": "Josep Ferrer"}, {"url": "https://medium.com/geekculture?source=read_next_recirc-----4cf764578e----0---------------------66f755bd_bfc3_43cc_8a57_72fdc91d8fb1-------", "anchor_text": "Geek Culture"}, {"url": "https://medium.com/geekculture/stop-doing-this-on-chatgpt-get-ahead-99-users-ai-artificial-intelligence-productivity-prompt-engineering-4-f3441bf7a25a?source=read_next_recirc-----4cf764578e----0---------------------66f755bd_bfc3_43cc_8a57_72fdc91d8fb1-------", "anchor_text": "Stop doing this on ChatGPT and get ahead of the 99% of its usersUnleash the Power of AI Writing with Effective Prompts"}, {"url": "https://medium.com/geekculture/stop-doing-this-on-chatgpt-get-ahead-99-users-ai-artificial-intelligence-productivity-prompt-engineering-4-f3441bf7a25a?source=read_next_recirc-----4cf764578e----0---------------------66f755bd_bfc3_43cc_8a57_72fdc91d8fb1-------", "anchor_text": "\u00b78 min read\u00b7Mar 31"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fgeekculture%2Ff3441bf7a25a&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fgeekculture%2Fstop-doing-this-on-chatgpt-get-ahead-99-users-ai-artificial-intelligence-productivity-prompt-engineering-4-f3441bf7a25a&user=Josep+Ferrer&userId=8213af8f3ccf&source=-----f3441bf7a25a----0-----------------clap_footer----66f755bd_bfc3_43cc_8a57_72fdc91d8fb1-------", "anchor_text": ""}, {"url": "https://medium.com/geekculture/stop-doing-this-on-chatgpt-get-ahead-99-users-ai-artificial-intelligence-productivity-prompt-engineering-4-f3441bf7a25a?source=read_next_recirc-----4cf764578e----0---------------------66f755bd_bfc3_43cc_8a57_72fdc91d8fb1-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "71"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff3441bf7a25a&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fgeekculture%2Fstop-doing-this-on-chatgpt-get-ahead-99-users-ai-artificial-intelligence-productivity-prompt-engineering-4-f3441bf7a25a&source=-----4cf764578e----0-----------------bookmark_preview----66f755bd_bfc3_43cc_8a57_72fdc91d8fb1-------", "anchor_text": ""}, {"url": "https://medium.com/geekculture/a-simple-way-to-run-stable-diffusion-2-0-locally-on-your-pc-no-code-guide-3beb911e444c?source=read_next_recirc-----4cf764578e----1---------------------66f755bd_bfc3_43cc_8a57_72fdc91d8fb1-------", "anchor_text": ""}, {"url": "https://jimclydemonge.medium.com/?source=read_next_recirc-----4cf764578e----1---------------------66f755bd_bfc3_43cc_8a57_72fdc91d8fb1-------", "anchor_text": ""}, {"url": "https://jimclydemonge.medium.com/?source=read_next_recirc-----4cf764578e----1---------------------66f755bd_bfc3_43cc_8a57_72fdc91d8fb1-------", "anchor_text": "Jim Clyde Monge"}, {"url": "https://medium.com/geekculture?source=read_next_recirc-----4cf764578e----1---------------------66f755bd_bfc3_43cc_8a57_72fdc91d8fb1-------", "anchor_text": "Geek Culture"}, {"url": "https://medium.com/geekculture/a-simple-way-to-run-stable-diffusion-2-0-locally-on-your-pc-no-code-guide-3beb911e444c?source=read_next_recirc-----4cf764578e----1---------------------66f755bd_bfc3_43cc_8a57_72fdc91d8fb1-------", "anchor_text": "A Simple Way To Run Stable Diffusion 2.0 Locally On Your PC \u2014 No Code GuideAn easy and no-code guide on how to run Stable Diffusion 2.0 on local PC with Web UI."}, {"url": "https://medium.com/geekculture/a-simple-way-to-run-stable-diffusion-2-0-locally-on-your-pc-no-code-guide-3beb911e444c?source=read_next_recirc-----4cf764578e----1---------------------66f755bd_bfc3_43cc_8a57_72fdc91d8fb1-------", "anchor_text": "\u00b73 min read\u00b7Dec 1, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fgeekculture%2F3beb911e444c&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fgeekculture%2Fa-simple-way-to-run-stable-diffusion-2-0-locally-on-your-pc-no-code-guide-3beb911e444c&user=Jim+Clyde+Monge&userId=819323b399ac&source=-----3beb911e444c----1-----------------clap_footer----66f755bd_bfc3_43cc_8a57_72fdc91d8fb1-------", "anchor_text": ""}, {"url": "https://medium.com/geekculture/a-simple-way-to-run-stable-diffusion-2-0-locally-on-your-pc-no-code-guide-3beb911e444c?source=read_next_recirc-----4cf764578e----1---------------------66f755bd_bfc3_43cc_8a57_72fdc91d8fb1-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "9"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3beb911e444c&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fgeekculture%2Fa-simple-way-to-run-stable-diffusion-2-0-locally-on-your-pc-no-code-guide-3beb911e444c&source=-----4cf764578e----1-----------------bookmark_preview----66f755bd_bfc3_43cc_8a57_72fdc91d8fb1-------", "anchor_text": ""}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----4cf764578e----2---------------------66f755bd_bfc3_43cc_8a57_72fdc91d8fb1-------", "anchor_text": ""}, {"url": "https://medium.com/@timothymugayi?source=read_next_recirc-----4cf764578e----2---------------------66f755bd_bfc3_43cc_8a57_72fdc91d8fb1-------", "anchor_text": ""}, {"url": "https://medium.com/@timothymugayi?source=read_next_recirc-----4cf764578e----2---------------------66f755bd_bfc3_43cc_8a57_72fdc91d8fb1-------", "anchor_text": "Timothy Mugayi"}, {"url": "https://betterprogramming.pub/?source=read_next_recirc-----4cf764578e----2---------------------66f755bd_bfc3_43cc_8a57_72fdc91d8fb1-------", "anchor_text": "Better Programming"}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----4cf764578e----2---------------------66f755bd_bfc3_43cc_8a57_72fdc91d8fb1-------", "anchor_text": "How To Build Your Own Custom ChatGPT With Custom Knowledge BaseFeed your ChatGPT bot with custom data sources"}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----4cf764578e----2---------------------66f755bd_bfc3_43cc_8a57_72fdc91d8fb1-------", "anchor_text": "\u00b711 min read\u00b7Apr 7"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fbetter-programming%2F4e61ad82427e&operation=register&redirect=https%3A%2F%2Fbetterprogramming.pub%2Fhow-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e&user=Timothy+Mugayi&userId=34774d6cac27&source=-----4e61ad82427e----2-----------------clap_footer----66f755bd_bfc3_43cc_8a57_72fdc91d8fb1-------", "anchor_text": ""}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----4cf764578e----2---------------------66f755bd_bfc3_43cc_8a57_72fdc91d8fb1-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "83"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4e61ad82427e&operation=register&redirect=https%3A%2F%2Fbetterprogramming.pub%2Fhow-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e&source=-----4cf764578e----2-----------------bookmark_preview----66f755bd_bfc3_43cc_8a57_72fdc91d8fb1-------", "anchor_text": ""}, {"url": "https://medium.com/@steinsfu/diffusion-model-clearly-explained-cd331bd41166?source=read_next_recirc-----4cf764578e----3---------------------66f755bd_bfc3_43cc_8a57_72fdc91d8fb1-------", "anchor_text": ""}, {"url": "https://medium.com/@steinsfu?source=read_next_recirc-----4cf764578e----3---------------------66f755bd_bfc3_43cc_8a57_72fdc91d8fb1-------", "anchor_text": ""}, {"url": "https://medium.com/@steinsfu?source=read_next_recirc-----4cf764578e----3---------------------66f755bd_bfc3_43cc_8a57_72fdc91d8fb1-------", "anchor_text": "Steins"}, {"url": "https://medium.com/@steinsfu/diffusion-model-clearly-explained-cd331bd41166?source=read_next_recirc-----4cf764578e----3---------------------66f755bd_bfc3_43cc_8a57_72fdc91d8fb1-------", "anchor_text": "Diffusion Model Clearly Explained!How does AI artwork work? Understanding the tech behind the rise of AI-generated art."}, {"url": "https://medium.com/@steinsfu/diffusion-model-clearly-explained-cd331bd41166?source=read_next_recirc-----4cf764578e----3---------------------66f755bd_bfc3_43cc_8a57_72fdc91d8fb1-------", "anchor_text": "\u00b77 min read\u00b7Dec 26, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2Fcd331bd41166&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40steinsfu%2Fdiffusion-model-clearly-explained-cd331bd41166&user=Steins&userId=a36be384d77d&source=-----cd331bd41166----3-----------------clap_footer----66f755bd_bfc3_43cc_8a57_72fdc91d8fb1-------", "anchor_text": ""}, {"url": "https://medium.com/@steinsfu/diffusion-model-clearly-explained-cd331bd41166?source=read_next_recirc-----4cf764578e----3---------------------66f755bd_bfc3_43cc_8a57_72fdc91d8fb1-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcd331bd41166&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40steinsfu%2Fdiffusion-model-clearly-explained-cd331bd41166&source=-----4cf764578e----3-----------------bookmark_preview----66f755bd_bfc3_43cc_8a57_72fdc91d8fb1-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----4cf764578e--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----4cf764578e--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----4cf764578e--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----4cf764578e--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----4cf764578e--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----4cf764578e--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----4cf764578e--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----4cf764578e--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----4cf764578e--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----4cf764578e--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}