{"url": "https://towardsdatascience.com/logistic-regressions-and-rare-events-weighted-maximum-likelihood-versus-oversampling-92a25ef74d4e", "time": 1683000676.2538428, "path": "towardsdatascience.com/logistic-regressions-and-rare-events-weighted-maximum-likelihood-versus-oversampling-92a25ef74d4e/", "webpage": {"metadata": {"title": "Logistic Regressions and Rare Events | by Ryan Stevens | Towards Data Science", "h1": "Logistic Regressions and Rare Events", "description": "I previously worked on designing some problem sets for a PhD class. One of the assignments dealt with a simple classification problem using data that I took from a kaggle challenge trying to predict\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.kaggle.com/mlg-ulb/creditcardfraud", "anchor_text": "kaggle challenge", "paragraph_index": 0}, {"url": "https://github.com/ryanlstevens/logistic_regression_rare_events", "anchor_text": "github", "paragraph_index": 3}], "all_paragraphs": ["I previously worked on designing some problem sets for a PhD class. One of the assignments dealt with a simple classification problem using data that I took from a kaggle challenge trying to predict fraudulent credit card transactions. The goal of the problem is to predict the probability that a specific credit card transaction is fraudulent. One unforeseen issue with the data was that the unconditional probability that a single credit card transaction is fraudulent is very small. This type of data is known as rare events data, and is common in many areas such as disease detection, conflict prediction and, of course, fraud detection.", "This note compares two common techniques to improve classification with rare events data. The basic idea behind these techniques is to increase the discriminative power of a classification algorithm by over-fitting on the rare events. One technique uses sampling weights in the likelihood function itself, whereas, the other is a type of case-control design where we over-sample from the rare class. Which is preferable depends on both predictive performance, as well as, computational complexity. In this example, we will be \u201cfixing\u201d computational complexity by comparing predictive performance across logistic regression models estimated using MLE.", "An additional motivation for this note is that looking under the hood of common ML libraries can be difficult. Given that many of methods are wrappers of C Libraries, tracking down exact implementation can be tedious. One hope of this exercise is to show how using sample_weights in the sklearn library compares to using a stratified sampling procedure I create.", "All of the code and a hyperlink to the data are available on my github.", "To motivate the problem, let\u2019s simply compare our standard likelihood when our data is Bernoulli distributed with a sample weighted likelihood. The \u201cstandard likelihood\u201d for a Bernoulli distributed random variable is as follows:", "A weighted likelihood approach simply attempts to re-weight our data to account for imbalance across classes. We introduce the following term to represent sample weights:", "The likelihood simply pre-multiplies each term by a weighting factor giving us a \u201cweighted likelihood\u201d:", "Instead of incorporating weights directly into the likelihood, we can instead re-balance our data using a re-sampling procedure. We then maximize the \u201cstandard likelihood\u201d in on this sample. The approach this note focuses on is a form of stratified sampling. The procedure is as follows:", "Because I am estimating using MLE in both approaches, these techniques are similar in computational complexity. However, the second technique is indifferent to the model you are estimating. This same procedure can be used for SVMs, Neural Networks and more complex models. Furthermore, it is flexible and additional stages of sampling or decision rules for inclusion in the sample can be changed. My next note will focus on an extension of this stratified procedure that attempts to create matched samples based on \u201ccloseness\u201d of observations across the classes.", "Another thing to note is a peculiarity about sklearn, the library has a StratifiedKFold method in the model_selection library. However, this method creates folds which preserve the balance between the classes. For example, if class 0 has 10 observations and class 1 has 90 observations, the splits will keep the proportion of 0 to 1 the same, i.e. there will always be 1 observation from class 0 to 9 observations from class 1.", "To summarize, this exercise will compare predictive performance using two separate ways to re-weight our data. Given some pre-chosen weights, which we denote by omega:", "We read in our data and split the dependent and independent variables from one another. Here the independent variable is defined as \u201cClass\u201d. We can see in the graph below that there are only 492 transactions that are fraudulent out of 284,807 total transactions.", "I am doing all this in python. While sklearn has a K-Fold class, which produces K-splits of the data, my specific task requires the following sampling procedure:", "In order to accomplish this, I extend the KFold class creating the oversample_KFold class. This new class requires a sample weight to sample from each class.", "We create a model pipeline that has the following steps:", "This is the meat of this exercise. What we will do is estimate both a weighted logistic regression and a standard logistic regression with stratified random sampling. We will then plot three relevant model score metrics: accuracy, recall and precision. What we will see is how bad accuracy is for predictions of rare events. We will then see how the two strategies differ in their recall versus the precision.", "We have three different performance metrics: accuracy, recall and precision. The blue lines in each plot are the metrics value for the standard MLE with no sample weights. The x-axis are the sample weights given to the rare events. As you increase this sample weight, you give more weight to rare event options. Note, that I run the models between 0.05 and 0.95, hence why the stratified sample procedure starts above the MLE line. Also, I only ran the re-sampling methods once, thus there is some noise in these estimates. However, given the monotonicity of the lines in the stratified sampling graphs, the sampling error must be minor.", "There are two things to note from these graphs. First, accuracy is a bad measure for both of these models. Looking at the stratified sample, even when giving nearly all weight to the stratified sample, the model has an accuracy of around 75%. Second, stratified sampling performs vastly better on recall, while, for high weights it performs vastly worse on precision than the weighted MLE.", "This dovetails nicely with thinking about the exact loss function you are trying to pin down in this problem. The stakeholders for this sort of algorithm would be a credit card company. For a credit card company, missing an instance of fraud is much worse than calling an instance of non-fraud a fraudulent activity. The company is on the hook for fraudulent charges, so they want to minimize those as much as possible. With that in mind, they may want to detect all the cases of fraud, regardless of the cases of non-fraud. To me, this sounds like recall, higher values of predicting fraud, or true positives, are preferred. If we just cared about recall, then our stratified sampling model does much better.", "However, there is some cost to a credit card company from rejecting too many charges. If you are a customer of a credit card company that denies all your charges, then you would probably get another credit card. In this case, a credit card company probably also care about minimizing false positives. With this in mind, we can look at precision. What we see if for low weights re-sampling does about as good as weighted MLE, but for higher weights it does much worse.", "The above exercise shows that model selection metrics are important for your task at hand. When faced with rare events data, accuracy performs poorly, whereas, recall and precision depend on the model at hand. A key place in this algorithm for improvement is the independence in sampling across classes. The current algorithm selects a class, then an observation from that class. However, it may be better to select an observation of interest, say an example of fraud, then select a very similar example of non-fraud to distinguish the differences between them. The underlying idea is that it is easy to spot obvious non-fraud, such as someone purchasing a $4 cup of coffee from their local coffee shop, and these are not the important cases that the algorithm relies upon to train.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Senior Data Scientist at Ramp; Ex-Meta; PhD in Economics, NYU."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F92a25ef74d4e&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regressions-and-rare-events-weighted-maximum-likelihood-versus-oversampling-92a25ef74d4e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regressions-and-rare-events-weighted-maximum-likelihood-versus-oversampling-92a25ef74d4e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regressions-and-rare-events-weighted-maximum-likelihood-versus-oversampling-92a25ef74d4e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regressions-and-rare-events-weighted-maximum-likelihood-versus-oversampling-92a25ef74d4e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----92a25ef74d4e--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----92a25ef74d4e--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://ryan-louis-stevens.medium.com/?source=post_page-----92a25ef74d4e--------------------------------", "anchor_text": ""}, {"url": "https://ryan-louis-stevens.medium.com/?source=post_page-----92a25ef74d4e--------------------------------", "anchor_text": "Ryan Stevens"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F14a02184ad5c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regressions-and-rare-events-weighted-maximum-likelihood-versus-oversampling-92a25ef74d4e&user=Ryan+Stevens&userId=14a02184ad5c&source=post_page-14a02184ad5c----92a25ef74d4e---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F92a25ef74d4e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regressions-and-rare-events-weighted-maximum-likelihood-versus-oversampling-92a25ef74d4e&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F92a25ef74d4e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regressions-and-rare-events-weighted-maximum-likelihood-versus-oversampling-92a25ef74d4e&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.kaggle.com/mlg-ulb/creditcardfraud", "anchor_text": "kaggle challenge"}, {"url": "https://github.com/ryanlstevens/logistic_regression_rare_events", "anchor_text": "github"}, {"url": "https://gist.github.com/7fd33cf5fbe824f9100ef5a071c08c03", "anchor_text": "http://github.com"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----92a25ef74d4e---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/logistic-regression?source=post_page-----92a25ef74d4e---------------logistic_regression-----------------", "anchor_text": "Logistic Regression"}, {"url": "https://medium.com/tag/sklearn?source=post_page-----92a25ef74d4e---------------sklearn-----------------", "anchor_text": "Sklearn"}, {"url": "https://medium.com/tag/crossvalidation?source=post_page-----92a25ef74d4e---------------crossvalidation-----------------", "anchor_text": "Crossvalidation"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F92a25ef74d4e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regressions-and-rare-events-weighted-maximum-likelihood-versus-oversampling-92a25ef74d4e&user=Ryan+Stevens&userId=14a02184ad5c&source=-----92a25ef74d4e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F92a25ef74d4e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regressions-and-rare-events-weighted-maximum-likelihood-versus-oversampling-92a25ef74d4e&user=Ryan+Stevens&userId=14a02184ad5c&source=-----92a25ef74d4e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F92a25ef74d4e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regressions-and-rare-events-weighted-maximum-likelihood-versus-oversampling-92a25ef74d4e&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----92a25ef74d4e--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F92a25ef74d4e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regressions-and-rare-events-weighted-maximum-likelihood-versus-oversampling-92a25ef74d4e&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----92a25ef74d4e---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----92a25ef74d4e--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----92a25ef74d4e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----92a25ef74d4e--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----92a25ef74d4e--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----92a25ef74d4e--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----92a25ef74d4e--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----92a25ef74d4e--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----92a25ef74d4e--------------------------------", "anchor_text": ""}, {"url": "https://ryan-louis-stevens.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://ryan-louis-stevens.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ryan Stevens"}, {"url": "https://ryan-louis-stevens.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "74 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F14a02184ad5c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regressions-and-rare-events-weighted-maximum-likelihood-versus-oversampling-92a25ef74d4e&user=Ryan+Stevens&userId=14a02184ad5c&source=post_page-14a02184ad5c--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fbc39743742fb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regressions-and-rare-events-weighted-maximum-likelihood-versus-oversampling-92a25ef74d4e&newsletterV3=14a02184ad5c&newsletterV3Id=bc39743742fb&user=Ryan+Stevens&userId=14a02184ad5c&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}