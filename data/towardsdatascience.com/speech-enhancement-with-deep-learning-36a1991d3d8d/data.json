{"url": "https://towardsdatascience.com/speech-enhancement-with-deep-learning-36a1991d3d8d", "time": 1683003053.577827, "path": "towardsdatascience.com/speech-enhancement-with-deep-learning-36a1991d3d8d/", "webpage": {"metadata": {"title": "Speech-enhancement with Deep learning | by Vincent Belz | Towards Data Science", "h1": "Speech-enhancement with Deep learning", "description": "Audio has many different ways to be represented, going from raw time series to time-frequency decompositions. The choice of representation is crucial for the performance of your system. Among\u2026"}, "outgoing_paragraph_urls": [{"url": "https://github.com/vbelz/Speech-enhancement", "anchor_text": "https://github.com/vbelz/Speech-enhancement", "paragraph_index": 2}, {"url": "http://www.openslr.org/12/", "anchor_text": "LibriSpeech", "paragraph_index": 4}, {"url": "https://sisec.inria.fr/sisec-2015/2015-two-channel-mixtures-of-speech-and-real-world-background-noise/", "anchor_text": "SiSec", "paragraph_index": 4}, {"url": "https://github.com/karoldvl/ESC-50", "anchor_text": "ESC-50 dataset", "paragraph_index": 4}, {"url": "https://www.ee.columbia.edu/~dpwe/sounds/", "anchor_text": "https://www.ee.columbia.edu/~dpwe/sounds/", "paragraph_index": 4}, {"url": "https://unsplash.com/", "anchor_text": "https://unsplash.com", "paragraph_index": 5}, {"url": "https://arxiv.org/abs/1505.04597", "anchor_text": "U-Net", "paragraph_index": 13}], "all_paragraphs": ["Audio has many different ways to be represented, going from raw time series to time-frequency decompositions. The choice of representation is crucial for the performance of your system. Among time-frequency decompositions, Spectrograms have been proved to be a useful representation for audio processing. They consist of 2D images representing sequences of Short Time Fourier Transform (STFT) with time and frequency as axes, and brightness representing the strength of a frequency component at each time frame. As such, they appear a natural domain to apply the CNNS architectures for images directly to sound. Between magnitude and phase spectrograms, magnitude spectrograms contain most the structure of the signal. Phase spectrograms appear to show only little temporal and spectral regularities.", "In this project, I will use magnitude spectrograms as a representation of sound (cf image below) in order to predict the noise model to be subtracted to a noisy voice spectrogram.", "The project is decomposed in three modes: data creation, training and prediction. The code is available at the Github repository: https://github.com/vbelz/Speech-enhancement", "To create the datasets for training, I gathered English speech examples with clean voices and environmental noises from different sources.", "The clean voices were mainly gathered from LibriSpeech: an ASR corpus based on public domain audiobooks. I used as well some data from SiSec. The environmental noises were gathered from ESC-50 dataset or https://www.ee.columbia.edu/~dpwe/sounds/.", "For this project, I focused on 10 classes of environmental noise: ticking clock, footsteps, bells, handsaw, alarm, fireworks, insects, brushing teeth, vacuum cleaner and snoring. These classes are illustrated in the image below (I created this image using pictures from https://unsplash.com.)", "To create the datasets for training/validation/testing, audios were sampled at 8kHz and I extracted windows slightly above 1 second. I performed some data augmentation for the environmental noises (taking the windows at different times creates different noise windows). Noises have been blended to clean voices with randomization of the noise level (between 20% and 80%). In the end, training data consisted of 10h of noisy voice & clean voices and validation data of 1h of sound.", "To prepare the data, I recommend creating data/Train and data/Test folders in a location separate from your code folder. Then create the following structure seen in the image below:", "You would modify the noise_dir, voice_dir, path_save_spectrogram, path_save_time_serie, and path_save_sound paths name accordingly into the args.py file that takes the default parameters for the program.", "Place your noise audio files into noise_dir directory and your clean voice files into voice_dir.", "Specify how many frames you want to create as nb_samples in args.py (or pass it as an argument from the terminal) I let nb_samples=50 by default for the demo but for production, I would recommend having 40 000 or more.", "Then run python main.py --mode='data_creation'. This will randomly blend some clean voices from voice_dir with some noises from noise_dir and save the spectrograms of noisy voices, noises and clean voices to disk as well as complex phases, time series and sounds (for QC or to test other networks). It takes the inputs parameters defined in args.py. Parameters for STFT, frame length, hop_length can be modified in args.py (or pass it as arguments from the terminal), but with the default parameters, each window will be converted into spectrogram matrix of size 128 x 128.", "Datasets to be used for training will be magnitude spectrograms of noisy voices and magnitude spectrograms of clean voices.", "The model used for the training is a U-Net, a Deep Convolutional Autoencoder with symmetric skip connections. U-Net was initially developed for Biomedical Image Segmentation. Here the U-Net has been adapted to denoise spectrograms.", "As input to the network, the magnitude spectrograms of the noisy voices. As output the Noise to model (noisy voice magnitude spectrogram \u2014 clean voice magnitude spectrogram). Both input and output matrix are scaled with a global scaling to be mapped into a distribution between -1 and 1.", "Many configurations have been tested during the training. For the preferred configuration the encoder is made of 10 convolutional layers (with LeakyReLU, maxpooling and dropout). The decoder is a symmetric expanding path with skip connections. The last activation layer is a hyperbolic tangent (tanh) to have an output distribution between -1 and 1. For training from scratch the initial random weights were set with He normal initializer.", "Model is compiled with Adam optimizer and the loss function used is the Huber loss as a compromise between the L1 and L2 loss.", "Training on a modern GPU takes a couple of hours.", "If you have a GPU for deep learning computation in your local computer, you can train with: python main.py --mode=\"training\". It takes as inputs parameters defined in args.py. By default, it will train from scratch (you can change this by turning training_from_scratch to false). You can start training from pre-trained weights specified in weights_folder and name_model. I let available model_unet.h5 with weights from my training in ./weights. The number of epochs and the batch size for training are specified by epochs and batch_size. Best weights are automatically saved during training as model_best.h5. You can call fit_generator to only load part of the data to disk at training time.", "Personally, I used the free GPU available at Google Colab for my training. I let a notebook example at./colab/Train_denoise.ipynb. If you have a large available space on your drive, you can load all your training data to your drive and load part of it at training time with the fit_generator option of tensorflow.keras. Personally I had limited space available on my Google drive so I pre-prepared in advanced batches of 5Gb to be loaded to drive for training. Weights were regularly saved and reload for the next training.", "In the end, I obtained a training loss of 0.002129 and a validation loss of 0.002406. Below a loss graph made in one of the training.", "For prediction, the noisy voice audios are converted into a NumPy time series of windows slightly above 1 second. Each time series is converted into a magnitude spectrogram and a phase spectrogram via STFT transforms. Noisy voice spectrograms are passed into the U-Net network that will predict the noise model for each window (cf graph below). Prediction time for one window once converted to magnitude spectrogram is around 80 ms using classical CPU.", "Then the model is subtracted from the noisy voice spectrogram (here I apply a direct subtraction as it was sufficient for my task, we could imagine training a second network to adapt the noise model, or applying a matching filter such as performed in signal processing). The \u201cdenoised\u201d magnitude spectrogram is combined with the initial phase as input for the inverse Short Time Fourier Transform (ISTFT). Our denoised time series can be then converted to audio (cf graph below).", "Let\u2019s have a look at the performance on validation data!", "Below I display some results from validation examples for Alarm/Insects/Vaccum cleaner/Bells noise. For each of them, I display the initial noisy voice spectrogram, the denoised spectrogram predicted by the network, and the true clean voice spectrogram. We can see that the network is well able to generalize the noise modelling, and produce a slightly smoothed version of the voice spectrogram, very close to the true clean voice spectrogram.", "More examples of spectrogram denoising on validation data are displayed in the initial gif on top of the repository.", "Let\u2019s hear the results converted back to sounds:", "Below I show the corresponding displays converting back to time series:", "You can have a look at these displays/audios in the jupyter notebook demo_predictions.ipynb that I provide in the ./demo_data folder.", "Below, I show the corresponding gif of the spectrogram denoising gif (top of the repository) in the time series domain.", "As extreme testing, I applied to some voices blended with many noises at a high level. The network appeared to work surprisingly well for the denoising. The total time to denoise a 5 seconds audio was around 4 seconds (using classical CPU).", "A Deep learning speech enhancement system to attenuate environmental noise has been presented. By using a magnitude spectrogram representation of sound, the audio denoising problem has been transformed into an image processing problem, simplifying its resolution. The Noise to remove has been modelled by a U-Net, a Deep Convolutional Autoencoder with symmetric skip connections. After training, the network is able to model 10 classes of environmental noises. This approach is imperfect as the noisy phase spectrogram is not modelled, which might decrease the total performance in particular cases. Additionally, this approach is more suitable for offline denoising as denoising a 5 seconds audio currently takes around 4 seconds (using classical CPU for prediction). Still, the approach is robust and able to generalize for many voices and noise configurations. Additionally, it can be transfer learned to other languages than English and new types of noise.", "Jansson, Andreas, Eric J. Humphrey, Nicola Montecchio, Rachel M. Bittner, Aparna Kumar and Tillman Weyde.Singing Voice Separation with Deep U-Net Convolutional Networks. ISMIR (2017).", "Grais, Emad M. and Plumbley, Mark D., Single Channel Audio Source Separation using Convolutional Denoising Autoencoders (2017).", "Ronneberger O., Fischer P., Brox T. (2015) U-Net: Convolutional Networks for Biomedical Image Segmentation. In: Navab N., Hornegger J., Wells W., Frangi A. (eds) Medical Image Computing and Computer-Assisted Intervention \u2014 MICCAI 2015. MICCAI 2015. Lecture Notes in Computer Science, vol 9351. Springer, Cham", "K. J. Piczak. ESC: Dataset for Environmental Sound Classification. Proceedings of the 23rd Annual ACM Conference on Multimedia, Brisbane, Australia, 2015.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F36a1991d3d8d&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspeech-enhancement-with-deep-learning-36a1991d3d8d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspeech-enhancement-with-deep-learning-36a1991d3d8d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspeech-enhancement-with-deep-learning-36a1991d3d8d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspeech-enhancement-with-deep-learning-36a1991d3d8d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----36a1991d3d8d--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----36a1991d3d8d--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://vincentbelz.medium.com/?source=post_page-----36a1991d3d8d--------------------------------", "anchor_text": ""}, {"url": "https://vincentbelz.medium.com/?source=post_page-----36a1991d3d8d--------------------------------", "anchor_text": "Vincent Belz"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F21dd18b1dfde&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspeech-enhancement-with-deep-learning-36a1991d3d8d&user=Vincent+Belz&userId=21dd18b1dfde&source=post_page-21dd18b1dfde----36a1991d3d8d---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F36a1991d3d8d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspeech-enhancement-with-deep-learning-36a1991d3d8d&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F36a1991d3d8d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspeech-enhancement-with-deep-learning-36a1991d3d8d&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://github.com/vbelz/Speech-enhancement", "anchor_text": "https://github.com/vbelz/Speech-enhancement"}, {"url": "http://www.openslr.org/12/", "anchor_text": "LibriSpeech"}, {"url": "https://sisec.inria.fr/sisec-2015/2015-two-channel-mixtures-of-speech-and-real-world-background-noise/", "anchor_text": "SiSec"}, {"url": "https://github.com/karoldvl/ESC-50", "anchor_text": "ESC-50 dataset"}, {"url": "https://www.ee.columbia.edu/~dpwe/sounds/", "anchor_text": "https://www.ee.columbia.edu/~dpwe/sounds/"}, {"url": "https://unsplash.com/", "anchor_text": "https://unsplash.com"}, {"url": "https://arxiv.org/abs/1505.04597", "anchor_text": "U-Net"}, {"url": "https://vbelz.github.io/Speech-enhancement/demo_data/validation/noisy_voice_alarm39.wav", "anchor_text": "Input example alarm"}, {"url": "https://vbelz.github.io/Speech-enhancement/demo_data/validation/voice_pred_alarm39.wav", "anchor_text": "Predicted output example alarm"}, {"url": "https://vbelz.github.io/Speech-enhancement/demo_data/validation/voice_alarm39.wav", "anchor_text": "True output example alarm"}, {"url": "https://vbelz.github.io/Speech-enhancement/demo_data/validation/noisy_voice_insect41.wav", "anchor_text": "Input example insects"}, {"url": "https://vbelz.github.io/Speech-enhancement/demo_data/validation/voice_pred_insect41.wav", "anchor_text": "Predicted output example insects"}, {"url": "https://vbelz.github.io/Speech-enhancement/demo_data/validation/voice_insect41.wav", "anchor_text": "True output example insects"}, {"url": "https://vbelz.github.io/Speech-enhancement/demo_data/validation/noisy_voice_vaccum35.wav", "anchor_text": "Input example vaccum cleaner"}, {"url": "https://vbelz.github.io/Speech-enhancement/demo_data/validation/voice_pred_vaccum35.wav", "anchor_text": "Predicted output example vaccum cleaner"}, {"url": "https://vbelz.github.io/Speech-enhancement/demo_data/validation/voice_vaccum35.wav", "anchor_text": "True output example vaccum cleaner"}, {"url": "https://vbelz.github.io/Speech-enhancement/demo_data/validation/noisy_voice_bells28.wav", "anchor_text": "Input example bells"}, {"url": "https://vbelz.github.io/Speech-enhancement/demo_data/validation/voice_pred_bells28.wav", "anchor_text": "Predicted output example bells"}, {"url": "https://vbelz.github.io/Speech-enhancement/demo_data/validation/voice_bells28.wav", "anchor_text": "True output example bells"}, {"url": "https://vbelz.github.io/Speech-enhancement/demo_data/test/noisy_voice_long_t2.wav", "anchor_text": "Input example test 1"}, {"url": "https://vbelz.github.io/Speech-enhancement/demo_data/save_predictions/denoise_t2.wav", "anchor_text": "Predicted output example test 1"}, {"url": "https://vbelz.github.io/Speech-enhancement/demo_data/test/noisy_voice_long_t1.wav", "anchor_text": "Input example test 2"}, {"url": "https://vbelz.github.io/Speech-enhancement/demo_data/save_predictions/denoise_t1.wav", "anchor_text": "Predicted output example test 2"}, {"url": "https://ejhumphrey.com/assets/pdf/jansson2017singing.pdf", "anchor_text": "https://ejhumphrey.com/assets/pdf/jansson2017singing.pdf"}, {"url": "https://arxiv.org/abs/1703.08019", "anchor_text": "https://arxiv.org/abs/1703.08019"}, {"url": "https://arxiv.org/abs/1505.04597", "anchor_text": "https://arxiv.org/abs/1505.04597"}, {"url": "http://dx.doi.org/10.1145/2733373.2806390", "anchor_text": "http://dx.doi.org/10.1145/2733373.2806390"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----36a1991d3d8d---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/tag/speech?source=post_page-----36a1991d3d8d---------------speech-----------------", "anchor_text": "Speech"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----36a1991d3d8d---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/unet?source=post_page-----36a1991d3d8d---------------unet-----------------", "anchor_text": "Unet"}, {"url": "https://medium.com/tag/cnn?source=post_page-----36a1991d3d8d---------------cnn-----------------", "anchor_text": "Cnn"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F36a1991d3d8d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspeech-enhancement-with-deep-learning-36a1991d3d8d&user=Vincent+Belz&userId=21dd18b1dfde&source=-----36a1991d3d8d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F36a1991d3d8d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspeech-enhancement-with-deep-learning-36a1991d3d8d&user=Vincent+Belz&userId=21dd18b1dfde&source=-----36a1991d3d8d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F36a1991d3d8d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspeech-enhancement-with-deep-learning-36a1991d3d8d&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----36a1991d3d8d--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F36a1991d3d8d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspeech-enhancement-with-deep-learning-36a1991d3d8d&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----36a1991d3d8d---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----36a1991d3d8d--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----36a1991d3d8d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----36a1991d3d8d--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----36a1991d3d8d--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----36a1991d3d8d--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----36a1991d3d8d--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----36a1991d3d8d--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----36a1991d3d8d--------------------------------", "anchor_text": ""}, {"url": "https://vincentbelz.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://vincentbelz.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Vincent Belz"}, {"url": "https://vincentbelz.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "116 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F21dd18b1dfde&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspeech-enhancement-with-deep-learning-36a1991d3d8d&user=Vincent+Belz&userId=21dd18b1dfde&source=post_page-21dd18b1dfde--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F892746427cf1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspeech-enhancement-with-deep-learning-36a1991d3d8d&newsletterV3=21dd18b1dfde&newsletterV3Id=892746427cf1&user=Vincent+Belz&userId=21dd18b1dfde&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}